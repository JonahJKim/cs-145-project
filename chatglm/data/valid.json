{"content": "The context is: bute this performance degradation to the oversmoothing issue <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, which states that   descriptions of the over-smoothing issue simplify the assumption of non-linear activation function <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> or make approximat , which means that representations of nodes converge to indistinguishable limits. To our knowledge, <ref type=\"bibr\" target=\"#b14\">[15]</ref> is the first attempt to demystify the over-smoothing issue  whole graph when the number of training nodes is limited under a semi-supervised learning setting. <ref type=\"bibr\" target=\"#b14\">[15]</ref> applies co-training and self-training to overcome the limi tations has a slight downward trend as the number of propagation iterations increases. According to <ref type=\"bibr\" target=\"#b14\">[15]</ref>, the node representations suffering from the oversmoothing ervation when building very deep graph neural networks, which aligns with the over-smoothing issue. <ref type=\"bibr\" target=\"#b14\">[15]</ref> and <ref type=\"bibr\" target=\"#b32\">[33]</ref> study the ov layers, are very difficult to be separated.  Several studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> attribute this performance degradation phenomenon to the ov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tive fields. Several recent works attribute this performance degradation to the oversmoothing issue <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" targ nerated by multiple GCN layers, like 6 layers, are very difficult to be separated.  Several studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> attribute this perfo filter on the spectral domain, thus deriving smoothing features across a graph. Another recent work <ref type=\"bibr\" target=\"#b2\">[3]</ref> verify that smoothing is the nature of most typical graph co shallow architectures. A smoothness regularizer term and adaptive edge optimization are proposed in <ref type=\"bibr\" target=\"#b2\">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, graph classificati es' influence distribution and random walk <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Recently, SGC <ref type=\"bibr\" target=\"#b30\">[31]</ref> is proposed by reducing unnecessary complexity in GCN. The ormation and propagation processes is also adopted in <ref type=\"bibr\" target=\"#b11\">[12]</ref> and <ref type=\"bibr\" target=\"#b30\">[31]</ref> but for the sake of reducing complexity.</p><p>In this wor E <ref type=\"bibr\" target=\"#b8\">[9]</ref>, APPNP <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and SGC <ref type=\"bibr\" target=\"#b30\">[31]</ref>. We aim to provide a rigorous and fair comparison between . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  MLP network for feature transformation. Theoretically, MLP can approximate any measurable function <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Obviously, Z only contains the information of individual n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mula><p>is a layer-specific trainable weight matrix. \u03c3 is a nonlinear activation function like ReLU <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Intuitively, GCN learns representation for each node by p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and link prediction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alized Laplacian Label Propagation (LabelProp NL) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Cheb-Net <ref type=\"bibr\" target=\"#b3\">[4]</ref>, Graph Convolutional Network (GCN) <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er of GCN layers, and evaluate them on three citation datasets; those are Cora, CiteSeer and PubMed <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>In particular, we also include a graph neural netwo  co-authorship, or co-purchase graphs for semi-supervised node classification tasks; those are Cora <ref type=\"bibr\" target=\"#b25\">[26]</ref>, CiteSeer <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PubMe vised node classification tasks; those are Cora <ref type=\"bibr\" target=\"#b25\">[26]</ref>, CiteSeer <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PubMed <ref type=\"bibr\" target=\"#b25\">[26]</ref>, Coautho ef type=\"bibr\" target=\"#b25\">[26]</ref>, CiteSeer <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PubMed <ref type=\"bibr\" target=\"#b25\">[26]</ref>, Coauthor CS <ref type=\"bibr\" target=\"#b27\">[28]</ref>,</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mula><p>is a layer-specific trainable weight matrix. \u03c3 is a nonlinear activation function like ReLU <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Intuitively, GCN learns representation for each node by p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alized Laplacian Label Propagation (LabelProp NL) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Cheb-Net <ref type=\"bibr\" target=\"#b3\">[4]</ref>, Graph Convolutional Network (GCN) <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , graph classification <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, graph classificati es' influence distribution and random walk <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Recently, SGC <ref type=\"bibr\" target=\"#b30\">[31]</ref> is proposed by reducing unnecessary complexity in GCN. The ormation and propagation processes is also adopted in <ref type=\"bibr\" target=\"#b11\">[12]</ref> and <ref type=\"bibr\" target=\"#b30\">[31]</ref> but for the sake of reducing complexity.</p><p>In this wor E <ref type=\"bibr\" target=\"#b8\">[9]</ref>, APPNP <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and SGC <ref type=\"bibr\" target=\"#b30\">[31]</ref>. We aim to provide a rigorous and fair comparison between . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te that the propagation process of the GCN model is a special symmetric form of Laplacian smoothing <ref type=\"bibr\" target=\"#b28\">[29]</ref>, which makes the representations of nodes in the same clas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b32\">[33]</ref> by analyzing the connection of nodes' influence distribution and random walk <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Recently, SGC <ref type=\"bibr\" target=\"#b30\">[31]</ref> i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mula><p>is a layer-specific trainable weight matrix. \u03c3 is a nonlinear activation function like ReLU <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Intuitively, GCN learns representation for each node by p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mula><p>is a layer-specific trainable weight matrix. \u03c3 is a nonlinear activation function like ReLU <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Intuitively, GCN learns representation for each node by p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  achieved for many applications, such as node classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targe eature x i . Most graph convolutions, like GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and GIN <ref uently utilized propagation mechanisms. The row-averaging normalization A \u2295 is adopted in GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DGCNN <ref type=\"bibr\" target=\"#b37\">[38]</ref>. The sym 29\">[30]</ref>, Mixture Model Network (MoNet) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Graph-SAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, APPNP <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and SGC <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b37\">38]</ref> and link prediction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Graph convolutions. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b37\">38]</ref> and link prediction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Graph convolutions. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar N <ref type=\"bibr\" target=\"#b10\">[11]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and GIN <ref type=\"bibr\" target=\"#b31\">[32]</ref>, can be Convolutional Network (GCN) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Graph Attention Network(GAT) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Mixture Model Network (MoNet) <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b32\">33]</ref>, graph classification <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vided in Appendix A.7. We implemented our proposed DAGNN and some necessary baselines using Pytorch <ref type=\"bibr\" target=\"#b23\">[24]</ref> and Pytorch Geometric <ref type=\"bibr\" target=\"#b4\">[5]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cal applications pertaining to fairness, privacy, and safety <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. For example, we can train a GNN model to predict the effec  on interpreting GNNs at the model-level. The existing study <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> only provides example-level explanations for graph models.  tudies focusing on the interpretability of deep graph models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type have no baseline to compare with. Note that existing studies <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> only focus on interpreting GNNs at example-level while igno [4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type=\"bibr\" target=\"#b39\">[40]</ref> proposes to explain deep graph models at the example-level. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and obtained the state-of-the-art performance on different graph tasks, such as node classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, graph classificat nal graph is generated based on G t +1 until termination and then evaluated by f (\u2022) using Equation <ref type=\"bibr\" target=\"#b10\">(11)</ref>. Then the evaluations for m final graphs are averaged to s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hs from biochemistry, neurobiology, ecology, and engineering <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hs from biochemistry, neurobiology, ecology, and engineering <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ploy policy gradient <ref type=\"bibr\" target=\"#b34\">[35]</ref> to train the generator. According to <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, the loss function. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ology, and engineering <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Different motif se t motif sets can be found in graphs with different functions <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, which means different motifs may directly relate to the fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b37\">[38]</ref>, ORGAN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Junction Tree VAE <ref type=\"bibr\" target=\"#b16\">[17]</ref>, DGMG <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and Graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" tar ient-based methods <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, visualizations of intermediate feature maps <ref type=\"bib =\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" target=\"#b25\">[26]</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. However, as discussed in Section 2.2, such optimization me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  several well-known image interpretation methods to graph models, such as sensitivity analysis (SA) <ref type=\"bibr\" target=\"#b11\">[12]</ref>, guided backpropagation (GBP) <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance on different graph tasks, such as node classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, graph classification <ref type=\"bibr\" target=\"#b38\">[39,</ and graph attention <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Since graph data widely exist in different real-world appl volution networks (GCNs) <ref type=\"bibr\" target=\"#b18\">[19]</ref>, graph attention networks (GATs) <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and graph isomorphism networks (GINs) <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" tar ient-based methods <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, visualizations of intermediate feature maps <ref type=\"bib =\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" target=\"#b25\">[26]</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. However, as discussed in Section 2.2, such optimization me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s reported that these numerical errors brought about huge reputation risk, and even economic losses <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Since the documents disclosed by the firm usually have the  significance testing, presented in the academic papers in major psychology journals. A recent study <ref type=\"bibr\" target=\"#b0\">[1]</ref> published a system called AutoDoc, and introduced the module uch more numerical facts in tables than textual paragraphs. Therefore, as an important extension to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we propose Automatic Numerical Cross-Checking over Tables ( ncial, and politic fields. It has attracted a lot of research interests in recent years. Cao et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> propose a system to cross-check numerical facts by extractin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed on the extracted tables, there are many understanding tasks, such as linking text to table cells <ref type=\"bibr\" target=\"#b5\">[6]</ref>, table cell search for a given query <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed on the extracted tables, there are many understanding tasks, such as linking text to table cells <ref type=\"bibr\" target=\"#b5\">[6]</ref>, table cell search for a given query <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on from large data source  to provide related evident paragraphs, and finally give a classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed on the extracted tables, there are many understanding tasks, such as linking text to table cells <ref type=\"bibr\" target=\"#b5\">[6]</ref>, table cell search for a given query <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> proposed a Random Forest classification to identify the complex headers in tables; Nagy et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> leveraged rule-based method to extract data categories and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cit hierarchical headers. There are some studies about recognizing this type of tables. Fang et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a Random Forest classification to identify the comp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ted numerical cross-checking systems. There are some related systems developed, such as ClaimBuster <ref type=\"bibr\" target=\"#b4\">[5]</ref> and StatCheck <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Cla ade by public figures. Verifying such claims includes detecting whether a statement in check-worthy <ref type=\"bibr\" target=\"#b4\">[5]</ref>, retrieve information from large data source  to provide rel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cit hierarchical headers. There are some studies about recognizing this type of tables. Fang et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a Random Forest classification to identify the comp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ted numerical cross-checking systems. There are some related systems developed, such as ClaimBuster <ref type=\"bibr\" target=\"#b4\">[5]</ref> and StatCheck <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Cla ade by public figures. Verifying such claims includes detecting whether a statement in check-worthy <ref type=\"bibr\" target=\"#b4\">[5]</ref>, retrieve information from large data source  to provide rel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elated evident paragraphs, and finally give a classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In academic field, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: supervised tasks, e.g., discriminating whether two subsequences come from the same user's behaviors <ref type=\"bibr\" target=\"#b35\">[36]</ref>. We further improve upon CLRec and propose Multi-CLRec, wh  the regular task where \ud835\udc65 is a sequence of clicks and \ud835\udc66 is the next click to be predicted. Task u2u <ref type=\"bibr\" target=\"#b35\">[36]</ref> adds an auxiliary loss where \ud835\udc65 and \ud835\udc66 are both sequences fr /div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B.4 Complex Pretext Tasks</head><p>In task u2u <ref type=\"bibr\" target=\"#b35\">[36]</ref>, \ud835\udc65 and \ud835\udc66 are both sequences from the same user, before and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tive Loss. We study the following type of contrastive loss <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> under a negative sampler \ud835\udc5d \ud835\udc5b (\ud835\udc66 | \ud835\udc65): where {\ud835\udc66 \ud835\udc56 } \ud835\udc3f \ud835\udc56=1 ar esigning a well-performing proposal distribution \ud835\udc5d \ud835\udc5b (\ud835\udc66 | \ud835\udc65) <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. InfoNCE <ref type=\"bibr\" target=\"#b38\">[39]</ref> demonstr  the present batch <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> (see Figure <ref type=\"figure\" target=\"#fig_13\">1a</ref>). . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch size, our implementation is then equivalent to sampling negative examples from the present batch <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lns=\"http://www.tei-c.org/ns/1.0\"><head>Method</head><p>Amazon UserBehavior DNN 0.7546 0.7460 SVD++ <ref type=\"bibr\" target=\"#b30\">[31]</ref> 0.7155 0.8371 GRU4Rec <ref type=\"bibr\" target=\"#b20\">[21]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>Some efforts <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref> have been spent on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: astive learning have been focusing on designing a well-performing proposal distribution \ud835\udc5d \ud835\udc5b (\ud835\udc66 | \ud835\udc65) <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. InfoNCE <ref type=\" sal distribution not only affects convergence, but also has a significant impact on the performance <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Empirically the number of the negative samples need to be l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the latter is a well-studied technique for bias reduction <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Our theory complem eir connection with the inverse propensity weighting (IPW) <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> techniques for bias ustrating its connection with inverse propensity weighting <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>[63] Hao Zou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: astive learning have been focusing on designing a well-performing proposal distribution \ud835\udc5d \ud835\udc5b (\ud835\udc66 | \ud835\udc65) <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. InfoNCE <ref type=\" sal distribution not only affects convergence, but also has a significant impact on the performance <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Empirically the number of the negative samples need to be l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ee Figure <ref type=\"figure\" target=\"#fig_2\">2</ref>) borrows the idea of intention disentanglement <ref type=\"bibr\" target=\"#b34\">[35]</ref>, which has a built-in routing mechanism for clustering. Le a set of trainable parameters \ud835\udf41 \u210e \u2208 R \ud835\udc51 , \u210e = 1, 2, . . . , \ud835\udc3b , to represent \ud835\udc3b intention prototypes <ref type=\"bibr\" target=\"#b34\">[35]</ref>, based on which each item \ud835\udc66 \ud835\udc61 is being routed into intenti >14,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, Taobao <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" tar here \ud835\udf0c = 0.07 following previous work <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>Some efforts <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" ta d their solutions, such as using multiple interest vectors <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> or multiple interest sub-models <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: putationally infeasible. Among the various sampling-based approximation strategies, sampled-softmax <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> usually outperforms  here and will show that the minor difference is crucial. There are many variants of sampeld softmax <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, among which the fol of negative samples, does not perform well in our settings. CLRec's improvement over sampledsoftmax <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> w.r.t. the offline m . The existing methods explicitly sample negative examples from a pre-defined proposal distribution <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tive behaviors, we use the embeddings of bucketized time intervals, which are used by previous work <ref type=\"bibr\" target=\"#b57\">[58]</ref>. To be specific, given the current timestamp \ud835\udc61 when the us P tasks and is adopted by sequential recommendation models <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>. We simplify the multi-head attention <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the previous methods, our Fig. <ref type=\"figure\">1</ref>: Comparing our ResNeSt block with SE-Net <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref>. A de then the intermediate representation of each group is Split Attention in Cardinal Groups. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, a combined repres ead of the 1 \u00d7 1 layer to better preserve such information <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Convolutional layers require handling featuremap boundarie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#b51\">[52]</ref>, in which each network block consists of different convolutional kernels. ResNeXt <ref type=\"bibr\" target=\"#b60\">[61]</ref> adopts group convolution <ref type=\"bibr\" target=\"#b33\">[3 ight) depicts an overview of a Split-Attention Block.</p><p>Feature-map Group. As in ResNeXt blocks <ref type=\"bibr\" target=\"#b60\">[61]</ref>, the feature can be divided into several groups, and the n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ces a channel-attention mechanism by adaptively recalibrating the channel feature responses. SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref> brings the feature-map attention across two network branch /ref>: Comparing our ResNeSt block with SE-Net <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref>. A detailed view of Split-Attention unit is shown in Figur -Net operates on top of the entire block regardless of multiple groups. Previous models like SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref> introduced feature attention between two network branches,  type=\"bibr\" target=\"#b28\">[29]</ref>, ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref> and SKNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Remarkably, our ResNeSt-50 achieves 80.64 top-1 accuracy, ach group is Split Attention in Cardinal Groups. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, a combined representation for each cardinal group can be o obal average pooling across spatial dimensions s k \u2208 R C/K <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Here the c-th component is calculated as:</p><formula xml: Our method generalizes prior work on feature-map attention <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> within a cardinal group setting <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: h-wise convolution <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>. Despite their supe  classification performance, such as: Amoe-baNet <ref type=\"bibr\" target=\"#b44\">[45]</ref>, MNASNet <ref type=\"bibr\" target=\"#b53\">[54]</ref>, and EfficientNet <ref type=\"bibr\" target=\"#b54\">[55]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n ImageNet using ResNeSt. (Right-Bottom) Transfer learning results: object detection mAP on MS-COCO <ref type=\"bibr\" target=\"#b41\">[42]</ref> and semantic segmentation mIoU on ADE20K <ref type=\"bibr\"  tei-c.org/ns/1.0\"><head n=\"6.1\">Object Detection</head><p>We report our detection result on MS-COCO <ref type=\"bibr\" target=\"#b41\">[42]</ref>  2017 validation set with 5k images (aka. minival) using t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ugmented example. Regularization. Very deep neural networks tend to overfit even for large datasets <ref type=\"bibr\" target=\"#b67\">[68]</ref>. To prevent this, dropout regularization randomly masks ou t network ensemble <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref>. A dropout layer with the dropout probability of 0.2 is app. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b54\">[55]</ref> as shown in Table <ref type=\"table\">1</ref>. Our single Cascade-RCNN <ref type=\"bibr\" target=\"#b2\">[3]</ref> model using a ResNeSt-101 backbone achieves 48.3% box mAP an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gt; 0. This mitigates network overconfidence and overfitting.</p><p>Auto Augmentation. Auto-Augment <ref type=\"bibr\" target=\"#b10\">[11]</ref> is a strategy that augments the training data with transfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p><p>Multi-path and Feature-map Attention. Multi-path representation has shown success in GoogleNet <ref type=\"bibr\" target=\"#b51\">[52]</ref>, in which each network block consists of different convolu d training crop size of 224, while the Inception-Net family<ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref> uses a training c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b59\">60]</ref> usually use a fixed training crop size of 224, while the Inception-Net family<ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n mAP on MS-COCO <ref type=\"bibr\" target=\"#b41\">[42]</ref> and semantic segmentation mIoU on ADE20K <ref type=\"bibr\" target=\"#b70\">[71]</ref>.</p><p>not even trainable on a GPU with an appropriate per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b68\">69]</ref> or introduce long-range connections <ref type=\"bibr\" target=\"#b55\">[56]</ref> or use cross-channel feature-map attention <ref type=\"bibr sks at the same time? Cross-channel information has demonstrated success in downstream applications <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b43\">[44]</ref>. Network weights are initialized using Kaiming Initialization <ref type=\"bibr\" target=\"#b23\">[24]</ref>. A drop layer is inserted before the final classification . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gt; 0. This mitigates network overconfidence and overfitting.</p><p>Auto Augmentation. Auto-Augment <ref type=\"bibr\" target=\"#b10\">[11]</ref> is a strategy that augments the training data with transfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: can be directly applied on many existing downstream models <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" tar ages (aka. minival) using the standard COCO AP metric of single scale. We train all models with FPN <ref type=\"bibr\" target=\"#b40\">[41]</ref>, synchronized batch normalization <ref type=\"bibr\" target= f> models with ResNeSt-50 and ResNeSt-101 as their backbones. All models are trained along with FPN <ref type=\"bibr\" target=\"#b40\">[41]</ref> and synchronized batch normalization. For data augmentatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p://www.tei-c.org/ns/1.0\"><head n=\"2\">Related Work</head><p>Modern CNN Architectures. Since AlexNet <ref type=\"bibr\" target=\"#b33\">[34]</ref>, deep convolutional neural networks <ref type=\"bibr\" targe t convolutional kernels. ResNeXt <ref type=\"bibr\" target=\"#b60\">[61]</ref> adopts group convolution <ref type=\"bibr\" target=\"#b33\">[34]</ref> in the ResNet bottle block, which converts the multi-path . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  CNN operators.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Method</head><p>Deformable <ref type=\"bibr\" target=\"#b71\">[72]</ref>   Beyond the paper contributions, we empirically find seve ade-Mask-RCNN on COCO val set. The ResNeSt-101 is applied with and without deformable convolution v2<ref type=\"bibr\" target=\"#b71\">[72]</ref>. It shows that our split-attention module is compatible wi nce segmentation, shown in  We also evaluate our ResNeSt with and without deformable convolution v2 <ref type=\"bibr\" target=\"#b71\">[72]</ref>. With its help, we are able to obtain a higher performance. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: not during inference) to form an implicit network ensemble <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref>. A dropout layer wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b54\">[55]</ref> as shown in Table <ref type=\"table\">1</ref>. Our single Cascade-RCNN <ref type=\"bibr\" target=\"#b2\">[3]</ref> model using a ResNeSt-101 backbone achieves 48.3% box mAP an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nes with our ResNeSt, while using the default settings for the hyper-parameters and detection heads <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>. Compared to the b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b31\">[32]</ref> is used after each convolutional layer before ReLU activation <ref type=\"bibr\" target=\"#b43\">[44]</ref>. Network weights are initialized using Kaiming Initializat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rided convolution at the 3 \u00d7 3 layer instead of the 1 \u00d7 1 layer to better preserve such information <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Convolutional lay vers (64 GPUs in total) in parallel. Our learning rates are adjusted according to a cosine schedule <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. We follow the com  small for j = c, while z c is being pushed to its optimal value \u221e, and this can induce overfitting <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. Rather than assig ><p>Tweaks from ResNet-D. We also adopt two simple yet effective ResNet modifications introduced by <ref type=\"bibr\" target=\"#b25\">[26]</ref>: (1) The first 7 \u00d7 7 convolutional layer is replaced with   including bias units, \u03b3 and \u03b2 in the batch normalization layers.</p><p>#P GFLOPs acc(%) ResNetD-50 <ref type=\"bibr\" target=\"#b25\">[26]</ref>  For example 2s2x40d denotes radix=2, cardinality=2 and wi ixup training, we simply mix each sample from the current mini-batch with its reversed order sample <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Batch Normalization <ref type=\"bibr\" target=\"#b31\">[32]</ /www.tei-c.org/ns/1.0\"><head n=\"5.2\">Ablation Study</head><p>ResNeSt is based on the ResNet-D model <ref type=\"bibr\" target=\"#b25\">[26]</ref>  ResNeSt-fast setting, the effective average downsampling  inality, and d the network width (0s represents the use of a standard residual block as in ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref>). We empirically find that increasing the radix from 0 to  ref type=\"bibr\" target=\"#b59\">[60]</ref>, SENet <ref type=\"bibr\" target=\"#b28\">[29]</ref>, ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref> and SKNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Remar to the weights of convolutional and fully connected layers <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. We do not subject any of the other network parameters to w ng on images that share the same crop size. ResNet variants<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, while recent image classification networks have focused more on group or depth-wise convolution <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta er well to other tasks as their isolated representations cannot capture cross-channel relationships <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Therefore, a netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b68\">69,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>.</p><p>We first consider the Cityscapes <ref type=\"bibr\" target=\"#b9\">[10]</ref> dataset, which consists of 5K highquality labeled images. W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rg/ns/1.0\"><head n=\"1\">Introduction</head><p>The majority of the research efforts on improving VAEs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> is dedicated to the st e posterior up to the (l \u2212 1) th group. The objective is trained using the reparameterization trick <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>The main questi  and bidirectional encoder networks <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>The goal of VAEs <ref type=\"bibr\" target=\"#b0\">[1]</ref> is to train a generative model in the form of p(x x x, z z z. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b22\">23]</ref>, or tackling posterior collapse <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> omit batch normalization (BN) <ref type=\"bibr\" target=\"#b36\">[37]</ref> to combat the sources of randomness that could potentially. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, formulating tighter bounds <ref type=\"bibr\" target=\"#b10\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nal layer, estimated using a single power iteration update <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>. Here, \u03bb controls to the level of smoothness imposed by L S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>Figure <ref type=\"figure\">1</ref>: 256\u00d7256-pixel samples generated by NVAE, trained on CelebA HQ <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p><p>However, VAEs can benefit from designing special ne ]</ref>, CIFAR-10 [72], ImageNet 32\u00d732 <ref type=\"bibr\" target=\"#b72\">[73]</ref>, CelebA HQ 256\u00d7256 <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and FFHQ 256\u00d7256 <ref type=\"bibr\" target=\"#b73\">[74]</ref 0\">[71]</ref>, CIFAR-10 [72], ImageNet 32 \u00d7 32 <ref type=\"bibr\" target=\"#b72\">[73]</ref>, CelebA HQ <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and FFHQ 256\u00d7256 <ref type=\"bibr\" target=\"#b73\">[74]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, reducing the gradient noise <ref type=\"bibr\" target=\"#b14\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: osterior distributions <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= of an approximate posterior distribution or encoder q(z z z|x x x).</p><p>In deep hierarchical VAEs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ood. In contrast, NVAE is trained directly with the VAE objective. Moreover, VQ-VAE-2 uses PixelCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref> in its prior for latent variables up to 128\u00d7128 dims that  ll similar to Fig. <ref type=\"figure\">3 (a)</ref> with the masking mechanism introduced in PixelCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>. In the autoregressive cell, BN is replaced with WN, and S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: simply regularize model predictions to be invariant to small noise applied to either input examples <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Sajjadi et  r\" target=\"#b1\">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropout noise; VAT <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018;</ref><ref type=\"bibr\">2016)</ref> defines the n type=\"bibr\" target=\"#b34\">(Sajjadi et al., 2016;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016;</ref><ref type=\"bibr\" target=\"#b23\">Miyato et al., 2018)</ref>. But different from existing work, we focu has been shown to be beneficial <ref type=\"bibr\" target=\"#b15\">(Grandvalet &amp; Bengio, 2005;</ref><ref type=\"bibr\" target=\"#b23\">Miyato et al., 2018)</ref>, we sharpen predictions when computing the  current parameters \u03b8 indicating that the gradient is not propagated through \u03b8, as suggested by VAT <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref>. We set \u03bb to 1 for most of our experiment cally, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref>, an algorithm that generates adversarial  =\"#b41\">(Tarvainen &amp; Valpola, 2017)</ref> Conv-Large 3.1M 12.31 \u00b1 0.28 3.95 \u00b1 0.19 VAT + EntMin <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 \u00b1 0.05 3.86 \u00b1 0.11 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se that are based on consistency training <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\" target=\"#b32\">Rasmus et al., 2015;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016;</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se that are based on consistency training <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\" target=\"#b32\">Rasmus et al., 2015;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016;</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: this weakness. The recent works in SSL are diverse but those that are based on consistency training <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\" target=\"#b32\">Rasmus et a Sajjadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b6\">Clark et al., 2018)</ref> or hidden states <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016)</  works in the consistency training family mostly differ in how the noise is defined: Pseudoensemble <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2017;</ref><ref type=\"bibr\" target=\"#b48\">Ye et al., 2019)</ref>. Invariant representation learning <ref type=\"bibr\" target=\"#b16\">(Liang et al., 2018;</ref><ref type=\"bibr\" target=\"#b35\">Salazar et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ss <ref type=\"bibr\" target=\"#b36\">(Salimans et al., 2016)</ref> works very well in practice. Later, <ref type=\"bibr\" target=\"#b11\">Dai et al. (2017)</ref> shows that this can be seen as an instantiati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se that are based on consistency training <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\" target=\"#b32\">Rasmus et al., 2015;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016;</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: this weakness. The recent works in SSL are diverse but those that are based on consistency training <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\" target=\"#b32\">Rasmus et a Sajjadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b6\">Clark et al., 2018)</ref> or hidden states <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016)</  works in the consistency training family mostly differ in how the noise is defined: Pseudoensemble <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2017;</ref><ref type=\"bibr\" target=\"#b48\">Ye et al., 2019)</ref>. Invariant representation learning <ref type=\"bibr\" target=\"#b16\">(Liang et al., 2018;</ref><ref type=\"bibr\" target=\"#b35\">Salazar et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The main concept behind these approaches is to interpret  s successfully transferred from NLP to protein sequences <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, with the excep f their surrounding context (residues next to it). As previously established for another protein LM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, the t-SNE projections (e.g. ProtBert Fig. <ref type=\"figu \">[20]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, we might expect an upper limit for what protein LMs can l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: training set: CB513 (513 proteins; <ref type=\"bibr\" target=\"#b41\">[42]</ref>), TS115 (115 proteins; <ref type=\"bibr\" target=\"#b42\">[43]</ref>) and CASP12 (21 proteins; <ref type=\"bibr\" target=\"#b43\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: breakthroughs in both fields. More powerful supercomputers <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref> and advanced libraries <ref type=\"bibr\" target=\"#b2\">[3]</re e SuperMUC-NG <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and through its components, such as TPU pods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, specifically designed to ease large scale neural network tr s the green energy-driven Summit <ref type=\"bibr\" target=\"#b0\">[1]</ref> and Google's cloud TPU Pod <ref type=\"bibr\" target=\"#b1\">[2]</ref>, combined with optimized libraries such as IBM DDL <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[26]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref> usually have to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6\">[37]</ref> (with 2,122M sequences). The latter merged all protein sequences available in UniProt <ref type=\"bibr\" target=\"#b37\">[38]</ref> and proteins translated from multiple metagenomic sequenci  BFD <ref type=\"bibr\" target=\"#b35\">[36]</ref>, more than an order of magnitude larger than UniProt <ref type=\"bibr\" target=\"#b37\">[38]</ref>, the standard in the field. Although bigger did not equate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: training set: CB513 (513 proteins; <ref type=\"bibr\" target=\"#b41\">[42]</ref>), TS115 (115 proteins; <ref type=\"bibr\" target=\"#b42\">[43]</ref>) and CASP12 (21 proteins; <ref type=\"bibr\" target=\"#b43\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref> usually have to search for evolutionary related proteins i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dels (including Bert and Albert). With the average length of an English sentence around 15-30 words <ref type=\"bibr\" target=\"#b50\">[51]</ref>, an upper sentence length limit is no problem for sentence. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d on proteins <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bib iven the experiments described here and in previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ated visually by projecting the high-dimensional representations down to two dimensions using t-SNE <ref type=\"bibr\" target=\"#b44\">[45]</ref>. A non-redundant (PIDE&lt;40%) version of the SCOPe databa grammar in NLP, we projected the highdimensional embedding space down to two dimensions using t-SNE <ref type=\"bibr\" target=\"#b44\">[45]</ref> and visualized proteins according to annotated structural,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r 3-and 8-state secondary structure prediction. The NetSurfP-2.0 dataset was created through PISCES <ref type=\"bibr\" target=\"#b39\">[40]</ref> selecting highest resolution protein structures (resolutio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: by all the modalities.</p><p>Inspired by such an assumption and the fact that the Total Correlation <ref type=\"bibr\" target=\"#b27\">[29]</ref> can measure the amount of information shared by M (M \u2265 2)  sifiers of each modality.</p><p>Total Correlation/Mutual information maximization Total Correlation <ref type=\"bibr\" target=\"#b27\">[29]</ref>, as an extension of Mutual Information, measures the amoun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: x 1 , ..., x M ) and q [M ] (x) \u2236= \u220f M i=1 p(x i ).</formula><p>According to dual representation in <ref type=\"bibr\" target=\"#b25\">[27]</ref>, we have the following lower bound for KL divergence betwe bound for KL divergence between p and q, and hence TC.</p><p>Lemma 1 (Dual version of f -divergence <ref type=\"bibr\" target=\"#b25\">[27]</ref>).</p><formula xml:id=\"formula_5\">D KL p [M ] q [M ] \u2265 sup . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  News-M10. They contain 500, 500, 1000 data points with 2, 5, 10 categories respectively. Following <ref type=\"bibr\" target=\"#b31\">[33]</ref>, we use 60% for training, 20% for validation and 20% for t es (the percentage of labeled data points in each modality): {10%, 30%} for each dataset. We follow <ref type=\"bibr\" target=\"#b31\">[33]</ref> for classifiers. Adam with default parameters and learning f type=\"bibr\" target=\"#b22\">[24]</ref> uses adversarial training for semi-supervised learning; PVCC <ref type=\"bibr\" target=\"#b31\">[33]</ref> that considers the consistency of data points under differ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  News-M10. They contain 500, 500, 1000 data points with 2, 5, 10 categories respectively. Following <ref type=\"bibr\" target=\"#b31\">[33]</ref>, we use 60% for training, 20% for validation and 20% for t es (the percentage of labeled data points in each modality): {10%, 30%} for each dataset. We follow <ref type=\"bibr\" target=\"#b31\">[33]</ref> for classifiers. Adam with default parameters and learning f type=\"bibr\" target=\"#b22\">[24]</ref> uses adversarial training for semi-supervised learning; PVCC <ref type=\"bibr\" target=\"#b31\">[33]</ref> that considers the consistency of data points under differ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tions within graph-structured data <ref type=\"bibr\" target=\"#b28\">[30]</ref>). Kong and Schoenebeck <ref type=\"bibr\" target=\"#b17\">[19]</ref> provide another mutual information estimator in the co-tra  identification of ground truth classifiers on semi-supervised multi-modality data, by generalizing <ref type=\"bibr\" target=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b6\">8]</ref> that can only handle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b4\">[6]</ref>. <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by Blum et. al <ref type=\"bibr\" target=\"#b4\">[6]</ref>. <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed in the literature, which can be roughly categorized into two branches: (i) co-training strategy <ref type=\"bibr\" target=\"#b4\">[6]</ref>; and (ii) learning joint representation across modalities in  in each modality are the same, which may not be satisfied in the real settings, as self-claimed in <ref type=\"bibr\" target=\"#b4\">[6]</ref>; while the latter branch of methods fails to capture the hig hared by all modalities. The first branch applies the co-training algorithm proposed by Blum et. al <ref type=\"bibr\" target=\"#b4\">[6]</ref>. <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" t representation. A common belief in multi-modality learning <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" targe only assumed in the literature of semi-supervised learning <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ethod TCGM has competitive performance compared to well established clustering algorithms K-means++ <ref type=\"bibr\" target=\"#b1\">[2]</ref> and spectral clustering <ref type=\"bibr\" target=\"#b23\">[25]< taset <ref type=\"foot\" target=\"#foot_2\">10</ref> , with 3D images sMRI and PET. DARTEL VBM pipeline <ref type=\"bibr\" target=\"#b1\">[2]</ref> is implemented to pre-process the sMRI data, and then images. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>Dataset We evaluate our methods on two multi-modal emotion recognition datasets: IEMOCAP dataset <ref type=\"bibr\" target=\"#b5\">[7]</ref> and MOSI dataset <ref type=\"bibr\" target=\"#b32\">[34]</ref>. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" target=\"#b18\">20]</ref> is that conditioning on ground truth label Y , these modali rget=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" target=\"#b18\">20]</ref>. However, the Y may not be the unique information intersect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ering what causes deep networks to be fragile to adversarial examples and how to improve robustness <ref type=\"bibr\" target=\"#b45\">[47,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" ta t has been theoretically shown that decreasing the input dimensionality of data improves robustness <ref type=\"bibr\" target=\"#b45\">[47]</ref>. Adversarial training <ref type=\"bibr\" target=\"#b33\">[35]< ining performance on natural examples.</p><p>Using the first order vulnerability of neural networks <ref type=\"bibr\" target=\"#b45\">[47]</ref>, we theoretically show that increasing output dimensionali ethods can improve the model's robustness without compromising clean accuracy. Simon-Gabriel et al. <ref type=\"bibr\" target=\"#b45\">[47]</ref> conducted a theoretical analysis of the vulnerability of n ref>. We denote the multitask predictor as F and each individual task predictor as F c . Prior work <ref type=\"bibr\" target=\"#b45\">[47]</ref> showed that the norm of gradients captures the vulnerabili rial noise is imperceptible, i.e., r \u2192 0, we can approximate \u2206L with a first-order Taylor expansion <ref type=\"bibr\" target=\"#b45\">[47]</ref>.</p><p>Lemma 1. For a given neural network F that predicts l></formula><p>Remark 1. By increasing the number of output tasks M , the first order vulnerability <ref type=\"bibr\" target=\"#b45\">[47]</ref> of network decreases. In the ideal case, if the model has  s we add more tasks, the norm of the joint gradient decreases, indicating improvement to robustness <ref type=\"bibr\" target=\"#b45\">[47]</ref>. The only exception is the depth estimation task, which we e same dimension for baselines and ours during comparison because input dimension impacts robustness<ref type=\"bibr\" target=\"#b45\">[47]</ref>.</note> \t\t</body> \t\t<back>  \t\t\t<div type=\"acknowledgement\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b56\">58,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>, yet they remain brittle to adversarial examples. A large b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -task and multitask models when one task is adversarially attacked.</p><p>losses have been proposed <ref type=\"bibr\" target=\"#b54\">[56,</ref><ref type=\"bibr\" target=\"#b5\">7]</ref>. This body of work s ' robustness is improved by regularizing the gradient norm <ref type=\"bibr\" target=\"#b39\">[41,</ref><ref type=\"bibr\" target=\"#b54\">56,</ref><ref type=\"bibr\" target=\"#b5\">7]</ref>. Parseval <ref type=\" larization methods <ref type=\"bibr\" target=\"#b37\">[39,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b54\">56,</ref><ref type=\"bibr\" target=\"#b5\">7]</ref>.</p></div> <div xmlns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -task and multitask models when one task is adversarially attacked.</p><p>losses have been proposed <ref type=\"bibr\" target=\"#b54\">[56,</ref><ref type=\"bibr\" target=\"#b5\">7]</ref>. This body of work s ' robustness is improved by regularizing the gradient norm <ref type=\"bibr\" target=\"#b39\">[41,</ref><ref type=\"bibr\" target=\"#b54\">56,</ref><ref type=\"bibr\" target=\"#b5\">7]</ref>. Parseval <ref type=\" larization methods <ref type=\"bibr\" target=\"#b37\">[39,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b54\">56,</ref><ref type=\"bibr\" target=\"#b5\">7]</ref>.</p></div> <div xmlns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b28\">30]</ref> and optimization procedures <ref type=\"bibr\" target=\"#b3\">[5,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b0\">2,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b44\">46,</ref><ref type=\"bibr\" target=\"#b52\">54,</ref><ref type=\"bibr\" target=\"#b35\">37]</ref> to fool target mode ataset splits <ref type=\"bibr\">[1]</ref>. We resize the images to 256 \u00d7 256.  We do not use the DAG <ref type=\"bibr\" target=\"#b52\">[54]</ref> attack for segmentation because it is an unrestricted atta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as demonstrated that images with human-imperceptible noise <ref type=\"bibr\" target=\"#b33\">[35,</ref><ref type=\"bibr\" target=\"#b1\">3,</ref><ref type=\"bibr\" target=\"#b9\">11,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l work showed the hardness of multi-objective optimization <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>, we leverage this motivation and prove that multitask model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l work showed the hardness of multi-objective optimization <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>, we leverage this motivation and prove that multitask model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: zation procedure. For instance, more training data -both labeled and unlabeled -improves robustness <ref type=\"bibr\" target=\"#b41\">[43,</ref><ref type=\"bibr\" target=\"#b50\">52]</ref>. It has been theor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ss. While previous work shows that multitask learning can improve the performance of specific tasks <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b46\">48]</ref>, we show that it inc ed work in multitask learning and adversarial attacks.</p><p>Multitask Learning: Multitask learning <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b13\">15,</ref><ref type=\"bibr\" targ k learning improves the performance of select tasks by introducing a knowledge-based inductive bias <ref type=\"bibr\" target=\"#b2\">[4]</ref>. However, multi-objective functions are hard to optimize, wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: milar semantics.</p><p>Sharing a similar philosophy, there have been works on contrastive attention <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. MGCAM <ref type=\"  attention <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. MGCAM <ref type=\"bibr\" target=\"#b30\">[31]</ref> uses the contrastive feature between persons and backgroun for context modeling; instead of using extra supervision to localize regions to compare as in MGCAM <ref type=\"bibr\" target=\"#b30\">[31]</ref>, ACM automatically learns to focus on meaningful regions t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: est X-ray datasets <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, there has been a lon est X-ray datasets <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> showed that commonly . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ists usually read tens or hundreds of X-rays every day. Several studies regarding radiologic errors <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> have reported that 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns for diagnosing chest diseases. In the US, more than 35 million chest X-rays are taken every year <ref type=\"bibr\" target=\"#b19\">[20]</ref>. It is primarily used to screen diseases such as lung canc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ss-level prediction with the potentially abnormal location <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> without the text rep arget=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. The dataset contain. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o incorporate group-wise operation. We replace all convolution operations with grouped convolutions <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, where the input a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  abnormal location <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> without the text reports on the location of the disease. Som get=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. The dataset contains 112,120 images from 30,805 unique pati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: localization and classification performances <ref type=\"bibr\" target=\"#b22\">[23]</ref>. Guan et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> also proposes a hierarchical hard-attention for cascaded i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns for diagnosing chest diseases. In the US, more than 35 million chest X-rays are taken every year <ref type=\"bibr\" target=\"#b19\">[20]</ref>. It is primarily used to screen diseases such as lung canc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  abnormal location <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> without the text reports on the location of the disease. Som get=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. The dataset contains 112,120 images from 30,805 unique pati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns for diagnosing chest diseases. In the US, more than 35 million chest X-rays are taken every year <ref type=\"bibr\" target=\"#b19\">[20]</ref>. It is primarily used to screen diseases such as lung canc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ile feature maps generated by the decoder contain high-level and coarse-gained semantic information <ref type=\"bibr\" target=\"#b14\">[15]</ref>. And skip connections, which combine the low-level and hig ive method to boost the semantic extraction ability of encoder-decoder frameworks.</p><p>In U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>, plain skip connections are substituted by nested and dens =\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The major C. Duan is with the State Key Laboratory of Inf =\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Excluding SegNet, DeepLab V3 and DeepLab V3+, the remaini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ts of three convolutions, which just cause finite increasing in additional computational complexity <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The effectiveness of ACB has been verified in the fields  [17]</ref>. The effectiveness of ACB has been verified in the fields including image classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>  should be robust to rotation and renders consistent results in different rotations. As reported in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, different asymmetric convolutions are robust with differe  network.</p><p>Based on above-mentioned insight, we modify the asymmetric convolutions proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> and design an asymmetric convolution block (ACB) to captur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o make utmost of the multi-scale features, the full-scale skip connections are designed in U-Net 3+ <ref type=\"bibr\" target=\"#b15\">[16]</ref>. However, the design philosophy of full-scale skip connect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b9\">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Deep  verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab  rg/ns/1.0\"><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: source management, yield estimation, and economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>. Hitherto the remote . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibility and adaptability of these methods.</p><p>More recently, Convolutional Neural Networks (CNN) <ref type=\"bibr\" target=\"#b8\">[9]</ref> have demonstrated its powerful capacity of automatically cap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and Gaofen Image Dataset (GID) <ref type=\"bibr\">[30]</ref>.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erarchical features from images, and have dramatically influenced the field of computer vision (CV) <ref type=\"bibr\" target=\"#b9\">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as well as realign channel-wise features. Motivated by Convolutional Block Attention Module (CBAM) <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we design the channel attention block (CAB) to reweightin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibility and adaptability of these methods.</p><p>More recently, Convolutional Neural Networks (CNN) <ref type=\"bibr\" target=\"#b8\">[9]</ref> have demonstrated its powerful capacity of automatically cap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erarchical features from images, and have dramatically influenced the field of computer vision (CV) <ref type=\"bibr\" target=\"#b9\">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: asets</head><p>The effectiveness of MACU-Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and Gaofen Image D. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion, the encoder-decoder frameworks such as SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref  the performance of proposed algorithm with SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Dee >To evaluate the effectiveness of MACU-Net, SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Dee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: asets</head><p>The effectiveness of MACU-Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and Gaofen Image D. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> have become the frequently-used schemes. Generally, feature =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>,  =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and Gaofen Image Dataset (GID) <ref type=\"bibr\">[30]</ref>.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cluding support vector machine (SVM) <ref type=\"bibr\" target=\"#b6\">[7]</ref> and random forest (RF) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, the high dependency on hand-crafted visual feature. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 7]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In <ref type=\"bibr\" target=\"#b18\">[19]</ref>, only the co get=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In <ref type=\"bibr\" target=\"#b18\">[19]</ref>, only the convolutional layers of encoder are replaced by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng the precise category to every pixel contained in an image <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, plays a critical role in wide range of application scenarios. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b9\">[10]</ref>. For semantic segmentation, the encoder-decoder frameworks such as SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Deep  verify the effectiveness of MACU-Net, we compare the performance of proposed algorithm with SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab  rg/ns/1.0\"><head>B. Experimental Setting</head><p>To evaluate the effectiveness of MACU-Net, SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion, the encoder-decoder frameworks such as SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref  the performance of proposed algorithm with SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Dee >To evaluate the effectiveness of MACU-Net, SegNet <ref type=\"bibr\" target=\"#b10\">[11]</ref>, U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Dee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sign assorted classifiers from diverse perspectives, from orthodox methods such as distance measure <ref type=\"bibr\" target=\"#b5\">[6]</ref>, to advanced methods including support vector machine (SVM) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d economic assessment <ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>. Hitherto the remote sensing community has tried to design a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> have become the frequently-used schemes. Generally, feature =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>,  =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 7]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In <ref type=\"bibr\" target=\"#b18\">[19]</ref>, only the co get=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In <ref type=\"bibr\" target=\"#b18\">[19]</ref>, only the convolutional layers of encoder are replaced by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b5\">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type=\"bibr\" target=\"#b6\">[7]</ref> and random forest (RF) <ref type=\"bibr\" target=\"#b7\">[8]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref> =\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng the precise category to every pixel contained in an image <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, plays a critical role in wide range of application scenarios. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he fields including image classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and medical image segmentation <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> have become the frequently-used schemes. Generally, feature =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>,  =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n this paper, we introduce TOAD-GAN as a solution to these problems. Our work is inspired by SinGAN <ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019)</ref>, a recent Generative Adversa s of TOAD-GAN on Super Mario Bros. level 1-2. The architecture is adapted from SinGAN (cf. Fig. 4 of<ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019)</ref>). We use a downsampling meth e training process.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>SinGAN</head><p>SinGAN <ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019</ref>) is a novel GAN architecture  p at the lowest scale. For a more in-depth explanation please refer to the original SinGAN paper by <ref type=\"bibr\" target=\"#b15\">Shaham, Dekel, and Michaeli (2019)</ref>.</p></div> <div xmlns=\"http:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es that use Neural Networks have also been applied extensively to the SMB level generation problem. <ref type=\"bibr\" target=\"#b10\">Hoover, Togelius, and Yannakis (2015)</ref> trained a neural network . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rrence of tokens (e.g. a single enemy or ground block) in existing game levels to identify patterns <ref type=\"bibr\" target=\"#b3\">(Dahlskog and Togelius 2012</ref>) and combined them using simple stat ref type=\"bibr\" target=\"#b11\">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation <ref type=\"bibr\" target=\"#b3\">Dahlskog and Togelius (2012)</ref> identified and analyzed patterns wi lined how those patterns could be combined and varied to create new levels. In their continued work <ref type=\"bibr\" target=\"#b3\">(Dahlskog and Togelius 2014)</ref>, they additionally defined micro-(v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the vast amount of PCG approaches.</p><p>For a review of pattern-based level generators for SMB see <ref type=\"bibr\" target=\"#b11\">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed using an embedding of the level slices that is inspired by the Fr\u00e9chet Inception Distance (FID) <ref type=\"bibr\" target=\"#b8\">(Heusel et al. 2017</ref>). In the second part, we show the generality. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re were proposed to stabilize the training process, for example minimizing the Wasserstein distance <ref type=\"bibr\" target=\"#b0\">(Arjovsky, Chintala, and Bottou 2017)</ref> and penalizing the norm of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rrence of tokens (e.g. a single enemy or ground block) in existing game levels to identify patterns <ref type=\"bibr\" target=\"#b3\">(Dahlskog and Togelius 2012</ref>) and combined them using simple stat ref type=\"bibr\" target=\"#b11\">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation <ref type=\"bibr\" target=\"#b3\">Dahlskog and Togelius (2012)</ref> identified and analyzed patterns wi lined how those patterns could be combined and varied to create new levels. In their continued work <ref type=\"bibr\" target=\"#b3\">(Dahlskog and Togelius 2014)</ref>, they additionally defined micro-(v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re were proposed to stabilize the training process, for example minimizing the Wasserstein distance <ref type=\"bibr\" target=\"#b0\">(Arjovsky, Chintala, and Bottou 2017)</ref> and penalizing the norm of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are not available <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t #b4\">[5]</ref>. The system first predicts 68 face landmarks from speech using an LSTM-based network <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and then predicts a few talking face images from the condit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\"   face landmarks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\"  loss besides the reconstruction loss to improve the temporal dependency across frames. Song et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> proposed another method that generates talking faces by usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 15\">[16]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr hat generate talking faces directly from a condition image and the speech signal. Vougioukas et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> proposed a temporal-GAN method to generate more realistic . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b13\">[14]</ref> and that we heavily rely on visual cues in emotion interpretation <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Therefore, to make the visual rendering more realistic an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ech comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> in noisy environmen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndmarks. They employ a discriminator network to improve image quality. In another work, Egor et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed a style-based landmark-to-image conversion method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref> concludes that different modalities complement each other,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es, when present, also play a vital role. The presence of visual cues improves speech comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: intermediate representations are all passed to the video decoder using U-Net style skip connections <ref type=\"bibr\" target=\"#b29\">[30]</ref>.</p><p>3) Emotion Encoder: The emotion label is first enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: motion perception from auditory and visual stimuli has been examined in recent years. Existing work <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bib t emotional visual content allows more reliable prediction of auditory information. Schirmer et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> explored modalities in term of neural responses and showed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Existing work <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bib is sensitive to stimuli from multiple modalities in acted data and naturalistic data. Jessen et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> suggested that emotional visual content allows more reliab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. These systems can increase the accessibility of abundantl b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref> and then estimate video frames using the predicted face la method is limited to only generating the lip region instead of the entire talking face. Zhou et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed a GAN-based method that models the whole face and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: CNN which is trained on pairs of artificially blurred images and their clear originals. Chen et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> proposed another method that predicts video frames of the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: der to provide the visual cues when they are not available <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t om researchers in recent years. One approach is to first convert the speech input to face landmarks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" MFCCs), energy, and the first-and second-order temporal derivatives of these features. Chung et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> proposed a CNN that takes a condition face image and speech . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ach is to first convert the speech input to face landmarks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" estimate video frames using the predicted face landmarks. In Suwajanakorn et al.'s two-stage system <ref type=\"bibr\" target=\"#b15\">[16]</ref>, an LSTM network first predicts the principal component an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ity between generated frames, and the synchronization between audio and visual data. Eskimez et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposed an end-to-end talking face generation system that ial network (GAN) framework. Our generator network architecture is built based on our previous work <ref type=\"bibr\" target=\"#b20\">[21]</ref>, with a modification to accept the emotion condition input the input speech waveform and outputs a speech embedding. It follows the original implementation of <ref type=\"bibr\" target=\"#b20\">[21]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head condition face image. The architecture follows the original implementation without any modification <ref type=\"bibr\" target=\"#b20\">[21]</ref>. It contains six layers of 2-D convolutional layers with t  image, and the emotion condition.</p><p>5) Video Decoder: We modify the video decoder described in <ref type=\"bibr\" target=\"#b20\">[21]</ref> to accept the additional emotion embedding. We concatenate that focus on different aspects of the generated videos: a mouth region mask (MRM) loss proposed in <ref type=\"bibr\" target=\"#b20\">[21]</ref> to improve mouth-audio synchronization, a perceptual loss  rames. To measure the audiovisual synchronization, we used the normalized landmarks distance (NLMD) <ref type=\"bibr\" target=\"#b20\">[21]</ref> between landmarks extracted from the generated and ground-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lass emotion classification accuracy on the ground-truth videos is 62.71%, which is comparable with <ref type=\"bibr\" target=\"#b36\">[37]</ref>, suggesting the validity of the video-based emotion classi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref> concludes that  ludes that different modalities complement each other, and there are also intermodal effects. Cowie <ref type=\"bibr\" target=\"#b25\">[26]</ref> showed that perception is sensitive to stimuli from multip. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndmarks. They employ a discriminator network to improve image quality. In another work, Egor et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed a style-based landmark-to-image conversion method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gruent stimuli from these two modalities; little work examined incongruent stimuli. Tsiourti et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> investigated human responses to emotions expressed by the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  where we do not use an activation since our system employs Wasserstein GAN with a gradient penalty <ref type=\"bibr\" target=\"#b30\">[31]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic, lowering the workers' confidence in rating the ground-truth videos. A Wilcoxon signed-rank test <ref type=\"bibr\" target=\"#b37\">[38]</ref> shows that the median difference between our ratings and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cal works include XLA <ref type=\"bibr\" target=\"#b8\">[9]</ref> (applicable to training as well), TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, Glow <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Tensor Com gebra operations and calls into backend-specific libraries for execution on different backends. TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref> is an end-to-end compiler framework with Halide at the core. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b19\">[20]</ref>, TIRAMISU <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Triton <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>In principle, accelerating primitives intends to op. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b19\">[20]</ref>, TIRAMISU <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Triton <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>In principle, accelerating primitives intends to op. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or fusion ineffective at all. Taking GPU as an example, one effective approach is to write one CUDA <ref type=\"bibr\" target=\"#b22\">[23]</ref> kernel function for the fused operator and complete the wh s well. Nonetheless, our paper will merely investigate optimization techniques on CUDA-enabled GPUs <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p><p>Halide compiler Halide is a DSL compiler based on t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e gradient update per data sample, PPO enables training with mini-batch updates. In addition, RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> is used to implement our search algorithm (see Figure <ref ormula_9\">L V F t the square-error loss V \u03b8 (s t ) \u2212 V target t</formula><p>. Please refer to RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> for more implementation details.</p></div> <div xmlns=\"htt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Popular deep learning frameworks such as Caffe <ref type=\"bibr\" target=\"#b4\">[5]</ref>, TensorFlow <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Mxnet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and PyTorch <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b10\">[11]</ref>, Tensor Comprehensions <ref type=\"bibr\" target=\"#b11\">[12]</ref>, nGraph <ref type=\"bibr\" target=\"#b12\">[13]</ref>, OpenVINO <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and T VM and aims to optimize CNN inference on CPUs by taking advantage of wide SIMD instructions. nGraph <ref type=\"bibr\" target=\"#b12\">[13]</ref> adopts a similar workflow to TVM, but was further extended. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e gradient update per data sample, PPO enables training with mini-batch updates. In addition, RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> is used to implement our search algorithm (see Figure <ref ormula_9\">L V F t the square-error loss V \u03b8 (s t ) \u2212 V target t</formula><p>. Please refer to RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> for more implementation details.</p></div> <div xmlns=\"htt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ilar workflow to TVM, but was further extended to support encypted data with homomorphic encryption <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Some other compiling frameworks (e.g. Tensor Comprehensio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bservation representation. For 2D convolutions that are basically most time-consuming in CNN models <ref type=\"bibr\" target=\"#b28\">[29]</ref>, the observation O conv is 17-dimensional and defined as</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b4\">[5]</ref>, TensorFlow <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Mxnet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and PyTorch <ref type=\"bibr\" target=\"#b7\">[8]</ref> all prov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ial for training large and complex deep learning architectures. RNN-T models are difficult to train <ref type=\"bibr\" target=\"#b10\">[11]</ref> and also require significantly large amount of data to joi oral classification (CTC) model <ref type=\"bibr\" target=\"#b1\">[2]</ref> or cross entropy (CE) model <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and the prediction network with LSTM language model (LM)  on.</p><p>model for encoder and prediction network in the context of TL the RNN-T model. Authors in <ref type=\"bibr\" target=\"#b10\">[11]</ref> have shown that CE initialized RNN-T models perform better in blocks. gets (necessary for CE training), is obtained from word level alignments as discussed in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. From the word alignments, the start frame, end frame and  ned by using byte pair encoding <ref type=\"bibr\" target=\"#b25\">[26]</ref> algorithm as described in <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>We also report the word error rate (WER) on hybrid . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar a well trained AM from high-resource language to bootstrap the low-resource AM; multi-task training <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and ensemble learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k</head><p>Several methods have been proposed to improve the performance of low-resource ASR models <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Successful strategies include transfer learning <ref type=  type=\"bibr\" target=\"#b17\">18]</ref> and ensemble learning <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> that aim to utilize multi-lingual data and share the model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k</head><p>Several methods have been proposed to improve the performance of low-resource ASR models <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Successful strategies include transfer learning <ref type=  type=\"bibr\" target=\"#b17\">18]</ref> and ensemble learning <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> that aim to utilize multi-lingual data and share the model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar uage to bootstrap the low-resource AM; multi-task training <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and ensemble learning <ref type=\"bibr\" target=\"#b18\">[19,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Successful strategies include transfer learning <ref type=  type=\"bibr\" target=\"#b17\">18]</ref> and ensemble learning <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> that aim to utilize multi-lingual data and share the model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  single neural network <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tc. There are also studies that leverage other architectures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> for sequential reco xt from both directions for sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n owing previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we apply the leave-oneout strategy for evaluation. Concret model, which uses the multi-head attention mechanism to recommend the next item.</p><p>(7) BERT4Rec <ref type=\"bibr\" target=\"#b22\">[23]</ref> uses a Cloze objective loss for sequential recommendation . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: equential recommenders <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. It has been demonstrated that contextual information is im ich is able to achieve the same effect as previous methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Besides, the pre-trained data representations can be also   them by the interaction timestamps ascendingly. Following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we only keep the 5-core datasets, and filter unpopular ite te the performance, which are widely used in related works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Since HR@1 is equal to NDCG@1, we report results on HR@{1, ation Machines to incorporate arbitrary real-valued features to the sequential recommendation. FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> employed a feature-level self-attention block to leverage  ttribute-aware sequential models such as TransFM <ref type=\"bibr\" target=\"#b15\">[16]</ref> and FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> leverage the contextual features to improve the sequential  and attribute as the input to the model. ( <ref type=\"formula\" target=\"#formula_16\">11</ref>) FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> constructs a feature sequence and uses a featurelevel self. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: @{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we apply the leave. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (such as item attributes) to neural sequential recommenders <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. It has been demonstr el by introducing pair-wise loss functions <ref type=\"bibr\" target=\"#b3\">[4]</ref>, memory networks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, hierarchical structur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: not been well captured in data representations. As shown in increasing evidence from various fields <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target  idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> is a newly emerging  tp://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Self-supervised Learning</head><p>Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target guide the visual feature learning <ref type=\"bibr\" target=\"#b4\">[5]</ref>. As for language modeling <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, it is a popular sel . It is beneficial to incorporate context from both directions for sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmln  or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref> and natural language understanding <ref type=\"bibr\" target=\" upervised learning objectives with the recently proposed pretraining framework in language modeling <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref ty multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, at the pre-training stage, we remove the mask mechanism to  deed observed by the model in the training process. Inspired by the masked language model like BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we propose to model the bidirectional information in item s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">[27]</ref>, etc. There are also studies that leverage other architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (such as item attributes) to neural sequential recommenders <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. It has been demonstr el by introducing pair-wise loss functions <ref type=\"bibr\" target=\"#b3\">[4]</ref>, memory networks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, hierarchical structur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b6\">7]</ref>, hierarchical structures <ref type=\"bibr\" target=\"#b16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b6\">7]</ref>, hierarchical structures <ref type=\"bibr\" target=\"#b16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p><p>Typically, s recommendation methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> capture useful sequ It has been found that such an optimization way is easy to suffer from issues such as data sparsity <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>.</p></div> <div xm the interaction records by users and sort them by the interaction timestamps ascendingly. Following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we only keep the   and Mean Reciprocal Rank (MRR) to evaluate the performance, which are widely used in related works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Since HR@1 is equ enhance data representations instead of making predictions.</p><p>Sequential models such as GRU4Rec <ref type=\"bibr\" target=\"#b20\">[21]</ref> and SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> mainly  as a similar effect to capture sequential dependencies as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> except that it can also utilize bidirectional sequential in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ial recommendation has been widely studied in the literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targe 4]</ref>.</p><p>Typically, sequential recommendation methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" targe s of works follow this line and extend it for high-order MCs <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. With the development ad><p>Existing studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> mainly emphasize the  d is a parameter matrix to learn. Note that existing methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> seldom directly model nal neural networks (CNNs) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and self-attention mechanisms <ref type=\"bibr\" target=\"#b7\">[8]</ref> have been proposed to learn good representations of user pre commendation with MIM, which is called S 3 -Rec. Based on a self-attentive recommender architecture <ref type=\"bibr\" target=\"#b7\">[8]</ref>, we propose to first pre-train the sequential recommender wi introduce the base model of our proposed approach that is developed on the Transformer architecture <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Then, we will describe how we utilize the correlation signa tions.</p><p>Sequential models such as GRU4Rec <ref type=\"bibr\" target=\"#b20\">[21]</ref> and SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> mainly focus on modeling the sequential dependencies between ng horizontal and vertical convolutional operations for sequential recommendation.</p><p>(6) SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> is a self-attention based sequential recommendation model, w  heads as 2. The dimension of the embedding is 64, and the maximum sequence length is 50 (following <ref type=\"bibr\" target=\"#b7\">[8]</ref>). Note that our training phase contains two stages (i.e., pr type=\"bibr\" target=\"#b26\">[27]</ref>, etc. There are also studies that leverage other architectures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ tem prediction loss L M I P in Eq. 12 has a similar effect to capture sequential dependencies as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> except that it can a qual to NDCG@1, we report results on HR@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ ems as candidates for testing. Following the common strategy <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, we pair the ground-truth item with 99 randomly sampled negat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ial recommendation has been widely studied in the literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targe 4]</ref>.</p><p>Typically, sequential recommendation methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" targe s of works follow this line and extend it for high-order MCs <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. With the development ad><p>Existing studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> mainly emphasize the  d is a parameter matrix to learn. Note that existing methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> seldom directly model nal neural networks (CNNs) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and self-attention mechanisms <ref type=\"bibr\" target=\"#b7\">[8]</ref> have been proposed to learn good representations of user pre commendation with MIM, which is called S 3 -Rec. Based on a self-attentive recommender architecture <ref type=\"bibr\" target=\"#b7\">[8]</ref>, we propose to first pre-train the sequential recommender wi introduce the base model of our proposed approach that is developed on the Transformer architecture <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Then, we will describe how we utilize the correlation signa tions.</p><p>Sequential models such as GRU4Rec <ref type=\"bibr\" target=\"#b20\">[21]</ref> and SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> mainly focus on modeling the sequential dependencies between ng horizontal and vertical convolutional operations for sequential recommendation.</p><p>(6) SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> is a self-attention based sequential recommendation model, w  heads as 2. The dimension of the embedding is 64, and the maximum sequence length is 50 (following <ref type=\"bibr\" target=\"#b7\">[8]</ref>). Note that our training phase contains two stages (i.e., pr type=\"bibr\" target=\"#b26\">[27]</ref>, etc. There are also studies that leverage other architectures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ tem prediction loss L M I P in Eq. 12 has a similar effect to capture sequential dependencies as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> except that it can a qual to NDCG@1, we report results on HR@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ ems as candidates for testing. Following the common strategy <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, we pair the ground-truth item with 99 randomly sampled negat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (such as item attributes) to neural sequential recommenders <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. It has been demonstr el by introducing pair-wise loss functions <ref type=\"bibr\" target=\"#b3\">[4]</ref>, memory networks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, hierarchical structur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b6\">7]</ref>, hierarchical structures <ref type=\"bibr\" target=\"#b16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive loss for sequential recommendation by the bidirectional self-attention mechanism.</p><p>(8) HGN <ref type=\"bibr\" target=\"#b12\">[13]</ref> is recently proposed and adopts hierarchical gating networ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on way is easy to suffer from issues such as data sparsity <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>a ased approaches such as Factorization Machine <ref type=\"bibr\" target=\"#b19\">[20]</ref> and AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> mainly learn data representations through the interaction  characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> utilizes the multi-head self-attentive neural network to l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on way is easy to suffer from issues such as data sparsity <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>a ased approaches such as Factorization Machine <ref type=\"bibr\" target=\"#b19\">[20]</ref> and AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> mainly learn data representations through the interaction  characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> utilizes the multi-head self-attentive neural network to l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: not been well captured in data representations. As shown in increasing evidence from various fields <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target  idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> is a newly emerging  tp://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Self-supervised Learning</head><p>Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target guide the visual feature learning <ref type=\"bibr\" target=\"#b4\">[5]</ref>. As for language modeling <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, it is a popular sel . It is beneficial to incorporate context from both directions for sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmln  or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref> and natural language understanding <ref type=\"bibr\" target=\" upervised learning objectives with the recently proposed pretraining framework in language modeling <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref ty multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, at the pre-training stage, we remove the mask mechanism to  deed observed by the model in the training process. Inspired by the masked language model like BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we propose to model the bidirectional information in item s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on way is easy to suffer from issues such as data sparsity <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>a ased approaches such as Factorization Machine <ref type=\"bibr\" target=\"#b19\">[20]</ref> and AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> mainly learn data representations through the interaction  characterizes the pairwise interactions between variables using factorized model.</p><p>(3) AutoInt <ref type=\"bibr\" target=\"#b21\">[22]</ref> utilizes the multi-head self-attentive neural network to l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons <ref type=\"bibr\" target=\"#b3\">[4]</ref>, memory networks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, hierarchical structures <ref type=\"bibr\" target=\"#b16\">[17]< rge, it is time-consuming to use all items as candidates for testing. Following the common strategy <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, we pair the ground-tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 16\">[17]</ref>, copy mechanism <ref type=\"bibr\" target=\"#b17\">[18]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b26\">[27]</ref>, etc. There are also studies that leverage other architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: not been well captured in data representations. As shown in increasing evidence from various fields <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target  idea of self-supervised learning for improving sequential recommendation. Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> is a newly emerging  tp://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Self-supervised Learning</head><p>Self-supervised learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target guide the visual feature learning <ref type=\"bibr\" target=\"#b4\">[5]</ref>. As for language modeling <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, it is a popular sel . It is beneficial to incorporate context from both directions for sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmln  or sequences can improve the performance of downstream tasks such as machine reading comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref> and natural language understanding <ref type=\"bibr\" target=\" upervised learning objectives with the recently proposed pretraining framework in language modeling <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>The overview of S 3 -Rec is presented in Fig. <ref ty multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, at the pre-training stage, we remove the mask mechanism to  deed observed by the model in the training process. Inspired by the masked language model like BERT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we propose to model the bidirectional information in item s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmance, most of them ignore the interactions' timestamp values. While recent works such as TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> successfully incorporated time information, their usage of ransformer <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cloze-task based training method. TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> enhanced SASRec by merging timestamp information into self st, they don't utilize timestamp values which hold important contextual information. While TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> successfully addressed this issue, they also used a simple pe=\"bibr\" target=\"#b31\">[32]</ref>,</p><p>SASRec <ref type=\"bibr\" target=\"#b9\">[10]</ref>, TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and BERT4Rec <ref type=\"bibr\" target=\"#b18\">[19]</ref>. T essing procedure from <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. We convert each da g the custom practice <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, we discard users a We use the rest for training. We follow the common practice <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> of letting the mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e strong order constraint of RNN models, CNN-based methods <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> were proposed. Some. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Despite their excellent performance, most of them ignore t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orms of time gates to better model the time intervals in user's interaction sequence. Recently, CTA <ref type=\"bibr\" target=\"#b24\">[25]</ref> used multiple parametrized kernel functions on temporal in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> brought the success. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b2\">[3]</ref>, MovieLens 20M <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Amazon Beauty <ref type=\"bibr\" target=\"#b15\">[16]</ref> and Amazon Game <ref type=\"bibr\" target=\"#b15\">[16]</ref>. br\" target=\"#b2\">[3]</ref>, Amazon Beauty <ref type=\"bibr\" target=\"#b15\">[16]</ref> and Amazon Game <ref type=\"bibr\" target=\"#b15\">[16]</ref>. We follow the common data preprocessing procedure from <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Despite their exc em sequence understanding. To overcome the strong order constraint of RNN models, CNN-based methods <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b2\">[3]</ref>, MovieLens 20M <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Amazon Beauty <ref type=\"bibr\" target=\"#b15\">[16]</ref> and Amazon Game <ref type=\"bibr\" target=\"#b15\">[16]</ref>. br\" target=\"#b2\">[3]</ref>, Amazon Beauty <ref type=\"bibr\" target=\"#b15\">[16]</ref> and Amazon Game <ref type=\"bibr\" target=\"#b15\">[16]</ref>. We follow the common data preprocessing procedure from <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: v-chain based methods <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref> assume that users' be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orms of time gates to better model the time intervals in user's interaction sequence. Recently, CTA <ref type=\"bibr\" target=\"#b24\">[25]</ref> used multiple parametrized kernel functions on temporal in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imilarity measure. A is the action space, f is the reward function, and T is the terminal condition <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Given an initial p (l ) r , the neighbor selector choose . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: otes as fraudulent entities. Though previous works have proposed other fraud datasets like Epinions <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Bitcoin <ref type=\"bibr\" target=\"#b39\">[40]</ref>, the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and industrial communities <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targ ><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> and practitioners <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e proposed other fraud datasets like Epinions <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Bitcoin <ref type=\"bibr\" target=\"#b39\">[40]</ref>, they only contain graph structures and compacted features. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s Enhancement. As the most popular deep learning framework on graph data, GNNs have two major types <ref type=\"bibr\" target=\"#b41\">[42]</ref>: 1) Spectral-based GNNs (GCN <ref type=\"bibr\" target=\"#b16. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tect opinion fraud <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, financial fraud <ref type=\"bibr\" target=\"#b22\">[23,</ref><  fraud detectors transfer the heterogeneous data into homogeneous data before applying GNNs. Fdgars <ref type=\"bibr\" target=\"#b38\">[39]</ref> and GraphConsis <ref type=\"bibr\" target=\"#b24\">[25]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tect opinion fraud <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, financial fraud <ref type=\"bibr\" target=\"#b22\">[23,</ref><  fraud detectors transfer the heterogeneous data into homogeneous data before applying GNNs. Fdgars <ref type=\"bibr\" target=\"#b38\">[39]</ref> and GraphConsis <ref type=\"bibr\" target=\"#b24\">[25]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elp. The Amazon dataset includes product reviews under the Musical Instruments category. Similar to <ref type=\"bibr\" target=\"#b46\">[47]</ref>, we label users with more than 80% helpful votes as benign 2 handcrafted features from <ref type=\"bibr\" target=\"#b28\">[29]</ref> (25 handcrafted features from <ref type=\"bibr\" target=\"#b46\">[47]</ref> resp.) as the raw node features for Yelp (Amazon resp.) da. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ture camouflage: smart fraudsters may adjust their behaviors <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, add special characters in reviews <ref type=\"bibr\" target=\" ster.</p><p>Considering the agility of real-world fraudsters <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, designing GNN-based detectors that exactly capture these ca  introduced various fraudster camouflage types from behavior <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and semantic <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: velopment of Graph Neural Networks (GNNs) (e.g., GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GAT <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and GraphSAGE <ref type=\"bibr\" target=\"#b11\">[12]</ref>),  vector, e.g., mean aggregation <ref type=\"bibr\" target=\"#b11\">[12]</ref> and attention aggregation <ref type=\"bibr\" target=\"#b33\">[34]</ref>. \u2295 is the operator that combines the information of v and   the semi-supervised learning setting. We select GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GAT <ref type=\"bibr\" target=\"#b33\">[34]</ref>, RGCN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and Graph nts of CARE-GNN: CARE-Att, CARE-Weight, and CARE-Mean, and they differ from each other in Attention <ref type=\"bibr\" target=\"#b33\">[34]</ref>, Weight <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and Mea placian matrix and make convolutional operations in the spectral domain. 2) Spatial-based GNNs (GAT <ref type=\"bibr\" target=\"#b33\">[34]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b11\">[12]</ref>): the >[23]</ref> learns convolutional layers and neighbor weights using LSTM and the attention mechanism <ref type=\"bibr\" target=\"#b33\">[34]</ref>. GEM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, SemiGNN <r information along  <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Based on the defined multi-relation graph in Definition 2.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s Enhancement. As the most popular deep learning framework on graph data, GNNs have two major types <ref type=\"bibr\" target=\"#b41\">[42]</ref>: 1) Spectral-based GNNs (GCN <ref type=\"bibr\" target=\"#b16. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD <ref type=\"bibr\" target=\"#b3\">[4]</ref> focuses on the lip region, we found that lip landmarks can b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  existing works is in terms of the vocabulary. Several works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> train on datasets w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  work is that it can simultaneously also encourage efforts <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: syncing talking face videos to match a given input audio stream has received considerable attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et audio. A viewer can recognize an out-of-sync video segment as small as just \u2248 0.05 \u2212 0.1 seconds <ref type=\"bibr\" target=\"#b8\">[9]</ref> in duration. Thus, convincingly lip-syncing a real-world vid s derived from three standard test sets. We also propose reliable evaluation metrics using Sync-Net <ref type=\"bibr\" target=\"#b8\">[9]</ref> to precisely evaluate lip sync in unconstrained videos. We a tasets <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> is the SyncNet <ref type=\"bibr\" target=\"#b8\">[9]</ref> model. We propose to adapt and train a modified version of S pe=\"bibr\" target=\"#b8\">[9]</ref> model. We propose to adapt and train a modified version of SyncNet <ref type=\"bibr\" target=\"#b8\">[9]</ref> for our task.</p></div> <div xmlns=\"http://www.tei-c.org/ns/ div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3.1\">Overview of SyncNet.</head><p>SyncNet <ref type=\"bibr\" target=\"#b8\">[9]</ref> inputs a window V of T v consecutive face frames (lower half c accuracy.</p><p>3.3.2 Our expert lip-sync discriminator. We make the following changes to SyncNet <ref type=\"bibr\" target=\"#b8\">[9]</ref> to train an expert lip-sync discriminator that suits our lip =\"4.2.1\">A Metric to</head><p>Measure the Lip-Sync Error. We propose to use the pre-trained SyncNet <ref type=\"bibr\" target=\"#b8\">[9]</ref> available publicly 3 to measure the lip-sync error between t  the randomly chosen speech segment. The accuracy of SyncNet averaged over a video clip is over 99% <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Thus, we believe this can be a good automatic evaluation me pert lip-sync discriminator that we have trained above, but the one released by Chung and Zisserman <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which was trained on a different, non-public dataset. Using  video with completely out-of-sync lip movements. Further details can be found in the SyncNet paper <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= s are most likely to be used. Further, given the sensitivity of humans to audio-lip synchronization <ref type=\"bibr\" target=\"#b8\">[9]</ref>, it is necessary to also evaluate our results with the help  rated real video results using our new automatic metrics, \"LSE-D\" and \"LSE-C\" obtained from SyncNet <ref type=\"bibr\" target=\"#b8\">[9]</ref>. For the human evaluation, we ask 14 evaluators to judge the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: given input audio stream has received considerable attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar  speech representations to lip landmarks using several hours of a single speaker. More recent works <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> in this line direc  large amount of data of a particular speaker, typically a few hours. A recent work along this line <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposes to seamlessly edit videos of individual speakers . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le a wide range of real-world applications where previous speaker-independent lipsyncing approaches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> struggle to produc  speech inputs. This has led to the creation of speaker-independent speech to lip generation models <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that are trained o -sync random identities for any speech. To the best of our knowledge, only two such prominent works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> exist in the curre b16\">[17]</ref> is an extended version of <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Both these works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> formulate the task We argue that the loss functions, namely the L1 reconstruction loss used in both the existing works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and the discrimina d n=\"4.3\">Comparing the Models on the New Benchmark</head><p>We compare the previous two approaches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> on our newly creat  our results in Table <ref type=\"table\">2</ref>. An outcome worth noting is that the previous works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> which produce seve nsiderable attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar #b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> exist in the current literature. Note that <ref type=\"bibr\" target=\"#b16\">[17]</ref> is an extended version of <ref type=\"bibr\" target=\"#b6\">[7 already well-trained lip-sync expert\". Unlike previous works that employ only a reconstruction loss <ref type=\"bibr\" target=\"#b16\">[17]</ref> or train a discriminator in a GAN setup <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le a wide range of real-world applications where previous speaker-independent lipsyncing approaches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> struggle to produc  speech inputs. This has led to the creation of speaker-independent speech to lip generation models <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that are trained o -sync random identities for any speech. To the best of our knowledge, only two such prominent works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> exist in the curre b16\">[17]</ref> is an extended version of <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Both these works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> formulate the task We argue that the loss functions, namely the L1 reconstruction loss used in both the existing works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and the discrimina d n=\"4.3\">Comparing the Models on the New Benchmark</head><p>We compare the previous two approaches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> on our newly creat  our results in Table <ref type=\"table\">2</ref>. An outcome worth noting is that the previous works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> which produce seve nsiderable attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar #b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> exist in the current literature. Note that <ref type=\"bibr\" target=\"#b16\">[17]</ref> is an extended version of <ref type=\"bibr\" target=\"#b6\">[7 already well-trained lip-sync expert\". Unlike previous works that employ only a reconstruction loss <ref type=\"bibr\" target=\"#b16\">[17]</ref> or train a discriminator in a GAN setup <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s where previous speaker-independent lipsyncing approaches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> struggle to produce satisfactory results.</p></div> \t\t\t</ab ion of speaker-independent speech to lip generation models <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that are trained on thousands of identities and voices. The o the best of our knowledge, only two such prominent works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> exist in the current literature. Note that <ref type=\"bibr\"  <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Both these works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> formulate the task of learning to lip-sync in the wild as f the L1 reconstruction loss used in both the existing works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and the discriminator loss in LipGAN <ref type=\"bibr\" targe  Benchmark</head><p>We compare the previous two approaches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> on our newly created test set using the LSE-D and LSE-C met </ref>. An outcome worth noting is that the previous works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> which produce several out-of-sync segments are less preferr rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> in the research com ny identity in any voice, including that of a synthetic speech generated by a text-to-speech system <ref type=\"bibr\" target=\"#b17\">[18]</ref>. However, to be used for applications like translating a l ficantly hampers a model from learning the vast diversity of phoneme-viseme mappings in real videos <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our work focuses on lip-syncing unconstrained talking fac en trying to lip-sync unconstrained videos in the wild. In contrast to the GAN setup used in LipGAN <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we use a pre-trained, accurate lip-sync discriminator tha rget=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and the discriminator loss in LipGAN <ref type=\"bibr\" target=\"#b17\">[18]</ref> are inadequate to penalize inaccurate lip-sync generation. ins morphing lips only at around half-way (\u2248 11 th epoch) through its training process (\u2248 20 epochs <ref type=\"bibr\" target=\"#b17\">[18]</ref>). Thus, it is crucial to have an additional discriminator  . Thus, it is crucial to have an additional discriminator to judge lip-sync, as also done in LipGAN <ref type=\"bibr\" target=\"#b17\">[18]</ref>. But, how powerful is the discriminator employed in LipGAN construction loss <ref type=\"bibr\" target=\"#b16\">[17]</ref> or train a discriminator in a GAN setup <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we use a pre-trained discriminator that is already quite  ner 3.4.1 Generator Architecture Details. We use a similar generator architecture as used by LipGAN <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our key contribution lies in training this with the exper sed architecture by explaining how it works during the inference on real videos. Similar to Lip-GAN <ref type=\"bibr\" target=\"#b17\">[18]</ref>, the model generates a talking face video frame-by-frame.   essential for future works that aspire to automatically translate videos (Face-to-Face Translation <ref type=\"bibr\" target=\"#b17\">[18]</ref>) or rapidly create new video content. We manually transcri ted from our proposed models (green and yellow outlines). We compare with the current best approach <ref type=\"bibr\" target=\"#b17\">[18]</ref> (red outline). The text is shown for illustration to denot. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , we feed color images. Secondly, our model is significantly deeper, with residual skip connections <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Thirdly, inspired by this public implementation 2 , we us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for that speaker. Another limitation of existing works is in terms of the vocabulary. Several works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  size) is usually a pre-specified parameter. Current works <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> did not efficiently handle the change of discriminative attr y a graph auto-encoder, but this method neglects linkage between paper and author and coauthorship. <ref type=\"bibr\" target=\"#b6\">[7]</ref> addresses the pairwise classification problem by extracting . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> did not efficiently ised <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based one  of author mentions belonging to the same author and are essential in the name disambiguation task. <ref type=\"bibr\" target=\"#b5\">[6]</ref> first learns representation for every name mention in a pair ative attribute to separate papers into small blocks and we use the same trainset and testset as in <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>In Semantic Scholar, the selected meta-paths of our m block as sequence s \u2208 S; 3 Construct meta-path based view {G p1</figDesc><table /><note>\u2022 Aminer-AND<ref type=\"bibr\" target=\"#b5\">[6]</ref>: This dataset contains 70,285 records of 12,798 unique autho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: blication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which aims to iden umber of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t ef type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Graph-based works  gical features in the academic network to enhance the representation of papers. For instance, GHOST <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructs document graph based on co-authorship. <ref type=  by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.\u2022 GHOST<ref type=\"bibr\" target=\"#b1\">[2]</ref>: GHOST use affinity propagation algorithm for clustering on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt years. Meta-path is designed to preserve diverse semantic information of node type and edge type <ref type=\"bibr\" target=\"#b19\">[20]</ref>- <ref type=\"bibr\" target=\"#b21\">[22]</ref>. GTN <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which aims to iden umber of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t ef type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Graph-based works  gical features in the academic network to enhance the representation of papers. For instance, GHOST <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructs document graph based on co-authorship. <ref type=  by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.\u2022 GHOST<ref type=\"bibr\" target=\"#b1\">[2]</ref>: GHOST use affinity propagation algorithm for clustering on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Fraser University. The change of discriminative attributes may lead to the paper separation problem <ref type=\"bibr\" target=\"#b3\">[4]</ref>, i.e., papers of an author are regarded as belonging to diff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sional vector while preserving graph structure and properties. Recently, Graph Neural Network (GNN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> has attracted rising attention due to effective representat </ref> has attracted rising attention due to effective representation ability. While most GNN works <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> focus on transdu ne score function is applied to measure the similarity of the two paper sequence representations as <ref type=\"bibr\" target=\"#b9\">(10)</ref>.</p><p>L sim = sim(h (1) , h (2) ) = h (1) \u2022 h (2)  h (1) \u2022. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which aims to iden umber of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t ef type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Graph-based works  gical features in the academic network to enhance the representation of papers. For instance, GHOST <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructs document graph based on co-authorship. <ref type=  by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.\u2022 GHOST<ref type=\"bibr\" target=\"#b1\">[2]</ref>: GHOST use affinity propagation algorithm for clustering on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  diverse semantic information of node type and edge type <ref type=\"bibr\" target=\"#b19\">[20]</ref>- <ref type=\"bibr\" target=\"#b21\">[22]</ref>. GTN <ref type=\"bibr\" target=\"#b22\">[23]</ref> converts he. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>I. INTRODUCTION</head><p>Namesake problem <ref type=\"bibr\" target=\"#b0\">[1]</ref> poses a huge challenge on many applications, e.g., informati 0\"><head>A. Name Disambiguation</head><p>Name disambiguation methods can be divided into supervised <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and network-based attention weights for knowledge graphs.</p><p>Heterogeneous information networks <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref> have been studi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: owledge graphs.</p><p>Heterogeneous information networks <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref> have been studied in recent years. Meta-path is designed t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt years. Meta-path is designed to preserve diverse semantic information of node type and edge type <ref type=\"bibr\" target=\"#b19\">[20]</ref>- <ref type=\"bibr\" target=\"#b21\">[22]</ref>. GTN <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>I. INTRODUCTION</head><p>Namesake problem <ref type=\"bibr\" target=\"#b0\">[1]</ref> poses a huge challenge on many applications, e.g., informati 0\"><head>A. Name Disambiguation</head><p>Name disambiguation methods can be divided into supervised <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and network-based attention weights for knowledge graphs.</p><p>Heterogeneous information networks <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref> have been studi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Fraser University. The change of discriminative attributes may lead to the paper separation problem <ref type=\"bibr\" target=\"#b3\">[4]</ref>, i.e., papers of an author are regarded as belonging to diff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: owledge graphs.</p><p>Heterogeneous information networks <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref> have been studied in recent years. Meta-path is designed t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s by aggregating their features. Both DeepGL and GraphSage are designed for homogeneous graphs. LAN <ref type=\"bibr\" target=\"#b14\">[15]</ref> aggregates neighbors with both rule-based and network-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . They brought great advancements in many applications of neural network, such as visual perception <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref> to automatically designed neural architectures <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t es human experts to frequently try and evaluate numerous different operation and connection options <ref type=\"bibr\" target=\"#b3\">[4]</ref>. In contrast to architectures that are manually designed, th NAS-generated architectures have shown promising results in many domains, such as image recognition <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t rates a total search space of itive training procedure of each selected architecture can be avoided <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> so that researche. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b33\">[34]</ref>. We choose these three datasets because CIFAR and ImageNet <ref type=\"bibr\" target=\"#b34\">[35]</ref> are the most popular image classification datasets.</p><p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e terminology \"architecture size\" or \"size\" refer to the number of channels in each layer following <ref type=\"bibr\" target=\"#b20\">[21]</ref>. for the DAG and the size of the operation set. We choose  e Search Space S s . The size search space is inspired by transformable architecture search methods <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bib , GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <ref type=\"bibr\" target=\"#b43\">[44]</ref>, TuNAS e interpolation to aggregate feature tensors with different shapes with the architecture parameters <ref type=\"bibr\" target=\"#b20\">[21]</ref>. (2) FBNetV2 utilises a masking mechanism to represent dif ple, how to design a FLOPs constrain loss to regularize the discovered architecture to be efficient <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b11\">[12]</ref>, accuracy prediction <ref type=\"bibr\" target=\"#b37\">[38]</ref>, mutation-based NAS <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, etc.</p><p>Arc Other methods mutate an architecture to become another one <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. With NATS-Bench, researchers could directly use the off-t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t. (1) Different search space is utilized, e.g., range of macro skeletons of the whole architecture <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref> and a different bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, different regularization <ref type=\"bibr\" target=\"#b10\">[11]</ref>, different scheduler <ref type=\"bibr\" target=\"#b14\">[15]</ ge as inspired by <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. We summarize characteristics of our NATS-Bench and NAS-Be ed NAS algorithms <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. As shown in the middle part of Figure <ref type=\"figure\"  ed NAS algorithms <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Since all cells in an architecture have the same topology ure to set up the hyper-parameters and training strategies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. We train each . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b2\">[3]</ref>, VGGNet <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and Transformer <ref type=\"bibr\" target=\"#b9\">[10]</ref>. However, manually designing one architecture requires huma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e employed to train this architecture and report the performance, e.g., different data augmentation <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, different regu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lgorithms, such as platformaware NAS <ref type=\"bibr\" target=\"#b11\">[12]</ref>, accuracy prediction <ref type=\"bibr\" target=\"#b37\">[38]</ref>, mutation-based NAS <ref type=\"bibr\" target=\"#b38\">[39]</r  to predict the final accuracy of an architecture based on the results of few early training epochs <ref type=\"bibr\" target=\"#b37\">[38]</ref>. These algorithms can be trained faster and the performanc 5\">[6]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, which learn to search based on an approximation of the pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rder. In H 0 , we train each architecture by 12 epochs, which can be used in banditbased algorithms <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Since 12 epoch y based methods <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, which learn to get=\"#b43\">[44]</ref>, TuNAS <ref type=\"bibr\" target=\"#b48\">[49]</ref>. (V) HPO methods, e.g., BOHB <ref type=\"bibr\" target=\"#b35\">[36]</ref>.</p><p>Among them, RANDOM, REA, REINFORCE, and BOHB are mu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1.0\"><head n=\"3.2\">Datasets</head><p>We train and evaluate each architecture on CIFAR-10, CIFAR-100 <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and ImageNet-16-120 <ref type=\"bibr\" target=\"#b33\">[34]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral architectures <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t image recognition <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and sequence modeling <ref type=\"bibr\" target=\"#b4\">[5]</ref  generate parameters of an architecture. Other methods mutate an architecture to become another one <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. With NATS-Bench, spective of both performance and efficiency. NAS has been dominated by multi-fidelity based methods <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" with parameter sharing (RSPS) <ref type=\"bibr\" target=\"#b26\">[27]</ref>. (II) ES methods, e.g., REA <ref type=\"bibr\" target=\"#b5\">[6]</ref>. (III) RL algorithms, e.g., REINFORCE <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ES methods, e.g., REA <ref type=\"bibr\" target=\"#b5\">[6]</ref>. (III) RL algorithms, e.g., REINFORCE <ref type=\"bibr\" target=\"#b47\">[48]</ref>, ENAS <ref type=\"bibr\" target=\"#b4\">[5]</ref>. (IV) Differ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e terminology \"architecture size\" or \"size\" refer to the number of channels in each layer following <ref type=\"bibr\" target=\"#b20\">[21]</ref>. for the DAG and the size of the operation set. We choose  e Search Space S s . The size search space is inspired by transformable architecture search methods <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bib , GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <ref type=\"bibr\" target=\"#b43\">[44]</ref>, TuNAS e interpolation to aggregate feature tensors with different shapes with the architecture parameters <ref type=\"bibr\" target=\"#b20\">[21]</ref>. (2) FBNetV2 utilises a masking mechanism to represent dif ple, how to design a FLOPs constrain loss to regularize the discovered architecture to be efficient <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 16-120. (II) In many vision tasks, pyramid structure has shown a surprising robustness and accuracy <ref type=\"bibr\" target=\"#b44\">[45]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Regarding the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: structure has shown a surprising robustness and accuracy <ref type=\"bibr\" target=\"#b44\">[45]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Regarding the parameters vs. the accuracy, the candidates. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [14]</ref>, different regularization <ref type=\"bibr\" target=\"#b10\">[11]</ref>, different scheduler <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and different selections of hyper-parameters <ref type=\"b ning strategies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. We train each architecture with the same strategy, which  We set the weight decay to 0.0005 and decay the learning rate from 0.1 to 0 with a cosine annealing <ref type=\"bibr\" target=\"#b14\">[15]</ref>. We use the same H 0 on different datasets, except for the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. In its early stage f type=\"bibr\" target=\"#b5\">[6]</ref> and sequence modeling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>Recently, a  f type=\"bibr\" target=\"#b23\">[24]</ref>, language modelling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, etc. Despite their efines operation candidates on the node, whereas we associate operations on the edge as inspired by <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t y Search Space S t . The topology search space is inspired by the popular cell-based NAS algorithms <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t e API. The implementation difference between DARTS <ref type=\"bibr\" target=\"#b7\">[8]</ref> and GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref> is only less than 20 lines of code. Our library reduces the  order DARTS (DARTS-V1) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref ty cy. Such strategy is more robust than using the arg max over the learned architecture parameters in <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <ref type=\"bibr\" target=\"#b43\">[44]</ref>, TuNAS <ref type=\"bibr\" target=\"#b48\">[49]</ref>. (V) HPO methods, e.g., BOHB <ref type=\"bibr\" target=\"#b35 f type=\"bibr\" target=\"#b43\">[44]</ref>. (3) TuNAS samples masks based on the learnable distribution <ref type=\"bibr\" target=\"#b48\">[49]</ref>. TAS and FBNetV2 optimize the architecture parameters in a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[3]</ref> to automatically designed neural architectures <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t omising results in many domains, such as image recognition <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and sequence modeli f type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and sequence modeling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t bibr\" target=\"#b11\">[12]</ref> and a different operation set for the micro cell within the skeleton <ref type=\"bibr\" target=\"#b4\">[5]</ref>, etc. (2) After a good architecture is selected, various str alidation set for testing the performance of the selected architecture is not split in the same way <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. These discrepancie type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, language modelling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t ncorporate this constraint in all NAS algorithms, such as NAS algorithms based on parameter sharing <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Therefore, many NA  space follows the design of its counterpart as used in the recent neural cell-based NAS algorithms <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t b5\">[6]</ref>. (III) RL algorithms, e.g., REINFORCE <ref type=\"bibr\" target=\"#b47\">[48]</ref>, ENAS <ref type=\"bibr\" target=\"#b4\">[5]</ref>. (IV) Differentiable algorithms. e.g., first order DARTS (DA nostic information and further motivate potential solutions for NAS. For example, parameter sharing <ref type=\"bibr\" target=\"#b4\">[5]</ref> is the crucial technique to improve searching efficiency, bu n multiple datasets. The model transferability can be thoroughly evaluated for most NAS algorithms. <ref type=\"bibr\" target=\"#b4\">(5)</ref> In NATS-Bench, we provide systematic analysis of the propose. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lgorithms, such as platformaware NAS <ref type=\"bibr\" target=\"#b11\">[12]</ref>, accuracy prediction <ref type=\"bibr\" target=\"#b37\">[38]</ref>, mutation-based NAS <ref type=\"bibr\" target=\"#b38\">[39]</r  to predict the final accuracy of an architecture based on the results of few early training epochs <ref type=\"bibr\" target=\"#b37\">[38]</ref>. These algorithms can be trained faster and the performanc 5\">[6]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, which learn to search based on an approximation of the pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eters for each architecture. This can provide ground truth label for hypernetwork-based NAS methods <ref type=\"bibr\" target=\"#b40\">[41]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, which learn to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nformation, they suffer from sensitivity to data alignment, often involve complicated architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bib -based proposal generation might fail in some cases that could only be observed from 3D space. MV3D <ref type=\"bibr\" target=\"#b10\">[11]</ref> and AVOD <ref type=\"bibr\" target=\"#b11\">[12]</ref> project. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and typically require pixel-level correspondences of sens  better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b13\">[14]</ref> to build dense LiDAR BEV feature maps and do point-wise fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  data alignment, often involve complicated architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bib  that could only be observed from 3D space. MV3D <ref type=\"bibr\" target=\"#b10\">[11]</ref> and AVOD <ref type=\"bibr\" target=\"#b11\">[12]</ref> project the raw point cloud into bird's eye view (BEV) to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: erate 3D proposals in a bottomup manner and then refines these proposals in a second stage. PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> leverages the advantages of both 3D voxel CNN and PointNet f type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref>. While not the top performers within the KITTI leaderboard as, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cascade R-CNN, as CLOCs PVCas. All the other combinati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etectors to demonstrate the flexibility of our proposed pipeline. The 2D detectors we used are: RRC <ref type=\"bibr\" target=\"#b28\">[29]</ref>, MS-CNN <ref type=\"bibr\" target=\"#b29\">[30]</ref> and Casc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> are hampered by typically lower input data resolution than v data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> uses PointNets <ref type=\"bibr\" target=\"#b6\">[7]</ref> in an  The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RC is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> publish their training configurations for class pedestrian a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. 3D Detection Using 2D Images</head><p>Mousavian et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> leverage the geometric constraints between 2D and 3D bound. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b23\">[24]</ref>, Pointfusion <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Frustum ConvNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> are the representatives of 2D driven 3D detectors, which e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t etworks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]< ted examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are 70400 (200 \u00d7 176 \u00d7 2) predictions in each frame.  scade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Point  official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, dation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> pub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> estimate 3D object information by calculating the similarity between 3D objects and CAD models. <ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b18\">[19]</ref> explore usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sor T. Note that for the first three convolution layers, after each convolution layer applied, ReLU <ref type=\"bibr\" target=\"#b26\">[27]</ref> is used. Since we have saved the indices of these non-empt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  parameters needed to specify 3D oriented bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t  is still relatively poor. Methods vary by how they encode and learn features from raw point cloud. <ref type=\"bibr\" target=\"#b4\">[5]</ref> uses voxels to encode the raw point cloud, and 3D CNNs (Conv d bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]</ref>, since raw LiDAR point cloud has very sparse data structure,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t etworks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]< ted examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are 70400 (200 \u00d7 176 \u00d7 2) predictions in each frame.  scade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Point  official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, dation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> pub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eatures. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t etworks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]< ted examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are 70400 (200 \u00d7 176 \u00d7 2) predictions in each frame.  scade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Point  official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, dation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> pub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erate 3D proposals in a bottomup manner and then refines these proposals in a second stage. PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> leverages the advantages of both 3D voxel CNN and PointNet f type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref>. While not the top performers within the KITTI leaderboard as, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cascade R-CNN, as CLOCs PVCas. All the other combinati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> are hampered by typically lower input data resolution than v data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> uses PointNets <ref type=\"bibr\" target=\"#b6\">[7]</ref> in an  The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RC is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> publish their training configurations for class pedestrian a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th the camera images together with the LiDAR point clouds to create the ground truth bounding boxes <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This motivates multi-modal sensor fusion as a way to impro ing boxes with confidence scores, as shown in Fig 2 <ref type=\"figure\">.</ref> In the KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref> only rotation in z axis is considered (yaw angle), while ro ate and confident scores. There are multiple ways to encode the 3D bounding boxes, in KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref>, a 7-digit vector containing 3D dimension (height, width an eriments, we focus on the car class since it has the most training and testing samples in the KITTI <ref type=\"bibr\" target=\"#b9\">[10]</ref> dataset.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\" ad><p>Our fusion system is evaluated on the challenging 3D object detection benchmark KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref> which has both LiDAR point clouds and camera images. There  \" target=\"#fig_7\">5</ref> shows some qualitative results of our proposed fusion method on the KITTI <ref type=\"bibr\" target=\"#b9\">[10]</ref> test set. Red bounding boxes represent wrong detections (fa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> estimate 3D object information by calculating the similarity between 3D objects and CAD models. <ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b18\">[19]</ref> explore usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and typically  etup for self-driving cars. Frustum PointNet <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Pointfusion <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Frustum ConvNet <ref type=\"bibr\" target=\"#b24\">[25]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sor T. Note that for the first three convolution layers, after each convolution layer applied, ReLU <ref type=\"bibr\" target=\"#b26\">[27]</ref> is used. Since we have saved the indices of these non-empt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sor T. Note that for the first three convolution layers, after each convolution layer applied, ReLU <ref type=\"bibr\" target=\"#b26\">[27]</ref> is used. Since we have saved the indices of these non-empt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t etworks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]< ted examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are 70400 (200 \u00d7 176 \u00d7 2) predictions in each frame.  scade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Point  official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, dation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> pub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ween 2D and 3D bounding boxes to recover 3D information. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> estimate 3D object information by calculating the similari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g for object avoidance and navigation. Compared to 2D object detection, which has been well-studied <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> are hampered by typ etection; it enables inference at 62 Hz; Compared with one-stage methods discussed above, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Fast PointRCNN <ref type=\"bibr\" target=\"#b19\">[20]</ref> an type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref>. While ef> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, written as CLOCs SecCas, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and typically  etup for self-driving cars. Frustum PointNet <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Pointfusion <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Frustum ConvNet <ref type=\"bibr\" target=\"#b24\">[25]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  CNN and PointNet-based set abstraction to learn more discriminative features. Besides, Part-A 2 in <ref type=\"bibr\" target=\"#b22\">[23]</ref> explores predicting intra-object part locations (lower lef. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t etworks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]< ted examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are 70400 (200 \u00d7 176 \u00d7 2) predictions in each frame.  scade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Point  official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, dation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> pub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. 3D Detection Using 2D Images</head><p>Mousavian et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> leverage the geometric constraints between 2D and 3D bound. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g for object avoidance and navigation. Compared to 2D object detection, which has been well-studied <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th the camera images together with the LiDAR point clouds to create the ground truth bounding boxes <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This motivates multi-modal sensor fusion as a way to impro ing boxes with confidence scores, as shown in Fig 2 <ref type=\"figure\">.</ref> In the KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref> only rotation in z axis is considered (yaw angle), while ro ate and confident scores. There are multiple ways to encode the 3D bounding boxes, in KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref>, a 7-digit vector containing 3D dimension (height, width an eriments, we focus on the car class since it has the most training and testing samples in the KITTI <ref type=\"bibr\" target=\"#b9\">[10]</ref> dataset.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\" ad><p>Our fusion system is evaluated on the challenging 3D object detection benchmark KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref> which has both LiDAR point clouds and camera images. There  \" target=\"#fig_7\">5</ref> shows some qualitative results of our proposed fusion method on the KITTI <ref type=\"bibr\" target=\"#b9\">[10]</ref> test set. Red bounding boxes represent wrong detections (fa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g for object avoidance and navigation. Compared to 2D object detection, which has been well-studied <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mising application area for deep learning <ref type=\"bibr\" target=\"#b11\">(Graves et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Ingraham et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Pereira et aphQA <ref type=\"bibr\" target=\"#b1\">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref type=\"bibr\" target=\"#b16\">(Ingraham et al., 2019)</ref> on CPD, and ProteinSolver <ref type=\"bi bstantial improvement both in terms of perplexity and sequence recovery over Structured Transformer <ref type=\"bibr\" target=\"#b16\">(Ingraham et al., 2019)</ref>, a GNN method which was trained using t ve model over the space of protein sequences conditioned on the given backbone structure. Following <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref>, we frame this as an autoregressive task  the same training and validation sets (Table <ref type=\"table\" target=\"#tab_3\">3</ref>). Following <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref>, we report evaluation on short (100 or f >Protein design As described in the main text, we use the CATH 4.2 dataset and splits as curated by <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref> and the TS50 test set as curated by <ref et should bear minimal similarity to the training structures. We use the CATH 4.2 dataset curated by<ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref> in which all available structures with 4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et, a recent 3D-aware GNN architecture which achieves state-of-the-art on many small-molecule tasks <ref type=\"bibr\" target=\"#b18\">(Klicpera et al., 2019)</ref>, on CASP 11-12. DimeNet differs from GV  xmlns=\"http://www.tei-c.org/ns/1.0\"><head>E ADDITIONAL RESULTS</head><p>E.1 DIMENET ON MQA DimeNet <ref type=\"bibr\" target=\"#b18\">(Klicpera et al., 2019</ref>) is a recent GNN architecture designed t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and orientation of the amino acids in space, which govern the dynamics and function of the molecule <ref type=\"bibr\" target=\"#b3\">(Berg et al., 2002)</ref>. On the other hand, proteins also possess re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ructure and contact-based features<ref type=\"bibr\" target=\"#b5\">(Cheng et al., 2019)</ref>. FaeNNz 7<ref type=\"bibr\" target=\"#b28\">(Studer et al., 2020)</ref>, ProQ3D<ref type=\"bibr\" target=\"#b30\">(Uz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target. GDT-TS is a scalar measure of how similar two protein backbones are after global alignment <ref type=\"bibr\" target=\"#b34\">(Zemla et al., 2001)</ref>.</p><p>In addition to accurately predictin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b0\">(Anand et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Shroff et al., 2019)</ref> exemplify the power of this approach.</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2019)</ref> and a number of CPD methods <ref type=\"bibr\" target=\"#b0\">(Anand et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Shroff et al. y % GVP-GNN 44.9 DenseCPD <ref type=\"bibr\" target=\"#b25\">(Qi &amp; Zhang, 2020)</ref> 50.7 ProDCoNN <ref type=\"bibr\" target=\"#b35\">(Zhang et al., 2019)</ref> 40.7 SBROF <ref type=\"bibr\" target=\"#b4\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b6\">(Cohen &amp; Shashua, 2017)</ref> and attention in natural language processing <ref type=\"bibr\" target=\"#b31\">(Vaswani et al., 2017)</ref>. What are the relevant considerations in unctions. <ref type=\"foot\" target=\"#foot_3\">4</ref>\u2022 A sinusoidal encoding of i \u2212 j as described in <ref type=\"bibr\" target=\"#b31\">Vaswani et al. (2017)</ref>, representing distance along the backbone. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ifferent domains require specialized normalization methods. In computer vision, batch normalization <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a standard component. While in natural language process h set of feature values the normalization is applied to. For example, in computer vision, BatchNorm <ref type=\"bibr\" target=\"#b15\">[16]</ref> is the de facto method that normalizes the feature values  ring testing, the estimated dataset-level statistics are used instead of the batch-level statistics <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>In GNNs, for each feature dimension, the BatchNorm  -invariant\" property <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In comparison, BatchNorm in the lower branch suffers from  e-invariant\" property<ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In comparison, BatchNorm in the lower branch suffers from  ed to improve the training process in different applications <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar h of the parameters; <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> show that the norma rom BatchNorm layers under different settings of batch sizes <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b31\">32</ref>, 64], as in Figure <  matrix in layer k. We apply the normalization after the linear transformation as in previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta hich try to provide accurate estimations for the mean and standard deviation over the whole dataset <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: luding their representation power <ref type=\"bibr\" target=\"#b36\">[37]</ref>, generalization ability <ref type=\"bibr\" target=\"#b38\">[39]</ref>, and infinite-width asymptotic behavior <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gregation strategy <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as been a surge of interest in Graph Neural Networks (GNNs) for learning with graph-structured data <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta arn the representations of nodes and graphs. Modern GNNs follow a neighborhood aggregation strategy <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nel model <ref type=\"bibr\" target=\"#b29\">[30]</ref>, diffusion-convolutional neural networks (DCNN) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Deep Graph CNN (DGCNN) <ref type=\"bibr\">[42]</ref> and Anon. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the whole dataset <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. During testing, the estimated dataset-level statistics are tings of batch sizes <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b31\">32</ref>, 64], as in Figure <ref type=\"figure\">9</ref>. We can see th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: luding their representation power <ref type=\"bibr\" target=\"#b36\">[37]</ref>, generalization ability <ref type=\"bibr\" target=\"#b38\">[39]</ref>, and infinite-width asymptotic behavior <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fferent applications <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for the mean and standard deviation over the whole dataset <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. During testing, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the whole dataset <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. During testing, the estimated dataset-level statistics are tings of batch sizes <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b31\">32</ref>, 64], as in Figure <ref type=\"figure\">9</ref>. We can see th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al networks has concentrated on node classification task <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib ake a specific node (e.g., a person in social networks) misclassified. In this scenario, Dai et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> study the adversarial attack on graph structure data and p olutional Network (SGC) <ref type=\"bibr\" target=\"#b16\">[17]</ref> and gradient-based attack methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we propose a n cient way to generate destructive adversarial examples. Focusing on the targeted attack, Dai et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose GradArgmax, which extracts gradients of the surrog  Network (GCN)</head><p>Since a number of existing works <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bib f-the-art targeted attack methods: Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref> and GradArgmax <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  ly nodes belonging to the same class/different classes will be disconnected/connected. \u2022 GradArgmax <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Since the attack budget \u2206 is defined as the degrees of ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: of the current work on attacking graph neural networks has concentrated on node classification task <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bib luence attack is limited to a few attacker nodes (usually the neighboring nodes of the target node) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Figure <ref type=\"figure\">1</ref> demonstrates a toy exam s based on gradients of a surrogate model so as to fool the classifiers. In addition, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> propose Nettack, which is capable of perturbing the Fig. < ween nodes with high correlation and connecting edges with low correlation. Moreover, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> study both poisoning attacks and evasion attacks (a.k.a te ies of graphs and ensure the unnoticeability in most cases.</p><p>To bridge this gap, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> enforce the perturbations to ensure its unnoticeability by \"><head n=\"3.2.1\">Vanilla Graph Convolution Network (GCN)</head><p>Since a number of existing works <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bib =\"#b22\">[23]</ref> to modify the graph structure or node features, respectively. Following the work <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we assume that attackers have prior knowledge about the g on task, here we briefly introduce other proposed state-of-the-art targeted attack methods: Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref> and GradArgmax <ref type=\"bibr\" target=\"#b12\">[13]</ref>.<  model SGC, A \u2286 V is the set of attacker nodes and the perturbations are constrained to these nodes <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>In particular, we set A = {t} for direct attack and  q is given. However, there is no solution for estimating q exactly yet. To this end, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> derive an efficient way to check for violations of the deg ula_37\">\u039b (q, q ; G, G ) &lt; \u03c4 \u2248 0.004 ,</formula><p>where \u039b is a discriminate function defined in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, G denotes the original graph and G the perturbed one.</p> er posts and comments on content in different topical communities. We follow the setting of Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref> and only consider the largest connected component of the g ame surrogate model SGC (if necessary), and share the same weights \u03b8. Follow the setting of Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the attack budget \u2206 is set to the degrees of target node   instantiation of the adjacency matrix and computes the gradients of all N 2 edges.</p><p>\u2022 Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Nettack is the strongest baseline that can modify the gra nt attack methods. Nettack, a strong baseline, also yields a significant performance as reported in <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Most remarkably, even in attacking other robust graph neu /www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">. We follow the attack settings of Nettack<ref type=\"bibr\" target=\"#b11\">[12]</ref> which modifies graph structure and node features by flippi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eep learning models have been proved vulnerable against perturbations. Specifically, Szegedy et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Goodfellow et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref nd security of deep learning models, there has been a surge of interests in the adversarial attacks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t ost commonly used method. Gradients have been successfully used to perform attacks in other domains <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Since most of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ielded several results with respect to certain important properties, including \"small-world effect\" <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref> and degree dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>R ECENTLY, with the enormous advancement of deep learning, many domains like speech recognition <ref type=\"bibr\" target=\"#b0\">[1]</ref> and visual object recognition <ref type=\"bibr\" target=\"#b1\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  underlying the challenge of poisoning attacks (a.k.a, training-time attacks). Similarly, Xu et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> propose PGD structure attack that conducts gradient attack \">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref> use vanilla GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ies focus primarily on the intrinsic properties of graph <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, but measuring the impact of graph adversarial attacks is  >. Unlike previous works, Newman et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Foster et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> focus on another important network feature, i.e., assortat ure the attack impacts, degree assortativity coefficient <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, it measures the tendency of nodes connected to each other. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gradients, attackers prefer to explore other heuristic algorithms to conduct attacks. Waniek et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> propose \"Disconnect Internally, Connect Externally\" (DICE). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lip one edge e = (u, v) \u2208 E (subi) with largest magnitude of gradient that fullfils the constraints <ref type=\"bibr\" target=\"#b34\">[35]</ref>: the edges connected to it should be considered as well.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ain important properties, including \"small-world effect\" <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref> and degree distributions <ref type=\"bibr\" target=\"#b28\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ain important properties, including \"small-world effect\" <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref> and degree distributions <ref type=\"bibr\" target=\"#b28\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mains like speech recognition <ref type=\"bibr\" target=\"#b0\">[1]</ref> and visual object recognition <ref type=\"bibr\" target=\"#b1\">[2]</ref>, have achieved a dramatic improvement out of the state-of-th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sets. We evaluate the performance of our method on four well-known datasets: Citeseer, Cora, Pubmed <ref type=\"bibr\" target=\"#b38\">[39]</ref> and Reddit <ref type=\"bibr\" target=\"#b39\">[40]</ref>. The . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  matrix A (sub) is computed for each edge e = (u, v) \u2208 E s .</p><p>However, as stated by Guo et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>, most of the modern neural networks are poorly calibrated,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aw distribution <ref type=\"bibr\" target=\"#b36\">[37]</ref>, i.e., <ref type=\"bibr\">3.</ref> Refer to <ref type=\"bibr\" target=\"#b35\">[36]</ref>, the time complexity of GCN is O(|E|d 2 ), and the computa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ies focus primarily on the intrinsic properties of graph <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, but measuring the impact of graph adversarial attacks is  >. Unlike previous works, Newman et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Foster et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> focus on another important network feature, i.e., assortat ure the attack impacts, degree assortativity coefficient <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, it measures the tendency of nodes connected to each other. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Undoubtedly, graph plays a crucial role in many high impact. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons in the world <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Therefore, the importance of the robustness of deep learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3\">[4]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. The obtained results suggest that deep learning models ar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: crucial role in many high impact applications in the world <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Therefore, the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep learning models have been proved vulnerable against perturbations. Specifically, Szegedy et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Goodfellow et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref nd security of deep learning models, there has been a surge of interests in the adversarial attacks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t ost commonly used method. Gradients have been successfully used to perform attacks in other domains <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Since most of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ur well-known datasets: Citeseer, Cora, Pubmed <ref type=\"bibr\" target=\"#b38\">[39]</ref> and Reddit <ref type=\"bibr\" target=\"#b39\">[40]</ref>. The first three are commonly citation networks on node cl <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b39\">[40]</ref>, ClusterGCN <ref type=\"bibr\" target=\"#b41\">[42]</ref>. For -attention mechanism to specifying different weights to different neighbor nodes.</p><p>\u2022 GraphSAGE <ref type=\"bibr\" target=\"#b39\">[40]</ref>. GraphSAGE is a general inductive framework, which uniform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y or with only very few labeled examples <ref type=\"bibr\" target=\"#b26\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze, 2020a)</ref>.</p><p>Very recently, <ref type=\"bib mited to a few hundred tokens.</p><p>An alternative to priming is pattern-exploiting training (PET) <ref type=\"bibr\" target=\"#b30\">(Schick and Sch\u00fctze, 2020a)</ref>, which combines the idea of reformu  are understood well by LMs is difficult <ref type=\"bibr\" target=\"#b12\">(Jiang et al., 2019)</ref>, <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref> propose PET, a method that uses know =\"#b4\">(Dagan et al., 2006)</ref> are textual entailment tasks like MNLI, so we use PVPs similar to <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>. For a premise p and hypothesis h, w ts. 5 We run PET on the FewGLUE training sets for all SuperGLUE tasks using the exact same setup as <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>. For COPA, WSC and ReCoRD, we use ou ref>). Given 32 examples, PET clearly outperforms both baselines, which is in line with findings by <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>.</p><p>We next compare PET directly  t segments and mark p in s with asterisks.</p><p>MultiRC Deviating from the hyperparameters used by <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2019)</ref>, we use a maximum sequence length of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , despite this having no effect on the model's prediction. We instead opt for multiclass hinge loss <ref type=\"bibr\" target=\"#b37\">(Weston and Watkins, 1999;</ref><ref type=\"bibr\" target=\"#b8\">Dogan e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation <ref type=\"bibr\" target=\"#b24\">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type=\"bibr\" target=\"#b5\">(Davison et al., 2019)</ref> and argumentative relation classification. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to assign probabilities to sequences of text; this is similar to using them in a generative fashion <ref type=\"bibr\" target=\"#b35\">(Wang and Cho, 2019)</ref> and has previously been investigated by <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze, 2020a)</ref>.</p><p>Very recently, <ref type=\"bibr\" target=\"#b1\">Brown et al. (2020)</ref> introduced GPT-3, a pretrained LM with an en ch uses gradient-based optimization, <ref type=\"bibr\" target=\"#b26\">Radford et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b1\">Brown et al. (2020)</ref> investigate priming, where examples are give  al., 2020)</ref>, we select only positive examples for WSC; for both MultiRC and ReCoRD, we follow <ref type=\"bibr\" target=\"#b1\">Brown et al. (2020)</ref> and select a total of 32 questions.</p><p>We perGLUE tasks, the WSC formulation of <ref type=\"bibr\" target=\"#b27\">Raffel et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b1\">Brown et al. (2020)</ref> requires free-form completion, meaning that  ixed random seed. Following previous work <ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al., 2020)</ref>, we select only positive examples for WSC; f er p refers to n. We follow previous work <ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al., 2020)</ref> and treat WSC as a generative task. We highl ly, PET with GPT-2 performs much worse than with both other models. As anticipated by previous work <ref type=\"bibr\" target=\"#b1\">(Brown et al., 2020)</ref>, a key reason for this drop in performance . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: probing the knowledge contained within LMs <ref type=\"bibr\" target=\"#b33\">(Trinh and Le, 2018;</ref><ref type=\"bibr\" target=\"#b22\">Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Talmor et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation <ref type=\"bibr\" target=\"#b24\">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type=\"bibr\" target=\"#b5\">(Davison et al., 2019)</ref> and argumentative relation classification. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed on the Transformers library <ref type=\"bibr\" target=\"#b39\">(Wolf et al., 2019)</ref> and PyTorch <ref type=\"bibr\" target=\"#b21\">(Paszke et al., 2017)</ref>. Unless explicitly stated differently, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  by randomly selecting 32 examples for each task using a fixed random seed. Following previous work <ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al. d pronoun p and noun n, and the task is to determine whether p refers to n. We follow previous work <ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al.  boundary between two text segments.</p><p>WSC Unlike other SuperGLUE tasks, the WSC formulation of <ref type=\"bibr\" target=\"#b27\">Raffel et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b1\">Brown et ss multiple evaluations and perform greedy decoding as described in Section 3.</p><p>We then follow <ref type=\"bibr\" target=\"#b27\">Raffel et al. (2019)</ref> to map the output produced by the LM to a  g on FewGLUE. State-of-the-art results when using the regular, full size training sets for all tasks<ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2019)</ref> are shown in italics.</figDesc><table><ro generation with bidirectional context (e.g.,<ref type=\"bibr\" target=\"#b17\">Lewis et al., 2020;</ref><ref type=\"bibr\" target=\"#b27\">Raffel et al., 2019)</ref>, we stick with MLMs as they are more light. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation <ref type=\"bibr\" target=\"#b24\">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type=\"bibr\" target=\"#b5\">(Davison et al., 2019)</ref> and argumentative relation classification. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tailment to yes, disagreement to no and neutral to maybe.</p><p>Given a premise p, the task in COPA <ref type=\"bibr\" target=\"#b28\">(Roemmele et al., 2011)</ref> is to determine the cause or effect of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p><p>Secondly, note imum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios<ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.Secondly, note that O n machine learning <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  two well-accepted propositions-deep neural networks learn meaningful patterns before fitting noise <ref type=\"bibr\" target=\"#b2\">[3]</ref> and minimum entropy regularisation principle [10]-we propose  meaningful patterns before fitting noise, even when severe label noise exists in human annotations <ref type=\"bibr\" target=\"#b2\">[3]</ref>. (2) As a learner attains confident knowledge as time progre e meaningful patterns before fitting noise, even when severe label noise exists in human annotations<ref type=\"bibr\" target=\"#b2\">[3]</ref>. (2) As a learner attains confident knowledge as time progre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and exploit their '. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e overconfident predictions for regularising deep neural networks. It includes label smoothing (LS) <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and confidence pen e, our work offers a defence of entropy minimisation against the recent confidence penalty practice <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta egative log-likelihood, and q serves as the probability mass function.</p><p>Label smoothing. In LS <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, we soften one-hot romotes entropy minimisation, which is in marked contrast to recent practices of confidence penalty <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta xml:id=\"formula_0\">LS CP 1/3 1/3 1/3 1 0 0 1/2 1/3 1/6 1 0 0 0 0 1 1</formula><p>(a) OR includes LS <ref type=\"bibr\" target=\"#b41\">[42]</ref> and CP <ref type=\"bibr\" target=\"#b32\">[33]</ref>. LS softe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is to annotate unlabelled samples or correct noisy labels.</p><p>LC and knowledge distillation (KD) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Mathematically, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion matrix, which defines the distribution of noise labels <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" targe gnostic.</p><p>Baselines. For loss correction and estimating the noisetransition matrix, S-adaption <ref type=\"bibr\" target=\"#b7\">[8]</ref> uses an extra softmax layer, while Masking <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is to annotate unlabelled samples or correct noisy labels.</p><p>LC and knowledge distillation (KD) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Mathematically, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atus or reward it?</p><p>Entropy minimisation is the most widely used principle in machine learning <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion is the most widely used principle in machine learning <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We do not consider DisturbLabel<ref type=\"bibr\" target=\"#b48\">[49]</ref>, which flips labels randomly and is counter-intuitive. It . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. A noise-transition target=\"#b45\">[46]</ref>. Forward is a loss correction approach that uses a noise-transition matrix <ref type=\"bibr\" target=\"#b31\">[32]</ref>. D2L monitors the subspace dimensionality change at traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is to annotate unlabelled samples or correct noisy labels.</p><p>LC and knowledge distillation (KD) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Mathematically, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and confidence penalty (CP) <ref type=\"bibr\" target=\"#b32\">[33]</ref>; <ref type=\"bibr\" target=\"#b1\">(2)</ref> Label correction  3 1/6 1 0 0 0 0 1 1</formula><p>(a) OR includes LS <ref type=\"bibr\" target=\"#b41\">[42]</ref> and CP <ref type=\"bibr\" target=\"#b32\">[33]</ref>. LS softens a target by adding a uniform label distributio ) = E qLS (\u2212 log p) = (1 \u2212 \u01eb)H(q, p)+\u01ebH(u, p).<label>(3)</label></formula><p>Confidence penalty. CP <ref type=\"bibr\" target=\"#b32\">[33]</ref> penalises highly confident predictions:</p><formula xml:id e penalty practice <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Finally, we summaris  marked contrast to recent practices of confidence penalty <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>.</p></div><figure xml. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ) Co-training strategies, which train two or more learners <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar s from the methods <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. In standard classification, minimising categorical cross e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e overconfident predictions for regularising deep neural networks. It includes label smoothing (LS) <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and confidence pen e, our work offers a defence of entropy minimisation against the recent confidence penalty practice <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta egative log-likelihood, and q serves as the probability mass function.</p><p>Label smoothing. In LS <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, we soften one-hot romotes entropy minimisation, which is in marked contrast to recent practices of confidence penalty <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta xml:id=\"formula_0\">LS CP 1/3 1/3 1/3 1 0 0 1/2 1/3 1/6 1 0 0 0 0 1 1</formula><p>(a) OR includes LS <ref type=\"bibr\" target=\"#b41\">[42]</ref> and CP <ref type=\"bibr\" target=\"#b32\">[33]</ref>. LS softe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n auxiliary trusted training set to differentiate examples <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. This requires extr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e overconfident predictions for regularising deep neural networks. It includes label smoothing (LS) <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and confidence pen e, our work offers a defence of entropy minimisation against the recent confidence penalty practice <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta egative log-likelihood, and q serves as the probability mass function.</p><p>Label smoothing. In LS <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, we soften one-hot romotes entropy minimisation, which is in marked contrast to recent practices of confidence penalty <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta xml:id=\"formula_0\">LS CP 1/3 1/3 1/3 1 0 0 1/2 1/3 1/6 1 0 0 0 0 1 1</formula><p>(a) OR includes LS <ref type=\"bibr\" target=\"#b41\">[42]</ref> and CP <ref type=\"bibr\" target=\"#b32\">[33]</ref>. LS softe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igher, a smaller B performs better.</p><p>We further discuss their differences on model calibration <ref type=\"bibr\" target=\"#b10\">[11]</ref> in Appendix B, the changes of entropy and \u01eb ProSelfLC duri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Indeed, breakthrough generative learning strategies, such as Generative Adversarial Networks (GANs) <ref type=\"bibr\" target=\"#b2\">3</ref> can learn multidimensional distributions in disparate scientif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  would contain novel and functionally relevant biological properties, we performed a search of CATH <ref type=\"bibr\" target=\"#b32\">33</ref> sequence models corresponding to all known 3D structural pro the sequence clusters at 35% sequence identity (v4.1) were downloaded from CATH database repository <ref type=\"bibr\" target=\"#b32\">33</ref> . To avoid biases in sequence scoring, generated sequences t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are estimated to fold into the defined three-dimensional structures to carry out specific functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b54\">55</ref> . The distance matrix was used with the scikit-learn t-SNE module <ref type=\"bibr\" target=\"#b55\">56</ref> with default settings (early exaggeration 12, learning rate . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igned with the training and validation datasets using BLAST <ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b50\">51</ref> . Throughout the training, BLOSUM45, E -value and identity s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cules that are used to generate the recombination libraries <ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39</ref> , current methods are fundamentally limited in the sequence . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b22\">23</ref> , i.e. the model is trained using readily available data, to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imited by the amount of sequence variations it can process <ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref> and, instead of d f depending on a blind search, is based on an inference-based process -it infers protein properties <ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20</ref> and function <ref ty sting machine learning models in biology are discriminative <ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: line in protein fitness with the number of random mutations <ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b36\">37</ref> or the number of parent molecules that are used to generate . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rn multidimensional distributions in disparate scientific domains to generate photorealistic images <ref type=\"bibr\" target=\"#b23\">24</ref> , hand-written text <ref type=\"bibr\" target=\"#b24\">25</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ultiple Reaction Monitoring (MRM) parameters optimized for Malic acid based on published parameters <ref type=\"bibr\" target=\"#b56\">57</ref> . Electrospray ionization parameters were optimized for 0.8m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eam generates about 1.2TB intermediate data to count 4-motif on the MiCo graph with 1 million edges <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>Recently, specialized systems have been developed f thms in these specialized pattern matching systems can be described with nested loops, and AutoMine <ref type=\"bibr\" target=\"#b17\">[18]</ref> and GraphZero <ref type=\"bibr\" target=\"#b11\">[12]</ref> re ate-of-the-art singlemachine pattern matching systems. GraphZero is an upgraded version of AutoMine <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and it outperforms AutoMine by up to 40\u00d7. Fractal is a JV ef>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Automine <ref type=\"bibr\" target=\"#b17\">[18]</ref> is built upon a set-based representation and uses compilat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5M US Patents LiveJournal <ref type=\"bibr\" target=\"#b12\">[13]</ref> 4.0M 34.7M Social network Orkut <ref type=\"bibr\" target=\"#b32\">[33]</ref> 3.1M 117.2M Social network Twitter <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" above the edge between A1 and B5 denotes id(A) &gt; id(B)). (b) is evaluated on the Patents graph <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> rent combinations shown in Figure <ref type=\"figure\">2</ref>(b) with GraphZero on the Patents graph <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Experimental results show that the best combination of sc  Wiki Editor Voting MiCo <ref type=\"bibr\" target=\"#b31\">[32]</ref> 96.6K 1.1M Co-authorship Patents <ref type=\"bibr\" target=\"#b26\">[27]</ref> 3.8M 16.5M US Patents LiveJournal <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve proposed several general-purpose graph mining systems <ref type=\"bibr\" target=\"#b13\">[14]</ref>- <ref type=\"bibr\" target=\"#b16\">[17]</ref>, such as Arabesque and RStream, which provide high-level a API for implementing various graph mining algorithms and an efficient runtime engine.</p><p>G-Miner <ref type=\"bibr\" target=\"#b16\">[17]</ref> models the processing of a graph mining job as an independ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rms several JVM-based specialized algorithms (MRSUB <ref type=\"bibr\" target=\"#b34\">[35]</ref>, SEED <ref type=\"bibr\" target=\"#b35\">[36]</ref> and QKCount <ref type=\"bibr\" target=\"#b36\">[37]</ref>) and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and frequent subgraph mining (FSM) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. ASAP <ref type rerun it without restrictions. The set of restrictions is cor-rect if ans</cell></row></table><note><ref type=\"bibr\" target=\"#b23\">24</ref> Function no_conflict(perm, res set): 25 g \u2190 an empty directe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b22\">[23]</ref>, and frequent subgraph mining (FSM) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. ASAP <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a distr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b34\">[35]</ref>, SEED <ref type=\"bibr\" target=\"#b35\">[36]</ref> and QKCount <ref type=\"bibr\" target=\"#b36\">[37]</ref>) and general-purpose systems (Arabesque <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blems have been extensively studied, and many efficient graph processing systems have been proposed <ref type=\"bibr\" target=\"#b3\">[4]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>. On the other han -to-one correspondences <ref type=\"bibr\">([4,5,6,7,3]</ref>, <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target= \" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, <ref type=\"bibr\" targe target=\"#b3\">4]</ref>, <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target= r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" targ , 2), (2, 3)and(4, 5) into g. After partitioning g, there are three connected components: [1,2,3] , <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> and <ref type=\"bibr\" t ), (2, 3), and (4, 5) into g. After partitioning g, there are three connected components: [1,2,3] , <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, and <ref type=\"bibr\"   numbers in a square bracket denote a one-to-one correspondence (a bijective function). For example,<ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ODUCTION</head><p>Graph data and algorithms are widely used in many fields, such as social networks <ref type=\"bibr\" target=\"#b0\">[1]</ref>, bioinformatics <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and there are n! possible relative magnitudes of n vertices in an embedding (e.g., when n = 5, they are <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target  target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target  target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with theoretical support. Previous works also show that ReLU MLPs have finitely many linear regions <ref type=\"bibr\" target=\"#b3\">[Arora et al., 2018</ref><ref type=\"bibr\" target=\"#b31\">, Hein et al., ny direction outside the training distribution.</p><p>Comparison with previous results. Previously, <ref type=\"bibr\" target=\"#b3\">Arora et al. [2018]</ref> show that ReLU MLPs have finitely many linea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a are different <ref type=\"bibr\" target=\"#b70\">[Zhao et al., 2019]</ref>. Distributional robustness <ref type=\"bibr\" target=\"#b26\">[Goh and Sim, 2010</ref><ref type=\"bibr\" target=\"#b55\">, Sinha et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b52\">, Savarese et al., 2019</ref><ref type=\"bibr\" target=\"#b16\">, Chizat and Bach, 2018</ref><ref type=\"bibr\" target=\"#b41\">, Li et al., 2019]</ref>. Closest to our work are results showing tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  capacity <ref type=\"bibr\" target=\"#b67\">[Zhang et al., 2017]</ref> and are universal approximators <ref type=\"bibr\" target=\"#b18\">[Cybenko, 1989</ref><ref type=\"bibr\" target=\"#b32\">, Hornik et al., 1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">, Mansour et al., 2009]</ref>, often by adversarial learning using samples from the target domain <ref type=\"bibr\" target=\"#b13\">[Blitzer et al., 2008</ref><ref type=\"bibr\" target=\"#b24\">, Ganin et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons <ref type=\"bibr\" target=\"#b28\">[Haley and</ref><ref type=\"bibr\">Soloway, 1992, Barnard and</ref><ref type=\"bibr\" target=\"#b7\">Wessels, 1992]</ref>. However, recent works show Graph Neural Networks ell <ref type=\"bibr\" target=\"#b28\">[Haley and</ref><ref type=\"bibr\">Soloway, 1992, Barnard and</ref><ref type=\"bibr\" target=\"#b7\">Wessels, 1992]</ref>. We instead show a general pattern of how ReLU ML. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\">,b, Allen-Zhu et al., 2019a</ref><ref type=\"bibr\" target=\"#b22\">, Du et al., 2019c</ref><ref type=\"bibr\" target=\"#b39\">,a, Li and Liang, 2018</ref><ref type=\"bibr\" target=\"#b34\">, Jacot et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.8\">Comparison Against CESP</head><p>Palacharla et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose the complexity-effective superscalar processor (CE oint out the most closely related work.</p><p>Complexity-Effective Architectures. Palacharla et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose the complexity-effective superscalar processors (C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: memory-hierarchy parallelism (MHP). <ref type=\"foot\" target=\"#foot_0\">1</ref> Load Slice Core (LSC) <ref type=\"bibr\" target=\"#b4\">[5]</ref> was the first work to propose an sOoO core; Freeway <ref typ TIVATION</head><p>In this section, we briefly cover the background on the two prior sOoO cores -LSC <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> -and we FSC.</p><p>Restricted Out-of-Order Microarchitectures. We extensively discussed the Load Slice Core <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> through. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ssociative cache with 2/2 read/write ports. We estimate power consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> a div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.7\">Power Consumption</head><p>We use McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> to calculate InO and OoO core power consumption. power con. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nates all out-of-order structures and is therefore more area-and power-efficient. Long-term parking <ref type=\"bibr\" target=\"#b15\">[16]</ref> saves power in an OoO core by allocating back-end resource. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) wor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ower consumption numbers provided by McPAT.</p><p>We create representative 1B-instruction SimPoints <ref type=\"bibr\" target=\"#b16\">[17]</ref> for the SPEC CPU2017 benchmarks. We sort the benchmarks by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roving performance. More recently, Clairvoyance <ref type=\"bibr\" target=\"#b19\">[20]</ref> and SWOOP <ref type=\"bibr\" target=\"#b20\">[21]</ref> exploit the decoupled nature of access and execute phases . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tch stalls when an independent instruction cannot be steered to an empty queue. Salverda and Zilles <ref type=\"bibr\" target=\"#b13\">[14]</ref> evaluate CESP in the context of a realistic baseline and p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roving performance. More recently, Clairvoyance <ref type=\"bibr\" target=\"#b19\">[20]</ref> and SWOOP <ref type=\"bibr\" target=\"#b20\">[21]</ref> exploit the decoupled nature of access and execute phases . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref> and OUTRIDER <ref type=\"bibr\" target=\"#b5\">[6]</ref> also exploit critical instruction slices <ref type=\"bibr\" target=\"#b23\">[24]</ref> for improving performance. More recently, Clairvoyance <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lative-slice execution <ref type=\"bibr\" target=\"#b22\">[23]</ref>, flea-flicker multipass pipelining <ref type=\"bibr\" target=\"#b2\">[3]</ref>, braid processing <ref type=\"bibr\" target=\"#b21\">[22]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ower consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> assuming a 22 nm technology node. Area and per-access powe er and floating-point registers. The MSHR is extended to support 8 outstanding misses. We use CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref> to estimate chip area. CACTI accounts for the area of circ  power consumption. power consumed by the additional FSC hardware structures is modeled using CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Table <ref type=\"table\" target=\"#tab_3\">3</ref> reports p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, flea-flicker multipass pipelining <ref type=\"bibr\" target=\"#b2\">[3]</ref>, braid processing <ref type=\"bibr\" target=\"#b21\">[22]</ref> and OUTRIDER <ref type=\"bibr\" target=\"#b5\">[6]</ref> also . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ssociative cache with 2/2 read/write ports. We estimate power consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> a div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.7\">Power Consumption</head><p>We use McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> to calculate InO and OoO core power consumption. power con. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecute phases of a program through coordinated queues. Proposals such as speculative-slice execution <ref type=\"bibr\" target=\"#b22\">[23]</ref>, flea-flicker multipass pipelining <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tch stalls when an independent instruction cannot be steered to an empty queue. Salverda and Zilles <ref type=\"bibr\" target=\"#b13\">[14]</ref> evaluate CESP in the context of a realistic baseline and p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, flea-flicker multipass pipelining <ref type=\"bibr\" target=\"#b2\">[3]</ref>, braid processing <ref type=\"bibr\" target=\"#b21\">[22]</ref> and OUTRIDER <ref type=\"bibr\" target=\"#b5\">[6]</ref> also . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> throughout the paper. Shioya et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> propose the front-end execution architecture which execute. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lative-slice execution <ref type=\"bibr\" target=\"#b22\">[23]</ref>, flea-flicker multipass pipelining <ref type=\"bibr\" target=\"#b2\">[3]</ref>, braid processing <ref type=\"bibr\" target=\"#b21\">[22]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ower consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> assuming a 22 nm technology node. Area and per-access powe er and floating-point registers. The MSHR is extended to support 8 outstanding misses. We use CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref> to estimate chip area. CACTI accounts for the area of circ  power consumption. power consumed by the additional FSC hardware structures is modeled using CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Table <ref type=\"table\" target=\"#tab_3\">3</ref> reports p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es <ref type=\"bibr\" target=\"#b23\">[24]</ref> for improving performance. More recently, Clairvoyance <ref type=\"bibr\" target=\"#b19\">[20]</ref> and SWOOP <ref type=\"bibr\" target=\"#b20\">[21]</ref> exploi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ower consumption numbers provided by McPAT.</p><p>We create representative 1B-instruction SimPoints <ref type=\"bibr\" target=\"#b16\">[17]</ref> for the SPEC CPU2017 benchmarks. We sort the benchmarks by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing significantly fewer training data comparing to the few previous works which address heterophily <ref type=\"bibr\" target=\"#b22\">(Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b38\">Zhu et al. 2020 nificantly smaller fraction of training samples compared to previous works that address heterophily <ref type=\"bibr\" target=\"#b22\">(Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b38\">Zhu et al. 2020 pe=\"bibr\" target=\"#b20\">Namata et al. 2012)</ref>. We use the features and class labels provided by <ref type=\"bibr\" target=\"#b22\">Pei et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n ch benchmark with an identity matrix I. We use the training, validation and test splits provided by <ref type=\"bibr\" target=\"#b22\">Pei et al. (2020)</ref>.</p><p>Heterophily. We report results on grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"bibr\" target=\"#b1\">(Ahmed et al. 2018;</ref><ref type=\"bibr\" target=\"#b23\">Rossi et al. 2020;</ref><ref type=\"bibr\" target=\"#b32\">Wu et al. 2019)</ref>. However, there are also many instances in the  worse for graphs without strong homophily. Theorem 1. The forward pass formulation of a 1-layer SGC <ref type=\"bibr\" target=\"#b32\">(Wu et al. 2019)</ref>, a simplified version of GCN without the non-l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2</ref>) can be solved with iterative methods (J. <ref type=\"bibr\" target=\"#b10\">Neville 2000;</ref><ref type=\"bibr\" target=\"#b17\">Lu and Getoor 2003)</ref>, graph-based regularization and probabilist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d learning (SSL) or collective classification <ref type=\"bibr\" target=\"#b27\">(Sen et al. 2008;</ref><ref type=\"bibr\" target=\"#b18\">McDowell, Gupta, and Aha 2007;</ref><ref type=\"bibr\" target=\"#b24\">Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nable parameters, and R (0) = X. We call our method with MLP-based estimator CPGNN-MLP. \u2022 GCN-Cheby <ref type=\"bibr\" target=\"#b3\">(Defferrard, Bresson, and Vandergheynst 2016)</ref>. We instantiate th pervised node classification problems thanks to their ability to learn through end-to-end training. <ref type=\"bibr\" target=\"#b3\">Defferrard, Bresson, and Vandergheynst (2016)</ref> proposed an early . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d learning (SSL) or collective classification <ref type=\"bibr\" target=\"#b27\">(Sen et al. 2008;</ref><ref type=\"bibr\" target=\"#b18\">McDowell, Gupta, and Aha 2007;</ref><ref type=\"bibr\" target=\"#b24\">Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ompatibility matrix H.</p><p>To propagate the belief vectors through linear formulations, following <ref type=\"bibr\" target=\"#b7\">Gatterbauer et al. (2015)</ref>, we first center B p with</p><formula  tions received from its neighborhood. <ref type=\"bibr\" target=\"#b13\">Koutra et al. (2011)</ref> and <ref type=\"bibr\" target=\"#b7\">Gatterbauer et al. (2015)</ref> have proposed linearized versions whic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d to belief propagation (BP) <ref type=\"bibr\" target=\"#b36\">(Yedidia, Freeman, and Weiss 2003;</ref><ref type=\"bibr\" target=\"#b25\">Rossi et al. 2018)</ref>, a message-passing approach where each node . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding recommendation systems <ref type=\"bibr\" target=\"#b37\">(Ying et al. 2018)</ref>, bioinformatics <ref type=\"bibr\" target=\"#b39\">(Zitnik, Agrawal, and Leskovec 2018;</ref><ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rial attacks, designing defense mechanisms or building robust variants of GNNs have become critical <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019)</ref>.</p><p>In this paper, we propose a new approa nism to attenuate the influence of neighbors with large variance (potentially corrupted). Following <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019)</ref>, we set hidden dimensions at 16 and assume a  poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>);</p><p>\u2022 UM-GNN achieves significantly lower  tions for specifically defending against adversarial attacks, the recent robust GCN (RGCN) approach <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>) has been the most effective, when compared to h a weighted aggregation of features in a closed neighborhood where the weights are trainable. RGCN <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>): This is a recently proposed ap-proach that e  random structural perturbations and its low performance strongly corroborates with the findings in <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>).</p><p>(ii) DICE Attack: In this challenging  <ref type=\"bibr\" target=\"#b26\">(Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018)</ref>. Recently, Zhu et al. <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>) introduced a robust variant of GCN based on a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ions directly using spatial neighborhoods <ref type=\"bibr\" target=\"#b7\">(Duvenaud et al. 2015;</ref><ref type=\"bibr\" target=\"#b0\">Atwood and Towsley 2016;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sarial attacks on images <ref type=\"bibr\" target=\"#b10\">(Goodfellow, Shlens, and Szegedy 2014;</ref><ref type=\"bibr\" target=\"#b18\">Szegedy et al. 2013)</ref> and their countermeasures <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amples from the approximate posterior through Monte Carlo sampling. Interestingly, it was showed in <ref type=\"bibr\" target=\"#b9\">(Gal and Ghahramani 2016)</ref> that the dropout inference minimizes t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020)</ref> library. Due to the lack of computationally e rnejad, and G\u00fcnnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020;</ref><ref type=\"bibr\" target=\"#b17\">Sun et al. 2018 accard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amples from the approximate posterior through Monte Carlo sampling. Interestingly, it was showed in <ref type=\"bibr\" target=\"#b9\">(Gal and Ghahramani 2016)</ref> that the dropout inference minimizes t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020)</ref> library. Due to the lack of computationally e rnejad, and G\u00fcnnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020;</ref><ref type=\"bibr\" target=\"#b17\">Sun et al. 2018 accard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020)</ref> library. Due to the lack of computationally e rnejad, and G\u00fcnnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020;</ref><ref type=\"bibr\" target=\"#b17\">Sun et al. 2018 accard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to solve challenging tasks including node classification, link prediction and graph classification <ref type=\"bibr\" target=\"#b23\">(Wu et al. 2020)</ref>.</p><p>Despite their wide-spread use, GNNs are al networks (GNNs) enables representation learning using both the graph structure and node features <ref type=\"bibr\" target=\"#b23\">(Wu et al. 2020)</ref>. While GNNs based on spectral convolutional ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s based on spectral convolutional approaches <ref type=\"bibr\" target=\"#b3\">(Bruna et al. 2013;</ref><ref type=\"bibr\" target=\"#b6\">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m 100 target nodes with FGA attack. A lower value implies improved robustness. and black-box attacks<ref type=\"bibr\" target=\"#b2\">(Bojchevski and G\u00fcnnemann 2019)</ref>. (ii) Attacker capability: based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b7\">Garg et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">May et al., 2010;</ref><ref type=\"bibr\" target=\"#b38\">Zhao et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Rudinger et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: that workers be in the United States and have a &gt; 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a pay rate of at least $15/hou eted at least 5,000 HITs, and to have greater than a 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a minimum of $15 hourly wage.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: most downstream tasks. Additionally, while BERT and ALBERT are trained on Wikipedia and BooksCorpus <ref type=\"bibr\" target=\"#b39\">(Zhu et al., 2015)</ref>, RoBERTa is also trained on OpenWebText <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b3\">Caliskan et al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Garg et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">May et al., 2010;</ref><ref type=\"bibr\" target=\"#b38\">Zhao et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b4\">(Collins, 2005;</ref><ref type=\"bibr\" target=\"#b17\">Madison, 2009;</ref><ref type=\"bibr\" target=\"#b9\">Gillespie, 2016)</ref>, they are asked to tag the example with the sin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that workers be in the United States and have a &gt; 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a pay rate of at least $15/hou eted at least 5,000 HITs, and to have greater than a 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a minimum of $15 hourly wage.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i et al., 2019)</ref>. The prompts are either premise sentences taken from MultiNLI's fiction genre <ref type=\"bibr\" target=\"#b36\">(Williams et al., 2018)</ref> or 2-3 sentence story openings taken fr , a crowdworker wrote standalone sentences inspired by a prompt that was drawn from either MultiNLI <ref type=\"bibr\" target=\"#b36\">(Williams et al., 2018)</ref> or ROCStories <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that workers be in the United States and have a &gt; 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a pay rate of at least $15/hou eted at least 5,000 HITs, and to have greater than a 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a minimum of $15 hourly wage.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: anguage processing research has recently been driven by the use of large pretrained language models <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Liu et al.,  ww.tei-c.org/ns/1.0\"><head n=\"4\">Experiments</head><p>We evaluate three widely used MLMs: BERT Base <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, RoBERTa Large <ref type=\"bibr\" target=\"#b metric bias aligns well with crowd judgements. <ref type=\"bibr\" target=\"#b26\">Rozado (2020)</ref>   <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> and ELMo <ref type=\"bibr\" target=\"#b24\">(P which U.S. stereotypical biases are present in large pretrained masked language models such as BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>. The dataset consists of 1,508 examples th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i et al., 2019)</ref>. The prompts are either premise sentences taken from MultiNLI's fiction genre <ref type=\"bibr\" target=\"#b36\">(Williams et al., 2018)</ref> or 2-3 sentence story openings taken fr , a crowdworker wrote standalone sentences inspired by a prompt that was drawn from either MultiNLI <ref type=\"bibr\" target=\"#b36\">(Williams et al., 2018)</ref> or ROCStories <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  reported in Appendix A.1.</p><p>End-to-End Entity Linking (EL) For EL, we reproduce the setting of <ref type=\"bibr\" target=\"#b27\">Kolitsas et al. (2018)</ref> using the same in-domain and out-of-doma ntity mentions (a span contained in d j ) and e i \u2208 E its corresponding entity in the KB. Following <ref type=\"bibr\" target=\"#b27\">Kolitsas et al. (2018)</ref>, we considered only mentions that have e tics for 10k steps and we do model selection on the validation set. Again, following previous works <ref type=\"bibr\" target=\"#b27\">(Kolitsas et al., 2018)</ref>, we considered only mentions that have . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tings (both in and out-ofdomain); (ii) end-to-end entity linking, with the GERBIL benchmarking tool <ref type=\"bibr\" target=\"#b52\">(R\u00f6der et al., 2018)</ref>, by using a novel dynamically markup-const nd out-of-domain datasets as well as evaluating the InKB micro-F 1 on the GERBIL benchmark platform <ref type=\"bibr\" target=\"#b52\">(R\u00f6der et al., 2018)</ref>. Similarly to the ED setting, we first pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b51\">(R\u00f6der et al., 2014)</ref>, and OKE challenge 2015 and 2016 (OKE15 and OKE16) <ref type=\"bibr\" target=\"#b42\">(Nuzzolese et al., 2015)</ref>. More task details and hyperparameters. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: discourse representation structure parsing <ref type=\"bibr\" target=\"#b36\">(Liu et al., 2018)</ref>  <ref type=\"bibr\" target=\"#b28\">(Konstas et al., 2017)</ref>. In these works a structured representat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b21\">Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Le &amp; Titov, 2018;</ref><ref type=\"bibr\" target=\"#b37\">Logeswaran et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Broscheit oftmax over all entities is very expensive, hence current solutions need to subsample negative data <ref type=\"bibr\" target=\"#b37\">(Logeswaran et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Karpukh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  generate entity names. This architecture has been shown to retain factual knowledge to some extent <ref type=\"bibr\" target=\"#b45\">(Petroni et al., 2019)</ref> and language translation skills <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Lewis et al., 2020)</ref>, and BLINK+flair <ref type=\"bibr\" target=\"#b64\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">Akbik et al., 2019)</ref>. No model except ours was trained on the ent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g using Natural Questions <ref type=\"bibr\" target=\"#b29\">(Kwiatkowski et al., 2019)</ref>, HotpotQA <ref type=\"bibr\" target=\"#b70\">(Yang et al., 2018c)</ref>, TriviaQA <ref type=\"bibr\" target=\"#b24\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: zed with dropout <ref type=\"bibr\" target=\"#b57\">(Srivastava et al., 2014)</ref> and label smoothing <ref type=\"bibr\" target=\"#b61\">(Szegedy et al., 2016)</ref>. Concretely, we use the objective that i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nowledge Bases (KBs) given a textual input is a fundamental building block for several applications <ref type=\"bibr\" target=\"#b14\">(Ferrucci, 2012;</ref><ref type=\"bibr\" target=\"#b56\">Slawski, 2015;</  (e.g., Wikipedia articles) to find knowledge for sustaining a conversation or answering a question <ref type=\"bibr\" target=\"#b14\">(Ferrucci, 2012;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2017. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: input and the output proximity. A different type of constraint, a structural constraint, is used in <ref type=\"bibr\" target=\"#b2\">Balakrishnan et al. (2019)</ref> to maintain a valid tree structure. O. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  statistics for 10k steps and we do model selection on the validation set. Following previous works <ref type=\"bibr\" target=\"#b66\">(Yamada et al., 2016;</ref><ref type=\"bibr\" target=\"#b16\">Ganea &amp;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b19\">(Hoffart et al., 2012)</ref>, N3-Reuters-128 (R128), N3-RSS-500 (R500) <ref type=\"bibr\" target=\"#b51\">(R\u00f6der et al., 2014)</ref>, and OKE challenge 2015 and 2016 (OKE15 an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: zed with dropout <ref type=\"bibr\" target=\"#b57\">(Srivastava et al., 2014)</ref> and label smoothing <ref type=\"bibr\" target=\"#b61\">(Szegedy et al., 2016)</ref>. Concretely, we use the objective that i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntity disambiguation on AIDA CoNLL-YAGO, WNED-WIKI and WNED-CWEB; dialogue with Wizard of Wikipedia <ref type=\"bibr\" target=\"#b10\">(Dinan et al., 2019)</ref>. We train GENRE on BLINK and all KILT data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the public leaderboard: DPR <ref type=\"bibr\" target=\"#b25\">(Karpukhin et al., 2020)</ref>, DPR+BERT <ref type=\"bibr\" target=\"#b9\">(Devlin et al., 2019)</ref>, DPR+BART, tf-idf <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the public leaderboard: DPR <ref type=\"bibr\" target=\"#b25\">(Karpukhin et al., 2020)</ref>, DPR+BERT <ref type=\"bibr\" target=\"#b9\">(Devlin et al., 2019)</ref>, DPR+BART, tf-idf <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: so related to metric learning works that employ generators <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>. Apart from not requiring labels, our method exploits the m od exploits the memory component, something not present in <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>. It has no extra parameters or loss terms that need to be o </ref><ref type=\"bibr\" target=\"#b36\">35]</ref>. Works like <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref> use generators to synthesize negatives in a supervised scen and exploit its memory component. What is more, and unlike <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>, we do not require a generator, i.e. have no extra paramete izing negatives was explored in metric learning literature <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87,</ref><ref type=\"bibr\" target=\"#b36\">35]</ref>. Works like <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: earning visual representations in a self-supervised manner <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>. Pushing the embeddings of two transformed versions of the  ations learned in an unsupervised way. It is however shown <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref> that increasing the memory/batch size leads to diminishing  t also use contrastive learning losses. These include MoCo <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=  contain the whole training set, while the recent Momentum Contrast (or MoCo) approach of He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref> keeps a queue with features of the last few batches as mem t of self-supervised representation learning. We delve deeper into learning with a momentum encoder <ref type=\"bibr\" target=\"#b31\">[30]</ref> and show evidence that harder negatives are required to fa ions. a)</head><p>We delve deeper into a top-performing contrastive self-supervised learning method <ref type=\"bibr\" target=\"#b31\">[30]</ref> and observe the need for harder negatives; b) We propose h air, which is contrasted with every feature n in the bank of negatives (Q) also called the queue in <ref type=\"bibr\" target=\"#b31\">[30]</ref>. A popular and highly successful loss function for contras \"bibr\" target=\"#b65\">64,</ref><ref type=\"bibr\" target=\"#b78\">77]</ref>, a queue of the last batches <ref type=\"bibr\" target=\"#b31\">[30]</ref>, or simply be every other image in the current minibatch < computing them as the encoder keeps changing. The Momentum Contrast (or MoCo) approach of He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref> offers a compromise between the two negative sampling extr  and all features in Q are encoded with the key encoder.</p><p>How hard are MoCo negatives? In MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> (resp. SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>) t  helping a lot towards learning the proxy task.</p><p>On the difficulty of the proxy task. For MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, InfoMin i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> and MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref>. MoC rop testing. For object detection on PASCAL VOC <ref type=\"bibr\" target=\"#b18\">[19]</ref> we follow <ref type=\"bibr\" target=\"#b31\">[30]</ref> and fine-tune a Faster R-CNN <ref type=\"bibr\" target=\"#b55 pe=\"foot\" target=\"#foot_2\">3</ref> code and report the common AP, AP50 and AP75 metrics. Similar to <ref type=\"bibr\" target=\"#b31\">[30]</ref>, we do not perform hyperparameter tuning for the object de tic segmentation on the COCO dataset <ref type=\"bibr\" target=\"#b42\">[41]</ref>. Following He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref>, we use Mask R-CNN <ref type=\"bibr\" target=\"#b29\">[29]</re nd on the train2017 set (118k images) and evaluate on val2017. We adopt feature normalization as in <ref type=\"bibr\" target=\"#b31\">[30]</ref> when fine-tuning. MoCHi and MoCo use the same hyper-parame llowing our discussion in Section 3.2, we wanted to verify that hardness of the proxy task for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> is directly correlated to the difficulty of the transforma i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref> and som et=\"#b53\">[52]</ref> study the robustness of contrastive self-supervised learning methods like MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> and PIRL <ref type=\"bibr\" target=\"#b47\">[46]</ref> and saw [11]</ref>, e.g. the addition of a target network whose parameter update is lagging similar to MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>  Synthesizing for supervised metric learning. Recently, sy rt in parenthesis the difference to MoCo-v2. * denotes reproduced results. \u2020 results are copied from<ref type=\"bibr\" target=\"#b31\">[30]</ref>. We bold (resp. underline) the highest results overall (re rg/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">In this section we study contrastive learning for MoCo<ref type=\"bibr\" target=\"#b31\">[30]</ref> on ImageNet-100, a subset of ImageNet consisting of 100 cl et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b47\">46,</ref><ref type=\"bibr\" tar d highly successful loss function for contrastive learning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b65\">64]</ref> is the following:</ 2 -normalized. In a number of recent successful approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b47\">46,</ref><ref type=\"bibr\" tar pervised learning papers do not discuss variance ; in fact only papers from highly resourceful labs <ref type=\"bibr\" target=\"#b31\">[30,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f high-level semantic information.</p><p>A number of works exploit the audio-visual nature of video <ref type=\"bibr\" target=\"#b38\">[37,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f high-level semantic information.</p><p>A number of works exploit the audio-visual nature of video <ref type=\"bibr\" target=\"#b38\">[37,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b66\">65,</ref><ref type=\"bibr\" target=\"#b74\">73]</ref>, heavy data augmentations applied to the same image are cru edding space.</p><p>Measuring the utilization of the embedding space. Very recently, Wang and Isola <ref type=\"bibr\" target=\"#b74\">[73]</ref> presented two losses/metrics for assessing contrastive lea /head><p>The uniformity experiment in Figure <ref type=\"figure\">3c</ref> is based on Wang and Isola <ref type=\"bibr\" target=\"#b74\">[73]</ref>. We follow the same definitions of the losses/metrics as p erstand the underlying mechanism that make it work so well <ref type=\"bibr\" target=\"#b68\">[67,</ref><ref type=\"bibr\" target=\"#b74\">73,</ref><ref type=\"bibr\" target=\"#b60\">59,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -supervised learning <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b77\">76,</ref><ref type=\"bibr\" tar contrastive learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b46\">45,</ref><ref type=\"bibr\" tar /ref><ref type=\"bibr\" target=\"#b77\">76,</ref><ref type=\"bibr\" target=\"#b79\">78]</ref>. Iscen et al. <ref type=\"bibr\" target=\"#b34\">[33]</ref> mine hard negatives from a large set by focusing on the fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -supervised learning <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b77\">76,</ref><ref type=\"bibr\" tar contrastive learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b46\">45,</ref><ref type=\"bibr\" tar /ref><ref type=\"bibr\" target=\"#b77\">76,</ref><ref type=\"bibr\" target=\"#b79\">78]</ref>. Iscen et al. <ref type=\"bibr\" target=\"#b34\">[33]</ref> mine hard negatives from a large set by focusing on the fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: speech transcripts <ref type=\"bibr\" target=\"#b63\">[62,</ref><ref type=\"bibr\" target=\"#b62\">61,</ref><ref type=\"bibr\" target=\"#b45\">44]</ref> or surrounding text <ref type=\"bibr\" target=\"#b23\">[24]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers of MoCHi are (N, s, s ).</p><p>We learn representations on two datasets, the common ImageNet-1K <ref type=\"bibr\" target=\"#b56\">[55]</ref>, and its smaller ImageNet-100 subset, also used in <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b46\">45,</ref><ref type=\"bibr\" target=\"#b58\">57,</ref><ref type=\"bibr\" target=\"#b76\">75,</ref><ref type=\"bibr\" target=\"#b82\">81]</ref>. Sampling negatives. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ervised (proxy) task harder. At the same time, data mixing techniques operating at either the pixel <ref type=\"bibr\" target=\"#b71\">[70,</ref><ref type=\"bibr\" target=\"#b84\">83,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b37\">36,</ref><ref type=\"bibr\" target=\"#b49\">48]</ref>. Instance discrimin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y shown to be a highly effective way of learning visual representations in a self-supervised manner <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>. Pushing the embed  evaluate the quality of visual representations learned in an unsupervised way. It is however shown <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref> that increasing th lude MoCo <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, PIRL <ref type=\"b  prediction tasks.</p><p>Most of the top-performing contrastive methods leverage data augmentations <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta r\" target=\"#b31\">[30]</ref>. A popular and highly successful loss function for contrastive learning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" ta erature parameter and all embeddings are 2 -normalized. In a number of recent successful approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" ta ww.tei-c.org/ns/1.0\"><head n=\"4.3\">Discussion and analysis of MoCHi</head><p>Recent approaches like <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> use a Multi-layer  s used for target tasks-a lower-layer embedding is used instead. Unless otherwise stated, we follow <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> and use a 2-layer  e contrastive loss, current top contrastive approaches either substantially increase the batch size <ref type=\"bibr\" target=\"#b10\">[11]</ref>, or keep large memory banks. Approaches like <ref type=\"bi  <ref type=\"bibr\" target=\"#b31\">[30]</ref>, or simply be every other image in the current minibatch <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>The log-likelihood function of Eq (1) is defined ov  target=\"#b82\">81]</ref>. Sampling negatives from the same batch leads to a need for larger batches <ref type=\"bibr\" target=\"#b10\">[11]</ref> while sampling negatives from a memory bank that contains  </p><p>How hard are MoCo negatives? In MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> (resp. SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>) the authors show that increasing the memory (resp. batch) ><p>On the difficulty of the proxy task. For MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, InfoMin <ref type=\"bibr\" target=\"#b66\">[65]</ref>, and ot at is not very far from the supervised case.</p><p>Focusing on the positive pair. Works like SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Tia ead the resulting features in the embedding space. BYOL makes a number of modifications over SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, e.g. the addition of a target network whose parameter upd ariance ; in fact only papers from highly resourceful labs <ref type=\"bibr\" target=\"#b31\">[30,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b66\">65]</ref> report averaged res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: part from audio, other methods have used use automatic extracted text, e.g. from speech transcripts <ref type=\"bibr\" target=\"#b63\">[62,</ref><ref type=\"bibr\" target=\"#b62\">61,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  best of both worlds by combining contrastive and clustering losses. Methods like local aggregation <ref type=\"bibr\" target=\"#b90\">[89]</ref>, Prototypical Contrastive learning <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ods in the table, e.g. PCL <ref type=\"bibr\" target=\"#b41\">[40]</ref>, or the clustering approach of <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Both unfortunately use a different setup for PASCAL VOC and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: part from audio, other methods have used use automatic extracted text, e.g. from speech transcripts <ref type=\"bibr\" target=\"#b63\">[62,</ref><ref type=\"bibr\" target=\"#b62\">61,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uring more comprehensive graph features for downstream tasks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>.</p><p>Multiscale f res in parallel and merge them as the final representation <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar #b41\">42]</ref>  (a) Encoder-decoder <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>(b) Graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</ref>. (c) Readout <ref type=\"bibr\" target=\"#b30\">[31]</ref>.</p rchanging across scales forms a crossing shape.</p><p>Remark: In each individual scale, graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</ref> simply uses skip connections while GXN uses multiple graph \"bibr\" target=\"#b34\">[35]</ref> designs various graph filters on the multiscale graphs. Graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</ref> and readout functions <ref type=\"bibr\" target=\"#b18\">[19,< tion. 2) GXN extracts hierarchical multiscale features through a deep network, previous Graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</ref> extracts features only once in each scale and then uses sk cess of vertex selection and graph pooling process.</p><p>To implement graph unpooling, inspired by <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we design an inverse process against graph pooling. We in eatures, explicitly utilizing some structural information. We use the same dataset separation as in <ref type=\"bibr\" target=\"#b19\">[20]</ref>, perform 10-fold cross-validation, and show the average ac ent datasets. GXN (gPool) and GXN (SAGPool) denote that we apply previous pooling operations, gPool <ref type=\"bibr\" target=\"#b19\">[20]</ref> and SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref> in o =\"bibr\" target=\"#b52\">[53]</ref>, AttPool <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Graph U-Net <ref type=\"bibr\" target=\"#b19\">[20]</ref>. In the VIPool, we use a 2-layer MLP and R-layer GCN (R =  pe=\"bibr\" target=\"#b52\">[53]</ref>, DiffPool <ref type=\"bibr\" target=\"#b49\">[50]</ref>, Graph U-Net <ref type=\"bibr\" target=\"#b19\">[20]</ref>, SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref>, AttPoo ditionally, we design several variants of GXN: 1) to test the superiority of VIPool, we apply gPool <ref type=\"bibr\" target=\"#b19\">[20]</ref>, SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Att ype=\"bibr\" target=\"#b6\">[7]</ref>, ASGCN <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and Graph U-Net <ref type=\"bibr\" target=\"#b19\">[20]</ref> for vertex classification. We reproduce these methods for  ifferent pooling methods with the same GXN model framework, where the pooling methods include gPool <ref type=\"bibr\" target=\"#b19\">[20]</ref>, SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Att p><p>We compare the proposed VIPool operation with several baseline methods: random sampling, gPool <ref type=\"bibr\" target=\"#b19\">[20]</ref>, SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Att er graph pooling methods adaptively select vertices based on their importance over the entire graph <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>; however, they fai tructure and information fusion easier to achieve. Compared to other vertex-selection-based methods <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, VIPool considers  association to preserve the original vertex information. The vertex-selection-based pooling methods <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> preserve selected . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [49]</ref>, and bioinformatic datasets: D&amp;D <ref type=\"bibr\" target=\"#b14\">[15]</ref>, PROTEINS <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and ENZYMES <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Tabl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, which expand deep learning techniques to ubiquitous non-Eu \">[49]</ref>, bioinformatic networks <ref type=\"bibr\" target=\"#b14\">[15]</ref> and human activities <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Achieving good performances on graph-related tasks, such . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. However, this generalization is technically nontrivial. Wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [49]</ref>, and bioinformatic datasets: D&amp;D <ref type=\"bibr\" target=\"#b14\">[15]</ref>, PROTEINS <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and ENZYMES <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Tabl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: feature learning on graphs enables capturing more comprehensive graph features for downstream tasks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targ >To aggregate features across multiple scales, existing attempts build encoder-decoder architecture <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" targ , various multiscale network structures have been explored. Hierarchical encoder-decoder structures <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" targ ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>  (a) Encoder-decoder <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>(b) Graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graphs into smaller ones. Conventional graph pooling methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>  (a) Encoder-decoder <ref type=\"bibr\" target=\"#b5\">[6]</ref designed based on graph sampling theory <ref type=\"bibr\" target=\"#b7\">[8]</ref> or graph coarsening <ref type=\"bibr\" target=\"#b41\">[42]</ref>. With the study of deep learning, some works down-scale gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. However, this genera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: convenience in optimization, fdivergence representation based on a non-KL divergence can be adopted <ref type=\"bibr\" target=\"#b37\">[38]</ref>, which still measures the vertex-neighborhood dependency.   f (x) = x log x \u2212 (x + 1) log(x + 1) and conjugate divergence function f * (t) = \u2212 log(1 \u2212 exp(t)) <ref type=\"bibr\" target=\"#b37\">[38]</ref>. We let the activation be a(\u2022) = \u2212 log(1 + exp(\u2022)). The GA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b14\">[15]</ref>, PROTEINS <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and ENZYMES <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Table <ref type=\"table\">1</ref> shows the dataset informati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion. To solve the vertex selection problem (1), we consider the submodularity of mutual information <ref type=\"bibr\" target=\"#b8\">[9]</ref> and employ a greedy algorithm: we select the first vertex wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b31\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">Lewis et al., 2020</ref>), yet they are far from perfect. In generati eration framework, built upon the pre-trained sequence-to-sequence (seq2seq) Transformer model BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref>. As shown in Figure <ref type=\"figure\" tar  et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al., 2019)</ref>. Our work uses BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref>, a state-of-the-art seq2seq model that off U card with 24 GB memory.</p><p>Model Sizes. Our generation model has the same architecture as BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref> with 406M parameters. The content planner . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tone of many state-of-the-art models in various natural language understanding and generation tasks <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b31\">Liu et al.,  els to generate more rele-vant and coherent text. We first study a planning model trained from BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> to produce the initial content plan, which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ions to attend to both left and right. To resolve this discrepancy, we apply causal attention masks <ref type=\"bibr\" target=\"#b6\">(Dong et al., 2019)</ref> over m to disallow attending to the future (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Automatic Evaluation</head><p>We report scores with BLEU <ref type=\"bibr\" target=\"#b37\">(Papineni et al., 2002)</ref>, which is based on n-gram precision (up. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ment has been studied in machine translation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Freitag et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Mansimov et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2019)</ref> of the output. Specific applications encourage the model to cover a given topic <ref type=\"bibr\" target=\"#b48\">(Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">See et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed entities <ref type=\"bibr\" target=\"#b10\">(Fan et al., 2018)</ref>, or display a certain attribute <ref type=\"bibr\" target=\"#b18\">(Hu et al., 2017;</ref><ref type=\"bibr\" target=\"#b32\">Luo et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive progress made in many generation tasks, neural systems are known to produce low-quality content <ref type=\"bibr\" target=\"#b51\">(Wiseman et al., 2017;</ref><ref type=\"bibr\" target=\"#b43\">Rohrbach e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -trained Transformer generators.</p><p>Iterative Refinement has been studied in machine translation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Freitag et al. inspired by iterative decoding designed for inference acceleration in non-autoregressive generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce words to appear at specific positions <ref type=\"bibr\" target=\"#b15\">(Hokamp and Liu, 2017;</ref><ref type=\"bibr\" target=\"#b39\">Post and Vilar, 2018;</ref><ref type=\"bibr\">Hu et al., 2019)</ref>, w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , 2018)</ref>, or display a certain attribute <ref type=\"bibr\" target=\"#b18\">(Hu et al., 2017;</ref><ref type=\"bibr\" target=\"#b32\">Luo et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">Balakrishnan et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd the rest (23k for news and 10k for opinion) are used for training. We apply the BPE tokenization <ref type=\"bibr\" target=\"#b46\">(Sennrich et al., 2016)</ref> for the generation model as BART does,  nt plan is predicted by the planner, the following post-processing steps are employed prior to the  <ref type=\"bibr\" target=\"#b46\">(Sennrich et al., 2016)</ref>. KP distance denotes the average number. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive progress made in many generation tasks, neural systems are known to produce low-quality content <ref type=\"bibr\" target=\"#b51\">(Wiseman et al., 2017;</ref><ref type=\"bibr\" target=\"#b43\">Rohrbach e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: na.</p><p>Are PAIR generations similar to humanwritten text in discourse structure? We utilize DPLP <ref type=\"bibr\" target=\"#b20\">(Ji and Eisenstein, 2014)</ref>, an off-theshelf Rhetorical Structure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ourse markers are crucial for coherence <ref type=\"bibr\" target=\"#b14\">(Grote and Stede, 1998;</ref><ref type=\"bibr\" target=\"#b1\">Callaway, 2003)</ref> and have received dedicated research efforts in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd Jur\u010d\u00ed\u010dek, 2016;</ref><ref type=\"bibr\" target=\"#b13\">Goyal and Durrett, 2020)</ref> and semantics <ref type=\"bibr\" target=\"#b49\">(Wen et al., 2015;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -autoregressive generation outputs <ref type=\"bibr\" target=\"#b12\">(Ghazvininejad et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al., 2019)</ref>. Our work uses BART <ref type=\"bibr\" tar cceleration in non-autoregressive generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al., 2019)</ref>, though their refinement mostly focuses . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b22\">(Keskar et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Moryossef et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Hua and Wang, 2019)</ref>, yet those solutions require model architec <ref type=\"bibr\" target=\"#b35\">Moryossef et al., 2019;</ref><ref type=\"bibr\">Yao et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Hua and Wang, 2019)</ref>. However, it is still an open question to i m three distinct domains for multiparagraph-level text generation: (1) argument generation (ARGGEN) <ref type=\"bibr\" target=\"#b19\">(Hua et al., 2019)</ref>, to produce a counter-argument to refute a g rgument generation, based on a dataset collected from Reddit r/ChangeMyView (CMV) in our prior work <ref type=\"bibr\" target=\"#b19\">(Hua et al., 2019)</ref>. This dataset contains pairs of original pos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b1\">Callaway, 2003)</ref> and have received dedicated research efforts in rulebased systems <ref type=\"bibr\" target=\"#b42\">(Reed et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Balakrishnan e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /www.tei-c.org/ns/1.0\"><head n=\"4.2\">Implementation Details</head><p>Our code is written in PyTorch <ref type=\"bibr\" target=\"#b38\">(Paszke et al., 2019)</ref>. For fine-tuning, we adopt the standard l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tection and 71.0 for relation prediction on news articles from the annotated RST Discourse Treebank <ref type=\"bibr\" target=\"#b2\">(Carlson et al., 2001)</ref>. We run this trained model on our data fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -autoregressive generation outputs <ref type=\"bibr\" target=\"#b12\">(Ghazvininejad et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al., 2019)</ref>. Our work uses BART <ref type=\"bibr\" tar cceleration in non-autoregressive generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al., 2019)</ref>, though their refinement mostly focuses . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Mix, a data augmentation method for generating sub-sequences along with their labels based on mixup <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref>. Under the active sequence labeling framew ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Sequence Mixup in the Embedding Space</head><p>Mixup <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref> is a data augmentation method that impleme lation-based Regularizations Mixup implements interpolation in the input space to regularize models <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type=\"b r limit rather than a too narrow score range setting.</p><p>For the mixing coefficient \u03bb, we follow <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref> to sample it from Beta(\u03b1, \u03b1) and explore \u03b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 6\">Hu et al. (2017)</ref> propose to augment text data in an encoder-decoder manner. Very recently, <ref type=\"bibr\" target=\"#b1\">(Anaby-Tavor et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Kobayas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: approaches which select samples based on the query policy design. So far, various uncertainty-based <ref type=\"bibr\" target=\"#b31\">(Scheffer et al., 2001;</ref><ref type=\"bibr\" target=\"#b7\">Culotta an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: equences that pass the sequence quality screening. For screening, we utilize a language model GPT-2 <ref type=\"bibr\" target=\"#b28\">(Radford et al., 2019)</ref> to score sequence x by computing its per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: so infeasible to apply heuristic data augmentation methods such as context-based words substitution <ref type=\"bibr\" target=\"#b20\">(Kobayashi, 2018)</ref>, synonym replacement, random insertion, swap, ncoder-decoder manner. Very recently, <ref type=\"bibr\" target=\"#b1\">(Anaby-Tavor et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Kobayashi, 2018)</ref> harness the power of pre-trained language mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: equences that pass the sequence quality screening. For screening, we utilize a language model GPT-2 <ref type=\"bibr\" target=\"#b28\">(Radford et al., 2019)</ref> to score sequence x by computing its per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bels jointly. Prevailing generative models <ref type=\"bibr\" target=\"#b49\">(Zhang et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Bowman et al., 2016)</ref> are inapplicable because they can only gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rize models <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type=\"bibr\" target=\"#b40\">(Verma et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Summers and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  models. The disagreement can be defined in several ways, here we take the vote entropy proposed by <ref type=\"bibr\" target=\"#b8\">(Dagan and Engelson, 1995)</ref>. Given a committee consist of C model m, 2005;</ref><ref type=\"bibr\" target=\"#b19\">Kim et al., 2006)</ref> and committee-based approaches <ref type=\"bibr\" target=\"#b8\">(Dagan and Engelson, 1995)</ref>   <ref type=\"formula\">2017</ref>) fur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: so infeasible to apply heuristic data augmentation methods such as context-based words substitution <ref type=\"bibr\" target=\"#b20\">(Kobayashi, 2018)</ref>, synonym replacement, random insertion, swap, ncoder-decoder manner. Very recently, <ref type=\"bibr\" target=\"#b1\">(Anaby-Tavor et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Kobayashi, 2018)</ref> harness the power of pre-trained language mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: proaches including synonym replancement, random insertion, swap and deletion for text augmentation, <ref type=\"bibr\" target=\"#b18\">Kafle et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b35\">Silfverberg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ods include AdaBoost <ref type=\"bibr\" target=\"#b10\">(Freund and Schapire, 1997)</ref> and RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref>, which target at classification and rankin ng model that makes more accurate predictions should receive a higher weight. Inspired by RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref>, we reduce the ranking combination problem  = {q1 = (The Tale of Genji, country, ?t) q2 = (The Tale of Genji, genre, ?t)} Similar to RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref> Ranking loss. The overall objective of KEn <p>Let the set of all the critical entity pairs from all the validation queries of an entity as P . <ref type=\"bibr\" target=\"#b9\">Freund et al. (2004)</ref> have proved that, when using RankBoost, thi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: erent KGs. Recent works on multilingual KG embeddings provide support for automated entity matching <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017</ref><ref type=\"bibr\" target=\"#b4\">(Chen et al., ,   extended embedding models to bridge multiple KGs, typically for KGs of multiple languages. MTransE <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref> jointly learns a transformation across two s The embedding learning process jointly trains the knowledge model and the alignment model following <ref type=\"bibr\" target=\"#b5\">Chen et al. (2017)</ref>, while self-learning is added to improve the  G i and G j . \u03bb is a positive hyperparameter that weights the two model components.</p><p>Following <ref type=\"bibr\" target=\"#b5\">Chen et al. (2017)</ref>, instead of directly optimizing J in Eq. ( <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 19a</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 2020a) )</ref> and degree centrality measures <ref type=\"bibr\" target=\"#b21\">(Pei et al., 2019)</ref>. A systematic summary of relevant approaches. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ootstrapping approach to iteratively propose new alignment labels to enhance the performance. MuGNN <ref type=\"bibr\" target=\"#b2\">(Cao et al., 2019)</ref> encodes KGs via multi-channel Graph Neural Ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Yang et al., 2019)</ref>, neighborhood information <ref type=\"bibr\" target=\"#b36\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: from latent representations of observed facts. Representative models including translational models <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b35\">Wang et al.,  weighting (KEnS m ): MRR is a widely-used metric for evaluating the ranking performance of a model <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al., , after applying a relation-specific translation vector r. The representative models include TransE <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref> and its extensions TransD <ref type=\"bibr\"  true triple (h, r, t).</p><p>We here consider two representative triple scoring techniques: TransE <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref> and RotatE <ref type=\"bibr\" target=\"#b30\"> higher. Although another common metric, Mean Reciprocal Rank (MRR), has been used in previous works <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref>, it is not applicable to the evaluation of ce techniques introduced in in Section 3. For baseline methods, besides the single-embedding TransE <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013)</ref> and RotatE <ref type=\"bibr\" target=\"#b30\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b16\">(Koncel-Kedziorski et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2018a;</ref><ref type=\"bibr\" target=\"#b1\">Bordes et al., 2014)</ref>. Recently, extensive efforts have been inve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 19a</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 2020a) )</ref> and degree centrality measures <ref type=\"bibr\" target=\"#b21\">(Pei et al., 2019)</ref>. A systematic summary of relevant approaches. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odels achieve satisfactory performance on KG completion and are robust against the sparsity of data <ref type=\"bibr\" target=\"#b11\">(Hao et al., 2019)</ref>. RotatE <ref type=\"bibr\" target=\"#b30\">(Sun . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b38\">Yang et al., 2015;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Sun et al., 2019a</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tive models including translational models <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b35\">Wang et al., 2014)</ref> and bilinear models <ref type=\"bibr\" target= e premise that the candidate space has excluded the triples that have been seen in the training set <ref type=\"bibr\" target=\"#b35\">(Wang et al., 2014)</ref>. Competitive methods. We compare six varian. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stMult <ref type=\"bibr\" target=\"#b38\">(Yang et al., 2015)</ref>, as well as neural models like HolE <ref type=\"bibr\" target=\"#b19\">(Nickel et al., 2016)</ref> and ConvE <ref type=\"bibr\" target=\"#b8\">( >(Yang et al., 2015)</ref>, TransD <ref type=\"bibr\" target=\"#b13\">(Ji et al., 2015)</ref>, and HolE <ref type=\"bibr\" target=\"#b19\">(Nickel et al., 2016)</ref>. After extensive hyperparameter tuning, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for estimating the prediction confidence of each language-specific embedding in ensemble inference <ref type=\"bibr\" target=\"#b23\">(Shen et al., 2017)</ref>. Let the MRR of f i be u i on the validatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  we only provide a highly selective summary here. Interested readers are referred to recent surveys <ref type=\"bibr\" target=\"#b34\">(Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b14\">Ji et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b5\">(Chen et al., 2017</ref><ref type=\"bibr\" target=\"#b4\">(Chen et al., , 2018b;;</ref><ref type=\"bibr\" target=\"#b26\">Sun et al., 2018</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 20 nsformation across two separate translational embedding spaces along with the KG structures. BootEA <ref type=\"bibr\" target=\"#b26\">(Sun et al., 2018)</ref> introduces a bootstrapping approach to itera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b16\">(Koncel-Kedziorski et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2018a;</ref><ref type=\"bibr\" target=\"#b1\">Bordes et al., 2014)</ref>. Recently, extensive efforts have been inve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\">), attributes (Trsedya et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Sun et al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Yang et al., 2019)</ref>, neighborhood information <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b4\">(Chen et al., , 2018b;;</ref><ref type=\"bibr\" target=\"#b26\">Sun et al., 2018</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 2020a))</ref>. However, the performance of the state-of r, the performance of the state-of-the-art (SOTA) entity matching methods is still far from perfect <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020a)</ref>, which may cause erroneous knowledge transf pe=\"bibr\" target=\"#b18\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Sun et al., 2019a</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., , 2020a) )</ref> and degree centrality measures <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tive models including translational models <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b35\">Wang et al., 2014)</ref> and bilinear models <ref type=\"bibr\" target= e premise that the candidate space has excluded the triples that have been seen in the training set <ref type=\"bibr\" target=\"#b35\">(Wang et al., 2014)</ref>. Competitive methods. We compare six varian. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ootstrapping approach to iteratively propose new alignment labels to enhance the performance. MuGNN <ref type=\"bibr\" target=\"#b2\">(Cao et al., 2019)</ref> encodes KGs via multi-channel Graph Neural Ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: user packets are not detoured in transmission, numerous network attack surfaces are opened up today <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b3\">[4]</ref>. For example, an at d path validation that fill the void, such as ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref> and OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>. However, for the targeting environment which is adversarial performance of the Click router as the baseline, and compare our PSVM with the-state-of-the-art OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>.  Method and parameter setup. For fairness, we use the same  ave been proposed in ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the Origin and Path Trace (OPT) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t enables receivers to efficiently localize faulty links <ref type=\"bibr\" target=\"#b24\">[26]</ref>- <ref type=\"bibr\" target=\"#b26\">[28]</ref>. In practice, most of these systems are expensive to deplo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he above problems, being unable to identify path deviation <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, there are several proposals addressing both sour. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 13\">[14]</ref>, obtained from some BGP related protocols <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, or employing the existing control plane routing protocols. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and PPV <ref type=\"bibr\" target=\"#b27\">[29]</ref>. In ICING, it requires each router to verify the optimized. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are several proposals addressing both source and path validation that fill the void, such as ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref> and OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>. However, fo itecture that separates general service functions from routing nodes, and its design is inspired by <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t Current techniques for both source authentication and path verification have been proposed in ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the Origin and Path Trace (OPT) <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: metric key (Key Session N i</p><p>) from the node's master key (without seeking help from node N i) <ref type=\"bibr\" target=\"#b12\">[13]</ref> to calculate the authentication structure for source and p ref type=\"bibr\" target=\"#b15\">[16]</ref>, or employing the existing control plane routing protocols <ref type=\"bibr\" target=\"#b12\">[13]</ref> (especially Pathlet <ref type=\"bibr\" target=\"#b16\">[17]</r metric key of N i (Key Session N i</p><p>) can be derived through pseudo-random operation functions <ref type=\"bibr\" target=\"#b12\">[13]</ref> by N i and its CGA, respectively, without long-term storag. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are several proposals addressing both source and path validation that fill the void, such as ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref> and OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>. However, fo itecture that separates general service functions from routing nodes, and its design is inspired by <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t Current techniques for both source authentication and path verification have been proposed in ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the Origin and Path Trace (OPT) <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pology analysis <ref type=\"bibr\" target=\"#b13\">[14]</ref>, obtained from some BGP related protocols <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, or employing t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he above problems, being unable to identify path deviation <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, there are several proposals addressing both sour. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Furthermore, we suppose that node N i and its CGA would use secret methods (such as Diffie-Hellman <ref type=\"bibr\" target=\"#b18\">[19]</ref>) to share the master key (Key N i ), which may be replaced. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targe ng artificial labels <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. We introduce Fix-Mat rget=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. We found that this w version of the same image. Inspired by UDA <ref type=\"bibr\" target=\"#b44\">[45]</ref> and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we leverage CutOut <ref type=\"bibr\" target=\"#b12\">[13]</ref br\" target=\"#b1\">[2]</ref>, we leverage CutOut <ref type=\"bibr\" target=\"#b12\">[13]</ref>, CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> fo <p>curacy on CIFAR-10 with 250 labeled examples compared to the previous state-of-the-art of 93.73% <ref type=\"bibr\" target=\"#b1\">[2]</ref> in the standard experimental setting from <ref type=\"bibr\" t periment with two such variants: RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Note that, unless otherwise stated, we use Cutout <ref type arget=\"#b44\">[45]</ref>.</p><p>Instead of setting the transformation magnitudes randomly, CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref> learns them online over the course of training. To do so, a  how close the model's prediction is to the true label. Further details on CTAugment can be found in <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= thms: Unsupervised Data Augmentation (UDA) <ref type=\"bibr\" target=\"#b44\">[45]</ref> and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>. UDA and ReMixMatch both use a weakly-augmented example to g e=\"bibr\" target=\"#b2\">[3]</ref> Weak Weak Sharpening Averages multiple artificial labels ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref> Weak Strong Sharpening Sums losses for multiple predictions  f type=\"bibr\" target=\"#b2\">[3]</ref>, UDA <ref type=\"bibr\" target=\"#b44\">[45]</ref>, and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With the exception of <ref type=\"bibr\" target=\"#b1\">[2]</re et=\"#b44\">[45]</ref>, and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With the exception of <ref type=\"bibr\" target=\"#b1\">[2]</ref>, previous work has not considered fewer than 25 labels per c 3.13%. Our results also compare favorably to recent state-of-the-art results achieved by ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>, despite the fact that we omit various components such as th ch (RA) uses RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and FixMatch (CTA) uses CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref> for strong-augmentation. All baseline models (\u03a0-Model <ref t f type=\"bibr\" target=\"#b2\">[3]</ref>, UDA <ref type=\"bibr\" target=\"#b44\">[45]</ref>, and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>) are tested using the same codebase.  using CTAugment and Ra e=\"table\" target=\"#tab_1\">3</ref>, FixMatch achieves the state-of-the-art performance of ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref> despite being significantly simpler.</p></div> <div xmlns=\"h FixMatch. Specifically, we chose RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref>, which have been used for state-of-the-art SSL algorithms su A) uses RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and the ones with (CTA) uses CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref> for strong-augmentation. All models are tested using the sam of image transformations used in RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref>. For completeness, we listed all transformation operations f d 2 loss [28, 45, 2], using stronger forms of augmentation <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, and using consistency regularization as a component in a lar tency regularization as a component in a larger SSL pipeline <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Pseudo-labeling leverages the idea that we should use   using strong data augmentation can produce better results <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. These heavily-augmented examples are almost certainly outsid resholding). Using sharpening instead of an arg max introduces a hyper-parameter: the temperature T <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>.</p><p>We study the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ransformation/Stability or TS for short) <ref type=\"bibr\" target=\"#b38\">[39]</ref> or the \"\u03a0-Model\" <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Early extensions included using an exponential moving ave head><p>Artificial label post-processing Notes TS <ref type=\"bibr\" target=\"#b38\">[39]</ref>/\u03a0-Model <ref type=\"bibr\" target=\"#b35\">[36]</ref> Weak Weak None Temporal Ensembling <ref type=\"bibr\" target ers are found in section 2.</p><p>that are similar to FixMatch and/or are state-of-the-art: \u03a0-Model <ref type=\"bibr\" target=\"#b35\">[36]</ref>, Mean Teacher <ref type=\"bibr\" target=\"#b42\">[43]</ref>, P gment <ref type=\"bibr\" target=\"#b1\">[2]</ref> for strong-augmentation. All baseline models (\u03a0-Model <ref type=\"bibr\" target=\"#b35\">[36]</ref>, Pseudo-Labeling <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  than UDA<ref type=\"bibr\" target=\"#b44\">[45]</ref>. Our top-5 error rate is 10.87\u00b10.28%. While S 4 L<ref type=\"bibr\" target=\"#b47\">[48]</ref> holds state-of-the-art on semi-supervised ImageNet with a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>.</p><p>A popular cl  to predict the artificial label when fed the unlabeled image as input. For example, pseudolabeling <ref type=\"bibr\" target=\"#b21\">[22]</ref> (also called self-training <ref type=\"bibr\" target=\"#b26\"> h all produce heavily distorted versions of a given image. Following the approach of pseudolabeling <ref type=\"bibr\" target=\"#b21\">[22]</ref>, we only retain an artificial label if the model assigns a  only retaining artificial labels whose largest class probability fall above a predefined threshold <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Letting q b = p m (y|u b ), pseudolabeling uses the follo ns are converted to hard labels and are only retained when the classifier is sufficiently confident <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Some studies have suggested that pseudolabeling is not co ibr\" target=\"#b35\">[36]</ref>, Mean Teacher <ref type=\"bibr\" target=\"#b42\">[43]</ref>, Pseudo-Label <ref type=\"bibr\" target=\"#b21\">[22]</ref>, Mix-Match <ref type=\"bibr\" target=\"#b2\">[3]</ref>, UDA <r gmentation. All baseline models (\u03a0-Model <ref type=\"bibr\" target=\"#b35\">[36]</ref>, Pseudo-Labeling <ref type=\"bibr\" target=\"#b21\">[22]</ref>, Mean Teacher <ref type=\"bibr\" target=\"#b42\">[43]</ref>, M \"#b25\">[26]</ref>, object detection <ref type=\"bibr\" target=\"#b36\">[37]</ref>, image classification <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, domain adaptation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .</p><p>For \"strong\" augmentation, we experiment with two approaches which are based on AutoAugment <ref type=\"bibr\" target=\"#b8\">[9]</ref>. AutoAugment learns an augmentation strategy based on transf . To do so, a wide range of transformation magnitude values is divided into bins (as in AutoAugment <ref type=\"bibr\" target=\"#b8\">[9]</ref>) and a weight (initially set to 1) is assigned to each bin.   model prediction at training. This observation is well-aligned with those from supervised learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: experiments, we use simple weight decay regularization. We also found that using the Adam optimizer <ref type=\"bibr\" target=\"#b18\">[19]</ref> resulted in worse performance and instead use standard SGD <ref type=\"bibr\" target=\"#b41\">[42]</ref> is not required for achieving an error below 5%. For Adam <ref type=\"bibr\" target=\"#b18\">[19]</ref>, none of the combinations of parameters for \u03b7, \u03b2 1 , \u03b2 2 t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ate than that of standard momentum SGD, but the difference was not significant.</p><p>As studied in <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, we did not find A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ts using the same codebase. In particular, we use the same network architecture (a Wide ResNet-28-2 <ref type=\"bibr\" target=\"#b46\">[47]</ref> with 1.5M parameters) and training protocol, including the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: experiments, we use simple weight decay regularization. We also found that using the Adam optimizer <ref type=\"bibr\" target=\"#b18\">[19]</ref> resulted in worse performance and instead use standard SGD <ref type=\"bibr\" target=\"#b41\">[42]</ref> is not required for achieving an error below 5%. For Adam <ref type=\"bibr\" target=\"#b18\">[19]</ref>, none of the combinations of parameters for \u03b7, \u03b2 1 , \u03b2 2 t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>.</p><p>A popular cl  to predict the artificial label when fed the unlabeled image as input. For example, pseudolabeling <ref type=\"bibr\" target=\"#b21\">[22]</ref> (also called self-training <ref type=\"bibr\" target=\"#b26\"> h all produce heavily distorted versions of a given image. Following the approach of pseudolabeling <ref type=\"bibr\" target=\"#b21\">[22]</ref>, we only retain an artificial label if the model assigns a  only retaining artificial labels whose largest class probability fall above a predefined threshold <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Letting q b = p m (y|u b ), pseudolabeling uses the follo ns are converted to hard labels and are only retained when the classifier is sufficiently confident <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Some studies have suggested that pseudolabeling is not co ibr\" target=\"#b35\">[36]</ref>, Mean Teacher <ref type=\"bibr\" target=\"#b42\">[43]</ref>, Pseudo-Label <ref type=\"bibr\" target=\"#b21\">[22]</ref>, Mix-Match <ref type=\"bibr\" target=\"#b2\">[3]</ref>, UDA <r gmentation. All baseline models (\u03a0-Model <ref type=\"bibr\" target=\"#b35\">[36]</ref>, Pseudo-Labeling <ref type=\"bibr\" target=\"#b21\">[22]</ref>, Mean Teacher <ref type=\"bibr\" target=\"#b42\">[43]</ref>, M \"#b25\">[26]</ref>, object detection <ref type=\"bibr\" target=\"#b36\">[37]</ref>, image classification <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, domain adaptation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cus only on methods closely related to FixMatch. Broader introductions to the field are provided in <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>The maximum length limit in BERT naturally reminds us the limited capacity of Working Memory <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a human cognitive system storing information for logical re ttentional system capable of selecting and operating control processes and strategies\", as Baddeley <ref type=\"bibr\" target=\"#b1\">[2]</ref> pointed out in his 1992 classic. Later research detailed tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: w method, hence we fail to answer the question.</p><p>Pretrained language models, pioneered by BERT <ref type=\"bibr\" target=\"#b11\">[12]</ref>, have emerged as silver bullets for many NLP tasks, such a ficial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, even if the embeddings for larger positions are . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type=\"bibr\" target=\"#b12\">[13]</ref>, as the answer sentence fails to be directly retrieved by  re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a human cognitive system storing information for logical reasoning and decision-making. Experiments <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rks to extract important sentences in unsupervised ways, e.g. based on the metadata about structure <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Experiments on 4 different large datasets show its compet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hich is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p></div> <div xmlns=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r achieves comparable performance with the state-of-the-art results on four tasks, including NewsQA <ref type=\"bibr\" target=\"#b43\">[44]</ref>, HotpotQA <ref type=\"bibr\" target=\"#b52\">[53]</ref>, 20New ask is to predict the answer span in the paragraph. We evaluate the performance of CogLTX on NewsQA <ref type=\"bibr\" target=\"#b43\">[44]</ref>, which contains 119,633 human-generated questions posed on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we can handle long texts with CogLTX, the prob. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion for logical reasoning and decision-making. Experiments <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> already showed that t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, but currently few of them have been successfully applied t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s posed on 12,744 long news articles. <ref type=\"foot\" target=\"#foot_2\">3</ref> Since previous SOTA <ref type=\"bibr\" target=\"#b42\">[43]</ref> is not BERT based (due to long texts) in NewsQA, to keep t , for example BiDAF <ref type=\"bibr\" target=\"#b40\">[41]</ref> (+17.8% F 1 ), previous SOTA DECAPROP <ref type=\"bibr\" target=\"#b42\">[43]</ref>, which incorporates elaborate self-attention and RNN mecha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are for normalized benchmarks, for example SQuAD <ref type=\"bibr\" target=\"#b37\">[38]</ref> and GLUE <ref type=\"bibr\" target=\"#b46\">[47]</ref>, but very common for more complex tasks <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a human cognitive system storing information for logical reasoning and decision-making. Experiments <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: key-blocks assumption is strongly related to latent variable models, which are usually solved by EM <ref type=\"bibr\" target=\"#b10\">[11]</ref> or variational bayes <ref type=\"bibr\" target=\"#b18\">[19]</ e viewed as a generalization of (conditional) latent variable model p(y|x; \u03b8) \u221d p(z|x)p(y|z; \u03b8). EM <ref type=\"bibr\" target=\"#b10\">[11]</ref> infers the distribution of z as posterior p(z|y, x; \u03b8) in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion for logical reasoning and decision-making. Experiments <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> already showed that t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type=\"bibr\" target=\"#b12\">[13]</ref>, as the answer sentence fails to be directly retrieved by  re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: key-blocks assumption is strongly related to latent variable models, which are usually solved by EM <ref type=\"bibr\" target=\"#b10\">[11]</ref> or variational bayes <ref type=\"bibr\" target=\"#b18\">[19]</ e viewed as a generalization of (conditional) latent variable model p(y|x; \u03b8) \u221d p(z|x)p(y|z; \u03b8). EM <ref type=\"bibr\" target=\"#b10\">[11]</ref> infers the distribution of z as posterior p(z|y, x; \u03b8) in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a human cognitive system storing information for logical reasoning and decision-making. Experiments <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b41\">42]</ref>, but currently few of them have been successfully applied to BERT <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p><p>The maximum l oup attention, but it is not friendly to GPU and still needs verification for BERT usage. BlockBERT <ref type=\"bibr\" target=\"#b34\">[35]</ref> cuts off unimportant attention heads to scale up BERT from. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re\">1</ref>). Since the problem roots in the high O(L 2 ) time and space complexity in transformers <ref type=\"bibr\" target=\"#b45\">[46]</ref> (L is the length of the text), another line of research at. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type=\"bibr\" target=\"#b12\">[13]</ref>, as the answer sentence fails to be directly retrieved by  re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: preprocessing step and performs standard logistic regression to remove redundant computation. PPRGo <ref type=\"bibr\" target=\"#b3\">[4]</ref> uses Personalized PageRank to capture multi-hop neighborhood cency matrix \u00c3 and feature matrix X in the precomputation phase, which requires O(LmF ) time. PPRGo <ref type=\"bibr\" target=\"#b3\">[4]</ref> calculates approximate the Personalized PageRank (PPR) matri in APPNP and PPRGo <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>; 2) w = 0 for = 0, . . . , L \u2212 1 and w L = 1, in which case P br\" target=\"#b36\">[37]</ref>, SGC and PPRGo (linear model) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p><p>We implement GBP in PyTorch and C++, and employ initia. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g has been adapted in various works on community detection <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. For each node, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (GNNs) has drawn increasing attention due to its wide range of applications such as social analysis <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al activations to restrict the number of sampled nodes and reduce the variance of sampling. FastGCN <ref type=\"bibr\" target=\"#b4\">[5]</ref> samples nodes of each layer independently based on each node chieve a better convergence rate. However, it suffers from worse time and space complexity. FastGCN <ref type=\"bibr\" target=\"#b4\">[5]</ref> and LADIES <ref type=\"bibr\" target=\"#b39\">[40]</ref> restric. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ws that GNNs are not better than the Weisfeiler-Lehman test in distinguishing graph structures. GDC <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposes to replace the graph convolutional matrix \u00c3 with  >, respectively. We can also manipulate the weights w to simulate various diffusion processes as in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, we will mainly focus on two setups in this paper s GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>, GDC <ref type=\"bibr\" target=\"#b16\">[17]</ref> and APPNP <ref type=\"bibr\" target=\"#b15\">[16]</ref> as the  becomes the Personalized PageRank used in APPNP and PPRGo <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>; 2) w = 0 for = 0, . . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: asing networks Amazon <ref type=\"bibr\" target=\"#b7\">[8]</ref> and a large social network Friendster <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Table <ref type=\"table\" target=\"#tab_1\">2</ref> summarize le GNNs on a billion-scale graph Friendster. We extracted the top-500 ground-truth communities from <ref type=\"bibr\" target=\"#b33\">[34]</ref> and use the community ids as the labels of each node. Note. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rsonalized PageRank to capture multi-hop neighborhood information and uses a forward push algorithm <ref type=\"bibr\" target=\"#b1\">[2]</ref> to accelerate computation.</p><p>While the above methods sig ulates approximate the Personalized PageRank (PPR) matrix \u221e =0 \u03b1(1 \u2212 \u03b1) \u00c3 by forward push algorithm <ref type=\"bibr\" target=\"#b1\">[2]</ref> and then applies the PPR matrix to the feature matrix X to d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >20,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, biology <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, recommendation systems <ref type=\"bibr\" target=\"#b35\">[36]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to attention model <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, where the adjacency matrix of each layer is replaced by a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. Graph Convolutional Network (GCN) <ref type=\"bibr\" target=\"#b14\">[15]</ref> adopts a message-passing approach and gathers information  vertices and edges, respectively. For ease of presentation, we assume that G is a self-looped graph <ref type=\"bibr\" target=\"#b14\">[15]</ref>, with a self-loop attached to each node in V . Let n = |V   and d(u) = |N (u)| is the degree of u. We use d = m n to denote the average degree of G. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we define the normalized adjacency matrix of G as \u00c3 = D \u2212 ork Friendster.</p><p>Baselines and detailed setup. We adopt three state-of-the-art GNN methods GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>, GDC <ref t e node classification task on the three small standard graphs Cora, Citeseer, and Pubmed. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we apply the standard fixed training/validation/testing s 0 and 1, the convolution matrix D r\u22121 AD \u2212r represents the symmetric normalization adjacency matrix <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [23,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, biology <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, recommendation sys. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: where permutation equivariance is either learned from data <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> or obtained by design <ref type=\"bibr\" target=\"#b23\">[24]</ ]</ref>. With node features acting as identifiers, MPNN were shown to become universal in the limit <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which implies that they can solve the graph isomorphism t d evidence that the power of MPNN grows as a function of depth and width for certain graph problems <ref type=\"bibr\" target=\"#b22\">[23]</ref>, showing that (both anonymous and non-anonymous) MPNN cann ether depth and width needs to grow with the number of nodes solely in the worst-case (as proven in <ref type=\"bibr\" target=\"#b22\">[23]</ref>) or with certain probability over the input distribution.< apacity is an effective generalization of the previously considered product between depth and width <ref type=\"bibr\" target=\"#b22\">[23]</ref>, being able to consolidate more involved properties, as we lower bounds rely on a new technique which renders them applicable not only to worst-case instances <ref type=\"bibr\" target=\"#b22\">[23]</ref>, but in expectation over the input distribution.</p><p>An  previous theoretical findings that non-anonymous MPNN are universal and can solve graph isomorphism <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, as well as that the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: re has been that of message-passing neural networks (MPNN). Since its inception by Scarselli et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref>, MPNN has been extended to include edge <ref type=\"bibr\" tar optimization procedure. It is common to parametrize these functions by feed-forward neural networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ label>(7)</label></formula><p>Substituting ( <ref type=\"formula\" target=\"#formula_33\">6</ref>) into <ref type=\"bibr\" target=\"#b6\">(7)</ref> gives:</p><formula xml:id=\"formula_35\">c both fisom log 2 s . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>, as well as the analysis of the power of particular architectures to compute graph properties <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and to distinguish. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> or obtained by design <ref type=\"bibr\" target=\"#b23\">[24]</ref>. With node features acting as identifiers, MPNN were shown. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9]</ref>. The rational is that, by the universal approximation theorem and its variants <ref type=\"bibr\" target=\"#b35\">[36]</ref><ref type=\"bibr\" target=\"#b36\">[37]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9]</ref>. The rational is that, by the universal approximation theorem and its variants <ref type=\"bibr\" target=\"#b35\">[36]</ref><ref type=\"bibr\" target=\"#b36\">[37]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: under what conditions current networks can (or perhaps cannot) distinguish between different graphs <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: under what conditions current networks can (or perhaps cannot) distinguish between different graphs <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe mutation equivariant by design. Xu et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Morris et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> established the equivalence of anonymous MPNN to the 1st-ord en MPNN can also be analyzed by equivalence to the 1-WL test <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. For trees, the 1-WL test requires n iterations because there ng injective aggregation functions (i.e., of unbounded width <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>), the equivalence doe _0\">1c</ref>). Specifically, methods for isomorphism testing <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> that compare graphs G a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b41\">[42]</ref><ref type=\"bibr\" target=\"#b42\">[43]</ref><ref type=\"bibr\" target=\"#b43\">[44]</ref><ref type=\"bibr\" target=\"#b44\">[45]</ref>.</p><p>Global state. In the description above, all message. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ults may also be easily extended to account for coarsening <ref type=\"bibr\" target=\"#b40\">[41]</ref><ref type=\"bibr\" target=\"#b41\">[42]</ref><ref type=\"bibr\" target=\"#b42\">[43]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ossibility to translate programming languages with machine translation. For instance, Nguyen et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> trained a Phrase-Based Statistical Machine Translation (PB  3. Chen et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> used the Java-C# dataset of Nguyen et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> to translate code with tree-to-tree neural networks.</p><p arget=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, which is not a reliable metric, as a generation can be a v arget=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, or other metrics based on the relative overlap between the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> and statistical models <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. We describe now som train it using the three principles of unsupervised machine translation identified in Lample et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref>, namely initialization, language modeling, and back-transl training</head><p>Pretraining is a key ingredient of unsupervised machine translation Lample et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref>. It ensures that sequences with a similar meaning are mapp an important component of unsupervised machine translation <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>In the unsuper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ref>. Recent approaches have also investigated the use of neural approaches for code decompilation <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. For instance, Kat \">[12]</ref>. Leveraging the compiler output or other approaches such as iterative error correction <ref type=\"bibr\" target=\"#b15\">[16]</ref> could also boost the performance.</p></div> <div xmlns=\"ht. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b8\">9]</ref>, composed of an encoder and a decoder with a transformer architecture <ref type=\"bibr\" target=\"#b44\">[45]</ref>. We use a single shared model for all programming language l. The transformer decoder, however, has extra parameters related to the source attention mechanism <ref type=\"bibr\" target=\"#b44\">[45]</ref>. Following Lample and Conneau <ref type=\"bibr\" target=\"#b2 f type=\"foot\" target=\"#foot_2\">4</ref> , and use the same learning rate scheduler as Vaswani et al. <ref type=\"bibr\" target=\"#b44\">[45]</ref>. We implement our models in PyTorch <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ranslation systems <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. More recently, sev o. Initially introduced to improve the performance of machine translation in the supervised setting <ref type=\"bibr\" target=\"#b40\">[41]</ref>, back-translation turned out to be an important component . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">34]</ref>, or error detection <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Recent approaches . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on existing transcompilers, to create parallel data. Moreover, they essentially rely on BLEU score <ref type=\"bibr\" target=\"#b37\">[38]</ref> to evaluate their translations <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ranslation systems <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. More recently, sev o. Initially introduced to improve the performance of machine translation in the supervised setting <ref type=\"bibr\" target=\"#b40\">[41]</ref>, back-translation turned out to be an important component . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ord embeddings and aligning them in an unsupervised manner <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>.</p><p>Subsequent work showed that pretraining the entire mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b35\">[36]</ref> trained a Phrase-Based Statistical Machine Translation (PBSMT) model, Moses <ref type=\"bibr\" target=\"#b26\">[27]</ref>, on a Java-C# parallel corpus. They created their dataset . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y<ref type=\"foot\" target=\"#foot_0\">2</ref> developed to port Python 2 code to Python 3. Chen et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> used the Java-C# dataset of Nguyen et al. <ref type=\"bibr\" eference match, i.e. the percentage of translations that perfectly match the ground truth reference <ref type=\"bibr\" target=\"#b11\">[12]</ref>. A limitation of these metrics is that they do not take in  ensure that the generated functions are syntactically correct, or by using dedicated architectures <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Leveraging the compiler output or other approaches such a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>. Recent approaches have also investigated the use of neural approaches for code decompilation <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. For instance, Kat \">[12]</ref>. Leveraging the compiler output or other approaches such as iterative error correction <ref type=\"bibr\" target=\"#b15\">[16]</ref> could also boost the performance.</p></div> <div xmlns=\"ht. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ively from monolingual corpora, using either neural models <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> and statistical models <ref type=\"bibr\" target=\"#b31\">[32,</r ializing the model with cross-lingual word representations <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. In the context of unsupervised English-French translation, t achine translation <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>In the unsupervised setting, a source-to-target model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e is given in Figure <ref type=\"figure\" target=\"#fig_3\">3</ref> in the appendix. We learn BPE codes <ref type=\"bibr\" target=\"#b41\">[42]</ref> on extracted tokens, and split tokens into subword units. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on existing transcompilers, to create parallel data. Moreover, they essentially rely on BLEU score <ref type=\"bibr\" target=\"#b37\">[38]</ref> to evaluate their translations <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ively from monolingual corpora, using either neural models <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> and statistical models <ref type=\"bibr\" target=\"#b31\">[32,</r ializing the model with cross-lingual word representations <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. In the context of unsupervised English-French translation, t achine translation <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>In the unsupervised setting, a source-to-target model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">34]</ref>, or error detection <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Recent approaches . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">34]</ref>, or error detection <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Recent approaches . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> developed a tool to mine parallel datasets from ported open source projects. Aggarwal et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> trained Moses on a Python 2 to Python 3 parallel corpus crea ntially rely on BLEU score <ref type=\"bibr\" target=\"#b37\">[38]</ref> to evaluate their translations <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe tudies in source code translation use the BLEU score to evaluate the quality of generated functions <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: achine translation <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. In particular, we follow the pretraining strategy of Lampl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s to code suggestion <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, or error detection <ref type=\"bibr\" target=\"#b12\">[13,</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on existing transcompilers, to create parallel data. Moreover, they essentially rely on BLEU score <ref type=\"bibr\" target=\"#b37\">[38]</ref> to evaluate their translations <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, in the re ifferent neighbors more precisely as a weighted average of the ego-and neighbor-features. GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> generalizes the aggregation beyond averaging, and models t -and the aggregated neighbor-embeddings without 'mixing' them is with concatenation as in GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>-rather than averaging all of them as in the GCN model by K ef type=\"bibr\" target=\"#b35\">[36]</ref> GCN-Cheby <ref type=\"bibr\" target=\"#b6\">[7]</ref> GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> H2GCN (prop ding transformations per round in H 2 GCN? GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> and other GNN models embed the intermediate representation &amp; GCN-Cheby <ref type=\"bibr\" target=\"#b16\">[17]</ref>: https://github.com/tkipf/gcn \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch  : 3 * early_stopping: 40 We report the best performance, for Set 1 with a = 64, b = 5e-4.\u2022 GraphSAGE<ref type=\"bibr\" target=\"#b10\">[11]</ref>:-hid_units: a \u2208 {64, 128} lr: b \u2208 {0.1, 0.7} epochs: 500</ led the default feature normalization in the official implementation for this baseline. \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>:</p><p>-hid_units: a \u2208 {64, 128} lr: b \u2208 {0.1, 0.7} epochs led the default feature normalization in the official implementation for this baseline. \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>\u2022 M intermediate representations. While these designs have been utilized separately in some prior works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" targ 1)d)</formula><p>Solving the above inequality for \u03b4 1 , we get the amount of perturbation needed as <ref type=\"bibr\" target=\"#b10\">(11)</ref> and the least absolute amount of perturbation needed is |\u03b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we are the first to discuss their importance under heterop  COMBINE functions that leverage each representation separately-e.g., concatenation, LSTM-attention <ref type=\"bibr\" target=\"#b37\">[38]</ref>. This design is introduced in jumping knowledge networks <  <ref type=\"bibr\" target=\"#b37\">[38]</ref>. This design is introduced in jumping knowledge networks <ref type=\"bibr\" target=\"#b37\">[38]</ref> and shown to increase the representation power of GCNs und ons from two rounds with the embedded ego-representation (following the jumping knowledge framework <ref type=\"bibr\" target=\"#b37\">[38]</ref>), GCN's accuracy increases to 58.93%\u00b13.17 for h = 0.1, a 2 label>(7)</label></formula><p>where we empirically find concatenation works better than max-pooling <ref type=\"bibr\" target=\"#b37\">[38]</ref> as the COMBINE function.</p><p>In the classification stage e compare GraphSAGE, GCN-Cheby and GCN to their corresponding variants enhanced with JK connections <ref type=\"bibr\" target=\"#b37\">[38]</ref>. GCN and GCN-Cheby benefit significantly from D3 in hetero with and without JK connections is similar (gaps mostly less than 2%), matching the observations in <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p><p>While other design choices and implementation detai K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=\"bibr\" target=\"#b37\">[38]</ref> without changing the number of layers or other hyperparame K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=\"bibr\" target=\"#b37\">[38]</ref> without changing the number of layers or other hyperparame. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cy matrix with a sparsified version of a diffusion matrix (e.g., heat kernel or PageRank). Geom-GCN <ref type=\"bibr\" target=\"#b25\">[26]</ref> precomputes unsupervised node embeddings and uses neighbor  0.7), and across the full spectrum (\"Overall\"). The \"*\" denotes ranks based on results reported in <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Real datasets &amp; setup We now evaluate the performance of nodes per class for train/validation/test<ref type=\"foot\" target=\"#foot_2\">2</ref> ) provided by <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>For Cora-Full, we generate 3 random splits, with 25 the best results among the three recentlyproposed GEOM-GCN variants ( \u00a7 4), directly from the paper <ref type=\"bibr\" target=\"#b25\">[26]</ref>: other models (including ours) outperform this method sign ng universities, originally collected by the CMU WebKB project. We used the preprocessed version in <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In these networks, nodes are web pages, which are classif br\" target=\"#b28\">[29]</ref>. For the classification task, we utilize the class labels generated by <ref type=\"bibr\" target=\"#b25\">[26]</ref>, where the nodes are categorized into 5 classes based on t erage traffic. \u2022 Actor is a graph representing actor co-occurrence in Wikipedia pages, processed by <ref type=\"bibr\" target=\"#b25\">[26]</ref> based on the film-director-actor-writer network in <ref ty ter network in <ref type=\"bibr\" target=\"#b34\">[35]</ref>. We also use the class labels generated by <ref type=\"bibr\" target=\"#b25\">[26]</ref>. \u2022 Cora, Pubmed and Citeseer are citation graphs originall fferent data splits. Best model per benchmark highlighted in gray. The \"*\" results are obtained from<ref type=\"bibr\" target=\"#b25\">[26]</ref> and \"N/A\" denotes non-reported results.</figDesc><table><r ix of A + I.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_2\"><ref type=\"bibr\" target=\"#b25\">[26]</ref> claims that the ratios are 60%/20%/20%, which is different atent space to define graph convolution. Some of these works <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> acknowledge the cha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oposed architectures <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Recent work has investigated GNN's ability to capture grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  We now evaluate the performance of our model and existing GNNs on a variety of real-world datasets <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta sed by <ref type=\"bibr\" target=\"#b25\">[26]</ref> based on the film-director-actor-writer network in <ref type=\"bibr\" target=\"#b34\">[35]</ref>. We also use the class labels generated by <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> with edge homophily r tion as the feature vector for each node. \u2022 Cora Full is an extended version of Cora, introduced in <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, which contain more . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nodes often belong to the same class or have similar features (\"birds of a feather flock together\") <ref type=\"bibr\" target=\"#b20\">[21]</ref>. For example, friends are likely to have similar political. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: at some pairs of classes exhibit homophily, while others exhibit heterophily. In belief propagation <ref type=\"bibr\" target=\"#b39\">[40]</ref>, a message-passing algorithm used for inference on graphic =\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. Belief propagation (BP) <ref type=\"bibr\" target=\"#b39\">[40]</ref> is a classic messagepassing algorithm for graph-based semi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ked into formulations which can better address heterophily: Before applying label propagation, Peel <ref type=\"bibr\" target=\"#b24\">[25]</ref> transforms the original graph into either a similarity gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oposed architectures <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Recent work has investigated GNN's ability to capture grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targe  and Citeseer are citation graphs originally introduced in <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, which are among the most widely used benchmarks for semi-s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: at some pairs of classes exhibit homophily, while others exhibit heterophily. In belief propagation <ref type=\"bibr\" target=\"#b39\">[40]</ref>, a message-passing algorithm used for inference on graphic =\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. Belief propagation (BP) <ref type=\"bibr\" target=\"#b39\">[40]</ref> is a classic messagepassing algorithm for graph-based semi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aph information, proposing diagnostic measurements based on feature smoothness and label smoothness <ref type=\"bibr\" target=\"#b11\">[12]</ref> that may guide the learning process. To capture more graph  Some of these works <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> acknowledge the challenges of learning from graphs with het. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  context. This observation is also confirmed by recent works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> in the context of binary attribute prediction.</p><p>Theoreti rity between node neighborhoods or a new graph connecting nodes that are two hops away; Chin et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> decouple graph smoothing where the notion of \"identity\" and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>. In general, the synthetic graphs are generated by a modified preferential attachment process <ref type=\"bibr\" target=\"#b2\">[3]</ref>: The number of class labels |Y| in the synthetic graph is pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e complex label correlations by integrating the compatibility matrix notion from belief propagation <ref type=\"bibr\" target=\"#b9\">[10]</ref> into GNNs. GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref> GA homophily or heterophily <ref type=\"bibr\" target=\"#b18\">[19]</ref> and has fast linearized versions <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Different from the s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  j [A] i,j . Without loss of generality, since the eigenvalues {\u03bb i } of L are real and nonnegative <ref type=\"bibr\" target=\"#b31\">[32]</ref>, we assume the following order for the eigenvalues of L:</ closer to \u03bb max would correspond to higher-frequency components. Interested readers are referred to <ref type=\"bibr\" target=\"#b31\">[32]</ref> for further details regarding signal processing on graphs.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dominant and thus provide more relevant context. This observation is also confirmed by recent works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> in the context of bina. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Y t as before, with the modification that in this case we have for signal Y s (similarly for Y t ): <ref type=\"bibr\" target=\"#b19\">(20)</ref>. The rest of the proof is similar to Proof 3.</p><formula  imate inference algorithms (e.g., iterative classification <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eal-world datasets <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar  of node classification by leveraging the correlations between the node labels and their attributes <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Since exact inference is NP-hard, approximate inference a d by sampling feature vectors of nodes from the corresponding class in a real benchmark (e.g., Cora <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> or ogbn-products <  target=\"#b25\">[26]</ref>. \u2022 Cora, Pubmed and Citeseer are citation graphs originally introduced in <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, which are among t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oposed architectures <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Recent work has investigated GNN's ability to capture grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  context. This observation is also confirmed by recent works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> in the context of binary attribute prediction.</p><p>Theoreti rity between node neighborhoods or a new graph connecting nodes that are two hops away; Chin et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> decouple graph smoothing where the notion of \"identity\" and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Y t as before, with the modification that in this case we have for signal Y s (similarly for Y t ): <ref type=\"bibr\" target=\"#b19\">(20)</ref>. The rest of the proof is similar to Proof 3.</p><formula  imate inference algorithms (e.g., iterative classification <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  context. This observation is also confirmed by recent works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> in the context of binary attribute prediction.</p><p>Theoreti rity between node neighborhoods or a new graph connecting nodes that are two hops away; Chin et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> decouple graph smoothing where the notion of \"identity\" and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  context. This observation is also confirmed by recent works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> in the context of binary attribute prediction.</p><p>Theoreti rity between node neighborhoods or a new graph connecting nodes that are two hops away; Chin et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> decouple graph smoothing where the notion of \"identity\" and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Since exact inference is NP-hard, approximate inference algorithms (e.g., iterative classification <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, loopy belief prop. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> with edge homophily ratio h ranging from strong heterophily e. \u2022 Cora Full is an extended version of Cora, introduced in <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, which contain more papers and research fields than Cora. T >22]</ref>, which are among the most widely used benchmarks for semi-supervised node classification <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. Each node is assi of words representation as the feature vector for each node.</p><p>Data Limitations As discussed in <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, Cora, Pubmed and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> with edge homophily ratio h ranging from strong heterophily e. \u2022 Cora Full is an extended version of Cora, introduced in <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, which contain more papers and research fields than Cora. T >22]</ref>, which are among the most widely used benchmarks for semi-supervised node classification <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. Each node is assi of words representation as the feature vector for each node.</p><p>Data Limitations As discussed in <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, Cora, Pubmed and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  apart from MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> (cf. \u00a7 3.1), Graph Diffusion Convolution <ref type=\"bibr\" target=\"#b17\">[18]</ref> replaces the adjacency matrix with a sparsified version of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  designs have been utilized separately in some prior works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=  applied to different neighborhoods can be the same or different. This design-employed in GCN-Cheby <ref type=\"bibr\" target=\"#b6\">[7]</ref> and MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>-augments  rhoods by combining Chebyshev polynomials to approximate a higher-order graph convolution operation <ref type=\"bibr\" target=\"#b6\">[7]</ref>, outperforms GCN and GAT, which aggregate over only the imme N <ref type=\"bibr\" target=\"#b16\">[17]</ref> GAT <ref type=\"bibr\" target=\"#b35\">[36]</ref> GCN-Cheby <ref type=\"bibr\" target=\"#b6\">[7]</ref> GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> MixHop <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oposed architectures <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Recent work has investigated GNN's ability to capture grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs are more likely to connect to accomplices than to other fraudsters in online purchasing networks <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Since many existing GNNs assume strong homophily, they fa  tendency of connection between each pair of classes. For instance, in an online purchasing network <ref type=\"bibr\" target=\"#b23\">[24]</ref> with three classes-fraudsters, accomplices, and honest use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph-based semi-supervised learning, which can be used for graphs exhibiting homophily or heterophily <ref type=\"bibr\" target=\"#b18\">[19]</ref> and has fast linearized versions <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: high memory footprint, both of which grow quadratically with respect to the image height (or width) <ref type=\"bibr\" target=\"#b37\">[38]</ref>. In real-world applications like content-based image searc ployed for higher efficiency. This differentiates our method from early recurrent attention methods <ref type=\"bibr\" target=\"#b37\">[38]</ref> which adopt pure recurrent models. In addition, we focus o \">8]</ref>.</p><p>One similar work to our GFNet is the recurrent visual attention model proposed in <ref type=\"bibr\" target=\"#b37\">[38]</ref>. However, our method differs from it in two important aspe ults with only a few class-discriminative patches, such as the head of a dog or the wings of a bird <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  configurations as <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>. In our implementation, we estimate the confidence threshol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ted to extract information from some task-relevant regions <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpetitive ILSVRC <ref type=\"bibr\" target=\"#b8\">[9]</ref> competition with 224\u00d7224 or 320\u00d7320 images <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  approach is to ensemble multiple models, and selectively execute a subset of them in the cascading <ref type=\"bibr\" target=\"#b3\">[4]</ref> or mixing <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b42\">[43]</ref>, CondenseNets <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNets <ref type=\"bibr\" target=\"#b58\">[59]</ref>, ProxylessNAS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, Ski. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: should be minimized for both safety and economical reasons <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>.</p><p>In this pape </ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, CondenseNet <ref type=\"bibr\" target=\"#b20\">[21]</ref>, ShuffleNets <ref type=\"bibr\" target=\"#b65\">[66,</ref><ref </ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, CondenseNet <ref type=\"bibr\" target=\"#b20\">[21]</ref>, ShuffleNets <ref type=\"bibr\" target=\"#b65\">[66,</ref><ref br\" target=\"#b35\">[36]</ref>, MobileNets-V2 <ref type=\"bibr\" target=\"#b42\">[43]</ref>, CondenseNets <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNets <ref type=\"bibr\" target=\"#b58\">[59]</ref>, Proxyle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oaches have been proposed to reduce the redundant computation in the spatial dimension. The OctConv <ref type=\"bibr\" target=\"#b5\">[6]</ref> reduces the spatial resolution by using low-frequency featur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  or 320\u00d7320 images <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > with various efficient CNNs (e.g., MobileNet-V3 <ref type=\"bibr\" target=\"#b15\">[16]</ref>, RegNet <ref type=\"bibr\" target=\"#b39\">[40]</ref>, EfficientNet <ref type=\"bibr\" target=\"#b49\">[50]</ref>, e l state-of-the-art CNNs, including MobileNet-V3 <ref type=\"bibr\" target=\"#b15\">[16]</ref>, RegNet-Y <ref type=\"bibr\" target=\"#b39\">[40]</ref>, EfficientNet <ref type=\"bibr\" target=\"#b49\">[50]</ref>, R  in Image Classification</head><p>A Implementation Details A.1 Recurrent Networks</p><p>For RegNets <ref type=\"bibr\" target=\"#b39\">[40]</ref>, MobileNets-V3 <ref type=\"bibr\" target=\"#b15\">[16]</ref> a pe=\"bibr\" target=\"#b13\">[14]</ref>, DenseNets <ref type=\"bibr\" target=\"#b21\">[22]</ref> and RegNets <ref type=\"bibr\" target=\"#b39\">[40]</ref>, we use a GRU with 1024 hidden units. For MobileNets-V3 <r \"bibr\" target=\"#b38\">[39]</ref>, for RegNets, we use the pre-trained models provided by their paper <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and for MobileNets-V3 and EfficientNets, we first train t e training process <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  patches, such as the head of a dog or the wings of a bird <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. These regions are ty rget=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>One similar wo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h each other. Current state-of-the-art models highly depend on Graph Convoluational Networks (GCNs) <ref type=\"bibr\" target=\"#b12\">[13]</ref> originated from the theory of Graph Fourier Transform (GFT tp://www.tei-c.org/ns/1.0\"><head>Spectral Graph Convolution</head><p>The Spectral Graph Convolution <ref type=\"bibr\" target=\"#b12\">[13]</ref> is composed of three steps.</p><p>(1) The multivariate tim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: m (DFT) is also useful for time-series analysis. For instance, State Frequency Memory (SFM) network <ref type=\"bibr\" target=\"#b31\">[32]</ref> combines the advantages of DFT and LSTM jointly for stock  target=\"#b29\">[30]</ref> forecasts univariate time-series with LSTM and fully-connected layers. SMF <ref type=\"bibr\" target=\"#b31\">[32]</ref> improves the LSTM model by breaking down the cell states o ith other state-of-the-art models, including FC-LSTM <ref type=\"bibr\" target=\"#b25\">[26]</ref>, SFM <ref type=\"bibr\" target=\"#b31\">[32]</ref>, N-BEATS <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DCRNN  elations</cell><cell></cell></row></table><note>FC-LSTM<ref type=\"bibr\" target=\"#b25\">[26]</ref> SFM<ref type=\"bibr\" target=\"#b31\">[32]</ref> N-BEATS<ref type=\"bibr\" target=\"#b18\">[19]</ref> TCN<ref t get=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the theory of Graph Fourier Transform (GFT). These models <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> stack GCN and temporal modules (e.g., LSTM, GRU) directly,  variate techniques <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar <ref type=\"bibr\" target=\"#b31\">[32]</ref>, N-BEATS <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DCRNN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LSTNet <ref type=\"bibr\" target=\"#b13\">[14]</ref>, ST-GCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e divided into two major categories: univariate techniques <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar ndividual time-series separately without considering the correlations between different time-series <ref type=\"bibr\" target=\"#b21\">[22]</ref>. For example, FC-LSTM <ref type=\"bibr\" target=\"#b29\">[30]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nes the advantages of DFT and LSTM jointly for stock price prediction; Spectral Residual (SR) model <ref type=\"bibr\" target=\"#b22\">[23]</ref> leverages DFT and achieves state-of-the-art performances i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nes the advantages of DFT and LSTM jointly for stock price prediction; Spectral Residual (SR) model <ref type=\"bibr\" target=\"#b22\">[23]</ref> leverages DFT and achieves state-of-the-art performances i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nes the advantages of DFT and LSTM jointly for stock price prediction; Spectral Residual (SR) model <ref type=\"bibr\" target=\"#b22\">[23]</ref> leverages DFT and achieves state-of-the-art performances i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the theory of Graph Fourier Transform (GFT). These models <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> stack GCN and temporal modules (e.g., LSTM, GRU) directly,  variate techniques <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar <ref type=\"bibr\" target=\"#b31\">[32]</ref>, N-BEATS <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DCRNN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LSTNet <ref type=\"bibr\" target=\"#b13\">[14]</ref>, ST-GCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Univariate techniq. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Univariate techniq. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the theory of Graph Fourier Transform (GFT). These models <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> stack GCN and temporal modules (e.g., LSTM, GRU) directly,  variate techniques <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar <ref type=\"bibr\" target=\"#b31\">[32]</ref>, N-BEATS <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DCRNN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LSTNet <ref type=\"bibr\" target=\"#b13\">[14]</ref>, ST-GCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tured data. In particular, the Information Bottleneck (IB) <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> provides a critical principle for representation learning: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a lower bound of I(Y ; Z (L) X ), which is reproduced from <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, and an upper bound of I(D; Z </p><formula xml:id=\"formula_ f. We use the Nguyen, Wainright &amp; Jordan's bound I NWJ <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>: Lemma B.1. <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref t </ref><ref type=\"bibr\" target=\"#b22\">23]</ref>: Lemma B.1. <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> For any two random variables X 1 , X 2 and any function g :. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe target.</p><p>We demonstrate the GIB principle by applying it to the Graph Attention Networks (GAT) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, where we leverage the attention weights of GAT to sample th ning. In the next subsection, we will introduce two instantiations of GIB, which is inspired by GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=  can be applied to many GNN models. As an example, we apply it to the Graph Attention Network model <ref type=\"bibr\" target=\"#b4\">[5]</ref> and present GIB-Cat and GIB-Bern. Algorithm 1 illustrates th resentations will be sampled. Note that we may also use a mechanism similar to multi-head attention <ref type=\"bibr\" target=\"#b4\">[5]</ref>: We split Z(l\u22121)</p><p>X into different channels w.r.t. its  e GIB-Cat and GIB-Bern with baselines including GCN <ref type=\"bibr\" target=\"#b2\">[3]</ref> and GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the most relevant baseline as GIB-Cat and GIB-Bern are to i he standard transductive node classification setting and standard trainvalidation-test split as GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The summary statistics of the datasets and their splitting   and GIB-Bern follows Alg. 1 (and Alg. 2 and 3 for the respective neighbor-sampling). We follow GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>'s default architecture, in which we use 8 attention heads, n rporate the attention mechanism to adaptively learn the correlation between a node and its neighbor <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Recent literature s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe target.</p><p>We demonstrate the GIB principle by applying it to the Graph Attention Networks (GAT) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, where we leverage the attention weights of GAT to sample th ning. In the next subsection, we will introduce two instantiations of GIB, which is inspired by GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=  can be applied to many GNN models. As an example, we apply it to the Graph Attention Network model <ref type=\"bibr\" target=\"#b4\">[5]</ref> and present GIB-Cat and GIB-Bern. Algorithm 1 illustrates th resentations will be sampled. Note that we may also use a mechanism similar to multi-head attention <ref type=\"bibr\" target=\"#b4\">[5]</ref>: We split Z(l\u22121)</p><p>X into different channels w.r.t. its  e GIB-Cat and GIB-Bern with baselines including GCN <ref type=\"bibr\" target=\"#b2\">[3]</ref> and GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the most relevant baseline as GIB-Cat and GIB-Bern are to i he standard transductive node classification setting and standard trainvalidation-test split as GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The summary statistics of the datasets and their splitting   and GIB-Bern follows Alg. 1 (and Alg. 2 and 3 for the respective neighbor-sampling). We follow GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>'s default architecture, in which we use 8 attention heads, n rporate the attention mechanism to adaptively learn the correlation between a node and its neighbor <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Recent literature s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive edges <ref type=\"bibr\" target=\"#b33\">[34]</ref>, low-rank approximation of the adjacency matrix <ref type=\"bibr\" target=\"#b34\">[35]</ref>, additional hinge loss for certified robustness <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l\u22121) X , A). Generally speaking, any node-pair representations, such as messages over edges in MPNN <ref type=\"bibr\" target=\"#b28\">[29]</ref>, can be leveraged to sample structures. Applying the GIB p egation from neighbors <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures as well as graph structure carry important information <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b0\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref> have demonstrated impressive performance, by learning to fus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a lower bound of I(Y ; Z (L) X ), which is reproduced from <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, and an upper bound of I(D; Z </p><formula xml:id=\"formula_ f. We use the Nguyen, Wainright &amp; Jordan's bound I NWJ <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>: Lemma B.1. <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref t </ref><ref type=\"bibr\" target=\"#b22\">23]</ref>: Lemma B.1. <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> For any two random variables X 1 , X 2 and any function g :. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: zation trick for Steps 3 and 7: Step 3 uses Gumbel-softmax <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> while Step 7 uses</p><formula xml:id=\"formula_17\">\u1e90(l) X,v  </label></formula><p>A ) is a non-informative distribution <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. Specifically, we use the uniform distribution for the cate . For the reparameterization in AIB, we use Gumbel-softmax <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> with temperature \u03c4 . For GIB-Cat, the number of neighbors k. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: representations. Other methods apply IB to various domains <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. The difference is that we develop information-theoretic mo  and <ref type=\"bibr\" target=\"#b5\">(6)</ref>. To allow more flexibility (in similar spirit as \u03b2-VAE <ref type=\"bibr\" target=\"#b40\">[41]</ref>), we allow the coefficient before AIB and XIB to be differ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s selected using Google Suggest API, where the answers are entities in Freebase. CuratedTREC (TREC) <ref type=\"bibr\" target=\"#b1\">(Baudi\u0161 and \u0160ediv\u1ef3, 2015)</ref>  </p></div> <div xmlns=\"http://www.tei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hyperlinks, has also been explored recently <ref type=\"bibr\" target=\"#b27\">(Min et al., 2019b;</ref><ref type=\"bibr\" target=\"#b0\">Asai et al., 2020)</ref>. The use of dense vector representations for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b41\">(Wang et al., 2019</ref><ref type=\"bibr\" target=\"#b40\">(Wang et al., , 2018;;</ref><ref type=\"bibr\" target=\"#b25\">Lin et al., 2018)</ref>.</p><p>Specifically, let P i \u2208 R L\u00d7h (1 \u2264 i \u2264. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref> and more recently for mini-batch <ref type=\"bibr\" target=\"#b13\">(Henderson et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Gillick et al., 2019)</ref>. It has been shown to be an effective str bibr\" target=\"#b47\">(Yih et al., 2011;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2013;</ref><ref type=\"bibr\" target=\"#b10\">Gillick et al., 2019)</ref>, with applications to cross-lingual docum. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , <ref type=\"bibr\" target=\"#b5\">Chen et al., 2017;</ref><ref type=\"bibr\">Yang et al., 2019a,b;</ref><ref type=\"bibr\" target=\"#b30\">Nie et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Min et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch has been widely used and studied, as well as its connection to cosine similarity and L2 distance <ref type=\"bibr\" target=\"#b29\">(Mussmann and Ermon, 2016;</ref><ref type=\"bibr\" target=\"#b34\">Ram an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o, 2019;</ref><ref type=\"bibr\" target=\"#b15\">Humeau et al., 2020)</ref>. Finally, a concurrent work <ref type=\"bibr\" target=\"#b19\">(Khattab and Zaharia, 2020)</ref> demonstrates the feasibility of ful. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b41\">(Wang et al., 2019</ref><ref type=\"bibr\" target=\"#b40\">(Wang et al., , 2018;;</ref><ref type=\"bibr\" target=\"#b25\">Lin et al., 2018)</ref>.</p><p>Specifically, let P i \u2208 R L\u00d7h (1 \u2264 i \u2264. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (2) a machine reader can thoroughly examine the retrieved contexts and identify the correct answer <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref>. Although reducing open-domain QA to machine ent that can select a small set of relevant texts, before applying the reader to extract the answer <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref>. <ref type=\"foot\" target=\"#foot_1\">4</ref>Fo e source documents for answering questions. We first apply the pre-processing code released in DrQA <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref> to extract the clean, text-portion of articl o-end QA results, measured by exact match with the reference answer after minor normalization as in <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al., 20 ike TF-IDF or BM25 have been used as the standard method applied broadly to various QA tasks (e.g., <ref type=\"bibr\" target=\"#b5\">Chen et al., 2017;</ref><ref type=\"bibr\">Yang et al., 2019a,b;</ref><r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned model <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> and a dual-encoder architecture <ref type=\"bibr\" target=\"#b3\">(Bromley et al., 1994)</ref>, we focus on developing the right trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ple, disjoint text blocks of 100 words as passages, serving as our basic retrieval units, following <ref type=\"bibr\" target=\"#b41\">(Wang et al., 2019)</ref>, which results in 21,015,324 passages in th it to selecting the passage from a small number of retrieved candidates has been shown to work well <ref type=\"bibr\" target=\"#b41\">(Wang et al., 2019</ref><ref type=\"bibr\" target=\"#b40\">(Wang et al.,   using fixed-length passages performs better in both retrieval and final QA accuracy, as observed by<ref type=\"bibr\" target=\"#b41\">Wang et al. (2019)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org ely.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"5\" xml:id=\"foot_2\">However,<ref type=\"bibr\" target=\"#b41\">Wang et al. (2019)</ref> also propose splitting documents into overla. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ively using reformulated question vectors. As an alternative approach that skips passage retrieval, <ref type=\"bibr\" target=\"#b37\">Seo et al. (2019)</ref> propose to encode candidate answer phrases as <note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"4\" xml:id=\"foot_1\">  4  Exceptions include<ref type=\"bibr\" target=\"#b37\">(Seo et al., 2019)</ref> and<ref type=\"bibr\" target=\"#b35\">(Roberts e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch has been widely used and studied, as well as its connection to cosine similarity and L2 distance <ref type=\"bibr\" target=\"#b29\">(Mussmann and Ermon, 2016;</ref><ref type=\"bibr\" target=\"#b34\">Ram an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emes, retrieval can be done efficiently using maximum inner product search (MIPS) algorithms (e.g., <ref type=\"bibr\" target=\"#b38\">Shrivastava and Li (2014)</ref>; <ref type=\"bibr\" target=\"#b11\">Guo e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: omplicated and consist of multiple components <ref type=\"bibr\" target=\"#b9\">(Ferrucci (2012)</ref>; <ref type=\"bibr\" target=\"#b28\">Moldovan et al. (2003)</ref>, inter alia), the advances of reading co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  dense encoders have become popular recently <ref type=\"bibr\" target=\"#b47\">(Yih et al., 2011;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2013;</ref><ref type=\"bibr\" target=\"#b10\">Gillick et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , <ref type=\"bibr\" target=\"#b5\">Chen et al., 2017;</ref><ref type=\"bibr\">Yang et al., 2019a,b;</ref><ref type=\"bibr\" target=\"#b30\">Nie et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Min et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to cosine similarity and L2 distance <ref type=\"bibr\" target=\"#b29\">(Mussmann and Ermon, 2016;</ref><ref type=\"bibr\" target=\"#b34\">Ram and Gray, 2012)</ref>. As our ablation study finds other similari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (2) a machine reader can thoroughly examine the retrieved contexts and identify the correct answer <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref>. Although reducing open-domain QA to machine ent that can select a small set of relevant texts, before applying the reader to extract the answer <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref>. <ref type=\"foot\" target=\"#foot_1\">4</ref>Fo e source documents for answering questions. We first apply the pre-processing code released in DrQA <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref> to extract the clean, text-portion of articl o-end QA results, measured by exact match with the reference answer after minor normalization as in <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al., 20 ike TF-IDF or BM25 have been used as the standard method applied broadly to various QA tasks (e.g., <ref type=\"bibr\" target=\"#b5\">Chen et al., 2017;</ref><ref type=\"bibr\">Yang et al., 2019a,b;</ref><r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a\">1</ref>)) becomes a good ranking function for retrieval is essentially a metric learning problem <ref type=\"bibr\" target=\"#b20\">(Kulis, 2013)</ref>. The goal is to create a vector space such that r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (2) a machine reader can thoroughly examine the retrieved contexts and identify the correct answer <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref>. Although reducing open-domain QA to machine ent that can select a small set of relevant texts, before applying the reader to extract the answer <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref>. <ref type=\"foot\" target=\"#foot_1\">4</ref>Fo e source documents for answering questions. We first apply the pre-processing code released in DrQA <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref> to extract the clean, text-portion of articl o-end QA results, measured by exact match with the reference answer after minor normalization as in <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al., 20 ike TF-IDF or BM25 have been used as the standard method applied broadly to various QA tasks (e.g., <ref type=\"bibr\" target=\"#b5\">Chen et al., 2017;</ref><ref type=\"bibr\">Yang et al., 2019a,b;</ref><r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: passages for each question. The trick of in-batch negatives has been used in the full batch setting <ref type=\"bibr\" target=\"#b47\">(Yih et al., 2011)</ref> and more recently for mini-batch <ref type=\" airs of queries and documents, discriminatively trained dense encoders have become popular recently <ref type=\"bibr\" target=\"#b47\">(Yih et al., 2011;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: assignment, non-maximum suppression (NMS) post-processing. They are not fully end-to-end. Recently, <ref type=\"bibr\" target=\"#b2\">Carion et al. (2020)</ref> proposed DETR to eliminate the need for suc d attention module suffers from a quadratic complexity growth with the feature map size. DETR. DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref> is built upon the Transformer encoder-deco ng different feature levels. Other hyper-parameter setting and training strategy mainly follow DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref>, except that Focal Loss <ref type=\"bibr\" t or 50 epochs and the learning rate is decayed at the 40-th epoch by a factor of 0.1. Following DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref>, we train our models using Adam optimizer . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b22\">Parmar et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Child et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Ho et al., 201 t loses global information. To compensate, <ref type=\"bibr\" target=\"#b3\">Child et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2  target=\"#b22\">Parmar et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b3\">Child et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\">) 43.8 62.6 47.8 26.5 47.3 58.1 4 BiFPN (Tan et al., 2020) 43.9 62.5 47.7 25.6 47.4</ref>   <ref type=\"bibr\" target=\"#b36\">(Xie et al., 2017)</ref>, our method achieves 48.7 AP and 49.0 AP wit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b26\">Roy et al. (2020)</ref>, where k-means finds out the most related keys. <ref type=\"bibr\" target=\"#b29\">Tay et al. (2020a)</ref> learns block permutation for block-wise spar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b22\">Parmar et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Child et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Ho et al., 201 t loses global information. To compensate, <ref type=\"bibr\" target=\"#b3\">Child et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2  target=\"#b22\">Parmar et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b3\">Child et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Zhao et al. (2019)</ref> proposes a U-shape module to fuse multi-scale features. Recently, NAS-FPN <ref type=\"bibr\" target=\"#b7\">(Ghiasi et al., 2019)</ref> and Auto-FPN <ref type=\"bibr\" target=\"#b37. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b32\">(Tian et al., 2019)</ref> ResNeXt-101 44.7 64.1 48.4 27.6 47.5 55.6 ATSS <ref type=\"bibr\" target=\"#b39\">(Zhang et al., 2020)</ref> ResNeXt-101 + DCN 50.7 68.9 56.3 33.2 52.9. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b11\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Ho et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Hu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Parmar et al., 2 ibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b10\">Hu et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b23\">Parmar et al.  te the theoretically reduced complexity, <ref type=\"bibr\" target=\"#b23\">Parmar et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b10\">Hu et al. (2019)</ref> admit such approaches are much slower in imple eature of query elements. Different from <ref type=\"bibr\" target=\"#b23\">Parmar et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b10\">Hu et al. (2019)</ref>, deformable attention is just slightly slower . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b39\">(Zhang et al., 2020)</ref> ResNeXt-101 + DCN 50.7 68.9 56.3 33.2 52.9 62.4 TSD <ref type=\"bibr\" target=\"#b27\">(Song et al., 2020)</ref> SENet154 + DCN 51.2 71.9 56.0 33.8 54.8 64.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . The module can be naturally extended to aggregating multi-scale features, without the help of FPN <ref type=\"bibr\" target=\"#b17\">(Lin et al., 2017a)</ref>. In Deformable DETR , we utilize (multi-sca ctors usually exploit multi-scale features to accommodate this. As one of the pioneering works, FPN <ref type=\"bibr\" target=\"#b17\">(Lin et al., 2017a)</ref> proposes a top-down path to combine multi-s . All the multi-scale feature maps are of C = 256 channels. Note that the top-down structure in FPN <ref type=\"bibr\" target=\"#b17\">(Lin et al., 2017a)</ref> is not used, because our proposed multi-sca /ref> is utilized as the backbone for ablations. Multi-scale feature maps are extracted without FPN <ref type=\"bibr\" target=\"#b17\">(Lin et al., 2017a)</ref>. M = 8 and K = 4 are set for deformable att e cost of unordered memory access. Thus, it is still slightly slower than traditional convolution.  <ref type=\"bibr\" target=\"#b17\">(Lin et al., 2017a</ref><ref type=\"bibr\">) 43.8 62.6 47.8 26.5 47.3 5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vie reviews (MR) <ref type=\"bibr\" target=\"#b36\">(Pang &amp; Lee, 2005)</ref>.</p><p>Comparison with <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref>: <ref type=\"bibr\" target=\"#b23\">Kalant , 2005)</ref>.</p><p>Comparison with <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref>: <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref> also consider ways to sample negatives. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  downloaded at www.graphlearning.io from the TU-Dataset repository of graph classification problems <ref type=\"bibr\" target=\"#b31\">(Morris et al., 2020)</ref>. Information on basic statistics of the d ccuracy 10-fold cross-validation using an SVM as the classifier (in line with the approach taken by <ref type=\"bibr\" target=\"#b31\">Morris et al. (2020)</ref>). Each experiment is repeated from scratch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m cropping, separating color channels, etc. <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref type=\"bibr\" target=\"#b47\">Tian et al., 2019)</ref>. Such bibr\" target=\"#b47\">(Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref type=\"bibr\" target=\"#b19\">He et al., 2020;</ref><ref typ his distribution by adopting a PU-learning viewpoint <ref type=\"bibr\">(Elkan &amp; Noto, 2008;</ref><ref type=\"bibr\" target=\"#b12\">Du Plessis et al., 2014;</ref><ref type=\"bibr\" target=\"#b9\">Chuang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b34\">(Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b47\">Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a)</ref>. In computer vision, unsupervised contrastiv s to nearby locations, and negative pairs farther apart; other objectives have also been considered <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref>. The success of the associated methods depe  the hard sampling method on vision tasks using the STL10, CIFAR100 and CIFAR10 data. We use SimCLR <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref> as the baseline method, and all models are  ted by, The vision experiments in the main body of the paper are all based off the SimCLR framework <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a)</ref>. They use a relatively small batch size (up \"bibr\" target=\"#b54\">Xu et al., 2013;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b48\">Tian et al., 2  practice, an empirical approximation of it <ref type=\"bibr\" target=\"#b47\">(Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref t s that preserve semantic content, e.g., jittering, random cropping, separating color channels, etc. <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"1\">INTRODUCTION</head><p>Owing to their empirical success, contrastive learning methods <ref type=\"bibr\" target=\"#b7\">(Chopra et al., 2005;</ref><ref type=\"bibr\" target=\"#b15\">Hadsell et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b34\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b37\">Purushwalkam &amp; Gupta, 2020;</ref><ref type=\"bibr\" target=\"#b40\">Sermanet et al., 2018)</ref>.</p><p>Surprisingly, the choice of negat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Such transformations have also been effective in learning control policies from raw pixel data <ref type=\"bibr\" target=\"#b42\">(Srinivas et al., 2020)</ref>. Positive sampling techniques have also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, or introducing a recurrence mechanism as in Transformer-XL for learning longer-range dependency <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. However, no much  an example of a REMI event sequence.</p><p>In our experiment (see Section 5), we use Transformer-XL <ref type=\"bibr\" target=\"#b10\">[11]</ref> to learn to compose Pop piano music using REMI as the unde xpressive classical piano music with a coherent structure of up to one minute.</p><p>Transformer-XL <ref type=\"bibr\" target=\"#b10\">[11]</ref> extends Transformer by introducing the notion of recurrenc s for the \ud835\udf0f-th segment, R denotes the relative positional encodings designed for the Transformer-XL <ref type=\"bibr\" target=\"#b10\">[11]</ref>, a \ud835\udc5b \ud835\udf0f,\ud835\udc56 indicates the attention features from the \ud835\udc56-th he ents (i.e., the segment length) and the recurrence length (i.e., the length of the segment \"cached\" <ref type=\"bibr\" target=\"#b10\">[11]</ref>) are both set to 512. The total number of learnable parame. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mentation. 7 We note that the idea of using Bar and Position has been independently proposed before <ref type=\"bibr\" target=\"#b18\">[19]</ref>, though in the context of RNN-based not Transformer-based . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te to the generative musical modeling on its own. Hence, high-level (semantic) information of music <ref type=\"bibr\" target=\"#b3\">[4]</ref>, such as downbeat, tempo and chord, are not included in this. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmation retrieval (MIR) techniques such as downbeat tracking <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and chord recognition , or from the result of automatic music transcription (with the help of a beat and downbeat tracker <ref type=\"bibr\" target=\"#b2\">[3]</ref>); they are simply discarded in the MIDI-like representation. rmances, one can derive such Tempo events with the use of an audio-domain tempo estimation function <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= racking</head><p>To create the Bar events, we employ the recurrent neural network model proposed by <ref type=\"bibr\" target=\"#b2\">[3]</ref> to estimate from the audio files the position of the 'downbe type=\"foot\" target=\"#foot_10\">13</ref> Then, we apply the joint beat and downbeat tracking model of <ref type=\"bibr\" target=\"#b2\">[3]</ref> to the resulting audio clips. <ref type=\"foot\" target=\"#foot =\"#b2\">[3]</ref> to the resulting audio clips. <ref type=\"foot\" target=\"#foot_11\">14</ref>The model <ref type=\"bibr\" target=\"#b2\">[3]</ref> has two components. The first one estimates the probability, f the DBN for the beat STD and downbeat STD. Specifically, given a piece of audio signal, the model <ref type=\"bibr\" target=\"#b2\">[3]</ref> returns a time-ordered series of (\ud835\udf0f B \ud835\udc61 , \ud835\udc60 B \ud835\udc61 ), where \ud835\udf0f B lns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"14\" xml:id=\"foot_11\">We choose to use the model of<ref type=\"bibr\" target=\"#b2\">[3]</ref> for it achieved state-of-the-art performance for beat and do. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b30\">30,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> as a prominent approach with great potential. <ref type=\"fo r example, extra events denoting the composer, instrumentation and global tempo are used in MuseNet <ref type=\"bibr\" target=\"#b32\">[32]</ref> for conditioned generation, and events specifying the Note. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" target=\"#b44\">44,</ref><ref type=\"bibr\" target=\"#b45\">45]</ref>. Among the approach other multimedia input such as the lyrics <ref type=\"bibr\" target=\"#b43\">[43]</ref> or a video clip <ref type=\"bibr\" target=\"#b44\">[44]</ref>, and also to study other model architectures that can mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic music composition <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>, it represents a brand new attempt in the context of Transf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te to the generative musical modeling on its own. Hence, high-level (semantic) information of music <ref type=\"bibr\" target=\"#b3\">[4]</ref>, such as downbeat, tempo and chord, are not included in this. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te audio-and symbolic-domain music information retrieval (MIR) techniques such as downbeat tracking <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target rst beat in each bar. The same model is used to track the beat positions to create the Tempo events <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We obtain the tick positions between beats by linear interp limitations.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\"><ref type=\"bibr\" target=\"#b1\">2</ref> Therefore, the model generates the pitch, velocity (dynamics),. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[33]</ref>, Insertion Transformer <ref type=\"bibr\" target=\"#b37\">[37]</ref>, and Tree Transformer <ref type=\"bibr\" target=\"#b40\">[40]</ref>, to be studied for this task in the future, since the mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o gather to the requesting core, only the requested elements from the data array.</p><p>Song et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> propose a graph processor based on sparse matrix algebra, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rg/ns/1.0\"><head>E. Comparison to other Graph Processors</head><p>The Cray Urika-GD graph processor <ref type=\"bibr\" target=\"#b7\">[8]</ref> was one of the first commercial graph-oriented big data proc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a graph. Determining the relationships between entities in a graph is the basis of graph analytics <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Graph analytics poses important challenges on existing proc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the network needs to have a high radix and a low diameter. This is achieved with a HyperX topology <ref type=\"bibr\" target=\"#b6\">[7]</ref>, with all-toall connections on each level. Links on the high. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: even worse performance for small node counts, due to the overhead of the fine-grained communication <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For PIUMA, the application code does not need to change f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: even worse performance for small node counts, due to the overhead of the fine-grained communication <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For PIUMA, the application code does not need to change f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are important contributors to PUMA's performance and energy efficiency.</p><p>The Emu architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref> is a recently proposed architecture for big data analysis, i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rg/ns/1.0\"><head>E. Comparison to other Graph Processors</head><p>The Cray Urika-GD graph processor <ref type=\"bibr\" target=\"#b7\">[8]</ref> was one of the first commercial graph-oriented big data proc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  progress. Nevertheless, GPUs usually perform better on graph algorithms than CPUs for small graphs <ref type=\"bibr\" target=\"#b4\">[5]</ref>, because they have more threads, which hides memory latency,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are important contributors to PUMA's performance and energy efficiency.</p><p>The Emu architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref> is a recently proposed architecture for big data analysis, i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the network needs to have a high radix and a low diameter. This is achieved with a HyperX topology <ref type=\"bibr\" target=\"#b6\">[7]</ref>, with all-toall connections on each level. Links on the high. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es has improved both the rate of learning and final performance. Similar to our findings about MoCo,<ref type=\"bibr\" target=\"#b32\">Wu et al. (2017)</ref> find that mining the very hardest negatives hu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b31\">(Weinberger &amp; Saul, 2009)</ref>. In this paradigm, selecting the hardest <ref type=\"bibr\" target=\"#b5\">(Bucher et al., 2016)</ref> or harder <ref type=\"bibr\" target=\"#b23\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n active learning, for example, it is common to favor examples on which the model is most uncertain <ref type=\"bibr\" target=\"#b14\">(Fu et al., 2013)</ref>. Work in object detection has also benefited . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b25\">(Sung, 1996;</ref><ref type=\"bibr\">Can\u00e9vet &amp; Fleuret, 2015;</ref><ref type=\"bibr\" target=\"#b24\">Shrivastava et al., 2016)</ref>. However, none of the aforementioned . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s for self-supervised have been proposed that do not work within the CID paradigm, including RotNet <ref type=\"bibr\" target=\"#b15\">(Gidaris et al., 2018)</ref>, Jigsaw <ref type=\"bibr\" target=\"#b22\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n active learning, for example, it is common to favor examples on which the model is most uncertain <ref type=\"bibr\" target=\"#b14\">(Fu et al., 2013)</ref>. Work in object detection has also benefited . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: paradigm, selecting the hardest <ref type=\"bibr\" target=\"#b5\">(Bucher et al., 2016)</ref> or harder <ref type=\"bibr\" target=\"#b23\">(Schroff et al., 2015)</ref>  </p></div> <div xmlns=\"http://www.tei-c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b11\">(Chen et al., 2020c)</ref> and the downstream task of linear classification on ImageNet <ref type=\"bibr\" target=\"#b13\">(Deng et al., 2009)</ref>. We make the following contributions (see F e query). To evaluate semantic similarity we used the ImageNet class hierarchy derived from WordNet <ref type=\"bibr\" target=\"#b13\">(Deng et al., 2009)</ref>. For each negative, we computed the tree de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b7\">(Caron et al., 2018)</ref>, SwAV <ref type=\"bibr\" target=\"#b8\">(Caron et al., 2020)</ref>, SeLa <ref type=\"bibr\" target=\"#b2\">(Asano et al., 2020)</ref>, PCL <ref type=\"bibr\" target=\"#b20\">(Li et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s for self-supervised have been proposed that do not work within the CID paradigm, including RotNet <ref type=\"bibr\" target=\"#b15\">(Gidaris et al., 2018)</ref>, Jigsaw <ref type=\"bibr\" target=\"#b22\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b11\">(Chen et al., 2020c)</ref> and the downstream task of linear classification on ImageNet <ref type=\"bibr\" target=\"#b13\">(Deng et al., 2009)</ref>. We make the following contributions (see F e query). To evaluate semantic similarity we used the ImageNet class hierarchy derived from WordNet <ref type=\"bibr\" target=\"#b13\">(Deng et al., 2009)</ref>. For each negative, we computed the tree de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b25\">(Sung, 1996;</ref><ref type=\"bibr\">Can\u00e9vet &amp; Fleuret, 2015;</ref><ref type=\"bibr\" target=\"#b24\">Shrivastava et al., 2016)</ref>. However, none of the aforementioned . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b7\">(Caron et al., 2018)</ref>, SwAV <ref type=\"bibr\" target=\"#b8\">(Caron et al., 2020)</ref>, SeLa <ref type=\"bibr\" target=\"#b2\">(Asano et al., 2020)</ref>, PCL <ref type=\"bibr\" target=\"#b20\">(Li et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s for self-supervised have been proposed that do not work within the CID paradigm, including RotNet <ref type=\"bibr\" target=\"#b15\">(Gidaris et al., 2018)</ref>, Jigsaw <ref type=\"bibr\" target=\"#b22\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: similar from every other image in the dataset <ref type=\"bibr\" target=\"#b50\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020)</ref>. We propose to train feature representations  \" target=\"#b26\">Misra &amp; Maaten, 2020;</ref><ref type=\"bibr\" target=\"#b15\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020)</ref> which aims to learn representations by consid main. We use a state-of-the-art self-supervised loss function based on contrastive learning: SimCLR <ref type=\"bibr\" target=\"#b1\">(Chen et al., 2020)</ref>. The SimCLR loss encourages two augmentation seline, SimCLR that uses the novel domain unlabeled data D u to train a representation using SimCLR <ref type=\"bibr\" target=\"#b1\">(Chen et al., 2020)</ref>, and then uses the resulting representation . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  loss formulation.</p><p>The first two terms are similar to those in prior self-training literature <ref type=\"bibr\" target=\"#b51\">(Xie et al., 2020)</ref>. However, while in prior self-training work, RTUP and train a linear classifier on the support set and evaluate the classifier on the query set. <ref type=\"bibr\" target=\"#b51\">Xie et al. (2020)</ref> found that training the student from scratch  \" xml:id=\"tab_1\"><head></head><label></label><figDesc>STARTUP ADDS NOISE WHICH INCREASES ROBUSTNESS.<ref type=\"bibr\" target=\"#b51\">Xie et al. (2020)</ref> posit that self-training introduces noise whe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  et al., 2019a)</ref> has shown that existing stateof-the-art few-shot learners fail to generalize. <ref type=\"bibr\" target=\"#b40\">Tseng et al. (2020)</ref> attempt to address this problem by simulati </ref>, which includes most stateof-the-art approaches as well as a cross-domain few-shot technique <ref type=\"bibr\" target=\"#b40\">Tseng et al. (2020)</ref>. The top performing among these is naive Tr 6\">(Finn et al., 2017)</ref>, MetaOpt: <ref type=\"bibr\" target=\"#b21\">(Lee et al., 2019)</ref> FWT: <ref type=\"bibr\" target=\"#b40\">(Tseng et al., 2020)</ref>. * Numbers reported in <ref type=\"bibr\" ta 6\">(Finn et al., 2017)</ref>, MetaOpt: <ref type=\"bibr\" target=\"#b21\">(Lee et al., 2019)</ref> FWT: <ref type=\"bibr\" target=\"#b40\">(Tseng et al., 2020)</ref>. * Numbers reported in <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of such extreme task/domain differences, underperforming even naive transfer learning from ImageNet <ref type=\"bibr\" target=\"#b12\">(Guo et al., 2020)</ref>.</p><p>Another alternative comes to light wh . In a recently released BSCD-FSL benchmark consisting of datasets from extremely different domains <ref type=\"bibr\" target=\"#b12\">(Guo et al., 2020)</ref>, we show that STARTUP provides significant g , 2019)</ref> FWT: <ref type=\"bibr\" target=\"#b40\">(Tseng et al., 2020)</ref>. * Numbers reported in <ref type=\"bibr\" target=\"#b12\">(Guo et al., 2020)</ref> We conjecture that the base embedding is not , 2019)</ref> FWT: <ref type=\"bibr\" target=\"#b40\">(Tseng et al., 2020)</ref>. * Numbers reported in <ref type=\"bibr\" target=\"#b12\">(Guo et al., 2020)</ref> ), from teacher model (STARTUP-T (no SS)). W \u2192BSCD-FSL for higher shots. Mean and 95% confidence interval are reported. * are methods reported in<ref type=\"bibr\" target=\"#b12\">(Guo et al., 2020)</ref>. Despite using their code, difference in bat assification (CD-FSL). When the domain gap between the base and novel dataset is large, recent work <ref type=\"bibr\" target=\"#b12\">(Guo et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 20 p; Komodakis, 2018</ref>) can be used with STARTUP. For simplicity and based on results reported by <ref type=\"bibr\" target=\"#b12\">Guo et al. (2020)</ref>, we freeze the representation \u03c6 after perform T DOMAINS</head><p>Benchmark. We experiment with the challenging (BSCD-FSL) benchmark introduced in <ref type=\"bibr\" target=\"#b12\">Guo et al. (2020)</ref>. The base dataset in this benchmark is miniIm he respective unlabeled datasets D u . We use the rest for sampling tasks for evaluation. Following <ref type=\"bibr\" target=\"#b12\">Guo et al. (2020)</ref>, we evaluate 5-way k-shot classification task alize to k \u2208 {20, 50}. See Appendix A.2).</p><p>Baselines. We compare to the techniques reported in <ref type=\"bibr\" target=\"#b12\">Guo et al. (2020)</ref>, which includes most stateof-the-art approach ochastic image augmentations for SimCLR, we use the augmentations defined for each novel dataset in <ref type=\"bibr\" target=\"#b12\">Guo et al. (2020)</ref>. These augmentations include the commonly use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eacher (ResNet 18 trained on ImageNet).</p><p>We then compute the adjusted mutual information (AMI) <ref type=\"bibr\" target=\"#b42\">(Vinh et al., 2010)</ref> between the resulting grouping and ground t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b17\">Hoffman et al., 2018;</ref><ref type=\"bibr\" target=\"#b24\">Long et al., 2018;</ref><ref type=\"bibr\" target=\"#b52\">Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">Laradji &amp; Ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b50\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Misra &amp; Maaten, 2020;</ref><ref type=\"bibr\" target=\"#b15\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b46\">Wang et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Kolesnikov et al., 2020;</ref><ref type=\"bibr\" target=\"#b39\">Tian et al., 2020)</ref>. Alternatively, they may assume that model i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\">Ravi &amp; Larochelle, 2017;</ref><ref type=\"bibr\" target=\"#b28\">Nichol &amp; Schulman;</ref><ref type=\"bibr\" target=\"#b34\">Rusu et al., 2019;</ref><ref type=\"bibr\" target=\"#b37\">Sun et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Semi-supervised few-shot learning (SS-FSL) <ref type=\"bibr\" target=\"#b32\">(Ren et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Yu et al., 2020;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Semi-supervised few-shot learning (SS-FSL) <ref type=\"bibr\" target=\"#b32\">(Ren et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Yu et al., 2020;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\">Ravi &amp; Larochelle, 2017;</ref><ref type=\"bibr\" target=\"#b28\">Nichol &amp; Schulman;</ref><ref type=\"bibr\" target=\"#b34\">Rusu et al., 2019;</ref><ref type=\"bibr\" target=\"#b37\">Sun et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b32\">(Ren et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Yu et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Rodr\u00edguez et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b13\">(Hariharan &amp; Girshick, 2017;</ref><ref type=\"bibr\">Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2019b)</ref>. Somewhat related is the use of a class-agno. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b13\">(Hariharan &amp; Girshick, 2017;</ref><ref type=\"bibr\">Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2019b)</ref>. Somewhat related is the use of a class-agno. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref>. The cSBM allows for smoothly controll rning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref> to generate synthetic graphs. We consi d have similar performances for \u03c6 and \u2212\u03c6. Due to space limitation we refer the interested reader to <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref> for a review of all formal theoretical philic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>. The results show that, asymptotically, ate synthetic data is that the information-theoretic limit of the model is already characterized in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p de et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>). Assume that n, f \u2192 \u221e, n f \u2192 \u03be and d \u2192. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a running time similar to that of APPNP. It is nevertheless worth pointing out that the authors of <ref type=\"bibr\" target=\"#b5\">Bojchevski et al. (2020)</ref> successfully scaled APPNP to operate on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of them seem to be tailor-made to work on homophilic (associative) graphs. The homophily principle <ref type=\"bibr\" target=\"#b26\">(McPherson et al., 2001)</ref> in the context of node classification . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g the citation graphs Cora, CiteSeer, PubMed <ref type=\"bibr\" target=\"#b29\">(Sen et al., 2008;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al., 2016)</ref> and the Amazon co-purchase graphs Computers . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n so far that they fail to properly solve learning problems on heterophilic (disassortative) graphs <ref type=\"bibr\" target=\"#b28\">(Pei et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Bojchevski et a iversality with respect to node label patterns: Homophily versus heterophily. In their recent work, <ref type=\"bibr\" target=\"#b28\">Pei et al. (2019)</ref> proposed an index to measure the level of hom  type=\"bibr\" target=\"#b18\">Kipf &amp; Welling (2017)</ref> while the dense setting is considered in <ref type=\"bibr\" target=\"#b28\">Pei et al. (2019)</ref> for studying heterophilic graphs. For all dat \" target=\"#b30\">Shchur et al., 2018)</ref>. We also use 5 heterophilic benchmark datasets tested in <ref type=\"bibr\" target=\"#b28\">Pei et al. (2019)</ref>, including Wikipedia graphs Chameleon and Squ hchur et al. (2018)</ref>. For the heterophilic datasets, we adopt dense splitting which is used in <ref type=\"bibr\" target=\"#b28\">Pei et al. (2019)</ref>.</p><p>Table <ref type=\"table\" target=\"#tab_1 re <ref type=\"figure\" target=\"#fig_1\">2</ref>). Furthermore, the homophily measure H(G) proposed by <ref type=\"bibr\" target=\"#b28\">Pei et al. (2019)</ref> cannot characterize such differences in heter et al., 2019)</ref>, SAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and Geom-GCN <ref type=\"bibr\" target=\"#b28\">(Pei et al., 2019)</ref>. For all these architectures, we use the cor not originally tested in the paper due to a preprocessing subroutine that is not publicly available <ref type=\"bibr\" target=\"#b28\">(Pei et al., 2019)</ref>.</p><p>The GPR-GNN model setup and hyperpara or Geom-GCN, we choose the datasets already tested in the paper were the method was first described <ref type=\"bibr\" target=\"#b28\">(Pei et al., 2019)</ref>. For SGC, we use the default K = 2 layers af E, we use 2 SAGE convolutional layers with 64 hidden units.</p><p>The heterophilic datasets used in <ref type=\"bibr\" target=\"#b28\">(Pei et al., 2019)</ref>. The graphs Chameleon, Actor, Squirrel, Texa quirrel, Texas and Cornell in their original form are directed graphs (see the github repository of <ref type=\"bibr\" target=\"#b28\">(Pei et al., 2019)</ref>). Since the usual setting for semi-supervise ble 7 :</head><label>7</label><figDesc>Results on homophilic real-world benchmark datasets tested in<ref type=\"bibr\" target=\"#b28\">(Pei et al., 2019)</ref>, dense splitting: Mean accuracy (%) \u00b1 95% co /ref> are all based on undirected graphs and hence the numbers are different from those reported in <ref type=\"bibr\" target=\"#b28\">(Pei et al., 2019)</ref>.    </p></div> <div xmlns=\"http://www.tei-c.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvised node classification and graph classification <ref type=\"bibr\" target=\"#b41\">(Zhu, 2005;</ref><ref type=\"bibr\" target=\"#b31\">Shervashidze et al., 2011;</ref><ref type=\"bibr\" target=\"#b24\">L\u00fc &am. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ;</ref><ref type=\"bibr\" target=\"#b18\">Kipf &amp; Welling, 2017)</ref>, graph attention layers (GAT) <ref type=\"bibr\" target=\"#b34\">(Velickovic et al., 2018)</ref> and many others <ref type=\"bibr\" targ th 6 baseline models: MLP, GCN <ref type=\"bibr\" target=\"#b18\">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b34\">(Velickovic et al., 2018)</ref>, JK-Net <ref type=\"bibr\" target=\"#b37. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of them seem to be tailor-made to work on homophilic (associative) graphs. The homophily principle <ref type=\"bibr\" target=\"#b26\">(McPherson et al., 2001)</ref> in the context of node classification . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atterns make the size of the model continuously increasing <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar required for the inference are also increasing accordingly <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar distillation (KD) in the computer vision field, a few work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have employed KD for RS to reduce the size of models while  eacher, and also has a lower latency due to its small size <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>The core idea behind this process is that the soft l sions from the teacher model, the state-of-the-art methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have achieved comparable or even better performance to the  p>However, there are still limitations in existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. First, the learning of the student is only guided by the t at Figure <ref type=\"figure\">1</ref>: The existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> distill the knowledge only based on the teacher's predictio of ranking orders among items. Unlike the existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> that distill the knowledge of an item at a time, RRD formul ation performance compared to the state-of-the-art methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. An unified framework. We propose a novel framework-DE-RRD- uge success of KD in the computer vision field, a few work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillati sting items, we adopt a ranking position importance scheme <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> that places more emphasis on the higher positions in the ra .1, 0.5, 1.0}. Following the notation of the previous work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, we call the student model trained without the help of the  recommendation list would have strong correlations to the items that the user has interacted before <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Also, the soft labels provide guidance for distinguishing e=\"bibr\" target=\"#b24\">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillation (RD) <ref type=\"bibr\" target=\"#b24\">[25]</ref> which applies KD for the ranking problem; Providing recomm items); the high-ranked items in the recommendation list would have strong correlations to the user <ref type=\"bibr\" target=\"#b24\">[25]</ref>. By using such additional supervisions from the teacher, t s. The proposed framework is compared with the following methods:</p><p>\u2022 Ranking Distillation (RD) <ref type=\"bibr\" target=\"#b24\">[25]</ref>: A KD method for recommender system that uses items with t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he teacher and that of the student. To this end, RRD adopts the list-wise learning-to-rank approach <ref type=\"bibr\" target=\"#b28\">[29]</ref> and learns to ensure the student to preserve the ranking o hat of the student model. To this end, RRD adopts the classical list-wise learning-to-rank approach <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Its core idea is to define a probability of a permutation d of the ground-truth ranking order. For more details about the list-wise approach, please refer to <ref type=\"bibr\" target=\"#b28\">[29]</ref>.</p><p>However, merely adopting the list-wise loss can hav p>Relaxed permutation probability. Then, RRD defines a relaxed permutation probability motivated by <ref type=\"bibr\" target=\"#b28\">[29]</ref>. For user \ud835\udc62, \ud835\udf45 \ud835\udc96 denotes a ranked list of all the sampled . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: commender can be any existing RS model such as BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>, NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and L \ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 is its loss function (e.g., binary cross-entropy (e.g., BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>) or their combined representation (e.g., NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>) based on the base model's structure and the type of selecte  model's structure and the types of hidden layer chosen for the distillation. Concretely, for NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, which is a state-ofthe-art deep recommender, the loss funct ems and optimizes Matrix Factorization (MF) with the pair-wise ranking loss function.</p><p>\u2022 NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>: The state-of-the-art deep model for implicit feedback. NeuM rs and items having fewer than five ratings for CiteULike, twenty ratings for Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target among items.</p><p>Evaluation Protocol. We follow the widely used leave-one-out evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target mplicit feedback, we evaluate the performance of each method with widely used three ranking metrics <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Specifically, tree-based data structures <ref type=\"bibr\" target=\"#b1\">[2]</ref>, data compression techniques <ref type=\"bibr\" target=\"#b25\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s have adopted hash techniques to reduce the inference cost <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings for CiteULike, twenty ratings for Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Data statistics are   We follow the widely used leave-one-out evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we le rmance of each method with widely used three ranking metrics <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>: hit ratio (H@\ud835\udc41 ), nor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. A large model with numerous parameters has a high capacity et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Due to the high latency, it becomes difficult to apply suc get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. They first learn binary representations of users and items. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"5.1\">Experimental Setup</head><p>Datasets. We use two public real-world datasets: CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref>, Foursquare <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: problems such as applicable only to specific models (e.g., k-d tree for metric learningbased models <ref type=\"bibr\" target=\"#b11\">[12]</ref>), or easily falling into a local optimum due to the local . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Specifically, tree-based data structures <ref type=\"bibr\" target=\"#b1\">[2]</ref>, data compression techniques <ref type=\"bibr\" target=\"#b25\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ques <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and approximated nearest neighbor search techniques <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> have been successful. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eter that controls the effects of RD. The base recommender can be any existing RS model such as BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>, NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and L \ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 . The output of the mapping function can be a separate representation of a user, an item (e.g., BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>) or their combined representation (e.g., NeuMF <ref type=\" p learning model that are broadly used for top-\ud835\udc41 recommendation with implicit feedback.</p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>: A learning-to-rank model for implicit feedback. It assume  Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Data statistics are summarized in Table <ref type=\"table\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly, it has a limitation in accurately maintaining the ranking orders predicted by the teacher model <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which leads to degraded recommendation performance.</p><p sly, it has a limitation in accurately maintaining the ranking orders in the teacher's ranking list <ref type=\"bibr\" target=\"#b23\">[24]</ref>. This can lead to limited recommendation performance.</p>< ms (e.g., pair-wise, list-wise) can achieve better ranking performance than the point-wise approach <ref type=\"bibr\" target=\"#b23\">[24]</ref>. RRD enables the model to capture the ranking orders among. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ut evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we leave out a single interacted item for t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s have adopted hash techniques to reduce the inference cost <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: commender can be any existing RS model such as BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>, NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and L \ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 is its loss function (e.g., binary cross-entropy (e.g., BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>) or their combined representation (e.g., NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>) based on the base model's structure and the type of selecte  model's structure and the types of hidden layer chosen for the distillation. Concretely, for NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, which is a state-ofthe-art deep recommender, the loss funct ems and optimizes Matrix Factorization (MF) with the pair-wise ranking loss function.</p><p>\u2022 NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>: The state-of-the-art deep model for implicit feedback. NeuM rs and items having fewer than five ratings for CiteULike, twenty ratings for Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target among items.</p><p>Evaluation Protocol. We follow the widely used leave-one-out evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target mplicit feedback, we evaluate the performance of each method with widely used three ranking metrics <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings for CiteULike, twenty ratings for Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Data statistics are   We follow the widely used leave-one-out evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we le rmance of each method with widely used three ranking metrics <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>: hit ratio (H@\ud835\udc41 ), nor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings for CiteULike, twenty ratings for Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Data statistics are   We follow the widely used leave-one-out evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we le rmance of each method with widely used three ranking metrics <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>: hit ratio (H@\ud835\udc41 ), nor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"5.1\">Experimental Setup</head><p>Datasets. We use two public real-world datasets: CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref>, Foursquare <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d on accelerating the inference of the existing recommenders <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Specifically, tree. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e range of the rest. To sample the interesting items, we adopt a ranking position importance scheme <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> that places more e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eter that controls the effects of RD. The base recommender can be any existing RS model such as BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>, NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and L \ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 . The output of the mapping function can be a separate representation of a user, an item (e.g., BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>) or their combined representation (e.g., NeuMF <ref type=\" p learning model that are broadly used for top-\ud835\udc41 recommendation with implicit feedback.</p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>: A learning-to-rank model for implicit feedback. It assume  Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Data statistics are summarized in Table <ref type=\"table\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t, thus it's effective to learn information from their interactions, such as low-order interactions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> or high-order interact. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 360 xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> 0.8093 0.4422 0.7878 0.3752 0.6709 0.1361 FED (Ours) 0.8113 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ased on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations among latent fields. Since each latent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion from their interactions, such as low-order interactions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> or high-order interactions <ref type=\"bibr\" target=\"#b2\">[3,<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ased on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations among latent fields. Since each latent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b7\">8]</ref> or high-order interactions <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, as shown in Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>. .1361 PNN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=\"b crease at 10 \u22123 level in Criteo dataset is already clear compared with recent works such as xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> and DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Sinc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b7\">8]</ref> or high-order interactions <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, as shown in Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>. .1361 PNN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=\"b crease at 10 \u22123 level in Criteo dataset is already clear compared with recent works such as xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> and DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Sinc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8 0.1372 DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8 0.1372 DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ts decrease along with neighbor hops increasing. Inspired by the architecture design of Transformer <ref type=\"bibr\" target=\"#b30\">[31]</ref>, we leverage attention mechanism to learn the weight of so. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  neural network (GGNN) <ref type=\"bibr\" target=\"#b18\">[19]</ref>, graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, graph inductive representation learning (GraphSAGE) <ref  ad><label></label><figDesc>GC-MC<ref type=\"bibr\" target=\"#b29\">[30]</ref> (C): This model adopts GCN<ref type=\"bibr\" target=\"#b16\">[17]</ref> encoders in useritem bipartite graph to generate the repre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , which use information propagation way to exploit high-order connectivity in graph.</p><p>\u2022 BPR-MF <ref type=\"bibr\" target=\"#b22\">[23]</ref> (A): This is a matrix factorization model optimized by the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: path based similarities between users, and infer the rating score based on similar users. Yu et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref> and Zhao et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s between nodes pairs, can be defined by the network schema and used for recommendation. Shi et al. <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> calculate the meta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -valued ones to learn disentangled node representations with GNNs' framework in homogeneous network <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rpretable which can directly find applications in various fields with semantic data, such as images <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, but still overlook the different aspects of semantic infor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: which aims to learn representations that separate explanatory factors of variations behind the data <ref type=\"bibr\" target=\"#b2\">[3]</ref>, has recently gained much attention. Not only such represent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cations in various fields with semantic data, such as images <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, texts <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" target=\"#b35\">[36]</ref><ref type=\"bibr\" t erage GNNs in user-item bipartite graph to CF. KGAT <ref type=\"bibr\" target=\"#b35\">[36]</ref>, KGCN <ref type=\"bibr\" target=\"#b34\">[35]</ref> and KGCN-LS <ref type=\"bibr\" target=\"#b33\">[34]</ref> Full. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. To generate diversified results, these methods either exp ef type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>Those existing approaches used greedy document sequ strategy may not lead to global optimal rankings. Based on the reinforced learning approach MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Feng <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed t rmula><p>Here \ud835\udc98 \ud835\udc5f is a learnable parameter. We use the same relevance features as the previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref> for \ud835\udc99 \ud835\udc5e and \ud835\udc99 \ud835\udc5e \ud835\udc56 , including BM25, TF-IDF, language model bers of incoming links and outgoing links, et al. More details about these features can be found in <ref type=\"bibr\" target=\"#b13\">[14]</ref> and we omit the details due to space limitation. In the fu  used are actually the document embeddings. We use the subtopic embeddings released by Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> based on doc2vec. The subtopic embeddings is produced from t diversification task is limited, we inherit the list-pairwise sampling approach from Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> in order to get enough training samples. We are using pair ance features and embeddings exactly the same as the DSSA, which have been released by Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> in the repository on GitHub<ref type=\"foot\" target=\"#foot_ =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>, all those metrics are computed on top 20 results of a doc \"#b11\">[12]</ref> and PAMM-NTN <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Inspired by previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we use the metric of \ud835\udefc \u2212 \ud835\udc5bDCG@20 to tune the parameters.   100-dimensional vectors generated by the LDA <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>. We train the DSSA model with the code and data released b lt is denoted as DSSA (doc2vec).</p><p>Since the deep reinforced learning based models e.g. MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref> and M2DIV <ref type=\"bibr\" target=\"#b14\">[15]</ref> are ta target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> (i.e., explicit approaches), or directly reduce result redu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  sequence. Recently, the models fully based on self-attention mechanism (denoted as self-attention  <ref type=\"bibr\" target=\"#b15\">[16]</ref> in the Neural Machine Translation (NMT) task, have achieve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b16\">[17]</ref>, BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ERNIE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, as alternatives to RNNs and CNNs in many NLP tasks. Howev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch result diversification are unsupervised and they are based on handcrafted features and functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe  the more diversified it will be. The most typical implicit model is the MMR (Max Margin Relevance) <ref type=\"bibr\" target=\"#b4\">[5]</ref> model:</p><formula xml:id=\"formula_0\">Score MMR = \ud835\udf06score(\ud835\udc51 \ud835\udc56  reduce result redundancy by comparing document-document similarity regardless the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mum additional information utility for the current selected document sequence. However, researchers <ref type=\"bibr\" target=\"#b14\">[15]</ref> have already proved that this greedy document selection me al ranking, the model has to search all the ranking space, which is an NP-hard problem. Feng et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed the M2DIV model with Monte-Caro Tree Search (MCTS ult to train since MCTS is so time consuming that the M2DIV propose another raw policy without MCTS <ref type=\"bibr\" target=\"#b14\">[15]</ref> in adaption to some online ranking tasks.</p><p>In this pa . Based on the reinforced learning approach MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Feng <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed the M2DIV model with the Monte-Caro Tree Search ( imal and global optimal rankings. However, M2DIV is difficult to train since MCTS is time consuming <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and M2DIV only models the document novelty, ignoring the  p reinforced learning based models e.g. MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref> and M2DIV <ref type=\"bibr\" target=\"#b14\">[15]</ref> are taking too much time to train, we do not take those mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref>, and these queries could be ambiguous or vague. For example,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ty evaluation metrics of Web Track include ERR-IA <ref type=\"bibr\" target=\"#b27\">[28]</ref>, \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b28\">[29]</ref> and NRBP <ref type=\"bibr\" target=\"#b29\">[30]</ref>, which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e initial distributed representation of the document \ud835\udc51 \ud835\udc61 . In order to avoid overfitting, we follow <ref type=\"bibr\" target=\"#b21\">[22]</ref> and use unsupervised methods doc2vec <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: archers have used self-attention networks, e.g. GPT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ERNIE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, as al extraction and document representation e.g. K-NRM <ref type=\"bibr\" target=\"#b26\">[27]</ref> or BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Research shows that most queries issued by users are short <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed and they are based on handcrafted features and functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" targe enerate diversified results, these methods either explicitly model subtopic coverage of the results <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" targe are used in our experiments. Besides the metrics above, we also include the metrics of Precision-IA <ref type=\"bibr\" target=\"#b5\">[6]</ref> (denoted as Pre-IA) and Subtopic Recall <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s/1.0\"><head n=\"1\">INTRODUCTION</head><p>Research shows that most queries issued by users are short <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mum additional information utility for the current selected document sequence. However, researchers <ref type=\"bibr\" target=\"#b14\">[15]</ref> have already proved that this greedy document selection me al ranking, the model has to search all the ranking space, which is an NP-hard problem. Feng et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed the M2DIV model with Monte-Caro Tree Search (MCTS ult to train since MCTS is so time consuming that the M2DIV propose another raw policy without MCTS <ref type=\"bibr\" target=\"#b14\">[15]</ref> in adaption to some online ranking tasks.</p><p>In this pa . Based on the reinforced learning approach MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Feng <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed the M2DIV model with the Monte-Caro Tree Search ( imal and global optimal rankings. However, M2DIV is difficult to train since MCTS is time consuming <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and M2DIV only models the document novelty, ignoring the  p reinforced learning based models e.g. MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref> and M2DIV <ref type=\"bibr\" target=\"#b14\">[15]</ref> are taking too much time to train, we do not take those mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b16\">[17]</ref>, BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ERNIE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, as alternatives to RNNs and CNNs in many NLP tasks. Howev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. While in recent year verage of the results <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe supervised explicit approaches are proposed e.g. xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSS lts of Lemur as initial ranking sequences.</p><p>xQuAD <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD and HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Research shows that most queries issued by users are short <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b16\">[17]</ref>, BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ERNIE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, as alternatives to RNNs and CNNs in many NLP tasks. Howev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed and they are based on handcrafted features and functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" targe enerate diversified results, these methods either explicitly model subtopic coverage of the results <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" targe are used in our experiments. Besides the metrics above, we also include the metrics of Precision-IA <ref type=\"bibr\" target=\"#b5\">[6]</ref> (denoted as Pre-IA) and Subtopic Recall <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hods in search result diversification in order to learn an optimized ranking function automatically <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta document-document similarity regardless the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta e. Inheriting the spirit of MMR, researchers have also proposed supervised methods, such as SVM-DIV <ref type=\"bibr\" target=\"#b9\">[10]</ref>, R-LTR <ref type=\"bibr\" target=\"#b10\">[11]</ref>), PAMM <re hts of the hierarchical subtopic layers. The parameters are tuned with cross validation and ListMLE <ref type=\"bibr\" target=\"#b9\">[10]</ref> is used to learn a prior relevance function with no diversi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch result diversification are unsupervised and they are based on handcrafted features and functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe  the more diversified it will be. The most typical implicit model is the MMR (Max Margin Relevance) <ref type=\"bibr\" target=\"#b4\">[5]</ref> model:</p><formula xml:id=\"formula_0\">Score MMR = \ud835\udf06score(\ud835\udc51 \ud835\udc56  reduce result redundancy by comparing document-document similarity regardless the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. While in recent years, more and more researchers tried to u target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> (i.e., explicit appr D <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>Th ber of the queries is 10, and the average subtopic number is about 9.48. As those previous works do <ref type=\"bibr\" target=\"#b8\">[9]</ref> we treat all those subtopics with uniform weights.</p><p>For \" target=\"#foot_2\">3</ref> as the non-diversified baseline. These results are released by Hu et at. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and can be found on the website <ref type=\"foot\" target=\"#fo ef type=\"bibr\" target=\"#b6\">[7]</ref>, PM2 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD and HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>. These are the unsupervised explicit baseline approaches for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hods in search result diversification in order to learn an optimized ranking function automatically <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta document-document similarity regardless the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta e. Inheriting the spirit of MMR, researchers have also proposed supervised methods, such as SVM-DIV <ref type=\"bibr\" target=\"#b9\">[10]</ref>, R-LTR <ref type=\"bibr\" target=\"#b10\">[11]</ref>), PAMM <re hts of the hierarchical subtopic layers. The parameters are tuned with cross validation and ListMLE <ref type=\"bibr\" target=\"#b9\">[10]</ref> is used to learn a prior relevance function with no diversi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urbations to the input node features with graph structures unchanged. FLAG leverages \"free\" methods <ref type=\"bibr\" target=\"#b27\">(Shafahi et al., 2019)</ref> to conduct efficient adversarial trainin de feature space.</p><p>Augmentation for \"free\". We leverage the \"free\" adversarial training method <ref type=\"bibr\" target=\"#b27\">(Shafahi et al., 2019)</ref> to craft adversarial data augmentations.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#b39\">(Zeng et al., 2019</ref>) can all work with FLAG to further boost performance, while Cluster <ref type=\"bibr\" target=\"#b3\">(Chiang et al., 2019)</ref> suffers an accuracy drop.</p><p>Compatibil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms.</p><p>One promising solution to combat overfitting in deep neural networks is data augmentation <ref type=\"bibr\" target=\"#b19\">(Krizhevsky et al., 2012)</ref>, which is commonplace in computer vis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng adversarial perturbations to augment datasets and ultimately alleviate overfitting. For example, <ref type=\"bibr\" target=\"#b32\">Volpi et al. (2018)</ref> showed adversarial data augmentation is a d ction of highly heuristic. In light of the positive effect of large perturbations on generalization <ref type=\"bibr\" target=\"#b32\">(Volpi et al., 2018)</ref>, and also to simplify hyperparameter searc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b31\">Veli\u010dkovi\u0107 et al., 2017;</ref><ref type=\"bibr\" target=\"#b37\">Xu et al., 2019)</ref> and show that FLAG brings consistent and signi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n. To clarify, FLAG is intrinsically different from the previous graph adversarial training methods <ref type=\"bibr\" target=\"#b8\">(Feng et al., 2019;</ref><ref type=\"bibr\" target=\"#b5\">Deng et al., 20 \" target=\"#b5\">Deng et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Jin &amp; Zhang, 2019)</ref>. <ref type=\"bibr\" target=\"#b8\">Feng et al. (2019)</ref> proposed to reinforce local smoothness to mak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, meta-learning <ref type=\"bibr\" target=\"#b10\">(Garcia &amp; Bruna, 2017)</ref>, social analysis <ref type=\"bibr\" target=\"#b25\">(Qiu et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Li &amp; Goldw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -distribution samples, and its effectiveness has been verified in domains including computer vision <ref type=\"bibr\" target=\"#b36\">(Xie et al., 2020)</ref>, language understanding <ref type=\"bibr\" tar t of attention paid to leverage adversarial training for better clean performance in varied domains <ref type=\"bibr\" target=\"#b36\">(Xie et al., 2020;</ref><ref type=\"bibr\" target=\"#b43\">Zhu et al., 20 s to generalize GAT, and FLAG works to push the improvement further. In the computer vision domain, <ref type=\"bibr\" target=\"#b36\">Xie et al. (2020)</ref> proposed a new batch norm method that makes a vely augment graph data using adversarial perturbations. On large-scale image classification tasks, <ref type=\"bibr\" target=\"#b36\">Xie et al. (2020)</ref> leveraged adversarial perturbations, along wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ghbors into node representation learning and achieve state-ofthe-art performance for recommendation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta pervision Signal. Most models approach the recommendation task under a supervised learning paradigm <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta n users and items, where \ud835\udc66 \ud835\udc62\ud835\udc56 indicates that user \ud835\udc62 has adopted item \ud835\udc56 before. Most existing models <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta rimental Settings</head><p>We conduct experiments on three widely used benchmark datasets: Yelp2018 <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, Amazon-Book <ref  2018 <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, Amazon-Book <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, and Alibaba-iFash shion <ref type=\"bibr\" target=\"#b5\">[6]</ref> <ref type=\"foot\" target=\"#foot_0\">1</ref> . Following <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, we use the same 1 et=\"#b35\">[36]</ref>, and PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> since the previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta  type=\"bibr\" target=\"#b1\">[2]</ref>, to GCN that propagates user and item embeddings over the graph <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta  embedding and item embedding. Here we implement it on a state-of-the-art GCN-based model, LightGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Experimental studies on three benchmark datasets demonstr ibr\" target=\"#b47\">48]</ref>, concatenation <ref type=\"bibr\" target=\"#b43\">[44]</ref>, or summation <ref type=\"bibr\" target=\"#b16\">[17]</ref> over the representations of all layers.</p><p>Supervised L rization coefficient \ud835\udf06 2 and the number of GCN layers within the suggested ranges.</p><p>\u2022 LightGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>. This is the state-of-the-art graph-based CF method which   between LightGCN and SGL-ED.trainable parameters, the space complexity remains the same as LightGCN<ref type=\"bibr\" target=\"#b16\">[17]</ref>. The time complexity of model inference is also the same, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  (or item) into an embedding vector. Some followon studies <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> enrich the single ID with interaction history for learning . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in pre-training natural language model <ref type=\"bibr\" target=\"#b7\">[8]</ref> and graph structure <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Thus, we would li ions according to mutual information between a node and the local structure. In addition, Hu et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> extend the idea to learn GCN for graph representation. Fur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s: generative models <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and contrastive models <ref type=\"bibr\" target=\"#b4\">[5,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y, we follow SimCLR <ref type=\"bibr\" target=\"#b4\">[5]</ref> and adopt the contrastive loss, InfoNCE <ref type=\"bibr\" target=\"#b11\">[12]</ref>, to maximize the agreement of positive pairs and minimize  s of nodes: users and items, we formulate the objective function of SSL as the summation of InfoNCE <ref type=\"bibr\" target=\"#b11\">[12]</ref> losses on both user and item sides. Here we conduct ablati te L \ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b according to Eq. (5) 6 Evaluate L \ud835\udc60\ud835\udc52\ud835\udc59 \ud835\udc53 according to Eq. (11) 7Evaluate L according to Eq.<ref type=\"bibr\" target=\"#b11\">(12)</ref> </figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> and LightGCN [17]. Despite effectiveness, we argue that th  type=\"bibr\" target=\"#b18\">[19]</ref>, GC-MC <ref type=\"bibr\" target=\"#b35\">[36]</ref>, and PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> since the previous work <ref type=\"bibr\" target=\"#b16\">[17 et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>Despite effectiveness, current GCN-based recommendat p>which can be simply set as the last-layer representation <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, concatenation <ref type=\"bibr\" target=\"#b43\">[44]</ref>, o et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Recently, attention mechanism is introduced into GCN-based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y, we follow SimCLR <ref type=\"bibr\" target=\"#b4\">[5]</ref> and adopt the contrastive loss, InfoNCE <ref type=\"bibr\" target=\"#b11\">[12]</ref>, to maximize the agreement of positive pairs and minimize  s of nodes: users and items, we formulate the objective function of SSL as the summation of InfoNCE <ref type=\"bibr\" target=\"#b11\">[12]</ref> losses on both user and item sides. Here we conduct ablati te L \ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b according to Eq. (5) 6 Evaluate L \ud835\udc60\ud835\udc52\ud835\udc59 \ud835\udc53 according to Eq. (11) 7Evaluate L according to Eq.<ref type=\"bibr\" target=\"#b11\">(12)</ref> </figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r fair comparison, all models are trained from scratch which are initialized with the Xavier method <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The models are optimized by the Adam optimizer with  <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> and LightGCN [17]. Despite effectiveness, we argue that th  type=\"bibr\" target=\"#b18\">[19]</ref>, GC-MC <ref type=\"bibr\" target=\"#b35\">[36]</ref>, and PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> since the previous work <ref type=\"bibr\" target=\"#b16\">[17 et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>Despite effectiveness, current GCN-based recommendat p>which can be simply set as the last-layer representation <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, concatenation <ref type=\"bibr\" target=\"#b43\">[44]</ref>, o et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Recently, attention mechanism is introduced into GCN-based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s: generative models <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and contrastive models <ref type=\"bibr\" target=\"#b4\">[5,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b20\">[21]</ref> extend the idea to learn GCN for graph representation. Furthermore, Kaveh et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> adopt the contrastive model for learning both node and gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"1\">INTRODUCTION</head><p>Self-attention-based architectures, in particular Transformers <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref>, have become the model of choice in natu ww.tei-c.org/ns/1.0\"><head n=\"3\">METHOD</head><p>In model design we follow the original Transformer <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref> as closely as possible. An advantage of  sulting sequence of embedding vectors serves as input to the encoder.</p><p>The Transformer encoder <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref> consists of alternating layers of multih xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">RELATED WORK</head><p>Transformers were proposed by <ref type=\"bibr\" target=\"#b43\">Vaswani et al. (2017)</ref> for machine translation, and have since b i-c.org/ns/1.0\"><head>APPENDIX A MULTIHEAD SELF-ATTENTION</head><p>Standard qkv self-attention (SA, <ref type=\"bibr\" target=\"#b43\">Vaswani et al. (2017)</ref>) is a popular building block for neural a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\">(Tschannen et al., 2020)</ref>, and S4L -supervised plus semi-supervised learning on ImageNet <ref type=\"bibr\" target=\"#b52\">(Zhai et al., 2019a)</ref>.</p><p>ViT-H/14 outperforms BiT-R152x4, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: works (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification <ref type=\"bibr\" target=\"#b3\">(Bello et al., 2019)</ref> or by further processing the output of a CN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b40\">Touvron et al., 2019;</ref><ref type=\"bibr\" target=\"#b51\">Xie et al., 2020)</ref>. Moreover, <ref type=\"bibr\" target=\"#b38\">Sun et al. (2017)</ref> study how CNN performance scales with dataset k with 21k classes and 14M images <ref type=\"bibr\" target=\"#b12\">(Deng et al., 2009)</ref>, and JFT <ref type=\"bibr\" target=\"#b38\">(Sun et al., 2017)</ref> with 18k classes and 303M high-resolution im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\">(Tschannen et al., 2020)</ref>, and S4L -supervised plus semi-supervised learning on ImageNet <ref type=\"bibr\" target=\"#b52\">(Zhai et al., 2019a)</ref>.</p><p>ViT-H/14 outperforms BiT-R152x4, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ransformers in the context of image processing, several approximations have been tried in the past: <ref type=\"bibr\" target=\"#b32\">Parmar et al. (2018)</ref> applied the self-attention only in local n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: training. Appendix B.1.2 contains further details. We leave exploration of contrastive pre-training <ref type=\"bibr\" target=\"#b8\">(Chen et al., 2020b;</ref><ref type=\"bibr\" target=\"#b16\">He et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the vertex adjacency information (e.g. GAE <ref type=\"bibr\" target=\"#b21\">[22]</ref> and GraphSAGE <ref type=\"bibr\" target=\"#b22\">[23]</ref> in network embedding). This scheme can be very limited (as tioned limitations of proximity-based pre-training methods <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar rying to reconstruct the adjacency information of vertices <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> can be treated as a kind of \"local contrast\", while over-em. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Recently, in visual representation learning, contrastive learning has renewed a surge of interest <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta recent surge of interest in visual representation learning <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar normalized temperature-scaled cross entropy loss (NT-Xent) <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>.</p><p>During GNN p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: architectures that suffer from gradient vanishing/explosion <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. The reasons behind the intriguing phenomena could be that  ameters in a \"better\" attraction basin around a local minimum associated with better generalization <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Therefore, we emphasize the significance of GNN pre-train. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  under proper transformations, raising a recent surge of interest in visual representation learning <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: evel representation learning methods as node2vec <ref type=\"bibr\" target=\"#b65\">[66]</ref>, sub2vec <ref type=\"bibr\" target=\"#b66\">[67]</ref>, graph2vec <ref type=\"bibr\" target=\"#b64\">[65]</ref> and I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, link prediction <ref type=\"bibr\" target=\"#b6\">[7]</ref> and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b5\">6]</ref>, link prediction <ref type=\"bibr\" target=\"#b6\">[7]</ref> and graph classification <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. Intriguingly, in most. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. However, it is not. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GK), we also compare with four unsupervised graph-level representation learning methods as node2vec <ref type=\"bibr\" target=\"#b65\">[66]</ref>, sub2vec <ref type=\"bibr\" target=\"#b66\">[67]</ref>, graph2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions <ref type=\"bibr\" target=\"#b22\">(Klicpera et al., 2018)</ref>. However, they focus on integrating thi ep, decoupled from the other steps. This type of prediction smoothing is similar in spirit to APPNP <ref type=\"bibr\" target=\"#b22\">(Klicpera et al., 2018)</ref>, which we compare against later. Howeve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b11\">Gleich &amp; Mahoney, 2015;</ref><ref type=\"bibr\">Peel, 2017;</ref><ref type=\"bibr\" target=\"#b5\">Chin et al., 2019)</ref> but have largely been ignored in GNNs. That b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  nodes is at the center of much network analysis and corresponds to homophily or assortative mixing <ref type=\"bibr\" target=\"#b27\">(McPherson et al., 2001;</ref><ref type=\"bibr\" target=\"#b30\">Newman, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: formance gains. We can also incorporate our techniques into big GNN models, providing modest gains. <ref type=\"bibr\" target=\"#b11\">Mahoney, 2015)</ref>. The salient point for this paper is that we ass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: diffusion-based semi-supervised learning algorithms on graphs such as the spectral graph transducer <ref type=\"bibr\" target=\"#b20\">(Joachims, 2003)</ref>, Gaussian random field models <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b11\">Gleich &amp; Mahoney, 2015;</ref><ref type=\"bibr\">Peel, 2017;</ref><ref type=\"bibr\" target=\"#b5\">Chin et al., 2019)</ref> but have largely been ignored in GNNs. That b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: three classic citation network benchmarks <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012) al to 0.01.</p><p>\u2022 Cora, Citseer, Pubmed <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on et al., 2017b)</ref>. In our pipeline, we augment features with a regularized spectral embedding <ref type=\"bibr\" target=\"#b3\">(Chaudhuri et al., 2012;</ref><ref type=\"bibr\" target=\"#b48\">Zhang &am. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of a European research institute, where classes are department membership and there are no features <ref type=\"bibr\" target=\"#b24\">(Leskovec et al., 2007;</ref><ref type=\"bibr\" target=\"#b46\">Yin et al target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012)</ref> and Email <ref type=\"bibr\" target=\"#b24\">(Leskovec et al., 2007;</ref><ref type=\"bibr\" target=\"#b46\">Yin et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: somorphism Networks <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2018)</ref>, and various deep models <ref type=\"bibr\" target=\"#b25\">(Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Rong et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: three classic citation network benchmarks <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012) al to 0.01.</p><p>\u2022 Cora, Citseer, Pubmed <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e features <ref type=\"bibr\" target=\"#b15\">(Henderson et al., 2011;</ref><ref type=\"bibr\">2012;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton et al., 2017b)</ref>. In our pipeline, we augment features w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: three classic citation network benchmarks <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012) al to 0.01.</p><p>\u2022 Cora, Citseer, Pubmed <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Networks <ref type=\"bibr\" target=\"#b40\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, Graph Isomorphism Networks <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2018)</ref>, and various deep models <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e dorm residences and features are attributes such as gender, major, and class year, amongst others <ref type=\"bibr\" target=\"#b38\">(Traud et al., 2012)</ref>, as well as a geographic dataset of US cou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: u et al., 2020)</ref>; the Cora, Citeseer, and Pubmed are three classic citation network benchmarks <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005 s: 3 layers and 256 hidden channels with learning rate equal to 0.01.</p><p>\u2022 Cora, Citseer, Pubmed <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2005\" xml:id=\"foot_2\">; Wang &amp; Zhang, 2007;<ref type=\"bibr\" target=\"#b33\">Raghavan et al., 2007;</ref> Gleich &amp;   </note> \t\t</body> \t\t<back. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  techniques have been used for learning on relational data from just the labels (i.e., no features) <ref type=\"bibr\" target=\"#b23\">(Koutra et al., 2011;</ref><ref type=\"bibr\" target=\"#b11\">Gleich &amp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b25\">(Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Rong et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020)</ref>. Many ideas for new GNN architectures are ada rd, as of October 1, 2020). For Cora, Citeseer and Pubmed, we reuse the top performance scores from <ref type=\"bibr\" target=\"#b4\">Chen et al. (2020)</ref>. For Email and US County, we use GCNII <ref t  from <ref type=\"bibr\" target=\"#b4\">Chen et al. (2020)</ref>. For Email and US County, we use GCNII <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020)</ref>. For Rice31, we use GCN with spectral and no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: posed to maximize the MI between node embeddings and a global summary embedding. Following DGI, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposes two node-level contrastive objectives to directly  the agreement of node embeddings across two corrupted views of the graph.</p><p>Following DGI, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref> employs two discriminators to directly measure MI between   summary, we provide a brief comparison between the  <ref type=\"bibr\" target=\"#b43\">[44]</ref>, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> in Ta ax (DGI) <ref type=\"bibr\" target=\"#b43\">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: icit data augmentation. Moreover, to supplement the input graph with more global information, MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposes to augment the input graph using graph diffusion. asure MI between input and representations of both nodes and edges without data augmentation; MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposes to learn both node-level and graph-level represen <ref type=\"bibr\" target=\"#b43\">[44]</ref>, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> in Table <ref type=\"table\" target=\"#tab_0\">1</ref>, where  MI) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Furthermore, we report the performance obtained using a l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Prior work on unsupervised graph representation learning f s circumstance are nodes appearing in the same random walk <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. For example, the pioneering work DeepWalk <ref type=\"bibr\" to be error-prone with inappropriate hyperparameter tuning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>.</p><p>Recent work on graph neural networks (GNN) employs m #b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. For example, the pioneering work DeepWalk <ref type=\"bibr\" target=\"#b31\">[32]</ref> models probabilities of node co-occurrence pairs using noi eline methods belonging to the following two categories, (1) traditional methods including DeepWalk <ref type=\"bibr\" target=\"#b31\">[32]</ref> and node2vec <ref type=\"bibr\" target=\"#b10\">[11]</ref> and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> and natural language processing <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: success in many fields, e.g., visual representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> and natural languag get=\"#b8\">[9]</ref>, color distortion <ref type=\"bibr\" target=\"#b22\">[23]</ref>, etc. Existing work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es about computer science and edges are hyperlinks between the articles. Nodes are labeled with ten <ref type=\"bibr\" target=\"#b9\">(10)</ref> classes each representing a branch of the field. Node featu ad>A.2 Hyperparameter Specifications</head><p>All models are initialized with Glorot initialization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and trained using Adam SGD optimizer <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b38\">39]</ref> and natural language processing <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. These CL methods see. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ikely to correspond to influential papers.</p><p>Eigenvector centrality. The eigenvector centrality <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> of a node is calcula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve samples. Other work <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> explores in-batch negative samples. For an image patch as t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and structural features. However, existing GNN models are mostly established in a supervised manner <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta tional methods. Among them, considerable literature has grown up around the theme of supervised GNN <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: representing a branch of the field. Node features are calculated as the average of pretrained GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref> word embeddings of the words in each article.</p><p>\u2022 Amaz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: adout function such that the graph embedding may be deteriorated. In contrast to DGI, previous work <ref type=\"bibr\" target=\"#b50\">[51]</ref> proposes to not rely on an explicit graph embedding, but r n, we consider a direct way for corrupting input graphs where we randomly remove edges in the graph <ref type=\"bibr\" target=\"#b50\">[51]</ref>. Formally, we sample a modified subset E from the original. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 32]</ref> models probabilities of node co-occurrence pairs using noise-contrastive estimation (NCE) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. These random-walk-based methods are proved to be equivale. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ILS A.1 Computing Infrastructures</head><p>All models are implemented using PyTorch Geometric 1.6.1 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, PyTorch 1.6.0 <ref type=\"bibr\" target=\"#b28\">[29]</ref>, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uivalent to factorizing some forms of graph proximity (e.g., transformation of the adjacent matrix) <ref type=\"bibr\" target=\"#b34\">[35]</ref>, which overly emphasize on the structural information enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b38\">39]</ref> and natural language processing <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. These CL methods see. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es about computer science and edges are hyperlinks between the articles. Nodes are labeled with ten <ref type=\"bibr\" target=\"#b9\">(10)</ref> classes each representing a branch of the field. Node featu ad>A.2 Hyperparameter Specifications</head><p>All models are initialized with Glorot initialization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and trained using Adam SGD optimizer <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: success in many fields, e.g., visual representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> and natural languag get=\"#b8\">[9]</ref>, color distortion <ref type=\"bibr\" target=\"#b22\">[23]</ref>, etc. Existing work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b17\">[18]</ref>.</p><p>Theoretical analysis sheds light on the reasons behind their success <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Objectives used in these methods can be seen as maximizin lower bound of the InfoNCE objective <ref type=\"bibr\" target=\"#b41\">[42]</ref>, which is defined as <ref type=\"bibr\" target=\"#b32\">[33]</ref>. According to van den Oord et al. <ref type=\"bibr\" target= ive J and the InfoNCE objective <ref type=\"bibr\" target=\"#b41\">[42]</ref> , which can be defined as <ref type=\"bibr\" target=\"#b32\">[33]</ref> \ud835\udc3c NCE (U; V) \u225c E   </p><p>Thus, we arrive at 2J \u2264 \ud835\udc3c (U; V) -c.org/ns/1.0\" xml:id=\"fig_7\"><head>( 17 )</head><label>17</label><figDesc>According to Poole et al.<ref type=\"bibr\" target=\"#b32\">[33]</ref>, the InfoNCE estimator is a lower bound of the true MI, i. een widely applied in the representation learning literature <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e paradigm as well <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Prior work on unsu br\" target=\"#b10\">[11]</ref> and (2) deep learning methods including Graph Autoencoders (GAE, VGAE) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b43\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tailed statistics is summarized in Table <ref type=\"table\" target=\"#tab_1\">2</ref>.</p><p>\u2022 Wiki-CS <ref type=\"bibr\" target=\"#b24\">[25]</ref> is a reference network constructed from Wikipedia.</p><p>T ures. For the Wiki-CS dataset, we evaluate the models on the public splits shipped with the dataset <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Regarding the other four co-coauthor and co-purchase data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 32]</ref> models probabilities of node co-occurrence pairs using noise-contrastive estimation (NCE) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. These random-walk-based methods are proved to be equivale. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion.</p><p>Accurately predicting (black-box) workload criticality is itself a challenge. Prior work <ref type=\"bibr\" target=\"#b5\">[6]</ref> associated a diurnal utilization pattern with user interacti system</head><p>To integrate predictions into VM scheduling in practice, we target Resource Central <ref type=\"bibr\" target=\"#b5\">[6]</ref>, the existing ML and predictionserving system in Azure. The  e workload of each VM is performance-critical or not, before we can train a model. As in prior work <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we consider a workload critical if it is user-facing, i.e.  he problem reduces to identifying VMs whose time series of CPU utilizations exhibit 24-hour periods <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Obviously, some background VMs may exhibit 24-hour pe identifying periods in time series, such as FFT or the autocorrelation function (ACF). For example, <ref type=\"bibr\" target=\"#b5\">[6]</ref> assumes a workload is user-facing if the FFT indicates a 24- /task length for provisioning or scheduling purposes, e.g. <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[20]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Both modeling/. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tiple services and deploy them to prevent correlated peaks <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urce demand, resource utilization, or job/task length for provisioning or scheduling purposes, e.g. <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orage to shave power peaks in oversubscribed datacenters <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>. When peaks last long, this approach may require large amo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8\">[9]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Our work extends these works by using predictions to plac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8\">[9]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Our work extends these works by using predictions to plac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: M contains a certain piece of knowledge <ref type=\"bibr\" target=\"#b8\">(Hewitt and Liang, 2019;</ref><ref type=\"bibr\" target=\"#b26\">Voita and Titov, 2020)</ref>. Attention visualization, another common. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ighly sensitive to this context: improperly-constructed contexts cause artificially low performance <ref type=\"bibr\" target=\"#b11\">(Jiang et al., 2020)</ref>. Overcoming the need to manually specify p cretely, we achieve 43.3% precision-at-1, compared to the current best singleprompt result of 34.1% <ref type=\"bibr\" target=\"#b11\">(Jiang et al., 2020)</ref>. We also introduce a variant of this task, reated prompts with missing objects, e.g. \"Obama was born in <ref type=\"bibr\">[MASK]</ref>.\". LPAQA <ref type=\"bibr\" target=\"#b11\">(Jiang et al., 2020)</ref> extends this idea by systematically creati >(Lewis et al., 2019)</ref>, fact recall <ref type=\"bibr\" target=\"#b19\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Bouraoui et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b19\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Bouraoui et al., 2019)</ref>, summarization <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ling task by appending \"TL;DR:\" to the end of an article and then generating from an LM. Similarly, <ref type=\"bibr\" target=\"#b19\">Petroni et al. (2019)</ref> manually reformulate a knowledge base com 1.0\"><head>Relation Breakdown</head><p>We also provide a detailed breakdown of the prompts found by <ref type=\"bibr\" target=\"#b19\">Petroni et al. (2019)</ref> and AUTOPROMPT, and their associated accu ecially well for relations that are difficult to specify in a natural language prompt. For example, <ref type=\"bibr\" target=\"#b19\">Petroni et al. (2019)</ref>'s prompt for the PO-SITION PLAYED ON TEAM ual Retrieval: On the left, we evaluate BERT on fact retrieval using the Original LAMA dataset from <ref type=\"bibr\" target=\"#b19\">Petroni et al. (2019)</ref>. For all three metrics (mean reciprocal r ction model <ref type=\"bibr\" target=\"#b24\">(Sorokin and Gurevych, 2017</ref>) that was also used by <ref type=\"bibr\" target=\"#b19\">Petroni et al. (2019)</ref>. To make the evaluation fair for the supe iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B Relation Extraction Details</head><p>Following <ref type=\"bibr\" target=\"#b19\">Petroni et al. (2019)</ref>, we use the pretrained RE model from <ref  type=\"table\">6</ref>: A breakdown of all relations for fact retrieval on the original dataset from <ref type=\"bibr\" target=\"#b19\">Petroni et al. (2019)</ref>. We compare P@1 of prompts generated by L =\"#b16\">(Marelli et al., 2014)</ref>. Next, we apply AUTOPROMPT to the fact retrieval tasks of LAMA <ref type=\"bibr\" target=\"#b19\">(Petroni et al., 2019)</ref>, where we are able to construct prompts  mportant question is whether pretrained MLMs know facts about real-world entities. The LAMA dataset <ref type=\"bibr\" target=\"#b19\">(Petroni et al., 2019)</ref> evaluates this using cloze tests that co  relation rel and the correct object obj is the label token. We use the original test set from LAMA <ref type=\"bibr\" target=\"#b19\">(Petroni et al., 2019)</ref>, henceforth Original. To collect trainin 0)</ref>, question answering <ref type=\"bibr\" target=\"#b13\">(Lewis et al., 2019)</ref>, fact recall <ref type=\"bibr\" target=\"#b19\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Jiang et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Shwartz et al., 2020)</ref>, question answering <ref type=\"bibr\" target=\"#b13\">(Lewis et al., 2019)</ref>, fact recall <ref type=\"bibr\" target=\"#b19. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t caused by the underlying target knowledge, leading to criticism against their use as explanations <ref type=\"bibr\" target=\"#b10\">(Jain and Wallace, 2019;</ref><ref type=\"bibr\" target=\"#b28\">Wiegreff ed for all inputs, and is learned using a variant of the gradient-based search strategy proposed in <ref type=\"bibr\" target=\"#b10\">Wallace et al. (2019)</ref>. The LM predictions for the prompt are co e modeling task using prompts. Here, we propose a method for automatic prompt construction based on <ref type=\"bibr\" target=\"#b10\">Wallace et al. (2019)</ref>. The idea is to add a number of \"trigger\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: AQA (their prompts are still useful here), as well as a recent supervised relation extraction model <ref type=\"bibr\" target=\"#b24\">(Sorokin and Gurevych, 2017</ref>) that was also used by <ref type=\"b ing <ref type=\"bibr\" target=\"#b19\">Petroni et al. (2019)</ref>, we use the pretrained RE model from <ref type=\"bibr\" target=\"#b24\">Sorokin and Gurevych (2017)</ref> as our baseline. To encode the sent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: using an LMs' representations as features <ref type=\"bibr\" target=\"#b3\">(Conneau et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Liu et al., 2019)</ref>. However, probing classifiers require additio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g.</p><p>Setup We apply our method to convert instances from the binary Stanford Sentiment Treebank <ref type=\"bibr\" target=\"#b23\">(Socher et al., 2013</ref>, SST-2) into prompts, using the standard t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b18\">(Peters et al., 2018)</ref>), and 69% accuracy on a balanced variant of the SICK-E dataset <ref type=\"bibr\" target=\"#b16\">(Marelli et al., 2014)</ref>. Next, we apply AUTOPROMPT to the fact r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sults in Table <ref type=\"table\" target=\"#tab_0\">1</ref>, along with reference scores from the GLUE <ref type=\"bibr\" target=\"#b27\">(Wang et al., 2019)</ref> SST-2 leaderboard, and scores for a linear . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e challenging. A number of GPM frameworks have been proposed to reduce the burden on the programmer <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta lgorithms. Low-level systems such as RStream <ref type=\"bibr\" target=\"#b55\">[56]</ref> and Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref> provide low-level API functions for the user to control th form the state-of-the-art GPM systems, AutoMine <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Peregrine <ref type=\"bibr\" target=\"#b28\">[29]</ref> b e an example in Appendix B.6). This technique is called Memoization of Embedding Connectivity (MEC) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. \u2022 For edge-induced extension, a set of edges instead of v tern Classification (CP): FP and CP are low-level optimizations enabled in a prior system, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, so we describe them in Appendices B.4 and B.5. To support type=\"foot\" target=\"#foot_0\">3</ref> : AutoMine <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Peregrine <ref type=\"bibr\" target=\"#b28\">[29]</ref>.  \"#b10\">[11]</ref> is a distributed GPM system which incorporates task-parallel processing. Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref> is a shared-memory GPM system targeting both CPU and GPU.  ferent patterns. For small implicit patterns, Sandslash uses customized pattern classification (CP) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For example, in FSM, the labeled wedge patterns can be di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: orms. For TC, there are parallel solvers on multicore CPUs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. One example is motif counting <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\">23,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which counts the number of occurrences of certain structur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. One example is motif counting <ref type=\"bibr\" target=\"#b5 domains, so they can be used as a \"signature\" to infer, for example, the probable origin of a graph <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>Programming efficient parallel solutions for GPM pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>. kClist <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a parallel \ud835\udc58-CL algorithm derived from <ref type=\"bibr\" target=\"#b13\">[14]</ref>. It constructs DAG using a core value based ordering to re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that uses a compressed sparse embedding (CSE) data structure to reduce memory consumption. G-Miner <ref type=\"bibr\" target=\"#b10\">[11]</ref> is a distributed GPM system which incorporates task-parall <ref type=\"bibr\" target=\"#b53\">[54]</ref>, RStream<ref type=\"bibr\" target=\"#b55\">[56]</ref>, G-Miner<ref type=\"bibr\" target=\"#b10\">[11]</ref>, and Fractal<ref type=\"bibr\" target=\"#b18\">[19]</ref>.</no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  GPM applications targeting various platforms. For TC, there are parallel solvers on multicore CPUs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b60\">61]</ref>. They can be catego ity, but they may not allow expressing more efficient algorithms. Low-level systems such as RStream <ref type=\"bibr\" target=\"#b55\">[56]</ref> and Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref> pro /ref> is a distributed GPM system that proposed the embedding-centric programming paradigm. RStream <ref type=\"bibr\" target=\"#b55\">[56]</ref> is an out-of-core GPM system on a single machine. Its prog faster than previous GPM systems such as Arabesque<ref type=\"bibr\" target=\"#b53\">[54]</ref>, RStream<ref type=\"bibr\" target=\"#b55\">[56]</ref>, G-Miner<ref type=\"bibr\" target=\"#b10\">[11]</ref>, and Fra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for FSM. We also include widely used large graphs (Lj, Or, Tw4, Fr, Uk), and a very large web-crawl <ref type=\"bibr\" target=\"#b9\">[10]</ref> (Gsh). These graphs do not have labels and are only used fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ss on non-autoregressive sequence generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein T  maximum number of decoding steps is reached <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019)</ref>.<ref type=\"foot\" target=\"#foot_3\">5 n et al., 2018)</ref> or multi-pass decoding <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Gu et s by iteratively editing the outputs from previous iterations. Edit operations such as substitution <ref type=\"bibr\" target=\"#b20\">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: h Repositioning (EDITOR), which builds on recent progress on non-autoregressive sequence generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad   et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Stern et al., 2018)</ref> or multi-pass decoding <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad  \"bibr\" target=\"#b22\">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad  s widely used in nonautoregressive generation <ref type=\"bibr\" target=\"#b21\">(Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Gu et al., 2019. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: slation quality on par or better than both EDITOR and Levenshtein Transformer with hard constraints <ref type=\"bibr\" target=\"#b47\">(Susanto et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c. r\" target=\"#b16\">(Dinu et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Post and Vilar, 2018;</ref><ref type=\"bibr\" target=\"#b47\">Susanto et al., 2020)</ref>. Compared to <ref type=\"bibr\" target=\"#b3 constraints (Table <ref type=\"table\" target=\"#tab_6\">6</ref>). Consistent with previous findings by <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref>, incorporating soft constraints in LevT i s in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on IATE. Enforcing hard constraints as in <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref> increases the term usage by +8-10% and im  they help close the small gap to reach 100% term usage and do not 14 We use our implementations of <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref>'s technique for a more controlled compari b47\">Susanto et al. (2020)</ref>'s technique for a more controlled comparison. The LevT baseline in <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref> achieves higher BLEU than ours on the sma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: een introduced by designing complex architectures tailored to specific content or style constraints <ref type=\"bibr\" target=\"#b0\">(Abu Sheikha and Inkpen, 2011;</ref><ref type=\"bibr\" target=\"#b29\">Mei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tence <ref type=\"bibr\" target=\"#b4\">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref type=\"bibr\" target=\"#b17\">(Durrani et al., 2015;</ref><ref type=\"bibr\" target=\"#b44\">Stahlberg . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y generating a sequence of tokens in parallel <ref type=\"bibr\" target=\"#b21\">(Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">van den Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Ma et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nts better? We verify that this is not the case by computing the target word F1 binned by frequency <ref type=\"bibr\" target=\"#b31\">(Neubig et al., 2019)</ref>. Figure <ref type=\"figure\" target=\"#fig_3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: seful for interactive machine translation <ref type=\"bibr\" target=\"#b19\">(Foster et al., 2002;</ref><ref type=\"bibr\" target=\"#b5\">Barrachina et al., 2009)</ref> and domain adaptation <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndependence assumptions between target tokens <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2019;</ref><ref type=\"bibr\" target=\"#b52\">Wang et al., 2019)</ref>. These issues have been addressed via partia. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een introduced by designing complex architectures tailored to specific content or style constraints <ref type=\"bibr\" target=\"#b0\">(Abu Sheikha and Inkpen, 2011;</ref><ref type=\"bibr\" target=\"#b29\">Mei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>): Romanian-English (Ro-En) from WMT16 (Bojar et al., 2016), English-German (En-De) from WMT14 <ref type=\"bibr\" target=\"#b6\">(Bojar et al., 2014)</ref>, and English-Japanese (En-Ja) from WAT2017 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ions close to the cross-entropy loss are better suited to deep neural models than the squared error <ref type=\"bibr\" target=\"#b25\">(Leblond et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Cheng et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  preferences have previously been incorporated by re-training NMT models with constraints as inputs <ref type=\"bibr\" target=\"#b43\">(Song et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Dinu et al.,  n parallel samples augmented with constraint target phrases in both the source and target sequences <ref type=\"bibr\" target=\"#b43\">(Song et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Dinu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g, the reposition operation is defined to preserve the ability to use the Levenshtein edit distance <ref type=\"bibr\" target=\"#b27\">(Levenshtein, 1966)</ref> as an efficient oracle. We also introduce a nd insertion operations used in EDITOR are designed so that the Levenshtein edit distance algorithm <ref type=\"bibr\" target=\"#b27\">(Levenshtein, 1966)</ref> can be used as the oracle. The reposition o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een introduced by designing complex architectures tailored to specific content or style constraints <ref type=\"bibr\" target=\"#b0\">(Abu Sheikha and Inkpen, 2011;</ref><ref type=\"bibr\" target=\"#b29\">Mei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  models that decode from left-to-right are the de facto standard for many sequence generation tasks <ref type=\"bibr\" target=\"#b12\">(Cho et al., 2014;</ref><ref type=\"bibr\" target=\"#b13\">Chorowski et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s/1.0\"><head n=\"3.2\">Dual-Path Imitation Learning</head><p>We train EDITOR using imitation learning <ref type=\"bibr\" target=\"#b15\">(Daum\u00e9 III et al., 2009;</ref><ref type=\"bibr\" target=\"#b39\">Ross et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nces in output lexical choice. Building on recent models for non-autoregressive sequence generation <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref>, EDI-TOR generates new sequences by iterative Empirically, EDITOR uses soft lexical constraints more effectively than the Levenshtein Transformer <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref> while speeding up decoding dramatically compa bibr\" target=\"#b20\">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein Transformer <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref> showed that iteratively refining output seque omparable or better translation quality with faster decoding speed than the Levenshtein Transformer <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref> on the standard MT tasks and exploit soft lex bstitution <ref type=\"bibr\" target=\"#b20\">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref> have reduced the quality gap between non-auto ng the roll-in sequences: we first create the initial sequence y 0 by applying random word dropping <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref> and random word shuffle <ref type=\"bibr\">(Lam 1) to (3). We stop refining if 1) the output sequences from two consecutive iterations are the same <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps i  their AR teachers with 2-4 times speedup. BLEU differences are small (\u2206 &lt; 1.1) as in prior work <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref>. The RIBES trends are more surprising: both N ed that ED-ITOR exploits soft lexical constraints more effectively than the Levenshtein Transformer <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2019)</ref> while speeding up decoding dramatically compa rget=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Gu et al., 2019)</ref>. This work adopts multipass decoding, where th e=\"bibr\" target=\"#b21\">(Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Gu et al., 2019)</ref>. Specifically, when training the non-autoregre e hidden state h i and each input embedding e j or the deletion vector b.</p><p>Insertion Following <ref type=\"bibr\" target=\"#b22\">Gu et al. (2019)</ref>, the insertion operation consists of two phase </formula><p>where the mixture factor \u03b1 \u2208 [0, 1] and random variable u \u223c Uniform(0, 1).</p><p>While <ref type=\"bibr\" target=\"#b22\">Gu et al. (2019)</ref> define roll-in using only the model's insertio  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Experimental Settings</head><p>Dataset Following <ref type=\"bibr\" target=\"#b22\">Gu et al. (2019)</ref>, we experiment on three language pairs spannin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: els and boldface the top scores among NAR models based on the paired bootstrap test with p &lt; 0.05<ref type=\"bibr\" target=\"#b14\">(Clark et al., 2011)</ref>. EDITOR decodes 6-7% faster than LevT on R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een introduced by designing complex architectures tailored to specific content or style constraints <ref type=\"bibr\" target=\"#b0\">(Abu Sheikha and Inkpen, 2011;</ref><ref type=\"bibr\" target=\"#b29\">Mei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Neural machine translation (MT) architectures <ref type=\"bibr\" target=\"#b3\">(Bahdanau et al., 2015;</ref><ref type=\"bibr\" target=\"#b49\">Vaswani et tion and reordering operations over bilingual minimal units. By contrast, autoregressive NMT models <ref type=\"bibr\" target=\"#b3\">(Bahdanau et al., 2015;</ref><ref type=\"bibr\" target=\"#b49\">Vaswani et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e-level models that generate a bag of target words that is reordered to construct a target sentence <ref type=\"bibr\" target=\"#b4\">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2002;</ref><ref type=\"bibr\" target=\"#b5\">Barrachina et al., 2009)</ref> and domain adaptation <ref type=\"bibr\" target=\"#b23\">(Hokamp and Liu, 2017)</ref>. Lexical constraints or preferences have pe=\"bibr\" target=\"#b48\">Tang et al., 2016)</ref>. Despite their success at domain adaptation for MT <ref type=\"bibr\" target=\"#b23\">(Hokamp and Liu, 2017)</ref> and caption generation <ref type=\"bibr\"  #b16\">Dinu et al., 2019)</ref> or with constrained beam search that drastically slows down decoding <ref type=\"bibr\" target=\"#b23\">(Hokamp and Liu, 2017;</ref><ref type=\"bibr\" target=\"#b36\">Post and V trained decoding where beam search is modified to include constraint words or phrases in the output <ref type=\"bibr\" target=\"#b23\">(Hokamp and Liu, 2017;</ref><ref type=\"bibr\" target=\"#b36\">Post and V. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: seful for interactive machine translation <ref type=\"bibr\" target=\"#b19\">(Foster et al., 2002;</ref><ref type=\"bibr\" target=\"#b5\">Barrachina et al., 2009)</ref> and domain adaptation <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b41\">(Sennrich et al., 2016a;</ref><ref type=\"bibr\" target=\"#b18\">Ficler and Goldberg, 2017;</ref><ref type=\"bibr\" target=\"#b40\">Scarton and Specia, 2018)</ref>, which condition generation on users'. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b21\">(Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">van den Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Ma et al., 2019)</ref>. However, their output quality suffers due to  y suffers due to the large decoding space and strong independence assumptions between target tokens <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2019;</ref><ref type=\"bibr\" target=\"#b52\">Wang et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: earning has been closing the gap with, and in some cases even surpassing its supervised counterpart <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ air), creating contradicting objectives.</p><p>While recent efforts focus on improved architectures <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ef type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and data augmentation <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, relatively little w get=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>.</p><p>In contrastive learning, the embedding space is govern different images, regardless of their semantic information <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Figure <ref type=\"fi s this problem by maintaining a momentum encoder and a limited queue of previous samples. SimCLR v1 <ref type=\"bibr\" target=\"#b8\">[9]</ref> eschews a momentum encoder in favor of a large batch size, a \"bibr\" target=\"#b33\">[34]</ref> 63.6 -PCL <ref type=\"bibr\" target=\"#b31\">[32]</ref> 65.9 -SimCLR v1 <ref type=\"bibr\" target=\"#b8\">[9]</ref> 69.3 89.0 MoCo v2 <ref type=\"bibr\" target=\"#b10\">[11]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b21\">22]</ref> and data augmentation <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, relatively little work considers the effects of negative s e use a 65k memory buffer for the momentum encoder, with a momentum of 0.999. Following Tian et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref>, we use random crops, color distortion, Gaussian blur, and target=\"#b10\">[11]</ref> 71.1 -SimCLR v2 <ref type=\"bibr\" target=\"#b9\">[10]</ref> 71.7 90.4 InfoMin <ref type=\"bibr\" target=\"#b42\">[43]</ref> 73.0 91.  Method AP50 Supervised 81.3 MoCo v2 <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Caron et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> iteratively improve the learned representations by clusterin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: epresentation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Caron et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> itera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in visual representations, there has been a recent surge in self-supervised representation learning <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta mploys proxy tasks to guide the learned embeddings, such as predicting the angle of a rotated image <ref type=\"bibr\" target=\"#b18\">[19]</ref>, the relative location of patches <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Caron et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> iteratively improve the learned representations by clusterin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ives. Most existing methods focus on mining hard negatives <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, or most recently, increasing positive samples to counter-b  hard negatives (i.e., those that are close to the anchor) <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, and using more positive samples to counter-balance the eff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b28\">[29]</ref>, Birdsnap <ref type=\"bibr\" target=\"#b3\">[4]</ref>, SUN397 <ref type=\"bibr\" target=\"#b45\">[46]</ref>, Cars <ref type=\"bibr\" target=\"#b27\">[28]</ref>, Aircraft . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a recent surge in self-supervised representation learning <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar ngle of a rotated image <ref type=\"bibr\" target=\"#b18\">[19]</ref>, the relative location of patches <ref type=\"bibr\" target=\"#b14\">[15]</ref>, or organizing shuffled patches to recover the original im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lassification Following SimCLR, we perform the same evaluations on 12 classification datasets: Food <ref type=\"bibr\" target=\"#b4\">[5]</ref>, CIFAR10 <ref type=\"bibr\" target=\"#b28\">[29]</ref>, CIFAR100. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly finds false negatives with \u223c40% accuracy among 1000 humandefined semantic categories on ImageNet <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Our work is the first to address this problem. \u2022 We propo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion probabilistic modeling (DDPM) <ref type=\"bibr\" target=\"#b39\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Ho et al., 2020)</ref> trains a sequence of probabilistic models to r  at generation of images <ref type=\"bibr\">(Song &amp; Ermon, 2019;</ref><ref type=\"bibr\">2020;</ref><ref type=\"bibr\" target=\"#b17\">Ho et al., 2020)</ref>, audio <ref type=\"bibr\" target=\"#b6\">(Chen et  s of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> has reported higher sample quality than <ref t  Eq. ( <ref type=\"formula\" target=\"#formula_3\">3</ref>) described here is equivalent to L simple in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, but we re-write it in a slightly different fo rget=\"#b16\">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the same DDPM model in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we obtain better bits/dim compared to the upp e=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref>. With PC samplers and the same model architecture in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, the gaps can be reduced, but score-based mode  data pxq. In our experiments, we let \u03b2min \" 0.1 and \u03b2max \" 20, which correspond to the settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. The perturbation kernel is given by</p><formu arget=\"#fig_4\">4</ref>, we use a DDPM model trained on 256 \u02c6256 CelebA-HQ with the same settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. We use the RK45 ODE solver <ref type=\"bibr\" t on and temperature scaling. The model tested here is a DDPM model trained with the same settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns tially a different discretization to the same reverse-time SDE. This unifies the sampling method in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> as a numerical solver to the reverse-time VP S  \u00b4\u03c32 i\u00b41 qIq, i \" 1, 2, \u00a8\u00a8\u00a8, N.</formula><p>Here we assume \u03c3 0 \" 0 to simplify notations. Following <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we can compute</p><formula xml:id=\"formula_55  \" x i `p\u03c3 2 i \u00b4\u03c32 i\u00b41 qs \u03b8 px i , iq,</formula><p>where s \u03b8 px i , iq is to estimate z{\u03c3 i . As in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we let \u03c4 i \"</p><formula xml:id=\"formula_58\"> DDITIONAL DETAILS ON PREDICTOR-CORRECTOR SAMPLERS</head><p>Training We use the same architecture in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for our score-based models. For the VE SDE, we  noise scales at test time. The specific architecture of the noise-conditional score-based model in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> uses sinusoidal positional embeddings for cond amples with TF-GAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We follow <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for optimization, including the learning rate, mula\" target=\"#formula_0\">1</ref>) and use a batch size of 128. Our architecture is mostly based on <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. We additionally search over the following com pe=\"bibr\" target=\"#b20\">Karras et al. (2018)</ref>; <ref type=\"bibr\">Song &amp; Ermon (2019)</ref>; <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, the FID value here is the lowest over the cou odel upon conditioning on continuous time variables, we change positional embeddings, the layers in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for conditioning on discrete time steps, to ra al., 2018)</ref> 3.40 -Flow++ <ref type=\"bibr\" target=\"#b16\">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.70 13.51 DDPM (Lsimple) <ref type=\"bibr\"  > 3.29 -DDPM (L) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.70 13.51 DDPM (Lsimple) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.75 3.17   <ref type=\"bibr\">(Song &amp; Er )</ref> 25.32 8.87 \u02d8.12 NCSNv2 <ref type=\"bibr\">(Song &amp; Ermon, 2020)</ref> 10.87 8.40 \u02d8.07 DDPM <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> 3.17 9.46 \u02d8.11 Exact likelihood computation L s on CIFAR-10 is 10.23 <ref type=\"bibr\">(Song &amp; Ermon, 2020)</ref>, whereas for DDPM it is 3.17 <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref>. With PC samplers and the same model architec \"#formula_52\">41</ref>)) reverse diffusion samplers.</p><p>Note that the ancestral sampling of DDPM <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> (Eq. ( <ref type=\"formula\" target=\"#formula_4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#fig_4\">4</ref>). RealNVP <ref type=\"bibr\" target=\"#b9\">(Dinh et al., 2016)</ref> 3.49 -iResNet <ref type=\"bibr\" target=\"#b2\">(Behrmann et al., 2019)</ref> 3.45 -Glow <ref type=\"bibr\" target=\"#b24. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  generative models, and related techniques <ref type=\"bibr\" target=\"#b3\">(Bordes et al., 2017;</ref><ref type=\"bibr\" target=\"#b13\">Goyal et al., 2017)</ref>, have proven effective at generation of ima. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC <ref type=\"bibr\" target=\"#b34\">(Parisi, 1981;</ref><ref type=\"bibr\" target=\"#b15\">Grenander &amp; Mi del s \u03b8 \u02dapx, tq \u00ab \u2207 x log p t pxq, we can employ score-based MCMC approaches, such as Langevin MCMC <ref type=\"bibr\" target=\"#b34\">(Parisi, 1981;</ref><ref type=\"bibr\" target=\"#b15\">Grenander &amp; Mi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 20b)</ref>.</p><p>\u2022 Replacing the original residual blocks in DDPM with residual blocks from BigGAN <ref type=\"bibr\" target=\"#b4\">(Brock et al., 2018)</ref>. \u2022 Increasing the number of residual blocks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE <ref type=\"bibr\" target=\"#b1\">(Anderson, 1982)</ref>, which can be derived from the forward SDE give  : R d \u00d1 R d\u02c6d . We follow the It\u00f4 interpretation of SDEs throughout this paper.</p><p>According to <ref type=\"bibr\" target=\"#b1\">(Anderson, 1982)</ref>, the reverse-time SDE is given by (cf ., Eq. (   xpT q \" p T and reversing the process, we can obtain samples xp0q \" p 0 . A remarkable result from <ref type=\"bibr\" target=\"#b1\">Anderson (1982)</ref> states that the reverse of a diffusion process i  is p 0 pxp0q | yq. The density at time t is p t pxptq | yq when conditioned on y. Therefore, using <ref type=\"bibr\" target=\"#b1\">Anderson (1982)</ref>, the reverse-time SDE is given by dx \" tf px, tq. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: training a score-based model on samples with score matching <ref type=\"bibr\">(Hyv\u00e4rinen, 2005;</ref><ref type=\"bibr\" target=\"#b42\">Song et al., 2019a</ref> </p></div> <div xmlns=\"http://www.tei-c.org/ ) uses denoising score matching, but other score matching objectives, such as sliced score matching <ref type=\"bibr\" target=\"#b42\">(Song et al., 2019a)</ref> and finite-difference score matching <ref   not require computing \u2207 xptq log p 0t pxptq | xp0qq. For example, when using sliced score matching <ref type=\"bibr\" target=\"#b42\">(Song et al., 2019a)</ref>, our training objective Eq. ( <ref type=\"f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oaches improve results and enable more efficient sampling, they remain slower at sampling than GANs <ref type=\"bibr\" target=\"#b12\">(Goodfellow et al., 2014)</ref> on the same datasets. Identifying way. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t with the Skilling-Hutchinson trace estimator <ref type=\"bibr\" target=\"#b38\">(Skilling, 1989;</ref><ref type=\"bibr\" target=\"#b18\">Hutchinson, 1990)</ref>.</p><p>In particular, we have</p><formula xml. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) <ref type=\"bibr\" target=\"#b39\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Ho . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b6\">(Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b26\">Kong et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b31\">(Niu et al., 2020)</ref>, and shapes <ref type=\"bibr\" target=\"#b5\">(C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b6\">(Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b26\">Kong et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b31\">(Niu et al., 2020)</ref>, and shapes <ref type=\"bibr\" target=\"#b5\">(C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> 3.28 46.37 FFJORD <ref type=\"bibr\" target=\"#b14\">(Grathwohl et al., 2018)</ref> 3.40 -Flow++ <ref type=\"bibr\" target=\"#b16\">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=\"bibr\" target=\"#b17\" are to models evaluated in the same way (excluding models evaluated with variational dequantization <ref type=\"bibr\" target=\"#b16\">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t with the Skilling-Hutchinson trace estimator <ref type=\"bibr\" target=\"#b38\">(Skilling, 1989;</ref><ref type=\"bibr\" target=\"#b18\">Hutchinson, 1990)</ref>.</p><p>In particular, we have</p><formula xml. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t with the Skilling-Hutchinson trace estimator <ref type=\"bibr\" target=\"#b38\">(Skilling, 1989;</ref><ref type=\"bibr\" target=\"#b18\">Hutchinson, 1990)</ref>.</p><p>In particular, we have</p><formula xml. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b8\">(Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Fey et al., 2020)</ref>. Graph neural networks have been proven to be  ibr\" target=\"#b0\">Abu-El-Haija et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Loukas, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Fey et al., 2020)</ref>. In the context of molecular property predicti olution on both the input graph and a coarser version of it from which all cycles have been removed <ref type=\"bibr\" target=\"#b6\">(Fey et al., 2020)</ref>. Despite interesting results, this approach g >Fey et al., 2020)</ref>. In the context of molecular property prediction, we highlight the work of <ref type=\"bibr\" target=\"#b6\">Fey et al. (2020)</ref> who proposed to perform message passing betwee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: systems, traffic predictions, and much more <ref type=\"bibr\" target=\"#b25\">(Zhou et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Wu et al., 2020)</ref>. Among those problems, supervised whole graph . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is to extend the current framework, to make it equivalent to higher order Weisfeiler Leman kernels <ref type=\"bibr\" target=\"#b19\">(Morris et al., 2019)</ref>, or to increase the receptive field of ea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  instead of handcrafting them or using other representation is an idea that emerged a few years ago <ref type=\"bibr\" target=\"#b5\">(Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b8\">Gilmer et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r graph containing no cycles. The authors rely on fixed rules similar to the junction tree approach <ref type=\"bibr\" target=\"#b11\">(Jin et al., 2018)</ref> to annotate graph cycles and represent them . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  instead of handcrafting them or using other representation is an idea that emerged a few years ago <ref type=\"bibr\" target=\"#b5\">(Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b8\">Gilmer et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the context of chemistry. We propose a very simple correction to the now standard GIN convolution <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref> that enables the network to detect small cycl <p>A second problem is that the most common framework, namely message passing neural network (MPNN) <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref>, is not an universal approximator of function  that the operation is injective on multisets, or at least on a large finite ensemble of multisets. <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref> have proposed graph isomorphism network(GIN), ations encoding structure and node features, some drawbacks have been pointed by followup works. In <ref type=\"bibr\" target=\"#b24\">Xu et al. (2019)</ref> it is shown that GCNs are limited in its capab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the context of chemistry. We propose a very simple correction to the now standard GIN convolution <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref> that enables the network to detect small cycl <p>A second problem is that the most common framework, namely message passing neural network (MPNN) <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref>, is not an universal approximator of function  that the operation is injective on multisets, or at least on a large finite ensemble of multisets. <ref type=\"bibr\" target=\"#b24\">(Xu et al., 2019)</ref> have proposed graph isomorphism network(GIN), ations encoding structure and node features, some drawbacks have been pointed by followup works. In <ref type=\"bibr\" target=\"#b24\">Xu et al. (2019)</ref> it is shown that GCNs are limited in its capab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oding. They have been utilized to perform multi-purpose text generation with a single unified model <ref type=\"bibr\" target=\"#b36\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al bility present in large pretrained models <ref type=\"bibr\" target=\"#b28\">(McCann et al., 2018;</ref><ref type=\"bibr\" target=\"#b36\">Radford et al., 2019;</ref><ref type=\"bibr\">Keskar et al., 2019;</ref ng with the vanilla BART model, we also include the zero-shot performance from GPT2 language models <ref type=\"bibr\" target=\"#b36\">(Radford et al., 2019)</ref> (without fine-tuning) as a reference poi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b16\">(Hu et al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Fu et al., 2018;</ref><ref type=\"bibr\" target=\"#b13\">He et al., 2020b)</ref>, topics <ref type=\"bibr\" target=\"#b44\">(Tang . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on provides a way for users to continuously control the information that is included in the summary <ref type=\"bibr\" target=\"#b0\">(Bornstein et al., 1999;</ref><ref type=\"bibr\" target=\"#b23\">Leuski et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -tuned as described in \u00a72.2. Our summarization model implementation is based on the fairseq toolkit <ref type=\"bibr\" target=\"#b33\">(Ott et al., 2019)</ref> and the automatic keyword extraction model i BART LARGE model in all our experiments. Specifically we use the bart.large checkpoint from fairseq <ref type=\"bibr\" target=\"#b33\">(Ott et al., 2019)</ref>. For all BART-based summarization models, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acts important portions of a document <ref type=\"bibr\" target=\"#b2\">(Cheng &amp; Lapata, 2016;</ref><ref type=\"bibr\" target=\"#b31\">Nallapati et al., 2017;</ref><ref type=\"bibr\" target=\"#b32\">Narayan e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -tuned as described in \u00a72.2. Our summarization model implementation is based on the fairseq toolkit <ref type=\"bibr\" target=\"#b33\">(Ott et al., 2019)</ref> and the automatic keyword extraction model i BART LARGE model in all our experiments. Specifically we use the bart.large checkpoint from fairseq <ref type=\"bibr\" target=\"#b33\">(Ott et al., 2019)</ref>. For all BART-based summarization models, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n a zero-shot setting. Specifically, we evaluate the CNNDM summarization models on in-domain NewsQA <ref type=\"bibr\" target=\"#b45\">(Trischler et al., 2017)</ref> and out-of-domain SQuAD 1.1 <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>. To remedy such a prob e roughly divided into two branches: random walk-based and reconstruction-based methods. The former <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target ures with the assumption that nodes within the sliding window or neighborhoods are closely relevant <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" targ  <ref type=\"bibr\" target=\"#b44\">[44]</ref>, PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref>, BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref> and FOBE <ref type=\"bibr\" target=\"#b32\">[32]</ref> are speci iv xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1.1\">Data Preprocessing.</head><p>As used in BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref>, we select 60% edges for training and remaining edges for te [36]</ref>. \u2022 Bipartite graph embedding: PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref> and BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref>.</p><p>PinSage integrates random walk into GNN architectures. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: plit graphs. So DMGI still puts more emphasis on learning the correlation of homogeneous nodes. GMI <ref type=\"bibr\" target=\"#b26\">[26]</ref> proposes a new approach  The yellow dotted lines (Eq.( <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s within a sliding window <ref type=\"bibr\" target=\"#b45\">[45]</ref>. The reconstruction-based works <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" ta \"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> and collaborative filtering <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b37\">37]</ref> are also connected ed on local subgraphs for the task of inductive matrix completion. \u2022 Collaborative filtering: NeuMF <ref type=\"bibr\" target=\"#b15\">[15]</ref> and NGCF <ref type=\"bibr\" target=\"#b37\">[37]</ref>. NeuMF . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  LINE (2nd) is exploited here due to its expressive performances. Based on variational auto-encoder <ref type=\"bibr\" target=\"#b18\">[18]</ref>, VGAE adopts the graph convolutional network (GCN) <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t provides a new approach for the task of unsupervised node classification. Based on DIM, InfoGraph <ref type=\"bibr\" target=\"#b31\">[31]</ref> tries to learn unsupervised graph representations via maxi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  LINE (2nd) is exploited here due to its expressive performances. Based on variational auto-encoder <ref type=\"bibr\" target=\"#b18\">[18]</ref>, VGAE adopts the graph convolutional network (GCN) <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  LINE (2nd) is exploited here due to its expressive performances. Based on variational auto-encoder <ref type=\"bibr\" target=\"#b18\">[18]</ref>, VGAE adopts the graph convolutional network (GCN) <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: The negative sampling used in Eq.( <ref type=\"formula\" target=\"#formula_17\">14</ref>) is similar to <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b21\">21]</ref>: \ud835\udc38 \u2032 (\ud835\udc62,\ud835\udc63) is comp n rate \ud835\udefd in Eq.( <ref type=\"formula\" target=\"#formula_10\">9</ref>) and the harmonic factor \ud835\udf06 in Eq. <ref type=\"bibr\" target=\"#b12\">(12)</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_5\">4</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >40,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> are closely related with collaborative filtering <ref type=\"bibr\" target=\"#b28\">[28]</ref>. They attempt to reconstruct the adjacency matrix by learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ually used for modeling bipartite graphs. The pioneering homogeneous graph methods include DeepWalk <ref type=\"bibr\" target=\"#b27\">[27]</ref>, LINE <ref type=\"bibr\" target=\"#b33\">[33]</ref>, Node2vec  following strong baselines which can be divided into:</p><p>\u2022 Homogeneous graph embedding: DeepWalk <ref type=\"bibr\" target=\"#b27\">[27]</ref>, LINE <ref type=\"bibr\" target=\"#b33\">[33]</ref>, Node2vec . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aph embedding paradigm <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b5\">5,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. Although they work pretty well in the settings of homogene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ations, arranging from recommender system <ref type=\"bibr\" target=\"#b34\">[34]</ref>, drug discovery <ref type=\"bibr\" target=\"#b39\">[39]</ref> to information retrieval <ref type=\"bibr\" target=\"#b43\">[4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b42\">42]</ref> train graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b22\">22,</ref><ref type=\"bibr\" tar volutional operators <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b13\">13,</ref><ref type=\"bibr\" target=\"#b20\">20]</ref>, we only aggregate neighborhood features, and the own featu ncoder <ref type=\"bibr\" target=\"#b18\">[18]</ref>, VGAE adopts the graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b20\">[20]</ref> as the basic encoder to learn graph-structured data.  Mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[35]</ref>. However, estimating MI is generally intractable in highdimensional continuous settings <ref type=\"bibr\" target=\"#b25\">[25]</ref>. MINE <ref type=\"bibr\" target=\"#b1\">[1]</ref> derives a lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed methods. The former <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target=\"#b44\">44]</ref> relies on designing the heuristics of random walks to gener graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE <ref type=\"bibr\" target=\"#b44\">[44]</ref>, PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref>, BiNE <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b22\">22,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> to learn node representations via aggregating features of n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>) and Eq.( <ref type=\"formula\" target=\"#formula_3\">4</ref>) are also weight matrices. Dropout <ref type=\"bibr\" target=\"#b30\">[30]</ref> is applied to each layer of our encoder to regularize mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: The negative sampling used in Eq.( <ref type=\"formula\" target=\"#formula_17\">14</ref>) is similar to <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b21\">21]</ref>: \ud835\udc38 \u2032 (\ud835\udc62,\ud835\udc63) is comp n rate \ud835\udefd in Eq.( <ref type=\"formula\" target=\"#formula_10\">9</ref>) and the harmonic factor \ud835\udf06 in Eq. <ref type=\"bibr\" target=\"#b12\">(12)</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_5\">4</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: types. It has been widely adopted in many realworld applications, arranging from recommender system <ref type=\"bibr\" target=\"#b34\">[34]</ref>, drug discovery <ref type=\"bibr\" target=\"#b39\">[39]</ref>  partite graphs closely. They propose various DNNs to solve recommendation tasks. For example, GC-MC <ref type=\"bibr\" target=\"#b34\">[34]</ref> uses one relation-aware graph convolution layer to learn n ointly optimizes explicit and implicit relations in a unified framework. \u2022 Matrix completion: GC-MC <ref type=\"bibr\" target=\"#b34\">[34]</ref> and IGMC <ref type=\"bibr\" target=\"#b42\">[42]</ref>. GC-MC  uction-based works <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b34\">34,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" tar tempt to reconstruct the adjacency matrix by learning different encoders. In particular, some works <ref type=\"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" ta ey mainly focus on how to model local graph structures in the latent space.</p><p>Matrix completion <ref type=\"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> and collaborative . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b10\">[10]</ref>, and the \"GAN\" distance and Jensen-Shannon divergence are closely related <ref type=\"bibr\" target=\"#b24\">[24]</ref>.</p><p>From Eq.( <ref type=\"formula\" target=\"#formula_4\">5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ations, arranging from recommender system <ref type=\"bibr\" target=\"#b34\">[34]</ref>, drug discovery <ref type=\"bibr\" target=\"#b39\">[39]</ref> to information retrieval <ref type=\"bibr\" target=\"#b43\">[4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: so commonly used to evaluate the task of community detection <ref type=\"bibr\" target=\"#b4\">[4,</ref><ref type=\"bibr\" target=\"#b23\">23]</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_6\">5</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r/item embeddings by mimicking the meta-learning setting via episode based training, as proposed in <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Specifically, we pick the users/items with sufficient int rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, which consists of metric-based recommendation <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"><head n=\"1\">INTRODUCTION</head><p>Recommendation systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> have been extensively deployed to alleviate information ove is the most widely adopted principle. The most common paradigm for CF, such as matrix factorization <ref type=\"bibr\" target=\"#b20\">[21]</ref> and neural collaborative filtering <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce between the predicted target embedding h \ud835\udc3f \ud835\udc62 and the ground-truth embedding h \ud835\udc62 , as proposed by <ref type=\"bibr\" target=\"#b15\">[16]</ref>, due to its popularity as an indicator for the semantic si models and the GNN models are initialized by the NCF embedding results. We use Spearman correlation <ref type=\"bibr\" target=\"#b15\">[16]</ref> to measure the agreement between the ground truth embeddin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by the recent development of graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, NGCF <ref type=\"bi stics of neighbors during the graph convolution process. Although some GNN models such as GrageSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> or FastGCN <ref type=\"bibr\" target=\"#b3\">[4]</ref> filter  esults in at most \ud835\udc3e \ud835\udc59 (1 \u2264 \ud835\udc59 \u2264 \ud835\udc3f) \ud835\udc59-order neighbors for each target user/item. Similar to GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>, we sample high-order neighbors to improve the computation i-layer Perceptron and matrix factorization to learn the embeddings of users and items. \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>: is a general GNN model which samples neighbors randomly a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> or external knowledge graphs (KGs) <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to compensate the  ef type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> and knowledge graphs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to enhance the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"><head n=\"1\">INTRODUCTION</head><p>Recommendation systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> have been extensively deployed to alleviate information ove is the most widely adopted principle. The most common paradigm for CF, such as matrix factorization <ref type=\"bibr\" target=\"#b20\">[21]</ref> and neural collaborative filtering <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, i.e. the preferences for users and items and then perform the prediction based on the embeddings <ref type=\"bibr\" target=\"#b12\">[13]</ref>. However, these models fail to learn high-quality embeddin ef type=\"bibr\" target=\"#b18\">19]</ref>, NGCF <ref type=\"bibr\" target=\"#b37\">[38]</ref> and LightGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> encode the high-order collaborative signal in the user-ite ction based on the explicit user-item and the implicit user-user/item-item interactions. \u2022 LightGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref>: is a special GNN model for recommendation, which discards <ref type=\"bibr\" target=\"#b44\">[45]</ref>, NGCF <ref type=\"bibr\" target=\"#b37\">[38]</ref>, LightGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref>, FBNE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and CAGR <re opting \ud835\udc56 1 is estimated, and cross-entropy loss <ref type=\"bibr\" target=\"#b2\">[3]</ref> or BPR loss <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> is usually adopted the true observations. Despite the success of capturing the high-order collaborative signal in GNNs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, the cold-start pr ulate the relevance score \ud835\udc66 (\ud835\udc62, \ud835\udc56) = h \ud835\udc3f \ud835\udc62 T h \ud835\udc3f \ud835\udc56 between user \ud835\udc62 and item \ud835\udc56 and adopt the BPR loss <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, i.e.,</p><formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -item interactions. One kind of the methods is meta-learning <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> or external knowledge graphs (KGs) <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to compensate the  ef type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> and knowledge graphs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to enhance the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: embeddings of \ud835\udc62 1 and \ud835\udc56 1 , the likelihood of \ud835\udc62 1 adopting \ud835\udc56 1 is estimated, and cross-entropy loss <ref type=\"bibr\" target=\"#b2\">[3]</ref> or BPR loss <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref ty tes neighbors by the same aggregator as GCN <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>\u2022 FBNE <ref type=\"bibr\" target=\"#b2\">[3]</ref>: is a special GNN model for recommendation, which samples th <ref type=\"bibr\" target=\"#b37\">[38]</ref>, LightGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref>, FBNE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and CAGR <ref type=\"bibr\" target=\"#b41\">[42]</ref>. Generall. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: embeddings of \ud835\udc62 1 and \ud835\udc56 1 , the likelihood of \ud835\udc62 1 adopting \ud835\udc56 1 is estimated, and cross-entropy loss <ref type=\"bibr\" target=\"#b2\">[3]</ref> or BPR loss <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref ty tes neighbors by the same aggregator as GCN <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>\u2022 FBNE <ref type=\"bibr\" target=\"#b2\">[3]</ref>: is a special GNN model for recommendation, which samples th <ref type=\"bibr\" target=\"#b37\">[38]</ref>, LightGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref>, FBNE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and CAGR <ref type=\"bibr\" target=\"#b41\">[42]</ref>. Generall. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d architecture efficient, as well as maintain higher prediction capacity?</p><p>Vanilla Transformer <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017</ref>) has three significant limitations when so tei-c.org/ns/1.0\"><head>Efficient Self-attention Mechanism</head><p>The canonical self-attention in <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017</ref>) is defined on receiving the tuple input ( ing long sequential outputs through one forward procedure</p><p>We use a standard decoder structure <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017)</ref> in Fig.</p><p>(2), and it is composed of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: m, let q i , k i , v i stand for the i-th row in Q, K, V respectively. Following the formulation in <ref type=\"bibr\" target=\"#b28\">(Tsai et al. 2019)</ref>, the i-th query's attention is defined as a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniqu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inance <ref type=\"bibr\" target=\"#b33\">(Zhu and Shasha 2002)</ref>, and disease propagation analysis <ref type=\"bibr\" target=\"#b18\">(Matsubara et al. 2014)</ref>. In these scenarios, we can leverage a  \">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, disease propagation analysis <ref type=\"bibr\" target=\"#b18\">(Matsubara et al. 2014)</ref>, economics and finance forecasting (Zhu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: reliable workhorse for time-series forecasting <ref type=\"bibr\" target=\"#b3\">(Box et al. 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -series forecasting is a critical ingredient across many domains, such as sensor network monitoring <ref type=\"bibr\" target=\"#b19\">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management,   forecasting (LSTF) problem. Some significant real-world applications are sensor network monitoring <ref type=\"bibr\" target=\"#b19\">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniqu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniqu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b19\">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, economics and finance <ref type=\"bibr\" target=\"#b33\">(Zhu and Shasha 2002)</ref>, and disease propagation analysis <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2018)</ref>, LSTMa <ref type=\"bibr\" target=\"#b1\">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type=\"bibr\" target=\"#b14\">(Lai et al. 2018</ref>) and DeepAR <ref type=\"bibr\" target=\"#b10\">(Fl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: reliable workhorse for time-series forecasting <ref type=\"bibr\" target=\"#b3\">(Box et al. 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inance <ref type=\"bibr\" target=\"#b33\">(Zhu and Shasha 2002)</ref>, and disease propagation analysis <ref type=\"bibr\" target=\"#b18\">(Matsubara et al. 2014)</ref>. In these scenarios, we can leverage a  \">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, disease propagation analysis <ref type=\"bibr\" target=\"#b18\">(Matsubara et al. 2014)</ref>, economics and finance forecasting (Zhu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniqu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on blocks in Fig. <ref type=\"figure\" target=\"#fig_3\">(3</ref>). Inspired by the dilated convolution <ref type=\"bibr\" target=\"#b32\">(Yu, Koltun, and Funkhouser 2017;</ref><ref type=\"bibr\" target=\"#b11\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: reliable workhorse for time-series forecasting <ref type=\"bibr\" target=\"#b3\">(Box et al. 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lculation.</p><p>Input: PSL Rules R, Prediction \u0177i , and Probability P(y|s i ), i = {1, 2, 3}; BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref>, to derive the sentence representation v i etails</head><p>In the framework of CTRL-PG, any contextualized word embedding method, such as BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref>, ELMo <ref type=\"bibr\" target=\"#b41\">(Pete nd RoBERTa <ref type=\"bibr\" target=\"#b37\">(Liu et al. 2019b)</ref>, can be utilized. We choose BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to derive contextualized sentence embeddin ch tokenized sequence and learns an embedding vector for it. We follow the experimental settings in <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to use 12 Transformer layers and attention #b29\">(Kingma and Ba 2014)</ref> to optimize the parameters. We follow the experimental settings in <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to set the dropout rate, and batch size as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b47\">Tang et al. 2013;</ref><ref type=\"bibr\" target=\"#b31\">Lee et al. 2016;</ref><ref type=\"bibr\" target=\"#b14\">Chikka 2016</ref>) such as SVMs, Max-Ent and CRFs, and neural network. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Navarro 2010; Sun, Rumshisky, and Uzuner 2013; <ref type=\"bibr\" target=\"#b49\">Xu et al. 2013;</ref><ref type=\"bibr\" target=\"#b47\">Tang et al. 2013;</ref><ref type=\"bibr\" target=\"#b31\">Lee et al. 2016. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2-2012 <ref type=\"bibr\" target=\"#b46\">(Sun, Rumshisky, and Uzuner 2013)</ref> and Clinical TempEval <ref type=\"bibr\" target=\"#b3\">(Bethard et al. 2015</ref><ref type=\"bibr\" target=\"#b4\">(Bethard et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al observations and reasoning <ref type=\"bibr\" target=\"#b46\">(Sun, Rumshisky, and Uzuner 2013;</ref><ref type=\"bibr\" target=\"#b11\">Chen, Podchiyska, and Altman 2016)</ref>. Extracting temporal relatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b33\">(Lin et al. , 2018;;</ref><ref type=\"bibr\">Dligach et al. 2017;</ref><ref type=\"bibr\" target=\"#b48\">Tourille et al. 2017;</ref><ref type=\"bibr\" target=\"#b34\">Lin et al. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  was before f. Some recent studies <ref type=\"bibr\" target=\"#b32\">(Leeuwenberg and Moens 2017;</ref><ref type=\"bibr\" target=\"#b40\">Ning, Feng, and Roth 2017;</ref><ref type=\"bibr\" target=\"#b27\">Han, Z ref type=\"bibr\">Han et al. 2019;</ref><ref type=\"bibr\" target=\"#b25\">Han, Ning, and Peng 2019;</ref><ref type=\"bibr\" target=\"#b40\">Ning, Feng, and Roth 2017)</ref> formulate the problem as a structure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: red knowledge, i.e. extract important clinical named entities and relationships from the narratives <ref type=\"bibr\" target=\"#b1\">(Aronson and Lang 2010;</ref><ref type=\"bibr\" target=\"#b44\">Savova et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b14\">Chikka 2016</ref>) such as SVMs, Max-Ent and CRFs, and neural network based methods <ref type=\"bibr\" target=\"#b34\">(Lin et al. 2017</ref><ref type=\"bibr\" target=\"#b33\">(Lin et al. , 20 ref type=\"bibr\">Dligach et al. 2017;</ref><ref type=\"bibr\" target=\"#b48\">Tourille et al. 2017;</ref><ref type=\"bibr\" target=\"#b34\">Lin et al. 2019;</ref><ref type=\"bibr\" target=\"#b23\">Guan et al. 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ue to the high demand for domain knowledge and high complexity of clinical language representations <ref type=\"bibr\" target=\"#b19\">(Galvan et al. 2018)</ref>. <ref type=\"bibr\" target=\"#b38\">Meng and R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b16\">(Deng and Wiebe 2015;</ref><ref type=\"bibr\" target=\"#b12\">Chen et al. 2019;</ref><ref type=\"bibr\" target=\"#b28\">Hu et al. 2016</ref>) have explored Probabilistic Soft Logic (PSL) <r ess <ref type=\"bibr\" target=\"#b18\">(Farnadi, Babaki, and Getoor 2019)</ref>, Model Interpretability <ref type=\"bibr\" target=\"#b28\">(Hu et al. 2016)</ref>, Probabilistic Reasoning (Augustine, Rekatsina. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. <ref type=\"bibr\" target=\"#b35\">(Yun et al. 2019;</ref><ref type=\"bibr\" target=\"#b33\">Xu, Joshi, and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ous success in the field of natural language processing (NLP) since the development of Transformers <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> which are currently the best performing n nal information. Since Laplacian PEs are generalization of the PE used in the original transformers <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> to graphs and these better help encode di r</head><p>The Graph Transformer is closely the same transformer architecture initially proposed in <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref>, see Figure <ref type=\"figure\">1</ref> (L target=\"#tab_2\">2</ref>), which employs multi-headed attention inspired by the original transformer <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> and have been often used in the literatur e each word attending to each other word in a sentence, as followed by the Transformer architecture <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017</ref>). -b) Next, the so-called graph considered. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s, same as in GNNs <ref type=\"bibr\" target=\"#b7\">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" target=\"#b13\">Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b17\">Monti et a ERT uses  <ref type=\"table\" target=\"#tab_1\">1</ref>) on each dataset against the GNN baselines (GCN <ref type=\"bibr\" target=\"#b13\">(Kipf and Welling 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b28\">(Ve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \u0125 +1 i</formula><p>, \u0125 +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=\"bibr\" target=\"#b1\">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2018;</ref><ref type=\"bibr\" target=\"#b23\">Radford et al or node Laplacian position eigenvectors <ref type=\"bibr\" target=\"#b2\">(Belkin and Niyogi 2003;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020)</ref>, or relative learnable positional informati You, Ying, and Leskovec 2019;</ref><ref type=\"bibr\" target=\"#b26\">Srinivasan and Ribeiro 2020;</ref><ref type=\"bibr\" target=\"#b8\">Dwivedi et al. 2020;</ref><ref type=\"bibr\" target=\"#b14\">Li et al. 202 arget=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref ty  'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref> for additional information, inlcuding prep e the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type=\"bibr\" target=\"#b8\">Dwivedi et al. (2020)</ref>.</p><p>PATTERN, Node Classification PATTER. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b13\">Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b17\">Monti et al. 2017;</ref><ref type=\"bibr\" target=\"#b9\">Gilmer et al. 2017;</ref><ref type=\"bibr\" target=\"#b28\">Veli\u010dkovi\u0107 et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>We evaluate the performance of proposed Graph Transformer on three benchmark graph datasets-ZINC <ref type=\"bibr\" target=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a rec chmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref type=\"bibr\" target=\"#b11\">(Irwin et al. 2012</ref>) is a molecular dataset with the task of gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and have achieved significant success on a wide range of applications, such as in knowledge graphs <ref type=\"bibr\" target=\"#b25\">(Schlichtkrull et al. 2018;</ref><ref type=\"bibr\" target=\"#b5\">Chami . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, same as in GNNs <ref type=\"bibr\" target=\"#b7\">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" target=\"#b13\">Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b17\">Monti et a ERT uses  <ref type=\"table\" target=\"#tab_1\">1</ref>) on each dataset against the GNN baselines (GCN <ref type=\"bibr\" target=\"#b13\">(Kipf and Welling 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b28\">(Ve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trained on graph datasets learn structural node information that are invariant to the node position <ref type=\"bibr\" target=\"#b26\">(Srinivasan and Ribeiro 2020)</ref>. This is a critical reason why si =\"#b19\">(Murphy et al. 2019;</ref><ref type=\"bibr\" target=\"#b34\">You, Ying, and Leskovec 2019;</ref><ref type=\"bibr\" target=\"#b26\">Srinivasan and Ribeiro 2020;</ref><ref type=\"bibr\" target=\"#b8\">Dwive. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: practical to have a Graph Transformer where a node attends to local node neighbors, same as in GNNs <ref type=\"bibr\" target=\"#b7\">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b33\">Xu, Joshi, and Bresson 2019;</ref><ref type=\"bibr\">Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b37\">Zhou et al. 2020</ref>). The model proposed in <ref type=\"bibr\" targe  based on the timestamp differences of the central node and the message-passing nodes. Furthermore, <ref type=\"bibr\" target=\"#b37\">Zhou et al. (2020)</ref> proposed a transformer based generative mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b13\">Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b17\">Monti et al. 2017;</ref><ref type=\"bibr\" target=\"#b9\">Gilmer et al. 2017;</ref><ref type=\"bibr\" target=\"#b28\">Veli\u010dkovi\u0107 et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \u0125 +1 i</formula><p>, \u0125 +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=\"bibr\" target=\"#b1\">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \u0125 +1 i</formula><p>, \u0125 +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=\"bibr\" target=\"#b1\">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ositions is challenging as there are symmetries which prevent canonical node positional information <ref type=\"bibr\" target=\"#b19\">(Murphy et al. 2019)</ref>. In fact, most of the GNNs which are train ormance on graph datasets. The issue of positional embeddings has been explored in recent GNN works <ref type=\"bibr\" target=\"#b19\">(Murphy et al. 2019;</ref><ref type=\"bibr\" target=\"#b34\">You, Ying, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trained on graph datasets learn structural node information that are invariant to the node position <ref type=\"bibr\" target=\"#b26\">(Srinivasan and Ribeiro 2020)</ref>. This is a critical reason why si =\"#b19\">(Murphy et al. 2019;</ref><ref type=\"bibr\" target=\"#b34\">You, Ying, and Leskovec 2019;</ref><ref type=\"bibr\" target=\"#b26\">Srinivasan and Ribeiro 2020;</ref><ref type=\"bibr\" target=\"#b8\">Dwive. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>We evaluate the performance of proposed Graph Transformer on three benchmark graph datasets-ZINC <ref type=\"bibr\" target=\"#b11\">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a rec chmark <ref type=\"bibr\" target=\"#b8\">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref type=\"bibr\" target=\"#b11\">(Irwin et al. 2012</ref>) is a molecular dataset with the task of gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  original graph computed using WL algorithm <ref type=\"bibr\" target=\"#b36\">(Zhang et al. 2020;</ref><ref type=\"bibr\" target=\"#b21\">Niepert, Ahmed, and Kutzkov 2016)</ref>, are not variant to the subgr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: practical to have a Graph Transformer where a node attends to local node neighbors, same as in GNNs <ref type=\"bibr\" target=\"#b7\">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trained on graph datasets learn structural node information that are invariant to the node position <ref type=\"bibr\" target=\"#b26\">(Srinivasan and Ribeiro 2020)</ref>. This is a critical reason why si =\"#b19\">(Murphy et al. 2019;</ref><ref type=\"bibr\" target=\"#b34\">You, Ying, and Leskovec 2019;</ref><ref type=\"bibr\" target=\"#b26\">Srinivasan and Ribeiro 2020;</ref><ref type=\"bibr\" target=\"#b8\">Dwive. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b46\">Thomas et al., 2018;</ref><ref type=\"bibr\">Kondor et al., 2018;</ref><ref type=\"bibr\" target=\"#b54\">Weiler et al., 2018b;</ref><ref type=\"bibr\">a;</ref><ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b36\">(Oliphant, 2006;</ref><ref type=\"bibr\" target=\"#b49\">Walt et al., 2011;</ref><ref type=\"bibr\" target=\"#b18\">Harris et al., 2020)</ref>, SciPy <ref type=\"bibr\" target=\"#b24\">(Jon. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Hence MSA can be generalised to other choices of kernels and normalisation that are equally valid <ref type=\"bibr\" target=\"#b51\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Tsai et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to thank the Python community <ref type=\"bibr\" target=\"#b47\">(Van Rossum &amp; Drake Jr, 1995;</ref><ref type=\"bibr\" target=\"#b37\">Oliphant, 2007)</ref> for developing the tools that enabled this work. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Hence MSA can be generalised to other choices of kernels and normalisation that are equally valid <ref type=\"bibr\" target=\"#b51\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Tsai et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ear group equivariant building blocks. In this paper we extend group equivariance to self-attention <ref type=\"bibr\" target=\"#b48\">(Vaswani et al., 2017)</ref>, a non-trivial non-linear map, with the  div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3.\">Self-attention</head><p>Self-attention <ref type=\"bibr\" target=\"#b48\">(Vaswani et al., 2017)</ref> is a mapping from an input set of N vect lding block of deep learning models in various data modalities, such as natural-language processing <ref type=\"bibr\" target=\"#b48\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Brown et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ls and normalisation that are equally valid <ref type=\"bibr\" target=\"#b51\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Tsai et al., 2019)</ref>.</p><formula xml:id=\"formula_10\">M SA(X) [f  t=\"#b6\">(Cohen &amp; Welling, 2016b;</ref><ref type=\"bibr\" target=\"#b55\">Worrall et al., 2017;</ref><ref type=\"bibr\" target=\"#b46\">Thomas et al., 2018;</ref><ref type=\"bibr\">Kondor et al., 2018;</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g loops of neural networks, blurring different dimensions of the problem. By contrast -similarly to <ref type=\"bibr\" target=\"#b35\">Parshakova et al. (2019a;</ref><ref type=\"bibr\">b)</ref> in a differe =\"bibr\" target=\"#b23\">(Kim &amp; Bengio, 2016;</ref><ref type=\"bibr\" target=\"#b34\">Owen, 2013;</ref><ref type=\"bibr\" target=\"#b35\">Parshakova et al., 2019a</ref>) SNIS consists in computing:</p><formu et=\"#b4\">Belanger &amp; McCallum, 2016)</ref>. Some current applications to text generation include <ref type=\"bibr\" target=\"#b35\">Parshakova et al. (2019a)</ref> and <ref type=\"bibr\" target=\"#b14\">De. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"formula\" target=\"#formula_0\">1</ref>) is a generalization of the Maximum Entropy Principle of <ref type=\"bibr\" target=\"#b21\">Jaynes (1957)</ref>, which corresponds to the limit case where a is t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b49\">Sheng et al. (2020)</ref> introduce a method relying on adversarial triggers <ref type=\"bibr\" target=\"#b54\">(Wallace et al., 2019)</ref>; this method does not de-bias the whole . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: certain a priori desirable features.</p><p>However, such an optimization process is not infallible; <ref type=\"bibr\" target=\"#b30\">Liu et al. (2016a)</ref> noted that it often leads to \"degeneration\", using a set of discriminators to ensure the quality of generated text in open-ended text generation.<ref type=\"bibr\" target=\"#b30\">Liu et al. (2016a)</ref>, however, show that defining a combination r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b8\">(Caccia et al., 2020)</ref>. So additionally, we report Self-BLEU-3,4,5 <ref type=\"bibr\" target=\"#b60\">(Zhu et al., 2018)</ref> to measure repetitions at a distributional l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on constraint computations. Baselines: We compare our method GDC to three baselines: (1) REINFORCE <ref type=\"bibr\" target=\"#b56\">(Williams, 1992b)</ref>, using the reward \u03c6(x), i.e. trying to maximi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: usually referred to as weighted decoding<ref type=\"bibr\" target=\"#b17\">Holtzman et al. (2018)</ref>;<ref type=\"bibr\" target=\"#b46\">See et al. (2019)</ref> this however still requires a heavy search pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores <ref type=\"bibr\" target=\"#b27\">(Li et al., 2016a)</ref> to measure repetitions within a single gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on constraint computations. Baselines: We compare our method GDC to three baselines: (1) REINFORCE <ref type=\"bibr\" target=\"#b56\">(Williams, 1992b)</ref>, using the reward \u03c6(x), i.e. trying to maximi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a few fine-tuning updates. The proposed learning to pre-train can be deemed a form of meta-learning <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017)</ref>, also known as learning to lear ermore, our strategy is a form of meta-learning, in particular, model agnostic meta-learning (MAML) <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017)</ref>. Meta-learning aims to learn pr hods directly adjust the optimization algorithm to enable quick adaptation with just a few examples <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017;</ref><ref type=\"bibr\" target=\"#b43\">Y. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b28\">(Navarin, Tran, and Sperduti 2018;</ref><ref type=\"bibr\" target=\"#b18\">Hu et al. 2019</ref><ref type=\"bibr\" target=\"#b17\">Hu et al. , 2020) )</ref> is to learn transferable prior knowledge fr t=\"#b18\">Hu et al. 2019)</ref>, or still require supervised information for graph-level pretraining <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref>). While at the node level, predicting links bet  is to learn a generic initialization for model parameters using readily available graph structures <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref><ref type=\"bibr\" target=\"#b18\">(Hu et al. , 2019 atasets. We conduct experiments on data from two domains: biological function prediction in biology <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref>) and research field prediction in bibliography. ers with three unsupervised tasks to capture different aspects of a graph. More recently, Hu et al. <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> propose different strategies to pre-train grap  subgraph is centered at a paper and contains the associated information of For biology data, as in <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref>, we use 306,925 unlabeled protein ego-networks  that correspond to 40 binary classification tasks. We split the downstream data with species split <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref>, and evaluate the test performance with averag  across the graph's patch representations; (3) Context Prediction strategy (denoted by ContextPred) <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> to explore graph structures and (4) Attribute  2020)</ref> to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> to learn the regularities of the node and edge which harms the generalization of the pre-trained GNNs. This finding confirms previous observations <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b31\">Rosenstein et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017)</ref>, recommendation systems <ref type=\"bibr\" target=\"#b11\">(Fan et al. 2019;</ref><ref type=\"bibr\" target=\"#b44\">Ying et al. 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata, which are often limited and expensive to obtain.</p><p>Inspired by pre-trained language models <ref type=\"bibr\" target=\"#b7\">(Devlin et al. 2019;</ref><ref type=\"bibr\">Mikolov et al. 2013</ref>) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  aggregation schemes have been proposed <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017;</ref><ref type=\"bibr\" target=\"#b39  target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\">Niepert, Ahmed, and Kutzkov 2016;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017;</ref><ref type=\"bibr\" target=\"#b39 , such as node and graph classification <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017)</ref>, recommendation systems <ref upervised objective of predicting the link between u and v <ref type=\"bibr\">(Tang et al. 2015;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, Ying, and Leskovec 2017)</ref>, as follows.</p><formula xml r self-supervised or unsupervised baselines: (1) the original Edge Prediction (denoted by EdgePred) <ref type=\"bibr\" target=\"#b14\">(Hamilton, Ying, and Leskovec 2017)</ref> to predict the connectivity  architectures, namely, GCN <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017)</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: entations on graphs, this concept is extended to convolution neural networks using spectral methods <ref type=\"bibr\" target=\"#b6\">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al. 2020)</ref>. Various GNN architectures with different aggregation schemes have been proposed <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, hese GNNs have achieved impressive performance in many tasks, such as node and graph classification <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton, et=\"#b42\">Xu et al. 2019a</ref>) and message passing architectures to aggregate neighbors' features <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017;</ref><ref type=\"bibr\">Niepert, Ahmed, and Kut ted for different GNN architectures. We experiment with four popular GNN architectures, namely, GCN <ref type=\"bibr\" target=\"#b19\">(Kipf and Welling 2017)</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \u2126(\u2022), and h G denotes the shifted graph representation by randomly shifting some dimensions of h G <ref type=\"bibr\" target=\"#b40\">(Velickovic et al. 2019)</ref>, serving as the negative sample.</p><p ovec 2017)</ref> to predict the connectivity of node pairs; (2) Deep Graph Infomax (denoted by DGI) <ref type=\"bibr\" target=\"#b40\">(Velickovic et al. 2019</ref>) to maximize local mutual information a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rk</head><p>GNNs have received significant attention due to the prevalence of graph-structured data <ref type=\"bibr\" target=\"#b4\">(Bronstein et al. 2017)</ref>. Originally proposed <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b45\">Ying et al. 2018b;</ref><ref type=\"bibr\" target=\"#b15\">Hasanzadeh et al. 2019;</ref><ref type=\"bibr\" target=\"#b30\">Qu, Bengio, and Tang 2019;</ref><ref type=\"bibr\" target=\"#b29\">Pei et \" target=\"#b41\">(Vilalta and Drissi 2002;</ref><ref type=\"bibr\" target=\"#b38\">Vanschoren 2018;</ref><ref type=\"bibr\" target=\"#b30\">Peng 2020</ref>). Among previous works on meta-learning, metric-based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odel-F), and consider three perspectives for comparison: Centered Kernel Alignment (CKA) similarity <ref type=\"bibr\" target=\"#b20\">(Kornblith et al. 2019)</ref> between the parameters of Model-P 0.6  . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: errard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" target=\"#b5\">Bruna et al. 2014;</ref><ref type=\"bibr\" target=\"#b22\">Levie et al. 2019;</ref><ref type=\"bibr\" target=\"#b42\">Xu et al. 2019. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b15\">Hasanzadeh et al. 2019;</ref><ref type=\"bibr\" target=\"#b30\">Qu, Bengio, and Tang 2019;</ref><ref type=\"bibr\" target=\"#b29\">Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b27\">Munkhdalai and Y. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b12\">(Finn, Abbeel, and Levine 2017;</ref><ref type=\"bibr\" target=\"#b43\">Yao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Lee et al. 2019;</ref><ref type=\"bibr\" target=\"#b24\">Lu, Fang, and Sh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b43\">Yao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Lee et al. 2019;</ref><ref type=\"bibr\" target=\"#b24\">Lu, Fang, and Shi 2020)</ref>.</p><p>3 Learning to Pre-train:</p></di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: res as well as node or edge features <ref type=\"bibr\" target=\"#b47\">(Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b42\">Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b10\">Dwivedi et al. 20 =\"bibr\" target=\"#b5\">Bruna et al. 2014;</ref><ref type=\"bibr\" target=\"#b22\">Levie et al. 2019;</ref><ref type=\"bibr\" target=\"#b42\">Xu et al. 2019a</ref>) and message passing architectures to aggregate al. 2019)</ref>. For a more comprehensive understanding of GNNs, we refer readers to the literature <ref type=\"bibr\" target=\"#b42\">(Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b2\">Battaglia et al. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o notice that some baselines give surprisingly limited performance gain and yield negative transfer <ref type=\"bibr\" target=\"#b31\">(Rosenstein et al. 2005</ref>) on the downstream task (i.e., EdgePred s. This finding confirms previous observations <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b31\">Rosenstein et al. 2005</ref>) that negative transfer results in limit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ghborhoods on the graph, naturally capturing both graph structures as well as node or edge features <ref type=\"bibr\" target=\"#b47\">(Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b42\">Wu et  bibr\" target=\"#b42\">(Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b2\">Battaglia et al. 2018;</ref><ref type=\"bibr\" target=\"#b47\">Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b48\">Zhou et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b15\">Hasanzadeh et al. 2019;</ref><ref type=\"bibr\" target=\"#b30\">Qu, Bengio, and Tang 2019;</ref><ref type=\"bibr\" target=\"#b29\">Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b27\">Munkhdalai and Y. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Hamilton, Ying, and Leskovec 2017;</ref><ref type=\"bibr\" target=\"#b39\">Velickovic et al. 2018;</ref><ref type=\"bibr\" target=\"#b0\">Abu-El-Haija et al. 2019)</ref>. For a more comprehensive understandin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b30\">Qu, Bengio, and Tang 2019;</ref><ref type=\"bibr\" target=\"#b29\">Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b27\">Munkhdalai and Yu 2017)</ref>. Empirically, these GNNs have achieved  ion over tasks, while model-based methods <ref type=\"bibr\" target=\"#b32\">(Santoro et al. 2016;</ref><ref type=\"bibr\" target=\"#b27\">Munkhdalai and Yu 2017)</ref> aim to design an architecture or traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )</ref>. Originally proposed <ref type=\"bibr\" target=\"#b26\">(Marco, Gabriele, and Franco 2005;</ref><ref type=\"bibr\" target=\"#b33\">Scarselli et al. 2008</ref>) as a framework of utilizing neural netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rnal layers in addition to final predictions <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Sun et al. 2020</ref><ref type=\"bibr\">Sun et al. , 2019))</ref>, but  ther models such as TinyBERT <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019)</ref> and MobileBERT <ref type=\"bibr\" target=\"#b21\">(Sun et al. 2020</ref>) also found it crucial for training competitiv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: been proposed to this end, which compare networks' internal layers in addition to final predictions <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Sun et al. 202 e teacher layers into m buckets with approximately the same sizes and pick only one layer from each <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\">Sun et al. 2019)</ref>. Ther PKD is not the only model that utilizes internal layers' information. Other models such as TinyBERT <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019)</ref> and MobileBERT <ref type=\"bibr\" target=\"#b21. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nguage understanding (NLU) models such as <ref type=\"bibr\">ELMO (Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models   (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> for this model. Therefore, our in-house ver -tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, so we skip that part and refer the reader  _0\">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nguage understanding (NLU) models such as <ref type=\"bibr\">ELMO (Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models   (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> for this model. Therefore, our in-house ver -tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, so we skip that part and refer the reader  _0\">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al. ( <ref type=\"formula\">2019</ref>) squeezed multiple translation engines into one transformer <ref type=\"bibr\" target=\"#b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al. ( <ref type=\"formula\">2019</ref>) squeezed multiple translation engines into one transformer <ref type=\"bibr\" target=\"#b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Introduction</head><p>Knowledge distillation (KD) <ref type=\"bibr\" target=\"#b5\">(Bucilu\u01ce, Caruana, and Niculescu-Mizil 2006;</ref><ref type=\"bibr\" tar w.tei-c.org/ns/1.0\"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type=\"bibr\" target=\"#b5\">(Bucilu\u01ce, Caruana, and Niculescu-Mizil 2006;</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onsider it as a complementary and generic add-on to enrich the training process of any neural model <ref type=\"bibr\" target=\"#b11\">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Introduction</head><p>Knowledge distillation (KD) <ref type=\"bibr\" target=\"#b5\">(Bucilu\u01ce, Caruana, and Niculescu-Mizil 2006;</ref><ref type=\"bibr\" tar w.tei-c.org/ns/1.0\"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type=\"bibr\" target=\"#b5\">(Bucilu\u01ce, Caruana, and Niculescu-Mizil 2006;</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onsider it as a complementary and generic add-on to enrich the training process of any neural model <ref type=\"bibr\" target=\"#b11\">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nguage understanding (NLU) models such as <ref type=\"bibr\">ELMO (Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models   (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> for this model. Therefore, our in-house ver -tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, so we skip that part and refer the reader  _0\">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onsider it as a complementary and generic add-on to enrich the training process of any neural model <ref type=\"bibr\" target=\"#b11\">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al. ( <ref type=\"formula\">2019</ref>) squeezed multiple translation engines into one transformer <ref type=\"bibr\" target=\"#b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distille. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b22\">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type=\"bibr\" target=\"#b25\">Wei et al. (2019)</ref> introduced a novel training procedure where t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve to automatically generate prompts given the few-shot training data using the generative T5 model <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>. This allows us to cheaply obtain effecti lly from a fixed set of label words M(Y). To address this challenging problem, we propose to use T5 <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>, a large pre-trained text-to-text Transfo e <ref type=\"table\">B</ref>.1.</p><p>For automatic template search with T5, we take the T5-3B model <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>, which is the largest publicly available . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  also connected to other few-shot learning paradigms in NLP, including (1) semi-supervised learning <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">Xie et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mple examples that are semantically close to x in . Specifically, we use a pretrained Sentence-BERT <ref type=\"bibr\" target=\"#b32\">(Reimers and Gurevych, 2019)</ref> model to obtain embeddings for all. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rstadt et al., 2019)</ref>, MNLI <ref type=\"bibr\" target=\"#b44\">(Williams et al., 2018)</ref>, QNLI <ref type=\"bibr\" target=\"#b31\">(Rajpurkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b9\">(Da. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">(Dolan and Brockett, 2005)</ref>, QQP<ref type=\"foot\" target=\"#foot_9\">12</ref> and STS-B <ref type=\"bibr\" target=\"#b8\">(Cer et al., 2017)</ref>, we follow Zhang et al. ( <ref type=\"formula\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  relations, e.g., (X, born in, ?), or require a large number of examples for gradient-guided search <ref type=\"bibr\" target=\"#b35\">(Shin et al., 2020)</ref>. Our approach aims to develop general-purpo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge models A number of recent studies have focused on better methods for fine-tuning language models <ref type=\"bibr\" target=\"#b17\">(Howard and Ruder, 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dodge et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b5\">Bentivogli et al., 2009)</ref>, MRPC <ref type=\"bibr\" target=\"#b13\">(Dolan and Brockett, 2005)</ref>, QQP<ref type=\"foot\" target=\"#foot_9. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b45\">Xie et al., 2020)</ref>, where a set of unlabeled examples are given; (2) meta-learning <ref type=\"bibr\" target=\"#b16\">(Yu et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Han et al., 201 labeled examples are given; (2) meta-learning <ref type=\"bibr\" target=\"#b16\">(Yu et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Han et al., 2018;</ref><ref type=\"bibr\">Bansal et al., 2020a,b;</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  relations, e.g., (X, born in, ?), or require a large number of examples for gradient-guided search <ref type=\"bibr\" target=\"#b35\">(Shin et al., 2020)</ref>. Our approach aims to develop general-purpo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lliams et al., 2018)</ref>, QNLI <ref type=\"bibr\" target=\"#b31\">(Rajpurkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b9\">(Dagan et al., 2005;</ref><ref type=\"bibr\" target=\"#b3\">Bar-Haim et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We generalize deep self-attention distillation in MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> by only using self-attention relation disti den size the same, layer-wisely transferring hidden states and self-attention distributions. MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> proposes deep self-attention distillation,   teacher.</p><p>In this work, we generalize and simplify deep self-attention distillation of MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> by using self-attention relation distillati the student has the same number of layers as its teacher to perform layer-wise distillation. MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> transfers selfattention knowledge of teache \"bibr\" target=\"#b14\">Jiao et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Sun et al., 2019b;</ref><ref type=\"bibr\" target=\"#b40\">Wang et al., 2020)</ref>. The student models are distilled from large \"bibr\" target=\"#b14\">Jiao et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Sun et al., 2019a;</ref><ref type=\"bibr\" target=\"#b40\">Wang et al., 2020)</ref>  MobileBERT compresses a specially designed  esults of Distil-BERT, TinyBERT 2 , BERT SMALL , Truncated BERT BASE and 6\u00d7768 MINILM are taken from<ref type=\"bibr\" target=\"#b40\">Wang et al. (2020)</ref>. BERT SMALL</figDesc><table><row><cell>Model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b27\">(Rajpurkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b6\">(Dagan et al., 2006;</ref><ref type=\"bibr\" target=\"#b2\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b10\">Giampiccolo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /www.tei-c.org/ns/1.0\"><head n=\"2.1\">Backbone Network: Transformer</head><p>Multi-layer Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017)</ref> has been widely adopted in pretrained mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xtractive question answering.</p><p>GLUE General Language Understanding Evaluation (GLUE) benchmark <ref type=\"bibr\" target=\"#b39\">(Wang et al., 2019)</ref> consists of two single-sentence classificat p>The summary of datasets used for the General Language Understanding Evaluation (GLUE) benchmark 4 <ref type=\"bibr\" target=\"#b39\">(Wang et al., 2019)</ref> is presented in Table <ref type=\"table\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms to predict a continuous sub-span of the passage to answer the question. We evaluate on SQuAD 2.0 <ref type=\"bibr\" target=\"#b26\">(Rajpurkar et al., 2018)</ref>, which has been served as a major ques org/ns/1.0\"><head>B SQuAD 2.0</head><p>We present the dataset statistics and metrics of SQuAD 2.0 5 <ref type=\"bibr\" target=\"#b26\">(Rajpurkar et al., 2018)</ref> in Table <ref type=\"table\" target=\"#ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b27\">(Rajpurkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b6\">(Dagan et al., 2006;</ref><ref type=\"bibr\" target=\"#b2\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b10\">Giampiccolo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /www.tei-c.org/ns/1.0\"><head n=\"2.1\">Backbone Network: Transformer</head><p>Multi-layer Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017)</ref> has been widely adopted in pretrained mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b10\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\">Bentivogli et al., 2009)</ref> and WNLI <ref type=\"bibr\" target=\"#b18\">(Levesque et al., 2012)</ref>). Following BERT <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b22\">Mukherjee and Awadallah, 2020;</ref><ref type=\"bibr\" target=\"#b20\">Xu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Hou et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Li et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ces and latency. Knowledge distillation (KD; <ref type=\"bibr\" target=\"#b11\">Hinton et al. 2015</ref><ref type=\"bibr\" target=\"#b28\">, Romero et al. 2015)</ref> has been widely employed to compress pret oft target probabilities to train student models. More fine-grained knowledge such as hidden states <ref type=\"bibr\" target=\"#b28\">(Romero et al., 2015)</ref> and attention distributions <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xtractive question answering.</p><p>GLUE General Language Understanding Evaluation (GLUE) benchmark <ref type=\"bibr\" target=\"#b39\">(Wang et al., 2019)</ref> consists of two single-sentence classificat p>The summary of datasets used for the General Language Understanding Evaluation (GLUE) benchmark 4 <ref type=\"bibr\" target=\"#b39\">(Wang et al., 2019)</ref> is presented in Table <ref type=\"table\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2019)</ref> through providing \"task descriptions\" without using any labeled examples. GPT-3 <ref type=\"bibr\" target=\"#b0\">(Brown et al., 2020)</ref> demonstrated an impressive few-shot learnin fine-tuned language models. Even GPT-3 failed to beat the BERT-base baseline on another NLI dataset <ref type=\"bibr\" target=\"#b0\">(Brown et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org guage modeling problems. This approach does not make use of the available training examples. Later, <ref type=\"bibr\" target=\"#b0\">Brown et al. (2020)</ref> demonstrate an effective fewshot transfer by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: age model pretraining has had a tremendous impact on solving many natural language processing tasks <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Radford, 2018  the parameters of the language model are frozen and a task-specific head is trained on top of them <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018)</ref>. The second approach fine-tunes all model   <ref type=\"bibr\" target=\"#b8\">(Radford, 2018)</ref>. The latter can sometimes yield better results <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2019)</ref>, while the first one usually offers better. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge inference (NLI) tasks and show that our method outperforms the frozen-features approach on SST-2 <ref type=\"bibr\" target=\"#b12\">(Socher et al., 2013)</ref> and MultiNLI <ref type=\"bibr\" target=\"#b1 T-2</head><p>SST-2 is the binary sentence classification version of the Stanford Sentiment Treebank <ref type=\"bibr\" target=\"#b12\">(Socher et al., 2013)</ref> consisting of short movie reviews from Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -features approach on SST-2 <ref type=\"bibr\" target=\"#b12\">(Socher et al., 2013)</ref> and MultiNLI <ref type=\"bibr\" target=\"#b13\">(Williams et al., 2018)</ref> tasks and has comparable performance wi (MultiNLI / MNLI) is a corpus of sentence pairs with human-annotated textual entailment information <ref type=\"bibr\" target=\"#b13\">(Williams et al., 2018)</ref>. The task is part of the GLUE benchmark. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge inference (NLI) tasks and show that our method outperforms the frozen-features approach on SST-2 <ref type=\"bibr\" target=\"#b12\">(Socher et al., 2013)</ref> and MultiNLI <ref type=\"bibr\" target=\"#b1 T-2</head><p>SST-2 is the binary sentence classification version of the Stanford Sentiment Treebank <ref type=\"bibr\" target=\"#b12\">(Socher et al., 2013)</ref> consisting of short movie reviews from Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arge <ref type=\"bibr\" target=\"#b5\">(Liu et al., 2019)</ref> model from the huggingface transformers <ref type=\"bibr\" target=\"#b14\">(Wolf et al., 2020)</ref> library.</p><p>Both the prompt and answer s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: age model pretraining has had a tremendous impact on solving many natural language processing tasks <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Radford, 2018  the parameters of the language model are frozen and a task-specific head is trained on top of them <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018)</ref>. The second approach fine-tunes all model   <ref type=\"bibr\" target=\"#b8\">(Radford, 2018)</ref>. The latter can sometimes yield better results <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2019)</ref>, while the first one usually offers better. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge inference (NLI) tasks and show that our method outperforms the frozen-features approach on SST-2 <ref type=\"bibr\" target=\"#b12\">(Socher et al., 2013)</ref> and MultiNLI <ref type=\"bibr\" target=\"#b1 T-2</head><p>SST-2 is the binary sentence classification version of the Stanford Sentiment Treebank <ref type=\"bibr\" target=\"#b12\">(Socher et al., 2013)</ref> consisting of short movie reviews from Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: age model pretraining has had a tremendous impact on solving many natural language processing tasks <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Radford, 2018  the parameters of the language model are frozen and a task-specific head is trained on top of them <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018)</ref>. The second approach fine-tunes all model   <ref type=\"bibr\" target=\"#b8\">(Radford, 2018)</ref>. The latter can sometimes yield better results <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2019)</ref>, while the first one usually offers better. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b8\">(Klicpera, Bojchevski, and G\u00fcnnemann 2019)</ref>. For disassortative networks, we add Geom-GCN <ref type=\"bibr\" target=\"#b15\">(Pei et al. 2020)</ref> and MLP as new benchmarks. All methods were i are different from those in Geom-GCN. The reason is that in the disassortative networks provided by <ref type=\"bibr\" target=\"#b15\">(Pei et al. 2020)</ref>, i.e., Cham-5 and Squi-5 in Table <ref type=\"  and G\u00fcnnemann 2019)</ref> incorporates personalized PageRank to the aggregation function; Geom-GCN <ref type=\"bibr\" target=\"#b15\">(Pei et al. 2020</ref>) utilizes the structural similarity to capture. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rful ability to learn node representations by jointly encoding network structures and node features <ref type=\"bibr\" target=\"#b21\">(Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b24\">Zhang, Cui, and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rix, which represents the convolutional kernel in the spectral domain, replacing U f . Spectral CNN <ref type=\"bibr\" target=\"#b0\">(Bruna et al. 2014</ref>) uses a non-parametric convolutional kernel g lution kernel in spectral domain, by leveraging the theory of graph signal processing. Spectral CNN <ref type=\"bibr\" target=\"#b0\">(Bruna et al. 2014</ref>) treats the convolution kernel as a trainable. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n be seen as a special form of low-pass filter <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b9\">Li et al. 2019)</ref>. Some recent studies <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n be seen as a special form of low-pass filter <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b9\">Li et al. 2019)</ref>. Some recent studies <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rful ability to learn node representations by jointly encoding network structures and node features <ref type=\"bibr\" target=\"#b21\">(Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b24\">Zhang, Cui, and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b22\">(Xu et al. 2019c)</ref>, <ref type=\"bibr\">GAT (Velickovic et al. 2018)</ref>, MoNet <ref type=\"bibr\" target=\"#b11\">(Monti et al. 2017)</ref>, GraphSAGE (Hamilton, Ying, and Leskovec 20 2018)</ref> employs self-attention to calculate the coefficients of neighbors in aggregation; MoNet <ref type=\"bibr\" target=\"#b11\">(Monti et al. 2017</ref>) provides a unified generalization of graph . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s by aggregating information from neighbors, which can be seen as a special form of low-pass filter <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b9\">Li et al. 2019)</ ing process of FAGCN. Here, we formally define the whole architecture of FAGCN. Some recent studies <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b2\">Cui et al. 2020</ n kernel  <ref type=\"figure\">2</ref>, compared with traditional low-pass filters, e.g., GCN and SGC <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019)</ref></p><formula xml:id=\"formula_7\">F 2 L with g \u03b8  </head><p>We compare FAGCN with two types of representative GNNs: Spectral-based methods, i.e., SGC <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019)</ref>, GCN <ref type=\"bibr\" target=\"#b7\">(Kipf and W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s by aggregating information from neighbors, which can be seen as a special form of low-pass filter <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b9\">Li et al. 2019)</ ing process of FAGCN. Here, we formally define the whole architecture of FAGCN. Some recent studies <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b2\">Cui et al. 2020</ n kernel  <ref type=\"figure\">2</ref>, compared with traditional low-pass filters, e.g., GCN and SGC <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019)</ref></p><formula xml:id=\"formula_7\">F 2 L with g \u03b8  </head><p>We compare FAGCN with two types of representative GNNs: Spectral-based methods, i.e., SGC <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019)</ref>, GCN <ref type=\"bibr\" target=\"#b7\">(Kipf and W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Even the raw features, containing both low-and high-frequency information, are alternative solution <ref type=\"bibr\" target=\"#b19\">(Wang et al. 2020b</ref>). Secondly, it is well established that the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Even the raw features, containing both low-and high-frequency information, are alternative solution <ref type=\"bibr\" target=\"#b19\">(Wang et al. 2020b</ref>). Secondly, it is well established that the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o be predicted by performing calculations on these vectors <ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>. However, traditi  learnt h\u00c3. Such estimation is sensitive to random variations of data which may lead to overfitting <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Therefore, in this paper, a complete Bayesian treatment i SVD++ model with 30 epochs with learning rate c \u00bc 0:004 and regularization parameter k \u00bc 0:13. RTTF <ref type=\"bibr\" target=\"#b14\">[15]</ref>: RTTF model considers the linguistic similarity between re ive function is equivalent to using SGD in Eq. ( <ref type=\"formula\" target=\"#formula_10\">14</ref>) <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Second, the impact of using different vector lengths to e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing as a complement to the rule-based method to reduce invalid recommendations due to missing rules <ref type=\"bibr\" target=\"#b43\">[44]</ref>. Additionally, the collaborative filtering method can also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing as a complement to the rule-based method to reduce invalid recommendations due to missing rules <ref type=\"bibr\" target=\"#b43\">[44]</ref>. Additionally, the collaborative filtering method can also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e recommendations <ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref>. With the development of related technologies in the field. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e score distribution of the most similar senior students to achieve a better prediction performance <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Recently, Lin et al. presented a convex optimization-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 25\">[26]</ref>. In addition, twostep collaborative filtering based method is proposed by Lee et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref>. This study divided Bayesian Personal Ranking Matrix Facto. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 25\">[26]</ref>. In addition, twostep collaborative filtering based method is proposed by Lee et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref>. This study divided Bayesian Personal Ranking Matrix Facto. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: extract formatted and un-formatted data consisting of both global and local features of the courses <ref type=\"bibr\" target=\"#b29\">[30]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed a neighborhood-based user collaborative filtering method to generate top-n course recommendation <ref type=\"bibr\" target=\"#b23\">[24]</ref>. This MF-based method was enhanced by Thanh-Nhan et al. wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alculate the topic of courses, thereby recommending them to students with similar topic preferences <ref type=\"bibr\" target=\"#b30\">[31]</ref>. To understand the features of a course better, a deep cou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e score distribution of the most similar senior students to achieve a better prediction performance <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Recently, Lin et al. presented a convex optimization-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odalities. The data consists of video clips obtained from real court trials, initially presented in <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>Unlike previous work on this dataset, which focuses o  trials. The dataset description is included here for completeness; further details can be found in <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= l, by carefully identifying and labeling truthful and deceptive video clips from trial's recordings <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>In this work, we focus on deception at the subject le life trial dataset was first presented, together with a video-clip level deception detection system <ref type=\"bibr\" target=\"#b8\">[9]</ref>. iv) Different from the earlier work, our evaluations are co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ons as available in Matlab. We use the PyTorch library for the implementation of the NN classifiers <ref type=\"bibr\" target=\"#b20\">[21]</ref>. During our experiments, all classifiers are evaluated usi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Among the studies that report results on this database, Jaiswal et. al. used the OpenFace toolkit <ref type=\"bibr\" target=\"#b72\">[72]</ref> to extract visual features and the OpenSmile toolkit <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nomize facial expressions and gestures for emotion-and deceit-related applications. Bartlett et al. <ref type=\"bibr\" target=\"#b54\">[54]</ref> introduced a real-time system to identify deceptive behavi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ther relate them to an act of deceit <ref type=\"bibr\" target=\"#b52\">[52]</ref>. Ekman and Rosenberg <ref type=\"bibr\" target=\"#b53\">[53]</ref> developed the Facial Action Coding System (FACS) to taxono. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: subjects to create a reliable deception detection system <ref type=\"bibr\" target=\"#b70\">[70]</ref>, <ref type=\"bibr\" target=\"#b71\">[71]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns such as deception and social behaviour <ref type=\"bibr\" target=\"#b63\">[63]</ref>. Hillman et al. <ref type=\"bibr\" target=\"#b64\">[64]</ref> determined that increased speech prompting gestures were a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o build deception models using machine learning approaches <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b35\">[35]</ref> and showed that the use of psycholinguistic information wa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lkit <ref type=\"bibr\" target=\"#b72\">[72]</ref> to extract visual features and the OpenSmile toolkit <ref type=\"bibr\" target=\"#b73\">[73]</ref> to extract acoustic features which are then fed to an SVM . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oth hands movement analysis as depicted in Figure <ref type=\"figure\">2</ref>. ten Brinke and Porter <ref type=\"bibr\" target=\"#b11\">[12]</ref> reported that deceptive people blink at a faster rate than 684, IEEE Transactions on Affective Computing public community for the return of a missing relative <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The work closest to ours is presented by Fornaciari and P  footage in which individuals pleading to the public community for the return of a missing relative <ref type=\"bibr\" target=\"#b11\">[12]</ref>. They report informative codings that reflect deception, e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b58\">[58]</ref>, <ref type=\"bibr\" target=\"#b59\">[59]</ref>. Meservy et al. <ref type=\"bibr\" target=\"#b60\">[60]</ref> used individual frames as well as videos to extract geomet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e failing test case to a minimal failing case once a bug is discovered.</p><p>Compared to BlueCheck <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a prior PBT framework for hardware, the key distinctions ar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: terexample. With these features, PBT can achieve the benefits of both CRT and IDT.</p><p>Hypothesis <ref type=\"bibr\" target=\"#b9\">[9]</ref> is a state-of-the-art Python PBT library that includes built. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: EDA tools, or Yosys-compatible Verilog accepted by OpenROAD, a state-of-the-art opensource EDA flow <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Py. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en-Source EDA usually exist in the form of design generators to maximize reuse across the community <ref type=\"bibr\" target=\"#b0\">[1]</ref>. However, design generators are significantly more difficult  test case involves relatively large numbers. IDT finds the bug in exactly 11 test cases [i.e., gcd <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b11\">11)</ref>]. PBT can find the b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mbers. IDT finds the bug in exactly 11 test cases [i.e., gcd <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b11\">11)</ref>]. PBT can find the bug quickly with large numbers, but then resses. It is possible to combine auto-shrinking with other sophisticated random program generators <ref type=\"bibr\" target=\"#b11\">[11]</ref> by carefully using PyH2P random strategies. PyH2P can also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b11\">[11]</ref> by carefully using PyH2P random strategies. PyH2P can also leverage Symbolic-QED <ref type=\"bibr\" target=\"#b12\">[12]</ref> by applying QED transformations to generated random progra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rameter space and the test-case space.</p><p>We see coverage-guided mutational fuzzing (e.g., RFUZZ <ref type=\"bibr\" target=\"#b2\">[3]</ref>) as complementary to PBT. PBT can be used to quickly find bu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mples the input space, but can produce simple counterexamples. PBT, first popularized by QuickCheck <ref type=\"bibr\" target=\"#b7\">[7]</ref>, is a high-level, black-box testing technique where one only. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: terexample. With these features, PBT can achieve the benefits of both CRT and IDT.</p><p>Hypothesis <ref type=\"bibr\" target=\"#b9\">[9]</ref> is a state-of-the-art Python PBT library that includes built. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k. Although significant progress has been made in generating videos using temporal-dependency model <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target =\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> and temporal-dependent ones <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ ssing The inputs to audio encoders are mel-frequency cepstral coefficient (MFCC) values. Similar to <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target  can better reflect the input audio (e.g., frames in red boxes). The outputs of existing approaches <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, on the other hand, ar n metrics, we quantitatively compare the performance of our AVWnet with state-of-the-art approaches <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> on two datasets: LRW a orporate valuable temporal information into the recurrent neural network (RNN), whereas Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> divide the training into two successive steps: training an a  in different subregions. Pumarola et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> and Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> adapt the facial attention masks and base color features to  e generator and multiform discriminators arranged in a tree-like structure. Inspired by Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref>, we leverage pixel-wise landmark attentions in different sta ss, given a random face image and a speech signal, we first employ the landmark generation model in <ref type=\"bibr\" target=\"#b3\">[4]</ref> to get the sequence of predicted landmarks instead of real l  of the existing works, single-level-based attention is widely utilized to recalibrate the features <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which usually limits the ability to capture more informativ  pixel loss L pi x imposed on mouth region by the mean absolute error and regression-based loss L R <ref type=\"bibr\" target=\"#b3\">[4]</ref> are also combined together to enforce the high-quality gener example landmark and future landmark sequences predicted by a pre-trained landmark-prediction model <ref type=\"bibr\" target=\"#b3\">[4]</ref>. For audio signal, we transform videos to raw audio format f to four sections.</p><p>Fig. <ref type=\"figure\">8</ref> The detailed comparison between Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> (top) and our approach (bottom). Green boxes highlight the a  region that AVWnet produces sharper details. In contrast, the mouth region produced by Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> are blurry</p><p>The initial section consists of a four-laye esults generated by the presented AVWnet with those of two state-of-the-art approaches: Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Chung et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Al ref type=\"figure\" target=\"#fig_2\">7</ref> compares the results generated by AVWnet with Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Chung et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Bo iceable lip movements. Figure <ref type=\"figure\">8</ref> further compares our work with Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> on two cases where their results contain blurry regions and  ics and for both datasets.</p><p>We also compare the computation time needed by AVWnet and approach <ref type=\"bibr\" target=\"#b3\">[4]</ref> for generating each frame under the same parameter settings  Card). We apply both methods to produce the same talking video with 31 frames, respectively. Method <ref type=\"bibr\" target=\"#b3\">[4]</ref> can generate one frame in 0.018 s, whereas our method takes  el. In terms of synchronization, 56% scored higher for AVWnet and 28% scored higher for Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= e\">9</ref> User study on videos generated using the proposed AVWnet and the state-of-the-art method <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which shows AVWnet outperforms <ref type=\"bibr\" target=\"#b3 the state-of-the-art method <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which shows AVWnet outperforms <ref type=\"bibr\" target=\"#b3\">[4]</ref> in synchronization and image quality The performances of the e whole video. To address this limitation, we conduct a user study between our model and Chen et al.<ref type=\"bibr\" target=\"#b3\">[4]</ref>, which has better performance than Chung et al.<ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> adopt a pre-trained VGG-M model on the VGG face dataset <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>Figure <ref type=\"figure\">5</ref> displays the atte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly categorized into two classes: temporalindependent methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar eech features and facial motions. Since then, various approaches have been proposed for audiodriven <ref type=\"bibr\" target=\"#b11\">[12]</ref>, video-driven <ref type=\"bibr\" target=\"#b29\">[30]</ref> or neural network for transferring phonetic context to a dynamic lower half of the face. Karras et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> present an end-to-end training network to learn the mappin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> and temporal-dependent ones <ref type=\"bibr\" target=\"#b3\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[25,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> using 3D convolutional layers. Tulyakov et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref> decompose video features to motion and content via a recur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  an end-to-end training network to learn the mapping between 3D meshes and raw speeches. Fan et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> generate the lower half region of the face by a bi-direction. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  work</head><p>Talking face modelling Research on talking face modelling was first studied in 1990s <ref type=\"bibr\" target=\"#b39\">[40]</ref>, which establishes the map-ping between acoustic speech fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he original video. Video generation With the booming studies of high-level representation of images <ref type=\"bibr\" target=\"#b10\">[11]</ref>, researchers extend techniques for image synthesis to vide. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mic features of audio and video sequences <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Deng and Neumann <ref type=\"bibr\" target=\"#b7\">[8]</ref> combine target phoneme instances and expressive features to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for its simplicity. Other more advanced lipreading models <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> can also be integrated in our network and potentially yield. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (WSIs) <ref type=\"bibr\" target=\"#b21\">[22]</ref> has become a hot task in the medical imaging field <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" t  the survival prediction task by conducting a regression model on the WSI for each patient directly <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta ets are used in two modes. On the one hand, we follow the experimental settings of previous methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta c representation, such as CNNbased model <ref type=\"bibr\" target=\"#b21\">[22]</ref>, GCN-based model <ref type=\"bibr\" target=\"#b13\">[14]</ref> and FCN-based model <ref type=\"bibr\" target=\"#b18\">[19]</r ction task, taking the sampled patches as input to build up the graph structure. (4), DeepGraphSurv <ref type=\"bibr\" target=\"#b13\">[14]</ref> employs spectral GCN to take the topological relationships settings are used: replacing the hazard prediction using hypergraph representation by DeepGraphSurv <ref type=\"bibr\" target=\"#b13\">[14]</ref> (line 1); removing the survival rank prediction procedure   hypergraph representation can achieve better performance compared with that of using DeepGraphSurv <ref type=\"bibr\" target=\"#b13\">[14]</ref>. (2) The use of ranking information can improve the perfor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\">(Cox)</ref> [3] (line 5), BoostCI <ref type=\"bibr\" target=\"#b14\">[15]</ref> (line 6), and EnCox <ref type=\"bibr\" target=\"#b17\">[18]</ref> (line 7), respectively. The following observations can be . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  it is noted that the ranking order plays an even more important role when comparing different data <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The ranking order information among the group is more essen ival rank prediction procedure (line 2); replacing the proposed survival rank prediction by RankNet <ref type=\"bibr\" target=\"#b0\">[1]</ref>, which is a commonly used ranking method (line 3) and the wh  better than the results in line 2). (3) The proposed rank prediction method is better than RankNet <ref type=\"bibr\" target=\"#b0\">[1]</ref> (the result in line 3). (4) The proposed loss function (BCR). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e BCR by traditional cox partial likelihood loss <ref type=\"bibr\">(Cox)</ref> [3] (line 5), BoostCI <ref type=\"bibr\" target=\"#b14\">[15]</ref> (line 6), and EnCox <ref type=\"bibr\" target=\"#b17\">[18]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cal imaging field <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref>. This task aims a h patient directly <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, while ignoring the relative ranking order among these pati d, we follow the experimental settings of previous methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, i.e., randomly sel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  after scaling-down, masking, splitting informative tissues or cells by adopting the OTSU algorithm <ref type=\"bibr\" target=\"#b15\">[16]</ref>, which could avoid certain noise regions like blood, blank ef>. After loading the raw WSI, we generate the background mask on one level. By utilizing the OTSU <ref type=\"bibr\" target=\"#b15\">[16]</ref> thresholding algorithm, we set the size of the patch as 25. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cal imaging field <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref>. This task aims a h patient directly <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, while ignoring the relative ranking order among these pati d, we follow the experimental settings of previous methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, i.e., randomly sel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bove, we further propose to utilize the hypergraph structure <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> to represent the hiera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sis difference between high-risk group and low-risk group, we refer to the univariate KM-estimation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, as shown in Fig. <ref type=\"figure\" target=\"#fig_2\">3</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e BCR by traditional cox partial likelihood loss <ref type=\"bibr\">(Cox)</ref> [3] (line 5), BoostCI <ref type=\"bibr\" target=\"#b14\">[15]</ref> (line 6), and EnCox <ref type=\"bibr\" target=\"#b17\">[18]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cal imaging field <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref>. This task aims a h patient directly <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, while ignoring the relative ranking order among these pati d, we follow the experimental settings of previous methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, i.e., randomly sel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ple vFPGAs over the AXI interconnect. For example, we ported publicly-available TCP and RoCE stacks <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> to Coyote, and the h-performance multi-tenant network stack based on our open-source TCP/IP and RDMA engine for FP-GAs <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Like the memory s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rted by any of the mainstream toolchains. Some progress in this direction has been made in academia <ref type=\"bibr\" target=\"#b26\">[27]</ref>, but with significant performance penalties and implementa get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">17,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t-physical addresses <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>, .</p><p>These appr queues between vFPGAs by analogy with IPC channels, pipes, etc., in a manner reminiscent of Centaur <ref type=\"bibr\" target=\"#b41\">[42]</ref>. This allows users e.g. to chain dataflow operators runnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>, considerable recen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 12]</ref>, providing more flexibility at lower power than ASICs or GPUs for many applications (e.g. <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b17\">19,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t to extend the process abstraction of the host CPU to the GPU, although in a somewhat limited form <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and this is the foundation for programming models like CUDA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b17\">19,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on from the application -even in cases where it is explicitly visible to user threads, as in Psyche <ref type=\"bibr\" target=\"#b32\">[33]</ref>. So-called \"cooperative multi-tasking systems\" (for exampl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: her recent systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b15\">17,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, provides flexible  s.</p><p>An alternative proposed in research systems (e.g. <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref> and others) is to p get=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>. Pinned physical bu ms, since the functionality it does provide would not benefit from such an environment.</p><p>ViTAL <ref type=\"bibr\" target=\"#b61\">[62]</ref> focusses on clusters of FPGAs and, unlike Coyote, addresse. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ome mediation, typically by the OS kernel, or a runtime specific to a programming model like OpenCL <ref type=\"bibr\" target=\"#b54\">[55]</ref>.</p><p>One approach to accessing host virtual memory from . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n on these devices is an important topic that has attracted much research attention in recent years <ref type=\"bibr\" target=\"#b18\">[23,</ref><ref type=\"bibr\" target=\"#b23\">28,</ref><ref type=\"bibr\" ta ype=\"bibr\" target=\"#b13\">[18]</ref>. An rOperator can also be optimized by an existing kernel tuner <ref type=\"bibr\" target=\"#b18\">[23]</ref>. Our experience shows that, on top of existing optimizatio o a parallel task, RAMMER relies on external tools to partition an rOperator into rTasks (e.g., TVM <ref type=\"bibr\" target=\"#b18\">[23]</ref>). In another word, RAMMER uses external heuristics to deci ultiple versions of rKernel implementations from dif-  ferent sources, e.g., auto-kernel generators <ref type=\"bibr\" target=\"#b18\">[23]</ref>, hand-tuned kernels, or converted from existing operators  mpilers, including TensorFlow (v1.15.2) representing the state-of-the-art DNN framework, TVM (v0.7) <ref type=\"bibr\" target=\"#b18\">[23]</ref> and TensorFlow-XLA representing the state-of-the-art DNN c works and compilers, e.g., TensorFlow <ref type=\"bibr\" target=\"#b13\">[18]</ref>, Py-Torch [15], TVM <ref type=\"bibr\" target=\"#b18\">[23]</ref>, XLA <ref type=\"bibr\" target=\"#b12\">[17]</ref>, etc. TASO  ne in TensorFlow, employs a technique called kernel fusion <ref type=\"bibr\" target=\"#b12\">[17,</ref><ref type=\"bibr\" target=\"#b18\">23]</ref>, which merges several DNN operators into a single one when . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b48\">[53]</ref>, Tiramisu <ref type=\"bibr\" target=\"#b16\">[21]</ref>, Halide <ref type=\"bibr\" target=\"#b38\">[43]</ref>, etc. RAMMER is compatible with all these optimizations th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  proposed software-based schedulers within a GPU to schedule general workload. For example, Juggler <ref type=\"bibr\" target=\"#b17\">[22]</ref> proposes a framework to dynamically execute a job represen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: S) <ref type=\"bibr\" target=\"#b49\">[54]</ref>. To highlight such extra benefit, we leverage NASBench <ref type=\"bibr\" target=\"#b45\">[50]</ref>, a state-of-the-art NAS benchmark, to randomly generate 5,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation, RAMMER dispatches  each vEU (implemented by a PTB) to a desired SM through the GPU scheduler <ref type=\"bibr\" target=\"#b43\">[48]</ref>. To improve hardware utilization, an SM can run multiple v 2]</ref> proposes a framework to dynamically execute a job represented as a DAG of tasks. Wu et al. <ref type=\"bibr\" target=\"#b43\">[48]</ref> proposes a software approach to control the job locality o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b23\">28,</ref><ref type=\"bibr\" target=\"#b27\">32,</ref><ref type=\"bibr\" target=\"#b35\">40,</ref><ref type=\"bibr\" target=\"#b47\">52]</ref>. One of the key factors that affect the efficiency of DNN c el generators. DNN inference and its optimization have attracted a lot of recent attention. DeepCPU <ref type=\"bibr\" target=\"#b47\">[52]</ref>, BatchMaker <ref type=\"bibr\" target=\"#b22\">[27]</ref>, GRN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b39\">[44]</ref>, PRETZEL <ref type=\"bibr\" target=\"#b33\">[38]</ref>, Clipper <ref type=\"bibr\" target=\"#b20\">[25]</ref>, TF-serving <ref type=\"bibr\" target=\"#b37\">[42]</ref>, etc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that has attracted much research attention in recent years <ref type=\"bibr\" target=\"#b18\">[23,</ref><ref type=\"bibr\" target=\"#b23\">28,</ref><ref type=\"bibr\" target=\"#b27\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b14\">[19]</ref> is a representative speech recognition model; and Seq2Seq <ref type=\"bibr\" target=\"#b41\">[46]</ref> is for neural machine translation. All the implementations. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b39\">[44]</ref>, PRETZEL <ref type=\"bibr\" target=\"#b33\">[38]</ref>, Clipper <ref type=\"bibr\" target=\"#b20\">[25]</ref>, TF-serving <ref type=\"bibr\" target=\"#b37\">[42]</ref>, etc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b48\">[53]</ref>, Tiramisu <ref type=\"bibr\" target=\"#b16\">[21]</ref>, Halide <ref type=\"bibr\" target=\"#b38\">[43]</ref>, etc. RAMMER is compatible with all these optimizations th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b60\">66]</ref>. Emerging programmable (\"smart\") NICs can help overcome this problem <ref type=\"bibr\" target=\"#b28\">[32]</ref>. There are many different types of offloads that can be im r from performance issues because embedded CPU cores add tens of microseconds of additional latency <ref type=\"bibr\" target=\"#b28\">[32]</ref>. Also, no existing manycore NICs provide performant mechan y, this makes them a poor fit for pipeline designs. To overcome this limitation, the Azure SmartNIC <ref type=\"bibr\" target=\"#b28\">[32]</ref> onloads computation from the programmable NIC to a core on -performance chaining. Further, manycore NICs even struggle to drive 100 Gbps and faster line-rates <ref type=\"bibr\" target=\"#b28\">[32]</ref>. Because a single embedded processor is not enough to satu for packets that only need to be processed by a hardware accelerator. For example, Firestone et al. <ref type=\"bibr\" target=\"#b28\">[32]</ref> report that processing a packet in one of the cores on a m  network stack, can reduce load on the general purpose CPU, reduce latency, and increase throughput <ref type=\"bibr\" target=\"#b28\">[32,</ref><ref type=\"bibr\" target=\"#b44\">48,</ref><ref type=\"bibr\" ta oads that run with low latency and at line-rate.</p><p>There exist many different programmable NICs <ref type=\"bibr\" target=\"#b28\">[32,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" ta get=\"#b44\">48,</ref><ref type=\"bibr\" target=\"#b53\">59,</ref><ref type=\"bibr\" target=\"#b38\">42,</ref><ref type=\"bibr\" target=\"#b28\">32,</ref><ref type=\"bibr\" target=\"#b33\">37,</ref><ref type=\"bibr\" tar oard FPGAs located as a \"bump-in-the-wire\" use this design <ref type=\"bibr\" target=\"#b48\">[52,</ref><ref type=\"bibr\" target=\"#b28\">32,</ref><ref type=\"bibr\" target=\"#b27\">31]</ref>, and other NICs use get=\"#b44\">48,</ref><ref type=\"bibr\" target=\"#b53\">59,</ref><ref type=\"bibr\" target=\"#b38\">42,</ref><ref type=\"bibr\" target=\"#b28\">32,</ref><ref type=\"bibr\" target=\"#b33\">37,</ref><ref type=\"bibr\" tar  to enable packets to be processed by a chain of functions <ref type=\"bibr\" target=\"#b48\">[52,</ref><ref type=\"bibr\" target=\"#b28\">32]</ref>. Chaining can be modified in these NICs today but requires  get=\"#b35\">39,</ref><ref type=\"bibr\" target=\"#b13\">15,</ref><ref type=\"bibr\" target=\"#b55\">61,</ref><ref type=\"bibr\" target=\"#b28\">32]</ref>. Each tenant may have its own offload chains that may need . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  NICs use an embedded CPU core to orchestrate the processing of a packet across offloaded functions <ref type=\"bibr\" target=\"#b30\">[34]</ref>. This is because the on-chip network cannot parse complex   avenue for future work once the DPU is generally available.</p><p>PANIC is also similar to FairNIC <ref type=\"bibr\" target=\"#b30\">[34]</ref>, which improves fairness between competing applications ru. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b42\">46,</ref><ref type=\"bibr\" target=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b36\">40,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b45\">49,</ref><ref type=\"bibr\" target=\"#b42\">46,</ref><ref type=\"bibr\" target=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" tar get=\"#b59\">65,</ref><ref type=\"bibr\" target=\"#b42\">46,</ref><ref type=\"bibr\" target=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b36\">40,</ref><ref type=\"bibr\" tar  it is possible to configure different credit numbers for each unit if needed. For example, ClickNP <ref type=\"bibr\" target=\"#b43\">[47]</ref> uses a SHA1 engine that can process 64 packets in parallel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: piration from recent work on reconfigurable (RMT) switches <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b61\">67,</ref><ref type=\"bibr\" target=\"#b62\">68,</ref><ref type=\"bibr\" tar age of the pipeline are limited to relatively simple atoms that can execute within 1-2 clock cycles <ref type=\"bibr\" target=\"#b61\">[67,</ref><ref type=\"bibr\" target=\"#b54\">60,</ref><ref type=\"bibr\" ta  is borrowed from the design used in programmable switches <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b61\">67]</ref>. When a packet is received by the NIC, the RMT pipeline fir olicy and Rank Computation: When a packet arrives, the central scheduler uses stateful atoms (ALUs) <ref type=\"bibr\" target=\"#b61\">[67]</ref> to take metadata about the packet, look it up in the RMT p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  but this requires additional buffer memory at each offload: packet arrivals in Ethernet are bursty <ref type=\"bibr\" target=\"#b37\">[41,</ref><ref type=\"bibr\" target=\"#b15\">17]</ref>, and it common for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y queue as a sorted array, it is less scalable relative to other priority queue designs, e.g., PIEO <ref type=\"bibr\" target=\"#b58\">[64]</ref>, which uses two levels of memory or pHeap <ref type=\"bibr\" ierarchy to efficiently distribute storage and processing across SRAM and LUTs. Recent advancements <ref type=\"bibr\" target=\"#b58\">[64]</ref> can be used to address this (Section 5). Overall, we find . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by the offload engine, including hardware IP cores, embedded processors, and even embedded FPGAs <ref type=\"bibr\" target=\"#b68\">[74]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" target=\"#b32\">36,</ref><ref type=\"bibr\" target=\"#b64\">70,</ref><ref type=\"bibr\" target=\"#b63\">69,</ref><ref type=\"bibr\" tar get=\"#b36\">40,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" target=\"#b32\">36,</ref><ref type=\"bibr\" target=\"#b64\">70,</ref><ref type=\"bibr\" target=\"#b63\">69,</ref><ref type=\"bibr\" tar and how chains share packet buffers. Similar to prior work <ref type=\"bibr\" target=\"#b62\">[68,</ref><ref type=\"bibr\" target=\"#b64\">70]</ref>, packet scheduling policies in PANIC are programmable. Furt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: single crossbar, we can switch to a more scalable (but higher-latency) flattened butterfly topology <ref type=\"bibr\" target=\"#b39\">[43]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b42\">46,</ref><ref type=\"bibr\" target=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" target=\"#b32\">36,</ref><ref type=\"bibr\" tar get=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b36\">40,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" target=\"#b32\">36,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gain feature maps for the graph kernel. Recently, Bai et al. <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> built a graph kernel based on multilayer representation as   G 2 . The corresponding condition is recorded by exploiting <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> C(i, j) = 1 if R (i, j) is the smallest element both in row rnel is connected with the layered representation defined in <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> . Nevertheless, there exists two distinctions. Firstly, the , there exists two distinctions. Firstly, the graphs in <ref type=\"bibr\" target=\"#b2\">[3]</ref> and <ref type=\"bibr\" target=\"#b30\">[31]</ref> were decomposed into Shannon entropy substructures with gr lop a method of point set matching. However, Bai et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> and <ref type=\"bibr\" target=\"#b30\">[31]</ref> adopted the Euclidean distance between the multilayer Shan <ref type=\"bibr\" target=\"#b6\">[7]</ref> , (3) the graph matching kernel with Shannon entropy (GMKS) <ref type=\"bibr\" target=\"#b30\">[31]</ref> , (4) the graph matching kernel with approximated von Neum rget=\"#b30\">[31]</ref> , (4) the graph matching kernel with approximated von Neumann entropy (GMKV) <ref type=\"bibr\" target=\"#b30\">[31]</ref> , <ref type=\"bibr\" target=\"#b4\">(5)</ref> the Jensen-Shann are our SREGK method with graph kernel from the deep representation based on Shannon entropy (GMKS) <ref type=\"bibr\" target=\"#b30\">[31]</ref> , since they are both graph kernels based on the depth-bas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: parison include (1) the depth-based representations of graphs through deep learning networks (DDBR) <ref type=\"bibr\" target=\"#b36\">[37]</ref> , (2) the depth-based complexity trace of graphs (DBCT) <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ropy-based method to model a decision-maker's subjective utility for a criterion value. Zhou et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> showed an ave-entropy based weight assignment process cons. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref> , Marginalized Graph Kernels <ref type=\"bibr\" target=\"#b5\">[6]</ref> , Graph Hopper Kernels <ref type=\"bibr\" target=\"#b24\">[25]</ref> , Deep Graph Kernels <ref type=\"bibr\" target=\"#b25\">[26]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref> , Marginalized Graph Kernels <ref type=\"bibr\" target=\"#b5\">[6]</ref> , Graph Hopper Kernels <ref type=\"bibr\" target=\"#b24\">[25]</ref> , Deep Graph Kernels <ref type=\"bibr\" target=\"#b25\">[26]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> ), financial data analysis <ref type=\"bibr\" target=\"#b8\">[9]</ref> , social network analysis <ref type=\"bibr\" target=\"#b9\">[10]</ref> , and HTML, XML, Internet for graph models <ref type=\"bibr\" l employing the Ihara zeta function based cycles (BRWK) <ref type=\"bibr\" target=\"#b39\">[40]</ref> , <ref type=\"bibr\" target=\"#b9\">(10)</ref> the Weisfeiler-Lehman subtree kernel (WL) <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> ), financial data analysis <ref type=\"bibr\" target=\"#b8\">[9]</ref> , social network analysis <ref type=\"bibr\" target=\"#b9\">[10]</ref> , and HTML, XML, Internet for graph models <ref type=\"bibr\" l employing the Ihara zeta function based cycles (BRWK) <ref type=\"bibr\" target=\"#b39\">[40]</ref> , <ref type=\"bibr\" target=\"#b9\">(10)</ref> the Weisfeiler-Lehman subtree kernel (WL) <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep Graph Kernels <ref type=\"bibr\" target=\"#b25\">[26]</ref> , and Multiscale Laplacian Graph Kernel <ref type=\"bibr\" target=\"#b26\">[27]</ref> . There are other convolution kernel functions: Shervashid ant graph kernel is studied in recent years <ref type=\"bibr\" target=\"#b28\">[29]</ref> . Zhao et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref> proposed a labeled graph kernel for behavior analysis, Xu . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> , (9) the backtraceless random walk kernel employing the Ihara zeta function based cycles (BRWK) <ref type=\"bibr\" target=\"#b39\">[40]</ref> , <ref type=\"bibr\" target=\"#b9\">(10)</ref> the Weisfeiler-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">[19]</ref> , Semi-supervised multi-view maximum entropy discrimination <ref type=\"bibr\" target=\"#b19\">[20]</ref> , and so on.</p><p>It is worth noting that in recent years. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  included. Moreover, other approaches for information entropy are such as: Min-entropy latent model <ref type=\"bibr\" target=\"#b17\">[18]</ref> , Multi-layer entropyguided pooling <ref type=\"bibr\" targe </ref> C(i, j) = 1 if R (i, j) is the smallest element both in row i and in column j; 0 otherwise . <ref type=\"bibr\" target=\"#b17\">(18)</ref> Eq. ( <ref type=\"formula\">18</ref>) suggests that if C(i, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pular technique has drawn applicability at the instruction level (to ease functional unit pressure) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, or even at functional en short-circuit subsequent occurrences. Such lookup table approaches have been studied in the past <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> in the context of sc le prior works use lookup tables to optimize redundant output generations in other workload domains <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, there is a clear co  To study the effects of short-circuiting the CPU computations alone using prior approaches such as <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Memoization</head><p>Prior works such as <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ ing such repeated computations in the CPU execution contexts for scientific workloads. For example, <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> use hardware table t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget optimizations towards a single component such as CPU <ref type=\"bibr\" target=\"#b48\">[49]</ref><ref type=\"bibr\" target=\"#b49\">[50]</ref><ref type=\"bibr\" target=\"#b50\">[51]</ref>, video codecs <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b47\">[48]</ref>, memory <ref type=\"bibr\" target=\"#b55\">[56]</ref>, neural/vision processing <ref type=\"bibr\" target=\"#b56\">[57]</ref> and battery <ref type=\"bibr\" target=\"#b57\">[58,</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ads) by instrumenting the emulator to record memory traces <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> along with additional information about the source of the d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onal tools such as <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. During this emulation, we dump the input and outputs consu ssing (across various game execution threads) by instrumenting the emulator to record memory traces <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> along with additio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  at the instruction level (to ease functional unit pressure) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, or even at functional levels <ref type=\"bibr\" target=\"#b4\">[ g/ns/1.0\"><head>A. Identifying necessary inputs</head><p>While dataflow analysis techniques such as <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> traverse through the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b47\">48]</ref> target optimizations towards a single component such as CPU <ref type=\"bibr\" target=\"#b48\">[49]</ref><ref type=\"bibr\" target=\"#b49\">[50]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b47\">48]</ref> target optimizations towards a single component such as CPU <ref type=\"bibr\" target=\"#b48\">[49]</ref><ref type=\"bibr\" target=\"#b49\">[50]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad><p>ML is emerging as a useful tool to optimize different parts of the system such as prefetchers <ref type=\"bibr\" target=\"#b59\">[60]</ref>, branch predictors <ref type=\"bibr\" target=\"#b60\">[61]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b49\">[50]</ref><ref type=\"bibr\" target=\"#b50\">[51]</ref>, video codecs <ref type=\"bibr\" target=\"#b51\">[52,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>, sensors <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  at the instruction level (to ease functional unit pressure) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, or even at functional levels <ref type=\"bibr\" target=\"#b4\">[ g/ns/1.0\"><head>A. Identifying necessary inputs</head><p>While dataflow analysis techniques such as <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> traverse through the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rf <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PAPI <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and LiMiT <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Each of these tools use different mechanisms to interact wi rograms, such as Firefox, could not even run properly when profiled with PAPI-C due to its overhead <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>This overhead issue becomes even more critical when t cy or closedsource programs.</p><p>This was improved upon when LiMiT was introduced by Demme et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref>. They addressed the expensive system call problem. LiMiT is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ad><p>In the first experiment, we used K-LEB to collect the performance counter samples for LINPACK <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The core of the LINPACK benchmark is to solve a dense sys. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Demme et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref>, online program verification by Bruska et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>, scheduling techniques by Torres et al. <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n by Bruska et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>, scheduling techniques by Torres et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and dynamic power estimation by Liu et al. <ref type=\"bib d with scheduling techniques can help improve the throughput of the workloads in their case studies <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>. These classifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve the throughput of the workloads in their case studies <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>. These classifications can be valuable to cloud service pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cation, because the attacks don't necessarily interfere with the program execution itself. Meltdown <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a classic example. Meltdown operates as a cache side-ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .</p><p>This limitation can be seen in prior scheduling research, such as the work by Torres et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref>. In their research, they used performance counter data to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or example, modern Intel processors can collect from more than three hundred unique hardware events <ref type=\"bibr\" target=\"#b12\">[13]</ref>. On the other hand, the ARM Cortex-A9 processor PMU can co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cation, because the attacks don't necessarily interfere with the program execution itself. Meltdown <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a classic example. Meltdown operates as a cache side-ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el performance counters. They are represented by Perf <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PAPI <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and LiMiT <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Each of  omes with the cost of decreased accuracy and increased overhead.</p><p>Another popular tool is PAPI <ref type=\"bibr\" target=\"#b6\">[7]</ref>. It provides an API that allows the user to attain fine-grai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  The results are presented in Figure <ref type=\"figure\" target=\"#fig_3\">5</ref>. Muralidhara et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> classified programs with MPKI higher than 10 as high memor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cting the Markov dependence to be a regular undirected graph. It was recently proved by Garg et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>, based on a new multi-matrix extension of the Golden-Thomps ot necessarily the stationary distribution), which significantly improves the result of Garg et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>. More formally, we prove the following theorem: Theorem 1 ( ry distribution \u03c0. Our strategy is to incorporate the concentration of matrix-valued functions from <ref type=\"bibr\" target=\"#b9\">[10]</ref> into the study of general Markov chains from <ref type=\"bib To bound the expectation term, we invoke the following multi-matrix Golden-Thompson inequality from <ref type=\"bibr\" target=\"#b9\">[10]</ref>, by letting</p><formula xml:id=\"formula_14\">H j = tf (v j )  tf (v j ), j \u2208 [k].</formula><p>Theorem 4 (Multi-matrix Golden-Thompson Inequality, Theorem 1.5 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let H The key point of this theorem is to relate the expo urther bounded via the following lemma by letting e i\u03c6 = \u03b3 + ib. Lemma 1 (Analogous to Lemma 4.3 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let P be a regular Markov chain with state space [N ] wit  of k.</p><p>The analysis relies on incorporating the concentration of matrix-valued functions from <ref type=\"bibr\" target=\"#b9\">[10]</ref> into the study of general Markov chains from <ref type=\"bib ctral expansion from such inner products. In contrast, the undirected regular graph case studied in <ref type=\"bibr\" target=\"#b9\">[10]</ref> can be handled using the standard inner products, as well a uality</head><p>We need the following multi-matrix Golden-Thompson inequality from from Garg et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Theorem 4 (Multi-matrix Golden-Thompson Inequality, Theore =\"bibr\" target=\"#b9\">[10]</ref>. Theorem 4 (Multi-matrix Golden-Thompson Inequality, Theorem 1.5 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let </p><formula xml:id=\"formula_45\">f : [N ] \u2192 R d\u00d7d be  te e i\u03c6 = \u03b3 + ib with \u03b3 2 + b 2 = |\u03b3 + ib| 2 = e i\u03c6 2 = 1:</p><p>Lemma 1 (Analogous to Lemma 4.3 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let P be a regular Markov chain with state space [N ] wit http://www.tei-c.org/ns/1.0\"><head>B.3 Proof of Lemma 1</head><p>Lemma 1 (Analogous to Lemma 4.3 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let P be a regular Markov chain with state space [N ] wit  \u03c0 \u221a d \u03b11 + \u03b12\u03b13 1 \u2212 \u03b14 k , which implies \u03c0 \u2297 vec(I d ), z k \u03c0 \u2264 \u03c6 \u03c0 d \u03b11 + \u03b12\u03b13 1 \u2212 \u03b14k The same as<ref type=\"bibr\" target=\"#b9\">[10]</ref>, we can bound \u03b1 1 , \u03b1 2 \u03b1 3 , \u03b1 4 by:\u03b11 = exp (t ) \u2212 t \u2264 1 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2 ) samples to achieve a good estimator of the co-occurrence matrix.</p><p>Previous work Hsu et al. <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> study a similar pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: isting research has attempted to extend the original Chernoff bound in one of these two limitations <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar ><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. Wigderson and Xiao <ref type=\"bibr\" target=\"#b52\">[53]</ref> conjectured that Chernoff bounds can be generalized to bot. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: avior of quantities beyond scalar-valued random variables, e.g., random features in kernel machines <ref type=\"bibr\" target=\"#b39\">[40]</ref> and co-occurrence statistics which are random matrices <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rrence matrix in Equation 1 can be simplified as 2 which is known as random-walk matrix polynomials <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. <ref type=\"bibr\">Chen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es <ref type=\"bibr\" target=\"#b39\">[40]</ref> and co-occurrence statistics which are random matrices <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>.</p><p>Existing re  language processing <ref type=\"bibr\">[32-34, 26, 37]</ref>, vertex co-occurrence in graph learning <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" ta bibr\" target=\"#b46\">[47]</ref> is widely used to benchmark graph representation learning algorithms <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta led from BlogCatalog, convert the co-occurrence matrix to the one implicitly factorized by DeepWalk <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, and factorize it  tarCraft II game replays, which are equivalent to 100 years of consecutive gameplay; Perozzi et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> samples large amounts of random walk sequences from graphs  learn a function from the vertices to a low dimensional vector space. Most of them (e.g., DeepWalk <ref type=\"bibr\" target=\"#b37\">[38]</ref>, node2vec <ref type=\"bibr\" target=\"#b11\">[12]</ref>, metap  could be used to select parameters for a popular graph representation learning algorithm, DeepWalk <ref type=\"bibr\" target=\"#b37\">[38]</ref>. We set the window size T = 10, which is the default value. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. Wigderson and Xiao <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he original Chernoff bound in one of these two limitations <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th L, the approximation errors of n = 50 and n = 100 are indeed very close.</p><p>BlogCatalog Graph <ref type=\"bibr\" target=\"#b46\">[47]</ref> is widely used to benchmark graph representation learning . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, natural language processing <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, and social networ eural Networks</head><p>We briefly introduce the concepts of supervised graph learning, Transformer <ref type=\"bibr\" target=\"#b52\">[53]</ref>, and GNNs in this section.</p><p>Supervised learning tasks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b24\">25]</ref> extend this framework to model bond interactions. Furthermore, <ref type=\"bibr\" target=\"#b29\">[30]</ref> builds a hierarchical GNN to capture multilevel interactio  target=\"#b12\">[13]</ref> and its variants DMPNN <ref type=\"bibr\" target=\"#b62\">[63]</ref> and MGCN <ref type=\"bibr\" target=\"#b29\">[30]</ref> are models considering the edge features during message pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cently, many works <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> explore the graph convolutional network to encode molecular f type=\"bibr\" target=\"#b23\">[24]</ref>, Weave  <ref type=\"bibr\" target=\"#b22\">[23]</ref> and SchNet <ref type=\"bibr\" target=\"#b44\">[45]</ref> are three graph convolutional models. MPNN <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  language modeling <ref type=\"bibr\" target=\"#b8\">[9]</ref>. The traditional graph embedding methods <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> define different k. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the expressive power of chemical fingerprints, some studies <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref> introduce convolutional layers to learn the neural fingerprin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  language modeling <ref type=\"bibr\" target=\"#b8\">[9]</ref>. The traditional graph embedding methods <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> define different k. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0]</ref> is a DNNbased mulitask framework taking the molecular fingerprints as the input. GraphConv <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Weave  <ref type=\"bibr\" target=\"#b22\">[23]</ref> and SchN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: upervised fashion. <ref type=\"bibr\" target=\"#b54\">[55,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> exploit the mutual information maximization scheme to const. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b18\">[19]</ref> models such as StyleGAN <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. These face GANs are capable of generating faithful faces w >Current face GANs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> are able to generate realistic and vivid human faces with a of pretrained GANs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> is previously exploit  GFP-GAN is comprised of a degradation removal module and a pretrained face GAN (such as Style-GAN2 <ref type=\"bibr\" target=\"#b36\">[37]</ref>) as prior. They are bridged by a latent code mapping and s r the solutions in the natural image manifold and generate realistic textures. Similar to StyleGAN2 <ref type=\"bibr\" target=\"#b36\">[37]</ref>, logistic loss <ref type=\"bibr\" target=\"#b18\">[19]</ref> i ery severe degradation on both details and color. Implementation. We adopt the pretrained StyleGAN2 <ref type=\"bibr\" target=\"#b36\">[37]</ref> with 512 2 outputs as our generative facial prior. The cha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b54\">[55]</ref>. We also adopt pixel-wise metrics (PSNR and SSIM) and the perceptual metric (LPIPS <ref type=\"bibr\" target=\"#b73\">[74]</ref>) for the CelebA-Test with Ground-Truth (GT). We measure th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b18\">[19]</ref> models such as StyleGAN <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. These face GANs a  textures and colors. Generative Priors of pretrained GANs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar l Prior and Latent Code Mapping</head><p>Current face GANs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> are able to generat . PULSE <ref type=\"bibr\" target=\"#b53\">[54]</ref> iteratively optimizes the latent code of StyleGAN <ref type=\"bibr\" target=\"#b35\">[36]</ref> until the distance between outputs and inputs is below a t .\">Datasets and Implementation</head><p>Training Datasets. We train our GFP-GAN on the FFHQ dataset <ref type=\"bibr\" target=\"#b35\">[36]</ref>, which consists of 70, 000 high-quality images.</p><p>We r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as low-resolution <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, noise <ref type=\"bibr\" target=\"#b71\">[72]</ref>, blur <ref t dmarks <ref type=\"bibr\" target=\"#b8\">[9]</ref>, parsing maps <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, facial component heatmaps <ref type=\"bibr\" target=\"#b69\">[70 >, face parsing maps <ref type=\"bibr\" target=\"#b59\">[60,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and facial component heatmaps <ref type=\"bibr\" target=\"#b69\"> ore challenging, due to more complicated degradation, diverse poses and expressions. Previous works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b69\">70,</ref><ref type=\"bibr\" targ , are incorporated to further improve the performance. The geometry priors include facial landmarks <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" targ >[73]</ref>. We believe the generative facial priors also incorporate conventional geometric priors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref> and even 3D priors < \"#b5\">6]</ref> typically exploit face-specific priors in face restoration, such as facial landmarks <ref type=\"bibr\" target=\"#b8\">[9]</ref>, parsing maps <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te multi-resolution features for subsequent operations. We also employ pyramid restoration guidance <ref type=\"bibr\" target=\"#b41\">[42]</ref> for intermediate supervision <ref type=\"bibr\" target=\"#b61. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b65\">66,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, while our work attem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r degradation removal consists of seven downsamples and seven upsamples, each with a residual block <ref type=\"bibr\" target=\"#b25\">[26]</ref>. For each CS-SFT layer, we use two convolutional layers to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: identity preserving loss. Reconstruction Loss. We adopt the widely-used L1 loss and perceptual loss <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> as our reconstruct arget=\"#b31\">[32]</ref> and apply identity preserving loss in our model. Similar to perceptual loss <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we define the loss based on the feature embedding of an i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b54\">[55]</ref>. We also adopt pixel-wise metrics (PSNR and SSIM) and the perceptual metric (LPIPS <ref type=\"bibr\" target=\"#b73\">[74]</ref>) for the CelebA-Test with Ground-Truth (GT). We measure th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b59\">60]</ref> and compression removal <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. To achieve visually-pleasing results, generative adversari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>\u2022 CelebA-Test is the synthetic dataset with 3,000 CelebA-HQ images from its testing partition <ref type=\"bibr\" target=\"#b52\">[53]</ref>. The generation way is the same as that during training.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b74\">[75]</ref>, ESRGAN <ref type=\"bibr\" target=\"#b65\">[66]</ref> and DeblurGANv2 <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and we finetune them on our face training set for fair co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olor prior in generative facial prior allows us to perform color enhancement including colorization <ref type=\"bibr\" target=\"#b72\">[73]</ref>. We believe the generative facial priors also incorporate . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>, compression artifacts <ref type=\"bibr\" target=\"#b12\">[13]</ref>, etc. When applied to real-world scenarios, it becomes mor type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref> and compression removal <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. To achieve visual. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch and diverse priors including geometry, textures and colors. Generative Priors of pretrained GANs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta g/ns/1.0\"><head n=\"3.3.\">Generative Facial Prior and Latent Code Mapping</head><p>Current face GANs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targ  are also included for comparison. We also compare our GFP-GAN with image restoration methods: RCAN <ref type=\"bibr\" target=\"#b74\">[75]</ref>, ESRGAN <ref type=\"bibr\" target=\"#b65\">[66]</ref> and Debl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: perations and purposes. We adopt spatial feature transform <ref type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref> on one split and leave the left split as identity to achiev f type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and image generation <ref type=\"bibr\" target=\"#b56\">[57]</ref>. In our task, a pair of affine transformation parameters (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref> is usually employed as loss supervisions to push the solutions closer to the natural manifold <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" ta Loss. We adopt the widely-used L1 loss and perceptual loss <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> as our reconstruction loss L rec , defined as follows:</p>< ation well with the subjective evaluation of human observers <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> and our model is not good at these two metrics.</p><p>Quali. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b67\">68,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>, two typical face-specific priors: geometry priors and refe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, noise <ref type=\"bibr\" target=\"#b71\">[72]</ref>, blur <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>, compression artifacts <ref type=\"bibr\" target=\"#b12\">[13]< </ref>, deblurring <ref type=\"bibr\" target=\"#b66\">[67,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref> and compression removal <ref type=\"bibr\" target=\"#b12\">[13, <ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b77\">78]</ref>, face parsing maps <ref type=\"bibr\" target=\"#b59\">[60,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sk structured prediction (Section 5.1). Recent work has highlighted benefits of multi-task learning <ref type=\"bibr\" target=\"#b4\">(Changpinyo et al., 2018)</ref> and transfer learning <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l Random Fields <ref type=\"bibr\" target=\"#b24\">(Lafferty et al., 2001)</ref>, Structured Perceptron <ref type=\"bibr\" target=\"#b6\">(Collins, 2002)</ref>, and Structured Support Vector Machines <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: om; A3: attribute; AM-MOD: modal; AM-NEG: negation. \u2022 CoNLL-2012. The argument notation, taken from <ref type=\"bibr\" target=\"#b42\">Pradhan et al. (2012)</ref>, is as follows.</p><p>Numbered arguments  ON</head><p>Datasets. We use the standard OntoNotes benchmark defined in the CoNLL-2012 shared task <ref type=\"bibr\" target=\"#b42\">(Pradhan et al., 2012)</ref>. It consists of 2,802 documents for trai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b7\">(Collobert et al., 2011)</ref> and incorporating SP directly into DL models <ref type=\"bibr\" target=\"#b9\">(Dyer et al., 2015)</ref>.</p><p>Current state-of-the-art approaches f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cally treated with discriminative methods have been successfully solved using generative approaches <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b18\">Izacard &amp;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> 90  <ref type=\"bibr\" target=\"#b21\">(Joshi et al., 2020)</ref> 85.3 78.1 75.3 79.6 BERT+c2r-coref <ref type=\"bibr\" target=\"#b20\">(Joshi et al., 2019)</ref> 81.4 83.5 71.7 75.3 68.8 71.9 73.9 76.9 Co  span representations taking into account higher-order relations between mentions. BERT + c2f-coref <ref type=\"bibr\" target=\"#b20\">(Joshi et al., 2019)</ref> combines the previous approach with BERT. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion <ref type=\"bibr\" target=\"#b32\">(Li et al., 2019c;</ref><ref type=\"bibr\">Zhao et al., 2020;</ref><ref type=\"bibr\" target=\"#b63\">Wu et al., 2020)</ref>. Additional task-specific prior work is discus , 2019)</ref> 88.4 63.2 MRC4ERE <ref type=\"bibr\">(Zhao et al., 2020)</ref> 88.9 71.9 85.5 62.1 RSAN <ref type=\"bibr\" target=\"#b63\">(Yuan et al., 2020)</ref> 84  <ref type=\"bibr\" target=\"#b31\">(Li et a \" target=\"#b20\">(Joshi et al., 2019)</ref> 81.4 83.5 71.7 75.3 68.8 71.9 73.9 76.9 CorefQA+SpanBERT <ref type=\"bibr\" target=\"#b63\">(Wu et al., 2020)</ref> 86  dataset-specific ones, such as the maximu n, TANL performs similarly to previous approaches that employ a BERT-base model, except for CorefQA <ref type=\"bibr\" target=\"#b63\">(Wu et al., 2020)</ref>. To the best of our knowledge, ours is the fi w pretraining method which is designed to better represent and predict spans of text.</p><p>CorefQA <ref type=\"bibr\" target=\"#b63\">(Wu et al., 2020)</ref> generate queries for each mention from a ment 0)</ref> improves on the question answering approach by leveraging a diverse set of questions. RSAN <ref type=\"bibr\" target=\"#b63\">(Yuan et al., 2020</ref>) is a sequence labeling approach which utili. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion <ref type=\"bibr\" target=\"#b32\">(Li et al., 2019c;</ref><ref type=\"bibr\">Zhao et al., 2020;</ref><ref type=\"bibr\" target=\"#b63\">Wu et al., 2020)</ref>. Additional task-specific prior work is discus , 2019)</ref> 88.4 63.2 MRC4ERE <ref type=\"bibr\">(Zhao et al., 2020)</ref> 88.9 71.9 85.5 62.1 RSAN <ref type=\"bibr\" target=\"#b63\">(Yuan et al., 2020)</ref> 84  <ref type=\"bibr\" target=\"#b31\">(Li et a \" target=\"#b20\">(Joshi et al., 2019)</ref> 81.4 83.5 71.7 75.3 68.8 71.9 73.9 76.9 CorefQA+SpanBERT <ref type=\"bibr\" target=\"#b63\">(Wu et al., 2020)</ref> 86  dataset-specific ones, such as the maximu n, TANL performs similarly to previous approaches that employ a BERT-base model, except for CorefQA <ref type=\"bibr\" target=\"#b63\">(Wu et al., 2020)</ref>. To the best of our knowledge, ours is the fi w pretraining method which is designed to better represent and predict spans of text.</p><p>CorefQA <ref type=\"bibr\" target=\"#b63\">(Wu et al., 2020)</ref> generate queries for each mention from a ment 0)</ref> improves on the question answering approach by leveraging a diverse set of questions. RSAN <ref type=\"bibr\" target=\"#b63\">(Yuan et al., 2020</ref>) is a sequence labeling approach which utili. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l of the referent, e.g., R-A1.</p><p>Baselines. We compare our results with Dependency and Span SRL <ref type=\"bibr\" target=\"#b33\">(Li et al., 2019d)</ref>, which uses a Bi-LSTM with highway connectio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eveloped algorithms to improve the performance of models that have been trained with very few data. <ref type=\"bibr\" target=\"#b3\">Finn et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b23\">Snell et al.  challenge. One of the most general algorithms for meta-learning is the optimizationbased algorithm. <ref type=\"bibr\" target=\"#b3\">Finn et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b7\">Li et al. (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ate novel class distribution for each dataset. We use the LR and SVM implementation of scikit-learn <ref type=\"bibr\" target=\"#b14\">(Pedregosa et al. (2011)</ref>) with the default settings. We use the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 1986)</ref> to generate samples <ref type=\"bibr\" target=\"#b32\">(Zhang et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b2\">Chen et al. (2019b)</ref>; <ref type=\"bibr\" target=\"#b22\">Schwartz et   autoencoder can also augment samples by projecting between the visual space and the semantic space <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2019b)</ref> or encoding the intra-class deformations <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ew training samples available. <ref type=\"bibr\" target=\"#b6\">Hariharan &amp; Girshick (2017)</ref>; <ref type=\"bibr\" target=\"#b27\">Wang et al. (2018)</ref> try to synthesize data or features by learni ditional data augmentation technique to construct pretext tasks for unsupervised few-shot learning. <ref type=\"bibr\" target=\"#b27\">Wang et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b6\">Hariharan  ented version of the image with different choices for the model's input, i.e., an image and a noise <ref type=\"bibr\" target=\"#b27\">(Wang et al., 2018)</ref> or the concatenation of multiple features <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that have been trained with very few data. <ref type=\"bibr\" target=\"#b3\">Finn et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b23\">Snell et al. (2017)</ref> train models in a meta-learning fashion so  tric learning. MatchingNet <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2016)</ref> and ProtoNet <ref type=\"bibr\" target=\"#b23\">(Snell et al., 2017)</ref> learned to classify samples by comparing t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b22\">(Schwartz et al., 2018)</ref>. <ref type=\"bibr\" target=\"#b10\">Liu et al. (2019b)</ref> and <ref type=\"bibr\" target=\"#b9\">Liu et al. (2019a)</ref> propose to generate features through the clas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b4\">Gao et al. (2018)</ref>) or features <ref type=\"bibr\" target=\"#b29\">(Xian et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b31\">Zhang et al. (2019)</ref>) to augment the training set. Specifically, 18)</ref> proposed to synthesize data by introducing an adversarial generator conditioned on tasks. <ref type=\"bibr\" target=\"#b31\">Zhang et al. (2019)</ref> tried to learn a variational autoencoder to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ew training samples available. <ref type=\"bibr\" target=\"#b6\">Hariharan &amp; Girshick (2017)</ref>; <ref type=\"bibr\" target=\"#b27\">Wang et al. (2018)</ref> try to synthesize data or features by learni ditional data augmentation technique to construct pretext tasks for unsupervised few-shot learning. <ref type=\"bibr\" target=\"#b27\">Wang et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b6\">Hariharan  ented version of the image with different choices for the model's input, i.e., an image and a noise <ref type=\"bibr\" target=\"#b27\">(Wang et al., 2018)</ref> or the concatenation of multiple features <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h as documents, a graph is needed to build that represents the relation between sentences. TextRank <ref type=\"bibr\" target=\"#b18\">[19]</ref> makes it possible to form a sentence extraction algorithm,  in the united sentence collection S. The sentence similarity is defined as the same as in TextRank <ref type=\"bibr\" target=\"#b18\">[19]</ref>, to measures the overlapping word ratio between two senten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ent matching tasks, where a query is a short-form text and a document is a long-form text. DeepRank <ref type=\"bibr\" target=\"#b23\">[24]</ref> is the first work to treat query and document differently, atching signals loss.</p><p>Inspired by the previous works <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, who tell us that two texts can help each other for noise d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: els fall into representation-based approaches, interaction-based approaches, and their combinations <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>Representation-based matching approaches are inspired. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he field of news recommendation <ref type=\"bibr\" target=\"#b15\">[16]</ref> and attachment suggestion <ref type=\"bibr\" target=\"#b13\">[14]</ref>. This is mainly because long-form text matching is quite d not been fully explored. In recent years, thanks to the pioneer work SMASH proposed by Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref>, they are the first to point out that long-form text match. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[9]</ref>.</p><p>Representation-based matching approaches are inspired by the Siamese architecture <ref type=\"bibr\" target=\"#b3\">[4]</ref>. This kind of approach aims at encoding each input text in a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t matching models have been proposed and gain some improvement, such as representation based models <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta  <ref type=\"bibr\" target=\"#b11\">[12]</ref>, C-DSSM <ref type=\"bibr\" target=\"#b28\">[29]</ref>, ARC-I <ref type=\"bibr\" target=\"#b10\">[11]</ref>, RNN-LSTM <ref type=\"bibr\" target=\"#b20\">[21]</ref> and MV o input text, rather than focusing on the text representations. The pioneering work includes ARC-II <ref type=\"bibr\" target=\"#b10\">[11]</ref>, MatchPyramid <ref type=\"bibr\" target=\"#b22\">[23]</ref>, a f type=\"bibr\" target=\"#b11\">[12]</ref>, C-DSSM <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and ARC-I <ref type=\"bibr\" target=\"#b10\">[11]</ref>, four interaction-based approaches, i.e. ARC-II <ref type= and ARC-I <ref type=\"bibr\" target=\"#b10\">[11]</ref>, four interaction-based approaches, i.e. ARC-II <ref type=\"bibr\" target=\"#b10\">[11]</ref>, MatchPyramid <ref type=\"bibr\" target=\"#b22\">[23]</ref>, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b32\">[33]</ref>, information retrieval <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and dialogue <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Many deep text matching models have been proposed and gai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent matching tasks, where a query is a short-form text and a document is a long-form text. DeepRank <ref type=\"bibr\" target=\"#b23\">[24]</ref> is the first work to treat query and document differently, atching signals loss.</p><p>Inspired by the previous works <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, who tell us that two texts can help each other for noise d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch as community question answering <ref type=\"bibr\" target=\"#b32\">[33]</ref>, information retrieval <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and dialogue <ref type=\"bibr\" target=\"#b17\">[18]</ref>. M s obtained by calculating the similarity between the two corresponding representation vectors. DSSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, C-DSSM <ref type=\"bibr\" target=\"#b28\">[29]</ref>, ARC-I < are three types of text matching models, including three representation-based approaches, i.e. DSSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, C-DSSM <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and ARC gain some improvement, such as representation based models <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l language processing a lot, where most well-known models are a member of this family, such as BERT <ref type=\"bibr\" target=\"#b6\">[7]</ref>, RoBERTa <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and GPT2 \"bibr\" target=\"#b19\">[20]</ref>, RE2 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, and BERT-Finetuning <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Some implementation details are listed as follows:</p><p>\u2022  nment and fusion. We use the default settings of the model in the original paper. \u2022 BERT-Finetuning <ref type=\"bibr\" target=\"#b6\">[7]</ref>: a combination model using an attention mechanism, especiall. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  be treated as a fully connected word-level similarity graph <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. PageRank is then ap . We can see this technique is different from previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> which focus on elimi s, that can be viewed as a fully connected graph among words <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Knowing that the up cross all the layers.</p><p>Discussions: Many previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> have also noticed th get=\"#b35\">36]</ref> have also noticed the relation between Transformer and graph. Star-Transformer <ref type=\"bibr\" target=\"#b9\">[10]</ref> adds a hub node to model the long-distance dependence and e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tempt. The most recent combinations are HyperRec <ref type=\"bibr\" target=\"#b35\">[36]</ref> and DHCF <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which borrow the strengths of hypergraph neural networks   method that models the recursive dynamic social diffusion in both the user and item spaces. \u2022 DHCF <ref type=\"bibr\" target=\"#b14\">[15]</ref> is a recent hypergraph convolutional network-based method  petence in all the cases. We are unable to reproduce its superiority reported in the original paper <ref type=\"bibr\" target=\"#b14\">[15]</ref>. There are two possible causes which might lead to its fai encounter the over-smoothing problem with the increase of depth. This problem is also found in DHCF <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which is based on hypergraph modeling as well. Considerin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ated relations are not a strong signal of close friendship <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, we discard those relations which are not part of any insta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nstruction. Motif, as the specific local structure involving multiple nodes, is first introduced in <ref type=\"bibr\" target=\"#b24\">[25]</ref>. It has been widely used to describe complex structures in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ations. Following this paradigm, a large number of social recommendation models have been developed <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: exible and efficient way by leveraging the associative property of matrix multiplication. Following <ref type=\"bibr\" target=\"#b57\">[58]</ref>, we let \ud835\udc69 = \ud835\udc7a \u2299 \ud835\udc7a \ud835\udc47 and \ud835\udc7c = \ud835\udc7a \u2212 \ud835\udc69 be the adjacency matrice. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . The latest advances in this area extend self-supervised learning to graph representation learning <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ergraphs, we innovatively integrate a self-supervised task <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> into the training of the multi-channel hypergraph convoluti et=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. These studies mainly develop self-supervision tasks from t ruent views of graphs with mutual information maximization <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> is another way to set up a self-supervised task, which has  ry task to enhance the recommendation task (primary task). The recent work Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b34\">[35]</ref> is a general and popular approach for learning node within  DGI to validate the rationality of our design. We implement DGI by referring to the original paper <ref type=\"bibr\" target=\"#b34\">[35]</ref>. The results are illustrated in Fig. <ref type=\"figure\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n.</head><p>To learn the parameters of MHCN, we employ the Bayesian Personalized Ranking (BPR) loss <ref type=\"bibr\" target=\"#b28\">[29]</ref>, which is a pairwise loss that promotes an observed entry  ith a set of strong and commonlyused baselines including MF-based and GNN-based models:</p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b28\">[29]</ref> is a popular recommendation model based on Bayesian person. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  self-supervised learning to graph representation learning <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar Besides, contrasting congruent and incongruent views of graphs with mutual information maximization <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> is another way to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ponse to what they perceive their friends might do or think, which is known as the social influence <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Meanwhile, there are also studies <ref type=\"bibr\" target=\" d n=\"2\">RELATED WORK 2.1 Social Recommendation</head><p>As suggested by the social science theories <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, users' preferences . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ork to collaboratively learn representations for two-fold social effects. Song et al. develop DGRec <ref type=\"bibr\" target=\"#b31\">[32]</ref> to model both users' session-based interests as well as dy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b17\">[18]</ref> proposed an extension architecture of GCNs named R-GCNs. Gong et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> presented a framework that augments GCNs and GATs with edges edges are labeled, which indicates that the edges cannot include continuous attributes. Gong et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> presented a framework enhances GCNs <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ly very few works have tried to integrate edge features into GNN architecture. Schlichtkrull et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> proposed an extension architecture of GCNs named R-GCNs. G e features in graph neural networks, and all of them have obvious limitations. Schlichtkrull et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> proposed R-GCNs to process modeling relational data. Howev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: amically aggregate node features. Numerous variants have been derived from such design. Wang et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> introduced heterogeneous graph attention networks (HANs) t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 23]</ref> 59.0% 59.6% 71.7% LP <ref type=\"bibr\" target=\"#b27\">[28]</ref> 68.0% 45.3% 63.0% DeepWalk <ref type=\"bibr\" target=\"#b15\">[16]</ref> 67.2% 43.2% 65.3% ICA <ref type=\"bibr\" target=\"#b11\">[12]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ggregating node features using several pre-defined aggregate operations. Inspired by RNNs like LSTM <ref type=\"bibr\" target=\"#b8\">[9]</ref> and GRU <ref type=\"bibr\" target=\"#b3\">[4]</ref>, gated graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed heterogeneous graph attention networks (HANs) to process various heterogeneous graphs. Ma et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> put forward disentangled graph convolutional networks usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erimental Setup</head><p>For all the experiments, we implement EGATs based on the Pytorch framework <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Because of the large memory usage for both adjacency and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > 75.1% 69.1% 73.9% Planetoid <ref type=\"bibr\" target=\"#b24\">[25]</ref> 75.7% 64.7% 77.2% Chebyshev <ref type=\"bibr\" target=\"#b4\">[5]</ref> 81.2% 69.8% 74.4% GCN <ref type=\"bibr\" target=\"#b9\">[10]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alance value of F \u2032 H : F \u2032 E so that edges will have a higher chance to show themselves. However,  <ref type=\"bibr\" target=\"#b0\">[1]</ref> 59.5% 60.1% 70.7% SemeiEmb <ref type=\"bibr\" target=\"#b22\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > 75.1% 69.1% 73.9% Planetoid <ref type=\"bibr\" target=\"#b24\">[25]</ref> 75.7% 64.7% 77.2% Chebyshev <ref type=\"bibr\" target=\"#b4\">[5]</ref> 81.2% 69.8% 74.4% GCN <ref type=\"bibr\" target=\"#b9\">[10]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erimental Setup</head><p>For all the experiments, we implement EGATs based on the Pytorch framework <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Because of the large memory usage for both adjacency and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce to show themselves. However,  <ref type=\"bibr\" target=\"#b0\">[1]</ref> 59.5% 60.1% 70.7% SemeiEmb <ref type=\"bibr\" target=\"#b22\">[23]</ref> 59.0% 59.6% 71.7% LP <ref type=\"bibr\" target=\"#b27\">[28]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Related work</head><p>Graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b16\">[17]</ref> first extended neural network architectures to graphs. As . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mber of classes), followed by a softmax function. To improve accuracy, some techniques like dropout <ref type=\"bibr\" target=\"#b19\">[20]</ref> and L 2 regularization are also used in EGATs. All these e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  routing algorithm. Several other works also made efforts to learn graph representations. GraphSAGE <ref type=\"bibr\" target=\"#b6\">[7]</ref> generates node embeddings by aggregating node features using. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  routing algorithm. Several other works also made efforts to learn graph representations. GraphSAGE <ref type=\"bibr\" target=\"#b6\">[7]</ref> generates node embeddings by aggregating node features using. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce to show themselves. However,  <ref type=\"bibr\" target=\"#b0\">[1]</ref> 59.5% 60.1% 70.7% SemeiEmb <ref type=\"bibr\" target=\"#b22\">[23]</ref> 59.0% 59.6% 71.7% LP <ref type=\"bibr\" target=\"#b27\">[28]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erimental Setup</head><p>For all the experiments, we implement EGATs based on the Pytorch framework <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Because of the large memory usage for both adjacency and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 4\">[5]</ref> 81.2% 69.8% 74.4% GCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> 81.5% 70.3% 79.0% Monet <ref type=\"bibr\" target=\"#b13\">[14]</ref> 81 there may be exceptions in some cases. For example, the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > 75.1% 69.1% 73.9% Planetoid <ref type=\"bibr\" target=\"#b24\">[25]</ref> 75.7% 64.7% 77.2% Chebyshev <ref type=\"bibr\" target=\"#b4\">[5]</ref> 81.2% 69.8% 74.4% GCN <ref type=\"bibr\" target=\"#b9\">[10]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 4\">[5]</ref> 81.2% 69.8% 74.4% GCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> 81.5% 70.3% 79.0% Monet <ref type=\"bibr\" target=\"#b13\">[14]</ref> 81 there may be exceptions in some cases. For example, the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 23]</ref> 59.0% 59.6% 71.7% LP <ref type=\"bibr\" target=\"#b27\">[28]</ref> 68.0% 45.3% 63.0% DeepWalk <ref type=\"bibr\" target=\"#b15\">[16]</ref> 67.2% 43.2% 65.3% ICA <ref type=\"bibr\" target=\"#b11\">[12]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rimary structure and then apply GCNNs to capture the tertiary structure. Also, the recent work from <ref type=\"bibr\" target=\"#b28\">Ingraham et al. (2019)</ref> proposes an amino acid encoder that can . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: roteins as graphs and applying GCNNs <ref type=\"bibr\" target=\"#b32\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b24\">Hamilton et al., 2017)</ref>. Works based on this technique represent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b54\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b55\">Xiong et al., 2019;</ref><ref type=\"bibr\" target=\"#b49\">Thomas et al., 2019)</ref>, it can also be understood in a message pa  GridPool (\u2022) overlays the protein with increasingly coarse grids and pools all atoms into one cell <ref type=\"bibr\" target=\"#b49\">(Thomas et al., 2019)</ref>; TopKPool (\u2022) learns a per-node importanc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rotein structure determination <ref type=\"bibr\" target=\"#b11\">(Callaway, 2020)</ref> and prediction <ref type=\"bibr\" target=\"#b46\">(Senior et al., 2020)</ref>. Moreover, like other commonly used appro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b48\">(Strodthoff et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Gligorijevic et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Kulmanov et al., 2017;</ref><ref type=\"bibr\" target=\"#b35\">Kulmanov & ref><ref type=\"bibr\" target=\"#b48\">Strodthoff et al., 2020)</ref>, 1D convolutional neural networks <ref type=\"bibr\" target=\"#b36\">(Kulmanov et al., 2017;</ref><ref type=\"bibr\" target=\"#b35\">Kulmanov  representations directly from an amino acid sequence, for tasks such as protein function prediction <ref type=\"bibr\" target=\"#b36\">(Kulmanov et al., 2017;</ref><ref type=\"bibr\" target=\"#b35\">Kulmanov . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd binding affinity <ref type=\"bibr\" target=\"#b44\">(Ragoza et al., 2017)</ref> and the binding site <ref type=\"bibr\" target=\"#b29\">(Jim\u00e9nez et al., 2017)</ref>, as well as the contact region between t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b35\">Kulmanov &amp; Hoehndorf, 2019)</ref>, protein-compound interaction <ref type=\"bibr\" target=\"#b52\">(Tsubaki et al., 2018)</ref>, or protein fold classification <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by methods based on hand-crafted features, usually extracted from multi-sequence alignment tools <ref type=\"bibr\" target=\"#b3\">(Altschul et al., 1990)</ref> or annotated databases <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ame fold or not. This is also known as one-shot learning and we adopt the siamese architecture from <ref type=\"bibr\" target=\"#b33\">Koch et al. (2015)</ref> using our protein encoder.</p><p>Data. We us uring training. It also shows that our method is flexible enough to take concepts like the one from <ref type=\"bibr\" target=\"#b33\">Koch et al. (2015)</ref>, which uses hierarchical image convolutions,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b26\">(Hermosilla et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Groh et al., 2018;</ref><ref type=\"bibr\" target=\"#b54\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b55\">Xiong et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: prohibitively expensive w.r.t. multiple metrics of cost and doesn't scale.</p><p>Once-For-All (OFA) <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref> proposed to address this challenge by decoupl nfeasible for an ever-growing need for multi-platform, multi-latency deployment. Once-For-All (OFA) <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref> proposed to reduce this cost by using weight- K i are lists denoting the width &amp; kernel sizes of each of these d i layers.</p><p>Once-For-All <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref> builds a family of networks N 1 , N 2 , . . . ence, the resulting number of possible networks is enormous, with O(10 19 ) models for m = 5 blocks <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n  complicates their simultaneous optimization and necessitates techniques like progressive shrinking <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref>, increasing training time. Further, extractin nd narrower support for diverse latency targets. We explore a middle ground between these works and <ref type=\"bibr\" target=\"#b4\">Cai et al. (2020)</ref> to build design spaces that are tractable yet  rms. For latency, we use a lookup-table based latency estimator for Samsung Note 10 CPU provided by <ref type=\"bibr\" target=\"#b4\">Cai et al. (2020)</ref>. For other hardware platforms -namely NVIDIA G on of accurate models with fewer sub-optimal models -i.e. a more accurate overall model population. <ref type=\"bibr\" target=\"#b4\">Cai et al. (2020)</ref> showed that OFA networks suffer a top-1 accura mlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_0\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b4\">Cai et al. (2020)</ref> proposed a \"progressive shrinking\" approach to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: as one way to address these inefficiencies <ref type=\"bibr\" target=\"#b1\">(Berman et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Bender et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Brock et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e shared across all the networks. The \"block\" used in OFA is the Inverted Residual from MobileNetV3 <ref type=\"bibr\" target=\"#b8\">(Howard et al., 2019)</ref> and hence \"width\" here refers to the chann. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pute-hungry. The technique of weight-sharing has emerged as one way to address these inefficiencies <ref type=\"bibr\" target=\"#b1\">(Berman et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Bender et al. thods <ref type=\"bibr\">(Tan et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Cai et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Berman et al., 2020;</ref><ref type=\"bibr\" target=\"#b14\">Stamoulis et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as one way to address these inefficiencies <ref type=\"bibr\" target=\"#b1\">(Berman et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Bender et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Brock et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed nodes, and this causes GNNs to learn suboptimal representations. Graph attention networks (GATs) <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> adopt self-attention to alleviate thi l., 2016)</ref>. A representative work in a non-spectral way is the graph attention networks (GATs) <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> which model relations in graphs using http://www.tei-c.org/ns/1.0\"><head n=\"3\">MODEL</head><p>In this section, we review the original GAT <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and then describe our selfsupervised   indicate the probability of edge between node i and j. The attention mechanism of the original GAT <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> is in the dashed rectangle.</p><p>Not AGE <ref type=\"bibr\" target=\"#b18\">(Hamilton et al., 2017)</ref>, and graph attention network (GAT) <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref>. Furthermore, for Cora, CiteSeer, and aining process of three runs using a single GPU (GeForce GTX 1080Ti). We compare our model with GAT <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and GAM <ref type=\"bibr\" target=\"#b46. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: supervision, if we have prior knowledge about what to attend, we can supervise attention using them <ref type=\"bibr\" target=\"#b29\">(Knyazev et al., 2019;</ref><ref type=\"bibr\" target=\"#b59\">Yu et al., work is motivated by studies that improve attention's expressive power by giving direct supervision <ref type=\"bibr\" target=\"#b29\">(Knyazev et al., 2019;</ref><ref type=\"bibr\" target=\"#b59\">Yu et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\">(Zitnik &amp; Leskovec, 2017;</ref><ref type=\"bibr\" target=\"#b18\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b47\">Subramanian et al., 2005)</ref> is a well-known benchmark in the indu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: F = 8 features and K = 8 attention heads (total 64 features). All models are implemented in PyTorch <ref type=\"bibr\" target=\"#b39\">(Paszke et al., 2019)</ref> and PyTorch Geometric <ref type=\"bibr\" ta ><p>For GAT and SuperGAT, we use our implementation (including hyperparameter settings) in Py-Torch <ref type=\"bibr\" target=\"#b39\">(Paszke et al., 2019)</ref>. For GAM, we adopt the code in TensorFlow. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mistry. Since their patterns are complex and irregular, learning to represent graphs is challenging <ref type=\"bibr\" target=\"#b5\">(Bruna et al., 2014;</ref><ref type=\"bibr\" target=\"#b19\">Henaff et al. =\"bibr\" target=\"#b27\">(Kipf &amp; Welling, 2017)</ref> which approximate spectral graph convolution <ref type=\"bibr\" target=\"#b5\">(Bruna et al., 2014;</ref><ref type=\"bibr\" target=\"#b9\">Defferrard et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e representation learning, and it adopts the same mathematical expression for link prediction score <ref type=\"bibr\" target=\"#b49\">(Tang et al., 2015;</ref><ref type=\"bibr\" target=\"#b26\">Kipf &amp; We  as in training word or graph embeddings <ref type=\"bibr\" target=\"#b36\">(Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b49\">Tang et al., 2015;</ref><ref type=\"bibr\" target=\"#b16\">Grover &amp; L. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e representation learning, and it adopts the same mathematical expression for link prediction score <ref type=\"bibr\" target=\"#b49\">(Tang et al., 2015;</ref><ref type=\"bibr\" target=\"#b26\">Kipf &amp; We  as in training word or graph embeddings <ref type=\"bibr\" target=\"#b36\">(Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b49\">Tang et al., 2015;</ref><ref type=\"bibr\" target=\"#b16\">Grover &amp; L. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d, we can supervise attention using them <ref type=\"bibr\" target=\"#b29\">(Knyazev et al., 2019;</ref><ref type=\"bibr\" target=\"#b59\">Yu et al., 2017)</ref>. Specifically, we exploit a self-supervised ta ssive power by giving direct supervision <ref type=\"bibr\" target=\"#b29\">(Knyazev et al., 2019;</ref><ref type=\"bibr\" target=\"#b59\">Yu et al., 2017)</ref>. Specifically, we employ a self-supervised tas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b24\">(Jiang et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Franceschi et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Klicpera et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Stretcu et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ches to pooling a graph. Node drop methods <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b)</ref> (Figure <ref type=\"figure\" target=\"#fig_0\">1 br\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b15\">Gao &amp; Ji, 2019;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b)</ref>. Moreover, node clustering methods cast the  s have limited scalability to large graphs <ref type=\"bibr\" target=\"#b5\">(Cangea et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b)</ref>. Therefore, we need a sophisticated graph po  which may be problematic for large graphs <ref type=\"bibr\" target=\"#b5\">(Cangea et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b)</ref>. On the other hand, our Graph Multiset Pooli rom exploiting sparsity in the graph topology, leading to excessively high computational complexity <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b)</ref>. Furthermore, since they need a single grap ptions of baselines and our model).</p><p>Implementation Details For a fair comparison of baselines <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b)</ref>, we fix the GCN <ref type=\"bibr\" target=\"#b /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Global</head><p>6) SAGPool. This method <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b)</ref> is the node drop baseline that selects the  er <ref type=\"bibr\" target=\"#b20\">(Kingma &amp; Ba, 2014)</ref>. For a fair comparison of baselines <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b)</ref>, we use the three GCN layers <ref type=\"bib  2019)</ref>, most deep graph pooling works <ref type=\"bibr\" target=\"#b48\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b;</ref><ref type=\"bibr\" target=\"#b15\">Gao &amp; Ji,  SAGPool. 6) TopKPool. 7) ASAP. The methods <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b;</ref><ref type=\"bibr\" target=\"#b15\">Gao &amp; Ji,   tackle this limitation, Node Drop methods <ref type=\"bibr\" target=\"#b15\">(Gao &amp; Ji, 2019;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b</ref>) select the high scored nodes i (l+1) \u2208 R n l x as described in the equation 4. However, this approach is well known for their scalability issues <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b;</ref><ref type=\"bibr\" target=\"#b5\">Cangea et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: > provide the theoretical grounds on permutation invariant functions for the set encoding. Further, <ref type=\"bibr\" target=\"#b22\">Lee et al. (2019a)</ref> propose Set Transformer, which uses attentio t H defined as follows: \u03c1( h\u2208H f (h)), where f and \u03c1 are mapping functions (see the proof of PMA in <ref type=\"bibr\" target=\"#b22\">Lee et al. (2019a)</ref>).</p><p>Since H is a countable set, there is l), which is inspired by the Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al., 2019a)</ref>, to compress the n nodes into the k typical  n (SelfAtt), inspired by the Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al., 2019a)</ref>, as follows (Figure <ref type=\"figure\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vation function. Instead of computing a single attention, we can further use a multi-head attention <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017)</ref>, by linearly projecting the query Q, key  R n\u00d7d from GNNs, we define a Graph Multiset Pooling (GMPool), which is inspired by the Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al. ent nodes. To this end, we propose a Self-Attention function (SelfAtt), inspired by the Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>. Also, we use the initial node features following the recently proposed fair comparison setup <ref type=\"bibr\" target=\"#b14\">(Errica et al., 2020)</ref>. Also, we evaluate the performance on OGB we use the 10 percent of the training data as a validation data following the fair comparison setup <ref type=\"bibr\" target=\"#b14\">(Errica et al., 2020)</ref>.</p><p>For all experiments on OGB dataset LLAB) on Social domain. We use the classification accuracy as an evaluation metric. As suggested by <ref type=\"bibr\" target=\"#b14\">Errica et al. (2020)</ref> for a fair comparison, we use the one-hot . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2017;</ref><ref type=\"bibr\" target=\"#b46\">Xu et al., 2019)</ref>. 3) Set2Set. Set pooling baseline <ref type=\"bibr\" target=\"#b40\">(Vinyals et al., 2016)</ref>. 4) SortPool. 5) SAGPool. 6) TopKPool. 7 line with Graph Isomorphism Network (GIN) as a message passing layer.</p><p>3) Set2Set. This method <ref type=\"bibr\" target=\"#b40\">(Vinyals et al., 2016)</ref> is the set pooling baseline that uses re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral clustering for the graph coarsening, similarly to existing spectral-based graph pooling methods <ref type=\"bibr\" target=\"#b25\">(Ma et al., 2019;</ref><ref type=\"bibr\">Wang et al., 2019)</ref> to c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b31\">(Qi et al., 2017a;</ref><ref type=\"bibr\" target=\"#b47\">Yi et al., 2019;</ref><ref type=\"bibr\" target=\"#b37\">Snell et al., 2017)</ref>, we regard the graph representation learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2017;</ref><ref type=\"bibr\" target=\"#b46\">Xu et al., 2019)</ref>. 3) Set2Set. Set pooling baseline <ref type=\"bibr\" target=\"#b40\">(Vinyals et al., 2016)</ref>. 4) SortPool. 5) SAGPool. 6) TopKPool. 7 line with Graph Isomorphism Network (GIN) as a message passing layer.</p><p>3) Set2Set. This method <ref type=\"bibr\" target=\"#b40\">(Vinyals et al., 2016)</ref> is the set pooling baseline that uses re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b31\">(Qi et al., 2017a;</ref><ref type=\"bibr\" target=\"#b47\">Yi et al., 2019;</ref><ref type=\"bibr\" target=\"#b37\">Snell et al., 2017)</ref>, we regard the graph representation learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lti-set encoding problem. Mathematically, <ref type=\"bibr\" target=\"#b51\">Zaheer et al. (2017);</ref><ref type=\"bibr\" target=\"#b32\">Qi et al. (2017b)</ref> provide the theoretical grounds on permutatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2017)</ref>, and have achieved a large success on node classification and link prediction tasks <ref type=\"bibr\" target=\"#b39\">(Velickovic et al., 2018;</ref><ref type=\"bibr\" target=\"#b49\">You et  ction with a soft adjacency matrix, which might be further connected to the Graph Attention Network <ref type=\"bibr\" target=\"#b39\">(Velickovic et al., 2018)</ref>.</p></div> <div xmlns=\"http://www.tei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral clustering for the graph coarsening, similarly to existing spectral-based graph pooling methods <ref type=\"bibr\" target=\"#b25\">(Ma et al., 2019;</ref><ref type=\"bibr\">Wang et al., 2019)</ref> to c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lti-set encoding problem. Mathematically, <ref type=\"bibr\" target=\"#b51\">Zaheer et al. (2017);</ref><ref type=\"bibr\" target=\"#b32\">Qi et al. (2017b)</ref> provide the theoretical grounds on permutatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Hamilton et al., 2017)</ref>, which aims to encode the nodes in a graph accurately, graph pooling <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b48\">Ying et al., es in an end-to-end manner. There are two dominant approaches to pooling a graph. Node drop methods <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al.,   specific manner. Node drop methods use learnable scoring functions to drop nodes with lower scores <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b15\">Gao &amp; Ji get=\"#b40\">(Vinyals et al., 2016)</ref>. 4) SortPool. 5) SAGPool. 6) TopKPool. 7) ASAP. The methods <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al.,  We evaluate the model performance on TU datasets for a conventional 10fold cross validation setting <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Xu et al., 2 y generally use Readout <ref type=\"bibr\" target=\"#b5\">(Cangea et al., 2018)</ref> or 1D convolution <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018)</ref> for this purpose. However, it is suboptima ef type=\"bibr\" target=\"#b4\">Bianchi et al., 2019)</ref> overlook graph isomorphism except for a few <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018)</ref>.</p><p>To obtain accurate representations  encode the set of all nodes, with content-based attention over them.</p><p>4) SortPool. This method <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018)</ref> is the node drop baseline that drops unimp e it with a global scheme following their original implementation, which is similar to the SortPool <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018)</ref>.</p><p>11) EdgePool. This method <ref type lly used training/validation/test splits <ref type=\"bibr\" target=\"#b30\">(Niepert et al., 2016;</ref><ref type=\"bibr\" target=\"#b52\">Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Xu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ithout utilizing message-passing after pooling. Also, there exists a semi-supervised pooling method <ref type=\"bibr\" target=\"#b47\">(Li et al., 2019)</ref> that scores nodes with an attention scheme <r ods which mainly consider non-graph problems <ref type=\"bibr\" target=\"#b31\">(Qi et al., 2017a;</ref><ref type=\"bibr\" target=\"#b47\">Yi et al., 2019;</ref><ref type=\"bibr\" target=\"#b37\">Snell et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  GraphSAINT <ref type=\"bibr\" target=\"#b26\">(Zeng et al., 2020)</ref>. On the other hand, ClusterGCN <ref type=\"bibr\" target=\"#b7\">(Chiang et al., 2019)</ref> is a heuristic in the sense that, despite  r the baselines: GraphSAINT <ref type=\"bibr\" target=\"#b26\">(Zeng et al., 2020)</ref> and ClusterGCN <ref type=\"bibr\" target=\"#b7\">(Chiang et al., 2019)</ref> (both are message passing methods); and al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2018b)</ref>, LADIES <ref type=\"bibr\" target=\"#b27\">(Zou et al., 2019)</ref>, and GraphSAINT <ref type=\"bibr\" target=\"#b26\">(Zeng et al., 2020)</ref>. On the other hand, ClusterGCN <ref type=\"b m <ref type=\"bibr\" target=\"#b12\">(Hu et al., 2020)</ref> the accuracy for the baselines: GraphSAINT <ref type=\"bibr\" target=\"#b26\">(Zeng et al., 2020)</ref> and ClusterGCN <ref type=\"bibr\" target=\"#b7. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: some variance) including SAGE <ref type=\"bibr\" target=\"#b11\">(Hamilton et al., 2017)</ref>, FastGCN <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2018b)</ref>, LADIES <ref type=\"bibr\" target=\"#b27\">(Zou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: u-El-Haija et al., 2018)</ref>. For LiveJournal, we copy accuracy numbers for DeepWalk and PBG from <ref type=\"bibr\" target=\"#b14\">(Lerer et al., 2019)</ref> -note that a well-engineered approach (PBG ef type=\"bibr\" target=\"#b14\">(Lerer et al., 2019)</ref> -note that a well-engineered approach (PBG, <ref type=\"bibr\" target=\"#b14\">(Lerer et al., 2019)</ref>), using a mapreduce-like framework, is und gs on Link Prediction. Left: Test ROC-AUC scores. Right: Mean Rank on the right for consistency with<ref type=\"bibr\" target=\"#b14\">Lerer et al. (2019)</ref>. *OOM = Out of Memory.</figDesc><table><row. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  straight-forward. For GAT <ref type=\"bibr\" target=\"#b20\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and GCNII <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2020)</ref>, as they are more intricate, we download the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nts of A are the stochastic training examples and analyzed per <ref type=\"bibr\">(Bottou, 1998;</ref><ref type=\"bibr\" target=\"#b3\">2004)</ref>, as explained by <ref type=\"bibr\" target=\"#b2\">Baldi &amp;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: u-El-Haija et al., 2018)</ref>. For LiveJournal, we copy accuracy numbers for DeepWalk and PBG from <ref type=\"bibr\" target=\"#b14\">(Lerer et al., 2019)</ref> -note that a well-engineered approach (PBG ef type=\"bibr\" target=\"#b14\">(Lerer et al., 2019)</ref> -note that a well-engineered approach (PBG, <ref type=\"bibr\" target=\"#b14\">(Lerer et al., 2019)</ref>), using a mapreduce-like framework, is und gs on Link Prediction. Left: Test ROC-AUC scores. Right: Mean Rank on the right for consistency with<ref type=\"bibr\" target=\"#b14\">Lerer et al. (2019)</ref>. *OOM = Out of Memory.</figDesc><table><row. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  different hardware efficiency metric (e.g., prioritizes latency or energy). For example, HW-NAS in <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> develops a differentiable neural architecture many works have pointed out that DNNs with fewer FLOPs are not necessarily faster or more efficient <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>. For example, NasNet-A has a comparable compl hardware devices, that are primarily targeted by SOTA HW-NAS works.</p><p>FBNet Search Space. FBNet <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> constructs a layer-wise search space with a f  and the last three layers with the remaining layers to be searched. In this way, networks in FBNet <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> search space have more regular structure than  unique architectures. While HW-NAS researchers can develop their search algorithms on top of FBNet <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> search space, tedious efforts are required to al., 2016)</ref>. Specifically, we adopt SOTA ASIC accelerator performance simulators (1) Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Para he Appendix D.</p><p>Note that to estimated the hardware-cost of networks in the FBNet search space <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> when being executed on the commercial categor in our HW-NAS-Bench, we sum up hardware-cost of all unique blocks (i.e., \"block\" in the FBNet space <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>) within the network architectures. To validat his experiment. As an example to use our HW-NAS-Bench, we use ProxylessNAS to search over the FBNet <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> space on CIFAR-100 <ref type=\"bibr\" target=\"# NG THE PERDITION OF EYERISS USING ACCELERGY+TIMELOOP AND DNN-CHIP-PREDICTOR</head><p>Both Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Para b_6\"><head>Table 6 :</head><label>6</label><figDesc>The differences of estimation given by Accelergy<ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop<ref type=\"bibr\" target=\"#b31\">(Paras head><label>7</label><figDesc>Left: the marco-architecture of the search space proposed in the FBNet<ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> for ImageNet classification. Right: our modif ormance metrics on Eyeriss, we adopt SOTA performance simulators for DNN accelerators (1) Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Para i) or do not provide hardware-cost metrics on real hardware, limiting their applicability to HW-NAS <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Wan et al., 202 ficiency, achieving promising results yet suffering from prohibitive search time/cost. In parallel, <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Wan et al., 202 o not consider hardware efficiency metrics on real hardware, limiting their applicability to HW-NAS <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Wan et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tions including classification <ref type=\"bibr\" target=\"#b26\">(Li et al., 2020)</ref>, segmentation <ref type=\"bibr\" target=\"#b36\">(Siam et al., 2018)</ref>, and depth estimation <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tions including classification <ref type=\"bibr\" target=\"#b26\">(Li et al., 2020)</ref>, segmentation <ref type=\"bibr\" target=\"#b36\">(Siam et al., 2018)</ref>, and depth estimation <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erformance simulators (1) Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Parashar et al., 2019)</ref> and (2) DNN-Chip Predictor <ref type=\"b P-PREDICTOR</head><p>Both Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Parashar et al., 2019)</ref> and <ref type=\"bibr\">DNN-Chip Predictor nces of estimation given by Accelergy<ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop<ref type=\"bibr\" target=\"#b31\">(Parashar et al., 2019)</ref> andDNN-Chip Predictor (Zhao et al., 202  for DNN accelerators (1) Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Parashar et al., 2019)</ref> and (2) <ref type=\"bibr\">DNN-Chip Predi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k et al., 2019)</ref>, targeting IoT and self-driving environments. Although widely-used TensorFlow <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref> and PyTorch <ref type=\"bibr\" target=\"#b32\"> ting on it, we compile the architecture candidates to (1) convert them into TensorFlow Lite (TFLite)<ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref> format (</note></figure> <figure xmlns=\"htt PDDR4 (Raspberry Pi Limited.). Similar as Edge GPU, Raspi 4 can run architectures in the TensorFlow <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>, PyTorch <ref type=\"bibr\" target=\"#b32\">(Pa hmark the possible architectures in HW-NAS-Bench on Raspi 4 after compiling them to TensorFlow Lite <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref> format to measure the resulting latency.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ep learning applications with specifically framework designs <ref type=\"bibr\">(Google LLC., f;</ref><ref type=\"bibr\" target=\"#b55\">Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b15\">Geiger &amp; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  have been shown to be more hardware friendly <ref type=\"bibr\" target=\"#b14\">(Fu et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Ma et al., 2018)</ref>. The 9 considered re-defined cell candidates a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ures under the target efficiency constraints. For example, <ref type=\"bibr\">(Tan et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Howard et al., 2019;</ref><ref type=\"bibr\">Tan &amp; Le, 2019)</ref>  as the target platform by recent NAS works <ref type=\"bibr\" target=\"#b49\">(Xiong et al., 2020;</ref><ref type=\"bibr\" target=\"#b23\">Howard et al., 2019;</ref><ref type=\"bibr\">Tan et al., 2019)</ref>. T as the target platform by recent NAS works <ref type=\"bibr\" target=\"#b49\">(Xiong et al., 2020;</ref><ref type=\"bibr\" target=\"#b23\">Howard et al., 2019;</ref><ref type=\"bibr\">Tan et al., 2019)</ref> an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: scal GPU, and a 8GB 128-bit LPDDR4, for various deep learning applications including classification <ref type=\"bibr\" target=\"#b26\">(Li et al., 2020)</ref>, segmentation <ref type=\"bibr\" target=\"#b36\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: kernel-size convolutions (mixconvs as introduced by Tan &amp; Le (2019b)). In their Blockswap work, <ref type=\"bibr\" target=\"#b31\">Turner et al. (2020)</ref> use Fisher information at initialization t  et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b6\">Figurnov et al. (2016)</ref>. More recently, <ref type=\"bibr\" target=\"#b31\">Turner et al. (2020)</ref> aggregated this fisher metric for all chan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and SVHN <ref type=\"bibr\" target=\"#b22\">(Netzer et al., 2011)</ref>, and ~200 models for ImageNet1k <ref type=\"bibr\" target=\"#b3\">(Deng et al., 2009)</ref>. Fig. <ref type=\"figure\">3</ref> shows the r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rgest NAS benchmark available with over 423k</p><p>CNN models and training statistics on CIFAR-10 ( <ref type=\"bibr\" target=\"#b35\">Ying et al., 2019)</ref>.</p><p>\u2022 NAS-Bench-NLP: <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kernel-size convolutions (mixconvs as introduced by Tan &amp; Le (2019b)). In their Blockswap work, <ref type=\"bibr\" target=\"#b31\">Turner et al. (2020)</ref> use Fisher information at initialization t  et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b6\">Figurnov et al. (2016)</ref>. More recently, <ref type=\"bibr\" target=\"#b31\">Turner et al. (2020)</ref> aggregated this fisher metric for all chan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding parameters) that are estimated to have the least effect on the loss. They build on the work of <ref type=\"bibr\" target=\"#b21\">Molchanov et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b6\">Figur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  rank consistency of conventional proxies on large-scale datasets: 15k models vs. 50 models used in <ref type=\"bibr\" target=\"#b36\">(Zhou et al., 2020)</ref>. Second, we show that zero-cost proxies can  10 the number of epochs was a reasonable proxy which yielded the best results for their experiment <ref type=\"bibr\" target=\"#b36\">(Zhou et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org ,625 models in NAS-Bench-201 -a far cry from the 0.87 achieved on the 50 models in the EcoNAS paper <ref type=\"bibr\" target=\"#b36\">(Zhou et al., 2020)</ref>. We additionally explore r 8 c 8 , r 16 c 4 ef> and others use smaller datasets (CIFAR-10) as a proxy to the full task (ImageNet1k). In EcoNAS, <ref type=\"bibr\" target=\"#b36\">Zhou et al. (2020)</ref> extensively investigated reduced-training pr  NAS, a proxy training regime is often used to predict a model's accuracy instead of full training. <ref type=\"bibr\" target=\"#b36\">Zhou et al. (2020)</ref> investigate conventional proxies in depth by EcoNAS is the first systematic study of conventional proxy tasks that we found. One main finding by <ref type=\"bibr\" target=\"#b36\">Zhou et al. (2020)</ref> is that using approximately 1 4 of the model chs instead of 200. This is a common technique used in speeding up convergence for training proxies <ref type=\"bibr\" target=\"#b36\">Zhou et al. (2020)</ref>. We acknowledge that slightly better correla ><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>ON NAS-BENCH-201Even though<ref type=\"bibr\" target=\"#b36\">Zhou et al. (2020)</ref> thoroughly investigated reduced-training pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arning (RL) <ref type=\"bibr\" target=\"#b37\">(Zoph &amp; Le, 2017)</ref>, aging evolution (AE) search <ref type=\"bibr\" target=\"#b25\">(Real et al., 2019)</ref> and predictor-based search <ref type=\"bibr\" oose the ones ranked highest as the initial population (pool) in the aging evolution (AE) algorithm <ref type=\"bibr\" target=\"#b25\">(Real et al., 2019)</ref>.  Binary Predictor We warm up a binary grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and SVHN <ref type=\"bibr\" target=\"#b22\">(Netzer et al., 2011)</ref>, and ~200 models for ImageNet1k <ref type=\"bibr\" target=\"#b3\">(Deng et al., 2009)</ref>. Fig. <ref type=\"figure\">3</ref> shows the r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dels learn contacts in the self-attention maps with state-of-the-art performance. We compare ESM-1b <ref type=\"bibr\" target=\"#b37\">(Rives et al., 2020)</ref>, a large-scale (650M parameters) Transform  LSTM pretrained on protein sequences to fit contacts. <ref type=\"bibr\">Rao et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b37\">Rives et al. (2020)</ref> perform benchmarking of multiple protein la gn a score of 0 for these sequences. We also provide a comparison to the bilinear model proposed by <ref type=\"bibr\" target=\"#b37\">Rives et al. (2020)</ref>. The logistic regression model achieves a l tion, we compare our logistic regression model to the bilinear contact prediction model proposed by <ref type=\"bibr\" target=\"#b37\">Rives et al. (2020)</ref>. This model trains two separate linear proj. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  language modeling with an unsupervised training objective has been investigated by multiple groups <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Alley et al., </p><p>A line of work in this emerging field proposes the Transformer for protein language modeling <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019;</ref><ref type=\"bibr\">Rao et al., 2019)</ref>. O ures across a variety of tasks, but were not able to show a benefit in the fully end-to-end setting <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019;</ref><ref type=\"bibr\">Rao et al., 2019;</ref><re learn underlying intrinsic properties of proteins such as structure and function from sequence data <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019)</ref>.</p><p>A line of work in this emerging fie d n=\"4.3\">TRANSFORMERS</head><p>We evaluate several pre-trained Transformer models, including ESM-1 <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019)</ref>, ProtBert-BFD <ref type=\"bibr\" target=\"#b1 g. A similar scaling law has been observed previously for supervised secondary structure prediction <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019)</ref>, and parallels observations in the NLP com ns/1.0\"><head>A.4 ESM-1 IMPLEMENTATION DETAILS</head><p>The original ESM-1 models were described in <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019)</ref>. ESM-1 is trained on Uniref50 in contrast  t in developing similar models for proteins <ref type=\"bibr\" target=\"#b2\">(Alley et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Rives et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Heinzinger et ref type=\"bibr\">Rao et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Elnaggar et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b36\">Rives et al. (2019)</ref> were the first to study protein Transformer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: secondary structure via attention by training a separate logistic regression on the Netsurf dataset <ref type=\"bibr\" target=\"#b23\">(Klausen et al., 2019)</ref>. As with the logistic regression on cont. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s constraints. Similarly, FragFold <ref type=\"bibr\">(Kosciolek &amp; Jones, 2014)</ref> and Rosetta <ref type=\"bibr\" target=\"#b31\">(Ovchinnikov et al., 2016)</ref> incorporate constraints from a Potts. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e models for natural language processing <ref type=\"bibr\" target=\"#b49\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Devlin et al., 2019)</ref>, there has been considerable interest in d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ckhmmer until a Neff of 128 was achieved (with a maximum of 8 iterations), a procedure described by <ref type=\"bibr\" target=\"#b56\">Zhang et al. (2020)</ref>. This resulted in very similar, but slightl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b5\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b39\">Seemayer et al., 2014)</ref>. Pseudolikelihood maximization is genera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  model trained on UniRef50 <ref type=\"bibr\" target=\"#b45\">(Suzek et al., 2007)</ref> to the Gremlin <ref type=\"bibr\" target=\"#b21\">(Kamisetty et al., 2013)</ref> pipeline which implements a log linear. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  sequences or evolutionary features as inputs <ref type=\"bibr\" target=\"#b3\">(AlQuraishi, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Ingraham et al., 2019)</ref>. <ref type=\"bibr\" target=\"#b54\">Xu et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l) to the underlying MSA, a technique known as Direct Coupling Analysis (DCA). This was proposed by <ref type=\"bibr\" target=\"#b25\">Lapedes et al. (1999)</ref> and reintroduced by <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: DCA) <ref type=\"bibr\" target=\"#b30\">(Morcos et al., 2011)</ref>, sparse inverse covariance (PSICOV) <ref type=\"bibr\" target=\"#b20\">(Jones et al., 2011)</ref> and pseudolikelihood maximization <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hods have been developed to fit the underlying Markov Random Field, including meanfield DCA (mfDCA) <ref type=\"bibr\" target=\"#b30\">(Morcos et al., 2011)</ref>, sparse inverse covariance (PSICOV) <ref  ethods have been proposed to extend contact prediction to structure prediction. For example, EVFold <ref type=\"bibr\" target=\"#b30\">(Marks et al., 2011)</ref> and DCAFold <ref type=\"bibr\" target=\"#b44\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ckhmmer until a Neff of 128 was achieved (with a maximum of 8 iterations), a procedure described by <ref type=\"bibr\" target=\"#b56\">Zhang et al. (2020)</ref>. This resulted in very similar, but slightl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  model trained with pseudolikelihood <ref type=\"bibr\" target=\"#b5\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013)</ref>. Contacts can be extracted from the atten f> and pseudolikelihood maximization <ref type=\"bibr\" target=\"#b5\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b39\">Seemayer et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b36\">(Rives et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Alley et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Heinzinger et al., 2019;</ref><ref type=\"bibr\">Rao et al., 2019;</ref ibr\" target=\"#b2\">(Alley et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Rives et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Heinzinger et al., 2019;</ref><ref type=\"bibr\">Rao et al., 2019;</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2019)</ref>. Other recent work with deep learning uses sequences or evolutionary features as inputs <ref type=\"bibr\" target=\"#b3\">(AlQuraishi, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Ingraham et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b3\">(AlQuraishi, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Ingraham et al., 2019)</ref>. <ref type=\"bibr\" target=\"#b54\">Xu et al. (2020)</ref> demonstrates the incorporation of coevolutiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned using Graph Neural Networks (GNNs). We used message passing neural networks (MPNNs) in practice <ref type=\"bibr\" target=\"#b7\">(Gilmer et al., 2017)</ref>, but other GNNs can fit the framework as w y parameters \u03b8, which has been proven powerful to predict chemical properties with molecular graphs <ref type=\"bibr\" target=\"#b7\">(Gilmer et al., 2017)</ref>. Particularly, given a molecule x, we inpu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ling schedule. In addition to this, other instantiations such as Metropolis-Hastings (MH) algorithm <ref type=\"bibr\" target=\"#b22\">(Metropolis et al., 1953)</ref> where \u03b1 = 1 are also feasible under o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ke chemical space is enormous, approximate 10 33 for realistic drugs that could ever be synthesized <ref type=\"bibr\" target=\"#b27\">(Polishchuk et al., 2013)</ref>. Therefore it is very challenging to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b9\">(G\u00f3mez-Bombarelli et al., 2018;</ref><ref type=\"bibr\" target=\"#b13\">Jin et al., 2018;</ref><ref type=\"bibr\" target=\"#b35\">Winter et al., 2019)</ref>. These methods rely heavily on the quality. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing multiple properties simultaneously, as GAs are likely to get trapped in regions of local optima <ref type=\"bibr\" target=\"#b26\">(Paszkowicz, 2009)</ref>. RationaleRL is a very strong baseline that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iteria to be considered in building a suitable drug candidate library in early-stage drug discovery <ref type=\"bibr\" target=\"#b11\">(Huggins et al., 2011)</ref>.</p><p>Implementation details. For the f t is more important to well explore the chemical space -find novel and diverse bio-active molecules <ref type=\"bibr\" target=\"#b11\">(Huggins et al., 2011)</ref>.</p></div> <div xmlns=\"http://www.tei-c.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urally, multi-objective molecule design is much more challenging than the single-objective scenario <ref type=\"bibr\" target=\"#b16\">(Jin et al., 2020)</ref>.</p><p>This paper studies the problem of mul b)</ref>. Current state-of-the-art multi-objective molecular generation is a rationale-based method <ref type=\"bibr\" target=\"#b16\">(Jin et al., 2020)</ref>. In this approach, the authors propose to bu . It performs Bayesian optimization (BO) to guide molecules towards desired properties. RationaleRL <ref type=\"bibr\" target=\"#b16\">(Jin et al., 2020)</ref> is a state-of-the-art approach for multi-pro s better. Among them, the current state-of-the-art approach is a rationale-based method proposed by <ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>. In this method, the authors propose to build .org/ns/1.0\"><head n=\"4\">EXPERIMENTS</head><p>4.1 EXPERIMENT SETUP Biological objectives. Following <ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>, we consider the following two properties as  JNK3+QED+SA: Inhibiting GSK3\u03b2 or JNK3 while being drug-like and synthetically accessible. Following <ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>, we adopt QED <ref type=\"bibr\" target=\"#b2\">(  an adversarial loss is incorporated in the fitness evaluation.</p><p>Evaluation metrics. Following <ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>, we generate N = 5000 molecules for each appr re synthetically accessible, and 0.67 is the rescaled threshold corresponding to the one reported in<ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ arget=\"#b30\">(Rogers &amp; Hahn, 2010)</ref> as input, and the positive threshold is set as \u03b4 = 0.5 <ref type=\"bibr\" target=\"#b16\">(Jin et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Li et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 00 frequently appearing fragments that contain no more than 10 heavy atoms from the ChEMBL database <ref type=\"bibr\" target=\"#b6\">(Gaulton et al., 2017)</ref> by enumerating single bonds to break. As . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  molecules with similarity less than 0.4 compared to the nearest neighbor x SNN in the training set <ref type=\"bibr\" target=\"#b25\">(Olivecrona et al., 2017)</ref>:</p><formula xml:id=\"formula_10\">Nov . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thms (EAs) and genetic algorithms (GAs) <ref type=\"bibr\" target=\"#b23\">(Nicolaou et al., 2012;</ref><ref type=\"bibr\" target=\"#b4\">Devi et al., 2015;</ref><ref type=\"bibr\" target=\"#b12\">Jensen, 2019;</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 00 frequently appearing fragments that contain no more than 10 heavy atoms from the ChEMBL database <ref type=\"bibr\" target=\"#b6\">(Gaulton et al., 2017)</ref> by enumerating single bonds to break. As . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  different channels is not effectively utilized. We introduce the squeeze-and-excitation operations <ref type=\"bibr\" target=\"#b29\">[30]</ref> to learn the attention weights of different feature channe  and H \u00d7 W is the size of feature map, the c th element of z is calculated via Eq. (3) by following <ref type=\"bibr\" target=\"#b29\">[30]</ref>:</p><formula xml:id=\"formula_3\">Z c = F sq (u c ) = 1 H \u00d7  ntrinsically introduce dynamics conditioned on the input, helping to boost feature discriminability <ref type=\"bibr\" target=\"#b29\">[30]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  l i + W T 2 g i + b 1 + b 2 ) ) + b 3 )<label>(7)</label></formula><p>where \u03c3 1 refers to the ReLU <ref type=\"bibr\" target=\"#b33\">[34]</ref> function and \u03c3 2 is a sigmoid <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  l i + W T 2 g i + b 1 + b 2 ) ) + b 3 )<label>(7)</label></formula><p>where \u03c3 1 refers to the ReLU <ref type=\"bibr\" target=\"#b33\">[34]</ref> function and \u03c3 2 is a sigmoid <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" target=\"#b25\">[26]</ref><ref type=\"bibr\" target=\"#b26\">[27]</ref>. For example, Li . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings from the captured images, has been widely applied in many applications, such as urban planning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, geographic informatio ref><ref type=\"bibr\" target=\"#b22\">[23]</ref>. There are also a few attempts on building extraction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oach by combining the exponential linear unit (ELU) and conditional random fields (CRFs). Wu et al. <ref type=\"bibr\" target=\"#b28\">[29]</ref> presented a boundary regulated network called BR-Net for a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[17]</ref>, some researchers applied deep learning approaches to remote sensing segmentation tasks <ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref>, traditional methods consider spectra, shape, and texture  e (SVM), random forest (RF), or AdaBoost as the classifier <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref> to extract buildi s are determined as true or false positives according to the area of overlap with ground-truth. Eq. <ref type=\"bibr\" target=\"#b12\">(13)</ref> shows the definition of IoU for evaluating whether a detec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings from the captured images, has been widely applied in many applications, such as urban planning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, geographic informatio ref><ref type=\"bibr\" target=\"#b22\">[23]</ref>. There are also a few attempts on building extraction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><formula xml:id=\"formula_2\">)</formula><p>where F is the recurrent operation, \u03c3 refers to the ReLU <ref type=\"bibr\" target=\"#b31\">[32]</ref> function, and u is the output of l th RCL.</p><p>To tackle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s can better understand their users' preferences using the social network. Following the convention <ref type=\"bibr\" target=\"#b12\">[14]</ref>, we call this variant of SR sessionbased social recommenda are not suitable for SSR because they do not consider the sequential order of user behaviors. DGRec <ref type=\"bibr\" target=\"#b12\">[14]</ref> is currently the only method for SSR but it is not efficie n-based recommendation. Currently, the only method for session-based social recommendation is DGRec <ref type=\"bibr\" target=\"#b12\">[14]</ref>, which models dynamic user behaviors with an RNN and conte online bookmarking system where users can assign a variety of semantic tags to bookmarks. Following <ref type=\"bibr\" target=\"#b12\">[14]</ref>, we consider a sequence of tags with timestamps assigned t session recommender could be used for PSR. ( <ref type=\"formula\" target=\"#formula_9\">8</ref>) DGRec <ref type=\"bibr\" target=\"#b12\">[14]</ref> is the state-of-the-art method for SSR that captures users We did not include the methods for social recommendation because they are uncompetitive as shown in <ref type=\"bibr\" target=\"#b12\">[14]</ref>. Following <ref type=\"bibr\" target=\"#b0\">[2,</ref><ref typ wing previous studies <ref type=\"bibr\" target=\"#b5\">[7,</ref><ref type=\"bibr\" target=\"#b9\">11,</ref><ref type=\"bibr\" target=\"#b12\">14,</ref><ref type=\"bibr\" target=\"#b17\">19]</ref>, we embed the user  monly used in the literature of SR and social recommendation <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b12\">14,</ref><ref type=\"bibr\" target=\"#b17\">19]</ref>: (1) Gowalla <ref t composes the functions defined in Equations ( <ref type=\"formula\" target=\"#formula_11\">10</ref>) to <ref type=\"bibr\" target=\"#b12\">(14)</ref>. It is easy to verify that the complexity of \ud835\udc53 is \ud835\udc42 (|\ud835\udc41 \ud835\udc3f . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rs' interests are influenced by their friends and connected users tend to share similar preferences <ref type=\"bibr\" target=\"#b7\">[9,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" targ ious studies attempted to leverage social networks to improve the recommendation results. Ma et al. <ref type=\"bibr\" target=\"#b7\">[9]</ref> incorporated social networks into recommender systems by reg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b8\">[10]</ref> proposed a hierarchical RNN model to capture users' evolving interests. Wu et al. <ref type=\"bibr\" target=\"#b18\">[20]</ref> extended SR-GNN <ref type=\"bibr\" target=\"#b17\">[19]</ref>  any previous studies <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b13\">15,</ref><ref type=\"bibr\" target=\"#b18\">20]</ref>, we obtain the final session representation \ud835\udc94 from both \ud835\udc94 \ud835\udc43. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he test set. Following <ref type=\"bibr\" target=\"#b0\">[2,</ref><ref type=\"bibr\" target=\"#b5\">7,</ref><ref type=\"bibr\" target=\"#b6\">8,</ref><ref type=\"bibr\" target=\"#b9\">11,</ref><ref type=\"bibr\" target technique described in <ref type=\"bibr\" target=\"#b0\">[2,</ref><ref type=\"bibr\" target=\"#b5\">7,</ref><ref type=\"bibr\" target=\"#b6\">8,</ref><ref type=\"bibr\" target=\"#b17\">19]</ref>. Some statistics of t \" target=\"#b0\">[2,</ref><ref type=\"bibr\" target=\"#b2\">4,</ref><ref type=\"bibr\" target=\"#b5\">7,</ref><ref type=\"bibr\" target=\"#b6\">8,</ref><ref type=\"bibr\" target=\"#b17\">19,</ref><ref type=\"bibr\" targe s' main purposes and sequential behaviors. ( <ref type=\"formula\" target=\"#formula_5\">5</ref>) STAMP <ref type=\"bibr\" target=\"#b6\">[8]</ref> is an ASR model that applies the attention mechanism to bett hown in <ref type=\"bibr\" target=\"#b12\">[14]</ref>. Following <ref type=\"bibr\" target=\"#b0\">[2,</ref><ref type=\"bibr\" target=\"#b6\">8]</ref>, we applied grid search to find the optimal hyper-parameters . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b8\">[10]</ref> proposed a hierarchical RNN model to capture users' evolving interests. Wu et al. <ref type=\"bibr\" target=\"#b18\">[20]</ref> extended SR-GNN <ref type=\"bibr\" target=\"#b17\">[19]</ref>  any previous studies <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b13\">15,</ref><ref type=\"bibr\" target=\"#b18\">20]</ref>, we obtain the final session representation \ud835\udc94 from both \ud835\udc94 \ud835\udc43. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  be useful to predict the next item in many previous studies <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b13\">15,</ref><ref type=\"bibr\" target=\"#b18\">20]</ref>, we obtain the fina. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  similar preferences <ref type=\"bibr\" target=\"#b7\">[9,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b23\">25]</ref>. Therefore, the recommender systems can better understand t gularizing the latent user factors so that connected users have similar latent factors. Zhao et al. <ref type=\"bibr\" target=\"#b23\">[25]</ref> extracted additional training instances from the social ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nsfer learning to model user-item interactions and social relationships simultaneously. Wang et al. <ref type=\"bibr\" target=\"#b14\">[16]</ref> enhanced user modeling by integrating the knowledge from m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nsfer learning to model user-item interactions and social relationships simultaneously. Wang et al. <ref type=\"bibr\" target=\"#b14\">[16]</ref> enhanced user modeling by integrating the knowledge from m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  be useful to predict the next item in many previous studies <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b13\">15,</ref><ref type=\"bibr\" target=\"#b18\">20]</ref>, we obtain the fina. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nsfer learning to model user-item interactions and social relationships simultaneously. Wang et al. <ref type=\"bibr\" target=\"#b14\">[16]</ref> enhanced user modeling by integrating the knowledge from m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: des ELMo <ref type=\"bibr\" target=\"#b2\">[3]</ref>, GPT <ref type=\"bibr\" target=\"#b3\">[4]</ref>, BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ALBERT <ref type=\"bibr\" target=\"#b5\">[6]</ref> and other mo bibr\" target=\"#b5\">[6]</ref> and other models. Among the pre-training methods mentioned above, BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref> has the most outstanding performance. The model uses masked  improving text summarization tasks. Yang Liu <ref type=\"bibr\" target=\"#b19\">[20]</ref> applied BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref> to extractive summaries for the first time, the experimental. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: late the importance of each sentence, and then select the sentence to form a summary. Shashi et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> performed global optimization of ROUGE <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: training data is less, usually pre-training and fine-tuning methods can achieve outstanding results <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In recent years, we have witnessed the impressive results o lassified into feature-based models and fine-tuning-based models according to their characteristics <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Feature-based methods mainly use pre-training models to pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  neural networks to solve it. Nallapati <ref type=\"bibr\" target=\"#b23\">[24]</ref> and Chopra et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> used RNN to replace traditional the encoder and decoder an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ration model mainly used human feature engineering, and its common methods include context matching <ref type=\"bibr\" target=\"#b12\">[13]</ref>, graph model <ref type=\"bibr\" target=\"#b13\">[14]</ref>, bu </ref> are seq2seq models based on RNN, without and with attention mechanism, respectively. CopyNet <ref type=\"bibr\" target=\"#b12\">[13]</ref> is a seq2seq model that uses both the copy mechanism and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  its common methods include context matching <ref type=\"bibr\" target=\"#b12\">[13]</ref>, graph model <ref type=\"bibr\" target=\"#b13\">[14]</ref>, but the model effect is not very satisfactory. With the a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ary. Shashi et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> performed global optimization of ROUGE <ref type=\"bibr\" target=\"#b17\">[18]</ref> metrics through reinforcement learning, conceptualized ext ion indicator. ROUGE is a text summary automatic evaluation method proposed by Chin-yew Lin in 2004 <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We use the standard F1 scores of ROUGE-1, ROUGE-2 and ROU. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erts manually create a number of hard templates, and then use templates to guide summary generation <ref type=\"bibr\" target=\"#b8\">[9]</ref>. However, it is unrealistic to create all templates manually. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: results under different topic data by classifying the training set data according to the topic. CGU <ref type=\"bibr\" target=\"#b33\">[34]</ref> is a seq2seq model base on the convolutional gated unit an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el training process, which may lead to summary length and excessive redundant information. The SUMO <ref type=\"bibr\" target=\"#b18\">[19]</ref> model proposed an end-to-end extractive text summary gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et. LCSTS is a public Chinese short text summary generation data set constructed by Hu Chen in 2015 <ref type=\"bibr\" target=\"#b29\">[30]</ref>, which contains 2.4 million real Chinese short text data a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b32\">(Toma 2010;</ref><ref type=\"bibr\" target=\"#b41\">Zhao and Jiang 2011;</ref><ref type=\"bibr\" target=\"#b33\">Tominaga and Hijikata 2015)</ref>. Because the sample sizes are so sm ove less accurate in the case of new users <ref type=\"bibr\" target=\"#b4\">(Bergsma et al. 2013;</ref><ref type=\"bibr\" target=\"#b33\">Tominaga and Hijikata 2015)</ref>. Increased data sources that includ f>, gender <ref type=\"bibr\" target=\"#b9\">(Filho et al. 2016)</ref>, tweets, followers and followees <ref type=\"bibr\" target=\"#b33\">(Tominaga and Hijikata 2015)</ref> are relevant to the creation of us  with different cultural backgrounds might select different types of avatars. Tominaga and Hijikata <ref type=\"bibr\" target=\"#b33\">(Tominaga and Hijikata 2015)</ref> collected the data from 300 users   partition scheme</head><p>A user partition scheme developed from the work of Tominaga and Hijikata <ref type=\"bibr\" target=\"#b33\">(Tominaga and Hijikata 2015)</ref>, Lim et al. <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r exceeded expectations, as it challenged the findings from a survey conducted by Japanese scholars <ref type=\"bibr\" target=\"#b17\">(Keliyan 2011)</ref>. Their survey identified more male than female u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s of gender on an SNS, the selection may be influenced by users' physiological sex, gender identity <ref type=\"bibr\" target=\"#b8\">(Egan and Perry 2001)</ref>, privacy awareness <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  an avatar partition scheme with more detail and additional dimensions (e.g. text-based information <ref type=\"bibr\" target=\"#b15\">(Huang et al. 2012)</ref>) could be introduced to make the dataset su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er modelling (Martinez-Villaseor, Gonzalez-Mendoza, and Hernandez-Gress 2012), behaviour prediction <ref type=\"bibr\" target=\"#b27\">(Shan et al. 2020)</ref>, recommendation <ref type=\"bibr\" target=\"#b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b22\">Lin et al. 2018)</ref>, etc., with different language environment <ref type=\"bibr\" target=\"#b2\">(Bahaddad et al. 2018</ref>).</p></div><figure xmlns=\"http://www.tei-c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dy could help to establish better personalised recommendation model as well as user detection model <ref type=\"bibr\" target=\"#b26\">(Senadheera, Warren, and Leitch 2017;</ref><ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  tweets in the dataset that was manually labelled Logistic regression, support vector machine (SVM) <ref type=\"bibr\" target=\"#b6\">(Chang and Lin 2011)</ref>, random forest (RF) <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2012), behaviour prediction <ref type=\"bibr\" target=\"#b27\">(Shan et al. 2020)</ref>, recommendation <ref type=\"bibr\" target=\"#b43\">(Zhu et al. 2020;</ref><ref type=\"bibr\" target=\"#b22\">Lin et al. 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t vector machine (SVM) <ref type=\"bibr\" target=\"#b6\">(Chang and Lin 2011)</ref>, random forest (RF) <ref type=\"bibr\" target=\"#b5\">(Breiman 2001</ref>) and gradient boosting decision tree (GBDT) <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lowers) to the number of accounts the user followed. FF scores are often used to evaluate influence <ref type=\"bibr\" target=\"#b20\">(Li et al. 2016b</ref>) and to detect spam <ref type=\"bibr\">(Xu, Sun,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  semantic traces and patterns derived from large, generalpurpose knowledge ontologies (e.g. DBPedia <ref type=\"bibr\" target=\"#b13\">[3]</ref>); (ii) formal logic rules; and (iii) attributed relations f help predict and justify the veracity of the facts under consideration. We use the DBPedia ontology <ref type=\"bibr\" target=\"#b13\">[3]</ref> (and its associated Wikipedia text corpus) as the knowledge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: their dimensionality 1</p><p>\u221a to prevent small gradient values due to large dot product magnitudes <ref type=\"bibr\" target=\"#b69\">[59]</ref>. denotes the concatenation of independent attention heads . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ce Metrics: We evaluate Task I using classification accuracy and F1-score. For Task II, we use BLEU <ref type=\"bibr\" target=\"#b44\">[34]</ref>, METEOR <ref type=\"bibr\" target=\"#b16\">[6]</ref>, ROUGE <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b46\">36,</ref><ref type=\"bibr\" target=\"#b55\">[45]</ref><ref type=\"bibr\" target=\"#b56\">[46]</ref><ref type=\"bibr\" target=\"#b57\">[47]</ref><ref type=\"bibr\" t et=\"#b23\">[13,</ref><ref type=\"bibr\" target=\"#b26\">16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b56\">46]</ref>. However, explanations formulated in this fashion are often get=\"#b26\">16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b36\">26,</ref><ref type=\"bibr\" target=\"#b56\">46]</ref>. To the best of our knowledge, this is the first attempt in lain fact veracity <ref type=\"bibr\" target=\"#b26\">[16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b56\">46]</ref>. Extractive explainable fact checking approaches <ref type= get=\"#b26\">16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b36\">26,</ref><ref type=\"bibr\" target=\"#b56\">46]</ref> (e.g baseline FACE-KEG-linear enc. in Section 4). We find t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b45\">35,</ref><ref type=\"bibr\" target=\"#b75\">65]</ref>, multimodally using images <ref type=\"bibr\" target=\"#b62\">[52]</ref>, via reasoning from relational tables <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b46\">36,</ref><ref type=\"bibr\" target=\"#b55\">[45]</ref><ref type=\"bibr\" target=\"#b56\">[46]</ref><ref type=\"bibr\" target=\"#b57\">[47]</ref><ref type=\"bibr\" target=\"#b66\">56]</ref>. However, most app get=\"#b15\">[5,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b57\">47,</ref><ref type=\"bibr\" target=\"#b81\">71]</ref>. Though such eviden >46]</ref>. Extractive explainable fact checking approaches <ref type=\"bibr\" target=\"#b12\">[2,</ref><ref type=\"bibr\" target=\"#b57\">47,</ref><ref type=\"bibr\" target=\"#b76\">66]</ref>. also take addition. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b55\">45,</ref><ref type=\"bibr\" target=\"#b66\">56,</ref><ref type=\"bibr\" target=\"#b67\">57,</ref><ref type=\"bibr\" target=\"#b81\">71]</ref>. It involves verifying if an input claim is supported or re get=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b57\">47,</ref><ref type=\"bibr\" target=\"#b81\">71]</ref>. Though such evidence can serve as an explanation, it is of get=\"#b46\">36,</ref><ref type=\"bibr\" target=\"#b55\">45,</ref><ref type=\"bibr\" target=\"#b66\">56,</ref><ref type=\"bibr\" target=\"#b81\">71]</ref> without justifying their decision. Fact verification aims t eracity of a claim <ref type=\"bibr\" target=\"#b32\">[22,</ref><ref type=\"bibr\" target=\"#b67\">57,</ref><ref type=\"bibr\" target=\"#b81\">71]</ref>. Many techniques <ref type=\"bibr\" target=\"#b20\">[10,</ref>< get=\"#b40\">30,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b78\">68,</ref><ref type=\"bibr\" target=\"#b81\">71]</ref> have been proposed to solve the popular FEVER shared task f o <ref type=\"bibr\" target=\"#b40\">[30]</ref>, DOMLIN <ref type=\"bibr\" target=\"#b61\">[51]</ref>, GEAR <ref type=\"bibr\" target=\"#b81\">[71]</ref>, KGAT <ref type=\"bibr\" target=\"#b38\">[28]</ref>. For Task  f Table <ref type=\"table\">2</ref>). The proposed FACE-KEG (last row) and the recently proposed GEAR <ref type=\"bibr\" target=\"#b81\">[71]</ref> and KGAT <ref type=\"bibr\" target=\"#b38\">[28]</ref> are com. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce Metrics: We evaluate Task I using classification accuracy and F1-score. For Task II, we use BLEU <ref type=\"bibr\" target=\"#b44\">[34]</ref>, METEOR <ref type=\"bibr\" target=\"#b16\">[6]</ref>, ROUGE <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n accuracy and F1-score. For Task II, we use BLEU <ref type=\"bibr\" target=\"#b44\">[34]</ref>, METEOR <ref type=\"bibr\" target=\"#b16\">[6]</ref>, ROUGE <ref type=\"bibr\" target=\"#b37\">[27]</ref>, Entity Ov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: intermediate size of 2000 units. The attention and FFN blocks are stacked = 6 times. We use dropout <ref type=\"bibr\" target=\"#b60\">[50]</ref> with a probability of 0.3 in the self attention layers, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of the attention layers are set to 500. In the graph transformer encoder, the FFN block has a PReLU <ref type=\"bibr\" target=\"#b33\">[23]</ref> activation function with an intermediate size of 2000 unit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tural language applications such as language understanding <ref type=\"bibr\" target=\"#b51\">[41,</ref><ref type=\"bibr\" target=\"#b70\">60]</ref>, knowledge graph completion <ref type=\"bibr\" target=\"#b27\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ransformer networks <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25,</ref><ref type=\"bibr\" target=\"#b79\">69]</ref> have been proposed to directly encode graph inputs. In this -KEG-GTN enc.: replaces our proposed graph encoder with a different graph transformer network (GTN) <ref type=\"bibr\" target=\"#b79\">[69]</ref>, that learns multiple meta-path graphs from the input grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s not restricted to small-scale domain-specific information <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25]</ref>; and (ii) leverages the rich graphical structure of linked  mploys a large scale knowledge base with generic entities, concepts and relationships; some studies <ref type=\"bibr\" target=\"#b35\">[25,</ref><ref type=\"bibr\" target=\"#b80\">70]</ref> utilize small, dom 59\">49]</ref> and more recently, graph transformer networks <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25,</ref><ref type=\"bibr\" target=\"#b79\">69]</ref> have been proposed  ype=\"bibr\" target=\"#b17\">[7]</ref>: encodes the input knowledge graph via a GNN.</p><p>(2) KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref>: their graph transformer encoder and knowledge graph const  FALSE: jacques cl\u00e9ment succeeded the throne catholic and become king of house of bourbon. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: in 1692, jacques cl\u00e9ment, a catholic tribe historian, ve been described to treat in memory caused by discrimination or other forms of phonation. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A:dissociative identity disorder did suffer from mental  economic plan by Labor Chris Bowen is the most comprehensive plan by an opposition leader. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: Economic history of Australia shows that 15 pages bud ields the earth from the sun's ultraviolet radiation, healing positive effect of lockdown. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: The Arctic ozone depletion has nothing to do with the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">[5,</ref><ref type=\"bibr\" target=\"#b26\">16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b46\">36,</ref><ref type=\"bibr\" tar  fact verification <ref type=\"bibr\" target=\"#b23\">[13,</ref><ref type=\"bibr\" target=\"#b32\">22,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b55\">45,</ref><ref type=\"bibr\" tar d promising results <ref type=\"bibr\" target=\"#b15\">[5,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b57\">47,</ref><ref type=\"bibr\" tar im is true or false <ref type=\"bibr\" target=\"#b15\">[5,</ref><ref type=\"bibr\" target=\"#b32\">22,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b46\">36,</ref><ref type=\"bibr\" tar get=\"#b32\">22,</ref><ref type=\"bibr\" target=\"#b38\">28,</ref><ref type=\"bibr\" target=\"#b40\">30,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b78\">68,</ref><ref type=\"bibr\" tar studied from several perspectives over the last few years, e.g. as natural language inference (NLI) <ref type=\"bibr\" target=\"#b43\">[33,</ref><ref type=\"bibr\" target=\"#b45\">35,</ref><ref type=\"bibr\" ta ef>;</p><p>(2) FEVER leader board methods: Athene <ref type=\"bibr\" target=\"#b32\">[22]</ref>, UCL MR <ref type=\"bibr\" target=\"#b43\">[33]</ref>, UNC <ref type=\"bibr\" target=\"#b78\">[68]</ref>, Papelo <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tural language applications such as language understanding <ref type=\"bibr\" target=\"#b51\">[41,</ref><ref type=\"bibr\" target=\"#b70\">60]</ref>, knowledge graph completion <ref type=\"bibr\" target=\"#b27\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n accuracy and F1-score. For Task II, we use BLEU <ref type=\"bibr\" target=\"#b44\">[34]</ref>, METEOR <ref type=\"bibr\" target=\"#b16\">[6]</ref>, ROUGE <ref type=\"bibr\" target=\"#b37\">[27]</ref>, Entity Ov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  based on the trustworthiness of the authors of the claims <ref type=\"bibr\" target=\"#b29\">[19,</ref><ref type=\"bibr\" target=\"#b58\">48,</ref><ref type=\"bibr\" target=\"#b73\">63]</ref>. Fact checking webs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eral-purpose knowledge bases, that (i) is not restricted to small-scale domain-specific information <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25]</ref>; and (ii) leverages 17\">[7,</ref><ref type=\"bibr\" target=\"#b59\">49]</ref> and more recently, graph transformer networks <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25,</ref><ref type=\"bibr\" tar propose a graph transformer network comparable to the graph-to-sequence model proposed by Cai et al <ref type=\"bibr\" target=\"#b19\">[9]</ref> (baseline CL'20 in Section 4). However, our work differs in phs only contain entity-relation edges, and no direct connections between entities.</p><p>(3) CL'20 <ref type=\"bibr\" target=\"#b19\">[9]</ref> and (4) YWW'20 <ref type=\"bibr\" target=\"#b77\">[67]</ref>: b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s not restricted to small-scale domain-specific information <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25]</ref>; and (ii) leverages the rich graphical structure of linked  mploys a large scale knowledge base with generic entities, concepts and relationships; some studies <ref type=\"bibr\" target=\"#b35\">[25,</ref><ref type=\"bibr\" target=\"#b80\">70]</ref> utilize small, dom 59\">49]</ref> and more recently, graph transformer networks <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25,</ref><ref type=\"bibr\" target=\"#b79\">69]</ref> have been proposed  ype=\"bibr\" target=\"#b17\">[7]</ref>: encodes the input knowledge graph via a GNN.</p><p>(2) KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref>: their graph transformer encoder and knowledge graph const  FALSE: jacques cl\u00e9ment succeeded the throne catholic and become king of house of bourbon. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: in 1692, jacques cl\u00e9ment, a catholic tribe historian, ve been described to treat in memory caused by discrimination or other forms of phonation. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A:dissociative identity disorder did suffer from mental  economic plan by Labor Chris Bowen is the most comprehensive plan by an opposition leader. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: Economic history of Australia shows that 15 pages bud ields the earth from the sun's ultraviolet radiation, healing positive effect of lockdown. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: The Arctic ozone depletion has nothing to do with the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  solve the popular FEVER shared task for fact verification <ref type=\"bibr\" target=\"#b67\">[57,</ref><ref type=\"bibr\" target=\"#b68\">58]</ref>. However unlike these methods, FACE-KEG does not take as in  datasets described in Table 1: (i) FEVER shared task data <ref type=\"bibr\" target=\"#b67\">[57,</ref><ref type=\"bibr\" target=\"#b68\">58]</ref> domains <ref type=\"bibr\" target=\"#b14\">[4]</ref>; and (iii) re FACE-KEG with:</p><p>(1) The FEVER shared task baseline <ref type=\"bibr\" target=\"#b67\">[57,</ref><ref type=\"bibr\" target=\"#b68\">58]</ref>;</p><p>(2) FEVER leader board methods: Athene <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion of instruction misses uncovered, and hence, there is a substantial opportunity for improvement <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b29\">32,</ref><ref type=\"bibr\" ta act that the sequence of instruction cache accesses or misses is repetitive, and hence, predictable <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b19\">22]</ref>. Consequently, tem it footprint, and a 12-bit pointer to the successor. The 8-bit footprint is derived from prior work <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" ta dvantage of the stream address buffer (SAB), which is previously used by prior temporal prefetchers <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" ta e lookahead is five, and four SABs are used where each one tracks seven consecutive spatial regions <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28]</ref>.</p><p>MANA: MANA  ng twelve entries. Prior work has shown that this configuration successfully exploits the potential <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28]</ref>. Moreover, MANA_Ta o have a dedicated bit in the footprint. Some pieces of prior work have used (2, 6) spatial regions <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b29\">32,</ref><ref type=\"bibr\" ta  that showed a hardware instruction prefetcher could eliminate most of the instruction cache misses <ref type=\"bibr\" target=\"#b18\">[21]</ref>. However, the proposed prefetcher is impractical because o ]</ref> records and replays the sequence of misses and offers adequately good results. However, PIF <ref type=\"bibr\" target=\"#b18\">[21]</ref> offers a more significant improvement as compared to TIFS  B comes from the prefetch buffers, and the rest is because of the changes made to the BTB. PIF: PIF <ref type=\"bibr\" target=\"#b18\">[21]</ref> records the sequence of spatial regions in a circular hist tly prefetched spatial regions. Prior work has suggested using four SABs that each one tracks seven <ref type=\"bibr\" target=\"#b18\">[21]</ref> or twelve <ref type=\"bibr\" target=\"#b25\">[28]</ref> spatia hers showed that recently-accessed addresses tend to recur <ref type=\"bibr\" target=\"#b16\">[19,</ref><ref type=\"bibr\" target=\"#b18\">21,</ref><ref type=\"bibr\" target=\"#b19\">22,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mporal prefetchers <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" target=\"#b26\">29]</ref>, to prefetch for the L1-I cache. SAB is a fixed-length sequ problem is twisted to BTB prefilling problem to offer a unified solution to the frontend bottleneck <ref type=\"bibr\" target=\"#b26\">[29,</ref><ref type=\"bibr\" target=\"#b30\">33,</ref><ref type=\"bibr\" ta ne program-profiling to choose where the prefetching instructions should be added.</p><p>Confluence <ref type=\"bibr\" target=\"#b26\">[29]</ref> is the first proposal that offers a unified solution to ad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b41\">44,</ref><ref type=\"bibr\" target=\"#b42\">45,</ref><ref type=\"bibr\" target=\"#b46\">49,</ref><ref type=\"bibr\" target=\"#b49\">52]</ref>.</p><p>Discontinuity prefetcher <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chers to develop effective and storage-efficient prefetchers <ref type=\"bibr\" target=\"#b5\">[8,</ref><ref type=\"bibr\" target=\"#b23\">26,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" tar  into the program code <ref type=\"bibr\" target=\"#b2\">[5,</ref><ref type=\"bibr\" target=\"#b5\">8,</ref><ref type=\"bibr\" target=\"#b23\">26,</ref><ref type=\"bibr\" target=\"#b34\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b23\">26,</ref><ref type=\"bibr\" target=\"#b34\">37,</ref><ref type=\"bibr\" target=\"#b35\">38,</ref><ref type=\"bibr\" target=\"#b50\">53]</ref>. These proposals use an offline or online program-profiling. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: efetchers are advertised as metadata-free prefetchers. Fetch Directed Instruction Prefetcher (FDIP) <ref type=\"bibr\" target=\"#b41\">[44]</ref> is the pioneer of such prefetchers. The main idea is to de get=\"#b30\">33,</ref><ref type=\"bibr\" target=\"#b31\">34,</ref><ref type=\"bibr\" target=\"#b37\">40,</ref><ref type=\"bibr\" target=\"#b41\">44,</ref><ref type=\"bibr\" target=\"#b42\">45,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ache block addresses.</p><p>Code-layout optimization is another way to tackle the L1-I miss problem <ref type=\"bibr\" target=\"#b36\">[39,</ref><ref type=\"bibr\" target=\"#b38\">41]</ref>. In these techniqu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b0\">[3,</ref><ref type=\"bibr\" target=\"#b1\">4,</ref><ref type=\"bibr\" target=\"#b5\">8,</ref><ref type=\"bibr\" target=\"#b14\">17,</ref><ref type=\"bibr\" target=\"#b21\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re sequential prefetchers that, upon activation, send prefetch requests for a few subsequent blocks <ref type=\"bibr\" target=\"#b44\">[47,</ref><ref type=\"bibr\" target=\"#b53\">56]</ref>. While sequential  et=\"#b22\">[25,</ref><ref type=\"bibr\" target=\"#b39\">42,</ref><ref type=\"bibr\" target=\"#b43\">46,</ref><ref type=\"bibr\" target=\"#b44\">47]</ref>, prior work has shown that such prefetchers leave a signifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b0\">[3,</ref><ref type=\"bibr\" target=\"#b1\">4,</ref><ref type=\"bibr\" target=\"#b5\">8,</ref><ref type=\"bibr\" target=\"#b14\">17,</ref><ref type=\"bibr\" target=\"#b21\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ache block addresses.</p><p>Code-layout optimization is another way to tackle the L1-I miss problem <ref type=\"bibr\" target=\"#b36\">[39,</ref><ref type=\"bibr\" target=\"#b38\">41]</ref>. In these techniqu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arned the fine-grained multimodal representations, inspired by the knowledge distillation technique <ref type=\"bibr\" target=\"#b24\">(Romero et al. 2015)</ref>, we further introduce the Multimodal Knowl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: res before predicting answers. In particular, inspired by the success of Multi-Head Attention (MHA) <ref type=\"bibr\" target=\"#b28\">(Vaswani et al. 2017)</ref>, we refer to the MHA mechanism and propos  and inter-modality of audio features and textual features, we adopt the Multi-Head Attention (MHA) <ref type=\"bibr\" target=\"#b28\">(Vaswani et al. 2017)</ref>, which compute the association weights be res P to summarize the properties of the audio features A.</p><p>According to the attention theorem <ref type=\"bibr\" target=\"#b28\">(Vaswani et al. 2017</ref>), taking the first situation as example, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b13\">(Lai et al. 2017</ref>) and the end-to-end spoken language understanding (audio) <ref type=\"bibr\" target=\"#b25\">(Serdyuk et al. 2018)</ref>, it is necessary to enable the multimodal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  an effective fusion of multimodality features <ref type=\"bibr\" target=\"#b18\">(Lu et al. 2016;</ref><ref type=\"bibr\" target=\"#b5\">Gao et al. 2019;</ref><ref type=\"bibr\" target=\"#b13\">Liu et al. 2019a<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sion (MC), which aims to teach the machine to answer questions after giving comprehension materials <ref type=\"bibr\" target=\"#b19\">(Nguyen et al. 2016;</ref><ref type=\"bibr\" target=\"#b23\">Rajpurkar et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  an effective fusion of multimodality features <ref type=\"bibr\" target=\"#b18\">(Lu et al. 2016;</ref><ref type=\"bibr\" target=\"#b5\">Gao et al. 2019;</ref><ref type=\"bibr\" target=\"#b13\">Liu et al. 2019a<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sion (MC), which aims to teach the machine to answer questions after giving comprehension materials <ref type=\"bibr\" target=\"#b19\">(Nguyen et al. 2016;</ref><ref type=\"bibr\" target=\"#b23\">Rajpurkar et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct with only audio or only text input. And there are also some related work studied different tasks <ref type=\"bibr\" target=\"#b31\">(Yang et al. 2003)</ref> or auxiliary tool <ref type=\"bibr\">(Zhang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b13\">(Lai et al. 2017</ref>) and the end-to-end spoken language understanding (audio) <ref type=\"bibr\" target=\"#b25\">(Serdyuk et al. 2018)</ref>, it is necessary to enable the multimodal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  an effective fusion of multimodality features <ref type=\"bibr\" target=\"#b18\">(Lu et al. 2016;</ref><ref type=\"bibr\" target=\"#b5\">Gao et al. 2019;</ref><ref type=\"bibr\" target=\"#b13\">Liu et al. 2019a<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al. (2016)</ref> proposed to deal with the MC task of spoken content by first employ an ASR model <ref type=\"bibr\" target=\"#b32\">(Yu and Li 2017)</ref> to recognize speech into text, then, an MC mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b29\">(Wang et al. 2018;</ref><ref type=\"bibr\" target=\"#b4\">Dhingra et al. 2017;</ref><ref type=\"bibr\" target=\"#b3\">Devlin et al. 2019</ref>) normally include a text encoder and an answe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e system is still a text-based MC system. <ref type=\"bibr\" target=\"#b2\">Chuang et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b12\">Kuo, Luo, and Chen (2020)</ref> studied the end-to-end spoken questio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nter ))</formula><p>It is worth noticing that to better promote the learning of intra-relationships <ref type=\"bibr\" target=\"#b9\">(Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al. 2018)<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Lai et al. 2017)</ref>, span extraction <ref type=\"bibr\" target=\"#b23\">(Rajpurkar et al. 2016;</ref><ref type=\"bibr\" target=\"#b22\">Rajpurkar, Jia, and Liang 2018)</ref>, and generative format <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  based on the given questions Q and candidate choices C candidate . Some well-performing frameworks <ref type=\"bibr\" target=\"#b29\">(Wang et al. 2018;</ref><ref type=\"bibr\" target=\"#b4\">Dhingra et al.  nning 2014)</ref>, and represented as P \u2208 R N \u00d7d (d = 300). The answer predictor, e.g., co-matching <ref type=\"bibr\" target=\"#b29\">(Wang et al. 2018)</ref>, is used to predict the correct choice C ans  MC systems, i.e., GA Reader <ref type=\"bibr\" target=\"#b4\">(Dhingra et al. 2017)</ref>, Co-Matching <ref type=\"bibr\" target=\"#b29\">(Wang et al. 2018)</ref>, and DCMN <ref type=\"bibr\" target=\"#b33\">(Zh unimodal model, the \"w/ MKD\" model, and the proposed multimodal model. We use the co-matching model <ref type=\"bibr\" target=\"#b29\">(Wang et al. 2018)</ref> as the predictor. Table <ref type=\"table\" ta lation. The predictor is adapted from the existing machine comprehension models, such as Co-Matching<ref type=\"bibr\" target=\"#b29\">(Wang et al. 2018)</ref>.</figDesc></figure> <figure xmlns=\"http://ww d>Table 1 :</head><label>1</label><figDesc><ref type=\"bibr\" target=\"#b4\">Dhingra et al. 2017)</ref> <ref type=\"bibr\" target=\"#b29\">(Wang et al. 2018)</ref> <ref type=\"bibr\" target=\"#b33\">(Zhang et al. \" target=\"#b33\">(Zhang et al. 2019)</ref> <ref type=\"bibr\" target=\"#b4\">(Dhingra et al. 2017)</ref> <ref type=\"bibr\" target=\"#b29\">(Wang et al. 2018)</ref> <ref type=\"bibr\" target=\"#b33\">(Zhang et al. rovement over the model under the conventional setting. The models are explored with the Co-Matching<ref type=\"bibr\" target=\"#b29\">(Wang et al. 2018)</ref> as the predictor.</figDesc><table><row><cell. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e system is still a text-based MC system. <ref type=\"bibr\" target=\"#b2\">Chuang et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b12\">Kuo, Luo, and Chen (2020)</ref> studied the end-to-end spoken questio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct with only audio or only text input. And there are also some related work studied different tasks <ref type=\"bibr\" target=\"#b31\">(Yang et al. 2003)</ref> or auxiliary tool <ref type=\"bibr\">(Zhang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ter promote the learning of intra-relationships <ref type=\"bibr\" target=\"#b9\">(Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al. 2018)</ref>, we further design a conditional gate operatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref> of dropout <ref type=\"bibr\" target=\"#b26\">(Srivastava et al. 2014)</ref>, shortcut connection <ref type=\"bibr\" target=\"#b7\">(He et al. 2016)</ref>, and layer normalization <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Recent advances in optimization-based approaches have evoked more interests in meta-learning. MAML <ref type=\"bibr\" target=\"#b3\">(Finn, Abbeel, and Levine 2017</ref>) is a general optimization algori separately, we effectively integrate all networks in a modified Model-Agnostic Meta-Learning (MAML) <ref type=\"bibr\" target=\"#b3\">(Finn, Abbeel, and Levine 2017)</ref> framework. An overview of our le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ew training samples. Previous methods studied meta-learning from the perspective of metric learning <ref type=\"bibr\" target=\"#b14\">(Koch 2015;</ref><ref type=\"bibr\" target=\"#b44\">Vinyals et al. 2016;< is non-trivial to train a reconstruction network from limited modalitycomplete samples. Inspired by <ref type=\"bibr\" target=\"#b14\">(Kuo et al. 2019)</ref>, we approximate the missing modality using a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ta-learning from the perspective of metric learning <ref type=\"bibr\" target=\"#b14\">(Koch 2015;</ref><ref type=\"bibr\" target=\"#b44\">Vinyals et al. 2016;</ref><ref type=\"bibr\" target=\"#b36\">Sung et al. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b13\">(Kingma and Welling 2013)</ref>, or Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b6\">(Goodfellow et al. 2014)</ref>, since they often require a significant \" target=\"#b40\">(Tran et al. 2017;</ref><ref type=\"bibr\" target=\"#b19\">Lee et al. 2019)</ref>, GANs <ref type=\"bibr\" target=\"#b6\">(Goodfellow et al. 2014)</ref>, and VAEs <ref type=\"bibr\" target=\"#b13. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us on how to fuse multimodal data effectively <ref type=\"bibr\" target=\"#b22\">(Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b47\">Zadeh et al. 2017a</ref>) and how to learn a good representation for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us on how to fuse multimodal data effectively <ref type=\"bibr\" target=\"#b22\">(Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b47\">Zadeh et al. 2017a</ref>) and how to learn a good representation for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs M which can be clustered among all modalitycomplete samples using K-means (MacQueen 1967) or PCA <ref type=\"bibr\" target=\"#b28\">(Pearson 1901)</ref>. Specifically, instead of generating \u03c9 = f \u03c6c (x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs M which can be clustered among all modalitycomplete samples using K-means (MacQueen 1967) or PCA <ref type=\"bibr\" target=\"#b28\">(Pearson 1901)</ref>. Specifically, instead of generating \u03c9 = f \u03c6c (x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: proaches learn the joint distribution of multimodal data. Multimodal variational autoencoder (MVAE) <ref type=\"bibr\" target=\"#b46\">(Wu and Goodman 2018</ref>) models the joint posterior as a product-o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kenel 2017)</ref>, robotics <ref type=\"bibr\" target=\"#b26\">(Noda et al. 2014)</ref>, and healthcare <ref type=\"bibr\" target=\"#b5\">(Frantzidis et al. 2010)</ref>. Generally speaking, existing research . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6 movies and 23 classes. We follow the training and validation splits provided in the previous work <ref type=\"bibr\" target=\"#b43\">(Vielzeuf et al. 2018)</ref>  <ref type=\"bibr\" target=\"#b42\">(Tzaneta  a learning rate of 10 \u22124 for inner-loop and 10 \u22123 fro outer-loop. Besides, we follow previous work <ref type=\"bibr\" target=\"#b43\">(Vielzeuf et al. 2018)</ref> to add a weight of 2.0 on the positive l  MM-IMDb dataset, we follow previous works <ref type=\"bibr\" target=\"#b0\">(Arevalo et al. 2017;</ref><ref type=\"bibr\" target=\"#b43\">Vielzeuf et al. 2018</ref>) by adopting the F1 Samples and F1 Micro t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs M which can be clustered among all modalitycomplete samples using K-means (MacQueen 1967) or PCA <ref type=\"bibr\" target=\"#b28\">(Pearson 1901)</ref>. Specifically, instead of generating \u03c9 = f \u03c6c (x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs M which can be clustered among all modalitycomplete samples using K-means (MacQueen 1967) or PCA <ref type=\"bibr\" target=\"#b28\">(Pearson 1901)</ref>. Specifically, instead of generating \u03c9 = f \u03c6c (x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: her improved the computation efficiency of MAML. Other works adapted MAML for domain generalization <ref type=\"bibr\" target=\"#b20\">(Li et al. 2018;</ref><ref type=\"bibr\" target=\"#b32\">Qiao, Zhao, and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ta-learning from the perspective of metric learning <ref type=\"bibr\" target=\"#b14\">(Koch 2015;</ref><ref type=\"bibr\" target=\"#b44\">Vinyals et al. 2016;</ref><ref type=\"bibr\" target=\"#b36\">Sung et al. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  has been widely adopted in previous studies <ref type=\"bibr\" target=\"#b45\">(Wang et al. 2017;</ref><ref type=\"bibr\" target=\"#b31\">Poria et al. 2016</ref> Recently, there have been a wide range of res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b47\">Zadeh et al. 2017a</ref>) and how to learn a good representation for each modality <ref type=\"bibr\" target=\"#b38\">(Tian, Krishnan, and Isola 2020)</ref>.</p><p>A common assumption und. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  has been widely adopted in previous studies <ref type=\"bibr\" target=\"#b45\">(Wang et al. 2017;</ref><ref type=\"bibr\" target=\"#b31\">Poria et al. 2016</ref> Recently, there have been a wide range of res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  added GRU units after convolutional layers to further boost the representation power of the model. <ref type=\"bibr\" target=\"#b8\">Guo et al. (2020)</ref> set the CNN and LSTM networks in parallel to c would directly affect the subsequent protein folding and tertiary structure prediction. Recent work <ref type=\"bibr\" target=\"#b8\">Guo et al. (2020)</ref> exploited the \"Bagging\" mechanism to obtain th otein sequences with low-quality PSSM into several divisions of MSA count and MSA meff according to <ref type=\"bibr\" target=\"#b8\">Guo et al. (2020)</ref>. Shown in Table <ref type=\"table\">2</ref> and  uality PSSM Enhancement. Since MSA and PSSM are critical for protein property prediction, \"Bagging\" <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020)</ref> is the first attempt to enhance the lowquality s=\"http://www.tei-c.org/ns/1.0\"><head>Loss Function</head><p>Additionally, same as in previous work <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020)</ref>, we use the mean square error (MSE) loss to di SSM in the training set, i.e., calculate the frequency of MSA count when the MSA count less than 60 <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020)</ref>. Once the prior distribution of native low-qua g via contrastive learning.</p><p>Contrastive Learning. Contrastive loss was introduced by Hadsell, <ref type=\"bibr\" target=\"#b8\">Chopra, and LeCun (2006)</ref> to learn representation by contrasting  ampling operation based on prior statistics instead of the fixed downsample ratio used in \"Bagging\" <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020</ref>) which details will be given in the experiment  er the vanilla model denoted as \"Real\" and surpasses the previous state-of-the-art method \"Bagging\" <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020</ref>) by 6% on average and nearly 8% in the extreme . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b5\">(Chen et al. 2017;</ref><ref type=\"bibr\" target=\"#b36\">Yim et al. 2017;</ref><ref type=\"bibr\" target=\"#b37\">Yu et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Schmitt et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ertiary structure (3D) prediction, i.e., X-ray crystallography and nuclear magnetic resonance (NMR) <ref type=\"bibr\" target=\"#b34\">(Wuthrich 1989</ref>), cryo-EM based methods <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b14\">(Li and Yu 2016;</ref><ref type=\"bibr\" target=\"#b33\">Wang et al. 2016;</ref><ref type=\"bibr\" target=\"#b38\">Zhou and Troyanskaya 2014)</ref> achieved satisfactory PSSP performan high-quality PSSM along with one-hot amino acid sequence as evolutionary information. Specifically, <ref type=\"bibr\" target=\"#b38\">Zhou and Troyanskaya (2014)</ref> used a deep convolutional network t 16\">Misra and Maaten 2020;</ref><ref type=\"bibr\" target=\"#b29\">Tian, Krishnan, and Isola 2019;</ref><ref type=\"bibr\" target=\"#b38\">Zhuang, Zhai, and Yamins 2019;</ref><ref type=\"bibr\" target=\"#b6\">Che dataset, any two proteins share less than 25% sequence identity. Following the same procedure as in <ref type=\"bibr\" target=\"#b38\">(Zhou and Troyanskaya 2014)</ref>, we divide the CullPDB dataset into. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b5\">(Chen et al. 2017;</ref><ref type=\"bibr\" target=\"#b36\">Yim et al. 2017;</ref><ref type=\"bibr\" target=\"#b37\">Yu et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Schmitt et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al protein applications, such as the understanding of the protein functions and the design of drugs <ref type=\"bibr\" target=\"#b17\">(Noble, Endicott, and Johnson 2004)</ref>. Currently, there are three. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b20\">Peters et al. 2018;</ref><ref type=\"bibr\" target=\"#b21\">Radford et al. 2019;</ref><ref type=\"bibr\" target=\"#b35\">Yang et al. 2019)</ref>. Like language, large unlabeled datasets of p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nyals, and Dean (2015)</ref>. Over the past years, knowledge distillation has numerous applications <ref type=\"bibr\" target=\"#b5\">(Chen et al. 2017;</ref><ref type=\"bibr\" target=\"#b36\">Yim et al. 2017. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: galy 2010;</ref><ref type=\"bibr\" target=\"#b23\">Remmert et al. 2012)</ref>, or a combination of both <ref type=\"bibr\" target=\"#b2\">(Altschul et al. 1997)</ref> to align the sequence against the given d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <head>Experiment Dataset</head><p>We train the PSSM-Distil framework on the training set of CullPDB <ref type=\"bibr\" target=\"#b30\">(Wang and Dunbrack Jr 2003)</ref>. CullPDB validation set, CB513 <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b14\">(Li and Yu 2016;</ref><ref type=\"bibr\" target=\"#b33\">Wang et al. 2016;</ref><ref type=\"bibr\" target=\"#b38\">Zhou and Troyanskaya 2014)</ref> achieved satisfactory PSSP performan high-quality PSSM along with one-hot amino acid sequence as evolutionary information. Specifically, <ref type=\"bibr\" target=\"#b38\">Zhou and Troyanskaya (2014)</ref> used a deep convolutional network t 16\">Misra and Maaten 2020;</ref><ref type=\"bibr\" target=\"#b29\">Tian, Krishnan, and Isola 2019;</ref><ref type=\"bibr\" target=\"#b38\">Zhuang, Zhai, and Yamins 2019;</ref><ref type=\"bibr\" target=\"#b6\">Che dataset, any two proteins share less than 25% sequence identity. Following the same procedure as in <ref type=\"bibr\" target=\"#b38\">(Zhou and Troyanskaya 2014)</ref>, we divide the CullPDB dataset into. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arse transformations tend to yield zero for the low-scoring in the vector, which is named sparsemax <ref type=\"bibr\" target=\"#b12\">(Martins and Astudillo 2016)</ref>:</p><formula xml:id=\"formula_1\">sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: steful, making models less interpretable and assigning probability mass to many implausible outputs <ref type=\"bibr\" target=\"#b14\">(Peters, Niculae, and Martins 2019)</ref>.</p><p>To overcome the issu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b9\">Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b11\">Luo et al. 2020)</ref>. The weighted sum of the items within the sess type=\"bibr\" target=\"#b9\">(Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b11\">Luo et al. 2020)</ref> or used as the query vector for the weight ass p bidirectional self-attention to model user behaviors for sequential recommendation.</p><p>\u2022 CoSAN <ref type=\"bibr\" target=\"#b11\">(Luo et al. 2020</ref>) learn the session representation and predict . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roach, which considers the sessions that contain any item of the current session as neighbors. STAN <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019</ref>) is an extension of SKNN that additionally con ><p>\u2022 SKNN (Jannach and Ludewig 2017) is a session-based k-nearest-neighbors approach.</p><p>\u2022 STAN <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019</ref>) is an extension of SKNN approach with some ad  and Martins 2019), to replace the softmax, which has been proven useful for the application of NLP <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019;</ref><ref type=\"bibr\" target=\"#b1\">Correia, Niculae. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: by the sequential connections between adjacent clicks. For sequenceaware recommendation tasks, FPMC <ref type=\"bibr\" target=\"#b16\">(Rendle, Freudenthaler, and Schmidt-Thieme 2010)</ref> combines Marko. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: by the sequential connections between adjacent clicks. For sequenceaware recommendation tasks, FPMC <ref type=\"bibr\" target=\"#b16\">(Rendle, Freudenthaler, and Schmidt-Thieme 2010)</ref> combines Marko. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b8\">(Li et al. 2017;</ref><ref type=\"bibr\" target=\"#b9\">Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b11\">Luo et al. 2020)< ally considered as the user's current interest <ref type=\"bibr\" target=\"#b9\">(Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b11\">Luo et al. 2020)<  Both two datasets are publicly available.</p><p>For a fair comparison, we follow the previous work <ref type=\"bibr\" target=\"#b22\">(Xu et al. 2019)</ref> to filter out all sessions of length less than noting that the session containing at least two items is more conducive to methods based on the GNN <ref type=\"bibr\" target=\"#b22\">(Xu et al. 2019)</ref>. The statistics of the three datasets after pr el that applies a graph neural network to learn the item and session representation.</p><p>\u2022 GC-SAN <ref type=\"bibr\" target=\"#b22\">(Xu et al. 2019</ref>) makes recommendations by combining a single-la. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: various session-based recommendation algorithms <ref type=\"bibr\" target=\"#b8\">(Li et al. 2017;</ref><ref type=\"bibr\" target=\"#b9\">Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al. 2019;</ hich is the last user action within a session, is usually considered as the user's current interest <ref type=\"bibr\" target=\"#b9\">(Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al. 2019;< d as the query vector for the weight assignment in <ref type=\"bibr\">RNN (Li et al. 2017)</ref>, MLP <ref type=\"bibr\" target=\"#b9\">(Liu et al. 2018)</ref>, and GNN models <ref type=\"bibr\" target=\"#b21\" idasi et al. 2016</ref>) is a session-based recommendation model based on GRU layers.</p><p>\u2022 STAMP <ref type=\"bibr\" target=\"#b9\">(Liu et al. 2018</ref>) is a short-term memory priority model, which e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roach, which considers the sessions that contain any item of the current session as neighbors. STAN <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019</ref>) is an extension of SKNN that additionally con ><p>\u2022 SKNN (Jannach and Ludewig 2017) is a session-based k-nearest-neighbors approach.</p><p>\u2022 STAN <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019</ref>) is an extension of SKNN approach with some ad  and Martins 2019), to replace the softmax, which has been proven useful for the application of NLP <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019;</ref><ref type=\"bibr\" target=\"#b1\">Correia, Niculae. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m weights into probabilities. Usually, the transformation function is a well-known function softmax <ref type=\"bibr\" target=\"#b0\">(Bridle 1990</ref>), which returns positive values and dense output pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: by the sequential connections between adjacent clicks. For sequenceaware recommendation tasks, FPMC <ref type=\"bibr\" target=\"#b16\">(Rendle, Freudenthaler, and Schmidt-Thieme 2010)</ref> combines Marko. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ter result.</p><p>Recently, methods based on nearest-neighbors obtain competitive performance. SKNN <ref type=\"bibr\" target=\"#b6\">(Jannach and Ludewig 2017</ref>) is a session-based k-nearest-neighbor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: been proven useful for the application of NLP <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019;</ref><ref type=\"bibr\" target=\"#b1\">Correia, Niculae, and Martins 2019)</ref>,</p><formula xml:id=\"formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: various session-based recommendation algorithms <ref type=\"bibr\" target=\"#b8\">(Li et al. 2017;</ref><ref type=\"bibr\" target=\"#b9\">Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al. 2019;</ hich is the last user action within a session, is usually considered as the user's current interest <ref type=\"bibr\" target=\"#b9\">(Liu et al. 2018;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al. 2019;< d as the query vector for the weight assignment in <ref type=\"bibr\">RNN (Li et al. 2017)</ref>, MLP <ref type=\"bibr\" target=\"#b9\">(Liu et al. 2018)</ref>, and GNN models <ref type=\"bibr\" target=\"#b21\" idasi et al. 2016</ref>) is a session-based recommendation model based on GRU layers.</p><p>\u2022 STAMP <ref type=\"bibr\" target=\"#b9\">(Liu et al. 2018</ref>) is a short-term memory priority model, which e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roach, which considers the sessions that contain any item of the current session as neighbors. STAN <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019</ref>) is an extension of SKNN that additionally con ><p>\u2022 SKNN (Jannach and Ludewig 2017) is a session-based k-nearest-neighbors approach.</p><p>\u2022 STAN <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019</ref>) is an extension of SKNN approach with some ad  and Martins 2019), to replace the softmax, which has been proven useful for the application of NLP <ref type=\"bibr\" target=\"#b2\">(Garg et al. 2019;</ref><ref type=\"bibr\" target=\"#b1\">Correia, Niculae. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: istic ways to generate the query vector, which ignores the real user's current preference. Bert4Rec <ref type=\"bibr\" target=\"#b17\">(Sun et al. 2019</ref>) employs the deep bidirectional self-attention trics HR@5 HR@10 HR@20 MRR@5 MRR@10 MRR@20 HR@5 HR@10 HR@20 MRR@5 MRR@10 MRR@20 S-POP 0. \u2022 Bert4Rec <ref type=\"bibr\" target=\"#b17\">(Sun et al. 2019</ref>) is a method that employ the deep bidirectiona ng, and the other is the positional embedding. We introduce a learnable positional embedding module <ref type=\"bibr\" target=\"#b17\">(Sun et al. 2019)</ref>, which is used to map the position index to a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve 3D information. To this end, we conduct formal analyses in the spherical coordinate system (SCS) <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2019)</ref>, and show that relative location of each ato. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b15\">Gao et al., 2018;</ref><ref type=\"bibr\">2021)</ref>. Currently, the message passing scheme <ref type=\"bibr\" target=\"#b19\">(Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Sanchez-Gon .1\">MESSAGE PASSING SCHEME</head><p>Currently, the class of message passing neural networks (MPNNs) <ref type=\"bibr\" target=\"#b19\">(Gilmer et al., 2017)</ref> are one of the most widely used architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b55\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b44\">Shervashidze et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Fout et al., 2017;</ref><ref type=\"bibr\" target=\"#b34\">Liu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: able <ref type=\"table\" target=\"#tab_9\">8</ref>, respectively. Code is integrated in the DIG library <ref type=\"bibr\" target=\"#b32\">(Liu et al., 2021)</ref>   The function \u03c6 e is applied to each edge k. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>In many real-world studies, structured objects such as molecules are naturally modeled as graphs <ref type=\"bibr\" target=\"#b21\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b55\">Wu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: O(nk 2 ). This enables the application of SMP to large molecules, like the newly released OC20 data <ref type=\"bibr\" target=\"#b5\">(Chanussot et al., 2021)</ref>. We rigorously investigate the complete UP</head><p>We apply our SphereNet to three benchmark datasets, including Open Catalyst 2020 (OC20) <ref type=\"bibr\" target=\"#b5\">(Chanussot et al., 2021)</ref>, QM9 <ref type=\"bibr\" target=\"#b39\">(Ra 2020 (OC20) dataset is a newly released large-scale dataset for catalyst discovery and optimization <ref type=\"bibr\" target=\"#b5\">(Chanussot et al., 2021)</ref>. It comprises millions of DFT relaxatio ion of the data is provided in Appendix E. Results for CGCNN, SchNet, and DimeNet++ are provided in <ref type=\"bibr\" target=\"#b5\">Chanussot et al. (2021)</ref>. The original GemNet paper does not cont ts for GemNet-T. We report evaluation results of fixed epochs for SphereNet. Following a setting in <ref type=\"bibr\" target=\"#b5\">Chanussot et al. (2021)</ref>, we use the direct approach and all the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>In many real-world studies, structured objects such as molecules are naturally modeled as graphs <ref type=\"bibr\" target=\"#b21\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b55\">Wu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b19\">(Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b51\">Vignac et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Battaglia et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b27\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b11\">Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b50\">Veli\u010dkovi\u0107 et al., 2018;</ref><ref type=\"bibr\" target=\"#b58\">Zhang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2021)</ref>, QM9 <ref type=\"bibr\" target=\"#b39\">(Ramakrishnan et al., 2014)</ref>, and MD17 <ref type=\"bibr\" target=\"#b7\">(Chmiela et al., 2017)</ref>. Baseline methods include <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graphs, and they can be important in molecular learning, such as bond lengths, angles between bonds <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017;</ref><ref type=\"bibr\" target=\"#b29\">Klicpera et ethods is in early stage, and existing studies focus on leveraging different geometries. The SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref> incorporates the distance information dur al., 2017)</ref>. Baseline methods include <ref type=\"bibr\">PPGN (Maron et al., 2019)</ref>, SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref>, PhysNet (Unke &amp; Meuwly, 2019), Cormo e models employ a joint loss of forces and conserved energy during training. In the original SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref> and DimeNet <ref type=\"bibr\" target=\"#b29 ine the expressive power of SphereNet for molecular dynamics simulations. Following the settings in <ref type=\"bibr\" target=\"#b41\">Sch\u00fctt et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b29\">Klicpera e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b4\">(Bronstein et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">De Haan et al., 2020;</ref><ref type=\"bibr\" target=\"#b37\">Perraudin et al., 2019)</ref>. When modeling 3D point clouds as 3D gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as bond lengths, angles between bonds <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017;</ref><ref type=\"bibr\" target=\"#b29\">Klicpera et al., 2020b)</ref>.</p><p>We first investigate complete re ates both the atom features and distance information in the proposed interaction block. The DimeNet <ref type=\"bibr\" target=\"#b29\">(Klicpera et al., 2020b)</ref> is developed based on the PhysNet and  >(Batzner et al., 2022)</ref>, MGCN <ref type=\"bibr\" target=\"#b35\">(Lu et al., 2019)</ref>, DimeNet <ref type=\"bibr\" target=\"#b29\">(Klicpera et al., 2020b)</ref>, DimeNet++ <ref type=\"bibr\" target=\"#b ning. In the original SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref> and DimeNet <ref type=\"bibr\" target=\"#b29\">(Klicpera et al., 2020b)</ref> papers, the authors set the weight of  s simulations. Following the settings in <ref type=\"bibr\" target=\"#b41\">Sch\u00fctt et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b29\">Klicpera et al. (2020b)</ref>, we train a separate model for each mol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Sch\u00fctt et al., 2021)</ref>, NequIP <ref type=\"bibr\" target=\"#b3\">(Batzner et al., 2022)</ref>, MGCN <ref type=\"bibr\" target=\"#b35\">(Lu et al., 2019)</ref>, DimeNet <ref type=\"bibr\" target=\"#b29\">(Klic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b50\">Veli\u010dkovi\u0107 et al., 2018;</ref><ref type=\"bibr\" target=\"#b58\">Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b57\">Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Gao &amp; Ji, 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b57\">Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Gao &amp; Ji, 2019;</ref><ref type=\"bibr\" target=\"#b15\">Gao et al., 2018;</ref><ref type=\"bibr\">2021)</ref>. Currently, the m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s in distinguishing geometric graph properties, such as girth and circumference, etc. Other studies <ref type=\"bibr\" target=\"#b25\">(Ingraham et al., 2019;</ref><ref type=\"bibr\" target=\"#b45\">Simm et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: veloped for learning from graph data <ref type=\"bibr\" target=\"#b27\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b11\">Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b50\">Veli\u010dkov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce on heterophily (or low-homophily) graphs <ref type=\"bibr\" target=\"#b15\">(Pei et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Zhu et al., 2020)</ref>, which-unlike homophily graphs-have many neig g works have proposed some effective designs <ref type=\"bibr\" target=\"#b15\">(Pei et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Zhu et al., 2020)</ref>, it remains an under-explored area. These two , 2002)</ref>. For instance, in protein networks, amino acids of different types tend to form links <ref type=\"bibr\" target=\"#b33\">(Zhu et al., 2020)</ref>, and in transaction networks, fraudsters are ls tackling heterophily: Geom-GCN <ref type=\"bibr\" target=\"#b15\">(Pei et al., 2019)</ref> and H2GCN <ref type=\"bibr\" target=\"#b33\">(Zhu et al., 2020)</ref>;</p><p>(3) the state-of-the-art model for ov e use the code from a well-accepted Github repository 2 . For GraphSage, we report the results from <ref type=\"bibr\" target=\"#b33\">(Zhu et al., 2020)</ref>, which uses the same data and splits. For th ayers. Best model per benchmark highlighted in gray. The \" \u2020 \" results (GraphSAGE) are obtained from<ref type=\"bibr\" target=\"#b33\">(Zhu et al., 2020)</ref>.</figDesc><table><row><cell></cell><cell>Tex s first outlined in the context of GNNs in <ref type=\"bibr\" target=\"#b15\">(Pei et al., 2019)</ref>. <ref type=\"bibr\" target=\"#b33\">Zhu et al. (2020)</ref> identified a set of effective designs that al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Shi et al., 2019)</ref>. A typical GNN architecture <ref type=\"bibr\" target=\"#b27\">(Xu et al., 2018a)</ref> for the node classification task can be deco eural Networks. In general, for the node classification task, GNNs can be decomposed into two steps <ref type=\"bibr\" target=\"#b27\">(Xu et al., 2018a)</ref>: (1) neighborhood propagation and aggregatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  they have some key limitations.</p><p>The first limitation is known as the \"oversmoothing\" problem <ref type=\"bibr\" target=\"#b10\">(Li et al., 2018)</ref>: the performance of GNNs degrade when stackin Hamilton et al. (2017)</ref>.</p><p>Oversmoothing. The oversmoothing problem was first discussed in <ref type=\"bibr\" target=\"#b10\">(Li et al., 2018)</ref>, which proved that by repeatedly applying Lap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ., 2019)</ref> to program understanding <ref type=\"bibr\" target=\"#b0\">(Allamanis et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Shi et al., 2019)</ref>. A typical GNN architecture <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he dynamics on how oversmoothing is triggered, or which nodes tend to cause it. To date, works like <ref type=\"bibr\" target=\"#b28\">(Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b26\">Wang et al., 2 residual connections and dilated con-volutions <ref type=\"bibr\">(Li et al., 2019)</ref>; skip links <ref type=\"bibr\" target=\"#b28\">(Xu et al., 2018b)</ref>; new normalization strategies <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b2\">(Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b26\">Wang et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Rong et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Rossi et al.,  rmalization strategies <ref type=\"bibr\" target=\"#b31\">(Zhao &amp; Akoglu, 2019)</ref>; edge dropout <ref type=\"bibr\" target=\"#b16\">(Rong et al., 2019)</ref>; and a new model that even increases perfor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er, 2019)</ref>, biology <ref type=\"bibr\" target=\"#b29\">(Yan et al., 2019)</ref>, algorithmic tasks <ref type=\"bibr\" target=\"#b25\">(Veli\u010dkovi\u0107 et al., 2020;</ref><ref type=\"bibr\" target=\"#b30\">Yan et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: erative models <ref type=\"bibr\" target=\"#b31\">(Kalibhat et al., 2020)</ref>, reinforcement learning <ref type=\"bibr\" target=\"#b67\">(Yu et al., 2020)</ref> and lifelong learning <ref type=\"bibr\" target \"bibr\" target=\"#b63\">You et al., 2020a;</ref><ref type=\"bibr\" target=\"#b24\">Gale et al., 2019;</ref><ref type=\"bibr\" target=\"#b67\">Yu et al., 2020;</ref><ref type=\"bibr\" target=\"#b31\">Kalibhat et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b30\">(H\u00fcbler et al., 2008;</ref><ref type=\"bibr\" target=\"#b8\">Chakeri et al., 2016;</ref><ref type=\"bibr\" target=\"#b7\">Calandriello et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Adhikari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  prediction <ref type=\"bibr\" target=\"#b68\">(Zhang &amp; Chen, 2018)</ref>, and graph classification <ref type=\"bibr\" target=\"#b62\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b59\">Xu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on graphs such as gating and attention <ref type=\"bibr\" target=\"#b3\">(Battaglia et al., 2016;</ref><ref type=\"bibr\" target=\"#b42\">Monti et al., 2017;</ref><ref type=\"bibr\" target=\"#b54\">Veli\u010dkovi\u0107 et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: improving upon limitations of existing models <ref type=\"bibr\" target=\"#b60\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b43\">Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Chen et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5\">Leskovec &amp; Faloutsos, 2006;</ref><ref type=\"bibr\" target=\"#b56\">Voudigari et al., 2016;</ref><ref type=\"bibr\" target=\"#b19\">Eden et al., 2018;</ref><ref type=\"bibr\" target=\"#b69\">Zhao, 2015;</r \">(Leskovec &amp; Faloutsos, 2006;</ref><ref type=\"bibr\" target=\"#b56\">Voudigari et al., 2016;</ref><ref type=\"bibr\" target=\"#b19\">Eden et al., 2018)</ref>. FastGCN <ref type=\"bibr\" target=\"#b9\">(Chen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b30\">(H\u00fcbler et al., 2008;</ref><ref type=\"bibr\" target=\"#b8\">Chakeri et al., 2016;</ref><ref type=\"bibr\" target=\"#b7\">Calandriello et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Adhikari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b9\">(Chen et al., 2018a)</ref>   <ref type=\"bibr\" target=\"#b20\">(Evci et al., 2019;</ref><ref type=\"bibr\" target=\"#b49\">Savarese et al., 2020;</ref><ref type=\"bibr\" target=\"#b39\">Liu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly. To our best knowledge, the hardware acceleration research on GNNs just starts to gain interests <ref type=\"bibr\" target=\"#b2\">(Auten et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Abadal et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a technical trend <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> is to develop end-t nt Modeling</head><p>Unlike the previous GNN-based studies <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> that assume no or o sonalized. (2) Unlike the ideas of using the decay factors <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> or regularization t Django Unchained), respectively. However, previous studies <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> only model KG relat et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> are founded upon th eover, KG relations are typically modeled in decay factors <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> of adjacent matrix, in order to control the influences of n recommender models <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> have shown that the neighborhood aggregation scheme is a pr sets. Experimental results show that our KGIN outperforms the state-of-the-art methods such as KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>, KGNN-LS <ref type=\"bibr\" target=\"#b37\">[38]</ref>, and CK , \ud835\udc56) pair indicates that user \ud835\udc62 has interacted with item \ud835\udc56 before. In some previous works like KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>, an additional relation interact-with is introduced to exp type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> or regularization terms <ref type=\"bibr\" target=\"#b40\">[41]</ref> in previous studies, we highlight the role of intent relat endation in the experiments:</p><p>(1) We use the Amazon-Book and Last-FM datasets released by KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>; And (2) we further introduce the Alibaba-iFashion dataset  so as to generate user-specific item representations. It models relations in decay factors. \u2022 KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref> is a state-of-the-art GNN-based recommender. It applies an presentations. As such, these methods are able to model long-range connectivity. For instance, KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref> combines user-item interactions and KG as a heterogeneous  intent graph (IG), which differs from the homogeneous collaborative graph adopted in previous works <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>.</p></div> <div xm s and construct triplets with the inverse relations in experiments. Closely Following prior studies <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>, we use the same d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dependence. Such an idea coincides with contrastive learning <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. More formally, the independence modeling is:</p><formula x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al commonsense knowledge, in the form of a heterogeneous graph or heterogeneous information network <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Let V be a set of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b43\">[44]</ref>, poor transferability to different domains <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, and/or unstable performance <ref type=\"bibr\" target=\"#b48\" hus resulting in poor transferability to different domains <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>.</p><p>Policy-based Methods <ref type=\"bibr\" target=\"#b39\"> pe=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>.</p><p>GNN-based Methods <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta tegies to separately spread collaborative signals and knowledge-aware signals. More recently, NIRec <ref type=\"bibr\" target=\"#b16\">[17]</ref> is proposed to combine path-and GNN-based models, which pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: op end-to-end models founded on graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar 6\">47]</ref> are founded upon the information aggregation mechanism of graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph, and then formulate our task.</p><p>Interaction Data. Here we focus on the implicit feedback <ref type=\"bibr\" target=\"#b25\">[26]</ref> in recommendation, where the signal that a user provides a ifferent behavioral patterns of users. This can supercharge the widely-used collaborative filtering <ref type=\"bibr\" target=\"#b25\">[26]</ref> effect with the finer-grained assumption -users driven by  <p>Graph. We first move on to refine collaborative information from IG. As mentioned, the CF effect <ref type=\"bibr\" target=\"#b25\">[26]</ref> is powerful to characterize user patterns, by assuming tha ://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Model Optimization</head><p>We opt for the pairwise BPR loss <ref type=\"bibr\" target=\"#b25\">[26]</ref> to reconstruct the historical data. Specifically, it consi free (MF), embedding-based (CKE), and GNN-based (KGAT, KGNN-LS, CKAN, and RGCN) methods:</p><p>\u2022 MF <ref type=\"bibr\" target=\"#b25\">[26]</ref> (matrix factorization) only considers the user-item intera nsE on KG triplets, and feed the knowledge-aware embeddings of items into matrix factorization (MF) <ref type=\"bibr\" target=\"#b25\">[26]</ref>. KTUP <ref type=\"bibr\" target=\"#b3\">[4]</ref> employs Tran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r feature transformation, which are rather heavy and burdensome to train, thus degrades performance <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>; (2) TransR in CKE tion aggregation mechanism of graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph, and then formulate our task.</p><p>Interaction Data. Here we focus on the implicit feedback <ref type=\"bibr\" target=\"#b25\">[26]</ref> in recommendation, where the signal that a user provides a ifferent behavioral patterns of users. This can supercharge the widely-used collaborative filtering <ref type=\"bibr\" target=\"#b25\">[26]</ref> effect with the finer-grained assumption -users driven by  <p>Graph. We first move on to refine collaborative information from IG. As mentioned, the CF effect <ref type=\"bibr\" target=\"#b25\">[26]</ref> is powerful to characterize user patterns, by assuming tha ://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Model Optimization</head><p>We opt for the pairwise BPR loss <ref type=\"bibr\" target=\"#b25\">[26]</ref> to reconstruct the historical data. Specifically, it consi free (MF), embedding-based (CKE), and GNN-based (KGAT, KGNN-LS, CKAN, and RGCN) methods:</p><p>\u2022 MF <ref type=\"bibr\" target=\"#b25\">[26]</ref> (matrix factorization) only considers the user-item intera nsE on KG triplets, and feed the knowledge-aware embeddings of items into matrix factorization (MF) <ref type=\"bibr\" target=\"#b25\">[26]</ref>. KTUP <ref type=\"bibr\" target=\"#b3\">[4]</ref> employs Tran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s. This module can be simply achieved by applying a statistical measure, such as mutual information <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Pearson correlation <ref type=\"bibr\" target=\"#b32\">[33]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Typically, it incorporates information from the one-hop no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f>, these encoders encode either structures <ref type=\"bibr\" target=\"#b26\">(Luo et al., 2018b;</ref><ref type=\"bibr\" target=\"#b60\">Ying et al., 2019;</ref><ref type=\"bibr\" target=\"#b48\">Wang et al., 2 ee major encoding-dependent subroutines as well as eight status quo NAS algorithms on NAS-Bench-101 <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref> (small), NAS-Bench-301 <ref type=\"bibr\" tar ctures. However, adjacency matrix-based encoding grows quadratically as the search space scales up. <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref> propose categorical adjacency matrixbased e ead><p>We restrict our search space to the cell-based architectures. Following the configuration in <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref>, each cell is a labeled directed acyclic gr div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>NAS-Bench-101</head><p>The NAS-Bench-101 search space <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref> consists of approximately 420K architecture -101. These encoding schemes include (1-3) one-hot/categorical/continuous adjacency matrix encoding <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref>, (4-6) one-hot/categorical/continuous path  ithout considering graph isomorphism, which is a much larger search space compared to NAS-Bench-101 <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref> and NAS-Bench-201 <ref type=\"bibr\" target=\"  structure encodings may not be computationally unique unless some certain graph hashing is applied <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Ning et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f>.</p><p>Context Dependency Our work is close to self-supervised learning in language models (LMs) <ref type=\"bibr\" target=\"#b9\">(Dong et al., 2019)</ref>. In particular, ELMo <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b21\">(Liu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b33\">Radosavovic et al., 2020;</ref><ref type=\"bibr\" target=\"#b38\">Ru et al., 2020)</ref> or designing efficient architecture search and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  has been started with genetic algorithms <ref type=\"bibr\" target=\"#b27\">(Miller et al., 1989;</ref><ref type=\"bibr\" target=\"#b17\">Kitano, 1990;</ref><ref type=\"bibr\" target=\"#b43\">Stanley &amp; Miikk. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  has been started with genetic algorithms <ref type=\"bibr\" target=\"#b27\">(Miller et al., 1989;</ref><ref type=\"bibr\" target=\"#b17\">Kitano, 1990;</ref><ref type=\"bibr\" target=\"#b43\">Stanley &amp; Miikk. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8\">Yan et al., 2020)</ref> or computations <ref type=\"bibr\" target=\"#b62\">(Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Ning et al., 2020;</ref><ref type=\"bibr\" target=\"#b53\">White et al.,  nless some certain graph hashing is applied <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Ning et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b53\">(White et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b65\">(Zoph et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Luo et al., 2018b)</ref>, SRM <ref type=\"bibr\" target=\"#b1\">(Baker et al., 2018)</ref>, MLP <ref type=\"bibr\" target=\"#b20\">(Liu et b57\">Yan et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Chu et al., 2019)</ref>, neural predictor <ref type=\"bibr\" target=\"#b1\">(Baker et al., 2018;</ref><ref type=\"bibr\" target=\"#b50\">Wen et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b8\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Liu et al., 2019b;</ref><ref type=\"bibr\" target=\"#b18\">Lewis et al., 2020;</ref><ref type=\"bibr\" target=\"#b34\">Raffel et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b26\">Luo et al., 2018b)</ref>, SRM <ref type=\"bibr\" target=\"#b1\">(Baker et al., 2018)</ref>, MLP <ref type=\"bibr\" target=\"#b20\">(Liu et al., 2018a;</ref><ref type=\"bibr\" target=\"#b48\">Wang et al.,  5 \u00d7 5 dilated conv, sum, c k }. Following NAS-Bench-301, we do not include zero operator. Following <ref type=\"bibr\" target=\"#b20\">(Liu et al., 2018a)</ref>, we use the same cell for both normal and r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b26\">(Luo et al., 2018b;</ref><ref type=\"bibr\" target=\"#b39\">Shi et al., 2020;</ref><ref type=\"bibr\" target=\"#b53\">White et al., 2021)</ref>, some of the most recent work (White et al. ibr\" target=\"#b62\">(Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Ning et al., 2020;</ref><ref type=\"bibr\" target=\"#b53\">White et al., 2021)</ref> of the neural architectures. Compared to st e=\"bibr\" target=\"#b58\">Yan et al., 2020;</ref><ref type=\"bibr\" target=\"#b39\">Shi et al., 2020;</ref><ref type=\"bibr\" target=\"#b53\">White et al., 2021)</ref> and local search <ref type=\"bibr\" target=\"# on encoding and addresses the drawbacks of <ref type=\"bibr\" target=\"#b62\">(Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b53\">White et al., 2021)</ref>.</p><p>Context Dependency Our work is close =\"bibr\" target=\"#b50\">(Wen et al., 2020;</ref><ref type=\"bibr\" target=\"#b39\">Shi et al., 2020;</ref><ref type=\"bibr\" target=\"#b53\">White et al., 2021)</ref>, Bayesian optimization with Gaussian proces  target=\"#b19\">Li &amp; Talwalkar, 2019;</ref><ref type=\"bibr\" target=\"#b58\">Yan et al., 2020;</ref><ref type=\"bibr\" target=\"#b53\">White et al., 2021)</ref> that use the common test evaluation script  br\" target=\"#b60\">(Ying et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Ning et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b53\">(White et al., 2021;</ref><ref type=\"bibr\" target=\"#b49\">Wei et al.,  ) one-hot/categorical/continuous path encoding and (7-9) their corresponding truncated counterparts <ref type=\"bibr\" target=\"#b53\">(White et al., 2021)</ref>, (10) D-VAE <ref type=\"bibr\" target=\"#b62\" 0.15 3.2 4 DARTS <ref type=\"bibr\" target=\"#b22\">(Liu et al., 2019a)</ref> 2.76 \u00b1 0.09 3.3 4 BANANAS <ref type=\"bibr\" target=\"#b53\">(White et al., 2021)</ref> 2.67 \u00b1 0.07 3.6 10.2 arch2vec-BO <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: formation, which is not sufficient for modeling deep interactions between the two directions. GPT-2 <ref type=\"bibr\" target=\"#b32\">(Radford et al., 2019)</ref> proposes an autoregressive language mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alkar, 2019)</ref>, evolutionary algorithms <ref type=\"bibr\" target=\"#b37\">(Real et al., 2019;</ref><ref type=\"bibr\" target=\"#b44\">Stanley et al., 2019)</ref>, reinforcement learning <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b21\">(Liu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b33\">Radosavovic et al., 2020;</ref><ref type=\"bibr\" target=\"#b38\">Ru et al., 2020)</ref> or designing efficient architecture search and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: om Forest (RF) <ref type=\"bibr\" target=\"#b4\">(Breiman, 2001)</ref>, Support Vector Regression (SVR) <ref type=\"bibr\" target=\"#b11\">(Drucker et al., 1997)</ref>, Graph Isomorphism Network (GIN) <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: formation, which is not sufficient for modeling deep interactions between the two directions. GPT-2 <ref type=\"bibr\" target=\"#b32\">(Radford et al., 2019)</ref> proposes an autoregressive language mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s from intricate feature engineering. Many researchers managed to apply DNNs to weather predictions <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Among these wor  more robust prediction.</p><p>DNN based weather pattern mining. As a breakthrough work, Shi et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> develop the conventional LSTM and propose convolutional LST nowledge from it. This task fits perfectly with the convolutional long short-term memory (ConvLSTM) <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which has proven successful in processing ST tensors. Foll. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rning has inspired some researchers applying data-driven models to weather predictions. Early works <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> usually treat machi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref> that improves ConvLSTM by actively learning the recurrent connection structure. Wang et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> presents a predictive recurrent neural network (PredRNN) i 5 convolution layer and a 1 \u00d7 1 locally-connected layer with constraints, </p><p>The expectation in <ref type=\"bibr\" target=\"#b10\">(11)</ref> can be estimated as <ref type=\"bibr\" target=\"#b23\">[24]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: prediction is conducted by a series of empirical functions <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> with the simulated atmospheric parameters as input. However,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: asting and numerical weather prediction (NWP) based forecasting. The extrapolation based nowcasting <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref> first identifies th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he great harm of lightning has driven significant interest in the prediction of this natural hazard <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>Traditional lightning prediction methods can be rough ions of various weather parameters for the past hours. We select three lightning-related parameters <ref type=\"bibr\" target=\"#b2\">[3]</ref> (i.e. average temperature, average relative humidity and pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b18\">[19]</ref>, Hetero-ConvLSTM <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DML <ref type=\"bibr\" target=\"#b20\">[21]</ref> and CoST-Net <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Pr hich introduces spatial graph features and spatial model ensemble on top of the basic ConvLSTM. DML <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposes a meta graph attention module and a meta recurren. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rous natural phenomenon, posing huge threats to human life, aviation and electrical infrastructures <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The great harm of lightning has driven significant interest ef> propose a new parameterization by combining PR92 and cloud droplet concentration. Mccaul et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> propose two approaches based on the upward fluxes of precipi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as become a basic tool for ST data mining. Furthermore, they also proposed the Trajectory GRU model <ref type=\"bibr\" target=\"#b16\">[17]</ref> that improves ConvLSTM by actively learning the recurrent . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  its i-th diagonal elements for 1 \u2264 i \u2264 n. Although p may not be properly normalized, previous work <ref type=\"bibr\" target=\"#b24\">[25]</ref> has shown that maximizing p is asymptotically consistent w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  researchers managed to apply DNNs to weather predictions <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Among these works, Wang et al. <ref type=\"bibr\" target=\"# 13]</ref>. Among these works, Wang et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> and Geng et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> both propose to predict weather via DNN models combining h space and time domains, which introduces irreparable biases to the prediction methods based on them <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Thirdly, despite the rich information sources for the lig ew view for nowcasting tasks. Wang et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> and Geng et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose to predict future weather by combining information te (x, y, z, t), and X t (a three-dimensional tensor) denotes the slice at the t-th hour. Following <ref type=\"bibr\" target=\"#b12\">[13]</ref>, we choose simulated micro-physical parameters, radar refl Considering the temporal correlation of thunderstorms, we introduce past lightning observation data <ref type=\"bibr\" target=\"#b12\">[13]</ref> into the input. Following the structure of the WRF simulat une to September in 2015, and May to September in both 2016 and 2017, 14 months in total. Following <ref type=\"bibr\" target=\"#b12\">[13]</ref>, we chronologically divide the total dataset by a ratio of et=\"#b28\">[29]</ref>,</p><p>StepDeep <ref type=\"bibr\" target=\"#b18\">[19]</ref>, StepDeep+, LightNet <ref type=\"bibr\" target=\"#b12\">[13]</ref> and LightNet+, where the methods with suffix \"+\" mean that  to distinguish from the results only employing WRF simulation and light observation as reported in <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Among these baselines, the first three are traditional li )/n. The equations for the three metrics are detailed in Table <ref type=\"table\">I</ref>. Following <ref type=\"bibr\" target=\"#b12\">[13]</ref>, we also calculate eight-neighborhood-based POD, FAR and E ll><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Results are reported in<ref type=\"bibr\" target=\"#b12\">[13]</ref>.</note></figure> \t\t</body> \t\t<back> \t\t\t<div type=\"referenc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Importance of Relation</head><p>The heterogeneity of KGs is usually reflected by the relation-path <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which demonstrates complex semantic features that involve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: these knowledge-driven applications, numerous types of KGs <ref type=\"bibr\" target=\"#b6\">[7]</ref>- <ref type=\"bibr\" target=\"#b8\">[9]</ref> have been developed in the past decades. KGs are multi-relat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Networks</head><p>GNNs aim to extend the deep neural networks for graph-structured data. Xu et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> present a theor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b37\">[38]</ref>, Nguyen et al. <ref type=\"bibr\" target=\"#b38\">[39]</ref>, and Vashishth et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref> adopt a multiple layer of convolutional neural networks (C s utilized to extract global relationships between entities and relations. 10) InteractE: InteractE <ref type=\"bibr\" target=\"#b39\">[40]</ref> intends to improve link prediction performance by increasi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gh a localized first-order approximation, named graph convolutional networks (GCN). Hamilton et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> propose the GraphSAGE framework based on node sampling and (e s , r, e o ) = \u03c3 (\u03d5(e s , r, e o ) + b) \u2208 (0, 1)</p><p>where b is a bias term, and \u03c3 (x) = 1/(   <ref type=\"bibr\" target=\"#b19\">(20)</ref> in which</p><formula xml:id=\"formula_34\">y = 1 if (e s , r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ework to show its expressive power for different graph structures and reasoning tasks. Bruna et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> first proposed convolutional networks to graphs based on s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graph-based applications, such as recommendation systems <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Inspired by the attention mechanisms, Velickovic et al. <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep neural networks for graph-structured data. Xu et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> present a theoretical framework to show its expressive pow. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ework to show its expressive power for different graph structures and reasoning tasks. Bruna et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> first proposed convolutional networks to graphs based on s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and question answering <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. To facilitate thes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd item representations <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Recently, GCN-based models <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta ually, current GCN-based recommendation models achieve their peak performance at most 3 or 4 layers <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Besides the over- CF on recommendation accuracy.</p><p>It is worth mentioning that the LightGCN proposed by He et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> has a similar formulation as LR-GCN. With careful experime pt the simplified network structure of LightGCN, as its effectiveness has been well demonstrated in <ref type=\"bibr\" target=\"#b13\">[14]</ref> and it can alleviate the over-smoothing problem to some ex epresentation of user \ud835\udc62 and item \ud835\udc56 as Eq. 2. Similar to LightGCN, \ud835\udefc \ud835\udc58 is set uniformly as 1/(\ud835\udc3e + 1) <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>With the learned embeddings of users (i.e., \ud835\udc86 \ud835\udc62 ) a the final representations of users and items, this formulation keeps consistent with it in LightGCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>:</p><formula xml:id=\"formula_17\">\ud835\udc6c = \ud835\udefc 0 \ud835\udc6c (0) + \ud835\udefc 1 \ud835\udc6c (1) der connectivities by performing embedding propagation in the user-item bipartite graph. \u2022 LightGCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>: It is an simplified version of NGCF by removing the featu zing high-order information directly in representation learning. Similar to the results reported in <ref type=\"bibr\" target=\"#b13\">[14]</ref>, LightGCN achieves substantially improvement over NGCF by  /ref>, researchers also introspect the complex design in GCN-based recommendation models. He at al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> pointed out that the two common designs feature transforma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the depth will lead to sharp performance degradation. In the domain of recommendation, Chen et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> have empirically demonstrated that the user/item embeddings  nd proposed LightGCN which substantially improves the performance over NDCG. Meanwhile, Chen et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> also proposed to remove the nonlinearity in the network and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: multiple graph convolution layers on the item-item graph for Pinterest image recommendation; MEIRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> utilizes metapath-guided neighbors to exploit rich structure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tead of implicitly capturing the high-order connectivity through the propagation embedding, SMOG-CF <ref type=\"bibr\" target=\"#b44\">[45]</ref> is proposed to directly capture the high-order connectivit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref><ref type=\"bibr\" target=\"#b31\">[32]</ref>) to deal with different tasks (e.g., context-aware <ref type=\"bibr\" target=\"#b21\">[22]</ref>, session-based <ref type=\"bibr\" target=\"#b22\">[23]</ref>).. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ia leveraging additional information (e.g., review <ref type=\"bibr\" target=\"#b24\">[25]</ref>, image <ref type=\"bibr\" target=\"#b11\">[12]</ref>, knowledge graph <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ay. It is a commonly used approach to make graph convolution network feasible for large-scale graph <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Let \ud835\udc6c (0) be the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tead of implicitly capturing the high-order connectivity through the propagation embedding, SMOG-CF <ref type=\"bibr\" target=\"#b44\">[45]</ref> is proposed to directly capture the high-order connectivit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b11\">[12]</ref>, knowledge graph <ref type=\"bibr\" target=\"#b29\">[30]</ref><ref type=\"bibr\" target=\"#b30\">[31]</ref><ref type=\"bibr\" target=\"#b31\">[32]</ref>) to deal with dif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e is fixed to 64 for all models and the embedding parameters are initialized with the Xavier method <ref type=\"bibr\" target=\"#b38\">[39]</ref>. We optimized our method with Adam <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: large unlabeled molecule dataset. Through contrastive loss <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, MolCLR learns the representations by contrasting positive  tum encoder, which builds an on-the-fly consistent dictionary. Instead of using memory bank, SimCLR <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> demonstrates contr .1\">MolCLR Framework</head><p>Our MolCLR model is developed upon the contrastive learning framework <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Latent representa rocessing and augmentation, GNN-based feature extractor, non-linear projection head, and an NT-Xent <ref type=\"bibr\" target=\"#b36\">[37]</ref> contrastive loss. In our case, we implement the commonly-u  The choice of the temperature parameter \u03c4 in Eq. 1 impacts the performance of contrastive learning <ref type=\"bibr\" target=\"#b36\">[37]</ref>. An appropriate \u03c4 benefits the model to learn from hard ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: roperty prediction <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> and virtual screening <ref type=\"bibr\" target=\"#b28\">[29,</ get=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b61\">62]</ref>. For readout operat f type=\"bibr\" target=\"#b59\">60]</ref> with edge feature involved in aggregation is compared. D-MPNN <ref type=\"bibr\" target=\"#b27\">[28]</ref> and MGCNN <ref type=\"bibr\" target=\"#b73\">[74]</ref>, which <ref type=\"bibr\" target=\"#b90\">91]</ref>. MPNN <ref type=\"bibr\" target=\"#b12\">[13]</ref> and D-MPNN <ref type=\"bibr\" target=\"#b27\">[28]</ref> implement a message-passing architecture to aggregate the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iven molecular representation learning and its applications, including chemical property prediction <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar turally encode the structure information, have been introduced to molecular representation learning <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar arning, especially deep learning driven by neural networks <ref type=\"bibr\" target=\"#b80\">[81,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. In conventional che. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sages based on the angle between atoms.</p><p>Benefiting from the growth of available molecule data <ref type=\"bibr\" target=\"#b91\">[92,</ref><ref type=\"bibr\" target=\"#b92\">93,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esentation learning <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b90\">91]</ref>. MPNN <ref type=\"bibr\" target=\"#b12\">[13]</ref> and D-MPNN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: licates the challenges, since the properties of interest range from quantum mechanics to biophysics <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Consequently, the rpasses other self-supervised learning and pre-training strategies in multiple molecular benchmarks <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Besides, in the downstream tasks, our MolCLR rivals or ev  Datasets. To benchmark the performance of our MolCLR framework, we use 7 datasets from MoleculeNet <ref type=\"bibr\" target=\"#b33\">[34]</ref>, containing in total 44 binary classification tasks. These able molecule data <ref type=\"bibr\" target=\"#b91\">[92,</ref><ref type=\"bibr\" target=\"#b92\">93,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref>, self-supervised/pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , visualization and interpretation of self-supervised learned representations are of great interest <ref type=\"bibr\" target=\"#b96\">[97]</ref>. Such investigations can help researchers better understan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roperty prediction <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> and virtual screening <ref type=\"bibr\" target=\"#b28\">[29,</ get=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b61\">62]</ref>. For readout operat f type=\"bibr\" target=\"#b59\">60]</ref> with edge feature involved in aggregation is compared. D-MPNN <ref type=\"bibr\" target=\"#b27\">[28]</ref> and MGCNN <ref type=\"bibr\" target=\"#b73\">[74]</ref>, which <ref type=\"bibr\" target=\"#b90\">91]</ref>. MPNN <ref type=\"bibr\" target=\"#b12\">[13]</ref> and D-MPNN <ref type=\"bibr\" target=\"#b27\">[28]</ref> implement a message-passing architecture to aggregate the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on, many recent works exploit Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, and have shown promising results in molecular property pre ive, while others are denoted as negative. A widely-used GNN model, Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b25\">[26]</ref>, is pretrained through MolCLR to extract informative repre ducing a meanpooling over the node itself and its adjacencies before the linear transformation. GIN <ref type=\"bibr\" target=\"#b25\">[26]</ref> utilizes an MLP and weighted summation of node features in e=\"bibr\" target=\"#b36\">[37]</ref> contrastive loss. In our case, we implement the commonly-used GIN <ref type=\"bibr\" target=\"#b25\">[26]</ref> aggregation operation and an average pooling as the readou  bond is embedded by its type and direction. We implement a 5-layer Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b25\">[26]</ref> with ReLU activation <ref type=\"bibr\" target=\"#b54\">[55]</ as the input. Besides, state-of-the-art graph-based neural networks are also included. Extended GIN <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref> with edge feature . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onventional molecular representations, like SMILES <ref type=\"bibr\" target=\"#b7\">[8]</ref> and ECFP <ref type=\"bibr\" target=\"#b8\">[9]</ref>, have became standard tools in computational chemistry. Rece es are represented in unique fingerprint vectors, such as Extended-Connectivity Fingerprints (ECFP) <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Given the fingerprints, deep neural networks are built to p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ure involved in aggregation is compared. D-MPNN <ref type=\"bibr\" target=\"#b27\">[28]</ref> and MGCNN <ref type=\"bibr\" target=\"#b73\">[74]</ref>, which are graph neural network models designed specifical. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch the scores of anomalies deviate from those of normal nodes.</p><p>According to previous studies <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, Gaussian distribu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: satisfactory results as they might still require a relatively large percentage of positive examples <ref type=\"bibr\" target=\"#b22\">[23]</ref>. To handle such incomplete supervision challenge <ref type a reference score for guiding the subsequent anomaly score learning. By leveraging a deviation loss <ref type=\"bibr\" target=\"#b22\">[23]</ref>, GDN is able to enforce statistically significant deviatio ng to the computed anomaly scores with few-shot labels. Here we propose to adopt the deviation loss <ref type=\"bibr\" target=\"#b22\">[23]</ref> to enforce the model to assign large anomaly scores to tho ion as defined in <ref type=\"bibr\" target=\"#b20\">[21]</ref> and is used as the evaluation metric in <ref type=\"bibr\" target=\"#b22\">[23]</ref>. \u2022 Precision@K is defined as the proportion of true anomal those of normal nodes.</p><p>According to previous studies <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, Gaussian distribution is commonly a robust choice to fit t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. In essence, the goal of meta-learning is to train a model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  try to measure the abnormality of nodes with the reconstruction errors of autoencoder-based models <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> or the residuals of  nt anomaly detection methods:</p><p>\u2022 AUC-ROC is widely used in previous anomaly detection research <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Area under curve (A of network anomaly detection using graph neural networks due to its strong modeling power. DOMINANT <ref type=\"bibr\" target=\"#b5\">[6]</ref> achieves superior performance over other shallow methods by  erizing the residuals of attribute information and its coherence with network structure. \u2022 DOMINANT <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a GCN-based autoencoder framework which computes anomaly . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ks has drawn increasing research attention in the community, and various methods have been proposed <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Among them, ConOu ef type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Among them, ConOut <ref type=\"bibr\" target=\"#b21\">[22]</ref> identifies the local context for each node and performs an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al model which adopts hierarchical attention to model the multi-view graph for fraud detection. GAS <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a GCN-based large-scale anti-spam method for detecting . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Zhao et al. propose a novel loss function to train GNNs for anomaly-detectable node representations <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Apart from the aforementioned methods, our approach focus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mal nodes.</p><p>anomalies 1 , whose patterns significantly deviate from the vast majority of nodes <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> to team collaboration networks <ref type=\"bibr\" target=\"#b52\">[53]</ref>, from citation networks <ref type=\"bibr\" target=\"#b31\">[32]</ref> to molecular graphs <ref type=\"bibr\" target=\"#b43\">[44]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \u2022 LOF [3] is a feature-based approach which detects outliers at the contextual level. \u2022 Autoencoder <ref type=\"bibr\" target=\"#b48\">[49]</ref> is a feature-based unsupervised deep autoencoder model whi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  previous research <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Table <ref type=\"t ef> summarizes the statistics of each dataset. The detailed description is as follows:</p><p>\u2022 Yelp <ref type=\"bibr\" target=\"#b23\">[24]</ref> is collected from Yelp.com and contains reviews for restau. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w different users, rendering properties like homophily not applicable to this type of relationships <ref type=\"bibr\" target=\"#b8\">[9]</ref>. As the existence of even few abnormal instances could cause. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rent manifolds. Previous studies on cross-network learning <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> mostly focus on transferring the knowledge only from a sing s on transferring the knowledge from only a single network <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which may cause negative transfer due to the divergent cha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, and here we employ Simple Graph Convolution (SGC) <ref typ 5,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, and here we employ Simple Graph Convolution (SGC) <ref type=\"bibr\" target=\"#b38\">[39]</ref> in our implementation. Abnormality Valuator. Afterwards, t lgorithm. Implementation Details. Regarding the proposed GDN model, we use Simple Graph Convolution <ref type=\"bibr\" target=\"#b38\">[39]</ref> to build the network encoder with degree \ud835\udc3e = 2 (two layers. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. In essence, the goal of meta-learning is to train a model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  substructure in which a small set of nodes are much more closely linked to each other than average <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Accordingly, we randomly select \ud835\udc50 nodes (i.e., clique siz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  try to measure the abnormality of nodes with the reconstruction errors of autoencoder-based models <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> or the residuals of  nt anomaly detection methods:</p><p>\u2022 AUC-ROC is widely used in previous anomaly detection research <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Area under curve (A of network anomaly detection using graph neural networks due to its strong modeling power. DOMINANT <ref type=\"bibr\" target=\"#b5\">[6]</ref> achieves superior performance over other shallow methods by  erizing the residuals of attribute information and its coherence with network structure. \u2022 DOMINANT <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a GCN-based autoencoder framework which computes anomaly . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f the broad applications in a variety of high-impact domains <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> to team collaboration networks <ref type=\"bibr\" target=\"#b52\">[53]</ref>, from citation networks <ref type=\"bibr\" target=\"#b31\">[32]</ref> to molecular graphs <ref type=\"bibr\" target=\"#b43\">[44]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nging from social networks <ref type=\"bibr\" target=\"#b44\">[45]</ref> to team collaboration networks <ref type=\"bibr\" target=\"#b52\">[53]</ref>, from citation networks <ref type=\"bibr\" target=\"#b31\">[32. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: different networks may be from very different manifolds. Previous studies on cross-network learning <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> mostly focus on tr  reason is that those methods merely focus on transferring the knowledge from only a single network <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which may cause n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent-based (GB) search; 2) reinforcement learning (RL) search; and 3) evolutionary algorithms (EAs). <ref type=\"bibr\" target=\"#b38\">Real et al. (2019)</ref> conducted a case study of the different sear arch using deep CNNs is attracting more and more attention in the artificial intelligence community <ref type=\"bibr\" target=\"#b38\">(Real et al., 2019;</ref><ref type=\"bibr\" target=\"#b44\">Wang et al. 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  land-use identification and urban planning <ref type=\"bibr\" target=\"#b5\">(Cheng et al., 2015;</ref><ref type=\"bibr\" target=\"#b57\">Zhao et al., 2019)</ref>. However, scene classification is still an a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et is one of the earliest deep CNNs, and has been employed in many image scene classification tasks <ref type=\"bibr\" target=\"#b35\">(Nogueira et al., 2017;</ref><ref type=\"bibr\" target=\"#b21\">Han et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Ma et al., 2015)</ref>, subpixel mapping <ref type=\"bibr\" target=\"#b41\">(Song et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Ma et al., 2018)</ref>, hyperspectral feature selection <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b52\">(Yao, 1993;</ref><ref type=\"bibr\" target=\"#b54\">Yao and Liu, 1997;</ref><ref type=\"bibr\" target=\"#b53\">Yao, 1999)</ref> suggested that neuroevolution is a different kind of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k should also be considered for a comprehensive evaluation <ref type=\"bibr\">(Lu et al., 2019c;</ref><ref type=\"bibr\" target=\"#b43\">Tan et al., 2019)</ref>. Thus, the computational complexity and the t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6\">(Wan et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Alok et al., 2016)</ref>, subpixel mapping <ref type=\"bibr\" target=\"#b41\">(Song et al., 2019)</ref>, sparse unmixing <ref type=\"bibr\" target=\"# s such as image clustering <ref type=\"bibr\" target=\"#b34\">(Ma et al., 2015)</ref>, subpixel mapping <ref type=\"bibr\" target=\"#b41\">(Song et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Ma et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h capability of the EA and the local search capability of the Bayesian optimization algorithm (BOA) <ref type=\"bibr\" target=\"#b37\">(Pelikan et al., 1999)</ref>.</p></div> <div xmlns=\"http://www.tei-c. he proposed SceneNet method. Moreover, a Bayesian network is proposed, which is inspired by the BOA <ref type=\"bibr\" target=\"#b37\">(Pelikan et al., 1999)</ref>.</p><p>The BOA is designed to optimize t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b55\">(Zhang et al., 2018)</ref>, and hyperspectral image sparse unmixing <ref type=\"bibr\" target=\"#b50\">(Xu and Shi, 2017)</ref>. Thus, in a NAS-based scene classification n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\">Lu et al. (2019a)</ref>, <ref type=\"bibr\" target=\"#b20\">Han et al. (2018)</ref>, and <ref type=\"bibr\" target=\"#b60\">Zhu et al. (2019)</ref>.</p><p>However, in order to design a satisfac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -level features, and have emerged as a dominant paradigm in pattern recognition and computer vision <ref type=\"bibr\" target=\"#b4\">(Chen and Liu, 2018)</ref>. In deep learning technology, convolutional. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion to particular regions in images <ref type=\"bibr\" target=\"#b37\">[38]</ref> or words in sentences <ref type=\"bibr\" target=\"#b38\">[39]</ref> in order to extract useful information of interest efficie rforming average-pooling over multiple frames to obtain video-level features, we use self-attention <ref type=\"bibr\" target=\"#b38\">[39]</ref> to assign weights to different frames to obtain a video-le formula><formula xml:id=\"formula_5\">)</formula><p>This is similar in spirit to multi-head attention <ref type=\"bibr\" target=\"#b38\">[39]</ref> that is widely used in transformer models for NLP tasks.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: utilize a VGG16 model <ref type=\"bibr\" target=\"#b36\">[37]</ref> pretrained on the Places365 dataset <ref type=\"bibr\" target=\"#b56\">[57]</ref> to extract scene features. We take the output from the las. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b22\">[23]</ref> and end-to-end models <ref type=\"bibr\" target=\"#b23\">[24]</ref>- <ref type=\"bibr\" target=\"#b27\">[28]</ref>). In contrast to these approaches that aim to model tempor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orks is motivated by human vision systems that pay visual attention to particular regions in images <ref type=\"bibr\" target=\"#b37\">[38]</ref> or words in sentences <ref type=\"bibr\" target=\"#b38\">[39]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s (sometimes they are even in disparate modalities space <ref type=\"bibr\" target=\"#b14\">[15]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>) is difficult, as they are usually high-dimensional. There. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d more importantly, there are large intra-class variations <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. For example, a \"dancing\" event can be captured from differe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: citly to assist the recognition of categories of interest <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This makes intuitive sense. For example, it is likely tha e usually performed in a single step, combining all features directly as inputs of a neural network <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref> or a kernel SVM NN model trained to recognize 15 000 objects can achieve decent performance for action recognition. <ref type=\"bibr\" target=\"#b11\">[12]</ref> designs an object and scene fusion network to discover how  representations are used as contextual clues to assist the recognition of video classes. Following <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we extract hig  and then concatenates these encodings to train a classifier; (6) Object Scene Fusion network (OSF) <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which uses a network to combine three features for predic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: opted fusion methods, i.e., early fusion and late fusion <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Early fusion aims to combine features directly and train . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and regularised filter learning <ref type=\"bibr\" target=\"#b4\">[5]</ref>- <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Most existing DCF  light the most discriminative and important visual information of a target. To this end, both fixed <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and adaptive spatia colour-histogram-based image segmentation to suppress the background area. In the same spirit, BACF <ref type=\"bibr\" target=\"#b4\">[5]</ref> employs a predefined binary matrix to crop valid training sa realised by a spatial regularisation have been widely studied in recent advanced DCF-based trackers <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t F <ref type=\"bibr\" target=\"#b15\">[16]</ref>, GFSDCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, BACF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SRDCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Staple <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>. First, the extracted multi-channel features maps, which m ST <ref type=\"bibr\" target=\"#b55\">[56]</ref>, MCPF <ref type=\"bibr\" target=\"#b56\">[57]</ref>, STRCF <ref type=\"bibr\" target=\"#b17\">[18]</ref>, LADCF <ref type=\"bibr\" target=\"#b12\">[13]</ref>, SiamFC <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uately explored <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bib  of channels, include irrelevant and redundant information <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The filters trained with such feature maps often contain  ion redundancy of the feature maps in DCF-based tracking <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Besides, for spatial regularisation, existing DCF tracker ture extraction <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>. However, these CF-based trackers <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In this work,   visual object tracking approaches, such as ASRCF <ref type=\"bibr\" target=\"#b15\">[16]</ref>, GFSDCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, BACF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SRDCF <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nd of limited interpretability.</p><p>Regarding DCF approaches, thanks to the seminal work of MOSSE <ref type=\"bibr\" target=\"#b35\">[36]</ref>, DCF has received much attention in visual object tracking. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: produced by the implicit assumption of periodicity of the input signal induced by circulant samples <ref type=\"bibr\" target=\"#b38\">[39]</ref>, a variety of advanced DCF-based trackers have been propos introduced a fixed opposite Gaussian-shaped spatial weighting mask for correlation filters learning <ref type=\"bibr\" target=\"#b38\">[39]</ref>, concentrating the energy of the learned filters on the ce #b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In this work, noting that each feature map (channel) refl SDCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, BACF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SRDCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Staple <ref type=\"bibr\" target=\"#b51\">[52]</ref>, STAPLE_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, LaSOT <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GOT-10K  convincing results in recent benchmarks and competitions <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>- <ref type=\"bib ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, LaSOT <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GOT-10K  LSART <ref type=\"bibr\" target=\"#b58\">[59]</ref>, DRT <ref type=\"bibr\" target=\"#b59\">[60]</ref>, MFT <ref type=\"bibr\" target=\"#b22\">[23]</ref>, SiamRPN <ref type=\"bibr\" target=\"#b31\">[32]</ref>, GCT <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>. First, the ext  <ref type=\"bibr\" target=\"#b60\">[61]</ref>, GradNet <ref type=\"bibr\" target=\"#b61\">[62]</ref>, UPDT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TADT <ref type=\"bibr\" target=\"#b62\">[63]</ref> and fdKCF . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: A <ref type=\"bibr\" target=\"#b6\">[7]</ref>, MD-Net <ref type=\"bibr\" target=\"#b52\">[53]</ref>, GOTURN <ref type=\"bibr\" target=\"#b53\">[54]</ref>, CF2 <ref type=\"bibr\" target=\"#b54\">[55]</ref> CSRDCF <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: N <ref type=\"bibr\" target=\"#b31\">[32]</ref>, GCT <ref type=\"bibr\" target=\"#b60\">[61]</ref>, GradNet <ref type=\"bibr\" target=\"#b61\">[62]</ref>, UPDT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TADT <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on a number of well-known benchmarks, i.e. OTB2015 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018 3 DCF tracker on four benchmarks including OTB2015 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b22\">[23]</ref>, LaSOT <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GOT-10K <ref type=\"bibr\" target=\"#b24\">[25]</ref> and TrackingNet <ref type=\"bibr\" target=\"#b25\">[26]</ref>. <ref type=\"bibr\" target=\"#b22\">[23]</ref>, LaSOT <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GOT-10K <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Track-ingNet <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed.</p><p>r An extensive evaluation is performed on a number of well-known benchmarks, i.e. OTB2015 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 < the problem of boundary effect and achieve convincing results in recent benchmarks and competitions <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib  Settings</head><p>We extensively evaluate our A 3 DCF tracker on four benchmarks including OTB2015 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref type=\"bibr\" target=\"#b55\">[56]</ref>, MCPF <ref type=\"bibr\" target=\"#b56\">[57]</ref>, STRCF <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b27\">[28]</ref>, StructSiam <ref type=\"bibr\" target=\"#b57\">[58]</ref>, LSART <ref type=\"bibr\" target=\"#b58\">[59]</ref>, DRT <ref type=\"bibr\" target=\"#b59\">[60]</ref>, MFT <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  centre location, other approaches were developed to support more complicated network constructions <ref type=\"bibr\" target=\"#b28\">[29]</ref>- <ref type=\"bibr\" target=\"#b30\">[31]</ref>. To accurately . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and adaptive spatial regularisation <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b15\">[16]</ref> have been explo riminative data fitting, have not been adequately explored <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib few works focusing on reducing the information redundancy of the feature maps in DCF-based tracking <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Besides, for s  CN and CNN, collaboratively for robust feature extraction <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib mples. In contrast to a fixed spatial mask, an adaptive spatial regularisation is proposed in LADCF <ref type=\"bibr\" target=\"#b12\">[13]</ref> to learn an adaptive DCF via spatial feature selection. Th F <ref type=\"bibr\" target=\"#b56\">[57]</ref>, STRCF <ref type=\"bibr\" target=\"#b17\">[18]</ref>, LADCF <ref type=\"bibr\" target=\"#b12\">[13]</ref>, SiamFC <ref type=\"bibr\" target=\"#b26\">[27]</ref>, DSiam < T \u03bb 2 + V j ( X) T V j ( X) \u00d7 (V j ( X) \u0176j +\u03bb 2 V j ( \u0134r )).<label>(13)</label></formula><p>In Eqn. <ref type=\"bibr\" target=\"#b12\">(13)</ref>, J r can be computed directly with predefined P r . Once J. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: racker <ref type=\"bibr\" target=\"#b0\">[1]</ref> that embeds kernel tricks in the circulant structure <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Considering the importance of robust feature representati circular convolution operator <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With the circulant structure <ref type=\"bibr\" target=\"#b36\">[37]</ref> and Fourier transform <ref type=\"bibr\" target=\"#b48\">[49]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed.</p><p>r An extensive evaluation is performed on a number of well-known benchmarks, i.e. OTB2015 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 < the problem of boundary effect and achieve convincing results in recent benchmarks and competitions <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib  Settings</head><p>We extensively evaluate our A 3 DCF tracker on four benchmarks including OTB2015 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and thus results in the use of more sophisticated features <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and regularised filter learning <ref type=\"bibr\" target=\"#b4 on, spatial attention mechanisms and discriminative data fitting, have not been adequately explored <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" features maps, which may exceed thousands of channels, include irrelevant and redundant information <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The filters trai  such feature maps often contain negligible energy and may degrade the performance of a DCF tracker <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>. So far there are ually use multiple features, such as HOG, CN and CNN, collaboratively for robust feature extraction <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\"  CSRDCF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref type=\"bibr\" target=\"#b55\">[56]</ref>, MCPF <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[20]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>- <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Nevertheless, these methods enforce the same attention me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nse <ref type=\"bibr\" target=\"#b26\">[27]</ref>. In view of the diversity of target categories, CFnet <ref type=\"bibr\" target=\"#b27\">[28]</ref> was proposed to generate adaptive filters with an addition C <ref type=\"bibr\" target=\"#b26\">[27]</ref>, DSiam <ref type=\"bibr\" target=\"#b29\">[30]</ref>, CFNet <ref type=\"bibr\" target=\"#b27\">[28]</ref>, StructSiam <ref type=\"bibr\" target=\"#b57\">[58]</ref>, LSA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: matrix structure, which simplifies the filter optimisation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref> and thus results in the use of more sophisticated features < eature extraction is crucial to advanced DCF training, Henriques et al. adopted HOG features in KCF <ref type=\"bibr\" target=\"#b1\">[2]</ref> and <ref type=\"bibr\">Danelljan et al.</ref> proposed to empl  \u03bb is a regularisation parameter. \u2022 F means Frobenius norm and is the circular convolution operator <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With the circulant structure <ref type=\"bibr\" target=\"#b36\" t=\"#b48\">[49]</ref>, the optimisation of Eqn. (1) can efficiently be solved in the frequency domain <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>To prevent their temporal degradation, the final filt e updated by utilising the same updating strategy as other DCF-based trackers, as presented in Eqn. <ref type=\"bibr\" target=\"#b1\">(2)</ref>. Additionally, similar to many other DCF-based trackers, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and adaptive spatial regularisation <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b15\">[16]</ref> have been explo riminative data fitting, have not been adequately explored <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib few works focusing on reducing the information redundancy of the feature maps in DCF-based tracking <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Besides, for s  CN and CNN, collaboratively for robust feature extraction <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib mples. In contrast to a fixed spatial mask, an adaptive spatial regularisation is proposed in LADCF <ref type=\"bibr\" target=\"#b12\">[13]</ref> to learn an adaptive DCF via spatial feature selection. Th F <ref type=\"bibr\" target=\"#b56\">[57]</ref>, STRCF <ref type=\"bibr\" target=\"#b17\">[18]</ref>, LADCF <ref type=\"bibr\" target=\"#b12\">[13]</ref>, SiamFC <ref type=\"bibr\" target=\"#b26\">[27]</ref>, DSiam < T \u03bb 2 + V j ( X) T V j ( X) \u00d7 (V j ( X) \u0176j +\u03bb 2 V j ( \u0134r )).<label>(13)</label></formula><p>In Eqn. <ref type=\"bibr\" target=\"#b12\">(13)</ref>, J r can be computed directly with predefined P r . Once J. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [9]</ref> and Convolutional Neural Network (CNN) features <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For each feature type, multiple channel features capturin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: A <ref type=\"bibr\" target=\"#b6\">[7]</ref>, MD-Net <ref type=\"bibr\" target=\"#b52\">[53]</ref>, GOTURN <ref type=\"bibr\" target=\"#b53\">[54]</ref>, CF2 <ref type=\"bibr\" target=\"#b54\">[55]</ref> CSRDCF <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b23\">[24]</ref>, GOT-10K <ref type=\"bibr\" target=\"#b24\">[25]</ref> and TrackingNet <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The experimental results demonstrate the superior perform \"bibr\" target=\"#b23\">[24]</ref>, GOT-10K <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Track-ingNet <ref type=\"bibr\" target=\"#b25\">[26]</ref>, compared with numerous state-of-the-art visual object tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rly, the semantic information captured by different deep CNN feature channels is typically distinct <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>Based on the above observation, we propose to perfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eloped to support more complicated network constructions <ref type=\"bibr\" target=\"#b28\">[29]</ref>- <ref type=\"bibr\" target=\"#b30\">[31]</ref>. To accurately predict the bounding box, the SiameseRPN fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: idge regression problem with a circulant matrix structure, which simplifies the filter optimisation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref> and thus results in eived much attention in visual object tracking, especially since the development of the CSK tracker <ref type=\"bibr\" target=\"#b0\">[1]</ref> that embeds kernel tricks in the circulant structure <ref ty otes the corresponding k-th channel filter. Y \u2208 R N \u00d7N stands for the desired detector response map <ref type=\"bibr\" target=\"#b0\">[1]</ref> of Gaussian shape. K is the number of feature channels and \u03bb. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  competitions <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>- <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Nevertheless, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net <ref type=\"bibr\" target=\"#b61\">[62]</ref>, UPDT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TADT <ref type=\"bibr\" target=\"#b62\">[63]</ref> and fdKCF <ref type=\"bibr\" target=\"#b63\">[64]</ref> respec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b1\">[2]</ref> and thus results in the use of more sophisticated features <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and regularised fil CF2 <ref type=\"bibr\" target=\"#b54\">[55]</ref> CSRDCF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net <ref type=\"bibr\" target=\"#b61\">[62]</ref>, UPDT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TADT <ref type=\"bibr\" target=\"#b62\">[63]</ref> and fdKCF <ref type=\"bibr\" target=\"#b63\">[64]</ref> respec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, LaSOT   <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, LaSOT . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: N <ref type=\"bibr\" target=\"#b31\">[32]</ref>, GCT <ref type=\"bibr\" target=\"#b60\">[61]</ref>, GradNet <ref type=\"bibr\" target=\"#b61\">[62]</ref>, UPDT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TADT <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b1\">[2]</ref> and thus results in the use of more sophisticated features <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and regularised fil CF2 <ref type=\"bibr\" target=\"#b54\">[55]</ref> CSRDCF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b1\">[2]</ref> and thus results in the use of more sophisticated features <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and regularised fil CF2 <ref type=\"bibr\" target=\"#b54\">[55]</ref> CSRDCF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [9]</ref> and Convolutional Neural Network (CNN) features <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For each feature type, multiple channel features capturin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Names (CN) <ref type=\"bibr\" target=\"#b8\">[9]</ref> and Convolutional Neural Network (CNN) features <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For each featur /p><formula xml:id=\"formula_17\">\u0124k = arg min \u0124k \u03bb 1 2 \u0124k 2 F + \u03bc 2 \u011ck \u2212 \u0124k + \u0393k \u03bc 2 F</formula><p>. <ref type=\"bibr\" target=\"#b9\">(10)</ref> This sub-problem can be easily solved by setting the deriva. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eloped to support more complicated network constructions <ref type=\"bibr\" target=\"#b28\">[29]</ref>- <ref type=\"bibr\" target=\"#b30\">[31]</ref>. To accurately predict the bounding box, the SiameseRPN fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2]</ref> and <ref type=\"bibr\">Danelljan et al.</ref> proposed to employ CN features <ref type=\"bibr\" target=\"#b42\">[43]</ref>, resulting in improved performance, as compared with the t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>. However, these DCF-based trackers simply assemble all the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b30\">[31]</ref>. To accurately predict the bounding box, the SiameseRPN framework <ref type=\"bibr\" target=\"#b31\">[32]</ref>- <ref type=\"bibr\" target=\"#b34\">[35]</ref> was designed to T <ref type=\"bibr\" target=\"#b59\">[60]</ref>, MFT <ref type=\"bibr\" target=\"#b22\">[23]</ref>, SiamRPN <ref type=\"bibr\" target=\"#b31\">[32]</ref>, GCT <ref type=\"bibr\" target=\"#b60\">[61]</ref>, GradNet <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref type=\"bibr\" target=\"#b55\">[56]</ref>, MCPF <ref type=\"bibr\" target=\"#b56\">[57]</ref>, STRCF <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esses of the attention mechanism on various applications <ref type=\"bibr\" target=\"#b31\">[31]</ref>, <ref type=\"bibr\" target=\"#b38\">[38]</ref>- <ref type=\"bibr\" target=\"#b40\">[40]</ref>, we build a sup. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b30\">[30]</ref>, many researchers have applied attention mechanisms to computer vision. Mnih et al. <ref type=\"bibr\" target=\"#b31\">[31]</ref> first used the attention mechanism with recurrent neural n  localization.</p><p>Encouraged by the successes of the attention mechanism on various applications <ref type=\"bibr\" target=\"#b31\">[31]</ref>, <ref type=\"bibr\" target=\"#b38\">[38]</ref>- <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ce temporal boundaries of actions and many other methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b16\">[16]</ref> generate proposals as candidate action instances for local  target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" tar rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" tar  target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24]</ref> and [0.6, 1, 2, 3,  rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n two challenging datasets: THUMOS2014 <ref type=\"bibr\" target=\"#b44\">[44]</ref> and ActivityNet1.3 <ref type=\"bibr\" target=\"#b45\">[45]</ref>.</p><p>The THUMOS2014 dataset contains videos from 20 clas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s in a video for action localization by learning action prototypes and actions jointly. Buch et al. <ref type=\"bibr\" target=\"#b25\">[25]</ref> employed a temporal segment network (TSN) <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tention-based LSTM to capture the long-term dependence and find the salient portions. Nguyen et al. <ref type=\"bibr\" target=\"#b34\">[34]</ref> used the attention mechanism to find the background or act. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  ActivityNet1.3 dataset as it does on the THUMOS2014 dataset compared with several existing methods <ref type=\"bibr\" target=\"#b58\">[58]</ref>, <ref type=\"bibr\" target=\"#b59\">[59]</ref>, probably due t  that the boundaries of long action instances are captured. Our method performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58] ethod performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> conducts frame-level predictions rather than segment-level 1.3 dataset. Nevertheless, our method yields a higher mAP at a threshold of 0.95 than the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref>, which indicates that our method locates the action bounda e difficult scenarios. Furthermore, although the average mAP of our method is 4% worse than that of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the ActivityNet1.3 dataset, our method achieves a signi ef> on the ActivityNet1.3 dataset, our method achieves a significant improvement over the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the THU-MOS2014 dataset, and the mAP at the threshold o 62\">[62]</ref> and two framelevel proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> for comparison, and the inference speed is directly copied od is slower than the frame-level proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> that perform fully convolutional operations on the frame l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\">64</ref>] on the ActivityNet1.3 dataset. Th get=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b48\">48,</ref><ref type=\"bibr\" target=\"#b56\">56]</ref> on the THUMOS2014 and ActivityNet1.3 datasets, respectively ream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods usually use average pooling or concatenatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\">64</ref>] on the ActivityNet1.3 dataset. Th get=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b48\">48,</ref><ref type=\"bibr\" target=\"#b56\">56]</ref> on the THUMOS2014 and ActivityNet1.3 datasets, respectively ream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods usually use average pooling or concatenatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: treated as the appearance features of segments. For the motion features, we follow the operation in <ref type=\"bibr\" target=\"#b50\">[50]</ref> and extract the 400dimensional feature vectors from the TS  means that the NMS is not used. We use the conventional average recall with 100 proposals (AR@100) <ref type=\"bibr\" target=\"#b50\">[50]</ref> to evaluate the performance of the proposal generator. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: treated as the appearance features of segments. For the motion features, we follow the operation in <ref type=\"bibr\" target=\"#b50\">[50]</ref> and extract the 400dimensional feature vectors from the TS  means that the NMS is not used. We use the conventional average recall with 100 proposals (AR@100) <ref type=\"bibr\" target=\"#b50\">[50]</ref> to evaluate the performance of the proposal generator. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2]</ref> resort to sliding windows to produce temporal boundaries of actions and many other methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b16\">[16]</ref> generate propos  recurrent sequence encoder to aggregate video segments for generating action proposals. Gao et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> used a cascaded boundary regression model to produce class p><p>(5) STAN also performs better than the state-of-the-art methods using deep two-stream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to the successes of deep learning on various visual tasks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, especially on video analysis <ref type=\"bibr\" target=\"#b7\"> Mechanism</head><p>Inspired by the successes of attention mechanisms in natural language processing <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b29\">[29]</ref>, <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n two challenging datasets: THUMOS2014 <ref type=\"bibr\" target=\"#b44\">[44]</ref> and ActivityNet1.3 <ref type=\"bibr\" target=\"#b45\">[45]</ref>.</p><p>The THUMOS2014 dataset contains videos from 20 clas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, because it effectively couples the attention mechanism an  state-of-the-art methods using deep two-stream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uage processing <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b29\">[29]</ref>, <ref type=\"bibr\" target=\"#b30\">[30]</ref>, many researchers have applied attention mechanisms to com. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st cases. Concretely, STAN outperforms RNN-based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, because it eff ng approaches <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b51\">[51]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref> on the THUMOS20 \">[14]</ref>, <ref type=\"bibr\" target=\"#b51\">[51]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref> on the THUMOS2014 dataset. It is interesting to notice tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ough there have been numerous studies conducted on temporal action localization in untrimmed videos <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>, achieving accurate e frames. All segment features are normalized using L2-normalization. The segment scales are set to <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> on the THUMOS2014 dataset and <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target  ActivityNet1.3 dataset. The overlap segment of sliding windows with different scales is set to [0, <ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tention-based LSTM to capture the long-term dependence and find the salient portions. Nguyen et al. <ref type=\"bibr\" target=\"#b34\">[34]</ref> used the attention mechanism to find the background or act. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ment scales are set to <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= THUMOS2014 dataset and <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= nt scales is set to [0, <ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ment scales are set to <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= THUMOS2014 dataset and <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= nt scales is set to [0, <ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, because it effectively couples the attention mechanism an  state-of-the-art methods using deep two-stream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on system. They also used a post-processing method to boost the localization performance. Xu et al. <ref type=\"bibr\" target=\"#b21\">[21]</ref> extracted CNN features and improved dense trajectories by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xploit the temporal context of actions for generating proposals and classifying actions. Gao et al. <ref type=\"bibr\" target=\"#b27\">[27]</ref> presented a temporal unit regression network to classify a e generated proposal contains an action instance. Moreover, we adopt the boundary regression method <ref type=\"bibr\" target=\"#b27\">[27]</ref> to accurately locate the boundary of the action.</p><p>We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  ActivityNet1.3 dataset as it does on the THUMOS2014 dataset compared with several existing methods <ref type=\"bibr\" target=\"#b58\">[58]</ref>, <ref type=\"bibr\" target=\"#b59\">[59]</ref>, probably due t  that the boundaries of long action instances are captured. Our method performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58] ethod performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> conducts frame-level predictions rather than segment-level 1.3 dataset. Nevertheless, our method yields a higher mAP at a threshold of 0.95 than the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref>, which indicates that our method locates the action bounda e difficult scenarios. Furthermore, although the average mAP of our method is 4% worse than that of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the ActivityNet1.3 dataset, our method achieves a signi ef> on the ActivityNet1.3 dataset, our method achieves a significant improvement over the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the THU-MOS2014 dataset, and the mAP at the threshold o 62\">[62]</ref> and two framelevel proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> for comparison, and the inference speed is directly copied od is slower than the frame-level proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> that perform fully convolutional operations on the frame l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xploit the temporal context of actions for generating proposals and classifying actions. Gao et al. <ref type=\"bibr\" target=\"#b27\">[27]</ref> presented a temporal unit regression network to classify a e generated proposal contains an action instance. Moreover, we adopt the boundary regression method <ref type=\"bibr\" target=\"#b27\">[27]</ref> to accurately locate the boundary of the action.</p><p>We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: round and fully exploit action informativeness in a video. Closely related to our work, Buch et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> used semantically constrained recurrent memory modules to  ethod still performs better than them in most cases. Concretely, STAN outperforms RNN-based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bib get=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" tar get=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  temporal convolutions. Based on the work of <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Chao et al. <ref type=\"bibr\" target=\"#b26\">[26]</ref> improved receptive field alignment to exploit the temporal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  temporal convolutions. Based on the work of <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Chao et al. <ref type=\"bibr\" target=\"#b26\">[26]</ref> improved receptive field alignment to exploit the temporal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o address challenging implicit feedback in recommendations <ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>. In this scenario o generate feature and then make recommendation via a RNN network structure with our previous study <ref type=\"bibr\" target=\"#b14\">[15]</ref>. However, However, in addition to application scenario, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Reference Corpus <ref type=\"bibr\" target=\"#b51\">[52]</ref>, Scholarly Paper Recommendation Dataset <ref type=\"bibr\" target=\"#b52\">[53]</ref>, Test Collection for Bibliographic Citation Recommendation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: papers and citations.</p><p>\u2022 PageRank-weighted CF model (PR-CF): This is a classic baseline method <ref type=\"bibr\" target=\"#b57\">[58]</ref>, which extends the conventional itembased CF model with gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iciency of academic resources to a great extent, especially for newcomers and students in this area <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In addition, as one of the important carriers of academics . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l TF-IDF-based word-level paper representations are proposed <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Apart from the TF-IDF model, there are also other approach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a length-limited inquiry text for recommendation and user's identity for personalized customization <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Under these requirements, the text feature used for the r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ending reference for draft completion, guiding beginners, keeping track of the current developments <ref type=\"bibr\" target=\"#b18\">[19]</ref>, etc. In this section, we review this representative liter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rogeneous data can be constructed by different methods according to their different data modalities <ref type=\"bibr\" target=\"#b45\">[46]</ref>. In this paper, we roughly divide these heterogeneous data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (CF), content based filtering (CBF), graph-based (GB) and a fusion of multiple approaches (Hybrid) <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. However, in the real-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: needs could shift while users are searching for papers in different time slices. Chakraborty et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> focused more on user queries, and their proposed system na. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  validate its usefulness with an online trade off strategy <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b61\">62]</ref>. (3) the system-level application scenario of such citation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ative model, when extended to condition on image captions, could also generate novel visual scenes. <ref type=\"bibr\" target=\"#b35\">Reed et al. (2016b)</ref> later demonstrated that using a generative  low et al., 2014)</ref>, rather than a recurrent variational auto-encoder, improved image fidelity. <ref type=\"bibr\" target=\"#b35\">Reed et al. (2016b)</ref> showed that this system could not only gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch as image-to-image translation at a rudimentary level. This previously required custom approaches <ref type=\"bibr\" target=\"#b13\">(Isola et al., 2017)</ref>, rather emerging as a capability of a sing. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: der and decoder are convolutional <ref type=\"bibr\" target=\"#b19\">(LeCun et al., 1998)</ref> ResNets <ref type=\"bibr\" target=\"#b10\">(He et al., 2016)</ref> with bottleneck-style resblocks. The models p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2017)</ref>, with the advantage that specialized GPU kernels are not required. We found that <ref type=\"bibr\" target=\"#b43\">Sun et al. (2020)</ref> had independently developed similar procedure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h the work of <ref type=\"bibr\" target=\"#b25\">Mansimov et al. (2015)</ref>, who showed that the DRAW <ref type=\"bibr\" target=\"#b9\">Gregor et al. (2015)</ref> generative model, when extended to conditio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in several domains such as text <ref type=\"bibr\" target=\"#b30\">(Radford et al., 2019)</ref>, images <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2020)</ref>, and audio <ref type=\"bibr\" target=\"#b7\">(Dh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e\" target=\"#fig_2\">2c</ref>). Prompts like the latter require the model to perform variable binding <ref type=\"bibr\" target=\"#b41\">(Smolensky, 1990)</ref> -it is the hedgehog that is in the christmas . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing the evidence lower bound (ELB) <ref type=\"bibr\" target=\"#b16\">(Kingma &amp; Welling, 2013;</ref><ref type=\"bibr\" target=\"#b36\">Rezende et al., 2014)</ref> on the joint likelihood of the model dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2017)</ref>, with the advantage that specialized GPU kernels are not required. We found that <ref type=\"bibr\" target=\"#b43\">Sun et al. (2020)</ref> had independently developed similar procedure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e\" target=\"#fig_2\">2c</ref>). Prompts like the latter require the model to perform variable binding <ref type=\"bibr\" target=\"#b41\">(Smolensky, 1990)</ref> -it is the hedgehog that is in the christmas . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Zhang et al., 2017;</ref><ref type=\"bibr\">2018)</ref>, integrating attention and auxiliary losses <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, and leveraging additional sources of conditi Results</head><p>We evaluate our model zero-shot by comparing it to three prior approaches: AttnGAN <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, DM-GAN <ref type=\"bibr\" target=\"#b52\">(Zhu e reas et al., 2017)</ref>, and is also similar to the auxiliary text-image matching loss proposed by <ref type=\"bibr\" target=\"#b49\">Xu et al. (2018)</ref>. Unless otherwise stated, all samples used for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t to three prior approaches: AttnGAN <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, DM-GAN <ref type=\"bibr\" target=\"#b52\">(Zhu et al., 2019)</ref>, and DF-GAN <ref type=\"bibr\" target=\"#b44\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Zhang et al., 2017;</ref><ref type=\"bibr\">2018)</ref>, integrating attention and auxiliary losses <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, and leveraging additional sources of conditi Results</head><p>We evaluate our model zero-shot by comparing it to three prior approaches: AttnGAN <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, DM-GAN <ref type=\"bibr\" target=\"#b52\">(Zhu e reas et al., 2017)</ref>, and is also similar to the auxiliary text-image matching loss proposed by <ref type=\"bibr\" target=\"#b49\">Xu et al. (2018)</ref>. Unless otherwise stated, all samples used for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l Captions, a dataset of 3.3 million text-image pairs that was developed as an extension to MS-COCO <ref type=\"bibr\" target=\"#b21\">(Lin et al., 2014)</ref>.</p><p>To scale up to 12-billion parameters,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>.</p><p>To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M <ref type=\"bibr\" target=\"#b42\">(Sun et al., 2017)</ref> by collecting 250 million text-images pairs . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: der and decoder are convolutional <ref type=\"bibr\" target=\"#b19\">(LeCun et al., 1998)</ref> ResNets <ref type=\"bibr\" target=\"#b10\">(He et al., 2016)</ref> with bottleneck-style resblocks. The models p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Zhang et al., 2017;</ref><ref type=\"bibr\">2018)</ref>, integrating attention and auxiliary losses <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, and leveraging additional sources of conditi Results</head><p>We evaluate our model zero-shot by comparing it to three prior approaches: AttnGAN <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, DM-GAN <ref type=\"bibr\" target=\"#b52\">(Zhu e reas et al., 2017)</ref>, and is also similar to the auxiliary text-image matching loss proposed by <ref type=\"bibr\" target=\"#b49\">Xu et al. (2018)</ref>. Unless otherwise stated, all samples used for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in several domains such as text <ref type=\"bibr\" target=\"#b30\">(Radford et al., 2019)</ref>, images <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2020)</ref>, and audio <ref type=\"bibr\" target=\"#b7\">(Dh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h the work of <ref type=\"bibr\" target=\"#b25\">Mansimov et al. (2015)</ref>, who showed that the DRAW <ref type=\"bibr\" target=\"#b9\">Gregor et al. (2015)</ref> generative model, when extended to conditio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b37\">Seemayer et al., 2014)</ref>. In this work we use Potts models fit wi  For comparison, we train the same residual network over co-evolutionary features from Potts models <ref type=\"bibr\" target=\"#b37\">(Seemayer et al., 2014)</ref>. Additionally we compare to features fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b18\">(Jones et al., 2012)</ref>, and the current state-of-the-art pseudolikelihood maximization <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Ekeber itting and ultimately perform better after more training.</p><p>The CCMpred implementation of Potts <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Ekeber The performance of this model is reported in Table <ref type=\"table\" target=\"#tab_8\">7</ref>. Potts <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011)</ref>, TAPE transformer <ref type=\"bibr\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: c.org/ns/1.0\"><head>Pre-training Objective</head><p>We adapt the masked language modeling objective <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> to the MSA setting. The loss for an MSA x,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  ( <ref type=\"formula\">2018</ref>) train deep variational autoencoders on MSAs to predict function. <ref type=\"bibr\" target=\"#b33\">Riesselman et al. (2019)</ref> train autoregressive models on MSAs, b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pervised contact prediction pipeline, features captured by the MSA Transformer outperform trRosetta <ref type=\"bibr\" target=\"#b51\">(Yang et al., 2019)</ref> on the CASP13 and CAMEO test sets. We find  upervised with binned pairwise distance distributions (distograms) using the trRosetta training set <ref type=\"bibr\" target=\"#b51\">(Yang et al., 2019)</ref> of 15,051 MSAs and structures.</p><p>We eva  ProTrans-T5 using the outer concatenation of the sequence embeddings. We also compare to trRosetta <ref type=\"bibr\" target=\"#b51\">(Yang et al., 2019)</ref>, a state-of-the-art supervised structure pr  is trained for 100k updates using a batch size of 512. Contact prediction on the trRosetta dataset <ref type=\"bibr\" target=\"#b51\">(Yang et al., 2019)</ref> is used as a validation task. Precision aft e results for long-range precision over the CASP-13 FM targets and CAMEO-hard domains referenced in <ref type=\"bibr\" target=\"#b51\">(Yang et al., 2019)</ref>. All baseline models are trained for 200 ep \"bibr\" target=\"#b49\">(Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b24\">Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b51\">Yang et al., 2019;</ref><ref type=\"bibr\" target=\"#b38\">Senior et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -field inference <ref type=\"bibr\">(Morcos et al., 2011)</ref>, sparse-inverse covariance estimation <ref type=\"bibr\" target=\"#b18\">(Jones et al., 2012)</ref>, and the current state-of-the-art pseudoli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (Sundaram et al., 2018;</ref><ref type=\"bibr\">Frazer et al., 2020)</ref>, remote homology detection <ref type=\"bibr\" target=\"#b17\">(Hou et al., 2018)</ref>, and protein design <ref type=\"bibr\">(Russ e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b18\">(Jones et al., 2012)</ref>, and the current state-of-the-art pseudolikelihood maximization <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Ekeber itting and ultimately perform better after more training.</p><p>The CCMpred implementation of Potts <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Ekeber The performance of this model is reported in Table <ref type=\"table\" target=\"#tab_8\">7</ref>. Potts <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011)</ref>, TAPE transformer <ref type=\"bibr\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Sequence variation within a protein family conveys information about the structure of the protein <ref type=\"bibr\" target=\"#b52\">(Yanofsky et al., 1964;</ref><ref type=\"bibr\" target=\"#b2\">Altschuh e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: de classification and link prediction, these methods can be prone to discrimination and instability <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al irness and Stability in GNNs. Recent studies addressed the issues of fairness and stability in GNNs <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b11\">, Fisher et al  biases prevalent in the data, but may also exacerbate them thanks to their message passing schemes <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>. Generally, in graphs such as social networ nodes with similar sensitive attribute (e.g., race, age) values are likely to connect to each other <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>. Since GNNs compute node representations by =|P (\u0177 u =1|y u =1, s=0)\u2212P (\u0177 u =1|y u =1, s=1)|, where probabilities are estimated on the test set <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>. To measure counterfactual fairness, we def t al., 2019]</ref>.  Baseline methods and implementation. We consider two baseline methods: FairGCN <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref> and RobustGCN <ref type=\"bibr\" target=\"#b38 ll><cell>12.41\u00b10.54 12.40\u00b11.62</cell><cell>10.16\u00b10.49 10.09\u00b11.55</cell></row></table><note>, FairGCN<ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>) and stability (i.e., RobustGCN<ref type=\"b bility in GNNs as independent problems and proposed standalone solutions for the same. For example, <ref type=\"bibr\" target=\"#b7\">Dai and Wang [2021]</ref> proposed FairGNN to promote fairness in GNNs  work de-biases embeddings with respect to sensitive attributes via adversarial learning frameworks <ref type=\"bibr\" target=\"#b7\">[Dai and</ref><ref type=\"bibr\">Wang, 2021, Bose and</ref><ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  group fairness measures such as statistical parity and equality of opportunity. On the other hand, <ref type=\"bibr\" target=\"#b38\">Zhu et al. [2019]</ref> aimed to make GNNs stable and robust to adver rget=\"#b2\">, Bose and Hamilton, 2019</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b38\">, Zhu et al., 2019</ref><ref type=\"bibr\" target=\"#b37\">, Zhang and Zi e-aggregation <ref type=\"bibr\" target=\"#b13\">[Geisler et al., 2020]</ref>, and attention mechanisms <ref type=\"bibr\" target=\"#b38\">[Zhu et al., 2019]</ref> to defend GNNs against a variety of attacks  wo baseline methods: FairGCN <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref> and RobustGCN <ref type=\"bibr\" target=\"#b38\">[Zhu et al., 2019]</ref>; all hyperparameters are set following the a te>, FairGCN<ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>) and stability (i.e., RobustGCN<ref type=\"bibr\" target=\"#b38\">[Zhu et al., 2019]</ref>) of GNNs. Shown is average performance acros. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: abled critical predictions in downstream applications-e.g., predicting protein-protein interactions <ref type=\"bibr\" target=\"#b12\">[Gainza et al., 2020</ref><ref type=\"bibr\">, Huang et al., 2020]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the issues of fairness and stability in GNNs <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b11\">, Fisher et al., 2020</ref><ref type=\"bibr\" target=\"#b13\">, Geisler e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: which emphasizes that minority groups should receive similar treatment as that of advantaged groups <ref type=\"bibr\" target=\"#b1\">[Berk et al., 2018</ref><ref type=\"bibr\">, Hardt et al., 2016]</ref>, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the issues of fairness and stability in GNNs <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b11\">, Fisher et al., 2020</ref><ref type=\"bibr\" target=\"#b13\">, Geisler e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b38\">, Zhu et al., 2019</ref><ref type=\"bibr\" target=\"#b37\">, Zhang and Zitnik, 2020]</ref>. To achieve fairness, existing work d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b18\">Hamilton et al. [2017]</ref>, <ref type=\"bibr\" target=\"#b32\">Xu et al. [2018</ref><ref type=\"bibr\" target=\"#b33\">Xu et al. [ , 2019]]</ref>, <ref type=\"bibr\" target=\"#b30\">Veli\u010dkovi\u0107 t=\"#b19\">[Hammond et al., 2011</ref><ref type=\"bibr\" target=\"#b8\">, Defferrard et al., 2016]</ref>  <ref type=\"bibr\" target=\"#b33\">[Xu et al., 2019]</ref> adaptively adjust the importance weights of n l., 2017]</ref>, Jumping Knowledge (JK) <ref type=\"bibr\" target=\"#b32\">[Xu et al., 2018]</ref>, GIN <ref type=\"bibr\" target=\"#b33\">[Xu et al., 2019]</ref>, and InfoMax <ref type=\"bibr\" target=\"#b30\">[ nting defendants who got released on bail at the U.S state courts during <ref type=\"bibr\">1990</ref><ref type=\"bibr\" target=\"#b33\">-2009</ref><ref type=\"bibr\" target=\"#b23\">[Jordan and Freiburger, 201 s of bail outcomes collected from several state courts in the US between <ref type=\"bibr\">1990</ref><ref type=\"bibr\" target=\"#b33\">-2009</ref><ref type=\"bibr\" target=\"#b23\">[Jordan and Freiburger, 201 enting individuals that we connected based on the similarity of their spending and payment patterns <ref type=\"bibr\" target=\"#b33\">[Yeh and Lien, 2009]</ref>. The task is to predict whether an individ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d neural layer is then used to transform and match the representations with each other. Inspired by <ref type=\"bibr\" target=\"#b16\">Grill et al. [2020]</ref>, we define a triplet-based objective functi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b18\">Hamilton et al. [2017]</ref>, <ref type=\"bibr\" target=\"#b32\">Xu et al. [2018</ref><ref type=\"bibr\" target=\"#b33\">Xu et al. [ , 2019]]</ref>, <ref type=\"bibr\" target=\"#b30\">Veli\u010dkovi\u0107 t=\"#b19\">[Hammond et al., 2011</ref><ref type=\"bibr\" target=\"#b8\">, Defferrard et al., 2016]</ref>  <ref type=\"bibr\" target=\"#b33\">[Xu et al., 2019]</ref> adaptively adjust the importance weights of n l., 2017]</ref>, Jumping Knowledge (JK) <ref type=\"bibr\" target=\"#b32\">[Xu et al., 2018]</ref>, GIN <ref type=\"bibr\" target=\"#b33\">[Xu et al., 2019]</ref>, and InfoMax <ref type=\"bibr\" target=\"#b30\">[ nting defendants who got released on bail at the U.S state courts during <ref type=\"bibr\">1990</ref><ref type=\"bibr\" target=\"#b33\">-2009</ref><ref type=\"bibr\" target=\"#b23\">[Jordan and Freiburger, 201 s of bail outcomes collected from several state courts in the US between <ref type=\"bibr\">1990</ref><ref type=\"bibr\" target=\"#b33\">-2009</ref><ref type=\"bibr\" target=\"#b23\">[Jordan and Freiburger, 201 enting individuals that we connected based on the similarity of their spending and payment patterns <ref type=\"bibr\" target=\"#b33\">[Yeh and Lien, 2009]</ref>. The task is to predict whether an individ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n be prone to discrimination and instability <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b2\">, Bose and H get=\"#b13\">, Geisler et al., 2020</ref><ref type=\"bibr\" target=\"#b2\">, Bose and Hamilton, 2019</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b38\">, Zhu et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b17\">[Gysi et al., 2020</ref><ref type=\"bibr\">, Zitnik et al., 2018]</ref>, crime forecasting <ref type=\"bibr\" target=\"#b22\">[Jin et al., 2020]</ref>, news and product recommendations <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: abled critical predictions in downstream applications-e.g., predicting protein-protein interactions <ref type=\"bibr\" target=\"#b12\">[Gainza et al., 2020</ref><ref type=\"bibr\">, Huang et al., 2020]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, 2) individual fairness, which requires that similar individuals should be treated similarly <ref type=\"bibr\" target=\"#b10\">[Dwork et al., 2012]</ref>, and 3) counterfactual fairness, which cap assification, we use AUROC and F1-score. To quantify group fairness, we use statistical parity (SP) <ref type=\"bibr\" target=\"#b10\">[Dwork et al., 2012]</ref>, defined as: \u2206 SP =|P (\u0177 u =1|s=0)\u2212P (\u0177 u . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b2\">, Bose and Hamilton, 2019]</ref>. Furthermore, prior work has argued t target=\"#b11\">, Fisher et al., 2020</ref><ref type=\"bibr\" target=\"#b13\">, Geisler et al., 2020</ref><ref type=\"bibr\" target=\"#b2\">, Bose and Hamilton, 2019</ref><ref type=\"bibr\" target=\"#b28\">, Rahman. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nstants L 1 and L 2 is a new Lipschitz continuous function with L 1 \u00d7 L 2 as the Lipschitz constant <ref type=\"bibr\" target=\"#b15\">[Gouk et al., 2021]</ref>. Putting it all together, we have:</p><form. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: put images and the ground-truth labels in CV tasks. To this end, Dosovitskiy et al. develop the ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which paves the way for transferring the success of transfo 2 , \u2022 \u2022 \u2022 , X n ] \u2208 R n\u00d7p\u00d7p\u00d73</formula><p>, where (p, p) is the resolution of each image patch. ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref> just utilizes a standard transformer to process the sequence 0 , Z 2 0 , \u2022 \u2022 \u2022 , Z n 0 ] \u2208 R (n+1)\u00d7d</formula><p>where Z class is the class token similar to ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and all of them are initialized as zero. In each layer, the rk Architecture</head><p>We build our TNT architectures by following the basic configuration of ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DeiT <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The patc that of DeiT <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The recent transformerbased models like ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DeiT <ref type=\"bibr\" target=\"#b30\">[31]</ref> are compa ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and a head width of 64 is recommended for visual tasks <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. We adopt the head w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  in DeiT <ref type=\"bibr\" target=\"#b30\">[31]</ref> including random crop, random clip, Rand-Augment <ref type=\"bibr\" target=\"#b6\">[7]</ref>, Random Erasing <ref type=\"bibr\" target=\"#b43\">[44]</ref>, M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: very high, e.g., there are over 120 M images with 1000 different categories in the ImageNet dataset <ref type=\"bibr\" target=\"#b25\">[26]</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_0\">1</ rg/ns/1.0\"><head n=\"3.1\">Datasets and Experimental Settings</head><p>Datasets. ImageNet ILSVRC 2012 <ref type=\"bibr\" target=\"#b25\">[26]</ref> is an image classification benchmark consisting of 1.2M tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eserved compared to DeiT. We also visualize all the 384 feature maps in the 12-th block using t-SNE <ref type=\"bibr\" target=\"#b32\">[33]</ref> (Fig. <ref type=\"figure\" target=\"#fig_3\">3(b)</ref>). We c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eserved compared to DeiT. We also visualize all the 384 feature maps in the 12-th block using t-SNE <ref type=\"bibr\" target=\"#b32\">[33]</ref> (Fig. <ref type=\"figure\" target=\"#fig_3\">3(b)</ref>). We c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: very high, e.g., there are over 120 M images with 1000 different categories in the ImageNet dataset <ref type=\"bibr\" target=\"#b25\">[26]</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_0\">1</ rg/ns/1.0\"><head n=\"3.1\">Datasets and Experimental Settings</head><p>Datasets. ImageNet ILSVRC 2012 <ref type=\"bibr\" target=\"#b25\">[26]</ref> is an image classification benchmark consisting of 1.2M tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b11\">[12]</ref>, RegNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> and EfficientNet <ref type=\"bibr\" target=\"#b27\">[28]</ref>. The results are shown in Table <ref type=\"table\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b11\">[12]</ref>, RegNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> and EfficientNet <ref type=\"bibr\" target=\"#b27\">[28]</ref>. The results are shown in Table <ref type=\"table\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding random crop, random clip, Rand-Augment <ref type=\"bibr\" target=\"#b6\">[7]</ref>, Random Erasing <ref type=\"bibr\" target=\"#b43\">[44]</ref>, Mixup <ref type=\"bibr\" target=\"#b41\">[42]</ref> and CutMi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al networks (CNNs) <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Differently, transformer is a type of neural network mainl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd Sch\u00fctze, 2020a)</ref>; our proposed self-debiasing algorithm bears some resemblance with that of <ref type=\"bibr\" target=\"#b35\">Schick and Sch\u00fctze (2020b)</ref>. It is also related to other recent . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eroshot learning using task descriptions <ref type=\"bibr\" target=\"#b30\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Puri and Catanzaro, 2019;</ref><ref type=\"bibr\" target=\"#b34\">Schick . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b27\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Radford et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Devlin et al., 2019)</ref>. With model sizes continually increasing <  and contextualized word embeddings (e.g., <ref type=\"bibr\" target=\"#b27\">Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Devlin et al., 2019)</ref> pretrained in a self-supervised fashion ex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b40\">Zhao et al., 2017;</ref><ref type=\"bibr\" target=\"#b33\">Rudinger et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Gonen and Goldberg, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Bordia a r\" target=\"#b41\">Zhao et al., 2018;</ref><ref type=\"bibr\" target=\"#b32\">Ravfogel et al., 2020;</ref><ref type=\"bibr\" target=\"#b14\">Gonen and Goldberg, 2019)</ref>, many of them being based on predefin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Gehman et al. (2020)</ref> propose domain-adaptive pretraining on non-toxic corpora as outlined by <ref type=\"bibr\" target=\"#b15\">Gururangan et al. (2020)</ref> and consider plug and play language mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gly, training different models for each desired behavior, which can cause high environmental impact <ref type=\"bibr\" target=\"#b38\">(Strubell et al., 2019)</ref>. We therefore argue that, instead of tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b5\">(Bolukbasi et al., 2016b;</ref><ref type=\"bibr\" target=\"#b8\">Caliskan et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Zhao et al., 2017;</ref><ref type=\"bibr\" target=\"#b33\">Rudinger et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nsurprisingly, language models trained on such data pick up, reproduce or even amplify these biases <ref type=\"bibr\" target=\"#b4\">(Bolukbasi et al., 2016a;</ref><ref type=\"bibr\" target=\"#b37\">Sheng et  2021)</ref>.</p><p>For static word embeddings, various algorithms for debiasing have been proposed <ref type=\"bibr\" target=\"#b4\">(Bolukbasi et al., 2016a;</ref><ref type=\"bibr\" target=\"#b41\">Zhao et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g keywords <ref type=\"bibr\" target=\"#b16\">(He et al., 2020)</ref> and to prefixconstrained decoding <ref type=\"bibr\" target=\"#b22\">(Knowles and Koehn, 2016;</ref><ref type=\"bibr\" target=\"#b39\">Wuebker. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b15\">Gururangan et al. (2020)</ref> and consider plug and play language models <ref type=\"bibr\" target=\"#b9\">(Dathathri et al., 2020)</ref>. In contrast to our proposed approach, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  With model sizes continually increasing <ref type=\"bibr\" target=\"#b30\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b31\">Raffel et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Brown et al.,  are only filtered with some basic rules <ref type=\"bibr\" target=\"#b30\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b31\">Raffel et al., 2020)</ref>. As a consequence, they contain Input: I h \"#b31\">Raffel et al., 2020)</ref>. As a consequence, they contain Input: I hate black __ so much.   <ref type=\"bibr\" target=\"#b31\">(Raffel et al., 2020)</ref> and GPT2-XL <ref type=\"bibr\" target=\"#b30 bibr\">Gehman et al., 2020, i.a.)</ref>.</p><p>Simple solutions such as using a list of banned words <ref type=\"bibr\" target=\"#b31\">(Raffel et al., 2020)</ref> fall short of mitigating this problem: No ad>Figure 1 :</head><label>1</label><figDesc>Figure1: Most probable continuations according to T5-3B<ref type=\"bibr\" target=\"#b31\">(Raffel et al., 2020)</ref> and GPT2-XL<ref type=\"bibr\" target=\"#b30\" w.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\">For example, the list of banned words used by<ref type=\"bibr\" target=\"#b31\">Raffel et al. (2020)</ref> contains phrases like \"tied up\", \"taste my. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b22\">(Knowles and Koehn, 2016;</ref><ref type=\"bibr\" target=\"#b39\">Wuebker et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Keskar et al., 2019)</ref>.</p></div> <div xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gly, training different models for each desired behavior, which can cause high environmental impact <ref type=\"bibr\" target=\"#b38\">(Strubell et al., 2019)</ref>. We therefore argue that, instead of tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b5\">(Bolukbasi et al., 2016b;</ref><ref type=\"bibr\" target=\"#b8\">Caliskan et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Zhao et al., 2017;</ref><ref type=\"bibr\" target=\"#b33\">Rudinger et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thm affects the overall quality of generated texts, we measure perplexity on the Wikitext-2 dataset <ref type=\"bibr\" target=\"#b23\">(Merity et al., 2017)</ref>. 6 We use a maximum sequence length of |x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> Building training datasets with more care and deliberation, an alternative solution discussed by <ref type=\"bibr\" target=\"#b2\">Bender et al. (2021)</ref>, is important, especially for addressing li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thm affects the overall quality of generated texts, we measure perplexity on the Wikitext-2 dataset <ref type=\"bibr\" target=\"#b23\">(Merity et al., 2017)</ref>. 6 We use a maximum sequence length of |x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b27\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Radford et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Devlin et al., 2019)</ref>. With model sizes continually increasing <  and contextualized word embeddings (e.g., <ref type=\"bibr\" target=\"#b27\">Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Devlin et al., 2019)</ref> pretrained in a self-supervised fashion ex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b26\">Nangia et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Nadeem et al., 2020;</ref><ref type=\"bibr\" target=\"#b19\">Kaneko and Bollegala, 2021a)</ref>. For text generation, <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">(Bolukbasi et al., 2016a;</ref><ref type=\"bibr\" target=\"#b37\">Sheng et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Basta et al., 2019;</ref><ref type=\"bibr\">Gehman et al., 2020, i.a.)</  target=\"#b6\">Bordia and Bowman, 2019;</ref><ref type=\"bibr\" target=\"#b37\">Sheng et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Basta et al., 2019;</ref><ref type=\"bibr\">Nangia et al., 2020, i.a.)</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: reproduce or even amplify these biases <ref type=\"bibr\" target=\"#b4\">(Bolukbasi et al., 2016a;</ref><ref type=\"bibr\" target=\"#b37\">Sheng et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Basta et al.,  t=\"#b14\">Gonen and Goldberg, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Bordia and Bowman, 2019;</ref><ref type=\"bibr\" target=\"#b37\">Sheng et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Basta et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b15\">Gururangan et al. (2020)</ref> and consider plug and play language models <ref type=\"bibr\" target=\"#b9\">(Dathathri et al., 2020)</ref>. In contrast to our proposed approach, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e that the topics discovered by a model describe different semantic topical meanings. Specifically, <ref type=\"bibr\" target=\"#b10\">[11]</ref> defines topic diversity to be the percentage of unique wor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o discover pairwise topic correlations by the covariance matrix. In terms of treestructured topics, <ref type=\"bibr\" target=\"#b23\">[24]</ref> introduces to generate a series of topics from the root to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pics (i.e., T ) to interpret the latent representations or each dimension remains another question. <ref type=\"bibr\" target=\"#b36\">[37]</ref> proposes the first answers to the above questions, where t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ver interpretable topics, it cannot learn topic distributions for documents. To address this issue, <ref type=\"bibr\" target=\"#b55\">[56]</ref> introduces an additional encoder that learns z for a given. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b2\">[3]</ref> proposes to use the document embedding vector generated by Sentence-BERT <ref type=\"bibr\" target=\"#b45\">[46]</ref> and to keep the remaining part of an NTM the same as <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  representative words (e.g, top 10 words) is inline with human evaluation of topic interpretability <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Various formulations have been proposed to compute TC, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a series of topics from the root to the leaf of a topic tree with a doubly-recurrent neural network <ref type=\"bibr\" target=\"#b0\">[1]</ref>. When applied in topic modelling, the gamma belief network (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ement to corpora, many domain knowledge graphs are available to provide knowledge. For example, OAG <ref type=\"bibr\" target=\"#b46\">[47]</ref> is the largest publicly available heterogeneous academic e ding AMiner <ref type=\"bibr\" target=\"#b40\">[41]</ref>, OAG <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, and Microsoft Academic Graph (MAG) <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aph. To better recommend papers for online academic search <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, graph information including related academic concepts and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rks including Heterogeneous Graph Transformer <ref type=\"bibr\" target=\"#b16\">[17]</ref> and GPT-GNN <ref type=\"bibr\" target=\"#b15\">[16]</ref> similarly borrow the idea from the natural language commun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e representations in the heterogeneous graph. To better recommend papers for online academic search <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, graph information. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"figure\" target=\"#fig_1\">3</ref>. Second, when decoding an entity from scratch, we use beam search <ref type=\"bibr\" target=\"#b41\">[42]</ref> to search the token combinations with the highest probabil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: challenge for OAG-BERT lies in how to integrate knowledge into language models. Previous approaches <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> mainly focus on in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us changes to the original BERT architecture, we further adopt the Pre-LN BERT as used in deepspeed <ref type=\"bibr\" target=\"#b37\">[38]</ref> , where layer normalization is placed inside the residual . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: predictions on each token and identify which tokens are part of entities. Some datasets like BC5CDR <ref type=\"bibr\" target=\"#b26\">[27]</ref> only need span range identification while other datasets l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: holar corpus <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Other large academic corpora including AMiner <ref type=\"bibr\" target=\"#b40\">[41]</ref>, OAG <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"b /p><p>In addition to that, we also integrate the OAG-BERT as a fundamental component for the AMiner <ref type=\"bibr\" target=\"#b40\">[41]</ref> system. In AMiner, we utilize OAG-BERT to handle rich info /ref>. Other large academic corpora including AMiner <ref type=\"bibr\" target=\"#b40\">[41]</ref>, OAG <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, and Microsoft Aca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b26\">[27]</ref> only need span range identification while other datasets like EBM-NLP <ref type=\"bibr\" target=\"#b33\">[34]</ref> also need entity type recognition. For sequence token clas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h establish new state-of-the-art on many domain-related benchmarks such as named entity recognition <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, topic classificati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d in label propagation methods represents structure-based prior, and has been shown to be underused <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> in graph convoluti N by adding regularizations <ref type=\"bibr\" target=\"#b29\">[30]</ref> or manipulating graph filters <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Their experimenta .7% in terms of classification accuracy. It is worth noting that we also apply our framework on GLP <ref type=\"bibr\" target=\"#b13\">[14]</ref> which unified GCN and label propagation by manipulating gr al prediction mechanisms, i.e., label propagation. For example, Generalized Label Propagation (GLP) <ref type=\"bibr\" target=\"#b13\">[14]</ref> modified graph convolutional filters to generate smooth fe ng to avoid oversmoothing of GCN model.</p><p>Here we use 16 layers GCNII as a teacher.</p><p>\u2022 GLP <ref type=\"bibr\" target=\"#b13\">[14]</ref> is a label-efficient model which combines label propagatio e effectiveness of our framework. \u2022 Note that the teacher model Generalized Label Propagation (GLP) <ref type=\"bibr\" target=\"#b13\">[14]</ref> has already incorporated the label propagation mechanism i \"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracies with teacher model as GLP<ref type=\"bibr\" target=\"#b13\">[14]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols , 0.6 as dropout probability, 256 as batch size and 0.1/0.0005 as learning rate decays.</p><p>\u2022 GLP <ref type=\"bibr\" target=\"#b13\">[14]</ref>: we use 16 as hidden-layer size, 0.01 as learning rate, 0.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  As previous works <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26</ref>] did, we only consider the largest connected component and r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e with fewer parameters using a local structure preserving module. Reliable Data Distillation (RDD) <ref type=\"bibr\" target=\"#b34\">[35]</ref> trained multiple GCN students with the same architecture a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have demonstrated their effectiveness in classifying node l  and employ several popular GNN models including GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref>, GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>, SAGE <ref type=\"bibr\" target=\"#b6\">[7]</ref>, APPNP <ref  s a variant of convolutional neural networks that operates on graphs. Graph Attention Network (GAT) <ref type=\"bibr\" target=\"#b28\">[29]</ref> further employed attention mechanism in the aggregation of ramework can be an arbitrary GNN model such as GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> or GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>. We denote the pretrained classifier in a teacher model as ssification accuracies with teacher models as GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>. \u2022 Citeseer <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a o the number of layers and we employ the most widely-used 2-layer setting in this work.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref> improves GCN by incorporating attention mechanism which as ze, 0.01 as learning rate, 0.8 as dropout probability and 0.001 as learning rate decay.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>: we use 64 as hidden-layer size, 0.01 as learning rate, 0.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d employ an Adam optimizer <ref type=\"bibr\" target=\"#b9\">[10]</ref> for parameter training. Dropout <ref type=\"bibr\" target=\"#b24\">[25]</ref> is also applied to alleviate overfitting.</p></div> <div x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> or regularize the label differences between neighbors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>.</p><p>With the success of deep learning, methods based on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he homophily assumption, many traditional methods are developed to propagate labels by random walks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> or regularize the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r framework based on Deep Graph Library (DGL) <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Pytorch <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and employ an Adam optimizer <ref type=\"bibr\" target=\"#b9. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ics of the datasets are shown in Table <ref type=\"table\" target=\"#tab_1\">1</ref>. As previous works <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have demonstrated their effectiveness in classifying node l  and employ several popular GNN models including GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref>, GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>, SAGE <ref type=\"bibr\" target=\"#b6\">[7]</ref>, APPNP <ref  s a variant of convolutional neural networks that operates on graphs. Graph Attention Network (GAT) <ref type=\"bibr\" target=\"#b28\">[29]</ref> further employed attention mechanism in the aggregation of ramework can be an arbitrary GNN model such as GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> or GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>. We denote the pretrained classifier in a teacher model as ssification accuracies with teacher models as GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>. \u2022 Citeseer <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a o the number of layers and we employ the most widely-used 2-layer setting in this work.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref> improves GCN by incorporating attention mechanism which as ze, 0.01 as learning rate, 0.8 as dropout probability and 0.001 as learning rate decay.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>: we use 64 as hidden-layer size, 0.01 as learning rate, 0.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /www.tei-c.org/ns/1.0\"><head n=\"2.1\">Graph Neural Networks</head><p>The concept of GNN was proposed <ref type=\"bibr\" target=\"#b20\">[21]</ref> before 2010 and has become a rising topic since the emerge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etworks (GNN) <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref> as basic modules to learn graph representation.</p><p>In d eatures. In order to generate coarsened graphs to represent graph substructures, following DIFFPOOL <ref type=\"bibr\" target=\"#b46\">[47]</ref>,</p><p>we learn an assignment matrix B l k+1 via another G %, GNN can automatically learn the node proximity and the appropriate number of meaningful clusters <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>Parameters in the graph filter. We analyze how the  ning, such as <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>. In order to generate graph representations, most works em node embeddings first, and then use some pooling or READOUT functions, such as hierarchical pooling <ref type=\"bibr\" target=\"#b46\">[47]</ref> and sum operations <ref type=\"bibr\" target=\"#b35\">[36]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns in a graph <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>. We adopt ProNE <ref type=\"bibr\" target=\"#b48\">[49]</ref>  ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>. We adopt ProNE <ref type=\"bibr\" target=\"#b48\">[49]</ref> to pretrain user embeddings in the friendship network, due e j-th eigenvalue \u03bb j is, the better partition effect it would achieve for dividing into j clusters <ref type=\"bibr\" target=\"#b48\">[49]</ref>. Thus we employ a graph filter g to adjust the eigenvalues tion.</p><p>To avoid explicit eigendecomposition and Fourier transform, we use the same trick as in <ref type=\"bibr\" target=\"#b48\">[49]</ref> to approximate g with a Chebyshev expansion and Bessel fun mpled ego network to 32. For pre-trained user embeddings, we generate 64-dim embeddings using ProNE <ref type=\"bibr\" target=\"#b48\">[49]</ref>. In the user feature smoothing method via graph filter, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b30\">[31]</ref> use Graph Attention Networks (GAT) to model social influence locality. Feng et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> propose a skip-gram architecture to learn user embeddings t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dly related to social influence <ref type=\"bibr\" target=\"#b37\">[38]</ref> and information diffusion <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Most methods a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: luence maximization in social networks has been studied in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Xin et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> stud. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elf-attention to select important nodes in the graph or cluster similar nodes together. Yuan et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> regard the node clustering problem as a structured predict. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elf-attention to select important nodes in the graph or cluster similar nodes together. Yuan et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> regard the node clustering problem as a structured predict. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Note that Qiu et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref> use Random Walk with Restart (RWR) <ref type=\"bibr\" target=\"#b13\">[14]</ref> to generate sampled ego networks. However, information dif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: network representation learning methods have been proposed to learn node representations in a graph <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bib  NetMF <ref type=\"bibr\" target=\"#b29\">[30]</ref>, (2) shallow embedding approaches such as DeepWalk <ref type=\"bibr\" target=\"#b28\">[29]</ref>, LINE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, HARP <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vant features and instances. The features used are the same as Logistic Regression.</p><p>\u2022 xDeepFM <ref type=\"bibr\" target=\"#b20\">[21]</ref>. xDeepFM is a framework based on the Factorization Machine. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alk <ref type=\"bibr\" target=\"#b28\">[29]</ref>, LINE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, HARP <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and (3) neural network approaches <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dy the indirect influence on Twitter. Micro influence, like pairwise influence, has been studied in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b33\">[34]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for \"wow\" prediction and 0.1 for click prediction. The L2 regularization weight is 0.0005. Adagrad <ref type=\"bibr\" target=\"#b7\">[8]</ref> is chosen as the optimizer.</p><p>As for the Weibo dataset, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: everaging GAT, too. The different parts of features are concatenated to make predictions. \u2022 SAGPool <ref type=\"bibr\" target=\"#b18\">[19]</ref>. SAGPool is a graph pooling method that uses self-attentio so some works using graph convolution architectures for graphlevel representation learning, such as <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bib perations <ref type=\"bibr\" target=\"#b35\">[36]</ref>.</p><p>Among hierarchical pooling methods, some <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref> use self-attent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as (1) factorization-based approaches such as GraRep <ref type=\"bibr\" target=\"#b3\">[4]</ref>, NetMF <ref type=\"bibr\" target=\"#b29\">[30]</ref>, (2) shallow embedding approaches such as DeepWalk <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elf-attention to select important nodes in the graph or cluster similar nodes together. Yuan et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> regard the node clustering problem as a structured predict. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: learning approaches can be broadly categorized as (1) factorization-based approaches such as GraRep <ref type=\"bibr\" target=\"#b3\">[4]</ref>, NetMF <ref type=\"bibr\" target=\"#b29\">[30]</ref>, (2) shallo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, <ref type=\"bibr\" target=\"#b33\">[34]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>. Liu et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> study the micro mechanism of influence diffusion in hetero. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: luence maximization in social networks has been studied in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Xin et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> stud. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: network representation learning methods have been proposed to learn node representations in a graph <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bib  NetMF <ref type=\"bibr\" target=\"#b29\">[30]</ref>, (2) shallow embedding approaches such as DeepWalk <ref type=\"bibr\" target=\"#b28\">[29]</ref>, LINE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, HARP <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: network representation learning methods have been proposed to learn node representations in a graph <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bib  NetMF <ref type=\"bibr\" target=\"#b29\">[30]</ref>, (2) shallow embedding approaches such as DeepWalk <ref type=\"bibr\" target=\"#b28\">[29]</ref>, LINE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, HARP <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 112\">114]</ref>, we introduce a graph representation method based on the work of Lov\u00e1sz and Szegedy <ref type=\"bibr\" target=\"#b78\">[79]</ref> and Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"  a sampled graphon W \u223c P(W ), where W : [0, 1] 2 \u2192 [0, 1] is a random symmetric measurable function <ref type=\"bibr\" target=\"#b78\">[79]</ref> sampled (according to some distribution) from D W , the se  G te N te := (A te , X te ).</p><p>Our SCM has a direct connection with graphon random graph model <ref type=\"bibr\" target=\"#b78\">[79]</ref>, and extend it by considering vertex attributes. Now we in  subgraph densities (more precisely, induced homomorphism densities) in graphon random graph models <ref type=\"bibr\" target=\"#b78\">[79]</ref> to learn E-invariant representations for the SCM defined i a have different distributions. By introducing a structural causal model inspired by graphon models <ref type=\"bibr\" target=\"#b78\">[79]</ref>, we defined a representation that is approximately invaria e W is the graphon function as illustrated in Definitions 1 and 2. As defined in Lov\u00e1sz and Szegedy <ref type=\"bibr\" target=\"#b78\">[79]</ref>,</p><formula xml:id=\"formula_23\">t(F k , W ) = [0,1] k ij\u2208 ctation before we observe any vertices. B m is a martingale for unattributed graphs (Theorem 2.5 of <ref type=\"bibr\" target=\"#b78\">[79]</ref>). And since in Definitions 1 and 2 we also use the graphon 9]</ref> and their connection to graphon random graph models <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b78\">79]</ref>:</p><formula xml:id=\"formula_0\">Definition 1 (Training Grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s has been used, for example, in time series forecasting <ref type=\"bibr\" target=\"#b111\">[113,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. This can come in the form of re-expressing phenomena in a wa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ure of the graph <ref type=\"bibr\" target=\"#b140\">[142,</ref><ref type=\"bibr\" target=\"#b85\">86,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b108\">110]</ref>. Relevantly, many. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  their subgraphs <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b129\">131,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b82\">83]</ref>. It is known that c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> and will inherit the limitations of WL GNNs like inability to represent cycles. Rieck et al. <ref type=\"bibr\" target=\"#b102\">[104]</ref> propose a persistent WL kernel that uses ideas from Topo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph representations <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b97\">99,</ref><ref type=\"bibr\" target=\"#b90\">91,</ref><ref type=\"bibr\" target=\"#b93\">94,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b113\">115,</ref><ref type=\"bibr\" target=\"#b84\">85,</ref><ref type=\"bibr\" target=\"#b79\">80,</ref><ref type=\"bibr\" target=\"#b118\">120,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: every vertex.</p><p>We use the WL graph kernel implementations provided by the graphkernels package <ref type=\"bibr\" target=\"#b121\">[123]</ref>. All kernel methods use a Support Vector Machine on scik. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s paper, are implemented in PyTorch <ref type=\"bibr\" target=\"#b95\">[96]</ref> and Pytorch Geometric <ref type=\"bibr\" target=\"#b38\">[39]</ref>.</p><p>Our GIN <ref type=\"bibr\" target=\"#b140\">[142]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph representations <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b97\">99,</ref><ref type=\"bibr\" target=\"#b90\">91,</ref><ref type=\"bibr\" target=\"#b93\">94,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b65\">66,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b146\">148,</ref><ref type=\"bibr\" target=\"#b100\">102,</ref><ref type=\"bibr\" target=\"#b81\">82,</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etroni et al., 2020)</ref> and applied to the Reddit-derived \"Explain Like I'm Five\" (ELI5) dataset <ref type=\"bibr\" target=\"#b9\">(Fan et al., 2019)</ref>, which is the only publicly-available large-s lns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">A state-of-the-art LFQA system</head><p>The ELI5 task <ref type=\"bibr\" target=\"#b9\">(Fan et al., 2019)</ref> asks models to generate paragraph-length answ d=\"fig_1\"><head>A. 1</head><label>1</label><figDesc>Dataset StatisticsWe downloaded the ELI5 dataset<ref type=\"bibr\" target=\"#b9\">(Fan et al., 2019)</ref> from the KILT Github repository (https:// git ng set, and almost all validation questions are topically similar to a training set question. While <ref type=\"bibr\" target=\"#b9\">Fan et al. (2019)</ref> attempted to identify and remove question over jor reason for ignoring retrievals is the large amount of train / validation overlap in ELI5. While <ref type=\"bibr\" target=\"#b9\">Fan et al. (2019)</ref> attempted to fix this issue through TF-IDF ove l dataset curation for LFQA tasks is needed to prevent du-plicates. We acknowledge the efforts from <ref type=\"bibr\" target=\"#b9\">Fan et al. (2019)</ref> to fix this issue, and suggest alternative met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ation study (see Appendix A.3). During inference, a MIPS search is conducted with the ScaNN library <ref type=\"bibr\" target=\"#b11\">(Guo et al., 2020)</ref> to efficiently find the top K documents; we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th answers. Significant progress has been made on open-domain QA datasets such as Natural Questions <ref type=\"bibr\" target=\"#b21\">(Kwiatkowski et al., 2019)</ref>, whose questions are answerable with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b12\">(Gururangan et al., 2018)</ref> and passage-only baselines for reading comprehension <ref type=\"bibr\" target=\"#b19\">(Kaushik and Lipton, 2018)</ref>. We evaluate two ROUGE-L lower bound. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\">3</ref> We fine-tune our model in a decoderonly fashion <ref type=\"bibr\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Wolf et al., 2018)</ref> by concatenating the top K retrieved documen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th answers. Significant progress has been made on open-domain QA datasets such as Natural Questions <ref type=\"bibr\" target=\"#b21\">(Kwiatkowski et al., 2019)</ref>, whose questions are answerable with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are answerable with short phrases and entities, by leveraging dense retrieval techniques like ORQA <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019)</ref>, REALM <ref type=\"bibr\" target=\"#b13\">(Guu e irming our design decisions.</p><p>Initialization R-Prec. R@5 REALM (pretrained) 6.6 14.9</p><p>ICT <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019)</ref> 9.3 16.5 REALM <ref type=\"bibr\" target=\"#b13 . As a baseline, we initialize our model with ICT, a weaker self-supervised retriever introduced in <ref type=\"bibr\" target=\"#b23\">Lee et al. (2019)</ref>. Both models are trained with minibatch sizes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ating scores across generated sentences, but appropriate penalties are needed for lack of diversity <ref type=\"bibr\" target=\"#b50\">(Zhu et al., 2018</ref>) and short lengths. Other possible fixes incl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a different retriever during inference (REALM or C-REALM), and different nucleus sampling p values <ref type=\"bibr\" target=\"#b14\">(Holtzman et al., 2020)</ref>. All variants outperform prior work in  n LFQA.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"5\" xml:id=\"foot_2\">As in<ref type=\"bibr\" target=\"#b14\">Holtzman et al. (2020)</ref>, a human study reveals that higher entro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nipulate the input image. Some recent efforts in developing physical-world attacks are addressed in <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" targ rial examples of small perturbations can be easily mitigated in complex physical-world environments <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ b24\">[26]</ref>, or camouflaging the adversarial patch into specific shape (e.g. eye-glasses frames <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, etc.). Due to vario re various adaptations over a distribution of transformations to adapt to physical-world conditions <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" targ ect to impose them <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b24\">26,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Besides the challeng physical-world attacks <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b19\">21]</ref>. Also, these attacks require attacker physically pasting th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">14]</ref>. The physical-world adversarial examples typically require . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etting</head><p>We test our proposed attacks in both digital-and physical-settings. We use ResNet50 <ref type=\"bibr\" target=\"#b11\">[12]</ref> as the target model for all the experiments. We randomly s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>. However, in physical #b13\">15,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref> or optimized perturbation with a given loss <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Adversarial examples  nsure that imperceptible to human observers. Normally, l 2 and l \u221e are the most commonly used norms <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [13,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b26\">28]</ref>, texture <ref type=\"bibr\" target=\"#b22\">[24]</ref>) to generate adversarial examples. Besides, there are also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the attacker prints adversarial perturbation as a sticker and then pastes it onto the target object <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ target=\"#b1\">[2]</ref> (c) AdvLB (Ours) the most effective area in the target object to impose them <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b24\">26,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the attacker prints adversarial perturbation as a sticker and then pastes it onto the target object <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ target=\"#b1\">[2]</ref> (c) AdvLB (Ours) the most effective area in the target object to impose them <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b24\">26,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that the model is more biased towards the feature brought by the laser beam. We further use the CAM <ref type=\"bibr\" target=\"#b27\">[29]</ref> to highlight the bias of model when the laser beam is adde. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vers. Normally, l 2 and l \u221e are the most commonly used norms <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b23\">25,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \"sticker-pasting\" setting: Shen et al. <ref type=\"bibr\" target=\"#b20\">[22]</ref> and Nguyen et al. <ref type=\"bibr\" target=\"#b17\">[19]</ref> proposed using a projector to project the adversarial pert the physicalworld attacks against face recognition systems <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b17\">19]</ref>. These attacks craft adversarial perturbation and then proj. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etting</head><p>We test our proposed attacks in both digital-and physical-settings. We use ResNet50 <ref type=\"bibr\" target=\"#b11\">[12]</ref> as the target model for all the experiments. We randomly s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b2\">[3]</ref> (i.e., the latest large-scale language model of OpenAI), pre-training language models <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" ta e.g., UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref>) and two-tower architecture (e.g., OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref>). In this project, similar to OpenAI CLIP, 'WenLan' adopts dea, we introduce comparative learning into our two-tower architecture. However, unlike OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> that adopts a simple contrastive learning method, we propo tly, our CMCL model outperforms both UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref> and OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> on the RUC-CAS-WenLan test set and AIC-ICC <ref type=\"bibr </p><p>Tasks Image-to-Text Retrieval Text-to-Image Retrieval Metrics R@1 R@5 R@10 R@1 R@5 R@10 CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> 13 </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><he sks</head><p>Image-to-Text Retrieval Text-to-Image Retrieval Metrics R@1 R@5 R@10 R@1 R@5 R@10 CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> 7 </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><hea  out over the text-image retrieval results obtained by the pre-training models (e.g., CMLC and CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref>). We select a group of image and text queries for testing. t=\"#tab_1\">4</ref>. As expected, the user study does validate that our CMCL outperforms OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref>. When the candidate set (per query) of UNITER is obtained . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: les of the image-text pair, and the number of negative samples is expanded based on the latest MoCo <ref type=\"bibr\" target=\"#b15\">[16]</ref> framework to improve the representation ability of the neu ize of D. Our image-text retrieval model leverages contrastive learning and expands the latest MoCo <ref type=\"bibr\" target=\"#b15\">[16]</ref> as the pre-training framework, as illustrated in Figure <r dal embedding space. Implementation Details We utilize the momentumupdated history queue as in MoCo <ref type=\"bibr\" target=\"#b15\">[16]</ref> for contrastive learning. We adopt clip-wise random crops,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, contrastive learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b29\">[30]</ref> to detect object boundingboxes from each image. We further utilize EfficientNet L2 <ref type=\"bibr\" target=\"#b31\">[32]</ref> to extract image features for computation efficiency. By a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, contrastive learning <ref type=\"bibr\" target=\"#b3\">[4,</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on a fruit cake.\"</p><p>\"Happy Birthday! Make a wish.\" GPT <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> have achieved signifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on a fruit cake.\"</p><p>\"Happy Birthday! Make a wish.\" GPT <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> have achieved signifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, contrastive learning <ref type=\"bibr\" target=\"#b3\">[4,</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>\"There are several burning candles on a fruit cake.\"</p><p>\"Happy Birthday! Make a wish.\" GPT <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: models such as BERT <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b18\">19</ref>] and * Co-corresponding authors.</p><p>\"There are several bu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -training models typically adopt two network architectures: single-tower architecture (e.g., UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref>) and two-tower architecture (e.g., OpenAI CLIP <ref type=\"bi ned on RUC-CAS-WenLan has 1 billion parameters. Importantly, our CMCL model outperforms both UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref> and OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> on to project the textual embedding to the cross-modal embedding space. Image Encoder Following UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we first employ pre-trained Faster-RCNN <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: be enforced for multi-modal pre-training. Thanks to the recent progress of self-supervised learning <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>\"There are several burning candles on a fruit cake.\"</p><p>\"Happy Birthday! Make a wish.\" GPT <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b29\">[30]</ref> to detect object boundingboxes from each image. We further utilize EfficientNet L2 <ref type=\"bibr\" target=\"#b31\">[32]</ref> to extract image features for computation efficiency. By a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on a fruit cake.\"</p><p>\"Happy Birthday! Make a wish.\" GPT <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> have achieved signifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: be enforced for multi-modal pre-training. Thanks to the recent progress of self-supervised learning <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, take an assumption that there exists strong semantic corre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -training models typically adopt two network architectures: single-tower architecture (e.g., UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref>) and two-tower architecture (e.g., OpenAI CLIP <ref type=\"bi ned on RUC-CAS-WenLan has 1 billion parameters. Importantly, our CMCL model outperforms both UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref> and OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> on to project the textual embedding to the cross-modal embedding space. Image Encoder Following UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we first employ pre-trained Faster-RCNN <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: be enforced for multi-modal pre-training. Thanks to the recent progress of self-supervised learning <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Most existing multi-modal pre-training models, especially those with the single-tower architecture <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, contrastive learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: be enforced for multi-modal pre-training. Thanks to the recent progress of self-supervised learning <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b18\">[16]</ref> or efficient design spaces <ref type=\"bibr\" target=\"#b23\">[21,</ref><ref type=\"bibr\" target=\"#b24\">22]</ref> can still pro- compound scaling (dwr), in which the width,  he state-of-the-art but faster. As a concrete example, we apply fast scaling to scale a RegNetY-4GF <ref type=\"bibr\" target=\"#b24\">[22]</ref> model to 16GF (gigaflops), and find it uses less memory an e tuned to a specific setting (e.g., dataset or flop regime). As an alternative, Radosavovic et al. <ref type=\"bibr\" target=\"#b24\">[22]</ref> recently introduced the idea of designing design spaces, a t; w and round w to be divisible by g otherwise (w will change by at most 1/3 under such a strategy <ref type=\"bibr\" target=\"#b24\">[22]</ref>).</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head tegies on three networks families: EfficientNet <ref type=\"bibr\" target=\"#b33\">[31]</ref>, Reg-NetY <ref type=\"bibr\" target=\"#b24\">[22]</ref>, and RegNetZ (described below). We chose these models as t n larger models.</p><p>RegNets. As an alternative to neural architecture search, Radosavovic et al. <ref type=\"bibr\" target=\"#b24\">[22]</ref> introduced the idea of designing design spaces, where a de esign spaces, where a design space is a parameterized population of models. Using this methodology, <ref type=\"bibr\" target=\"#b24\">[22]</ref> designed a design space consisting of simple, regular netw rmined by a quantized linear function which has 4 parameters (d, w 0 , w a , w m ), for details see <ref type=\"bibr\" target=\"#b24\">[22]</ref>. Any other block parameters (like group width or bottlenec al 1\u00d71 conv. The 1\u00d71 convs can change w via the bottleneck ratio b, however, we set b = 1 following <ref type=\"bibr\" target=\"#b24\">[22]</ref>. BatchNorm <ref type=\"bibr\" target=\"#b16\">[14]</ref> and R lly it uses a Squeezeand-Excitation (SE) layer <ref type=\"bibr\" target=\"#b14\">[12]</ref>. Following <ref type=\"bibr\" target=\"#b24\">[22]</ref>, we set the bottleneck ratio b to 1 (effectively no bottle ck). A Reg-NetY model is thus fully specified with 5 parameters: d, w 0 , w a , w m , and g. Unlike <ref type=\"bibr\" target=\"#b24\">[22]</ref>, we additionally vary the image input resolution r (bringi ofthe-art results. This creates a tension between using a simple yet weak optimization setup (e.g., <ref type=\"bibr\" target=\"#b24\">[22]</ref>) versus a strong setup that yields good results but may be </ref> we report results for baseline RegNet models. We obtain these models via random search as in <ref type=\"bibr\" target=\"#b24\">[22]</ref>. 3 Note that there are two versions of the 4GF RegNets (us  random models in a given flop regime is typically sufficient to obtain accurate models as shown in <ref type=\"bibr\" target=\"#b24\">[22]</ref>. (Right) Models obtained with w scaling are much faster th nal 1\u00d71 conv. The 1\u00d71 convs can change w via the bottleneck ratio b, however, we set b = 1 following<ref type=\"bibr\" target=\"#b24\">[22]</ref>. BatchNorm<ref type=\"bibr\" target=\"#b16\">[14]</ref> and Re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  search algorithms <ref type=\"bibr\" target=\"#b18\">[16,</ref><ref type=\"bibr\" target=\"#b22\">20,</ref><ref type=\"bibr\" target=\"#b19\">17]</ref>. For example, DARTS <ref type=\"bibr\" target=\"#b19\">[17]</re ref type=\"bibr\" target=\"#b22\">20,</ref><ref type=\"bibr\" target=\"#b19\">17]</ref>. For example, DARTS <ref type=\"bibr\" target=\"#b19\">[17]</ref> proposed a differentiable search strategy that does not re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 11\">9]</ref>. However, gains from depth scaling plateaued, leading to explorations of scaling width <ref type=\"bibr\" target=\"#b36\">[34]</ref> and resolution <ref type=\"bibr\" target=\"#b13\">[11]</ref>.   in activations. Indeed, it is well known that wide networks are quite efficient in wall-clock time <ref type=\"bibr\" target=\"#b36\">[34]</ref>. Unfortunately, wide networks may not always achieve top r r\" target=\"#b11\">[9]</ref>. Next, wider models proved not only effective but particularly efficient <ref type=\"bibr\" target=\"#b36\">[34,</ref><ref type=\"bibr\" target=\"#b13\">11]</ref>. The use of depthw 8\">[26,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" target=\"#b11\">9]</ref>, width <ref type=\"bibr\" target=\"#b36\">[34,</ref><ref type=\"bibr\" target=\"#b13\">11]</ref> and resolution <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pute regimes, efficient search <ref type=\"bibr\" target=\"#b18\">[16]</ref> or efficient design spaces <ref type=\"bibr\" target=\"#b23\">[21,</ref><ref type=\"bibr\" target=\"#b24\">22]</ref> can still pro- com. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1,</ref><ref type=\"bibr\" target=\"#b15\">13]</ref>. The recently introduced compound scaling strategy <ref type=\"bibr\" target=\"#b32\">[30]</ref>, which scales along all three dimensions at once, achieves model (EfficientNet-B0) was optimized in the mobile regime (400MF) using neural architecture search <ref type=\"bibr\" target=\"#b32\">[30]</ref> and scaled to larger sizes (B1-B7) via compound scaling. F. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: omes essential. Scaling has proven effective in terms of obtaining larger models with good accuracy <ref type=\"bibr\" target=\"#b33\">[31]</ref>. However, existing work on model scaling focuses on model  arget=\"#b13\">[11]</ref>. More recently scaling multiple dimensions at once, coined compound scaling <ref type=\"bibr\" target=\"#b33\">[31]</ref>, has been shown to achieve excellent accuracy.</p><p>Exist GF (gigaflops), and find it uses less memory and is faster (and more accurate) than EfficientNet-B4 <ref type=\"bibr\" target=\"#b33\">[31]</ref> -a model with 4\u00d7 fewer flops.</p><p>In order to facilitate imension is equal. Scaling uniformly along all dimensions, which closely resembles compound scaling <ref type=\"bibr\" target=\"#b33\">[31]</ref>, results in near linear scaling of activations.</p><p>Comm on, an intuitive approach is to scale along multiple dimensions at once. Coined compound scaling by <ref type=\"bibr\" target=\"#b33\">[31]</ref>, such an approach has been shown to achieve higher accurac , we scale g proportionally to w. For networks that use depthwise conv (g = 1), as in previous work <ref type=\"bibr\" target=\"#b33\">[31]</ref>, we do not scale g. Finally, we note that when scaling g,  line networks. In this work we evaluate scaling strategies on three networks families: EfficientNet <ref type=\"bibr\" target=\"#b33\">[31]</ref>, Reg-NetY <ref type=\"bibr\" target=\"#b24\">[22]</ref>, and R ur scaling experiments. Moreover, Ef-ficientNet was introduced in the context of model scaling work <ref type=\"bibr\" target=\"#b33\">[31]</ref>, making it an excellent candidate for our study.</p><p>Eff [30]</ref> and scaled to larger sizes (B1-B7) via compound scaling. For further details, please see <ref type=\"bibr\" target=\"#b33\">[31]</ref>.</p><p>Note that EfficientNets are specified by \u223c30 parame [22]</ref>) versus a strong setup that yields good results but may be difficult to reproduce (e.g., <ref type=\"bibr\" target=\"#b33\">[31]</ref>). To address this, we use a training setup that effectivel icientNet reproduction. The first set of results includes the originally reported errors (from ICML <ref type=\"bibr\" target=\"#b33\">[31]</ref> and updated numbers later reported on arXiv), the second s e biggest nets they slightly lag the updated arXiv errors). We emphasize that unlike the results in <ref type=\"bibr\" target=\"#b33\">[31]</ref>, we use the same, easy to reproduce optimization setup for e=\"table\">5</ref>, we report Efficient-Net results using our optimization setup versus results from <ref type=\"bibr\" target=\"#b33\">[31]</ref>. We report our results using a '1\u00d7', '2\u00d7', or '4\u00d7' schedul nce, we also show the original EfficientNet models (orig) obtained via non-uniform compound scaling <ref type=\"bibr\" target=\"#b33\">[31]</ref>, the results closely match uniform compound scaling (dwr). 6.2.\">Simple and Compound Scaling</head><p>We now turn to evaluation of simple and compound scaling <ref type=\"bibr\" target=\"#b33\">[31]</ref> described in \u00a73.3 and \u00a73.4, respectively. For these experi <p>We also compare uniform compound scaling (dwr) to the original compound scaling rule (orig) from <ref type=\"bibr\" target=\"#b33\">[31]</ref>, which empirically set the per-dimension scalings factors. ence, we also show the original EfficientNet models (orig) obtained via non-uniform compound scaling<ref type=\"bibr\" target=\"#b33\">[31]</ref>, the results closely match uniform compound scaling (dwr). nsion and by 3 \u221a s 3 = s in total.Interestingly, the compound scaling rule discovered empirically in<ref type=\"bibr\" target=\"#b33\">[31]</ref> scaled by 1.2, 1.1, and 1.15 along d, w, and r, which corr  top results compared to deeper or higher-resolution models <ref type=\"bibr\" target=\"#b11\">[9,</ref><ref type=\"bibr\" target=\"#b33\">31]</ref>.</p><p>To address this, in this work we introduce the conce. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y increasing depth <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" target=\"#b11\">9]</ref>. However, gains from depth scaling plateaued, leading to exp lude scaling depth <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" target=\"#b11\">9]</ref>, width <ref type=\"bibr\" target=\"#b36\">[34,</ref><ref type=\"b pe=\"bibr\" target=\"#b31\">29]</ref>. This trend culminated with the introduction of residual networks <ref type=\"bibr\" target=\"#b11\">[9]</ref>. Next, wider models proved not only effective but particula F models models trained using our 1\u00d7, 2\u00d7, and 4\u00d7 schedules. For reference, we also retrain ResNet50 <ref type=\"bibr\" target=\"#b11\">[9]</ref> and ResNeXt50 <ref type=\"bibr\" target=\"#b34\">[32]</ref> usi ly, wide networks may not always achieve top results compared to deeper or higher-resolution models <ref type=\"bibr\" target=\"#b11\">[9,</ref><ref type=\"bibr\" target=\"#b33\">31]</ref>.</p><p>To address t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emerged naturally in deep learning, with early work focused on scaling networks by increasing depth <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" ta ber of models. Thus model scaling becomes crucial. Popular scaling strategies include scaling depth <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" ta r and more accurate models. Increasing model depth led to rapid gains, notable examples include VGG <ref type=\"bibr\" target=\"#b28\">[26]</ref> and Inception <ref type=\"bibr\" target=\"#b30\">[28,</ref><re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  more accurate models include the inverted bottleneck <ref type=\"bibr\" target=\"#b27\">[25]</ref>, SE <ref type=\"bibr\" target=\"#b14\">[12]</ref>, and new nonlinearities <ref type=\"bibr\" target=\"#b12\">[10 ual, and (4) b &lt; 1 (we use b = 1/4 in all experiments). Finally, a Squeezeand-Excitation (SE) op <ref type=\"bibr\" target=\"#b14\">[12]</ref> (reduction ratio of 1/4) follows the 3\u00d73 conv for both the  <ref type=\"bibr\" target=\"#b34\">[32]</ref>. Additionally it uses a Squeezeand-Excitation (SE) layer <ref type=\"bibr\" target=\"#b14\">[12]</ref>. Following <ref type=\"bibr\" target=\"#b24\">[22]</ref>, we s dual, and (4) b &lt; 1 (we use b = 1/4 in all experiments). Finally, a Squeezeand-Excitation (SE) op<ref type=\"bibr\" target=\"#b14\">[12]</ref> (reduction ratio of 1/4) follows the 3\u00d73 conv for both the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: early work focused on scaling networks by increasing depth <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" target=\"#b11\">9]</ref>. However, gains from  crucial. Popular scaling strategies include scaling depth <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" target=\"#b11\">9]</ref>, width <ref type=\"bi o rapid gains, notable examples include VGG <ref type=\"bibr\" target=\"#b28\">[26]</ref> and Inception <ref type=\"bibr\" target=\"#b30\">[28,</ref><ref type=\"bibr\" target=\"#b31\">29]</ref>. This trend culmin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b8\">[10,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref> and analysis tools <ref type=\"bibr\" target=\"#b28\">[30]</ref> for DNNs analyze a broad space of software mappings of a D n its analytical network-on-chip (NoC) model based on a pipe model similar to other analytic models <ref type=\"bibr\" target=\"#b28\">[30]</ref>. The pipe model utilizes two parameters, the pipe width (b et=\"#b10\">[12,</ref><ref type=\"bibr\" target=\"#b23\">25,</ref><ref type=\"bibr\" target=\"#b24\">26,</ref><ref type=\"bibr\" target=\"#b28\">30]</ref>. These data-centric directives can express a wide range of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  with accuracy close to and even surpassing that of humans <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>. Tight latency, thr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en some works related to exploring data-centric approaches <ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" target=\"#b20\">[22]</ref><ref type=\"bibr\" target=\"#b21\">[23]</ref>, where the approa ructure centric analysis, for localityenhancement transformations such as multi-level data blocking <ref type=\"bibr\" target=\"#b20\">[22]</ref> and data shackling <ref type=\"bibr\" target=\"#b19\">[21]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing that of humans <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>. Tight latency, throughput, and energy constraints when run. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en some works related to exploring data-centric approaches <ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" target=\"#b20\">[22]</ref><ref type=\"bibr\" target=\"#b21\">[23]</ref>, where the approa ructure centric analysis, for localityenhancement transformations such as multi-level data blocking <ref type=\"bibr\" target=\"#b20\">[22]</ref> and data shackling <ref type=\"bibr\" target=\"#b19\">[21]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: computes the energy cost. MAESTRO includes an energy model based on those activity counts and Cacti <ref type=\"bibr\" target=\"#b27\">[29]</ref> simulation, which can be replaced by any other energy mode NN operators. For energy estimation, we multiply activity counts with base energy values from Cacti <ref type=\"bibr\" target=\"#b27\">[29]</ref> simulation (28nm, 2KB L1 scratchpad, and 1MB shared L2 buf ref>. The access counts generated by MAESTRO are multiplied by appropriate energy values from Cacti <ref type=\"bibr\" target=\"#b27\">[29]</ref>. The values are normalized to the MAC energy of C-P.</p></ able3. The access counts generated by MAESTRO are multiplied by appropriate energy values from Cacti<ref type=\"bibr\" target=\"#b27\">[29]</ref>. The values are normalized to the MAC energy of C-P.</figD. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[6,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b33\">35]</ref>. Early and late lay br\" target=\"#b33\">[35]</ref>, MobileNetV2 <ref type=\"bibr\" target=\"#b26\">[28]</ref>,</p><p>and UNet <ref type=\"bibr\" target=\"#b32\">[34]</ref>. The final column (f) presents the average results across  type=\"bibr\" target=\"#b33\">[35]</ref>, MobileNetV2<ref type=\"bibr\" target=\"#b26\">[28]</ref>, and UNet<ref type=\"bibr\" target=\"#b32\">[34]</ref>. The final column (f) presents the average results across . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g custom accelerators within a variety of constraints. In contrast, recent proposals on compilation <ref type=\"bibr\" target=\"#b8\">[10,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref> and analysis tools  vations) -Activation row (Y) and filter column (S) parallelism -Row-stationary -Motivated by Eyeriss<ref type=\"bibr\" target=\"#b8\">[10]</ref> -Spatial reuse of input activation -Large spatial reduction. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[6,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b33\">35]</ref>. Early and late lay br\" target=\"#b33\">[35]</ref>, MobileNetV2 <ref type=\"bibr\" target=\"#b26\">[28]</ref>,</p><p>and UNet <ref type=\"bibr\" target=\"#b32\">[34]</ref>. The final column (f) presents the average results across  type=\"bibr\" target=\"#b33\">[35]</ref>, MobileNetV2<ref type=\"bibr\" target=\"#b26\">[28]</ref>, and UNet<ref type=\"bibr\" target=\"#b32\">[34]</ref>. The final column (f) presents the average results across . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b34\">36,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" tar get=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b34\">36,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lers in estimating reuses in guiding optimal loop transformations for both parallelism and locality <ref type=\"bibr\" target=\"#b6\">[8,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" targ ler work that performs reuse analysis on sequential programs <ref type=\"bibr\" target=\"#b5\">[7,</ref><ref type=\"bibr\" target=\"#b6\">8,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the semantic difference. Mean pooling and Vector of Locally Aggregated Descriptors (VLAD) encoding <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref> fail to take th nce.</p><p>Second, multi-scale features from convolutional layers are not well exploited. Xu et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> found that fusing the features of different layers can boo  layers.</p><p>Some works have been conducted on encoding deep features to a global representation. <ref type=\"bibr\" target=\"#b11\">[12]</ref> showed that VLAD encoded ConvNet activations can significa d Attention</head><p>The classical VLAD encoding method has been found useful for video aggregation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Given a set of date that 2 normalization on global video representation will lead to better classification results <ref type=\"bibr\" target=\"#b11\">[12]</ref> and apply it to our models directly.</p><p>Compared to ave  classical VLAD encoding. To train the unsupervised VLAD encoding, we mainly follow the settings in <ref type=\"bibr\" target=\"#b11\">[12]</ref>. We first apply PCA with whitening to the convolutional ac on, intra-normalization and 2 normalization to the encoded vector, as they show good performance in <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>We first use 10 centers for the unsupervised VLAD e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  well as encoding the temporal context, we borrow an unsupervised context reconstruction block from <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We use the same structure, but extend it by training the  boring clips are related. Recurrent neural networks have been developed in modeling sequential data <ref type=\"bibr\" target=\"#b18\">[19]</ref>. However, they ignored that the neighboring clips are more previously described, video sequences are context relevant. We follow the RNN training described in <ref type=\"bibr\" target=\"#b18\">[19]</ref> to train the context reconstruction. Instead of training r f> to train the context reconstruction. Instead of training reconstruction module in separate stage <ref type=\"bibr\" target=\"#b18\">[19]</ref> (i.e., unsupervised context reconstruction and supervised  construction module into a joint end-to-end learning framework. The original context-reconstruction <ref type=\"bibr\" target=\"#b18\">[19]</ref> considered the reconstruction at the sequence-level, e.g., uage and both of them have been used to model video data <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Following <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we u ata <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Following <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we use the GRU architecture as it has shown similar perfo nabled to predict the previous frame which are combined together to form the bi-directional network <ref type=\"bibr\" target=\"#b18\">[19]</ref>. This bi-direction process forces the encoded representati iginal feature, we can obtain more expressive representation for the classification task. Following <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we choose the huber loss for reconstruction,</p><formula  ess of neighboring reconstruction, we compare to a sequence-level reconstruction method proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We use the sequence length to 30 as <ref type=\"bibr\" targ n method proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We use the sequence length to 30 as <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We use the schema (a) described in Section III-C and the  deling method compared with the mean pooling method, especially on the HMDB-51 dataset. Compared to <ref type=\"bibr\" target=\"#b18\">[19]</ref>, our neighboring reconstruction outperforms it by 0.6% on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f video modeling, motivated by its tremendous success on image analysis, such as object recognition <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Recent progress on action recognition has shown that prop  not affect the prediction block, which can stabilize the training procedure. A shortcut connection <ref type=\"bibr\" target=\"#b19\">[20]</ref> is introduced to connect the input features and the output. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wo-stream methods <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Tran et al. <r upervised video feature learning. Leveraging the attention mechanism for videos has been studied by <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Dong et al. <r y <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Dong et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> leveraged three modalities to learn a spatio-temporal atte ion <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and has also been extensively used in video analysis <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Visual attention has also been used in image recognition,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uipped with 3D convolution, using only the RGB stream can outperform frame-based two-stream methods <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Tran et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref> tackled action recognition with a spatio-temporal feature . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: weight assignment \u03b1 ik with the soft attention mechanism <ref type=\"bibr\" target=\"#b33\">[34]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>. The soft attention mechanism has become a powerful method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Vector of Locally Aggregated Descriptors (VLAD) encoding <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref> fail to take the sequence order into account, ignoring seq n significantly boost the performance compared with average pooling in event detection. Ac-tionVLAD <ref type=\"bibr\" target=\"#b12\">[13]</ref> has been used for video data modeling by leveraging learna oding method has been found useful for video aggregation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Given a set of inputs X = {x 1 , x 2 , . . ., x T } and c  otherwise. We have K k=1 \u03b1 ik = 1 if only one nearest neighbor is considered.</p><p>Girdhar et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed to learn the assignment with a softmax layer. The. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f video modeling, motivated by its tremendous success on image analysis, such as object recognition <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Recent progress on action recognition has shown that prop  not affect the prediction block, which can stabilize the training procedure. A shortcut connection <ref type=\"bibr\" target=\"#b19\">[20]</ref> is introduced to connect the input features and the output. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b20\">[21]</ref>, 3D convolution neural networks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b24\">[25]</ref> have shown stro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emporal convolution with a (2+1)D block, which can double the number of non-linearities. Xie et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> used a similar approach which replaces some 3D convolution. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e decay of 0.999 through  all experiments. The entire network is trained end-to-end with TensorFlow <ref type=\"bibr\" target=\"#b38\">[39]</ref> distributed machine learning system on four NVidia V100 GP  x-axis is the iteration number and the y-axis is the value of activations. We use the Ten-sorBoard <ref type=\"bibr\" target=\"#b38\">[39]</ref> to visualize the distributions which shows the percentage . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: neural networks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b24\">[25]</ref> have shown strong performance in spatio-temporal represent </ref> used inflated 3D convolution from 2D convolution, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref> decomposited 3D convolution into a 2D spatial convolution  ced I3D to adapt the Inception-V1 architecture and inflates the 2D filter to 3D filters. Qiu et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> replaced the 3D convolution with spatial and temporal conv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Vector of Locally Aggregated Descriptors (VLAD) encoding <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref> fail to take the sequence order into account, ignoring seq n significantly boost the performance compared with average pooling in event detection. Ac-tionVLAD <ref type=\"bibr\" target=\"#b12\">[13]</ref> has been used for video data modeling by leveraging learna oding method has been found useful for video aggregation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Given a set of inputs X = {x 1 , x 2 , . . ., x T } and c  otherwise. We have K k=1 \u03b1 ik = 1 if only one nearest neighbor is considered.</p><p>Girdhar et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed to learn the assignment with a softmax layer. The. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s (iDT). Dense Trajectories <ref type=\"bibr\" target=\"#b0\">[1]</ref> and improved Dense Trajectories <ref type=\"bibr\" target=\"#b1\">[2]</ref> were introduced for action recognition with considerable per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  plugged into these structures to generate more discriminative representations. Besides, Liu et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> introduce a temporal adaptive module that learns local imp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ayers while our method introduces cross-layer aggregation using recurrent layers.</p><p>Zhao et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref> encoded frame sequences via a visual dictionary to generat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Long Short-Term Memory (LSTM) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, Gated Recurrent Unit (GRU) <ref type=\"bibr\" target=\"#b17\">[18]</ref>, convolutional features are first fed into RNN, and aggreg g Short-Term Memory (LSTM) <ref type=\"bibr\" target=\"#b16\">[17]</ref> and Gated Recurrent Unit (GRU) <ref type=\"bibr\" target=\"#b17\">[18]</ref> are two popular architectures in modeling language and bot. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fication models based on Recurrent Neural Networks (RNN) <ref type=\"bibr\" target=\"#b13\">[14]</ref>- <ref type=\"bibr\" target=\"#b15\">[16]</ref>, e.g., Long Short-Term Memory (LSTM) <ref type=\"bibr\" targ g the attention mechanism for videos has been studied by <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Dong et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> lev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s (iDT). Dense Trajectories <ref type=\"bibr\" target=\"#b0\">[1]</ref> and improved Dense Trajectories <ref type=\"bibr\" target=\"#b1\">[2]</ref> were introduced for action recognition with considerable per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rent best architectures. This makes DNAS methods inflexible.</p><p>In contrast, single-path methods <ref type=\"bibr\" target=\"#b17\">[18]</ref>[10] <ref type=\"bibr\" target=\"#b8\">[9]</ref>[40] decouple s different search strategies, like the evolution algorithm <ref type=\"bibr\" target=\"#b39\">[40]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref>, can be used to search the architecture under different co huge number of supernet parameters and complex supernet structure. Previous single-path NAS methods <ref type=\"bibr\" target=\"#b17\">[18]</ref>[40] <ref type=\"bibr\" target=\"#b8\">[9]</ref> determine a bl d n=\"2.\">Related Work</head><p>Recently, one-shot NAS <ref type=\"bibr\" target=\"#b25\">[26]</ref>[37] <ref type=\"bibr\" target=\"#b17\">[18]</ref> has received much attention because of reduced search cost uld be retrained for N times. This makes DNAS less flexible.</p><p>In contrast, single-path methods <ref type=\"bibr\" target=\"#b17\">[18]</ref>[10] <ref type=\"bibr\" target=\"#b8\">[9]</ref>[8] decouple su .3.\">Unified Supernet</head><p>Previous single-path NAS <ref type=\"bibr\" target=\"#b9\">[10]</ref>[9] <ref type=\"bibr\" target=\"#b17\">[18]</ref>[40] adopts the MobilenetV2 inverted bottleneck <ref type=\" 2\">[23]</ref>. Besides, we also search the network with FLOPs under 320M by the evolution algorithm <ref type=\"bibr\" target=\"#b17\">[18]</ref> as another baseline. Table <ref type=\"table\" target=\"#tab_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: datasets like CIFAR10 or part of ImageNet was proposed in <ref type=\"bibr\" target=\"#b36\">[37]</ref> <ref type=\"bibr\" target=\"#b35\">[36]</ref>[21] <ref type=\"bibr\" target=\"#b25\">[26]</ref>[39] <ref typ respectively, because of the cost of re-executing search. Supernet retraining is needed for FBNetV2 <ref type=\"bibr\" target=\"#b35\">[36]</ref> and AtomNAS <ref type=\"bibr\" target=\"#b27\">[28]</ref>, whi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  also highly correlated with latency, as mentioned in <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b20\">[21]</ref>. By combining the hardware constraint loss L C and the cro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>). We directly modify the searched architecture by replacing all ReLU activation with H-Swish <ref type=\"bibr\" target=\"#b18\">[19]</ref> activation and equip it with the squeeze-and-excitation mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pled a small part of the supernet for training in each iteration to reduce computation cost. DA-NAS <ref type=\"bibr\" target=\"#b11\">[12]</ref> designed a dataadaptive pruning strategy for efficient arc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atch, the evaluation results show that SGNAS achieves 77.1% top-1 accuracy on the Ima-geNet dataset <ref type=\"bibr\" target=\"#b13\">[14]</ref> at around 370M FLOPs, which is comparable with the state-o se refer to supplementary materials for more details.</p><p>For experiments on the ImageNet dataset <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we train the unified supernet for 50 epochs using batch s ad><p>A. <ref type=\"bibr\">1. Dataset</ref> We perform all experiments based on the ImageNet dataset <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Same as the settings in <ref type=\"bibr\" target=\"#b4\">[5]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bustness of SGNAS more fairly, we evaluate it based on a NAS benchmark dataset called NAS-Bench-201 <ref type=\"bibr\" target=\"#b16\">[17]</ref>. NAS-Bench-201 includes 15,625 architectures in total. It . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : differentiable NAS (DNAS) and single-path NAS.</p><p>In addition to optimizing the supernet, DNAS <ref type=\"bibr\" target=\"#b15\">[16]</ref>[26] <ref type=\"bibr\" target=\"#b36\">[37]</ref>[38] <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>). We directly modify the searched architecture by replacing all ReLU activation with H-Swish <ref type=\"bibr\" target=\"#b18\">[19]</ref> activation and equip it with the squeeze-and-excitation mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <figDesc>Comparison with the SOTAs for different hardware constraints. \u2020 : training with AutoAugment<ref type=\"bibr\" target=\"#b10\">[11]</ref>. \u2021 : searching on a proxy dataset. The unit of search time. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y represent one configuration with one kernel size and one expansion rate, previous single-path NAS <ref type=\"bibr\" target=\"#b4\">[5]</ref>[9][10] needs to construct blocks of various configurations i , channel size of each layer and layer number) same as <ref type=\"bibr\" target=\"#b9\">[10]</ref> and <ref type=\"bibr\" target=\"#b4\">[5]</ref>, but utilize the proposed unified blocks to reduce GPU memor erations is 13 times larger than FairNAS <ref type=\"bibr\" target=\"#b9\">[10]</ref> and Proxy-lessNAS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, but the number of supernet parameters for the unified super .0\" xml:id=\"fig_7\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b39\">[40]</ref>[9]<ref type=\"bibr\" target=\"#b4\">[5]</ref> when the number of possible operations in each layer increas ts based on the ImageNet dataset <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Same as the settings in <ref type=\"bibr\" target=\"#b4\">[5]</ref> <ref type=\"bibr\" target=\"#b39\">[40]</ref>[9], we randomly sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ld <ref type=\"bibr\" target=\"#b28\">(Morcos et al., 2011)</ref>, sparse inverse covariance estimation <ref type=\"bibr\" target=\"#b20\">(Jones et al., 2011)</ref>, and pseudolikelihood maximization <ref ty vised structure learning with Potts models performs poorly when few related sequences are available <ref type=\"bibr\" target=\"#b20\">(Jones et al., 2011;</ref><ref type=\"bibr\" target=\"#b21\">Kamisetty et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >, and pseudolikelihood maximization <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b21\">Kamisetty e  commonly used to fit the parameters <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013)</ref>. Pseudolikelihood approximates the likeli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: behavioral cloning and imitation learning <ref type=\"bibr\" target=\"#b11\">(Duriez et al., 2017;</ref><ref type=\"bibr\" target=\"#b31\">Ratliff et al., 2007;</ref><ref type=\"bibr\" target=\"#b2\">Bain &amp; S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: behavioral cloning and imitation learning <ref type=\"bibr\" target=\"#b11\">(Duriez et al., 2017;</ref><ref type=\"bibr\" target=\"#b31\">Ratliff et al., 2007;</ref><ref type=\"bibr\" target=\"#b2\">Bain &amp; S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b21\">Kamisetty et al., 2013)</ref>. To construct the MSA for a given input y when few related sequences are available <ref type=\"bibr\" target=\"#b20\">(Jones et al., 2011;</ref><ref type=\"bibr\" target=\"#b21\">Kamisetty et al., 2013;</ref><ref type=\"bibr\" target=\"#b29\">Moult et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n et al., 2008)</ref>. For the independent Potts model baselines in all experiments, we use CCMpred <ref type=\"bibr\" target=\"#b34\">(Seemayer et al., 2014)</ref>, a GPU implementation of pseudolikeliho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"table\">1</ref> compares amortized optimization and NPM, making a connection to multi-task learning <ref type=\"bibr\" target=\"#b6\">(Caruana, 1998)</ref>. Additionally, we could frame NPM as a hypernetw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eins of greatest interest for unsupervised structure prediction are likely to have lower depth MSAs <ref type=\"bibr\" target=\"#b41\">(Tetchner et al., 2014)</ref>. This is especially a problem for highe 14)</ref>. This is especially a problem for higher organisms, where there are fewer related genomes <ref type=\"bibr\" target=\"#b41\">(Tetchner et al., 2014)</ref>. The hope is that for low-depth MSAs, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a Potts model energy function to the MSA <ref type=\"bibr\" target=\"#b24\">(Lapedes et al., 1999;</ref><ref type=\"bibr\" target=\"#b42\">Thomas et al., 2008;</ref><ref type=\"bibr\" target=\"#b45\">Weigt et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b46\">(Xu, 2018;</ref><ref type=\"bibr\" target=\"#b35\">Senior et al., 2019;</ref><ref type=\"bibr\" target=\"#b48\">Yang et al., 2019)</ref>. State-of-the-art methods use supervised lea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e standard method for unsupervised contact prediction fits a Potts model energy function to the MSA <ref type=\"bibr\" target=\"#b24\">(Lapedes et al., 1999;</ref><ref type=\"bibr\" target=\"#b42\">Thomas et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed in the MSA. These patterns are in turn associated with the structure and function of the protein <ref type=\"bibr\" target=\"#b15\">(G\u00f6bel et al., 1994)</ref>. Unsupervised contact prediction aims to d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"table\">1</ref> compares amortized optimization and NPM, making a connection to multi-task learning <ref type=\"bibr\" target=\"#b6\">(Caruana, 1998)</ref>. Additionally, we could frame NPM as a hypernetw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g performs best. We now perform an evaluation in the more realistic setting of the UniRef50 dataset <ref type=\"bibr\" target=\"#b40\">(Suzek et al., 2007)</ref>. First we examine MSA depth across UniRef5 ef type=\"bibr\" target=\"#b40\">(Suzek et al., 2007)</ref>. First we examine MSA depth across UniRef50 <ref type=\"bibr\" target=\"#b40\">(Suzek et al., 2007)</ref>. Appendix C.4 Fig. <ref type=\"figure\" targ EF50 TRAINING DATA AND SETUP</head><p>For the experiments in Section 4.2, we retrieve the UniRef-50 <ref type=\"bibr\" target=\"#b40\">(Suzek et al., 2007)</ref> database dated 2018-03. The UniRef50 clust. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b45\">Weigt et al., 2009)</ref>. Various approximations are used in practice including mean field <ref type=\"bibr\" target=\"#b28\">(Morcos et al., 2011)</ref>, sparse inverse covariance estimation <re rization \u03c1(W ) = \u03bb J J 2 + \u03bb h h 2 is added, and sequences are reweighted to account for redundancy <ref type=\"bibr\" target=\"#b28\">(Morcos et al., 2011)</ref>. We write the regularized finite sample e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e standard method for unsupervised contact prediction fits a Potts model energy function to the MSA <ref type=\"bibr\" target=\"#b24\">(Lapedes et al., 1999;</ref><ref type=\"bibr\" target=\"#b42\">Thomas et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b0\">Alley et al. (2019)</ref>, <ref type=\"bibr\" target=\"#b19\">Heinzinger et al. (2019), and</ref><ref type=\"bibr\" target=\"#b26\">Madani et al. (2020)</ref> trained models with autoregressive objecti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mation <ref type=\"bibr\" target=\"#b20\">(Jones et al., 2011)</ref>, and pseudolikelihood maximization <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeber  the normalization constant is intractable, pseudolikelihood is commonly used to fit the parameters <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeber  target=\"#b34\">(Seemayer et al., 2014)</ref>, a GPU implementation of pseudolikelihood maximization <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011)</ref>. The coupling matrix J from the inde. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b21\">Kamisetty et al., 2013)</ref>. To construct the MSA for a given input y when few related sequences are available <ref type=\"bibr\" target=\"#b20\">(Jones et al., 2011;</ref><ref type=\"bibr\" target=\"#b21\">Kamisetty et al., 2013;</ref><ref type=\"bibr\" target=\"#b29\">Moult et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s seek to make the class distribution more balanced, using over-sampling or downsampling techniques <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>; algorithm-level app t reduces this imbalance, but can lead to over-fitting as no extra information is introduced. SMOTE <ref type=\"bibr\" target=\"#b7\">[8]</ref> addresses this problem by generating new samples, performing nority so as to alleviate the issue of majority classes dominating the loss function.</p><p>\u2022 SMOTE <ref type=\"bibr\" target=\"#b7\">[8]</ref>: Synthetic minority oversampling techniques generate synthet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b37\">38]</ref>. One typical task is semi-supervised node classification <ref type=\"bibr\" target=\"#b38\">[39]</ref>, where we have a large graph with a small ratio of nodes l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral networks(GNNs) <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. One typical task is semi-supervised node classification <r ]</ref>, which samples and aggregates embedding from local neighbors of each sample. More recently, <ref type=\"bibr\" target=\"#b37\">[38]</ref> extends expressive power of GNNs to that of WL test, and <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chniques such as SMOTE are the most popular and effective approaches for addressing class imbalance <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. However, existing w lanced is a good choice, which is consistent with existing work for synthetic minority oversampling <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral networks(GNNs) <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. One typical task is semi-supervised node classification <r ]</ref>, which samples and aggregates embedding from local neighbors of each sample. More recently, <ref type=\"bibr\" target=\"#b37\">[38]</ref> extends expressive power of GNNs to that of WL test, and <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cally introduce different mis-classification penalties or prior probabilities for different classes <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ular and effective approaches for addressing class imbalance <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. However, existing work are overwhelmingly dedicated to i.i .1.3\">Evaluation Metrics.</head><p>Following existing works in evaluating imbalanced classification <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, we adopt three cr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ssing framework, which is composed of pattern extraction and interaction modeling within each layer <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Generally, existing GNN frameworks can be categorized int. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imation to F measurement, which can be directly optimized by gradient propagation. Threshold moving <ref type=\"bibr\" target=\"#b20\">[21]</ref> modifies the inference process after the classifier is tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n. To overcome this deficiency, many extensions are proposed to remove only redundant samples, like <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. The second group of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing existing works in evaluating imbalanced classification <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, we adopt three criteria: classification accuracy(ACC), mea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the Open Graph Benchmark (OGB) has been introduced to provide a collection of larger graph datasets <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a)</ref>, but they are still small compared to graphs room for further improvement. All the OGB-LSC datasets are available through the OGB Python package <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a)</ref>. All the baseline and package code is availa  molecular feature developed by the chemistry community) can be obtained. By default, we follow OGB <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a)</ref> to convert the SMILES string into a molecula virtual node is shown to be effective across a wide range of graph-level prediction datasets in OGB <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a)</ref>. Edge features are incorporated following <r heir PubChem ID (CID) with ratio 80/10/10. Our original intention was to provide the scaffold split <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a;</ref><ref type=\"bibr\" target=\"#b64\">Wu et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b45\">(Sch\u00fctt et al., 2017;</ref><ref type=\"bibr\" target=\"#b27\">Klicpera et al., 2020;</ref><ref type=\"bibr\" target=\"#b43\">Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Hu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )</ref>.</p><p>PCQM4M is a quantum chemistry dataset originally curated under the PubChemQC project <ref type=\"bibr\" target=\"#b36\">(Nakata, 2015;</ref><ref type=\"bibr\" target=\"#b37\">Nakata and Shimaza. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aseline code and Python package are built on top of excellent open-source software, including NUMPY <ref type=\"bibr\" target=\"#b18\">(Harris et al., 2020)</ref>, PYTORCH <ref type=\"bibr\" target=\"#b38\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2020)</ref>.</p><p>Hyper-parameters. For the loss function, we use the negative sampling loss from <ref type=\"bibr\" target=\"#b53\">Sun et al. (2019)</ref>, where we pick margin \u03b3 from {1,4,8,10,100}. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a homogeneous graph, but we extend the analysis to the heterogenous graph by considering meta-paths <ref type=\"bibr\" target=\"#b52\">(Sun et al., 2011)</ref>-a path consisting of a sequence of relations. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b8\">Chen et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Chiang et al., 2019;</ref><ref type=\"bibr\" target=\"#b71\">Zeng et al., 2020)</ref>. More recently, researchers improve the mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ted into undirected and unlabeled homogeneous graphs with duplicated edges removed. The SNAP library<ref type=\"bibr\" target=\"#b29\">(Leskovec and Sosi\u010d, 2016)</ref> is then used to compute the graph st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mmender systems <ref type=\"bibr\" target=\"#b70\">(Ying et al., 2018)</ref>, hyperlinked Web documents <ref type=\"bibr\" target=\"#b26\">(Kleinberg, 1999)</ref>, knowledge graphs (KGs) <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2020)</ref>.</p><p>Hyper-parameters. For the loss function, we use the negative sampling loss from <ref type=\"bibr\" target=\"#b53\">Sun et al. (2019)</ref>, where we pick margin \u03b3 from {1,4,8,10,100}. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the GNN models with NS, we use a hidden dimensionality of 1024. We make use of batch normalization <ref type=\"bibr\" target=\"#b24\">(Ioffe and Szegedy, 2015)</ref> and ReLU activation in all models.</p mmation is used for graph-level pooling. For all MLPs (including GIN's), we use batch normalization <ref type=\"bibr\" target=\"#b24\">(Ioffe and Szegedy, 2015)</ref> and ReLU activation.</p><p>Discussion. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ies designed for natural language processing <ref type=\"bibr\" target=\"#b6\">[7]</ref> and graph data <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta e and graph levels, and an example is the self-supervised learning method for graph neural networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Similarly, the generative framework GPT-GNN <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing <ref type=\"bibr\" target=\"#b6\">[7]</ref> and graph data <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar ral networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Similarly, the generative framework GPT-GNN <ref type=\"bibr\" target=\"#b14\">[15]</ref> employs a self-supervised attributed graph generation task. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: consider a specific type of side information of items for the dedicated recommendation applications <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p contextdependent word representations. The BERT <ref type=\"bibr\" target=\"#b6\">[7]</ref> and XLNET <ref type=\"bibr\" target=\"#b37\">[38]</ref> models use attention mechanisms to learn the word represen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #fig_0\">1</ref>(a). Traditional methods exploit item side information by manual feature engineering <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and then employ factorization machine <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge of recommendation techniques have been proposed, from classic collaborative filtering techniques <ref type=\"bibr\" target=\"#b29\">[30]</ref> to the recent deep learning models <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge of recommendation techniques have been proposed, from classic collaborative filtering techniques <ref type=\"bibr\" target=\"#b29\">[30]</ref> to the recent deep learning models <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: items has also been exploited and showed effectiveness in further improving recommendation accuracy <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Example side information of items include textual descrip. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b25\">[26]</ref>, LINE <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and Node2vec <ref type=\"bibr\" target=\"#b7\">[8]</ref>. The recent popularity of GNNs motivates the development of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: items has also been exploited and showed effectiveness in further improving recommendation accuracy <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Example side information of items include textual descrip. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  track from the movie trailer with FFmpeg<ref type=\"foot\" target=\"#foot_5\">6</ref> and adopt VGGish <ref type=\"bibr\" target=\"#b12\">[13]</ref> to obtain the acoustic modality of the movie. The dimensio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons. Multimodal Feature Extraction. In the experiments, we use the pre-trained Inception-v4 network <ref type=\"bibr\" target=\"#b31\">[32]</ref> to extract the visual features of each image. Then, we ave. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion of items for the dedicated recommendation applications <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. The full multimoda. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en the node representations and the representation of the graph. Recently, self-supervised learning <ref type=\"bibr\" target=\"#b15\">[16]</ref> is employed to simultaneously pre-train GNNs at both node . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ther domain where pre-training is usually adopted. The shallow pre-training methods, e.g., word2vec <ref type=\"bibr\" target=\"#b22\">[23]</ref> and GloVe <ref type=\"bibr\" target=\"#b24\">[25]</ref>, learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le, a general pretraining paradigm for CV tasks is firstly training a model on the ImageNet dataset <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and then fine-tuning the pre-trained model for a specific t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b11\">[12]</ref> as the base model for item recommendation task, and Deep &amp; Cross Network (DCN) <ref type=\"bibr\" target=\"#b35\">[36]</ref> as the base model for CTR prediction task. For item recomm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gs</head><p>4.1.1 Experimental Datasets. The experiments are performed on the Amazon review dataset <ref type=\"bibr\" target=\"#b23\">[24]</ref> and Movielens-20M dataset<ref type=\"foot\" target=\"#foot_1\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eature engineering <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and then employ factorization machine <ref type=\"bibr\" target=\"#b28\">[29]</ref> or gradient boosting machine <ref type=\"bibr\" target=\"#b3\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation applications <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. The full multimodality side information of items are not f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he shallow pre-training methods, e.g., word2vec <ref type=\"bibr\" target=\"#b22\">[23]</ref> and GloVe <ref type=\"bibr\" target=\"#b24\">[25]</ref>, learn word representations based on the word cooccurrence. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onization quality, Action Units (AU) detection <ref type=\"bibr\" target=\"#b2\">[3]</ref> (by OpenFace <ref type=\"bibr\" target=\"#b3\">[4]</ref>) for facial action coding consistency between source and gen dence values are better.</p><p>Besides, we employ an action units (AU) detection module by OpenFace <ref type=\"bibr\" target=\"#b3\">[4]</ref> to compute the facial action units intensities for the sourc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: framework in PyTorch <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Both networks are trained with Adam <ref type=\"bibr\" target=\"#b19\">[20]</ref> solver with initial learning rate 0.0005. We train each mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[34]</ref>. Based on the developed 3D face reconstruction <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> and generative adversarial networks, more and more approach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b35\">[36]</ref> or 2D landmarks <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Due to the information loss caused by the intermediate rep ef type=\"bibr\" target=\"#b43\">44]</ref> or facial landmarks <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. In contrast to our method, they are restrained by the repr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: low-up works extend this idea by using in-the-wild training data including appearance interpolation <ref type=\"bibr\" target=\"#b21\">[22]</ref>, introducing deformable neural radiance fields to represen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ions that fulfill the train-ing data of a still facial image <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" targe ethod. There are a branch of talking head generation methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  establish the relationships between audio semantics and lip motions, such as phonemeviseme mapping <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Therefore, they are inconvenient for general applications. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Earlier methods built upon professional artist modelling <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> or complicated motion capture system <ref type=\"bibr\" targe l-based approaches <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> require expertise works to establish the relationships betw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: framework in PyTorch <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Both networks are trained with Adam <ref type=\"bibr\" target=\"#b19\">[20]</ref> solver with initial learning rate 0.0005. We train each mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> or facial landmarks <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. In contrast to ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed in high-end areas of the movie and game industry. Recently, many deep-learning-based tech-niques <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> or fixed by static head pose <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ere is a similar problem setting in information theory called information bottleneck (IB) principle <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which aims to juice out a compressed code from the origin ng, attempts to find a short code of the input signal but preserves maximum information of the code <ref type=\"bibr\" target=\"#b23\">[24]</ref>. <ref type=\"bibr\" target=\"#b25\">[26]</ref> firstly bridges. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  among graphs <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Recently, it is popular to select a neighborhood subgraph  among graphs <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>. However, it is troublesome to directly optimize the above. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hborhood subgraph of a central node to do message passing in node representation learning. DropEdge <ref type=\"bibr\" target=\"#b38\">[39]</ref> relieves the over-smoothing phenomenon in deep GCNs by ran pe=\"bibr\" target=\"#b53\">[54]</ref> in terms of classification accuracy. Moreover, we apply DropEdge <ref type=\"bibr\" target=\"#b38\">[39]</ref> to GAT, namely GAT+DropEdge, which randomly drop 30% edges p><p>Recent work shows that regularization on messagepassing can lead to better node representation <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref>. This is orthog. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms to find the subgraph with the highest density (e.g. the number of edges over the number of nodes <ref type=\"bibr\" target=\"#b33\">[34]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>). Frequent subg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[22]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>, <ref type=\"bibr\" target=\"#b52\">[53]</ref>, <ref type=\"bibr\" target=\"#b53\">[54]</ref>. <ref type=\"bibr\" target=\"#b21\">[22]</ref> enhances the gr #b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>, <ref type=\"bibr\" target=\"#b53\">[54]</ref> in terms of classification accuracy. Moreover, we apply Dr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>Along with wide applications, the major difficulty  \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>. Beyond that, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ition and propose a loss to stabilize the training process.</p><p>1. Notice that the MINE estimator <ref type=\"bibr\" target=\"#b56\">[57]</ref> straightforwardly uses the DONSKER-VARADHAN representation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  3D geometric data. Traditional convolutional network can not consume such irregular data. PointNet <ref type=\"bibr\" target=\"#b48\">[49]</ref> first proposes a unified framework to process the point cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>Along with wide applications, the major difficulty  \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>. Beyond that, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, natural language processing <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and speech and acoustics <ref type=\"bibr\" target=\"#b47\">[48]</ref> due to the capability of learning compact and meaningful r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: enerally leverage all structure information for prediction <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, the interpretation for their prediction results has lagged yielding graph representation besides mean/sum aggregation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref> and pooling aggr n. Similarly, researchers discover the vital substructure at node level via the attention mechanism <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr raph. We plug SIB into various backbones including GCN <ref type=\"bibr\" target=\"#b0\">[1]</ref>, GAT <ref type=\"bibr\" target=\"#b9\">[10]</ref>, GIN <ref type=\"bibr\" target=\"#b12\">[13]</ref> and GraphSAG  compare the proposed method with the mean/sum aggregation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: popularity in many natural language processing (NLP) tasks <ref type=\"bibr\" target=\"#b14\">[12,</ref><ref type=\"bibr\" target=\"#b44\">42,</ref><ref type=\"bibr\" target=\"#b46\">44]</ref>.</p><p>Similarly, n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b30\">[28,</ref><ref type=\"bibr\" target=\"#b65\">63]</ref>, cascaded frameworks <ref type=\"bibr\" target=\"#b16\">[14,</ref><ref type=\"bibr\" target=\"#b52\">50]</ref>, and model update  s are shown in Figure <ref type=\"figure\" target=\"#fig_5\">7</ref>, where the recently proposed C-RPN <ref type=\"bibr\" target=\"#b16\">[14]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[28]</ref>, ATOM. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tly surpasses them such as SiamRPN++, SiamDW <ref type=\"bibr\" target=\"#b65\">[63]</ref> and SiamMask <ref type=\"bibr\" target=\"#b58\">[56]</ref> by a considerable margin. The VOT2019 challenge winner (i.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trackers as TrSiam and TrDiMP, respectively. In these two versions, the backbone model is ResNet-50 <ref type=\"bibr\" target=\"#b20\">[18]</ref> for feature extraction. Before the encoder and decoder, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b52\">50]</ref>, and model update mechanisms <ref type=\"bibr\" target=\"#b18\">[16,</ref><ref type=\"bibr\" target=\"#b19\">17,</ref><ref type=\"bibr\" target=\"#b62\">60,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b22\">[20,</ref><ref type=\"bibr\" target=\"#b38\">36,</ref><ref type=\"bibr\" target=\"#b37\">35,</ref><ref type=\"bibr\" target=\"#b17\">15,</ref><ref type=\"bibr\" target=\"#b40\">38,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: such as DiMP-50 <ref type=\"bibr\">[3]</ref>, D3S <ref type=\"bibr\" target=\"#b36\">[34]</ref>, SiamFC++ <ref type=\"bibr\" target=\"#b61\">[59]</ref>, Retain-MAML <ref type=\"bibr\" target=\"#b51\">[49]</ref>, DC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance for visual tracking. However, most tracking paradigms <ref type=\"bibr\" target=\"#b31\">[29,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" target=\"#b51\">49]</ref> handle this task by \" target=\"#b53\">[51,</ref><ref type=\"bibr\" target=\"#b55\">53]</ref>, sophisticated backbone networks <ref type=\"bibr\" target=\"#b30\">[28,</ref><ref type=\"bibr\" target=\"#b65\">63]</ref>, cascaded framewor [3]</ref> is superior to the simple cross-correlation in Siamese trackers <ref type=\"bibr\">[1,</ref><ref type=\"bibr\" target=\"#b30\">28]</ref>. Nevertheless, in the experiments, we show that with the he _5\">7</ref>, where the recently proposed C-RPN <ref type=\"bibr\" target=\"#b16\">[14]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[28]</ref>, ATOM <ref type=\"bibr\" target=\"#b9\">[7]</ref>, DiMP-50 <re  the accuracy, robustness, and EAO scores of the recent top-performing trackers including SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[28]</ref>, DiMP-50 <ref type=\"bibr\">[3]</ref>, PrDiMP-50 <ref type=\" this dataset, our TrDiMP achieves an AUC score of 71.1%, surpassing the recently proposed SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[28]</ref>, PrDiMP-50 <ref type=\"bibr\" target=\"#b11\">[9]</ref>, SiamR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd freely convey various signals in the temporal domain.</p><p>Generally, most tracking methods [1, <ref type=\"bibr\" target=\"#b48\">46,</ref><ref type=\"bibr\" target=\"#b31\">29,</ref><ref type=\"bibr\" tar thods such as Siamese network <ref type=\"bibr\">[1]</ref> or discriminative correlation filter (DCF) <ref type=\"bibr\" target=\"#b48\">[46,</ref><ref type=\"bibr\" target=\"#b9\">7,</ref><ref type=\"bibr\">3]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kernel in an end-to-end manner, which is further promoted by the probabilistic regression framework <ref type=\"bibr\" target=\"#b11\">[9]</ref>.</p><p>Despite the impressive performance, most existing me he original tracking parts (e.g., tracking optimization model <ref type=\"bibr\">[3]</ref> and IoUNet <ref type=\"bibr\" target=\"#b11\">[9]</ref>) in an end-toend manner. Our framework is trained for 50 ep predicting the response map for target localization, they all adopt the recent probabilistic IoUNet <ref type=\"bibr\" target=\"#b11\">[9]</ref> for target scale estimation. Our trackers are implemented i r block, we merely utilize encoder to promote the feature fusion of 1 With the probabilistic IoUNet <ref type=\"bibr\" target=\"#b11\">[9]</ref> and a larger search area, our baseline performance is bette ef type=\"bibr\" target=\"#b51\">[49]</ref>, DCFST <ref type=\"bibr\" target=\"#b66\">[64]</ref>, PrDiMP-50 <ref type=\"bibr\" target=\"#b11\">[9]</ref>, KYS <ref type=\"bibr\" target=\"#b4\">[2]</ref>, and Siam-RCNN f>, ATOM <ref type=\"bibr\" target=\"#b9\">[7]</ref>, DiMP-50 <ref type=\"bibr\">[3]</ref>, and PrDiMP-50 <ref type=\"bibr\" target=\"#b11\">[9]</ref> are included for comparison. Our TrSiam and TrDiMP outperfo  SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[28]</ref>, DiMP-50 <ref type=\"bibr\">[3]</ref>, PrDiMP-50 <ref type=\"bibr\" target=\"#b11\">[9]</ref>, Retain-MAML <ref type=\"bibr\" target=\"#b51\">[49]</ref>, KYS roaches in Table <ref type=\"table\">5</ref>. Specifically, our TrDiMP performs on par with PrDiMP-50 <ref type=\"bibr\" target=\"#b11\">[9]</ref>, which represents the current best algorithm on this benchm 1%, surpassing the recently proposed SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[28]</ref>, PrDiMP-50 <ref type=\"bibr\" target=\"#b11\">[9]</ref>, SiamR-CNN <ref type=\"bibr\" target=\"#b50\">[48]</ref>, and K temporal information to some extent. Besides, our baseline includes the recent probabilistic IoUNet <ref type=\"bibr\" target=\"#b11\">[9]</ref> for accurate target scale estimation and adopts a larger se ich is more efficient than the recent approaches such as DiMP <ref type=\"bibr\">[3]</ref> and PrDiMP <ref type=\"bibr\" target=\"#b11\">[9]</ref>.</p></div><figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:i  0.1. Similar to the previous works <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\">3,</ref><ref type=\"bibr\" target=\"#b11\">9,</ref><ref type=\"bibr\" target=\"#b4\">2]</ref>, we utilize the traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mer models <ref type=\"bibr\" target=\"#b54\">(Vaswani et al., 2017)</ref>, we used the pretrained BART <ref type=\"bibr\" target=\"#b26\">(Lewis et al., 2020)</ref> for NQG, CNN/DM, and XSum; we used standar tectures. For NQG, CNN/DM, and XSum, we also experiment with one of the top-performing models, BART <ref type=\"bibr\" target=\"#b26\">(Lewis et al., 2020)</ref>. Our experiments are based on the pretrain. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing-to-search has been applied to RNNs to incorporate losses of sequences deviating from references <ref type=\"bibr\" target=\"#b24\">(Leblond et al., 2018)</ref>, but they require annotations or cost fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olicy gradient to optimize reward from a discriminator that differentiates good vs. bad generations <ref type=\"bibr\" target=\"#b61\">(Yu et al., 2017;</ref><ref type=\"bibr\" target=\"#b27\">Li et al., 2017. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibution \u03c0 b is importance sampling, which leads to the following unbiased estimator of the gradient <ref type=\"bibr\" target=\"#b38\">(Precup et al., 2000)</ref>:</p><formula xml:id=\"formula_4\">E \u03c4 \u223c\u03c0 b . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing-to-search has been applied to RNNs to incorporate losses of sequences deviating from references <ref type=\"bibr\" target=\"#b24\">(Leblond et al., 2018)</ref>, but they require annotations or cost fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mer models <ref type=\"bibr\" target=\"#b54\">(Vaswani et al., 2017)</ref>, we used the pretrained BART <ref type=\"bibr\" target=\"#b26\">(Lewis et al., 2020)</ref> for NQG, CNN/DM, and XSum; we used standar tectures. For NQG, CNN/DM, and XSum, we also experiment with one of the top-performing models, BART <ref type=\"bibr\" target=\"#b26\">(Lewis et al., 2020)</ref>. Our experiments are based on the pretrain. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: EGASUS; <ref type=\"bibr\" target=\"#b62\">Zhang et al., 2020)</ref> and 44.20/21.17/41.30 (ProphetNet; <ref type=\"bibr\" target=\"#b39\">Qi et al., 2020)</ref>, both slightly higher than BART. Note the PEGA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[14]</ref> proposed a DSM fusion (DSMF) branch structure model.</p><p>Pyramid pooling layer (PPL) <ref type=\"bibr\" target=\"#b14\">[15]</ref> and DeepLab <ref type=\"bibr\" target=\"#b15\">[16]</ref> have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: propose an adaptive feature selection (AFS) module to deal with the shortage. Previously, Li et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposed a selective kernel (SK) unit by fusing multiple b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ture model.</p><p>Pyramid pooling layer (PPL) <ref type=\"bibr\" target=\"#b14\">[15]</ref> and DeepLab <ref type=\"bibr\" target=\"#b15\">[16]</ref> have shown remarkable learning capabilities of feature rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  surface models (DSMs) information to further improve the semantic segmentation results, Cao et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed a DSM fusion (DSMF) branch structure model.</p><p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o achieve outstanding performance in many vision tasks, such as fully convolutional networks (FCNs) <ref type=\"bibr\" target=\"#b3\">[4]</ref>, U-Net <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepLab <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t results on Vaihingen data set <ref type=\"bibr\" target=\"#b17\">[18]</ref> and WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> e used for training in the testing phase. Additionally, we test the method on WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref> to further evaluate the performance of AFS-based models. S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o achieve outstanding performance in many vision tasks, such as fully convolutional networks (FCNs) <ref type=\"bibr\" target=\"#b3\">[4]</ref>, U-Net <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepLab <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t results on Vaihingen data set <ref type=\"bibr\" target=\"#b17\">[18]</ref> and WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> e used for training in the testing phase. Additionally, we test the method on WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref> to further evaluate the performance of AFS-based models. S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the characteristics of inconsistent scale from high-resolution remote sensing images. Wang et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed an integrated network, which combines the advanta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ave been made by using these existing deep learning models <ref type=\"bibr\" target=\"#b7\">[8]</ref>- <ref type=\"bibr\" target=\"#b9\">[10]</ref> (such as, Hourglass, U-Net, DenseNet, and so on) in the fie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [5]</ref>, DeepLab <ref type=\"bibr\" target=\"#b5\">[6]</ref>, pyramid scene parsing network (PSP-Net) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and so on. Over the last few years, numerous attempts have  org/ns/1.0\"><head>B. PSPNet-AFS</head><p>To validate the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the characteristics of inconsistent scale from high-resolution remote sensing images. Wang et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed an integrated network, which combines the advanta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  surface models (DSMs) information to further improve the semantic segmentation results, Cao et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed a DSM fusion (DSMF) branch structure model.</p><p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d effectively reducing the redundant features to achieve AFS. Squeeze-andexcitation network (SENet) <ref type=\"bibr\" target=\"#b19\">[20]</ref> utilizes the gating mechanism to adaptively recalibrate th orth noting that the PSPNet-AFS achieved better results by using squeeze-and-excitation (SE) module <ref type=\"bibr\" target=\"#b19\">[20]</ref> than PSPNet at the same scales. Although the proposed modu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that can be used as encoding backbone, such as VGG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, ResNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the encoding networks. PSPNet is a scene segmentation mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: problem of the loss of detailed information due to the use of downsampling operations, Sun and Wang <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a maximum fusion scheme, which aims to improve th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r the last few years, numerous attempts have been made by using these existing deep learning models <ref type=\"bibr\" target=\"#b7\">[8]</ref>- <ref type=\"bibr\" target=\"#b9\">[10]</ref> (such as, Hourglas  eight scales is used to validate the effect of the number of feature maps compared with PSPNet-AFS <ref type=\"bibr\" target=\"#b7\">(8,</ref><ref type=\"bibr\" target=\"#b0\">1)</ref>. Not surprisingly, our. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: problem of the loss of detailed information due to the use of downsampling operations, Sun and Wang <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a maximum fusion scheme, which aims to improve th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o achieve outstanding performance in many vision tasks, such as fully convolutional networks (FCNs) <ref type=\"bibr\" target=\"#b3\">[4]</ref>, U-Net <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepLab <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that can be used as encoding backbone, such as VGG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, ResNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the encoding networks. PSPNet is a scene segmentation mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the characteristics of inconsistent scale from high-resolution remote sensing images. Wang et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed an integrated network, which combines the advanta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that can be used as encoding backbone, such as VGG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, ResNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the encoding networks. PSPNet is a scene segmentation mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d effectively reducing the redundant features to achieve AFS. Squeeze-andexcitation network (SENet) <ref type=\"bibr\" target=\"#b19\">[20]</ref> utilizes the gating mechanism to adaptively recalibrate th orth noting that the PSPNet-AFS achieved better results by using squeeze-and-excitation (SE) module <ref type=\"bibr\" target=\"#b19\">[20]</ref> than PSPNet at the same scales. Although the proposed modu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that can be used as encoding backbone, such as VGG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, ResNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the encoding networks. PSPNet is a scene segmentation mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t results on Vaihingen data set <ref type=\"bibr\" target=\"#b17\">[18]</ref> and WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> e used for training in the testing phase. Additionally, we test the method on WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref> to further evaluate the performance of AFS-based models. S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t results on Vaihingen data set <ref type=\"bibr\" target=\"#b17\">[18]</ref> and WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> e used for training in the testing phase. Additionally, we test the method on WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref> to further evaluate the performance of AFS-based models. S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r the last few years, numerous attempts have been made by using these existing deep learning models <ref type=\"bibr\" target=\"#b7\">[8]</ref>- <ref type=\"bibr\" target=\"#b9\">[10]</ref> (such as, Hourglas  eight scales is used to validate the effect of the number of feature maps compared with PSPNet-AFS <ref type=\"bibr\" target=\"#b7\">(8,</ref><ref type=\"bibr\" target=\"#b0\">1)</ref>. Not surprisingly, our. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [5]</ref>, DeepLab <ref type=\"bibr\" target=\"#b5\">[6]</ref>, pyramid scene parsing network (PSP-Net) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and so on. Over the last few years, numerous attempts have  org/ns/1.0\"><head>B. PSPNet-AFS</head><p>To validate the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b0\">[1]</ref>, ResNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As an important branch of computer vision, semantic segment. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that can be used as encoding backbone, such as VGG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, ResNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the encoding networks. PSPNet is a scene segmentation mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  surface models (DSMs) information to further improve the semantic segmentation results, Cao et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed a DSM fusion (DSMF) branch structure model.</p><p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: NNs). However, recent literature has successfully used the idea of defining a sequence over a graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bib  'structured' graph, <ref type=\"bibr\">Mao et al.</ref> showed that graph models can outperform RNNs <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Motivated by these recent successes and in the pursuit of ref>, object detection <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and video representation learning <ref type=\"bibr\" target=\"#b10\">[11]</ref>. GCNs have been used to address skeleton-based action reco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: st of the paper.</p><p>\u2022 Graph inception. We extend the idea of inception layer in traditional CNNs <ref type=\"bibr\" target=\"#b43\">[44]</ref> to the graph domain, and introduce a graph inception modul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  over a graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Considering a video frame sequence as a 'structured' grap ef>. GCNs have been used to address skeleton-based action recognition recorded using motion capture <ref type=\"bibr\" target=\"#b12\">[13]</ref>. The application of graph networks has also started emergi nown. A common way to define the elements in A is through constructing a distance function manually <ref type=\"bibr\" target=\"#b12\">[13]</ref>. However, this may result into a suboptimal graph represen s a binary adjacency matrix constructed following the method used in graph-based action recognition <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>In addition to the baselines, we compare with two s adjacency: a natural choice is a binary adjacency matrix as used for graph-based action recognition <ref type=\"bibr\" target=\"#b12\">[13]</ref>. This is defined as (A b ) ij = 1 if |i \u2212 j| = 1 and 0 oth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: otion classes. Recent efforts in image-based recognition are focused on using CNNs and its variants <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, and on using a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, speech <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and body gestures <ref type=\"bibr\" target=\"#b4\">[5]</ref>. S ic emotion recognition, recurrent models, such as Long Short Term Memory networks (LSTM) are common <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. These networks of  <ref type=\"bibr\" target=\"#b55\">[56]</ref> and LSTM with Connectionist Temporal Modeling (LSTM-CTC) <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>Results: Table <ref type=\"table\" target=\"#tab_1\">II</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rst order delta coefficients. In addition, motivated by a recent work on speech emotion recognition <ref type=\"bibr\" target=\"#b59\">[60]</ref>, we also add spontaneity as a binary feature. The spontane. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GCNs have been successfully applied to various image and video-based tasks, such as face clustering <ref type=\"bibr\" target=\"#b15\">[16]</ref>, object detection <ref type=\"bibr\" target=\"#b16\">[17]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: usually used as inputs to CNN models) <ref type=\"bibr\" target=\"#b33\">[34]</ref> and even raw speech <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Recurrent models are prevalent due to their ability to ca heir ability to capture the temporal dynamics of emotion <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>. A 3D RNN model has been recently proposed for end-to-end  dition, we also compare our model with four state-of-art methods in speech emotion recognition: CNN <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CNN-LSTM <ref type=\"bibr\" target=\"#b34\">[35]</ref>, repre -art methods in speech emotion recognition: CNN <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CNN-LSTM <ref type=\"bibr\" target=\"#b34\">[35]</ref>, representation learning <ref type=\"bibr\" target=\"#b55\">[5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ress has been made towards the recognition and analysis of emotion using dynamic facial expressions <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, speech <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: models for classification. Other approaches use spectrograms (usually used as inputs to CNN models) <ref type=\"bibr\" target=\"#b33\">[34]</ref> and even raw speech <ref type=\"bibr\" target=\"#b34\">[35]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  such as Long Short Term Memory networks (LSTM) are common <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. These networks often have complex architecture with millio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b34\">[35]</ref>, CNN-LSTM <ref type=\"bibr\" target=\"#b34\">[35]</ref>, representation learning <ref type=\"bibr\" target=\"#b55\">[56]</ref> and LSTM with Connectionist Temporal Modeling (LSTM-CTC) <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b34\">[35]</ref>, CNN-LSTM <ref type=\"bibr\" target=\"#b34\">[35]</ref>, representation learning <ref type=\"bibr\" target=\"#b55\">[56]</ref> and LSTM with Connectionist Temporal Modeling (LSTM-CTC) <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GCNs have been successfully applied to various image and video-based tasks, such as face clustering <ref type=\"bibr\" target=\"#b15\">[16]</ref>, object detection <ref type=\"bibr\" target=\"#b16\">[17]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en done to analyze emotion evoked by natural images <ref type=\"bibr\" target=\"#b6\">[7]</ref>, videos <ref type=\"bibr\" target=\"#b7\">[8]</ref> and music <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>A. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n used for video-based emotion recognition due to their ability to capture the temporal information <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Another line o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Attentionbased techniques have been widely explored <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, while transfor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ution operation as a frequency domain filtering operation following the theory of signal processing <ref type=\"bibr\" target=\"#b21\">[22]</ref>, where convolution filters are seen as a set of learnable . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: due to their ability to capture the temporal information <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Another line of work focuses on the dynamics of landmark . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sfully used the idea of defining a sequence over a graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Considering a  ost attention <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. GCNs have been successfully applied to various image and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: due to their ability to capture the temporal information <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Another line of work focuses on the dynamics of landmark . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eo-based tasks, such as face clustering <ref type=\"bibr\" target=\"#b15\">[16]</ref>, object detection <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and video representation learning <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: de attributes. This is because landmark points are known to effectively capture the facial dynamics <ref type=\"bibr\" target=\"#b51\">[52]</ref>. We extract 68 landmark points at every video frame using . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b38\">[39]</ref>, while transformer-based architectures are gaining momentum in this field <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Body emotion recognition.: Body expressions are relativel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, and on using adversarial learning <ref type=\"bibr\" target=\"#b25\">[26]</ref>. A few works have proposed to use attention networks to ac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Model Accuracy (%) Parameters</head><p>Databases: We use the MPI emotional body expression database <ref type=\"bibr\" target=\"#b60\">[61]</ref> for our experiments. This database contains 1447 body moti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd acceleration <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>. A trajectory learning approach <ref type=\"bibr\" target=\"#  been considered for recognizing emotion, where a spatial GCN is used to detect the emotional state <ref type=\"bibr\" target=\"#b41\">[42]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sfully used the idea of defining a sequence over a graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Considering a  ost attention <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. GCNs have been successfully applied to various image and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rk proposed to develop the graph structure considering the Weisfeiler-Lehman graph isomorphism test <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and achieved state-of-the-art performance in node classif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Model Accuracy (%) Parameters</head><p>Databases: We use the MPI emotional body expression database <ref type=\"bibr\" target=\"#b60\">[61]</ref> for our experiments. This database contains 1447 body moti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sfully used the idea of defining a sequence over a graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Considering a  ost attention <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. GCNs have been successfully applied to various image and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Model Accuracy (%) Parameters</head><p>Databases: We use the MPI emotional body expression database <ref type=\"bibr\" target=\"#b60\">[61]</ref> for our experiments. This database contains 1447 body moti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s <ref type=\"bibr\" target=\"#b6\">[7]</ref>, videos <ref type=\"bibr\" target=\"#b7\">[8]</ref> and music <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>A. Shirian and T. Guha are with the Department of Com. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  it has recently demonstrated promising results on certain tasks, specifically image classification <ref type=\"bibr\" target=\"#b18\">[19]</ref> and joint vision-language modeling <ref type=\"bibr\" target -purpose backbone for various vision tasks, in contrast to previous Transformer based architectures <ref type=\"bibr\" target=\"#b18\">[19]</ref> which produce feature maps of a single resolution and have .</p><p>Transformer based vision backbones Most related to our work is the Vision Transformer (ViT) <ref type=\"bibr\" target=\"#b18\">[19]</ref> and its follow-ups <ref type=\"bibr\" target=\"#b59\">[60,</re  architecture <ref type=\"bibr\" target=\"#b60\">[61]</ref> and its adaptation for image classification <ref type=\"bibr\" target=\"#b18\">[19]</ref> both conduct global selfattention, where the relationships n in Table <ref type=\"table\">4</ref>. Further adding absolute position embedding to the input as in <ref type=\"bibr\" target=\"#b18\">[19]</ref> drops performance slightly, thus it is not adopted in our  ar classifier. We find this strategy to be as accurate as using an additional class token as in ViT <ref type=\"bibr\" target=\"#b18\">[19]</ref> and DeiT <ref type=\"bibr\" target=\"#b59\">[60]</ref>. In eva get=\"#b52\">53]</ref>. In existing Transformer-based models <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, tokens are all of a fixed scale, a property unsuitable for  of image classification, object detection and semantic segmentation. It outperforms the ViT / DeiT <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref> and ResNe(X)t mode d to initialize a model for fine-tuning with a different window size through bi-cubic interpolation <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>.</p></div> <div xm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b71\">[72]</ref>, random erasing <ref type=\"bibr\" target=\"#b78\">[79]</ref> and stochastic depth <ref type=\"bibr\" target=\"#b33\">[34]</ref>, but not repeated augmentation <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>, more extensive connections <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and more sophisticated forms of convolution <ref type=\"bi ef type=\"bibr\" target=\"#b55\">[56]</ref>, ResNet <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DenseNet <ref type=\"bibr\" target=\"#b32\">[33]</ref>, HRNet <ref type=\"bibr\" target=\"#b61\">[62]</ref>, and Effi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b12\">[13]</ref>, which is one of the fastest Transformer architectures (see <ref type=\"bibr\" target=\"#b58\">[59]</ref>), the proposed shifted window based self-attention computa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2\">[33]</ref>, and more sophisticated forms of convolution <ref type=\"bibr\" target=\"#b66\">[67,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b80\">81]</ref>. With CNNs serving   such as depthwise convolution <ref type=\"bibr\" target=\"#b66\">[67]</ref> and deformable convolution <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b80\">81]</ref>.</p><p>While the C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> or head networks <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> by providing the c attention, we follow <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> by including a rela. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eficial in modeling the high correlation in visual signals <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. Our approach is bo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ree tasks, showing that they are similarly accurate in visual modeling.</p><p>Compared to Performer <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which is one of the fastest Transformer architectures (se. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> in training, except for repeated augmentation <ref type=\"bibr\" target=\"#b29\">[30]</ref> and EMA <ref type=\"bibr\" target=\"#b43\">[44]</ref>, which do not enhance performance. Note that this is contr epeated augmentation <ref type=\"bibr\" target=\"#b29\">[30]</ref> and Exponential Moving Average (EMA) <ref type=\"bibr\" target=\"#b43\">[44]</ref> which do not enhance performance. Note that this is contra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion challenge, CNN architectures have evolved to become increasingly powerful through greater scale <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>, more extensive co f type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref> and ResNe(X)t models <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b66\">67]</ref> significantly with f type=\"bibr\" target=\"#b50\">[51]</ref>, GoogleNet <ref type=\"bibr\" target=\"#b55\">[56]</ref>, ResNet <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DenseNet <ref type=\"bibr\" target=\"#b32\">[33]</ref>, HRNet e of typical convolutional networks, e.g., VGG <ref type=\"bibr\" target=\"#b50\">[51]</ref> and ResNet <ref type=\"bibr\" target=\"#b28\">[29]</ref>. As a result, the proposed architecture can conveniently r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roup convolution characterizes many recent works on images <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, spherical signal <ref type=\"bibr\" target=\"#b39\">[41]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: larger group of symmetries have been shown to be more discriminative and powerful in recent studies <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" targ variant approaches that have seen recent progress, extending from mathematical framework derived in <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Specifically, <ref ty \">48]</ref>. Cohen and Welling later extend the domain of 2D CNNs from translation to finite groups <ref type=\"bibr\" target=\"#b4\">[5]</ref> and further to arbitrary compact groups <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ated LRF to disentangle shape and pose information. By only taking point pair as input, PPF-FoldNet <ref type=\"bibr\" target=\"#b8\">[9]</ref> can learn rotation-invariant descriptors using folding-based erforming RANSAC, following <ref type=\"bibr\" target=\"#b9\">[10]</ref>. We also follow previous works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe  precompute an invariant representation or a local reference frame. Compare to some baselines (e.g. <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>) that requires dense eep learning, e.g. <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, 3) features learned from LRF aligned input <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b9\">10]</ref> 2) handcrafted invariant features w/ and w/o deep learning, e.g. <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plementation would be computational prohibitive. Inspired by the core idea of separable convolution <ref type=\"bibr\" target=\"#b3\">[4]</ref>, we observe that the kernel h with a kernel size |P| \u00d7 |G| c r to that of the Inception module <ref type=\"bibr\" target=\"#b40\">[42]</ref> and its follow-up works <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which have shown the promising property of separable convol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> and voxels <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b9\">10]</ref> 2) handcrafted invariant features w/ and w/o deep learning, e.g. <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b44\">[46,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b51\">53]</ref> is widely employed to transform the local neighborhood of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> and voxels <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plementation would be computational prohibitive. Inspired by the core idea of separable convolution <ref type=\"bibr\" target=\"#b3\">[4]</ref>, we observe that the kernel h with a kernel size |P| \u00d7 |G| c r to that of the Inception module <ref type=\"bibr\" target=\"#b40\">[42]</ref> and its follow-up works <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which have shown the promising property of separable convol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b49\">51]</ref>. We point interested readers to <ref type=\"bibr\" target=\"#b  this end, we evaluate our approach on two rotationrelated datasets: the rotated Modelnet40 dataset <ref type=\"bibr\" target=\"#b49\">[51]</ref> and the 3DMatch dataset <ref type=\"bibr\" target=\"#b52\">[54 eate the rotated ModelNet40 dataset based on the train/test split of the aligned ModelNet40 dataset <ref type=\"bibr\" target=\"#b49\">[51]</ref>. We mainly focus on a more challenging \"rotated\" setting w n and Retrieval. The classification and retrieval tasks on Modelnet40 follow evaluation metric from <ref type=\"bibr\" target=\"#b49\">[51]</ref>. In addition, our network is trained with GA pooling and p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, spherical signal <ref type=\"bibr\" target=\"#b39\">[41]</ref>, voxel grid <ref type=\"bibr\" target=\"#b47\">[49]</ref> and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. The other line of research strives to achieve rotation-inv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e mainstream strategy resorts to local shape context encoded by geometry histogram and its variants <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b9\">10]</ref> 2) handcrafted invariant features w/ and w/o deep learning, e.g. <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> and voxels <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ape context encoded by geometry histogram and its variants <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" tar erence frame (LRF) <ref type=\"bibr\" target=\"#b44\">[46,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b51\">53]</ref> is widely employed  dcrafted invariant features w/ and w/o deep learning, e.g. <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, 3) features learned . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>. Other works consider different representations of point clouds, noticeably image projection <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> and voxels <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: res of point clouds. In particular, inspired by the spirit of \"going wider\" in the Inception module <ref type=\"bibr\" target=\"#b40\">[42]</ref>, we first propose SE(3) separable convolution, a novel par the supplementary materials). The working principle here is similar to that of the Inception module <ref type=\"bibr\" target=\"#b40\">[42]</ref> and its follow-up works <ref type=\"bibr\" target=\"#b3\">[4]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e mainstream strategy resorts to local shape context encoded by geometry histogram and its variants <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with localized filters to align with the regular grid CNNs <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Explicit convoluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oposal extends such a solution to all memory hierarchy, which is getting deeper recently.</p><p>D2M <ref type=\"bibr\" target=\"#b26\">[27]</ref> finds the actual location of a block in the memory hierarc p><p>Cache level prediction can achieve high accuracy with a simple table. Also, in contrast to D2M <ref type=\"bibr\" target=\"#b26\">[27]</ref>, the overall system design can remain untouched because it. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  entries and OS changes.</p><p>Cache bypassing technique <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> selectively inserts data blocks in the cache. Because many. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ot successfully filter requests, only adding lookup delays <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>.</p><p>Prefetch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tween levels prior to the execution of load instructions <ref type=\"bibr\" target=\"#b10\">[11]</ref>- <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bib curacy of numerous state-of-the-art academic prefetchers <ref type=\"bibr\" target=\"#b10\">[11]</ref>- <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bib ed by prefetching, and accuracy is the fraction of useful prefetches. In the best cases (e.g., DCPT <ref type=\"bibr\" target=\"#b12\">[13]</ref>), 40% of misses are eliminated but with high overhead of 4 eir combinations. The highest-performing scheme overall in our experiments uses the DCPT prefetcher <ref type=\"bibr\" target=\"#b12\">[13]</ref> with degree 2 in L3 and tagged next-line prefetchers of de refetchers have been proposed in the past decade [3]- <ref type=\"bibr\" target=\"#b4\">[5]</ref> [11]- <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , however, it also needs to enlarge the TLB entries and OS changes.</p><p>Cache bypassing technique <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> selectively ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: requests. Way prediction reduces energy consumption by avoiding searching all ways to match the tag <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>Software prefetching is an appealing solution to re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bib \">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bib \">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in higher load latencies when caches do not successfully filter requests, only adding lookup delays <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rediction (also referred to as memory access prediction) <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bib ibr\" target=\"#b28\">[29]</ref>, and a \"miss map\" approach <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p><p>The miss-history predictors rely on classic binary  s-map approach, therefore, uses a table <ref type=\"bibr\" target=\"#b19\">[20]</ref> or a Bloom filter <ref type=\"bibr\" target=\"#b22\">[23]</ref> to track which blocks are in a specific cache.</p><p>One a type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref> and address-indexed <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref> miss predictors 0\"><head>VI. RELATED WORK</head><p>Per level hit/miss predictor is either used in front of L1 cache <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref> to handle instr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ring posterior and prior distributions for the gating function. There are also concurrent MoE works <ref type=\"bibr\" target=\"#b40\">(Ma et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Qin et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b3\">(Chawla et al., 2002;</ref><ref type=\"bibr\" target=\"#b19\">Han et al., 2005;</ref><ref type=\"bibr\" target=\"#b22\">He et al., 2008)</ref>. Nevertheless, traditional random sampling met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Kong et al., 2020)</ref>, GSN <ref type=\"bibr\" target=\"#b1\">(Bouritsas et al., 2020)</ref>, WEGL <ref type=\"bibr\" target=\"#b34\">(Kolouri et al., 2021)</ref>. For all these methods, we use official . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b13\">Fey et al., 2018;</ref><ref type=\"bibr\" target=\"#b63\">Zhang &amp; Chen, 2019;</ref><ref type=\"bibr\" target=\"#b29\">Jia et al., 2020)</ref>. For instance, in chemistry, a molecule could. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b38\">(Lin et al., 2017)</ref> lowers the weights of the well-classified samples. GHM <ref type=\"bibr\" target=\"#b37\">(Li et al., 2019)</ref> improves FocalLoss by further lowering the we 38\">(Lin et al., 2017)</ref>, LDAM <ref type=\"bibr\" target=\"#b56\">(Wallach et al., 2020)</ref>, GHM <ref type=\"bibr\" target=\"#b37\">(Li et al., 2019)</ref>, and Decoupling <ref type=\"bibr\" target=\"#b30. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng et al. (2019)</ref> propose to use re-sampling strategy in a two-stage training scheme. Besides, <ref type=\"bibr\" target=\"#b39\">Liu et al. (2020)</ref>, <ref type=\"bibr\" target=\"#b31\">Kim et al. (2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y a growing class of graph isomorphic methods <ref type=\"bibr\" target=\"#b59\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b43\">Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Corso et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Pan et al., 2015)</ref> and functional brain analysis <ref type=\"bibr\" target=\"#b45\">(Pan et al., 2016)</ref>, graph datasets contain multiple classificat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in more common cases, such as drug discovery <ref type=\"bibr\" target=\"#b24\">(Hu et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">B\u00e9cigneul et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Pan et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for end-to-end learning to directly optimize ranking metrics <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> by creating a differentiable approximation. ApproxNDCG <ref t </ref><ref type=\"bibr\" target=\"#b7\">8]</ref> by creating a differentiable approximation. ApproxNDCG <ref type=\"bibr\" target=\"#b7\">[8]</ref> revisits the idea proposed in <ref type=\"bibr\" target=\"#b30\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b23\">[24]</ref> and neural networks <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. For learning the univariate scoring function, Deep Structu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ised machine learning methods for implicit diversification <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" tar oves the scoring function by considering difference between positive and negative rankings. NTN-DIV <ref type=\"bibr\" target=\"#b40\">[41]</ref> uses a neural tensor network to learn document similarity  features as input, while methods below are taking the distributed representations as input. NTN-DIV <ref type=\"bibr\" target=\"#b40\">[41]</ref>: a learning approach which learns novelty features based o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ieval community, with the seminal work of Maximal Marginal Relevance (MMR) dating back to year 1998 <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Since users often do not examine all the returned re ost implicit approaches are inspired by the seminal work of Maximal Marginal Relevance (MMR) method <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which iteratively picks relevant documents that are novel  We compare with the following baseline methods, including several recent state-of-the-art ones: MMR <ref type=\"bibr\" target=\"#b9\">[10]</ref>: a heuristic approach with the documents selected sequentia by top-ranked ones.</p><p>While traditional methods for diversification are mainly manually crafted <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etrics is LambdaRank <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, which uses the metric delta when two documents are swapped. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ://www.tei-c.org/ns/1.0\"><head n=\"3.1.2\">ERR-IA.</head><p>For the same setting as above, the ERR-IA <ref type=\"bibr\" target=\"#b11\">[12]</ref> is defined as</p><formula xml:id=\"formula_5\">ERR-IA \u2261 \ud835\udc5b \ud835\udc56=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esearch in this area shifts to supervised learning methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" tar sts from a distribution. Recently proposed PAMM <ref type=\"bibr\" target=\"#b39\">[40]</ref> and DVGAN <ref type=\"bibr\" target=\"#b25\">[26]</ref> maximize the margin between sampled positive and negative  tter performance. However, the huge number of candidates poses challenges for high-quality sampling <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>The main difficulty of the existing learning-based   thus belongs to the implicit category. This is different from the explicit approaches (e.g., DVGAN <ref type=\"bibr\" target=\"#b25\">[26]</ref> or DSSA <ref type=\"bibr\" target=\"#b22\">[23]</ref>), which   select relevant documents which are diverse to current selected set. As a follow-up to DSSA, DVGAN <ref type=\"bibr\" target=\"#b25\">[26]</ref> uses Generative Adversarial Networks to frame the diversif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ocument list {\ud835\udc51 \ud835\udc56 }, which can be generated by a trainable neural encoder. Popular choices are BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> or doc2vec <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Esse. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" tar try to maximize the expected rewards over sampled lists from a distribution. Recently proposed PAMM <ref type=\"bibr\" target=\"#b39\">[40]</ref> and DVGAN <ref type=\"bibr\" target=\"#b25\">[26]</ref> maximi rank framework, to model document relations of an ideally diversified ranking. Based on R-LTR, PAMM <ref type=\"bibr\" target=\"#b39\">[40]</ref> improves the scoring function by considering difference be \"#b45\">[46]</ref>: a learning approach developed in the relational learning to rank framework; PAMM <ref type=\"bibr\" target=\"#b39\">[40]</ref>: a learning approach that optimizes \ud835\udefc-NDCG using structure ons for inter-document similarity. Supervised machine learning methods for implicit diversification <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ieval community, with the seminal work of Maximal Marginal Relevance (MMR) dating back to year 1998 <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Since users often do not examine all the returned re ost implicit approaches are inspired by the seminal work of Maximal Marginal Relevance (MMR) method <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which iteratively picks relevant documents that are novel  We compare with the following baseline methods, including several recent state-of-the-art ones: MMR <ref type=\"bibr\" target=\"#b9\">[10]</ref>: a heuristic approach with the documents selected sequentia by top-ranked ones.</p><p>While traditional methods for diversification are mainly manually crafted <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </head><p>Neural networks are amenable for end-to-end learning to directly optimize ranking metrics <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> by creating a differen ation of the approximated metric (over induced permutations) is shown to gain additional robustness <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Based on these, we  . Finally, another useful variation to this diversity ranking loss is to add a stochastic treatment <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><formula xml:id=\"formula_11\">L Gumbel-\ud835\udefc-DCG ({\ud835\udc60 \ud835\udc5e \ud835\udc56 }) =. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: document interactions as listwise context have been proposed in such as Deep Listwise Context Model <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Groupwise Scoring Functions <ref type=\"bibr\" target=\"#b2\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: document interactions as listwise context have been proposed in such as Deep Listwise Context Model <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Groupwise Scoring Functions <ref type=\"bibr\" target=\"#b2\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imilar to Transformer <ref type=\"bibr\" target=\"#b36\">[37]</ref>, we also apply residual connections <ref type=\"bibr\" target=\"#b20\">[21]</ref> and layer normalization <ref type=\"bibr\" target=\"#b3\">[4]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: directly optimize user's utility instead of any proxies of diversity. Determinantal Point Processes <ref type=\"bibr\" target=\"#b38\">[39]</ref> and Multi-Armed Bandits <ref type=\"bibr\" target=\"#b35\">[36. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to the diversity evaluation metrics. However, different from the standard learning-to-rank setting <ref type=\"bibr\" target=\"#b26\">[27]</ref>, the design of learning approaches for diversification is  gated score features like BM25 or TF-IDF (commonly used in the traditional learning-to-rank setting <ref type=\"bibr\" target=\"#b26\">[27]</ref>), we resort to the distributed representations of queries  ei-c.org/ns/1.0\"><head n=\"2.2\">Learning To Rank</head><p>Learning to rank has traditionally focused <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> on relevance. A sc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and shows superior performance with respect to the diversit et=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> learn a scoring function that optimize diversification usin ing lists to consider. Different methods are proposed to alleviate this problem. For example, R-LTR <ref type=\"bibr\" target=\"#b45\">[46]</ref> and SVM-DIV <ref type=\"bibr\" target=\"#b43\">[44]</ref> main M-DIV <ref type=\"bibr\" target=\"#b43\">[44]</ref> utilizes structural SVMs for scoring, whereas R-LTR <ref type=\"bibr\" target=\"#b45\">[46]</ref> proposes a relational learning to rank framework, to model [44]</ref>: a learning approach which utilizes structural SVMs to optimize subtopic coverage; R-LTR <ref type=\"bibr\" target=\"#b45\">[46]</ref>: a learning approach developed in the relational learning . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and shows superior  it diversification <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> learn a scoring fun to alleviate this problem. For example, R-LTR <ref type=\"bibr\" target=\"#b45\">[46]</ref> and SVM-DIV <ref type=\"bibr\" target=\"#b43\">[44]</ref> mainly focus on the ideal diversified ranking lists. Reinf  scoring function that optimize diversification using remote proxies of evaluation metrics. SVM-DIV <ref type=\"bibr\" target=\"#b43\">[44]</ref> utilizes structural SVMs for scoring, whereas R-LTR <ref t </ref>: a heuristic method of optimizing proportionality for search result diversification; SVM-DIV <ref type=\"bibr\" target=\"#b43\">[44]</ref>: a learning approach which utilizes structural SVMs to opt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly manually crafted <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, recent research in this area shifts to supervised learning >Explicit diversification approaches, e.g., PM-2 <ref type=\"bibr\" target=\"#b13\">[14]</ref> or xQuAD <ref type=\"bibr\" target=\"#b33\">[34]</ref> improve the coverage over subtopics relevant to a query ba ic approach with the documents selected sequentially according to maximal marginal relevance; xQuAD <ref type=\"bibr\" target=\"#b33\">[34]</ref>: a representative method which models subtopics of the ori. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esearch in this area shifts to supervised learning methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" tar sts from a distribution. Recently proposed PAMM <ref type=\"bibr\" target=\"#b39\">[40]</ref> and DVGAN <ref type=\"bibr\" target=\"#b25\">[26]</ref> maximize the margin between sampled positive and negative  tter performance. However, the huge number of candidates poses challenges for high-quality sampling <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>The main difficulty of the existing learning-based   thus belongs to the implicit category. This is different from the explicit approaches (e.g., DVGAN <ref type=\"bibr\" target=\"#b25\">[26]</ref> or DSSA <ref type=\"bibr\" target=\"#b22\">[23]</ref>), which   select relevant documents which are diverse to current selected set. As a follow-up to DSSA, DVGAN <ref type=\"bibr\" target=\"#b25\">[26]</ref> uses Generative Adversarial Networks to frame the diversif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ised machine learning methods for implicit diversification <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" tar oves the scoring function by considering difference between positive and negative rankings. NTN-DIV <ref type=\"bibr\" target=\"#b40\">[41]</ref> uses a neural tensor network to learn document similarity  features as input, while methods below are taking the distributed representations as input. NTN-DIV <ref type=\"bibr\" target=\"#b40\">[41]</ref>: a learning approach which learns novelty features based o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he relationship between query-document pairs. In this work, we use a simple algorithm, latent cross <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which can effectively generate high-order interaction featu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e user preferences <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar o help construct better user profiles. Besides, some works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> attempted to disamb a used to help ranking, such as the shared word embeddings <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. The association be t=\"#b8\">[9]</ref> extracts topic-based and clicked-based features from the search history, and PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref> trains personal word embeddings with the user's individual k. In the first one, namely FedPSFlat, we adapt the state-of-the-art personalized search model PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref> to make it privacy compatible. We select PEPS because it u vior representation vectors <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and personal word embeddings <ref type=\"bibr\" target=\"#b46\">[47]</ref>. User profiles are usually aggregated vectorized represent ing on every client as a separate task. Focusing on state-of-the-art personalized search model PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref>, it sets up a module which contains personal word embeddin 1\">2</ref>. The main modules are briefly introduced as follows, and more details can be referred to <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>Word embedding layer. There is a global word embedd \"#b40\">[41]</ref> to enhance the representation of the current query for disambiguation.</p><p>PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref>: It trained personal word embeddings for each user to clar initialize the global and personal word embeddings. Other hyper-parameters are set the same as PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref>. We adopt Adam optimizer to train the personalization mode , MRR and P@1. A more reliable metric for personalized search P-Improve is also employed. Following <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, we only use P-Imp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  proposed, including traditional methods relying on features <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targe  We first mine the user's preferences from the log as her interest profile. For example, SLTB model <ref type=\"bibr\" target=\"#b8\">[9]</ref> extracts topic-based and clicked-based features from the sea >[42]</ref><ref type=\"bibr\" target=\"#b42\">[43]</ref><ref type=\"bibr\" target=\"#b43\">[44]</ref>. SLTB <ref type=\"bibr\" target=\"#b8\">[9]</ref> combined both the click-based and topic-based features extra user profiles are used in existing works. Typical profiles include term, topic, click distributions <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, sequential search b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  under a query. This raises the risk of user privacy leakage <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. At present, the pr privacy protection is receiving more and more attention. Many countries formulate some privacy laws <ref type=\"bibr\" target=\"#b22\">[23]</ref>. In this paper, we focus on the privacy protection issue i ious attackers on the web, and the obtained personal data can be used in various ways. According to <ref type=\"bibr\" target=\"#b22\">[23]</ref>, most users are worried about their personal data being co ocess to infer her query intents and interests. As people become more concerned about their privacy <ref type=\"bibr\" target=\"#b22\">[23]</ref>, privacy protection in personalization receives widespread. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: clicked documents which are used to express user interests <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: documents. As for the four types of word representations, we use the neural matching component KNRM <ref type=\"bibr\" target=\"#b45\">[46]</ref> to compute the interactive matching scores, i.e. \ud835\udc39 \ud835\udc43\ud835\udc4a , \ud835\udc39  ines, including neural ranking models, personalized search models and privacy enhanced models. KNRM <ref type=\"bibr\" target=\"#b45\">[46]</ref>: It is a neural model using kernels to extract features fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o infer their interests and real intent under a query. This raises the risk of user privacy leakage <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ bibr\" target=\"#b4\">[5]</ref>. To reduce the linkability, the query obfuscation solution is explored <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target Additional fake queries are generated along with the real query to obscure the user's query intents <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target er profile at the group level, hiding the privacy information of each single user.</p><p>CoverQuery <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>: This approach first a ons for privacy protection in search mainly consider the identifiability and linkability of privacy <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Identifiability means who is the user. Linkability is the p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b11\">[12]</ref>. Moreover, some works <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> demonstrated that the user's location, reading level and ot. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  models that employ deep learning to mine user preferences <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" tar  hidden in the history and dynamically build user profile according to the current query. Lu et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> and Yao et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> r ent list and the information of click behaviors. Following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, we view the documents with more than 30 seconds of dwellin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b42\">43]</ref> and learning based models that employ deep learning to mine user preferences <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta user privacy.</p><p>\u2022 Other auxiliary data used to help ranking, such as the shared word embeddings <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" ta ach query corresponds to a complete document list and the information of click behaviors. Following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, we view the docum  type=\"bibr\" target=\"#b38\">[39]</ref> used the individual data to adapt the global model. Ge et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> designed a hierarchical RNN with queryaware attention to c 9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, sequential search behavior representation vectors <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and personal word embeddings <ref type=\"bibr\" target=\"#b4 ernels to extract features from interactions between the query and document for ranking.</p><p>HRNN <ref type=\"bibr\" target=\"#b20\">[21]</ref>: This work employed a hierarchical RNN with queryaware att. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. It aims to hide the user's real search intent among a set  y obfuscation method <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> to hide the user's genuine query intent among a set of gene  key problem of this paper, and more details can be found in <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. Different from recording cover queries into the search log. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o infer their interests and real intent under a query. This raises the risk of user privacy leakage <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ bibr\" target=\"#b4\">[5]</ref>. To reduce the linkability, the query obfuscation solution is explored <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target Additional fake queries are generated along with the real query to obscure the user's query intents <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target er profile at the group level, hiding the privacy information of each single user.</p><p>CoverQuery <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>: This approach first a ons for privacy protection in search mainly consider the identifiability and linkability of privacy <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Identifiability means who is the user. Linkability is the p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: earch due to its representation learning ability for mining potential user preferences. Song et al. <ref type=\"bibr\" target=\"#b38\">[39]</ref> used the individual data to adapt the global model. Ge et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. It aims to hide the user's real search intent among a set  y obfuscation method <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> to hide the user's genuine query intent among a set of gene  key problem of this paper, and more details can be found in <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. Different from recording cover queries into the search log. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: user privacy leakage <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. At present, the problem of user privacy protection is rece bserved query behaviors. Some studies utilize anonymous user id or group id to mask user identities <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. But some users ma [23]</ref>, privacy protection in personalization receives widespread attentions.</p><p>Shen et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> defined four levels of privacy protection in personalized . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: earch due to its representation learning ability for mining potential user preferences. Song et al. <ref type=\"bibr\" target=\"#b38\">[39]</ref> used the individual data to adapt the global model. Ge et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  under a query. This raises the risk of user privacy leakage <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. At present, the pr privacy protection is receiving more and more attention. Many countries formulate some privacy laws <ref type=\"bibr\" target=\"#b22\">[23]</ref>. In this paper, we focus on the privacy protection issue i ious attackers on the web, and the obtained personal data can be used in various ways. According to <ref type=\"bibr\" target=\"#b22\">[23]</ref>, most users are worried about their personal data being co ocess to infer her query intents and interests. As people become more concerned about their privacy <ref type=\"bibr\" target=\"#b22\">[23]</ref>, privacy protection in personalization receives widespread. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o clarify the keywords by modeling the interactions between contexts with multi-head self-attention <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and personalized query representation \ud835\udc43\ud835\udc44 \ud835\udc5e .</p><p>With t  target=\"#b49\">[50]</ref> proposed to encode historical queries as the context with the transformer <ref type=\"bibr\" target=\"#b40\">[41]</ref> to enhance the representation of the current query for dis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: documents. As for the four types of word representations, we use the neural matching component KNRM <ref type=\"bibr\" target=\"#b45\">[46]</ref> to compute the interactive matching scores, i.e. \ud835\udc39 \ud835\udc43\ud835\udc4a , \ud835\udc39  ines, including neural ranking models, personalized search models and privacy enhanced models. KNRM <ref type=\"bibr\" target=\"#b45\">[46]</ref>: It is a neural model using kernels to extract features fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>. Consequently, several defense methods were proposed, such as Multi-Party Computation (MPC) <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, Homomorphic Encry. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Without centrally collected query logs, we initialize the global word embeddings with the word2vec <ref type=\"bibr\" target=\"#b30\">[31]</ref> model trained on the document collection or the Wikipedia . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b42\">43]</ref> and learning based models that employ deep learning to mine user preferences <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta user privacy.</p><p>\u2022 Other auxiliary data used to help ranking, such as the shared word embeddings <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" ta ach query corresponds to a complete document list and the information of click behaviors. Following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, we view the docum  type=\"bibr\" target=\"#b38\">[39]</ref> used the individual data to adapt the global model. Ge et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> designed a hierarchical RNN with queryaware attention to c 9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, sequential search behavior representation vectors <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and personal word embeddings <ref type=\"bibr\" target=\"#b4 ernels to extract features from interactions between the query and document for ranking.</p><p>HRNN <ref type=\"bibr\" target=\"#b20\">[21]</ref>: This work employed a hierarchical RNN with queryaware att. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>. Consequently, several defense methods were proposed, such as Multi-Party Computation (MPC) <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, Homomorphic Encry. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  based on artificial design but fails to preserve more complex local structures. Besides, GeniePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> learns the importance of different neighbours and builds a rove the use of graph information based on the smoothness values of different networks. \u2022 GeniePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> is also a GNN model with adaptive receptive fields by the  t=\"#b12\">[13]</ref> 76.13 \u00b1 1.38% 67.51 \u00b1 1.07% 72.38 \u00b1 0.37% 80.03 \u00b1 1.23% 34.12 \u00b1 3.26% GeniePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> 76.59 \u00b1 1.92% 66.95 \u00b1 1.33% 73.   </p></div> <div xmlns=\"h \">[33]</ref> 59.34 \u00b1 1.10% CS-GNN <ref type=\"bibr\" target=\"#b12\">[13]</ref> 59.91 \u00b1 0.22% GeniePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> 62.31 \u00b1 0.34% STAR-GNN (ours) 69.29 \u00b1 1.43% </p></div> <di \">[34]</ref> takes advantage of anchor sets to incorporate position information in GNNs. Ge-niePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposes a breadth function to learn the importance of dif  GNNs with adaptive fields, most attention scores are calculated by the similarity of node features <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Some existing mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lative node similarity <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Methods based on Skip-Gram like <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ferent graph structures with the same ability as Weisfeiler-Lehman graph isomorphism test. \u2022 JK-Net <ref type=\"bibr\" target=\"#b32\">[33]</ref> leverages different neighbourhood ranges to enable structu rget=\"#b31\">[32]</ref> 72.51 \u00b1 0.99% 64.50 \u00b1 0.95% 73.38 \u00b1 1.96% 76.29 \u00b1 2.63% 30.35 \u00b1 2.09% JK-Net <ref type=\"bibr\" target=\"#b32\">[33]</ref> 78.38 \u00b1 0.38% 66.94 \u00b1 0.75% 75.07 \u00b1 0.29% 78.46 \u00b1 1.15% 37 =\"#b30\">[31]</ref> 60.36 \u00b1 0.21% GIN <ref type=\"bibr\" target=\"#b31\">[32]</ref> 62.00 \u00b1 1.10% JK-Net <ref type=\"bibr\" target=\"#b32\">[33]</ref> 59.34 \u00b1 1.10% CS-GNN <ref type=\"bibr\" target=\"#b12\">[13]</ ighbourhoods and a depth function to filter signals aggregated from neighbours respectively. JK-Net <ref type=\"bibr\" target=\"#b32\">[33]</ref> designs an architecture to leverages different neighbourho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  traditional methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. GNNs define the convolution operations on the neighbourhoo ed on Skip-Gram like <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and variational auto-encoder methods like <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  local graph structures. But it can only capture artificially designed structural information. PGNN <ref type=\"bibr\" target=\"#b33\">[34]</ref> takes advantage of anchor sets to incorporate position inf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion, link prediction, etc, where they achieve great improvements compared with traditional methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ ibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Methods based on Skip-Gram like <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: passing techniques <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. These approaches s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h-based theoretic work <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, these methods exploit classification task to model structu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and the whole graphs. With solid graph-based theoretic work <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, these methods exploi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> adopted the idea of <ref type=\"bibr\" target=\"#b4\">[5]</ref> to maximize the mutual information between inputs and output. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: larity of topological structures of the neighbourhood. On the other hand, as mentioned in the paper <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the soft attention method has the problem of over-smoothing he size of the neighbourhood increases, the phenomenon of over-smoothing will occur, as analyzed in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. <ref type=\"bibr\" target=\"#b2\">[3]</ref> utilizes reinforcem he phenomenon of over-smoothing will occur, as analyzed in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. <ref type=\"bibr\" target=\"#b2\">[3]</ref> utilizes reinforcement learning to construct discrete adapti ll based on soft-attention and prone to the oversmoothness, leading to poor results. Besides, GRARF <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposes to construct adaptive receptive fields with the hel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b31\">32]</ref> data using GNNs, such as HAN <ref type=\"bibr\" target=\"#b38\">[39]</ref> and MAGNN <ref type=\"bibr\" target=\"#b10\">[11]</ref>. These GNN-based heterogeneous models can be interpreted a  attribute information of actors and directors will also have a great impact on node analysis tasks <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The missing attributes will significantly affect the perf >[30,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> based neighbors in a hierarchical manner. MAGNN <ref type=\"bibr\" target=\"#b10\">[11]</ref> model which contains the node content transformation to en sed neighbors aggregation, and heterogeneous types combination.  <ref type=\"formula\">6</ref>) MAGNN <ref type=\"bibr\" target=\"#b10\">[11]</ref>: a heterogeneous GNN. This model realizes the prediction t  Mechanism</head><p>For the case that some types of nodes have no attributes, some previous studies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>Graph embedding <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> aims to project nodes in a graph into a low-dimensional vec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, link prediction <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. The methods based on spectral domain adopt the spectral re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of the attributes of these attributed nodes. Specifically, HGNN-AC first uses HIN-Embedding methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe o capture the rich semantic information brought by network heterogeneity. For example, metapath2vec <ref type=\"bibr\" target=\"#b7\">[8]</ref> generates random node sequences guided by meta-paths <ref ty on. In this paper, HGNN-AC uses existing heterogeneous graph embedding methods such as metapath2vec <ref type=\"bibr\" target=\"#b7\">[8]</ref> or HHNE <ref type=\"bibr\" target=\"#b39\">[40]</ref> to get nod  of MAGNN-AC and GTN-AC with some existing methods, including MAGNN and GTN.</p><p>(1) Metapath2vec <ref type=\"bibr\" target=\"#b7\">[8]</ref>: a skip-gram based heterogeneous embedding method which perf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. The methods based on spectral domain adopt the spectral re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tured data. They are divided into two types: spectral domain <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b25\">[26]</ref>, LINE <ref type=\"bibr\" target=\"#b34\">[35]</ref>, node2vec <ref type=\"bibr\" target=\"#b12\">[13]</ref> and struc2vec <ref type=\"bibr\" target=\"#b26\">[27]</ref>. R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to learn a sortable score to determine which directly conne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iple that a contact between similar entities occurs at a higher rate than among dissimilar entities <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Due to the existence of network homophily <ref type=\"bibr milar entities <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Due to the existence of network homophily <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  approach addresses these problems through the use of a number of locality-sensitive hash functions <ref type=\"bibr\" target=\"#b8\">[9]</ref> which hash each incoming tuple into a fixed number of bucket. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  for representing the entire network, instead of dealing with each tensor in the stream separately. <ref type=\"bibr\" target=\"#b29\">[30]</ref> uses subspace learning in tensors to find anomalies. STA m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se subtensor detection based: Dense-subtensor detection has been used to detect anomalies in M-ZOOM <ref type=\"bibr\" target=\"#b60\">[61]</ref>, D-CUBE <ref type=\"bibr\" target=\"#b61\">[62]</ref>, <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b69\">70,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" target=\"#b79\">80]</ref>, a variety of different approaches have been used in the li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b16\">[17]</ref>, MEG <ref type=\"bibr\" target=\"#b28\">[29]</ref> and Fence GAN <ref type=\"bibr\" target=\"#b40\">[41]</ref> have been successfully used to detect anomalies.</p><p>For. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s points which are separated from the rest of the data at low depth values. Random Cut Forest (RCF) <ref type=\"bibr\" target=\"#b18\">[19]</ref> improves upon isolation forest by creating multiple random. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  deep learning approaches for anomaly detection in multi-aspect data have also been proposed. DAGMM <ref type=\"bibr\" target=\"#b80\">[81]</ref> learns a Gaussian Mixture density model (GMM) over a low-d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 7]</ref>, Random Forests <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and deep learning based methods <ref type=\"bibr\" target=\"#b68\">[69]</ref>  <ref type=\"bibr\" target=\"#b24\">[25]</ref>. However, we re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: derstand the complex relationships between users and items <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. A large recommender with many learning parameters usually  In this regard, unlike the motivation of the previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, items merely ranked highly by the teacher may be not infor  by the student and vice versa. Thus, the existing methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> that simply choose the high-ranked items cannot give enough ded from the public implementation and the original papers <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. For BD, \ud835\udf06 \ud835\udc47 \u2192\ud835\udc46 and \ud835\udf06 \ud835\udc46\u2192\ud835\udc47 are set to 0.5, the number of ite m, a few recent work <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23</ref>] has adopted Knowledge Distillation (KD) to RS. KD is a mode a few recent methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23</ref>] have adopted knowledge distillation to RS. KD is a model-ag e recommendation list predicted by the teacher along with the binary training set. Specifically, in <ref type=\"bibr\" target=\"#b22\">[23]</ref>, the student is trained to give high scores on the top-ran 0 N@50 N@100 H@50 H@100 N@50 N@100 H@50 H@100 N@50 N@100 Teacher 0.1566 \u2022 Ranking Distillation (RD) <ref type=\"bibr\" target=\"#b22\">[23]</ref>: A pioneering KD method for top-\ud835\udc3e RS. RD makes the student ware sampling, 2) Rank-aware sampling <ref type=\"bibr\" target=\"#b15\">[16]</ref>, 3) Top-\ud835\udc41 selection <ref type=\"bibr\" target=\"#b22\">[23]</ref>, 4) Uniform sampling, 5) Swapped rank discrepancy-aware sa  RS. KD is a model-agnostic strategy and we can employ any recommender system as the base model. RD <ref type=\"bibr\" target=\"#b22\">[23]</ref> firstly proposes a KD method that makes the student give h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sticated model architectures to better understand the complex relationships between users and items <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. A large recommend re already ranked highly by the student. In this regard, unlike the motivation of the previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, items merely rank  by the teacher are already ranked highly by the student and vice versa. Thus, the existing methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> that simply choose perparameters, we use the values recommended from the public implementation and the original papers <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. For BD, \ud835\udf06 \ud835\udc47 \u2192\ud835\udc46 an e platforms.</p><p>To tackle this problem, a few recent work <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23</ref>] has adopted Knowled odels).</p><p>To tackle this challenge, a few recent methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23</ref>] have adopted knowle ned to give high scores on the top-ranked items of the teacher's recommendation list. Similarly, in <ref type=\"bibr\" target=\"#b15\">[16]</ref>, the student is trained to imitate the teacher's predictio \"bibr\" target=\"#b25\">[26]</ref> and Foursquare <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Following <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we hold out the last interaction of each user as test int tab_1\">1</ref>. We also report the experimental results on ML100K and AMusic, which are used for CD <ref type=\"bibr\" target=\"#b15\">[16]</ref>, in Appendix for the direct comparison.</p><p>5.1.2 Evalua  the student give high scores on top-ranked items by the teacher. \u2022 Collaborative Distillation (CD) <ref type=\"bibr\" target=\"#b15\">[16]</ref>: A state-of-the-art KD method for top-\ud835\udc3e RS. CD makes the s e different sampling schemes as follows: 1) Rank discrepancy-aware sampling, 2) Rank-aware sampling <ref type=\"bibr\" target=\"#b15\">[16]</ref>, 3) Top-\ud835\udc41 selection <ref type=\"bibr\" target=\"#b22\">[23]</r ted due to the restricted capability and thus the loss of recommendation performance is unavoidable <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Second, several methods try to accelerate the inference p tudent give high scores on the top-ranked items of the teacher's recommendation list. Similarly, CD <ref type=\"bibr\" target=\"#b15\">[16]</ref> makes the student imitate the teacher's prediction scores  foot\" n=\"3\" xml:id=\"foot_2\">We sample the items by using the rank-aware sampling scheme suggested in<ref type=\"bibr\" target=\"#b15\">[16]</ref>. Note that in<ref type=\"bibr\" target=\"#b15\">[16]</ref>, th g the rank-aware sampling scheme suggested in<ref type=\"bibr\" target=\"#b15\">[16]</ref>. Note that in<ref type=\"bibr\" target=\"#b15\">[16]</ref>, the sampled items are used for the distillation.</note> \t ><p>In this section, we report the experimental results on ML100K and AMusic, which are used for CD <ref type=\"bibr\" target=\"#b15\">[16]</ref>, for the direct comparison with CD. We do not include this nd the second last interacted item for validation as done in <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. If there is no timestamp in the dataset, we randomly take  re thorough evaluation compared to using random candidates <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>.</p><p>As we focus on top-\ud835\udc3e recommendation for implicit fee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: acher and the student on the test set. We adopt ResNet-80 as the teacher and ResNet-8 as the student<ref type=\"bibr\" target=\"#b3\">[4]</ref> and train them separately for the image classification task . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ated calculations in high-dimensional space for correct prediction. As shown in the computer vision <ref type=\"bibr\" target=\"#b10\">[11]</ref>, a simple image without complex background or patterns can. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. They learn discret. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ltaneously for better generalization performance in computer vision and natural language processing <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ formation and regularization to each classifier head. In natural language processing, Dual Learning <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposes a learning mechanism that two machine translators t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  language processing <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. In computer vision, Deep Mutual Learning (DML) <ref type=\" \">22,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. In computer vision, Deep Mutual Learning (DML) <ref type=\"bibr\" target=\"#b31\">[32]</ref> trains a cohort of multiple classifiers simultaneously in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>: A learing-to-rank recommender that adopts MF <ref type=\"bibr\" target=\"#b12\">[13]</ref> to model the user-item interaction. BPR uses the pair-wise. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. They learn discrete representations of users and items to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tio (H@\ud835\udc3e) <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Normalized Discounted Cumulative Gain (N@\ud835\udc3e) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. H@\ud835\udc3e measures whether the test item is present in the top-\ud835\udc3e . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iscretization techniques to reduce the size of recommenders <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the inference phase by adopting model-dependent techniques <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. For example, the o order-preserving transformations <ref type=\"bibr\" target=\"#b0\">[1]</ref> and the pruning techniques <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> have been adopted . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ize of recommenders <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acher and the student on the test set. We adopt ResNet-80 as the teacher and ResNet-8 as the student<ref type=\"bibr\" target=\"#b3\">[4]</ref> and train them separately for the image classification task . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ltaneously for better generalization performance in computer vision and natural language processing <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ formation and regularization to each classifier head. In natural language processing, Dual Learning <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposes a learning mechanism that two machine translators t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dent on two real-world datasets: CiteULike <ref type=\"bibr\" target=\"#b25\">[26]</ref> and Foursquare <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Following <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we h rameters of BD (Section 5.6). Foursquare <ref type=\"foot\" target=\"#foot_5\">6</ref> (Tokyo Check-in) <ref type=\"bibr\" target=\"#b27\">[28]</ref> and Yelp<ref type=\"foot\" target=\"#foot_6\">7</ref>  <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tio (H@\ud835\udc3e) <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Normalized Discounted Cumulative Gain (N@\ud835\udc3e) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. H@\ud835\udc3e measures whether the test item is present in the top-\ud835\udc3e . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ize of recommenders <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ize of recommenders <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the inference phase by adopting model-dependent techniques <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. For example, the o order-preserving transformations <ref type=\"bibr\" target=\"#b0\">[1]</ref> and the pruning techniques <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> have been adopted . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  user-item relationships. NeuMF uses the pointwise loss function for the optimization.</p><p>\u2022 CDAE <ref type=\"bibr\" target=\"#b26\">[27]</ref>: A deep recommender that adopts Denoising Autoencoders (DA ive samples is set to 1 for NeuMF and BPR, 5 * |I + \ud835\udc62 | for CDAE as suggested in the original paper <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>For the distillation, we adopt as many learning par. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  recommender for real-time and web-scale platforms.</p><p>To tackle this problem, a few recent work <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targ fic models (e.g., inner product-based models).</p><p>To tackle this challenge, a few recent methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targ cores with particular emphasis on the items ranked highly by the teacher. The most recent work, RRD <ref type=\"bibr\" target=\"#b8\">[9]</ref>, formulates the distillation process as a relaxed ranking ma teacher, they focus on the high-ranked items, which can affect the top-\ud835\udc3e recommendation performance <ref type=\"bibr\" target=\"#b8\">[9]</ref>. By using such supplementary supervisions from the teacher, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atching heuristics to obtain pseudo summaries based on a set of keywords. We use TextRank algorithm <ref type=\"bibr\" target=\"#b20\">[19]</ref> to extract the keywords from the reference summary and obt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b9\">8,</ref><ref type=\"bibr\" target=\"#b10\">9,</ref><ref type=\"bibr\" target=\"#b22\">21,</ref><ref type=\"bibr\" target=\"#b27\">26]</ref> have made great progress on abstractive summarization. The  get=\"#b18\">17,</ref><ref type=\"bibr\" target=\"#b22\">21,</ref><ref type=\"bibr\" target=\"#b25\">24,</ref><ref type=\"bibr\" target=\"#b27\">26]</ref> in abstractive summarization, the ext-abs framework trains  ory. Table <ref type=\"table\">1</ref> shows that two state-of-the-art enc-dec models, i.e., PointGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> and BART <ref type=\"bibr\" target=\"#b14\">[13]</ref>, both f oom. the four store workers arrested could spend three years each in prison if convicted . PointGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> four employees of a popular indian ethnic chain have been  e two representative Enc-Dec model options as our abstractor: the standard Enc-Dec model PointerGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> with attention mechanism <ref type=\"bibr\" target=\"#b19\">[1 rget=\"#b22\">[21]</ref> with the data preprocessing and use the non-anonymized version as See et al. <ref type=\"bibr\" target=\"#b27\">[26]</ref>.</p><p>Webis-TLDR-17 Corpus <ref type=\"bibr\" target=\"#b33\" ibr\" target=\"#b3\">[2,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref>. The pointer-generator networks <ref type=\"bibr\" target=\"#b27\">[26]</ref> consisting of copy mechanism and coverage model are the mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  state-of-the-art enc-dec models, i.e., PointGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> and BART <ref type=\"bibr\" target=\"#b14\">[13]</ref>, both frequently make incorrect alignments by either missi t large . authorities sealed off the store and summoned six top officials from fabindia.</p><p>BART <ref type=\"bibr\" target=\"#b14\">[13]</ref> federal education minister smriti irani was visiting a fab  <ref type=\"bibr\" target=\"#b19\">[18]</ref> and copy mechanism, and a pretrained language model BART <ref type=\"bibr\" target=\"#b14\">[13]</ref> finetuned on our pseudo summaries and reference summaries.  \ud835\udf06 \ud835\udc50 = 1.0, \ud835\udf06 \ud835\udc58 = 0.5, \ud835\udf06 \ud835\udc60 = 0.5 (Eq. 7). For abstractor, the lr of PG is 1\ud835\udc52 \u2212 03. We follow Lewise <ref type=\"bibr\" target=\"#b14\">[13]</ref> in fine-tuning BART with \ud835\udc59\ud835\udc5f = 3\ud835\udc52 \u2212 05 and warmup = 500. Fo -layer transformer network, including unidirctional, bidirectional and seq2seq language model. BART <ref type=\"bibr\" target=\"#b14\">[13]</ref> takes combines bidirectional transformer encoder and auto- hich has been a popular method for abstractive summarization recently. Unlike the end-to-end models <ref type=\"bibr\" target=\"#b14\">[13,</ref><ref type=\"bibr\" target=\"#b18\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tandard Enc-Dec model PointerGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> with attention mechanism <ref type=\"bibr\" target=\"#b19\">[18]</ref> and copy mechanism, and a pretrained language model BART <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b24\">[23]</ref> learns two unidirectional language models of forward and backward. BERT <ref type=\"bibr\" target=\"#b8\">[7]</ref> uses a bidirectional transformer encoder to predict the mask. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he representation of \ud835\udc56-th sentence.</formula><p>The HIBERT document encoder is a pretrained encoder <ref type=\"bibr\" target=\"#b35\">[34]</ref>, which contains two Transformer-based sub-encoders. We com  encoder and HIBERT encoder, as described by Chen <ref type=\"bibr\" target=\"#b6\">[5]</ref> and Zhang <ref type=\"bibr\" target=\"#b35\">[34]</ref>. We fine-tune HIBERT encoder with learning rate (lr) 5\ud835\udc52 \u2212   excellent on language model, Liu et al. <ref type=\"bibr\" target=\"#b17\">[16]</ref> and Zhang et al. <ref type=\"bibr\" target=\"#b35\">[34]</ref> apply pretrained transformers to extractive summarization. models or representations on summarization task, the quality of generated summaries can be improved <ref type=\"bibr\" target=\"#b35\">[34]</ref><ref type=\"bibr\" target=\"#b36\">[35]</ref><ref type=\"bibr\" t tuning the pretrained models on summarization task, the quality of generated summaries are improved <ref type=\"bibr\" target=\"#b35\">[34,</ref><ref type=\"bibr\" target=\"#b37\">36]</ref>.</p><p>The reinfor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n task, the quality of generated summaries can be improved <ref type=\"bibr\" target=\"#b35\">[34]</ref><ref type=\"bibr\" target=\"#b36\">[35]</ref><ref type=\"bibr\" target=\"#b37\">[36]</ref>. PEGASUS <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h are either news, web pages or user generated QAs on the web for training and test. CNN/Daily Mail <ref type=\"bibr\" target=\"#b11\">[10]</ref> (CNNDM) is a popular summarization dataset, which contains. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  models, such as reinforcement learning <ref type=\"bibr\" target=\"#b23\">[22]</ref> and joint scoring <ref type=\"bibr\" target=\"#b38\">[37]</ref>. As transformer preforms excellent on language model, Liu . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion task, the quality of generated summaries are improved <ref type=\"bibr\" target=\"#b35\">[34,</ref><ref type=\"bibr\" target=\"#b37\">36]</ref>.</p><p>The reinforcement learning (RL) is always used to co s can be improved <ref type=\"bibr\" target=\"#b35\">[34]</ref><ref type=\"bibr\" target=\"#b36\">[35]</ref><ref type=\"bibr\" target=\"#b37\">[36]</ref>. PEGASUS <ref type=\"bibr\" target=\"#b34\">[33]</ref> is a ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b35\">[34]</ref> apply pretrained transformers to extractive summarization. Zhou <ref type=\"bibr\" target=\"#b39\">[38]</ref> and Li et al. <ref type=\"bibr\" target=\"#b15\">[14]</ref> ha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  models, such as reinforcement learning <ref type=\"bibr\" target=\"#b23\">[22]</ref> and joint scoring <ref type=\"bibr\" target=\"#b38\">[37]</ref>. As transformer preforms excellent on language model, Liu . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onal prediction. GPT <ref type=\"bibr\" target=\"#b26\">[25]</ref> employs a unidirectional transformer <ref type=\"bibr\" target=\"#b31\">[30]</ref> to predict the sequence. ELMo <ref type=\"bibr\" target=\"#b2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tence in the reference summary. To this end, they extract the sentence with the highest ROUGE score <ref type=\"bibr\" target=\"#b16\">[15]</ref> for each reference sentence. This simple assumption gives  n.</p><p>Automatic Metrics. ROUGE scores (F1) include ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L(R-L) <ref type=\"bibr\" target=\"#b16\">[15]</ref>.</p><p>Human Evaluation. We randomly select 100 samples fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  from the source to the output. Recently, encoder-decoder (enc-dec) models with attention mechanism <ref type=\"bibr\" target=\"#b4\">[3,</ref><ref type=\"bibr\" target=\"#b5\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e quality of overall summary because of overlapping contents <ref type=\"bibr\" target=\"#b2\">[1,</ref><ref type=\"bibr\" target=\"#b23\">22]</ref>, while summary-level rewards ignore the accuracy of the sen =\"bibr\" target=\"#b21\">20]</ref>. It is extended with variant models, such as reinforcement learning <ref type=\"bibr\" target=\"#b23\">[22]</ref> and joint scoring <ref type=\"bibr\" target=\"#b38\">[37]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  pseudo summaries: sentence-level <ref type=\"bibr\" target=\"#b6\">[5]</ref> and summary-level methods <ref type=\"bibr\" target=\"#b21\">[20,</ref><ref type=\"bibr\" target=\"#b28\">27]</ref>.</p><p>Sentence-le cal neural network as encoder and pointer network as decoder <ref type=\"bibr\" target=\"#b7\">[6,</ref><ref type=\"bibr\" target=\"#b21\">20]</ref>. It is extended with variant models, such as reinforcement . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  from the source to the output. Recently, encoder-decoder (enc-dec) models with attention mechanism <ref type=\"bibr\" target=\"#b4\">[3,</ref><ref type=\"bibr\" target=\"#b5\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ument, where the pseudo summary can be either sentence-level <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b12\">11,</ref><ref type=\"bibr\" target=\"#b30\">29]</ref> or summary-level <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h are either news, web pages or user generated QAs on the web for training and test. CNN/Daily Mail <ref type=\"bibr\" target=\"#b11\">[10]</ref> (CNNDM) is a popular summarization dataset, which contains. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion task, the quality of generated summaries are improved <ref type=\"bibr\" target=\"#b35\">[34,</ref><ref type=\"bibr\" target=\"#b37\">36]</ref>.</p><p>The reinforcement learning (RL) is always used to co s can be improved <ref type=\"bibr\" target=\"#b35\">[34]</ref><ref type=\"bibr\" target=\"#b36\">[35]</ref><ref type=\"bibr\" target=\"#b37\">[36]</ref>. PEGASUS <ref type=\"bibr\" target=\"#b34\">[33]</ref> is a ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tence in the reference summary. To this end, they extract the sentence with the highest ROUGE score <ref type=\"bibr\" target=\"#b16\">[15]</ref> for each reference sentence. This simple assumption gives  n.</p><p>Automatic Metrics. ROUGE scores (F1) include ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L(R-L) <ref type=\"bibr\" target=\"#b16\">[15]</ref>.</p><p>Human Evaluation. We randomly select 100 samples fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e quality of overall summary because of overlapping contents <ref type=\"bibr\" target=\"#b2\">[1,</ref><ref type=\"bibr\" target=\"#b23\">22]</ref>, while summary-level rewards ignore the accuracy of the sen =\"bibr\" target=\"#b21\">20]</ref>. It is extended with variant models, such as reinforcement learning <ref type=\"bibr\" target=\"#b23\">[22]</ref> and joint scoring <ref type=\"bibr\" target=\"#b38\">[37]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h are either news, web pages or user generated QAs on the web for training and test. CNN/Daily Mail <ref type=\"bibr\" target=\"#b11\">[10]</ref> (CNNDM) is a popular summarization dataset, which contains. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  pseudo summaries: sentence-level <ref type=\"bibr\" target=\"#b6\">[5]</ref> and summary-level methods <ref type=\"bibr\" target=\"#b21\">[20,</ref><ref type=\"bibr\" target=\"#b28\">27]</ref>.</p><p>Sentence-le cal neural network as encoder and pointer network as decoder <ref type=\"bibr\" target=\"#b7\">[6,</ref><ref type=\"bibr\" target=\"#b21\">20]</ref>. It is extended with variant models, such as reinforcement . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h are either news, web pages or user generated QAs on the web for training and test. CNN/Daily Mail <ref type=\"bibr\" target=\"#b11\">[10]</ref> (CNNDM) is a popular summarization dataset, which contains. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  unidirectional transformer <ref type=\"bibr\" target=\"#b31\">[30]</ref> to predict the sequence. ELMo <ref type=\"bibr\" target=\"#b24\">[23]</ref> learns two unidirectional language models of forward and b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons (\u210e \u2032 0 , \u210e \u2032 1 , ..., \u210e \u2032 \ud835\udc5b ) as the output. Aligned Pointer Decoder. We extend Pointer Network <ref type=\"bibr\" target=\"#b32\">[31]</ref> as the decoder. The pseudo summary consisting of multi-sen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tence in the reference summary. To this end, they extract the sentence with the highest ROUGE score <ref type=\"bibr\" target=\"#b16\">[15]</ref> for each reference sentence. This simple assumption gives  n.</p><p>Automatic Metrics. ROUGE scores (F1) include ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L(R-L) <ref type=\"bibr\" target=\"#b16\">[15]</ref>.</p><p>Human Evaluation. We randomly select 100 samples fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: maries and reference summaries.</p><p>Abstractive models are based on sequence-to-sequence learning <ref type=\"bibr\" target=\"#b3\">[2,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref>. The pointer-generat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th attention mechanism <ref type=\"bibr\" target=\"#b4\">[3,</ref><ref type=\"bibr\" target=\"#b5\">4,</ref><ref type=\"bibr\" target=\"#b9\">8,</ref><ref type=\"bibr\" target=\"#b10\">9,</ref><ref type=\"bibr\" target er to predict the masked words. NLG models pretrain on sequence-to-sequence (seq2seq) models. UniLM <ref type=\"bibr\" target=\"#b9\">[8]</ref> is a multi-layer transformer network, including unidirctiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tence in the reference summary. To this end, they extract the sentence with the highest ROUGE score <ref type=\"bibr\" target=\"#b16\">[15]</ref> for each reference sentence. This simple assumption gives  n.</p><p>Automatic Metrics. ROUGE scores (F1) include ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L(R-L) <ref type=\"bibr\" target=\"#b16\">[15]</ref>.</p><p>Human Evaluation. We randomly select 100 samples fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ument, where the pseudo summary can be either sentence-level <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b12\">11,</ref><ref type=\"bibr\" target=\"#b30\">29]</ref> or summary-level <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n task, the quality of generated summaries can be improved <ref type=\"bibr\" target=\"#b35\">[34]</ref><ref type=\"bibr\" target=\"#b36\">[35]</ref><ref type=\"bibr\" target=\"#b37\">[36]</ref>. PEGASUS <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b5\">4,</ref><ref type=\"bibr\" target=\"#b9\">8,</ref><ref type=\"bibr\" target=\"#b10\">9,</ref><ref type=\"bibr\" target=\"#b22\">21,</ref><ref type=\"bibr\" target=\"#b27\">26]</ref> have made great pro  end-to-end models <ref type=\"bibr\" target=\"#b14\">[13,</ref><ref type=\"bibr\" target=\"#b18\">17,</ref><ref type=\"bibr\" target=\"#b22\">21,</ref><ref type=\"bibr\" target=\"#b25\">24,</ref><ref type=\"bibr\" tar contains 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs. We follow Nallapati <ref type=\"bibr\" target=\"#b22\">[21]</ref> with the data preprocessing and use the non-anonymized ver. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b35\">[34]</ref> apply pretrained transformers to extractive summarization. Zhou <ref type=\"bibr\" target=\"#b39\">[38]</ref> and Li et al. <ref type=\"bibr\" target=\"#b15\">[14]</ref> ha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h are either news, web pages or user generated QAs on the web for training and test. CNN/Daily Mail <ref type=\"bibr\" target=\"#b11\">[10]</ref> (CNNDM) is a popular summarization dataset, which contains. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: maries and reference summaries.</p><p>Abstractive models are based on sequence-to-sequence learning <ref type=\"bibr\" target=\"#b3\">[2,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref>. The pointer-generat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ither sentence-level <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b12\">11,</ref><ref type=\"bibr\" target=\"#b30\">29]</ref> or summary-level <ref type=\"bibr\" target=\"#b2\">[1,</ref><re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: maries and reference summaries.</p><p>Abstractive models are based on sequence-to-sequence learning <ref type=\"bibr\" target=\"#b3\">[2,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref>. The pointer-generat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ither sentence-level <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b12\">11,</ref><ref type=\"bibr\" target=\"#b30\">29]</ref> or summary-level <ref type=\"bibr\" target=\"#b2\">[1,</ref><re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  pseudo summaries: sentence-level <ref type=\"bibr\" target=\"#b6\">[5]</ref> and summary-level methods <ref type=\"bibr\" target=\"#b21\">[20,</ref><ref type=\"bibr\" target=\"#b28\">27]</ref>.</p><p>Sentence-le cal neural network as encoder and pointer network as decoder <ref type=\"bibr\" target=\"#b7\">[6,</ref><ref type=\"bibr\" target=\"#b21\">20]</ref>. It is extended with variant models, such as reinforcement . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b38\">[37]</ref>. As transformer preforms excellent on language model, Liu et al. <ref type=\"bibr\" target=\"#b17\">[16]</ref> and Zhang et al. <ref type=\"bibr\" target=\"#b35\">[34]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bstractive models are based on sequence-to-sequence learning <ref type=\"bibr\" target=\"#b3\">[2,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref>. The pointer-generator networks <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: domain. Although being just in its infancy, the field has already achieved quite remarkable success <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The prime example is perhaps constituted by data naturally  ises the formalization of learning of data embeddings as functions defined on non-Euclidean domains <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Hyperbolic manifolds, for example, constitute an important  hat in our framework, this hyperparameter controls the amount of parameter sharing in the PHM layer <ref type=\"bibr\" target=\"#b2\">(3)</ref>. In all our experiments, we report the test performance eval. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b9\">[10]</ref>. Such advances led to the development of hyperbolic GNNs. The works <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> have empirically sho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  attention in several applications, from computer vision to natural language processing (NLP) tasks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" ta stabilize training by keeping activations of the network at zero mean and unit variance. Prior work <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> introduced complex ized the component weight matrices as initially described in complex-and quaternion Neural Networks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Both works start . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sing, where vector messages between connected nodes are exchanged and updated using neural networks <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Most of the literature on GNNs has focused on M = R n , t ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. Model performances marked with * include a virtual-node <ref type=\"bibr\" target=\"#b11\">[12]</ref> in their underlying method. U i consists of kd n + n 3 = k. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: computer vision to natural language processing (NLP) tasks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar ></formula><p>This constitute one of the main building blocks for quaternion-valued neural networks <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. The matrix versio  of the network at zero mean and unit variance. Prior work <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> introduced complex and quaternion batch normalization, whic n weights in <ref type=\"bibr\" target=\"#b14\">(15)</ref> by calculating the real logits as defined in <ref type=\"bibr\" target=\"#b10\">(11)</ref>, followed by a sigmoidal activation function \u03c3(\u2022), that is ugh the PHM-layer (3), followed by an additional linear layer to compute the logits as described in <ref type=\"bibr\" target=\"#b10\">(11)</ref>.</p><p>Although we define our GNN as graph classification . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ficiency as well as generalization performance. Some examples include low-rank matrix factorization <ref type=\"bibr\" target=\"#b34\">[35]</ref>, knowledge distillation of large models into smaller model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. Hypercomplex repre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b9\">[10]</ref>. Such advances led to the development of hyperbolic GNNs. The works <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> have empirically sho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ances led to the development of hyperbolic GNNs. The works <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> have empirically shown that the hyperbolic setting is better . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oducts necessary to be computed (n for PHC-n). Our models were implemented in PyTorch version 1.7.1 <ref type=\"bibr\" target=\"#b33\">[34]</ref> which does not provide a CUDA implementation of the Kronec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ward and backward, and advocate setting g l to the identity function.</p><p>The vision transformers <ref type=\"bibr\" target=\"#b18\">[19]</ref> instantiate a particular form of residual architecture: af consider more specifically the vision transformer (ViT) architecture proposed by Dosovitskiy et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> as the reference architecture and adopt the data-efficient [63]</ref> fails to properly converge above 18 layers without adjusting hyper-parameters. Large ViT <ref type=\"bibr\" target=\"#b18\">[19]</ref> models with 24 and 32 layers were trained with large train ween the width and the depth, both contribute to the performance as reported by Dosovitskiy et al.. <ref type=\"bibr\" target=\"#b18\">[19]</ref> with longer training schedules. But if one parameter is to d without external data. We compare CaiT with DeiT <ref type=\"bibr\" target=\"#b62\">[63]</ref>, Vit-B <ref type=\"bibr\" target=\"#b18\">[19]</ref>, TNT <ref type=\"bibr\" target=\"#b25\">[26]</ref>, T2T <ref t  transfer learning results to those of Efficient-Net <ref type=\"bibr\" target=\"#b61\">[62]</ref>, ViT <ref type=\"bibr\" target=\"#b18\">[19]</ref> and DeiT <ref type=\"bibr\" target=\"#b62\">[63]</ref>. These  er architecture working directly on small patches has obtained state of the art results on ImageNet <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Nevertheless, the state of the art has since returned to  : as discussed in the introduction, the architecture (a) of ViT and DeiT is a pre-norm architecture <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, in which the laye ion 224, and optionally fine-tune them at a higher resolution to trade performance against accuracy <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mples and train during 400 epochs with repeated augmentation <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. The learning rate of the AdamW optimizer <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b10\">[11]</ref>. Originally, transformers without convolutions were applied on pixels directly <ref type=\"bibr\" target=\"#b46\">[47]</ref>, even scaling to hundred of layers <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: better performance <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b60\">61]</ref>, however this complicates their training process <ref type=  target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>, multiple loss at different depth <ref type=\"bibr\" target=\"#b60\">[61]</ref>, adding components in the architecture <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: echanism have been used successfully to give a global view in conjunction with (local) convolutions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ature essentially differ on how one defines this residual branch R l is constructed or parametrized <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" ta (\u03a5) as suggested by Touvron et al.. <ref type=\"bibr\" target=\"#b62\">[63]</ref>. We use a RegNet-16GF <ref type=\"bibr\" target=\"#b49\">[50]</ref> as teacher and adopt the \"hard distillation\" <ref type=\"bi ref>, T2T <ref type=\"bibr\" target=\"#b73\">[74]</ref> and to several state-of-theart convnets: Regnet <ref type=\"bibr\" target=\"#b49\">[50]</ref> improved by Touvron et al. <ref type=\"bibr\" target=\"#b62\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mples and train during 400 epochs with repeated augmentation <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. The learning rate of the AdamW optimizer <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b75\">76,</ref><ref type=\"bibr\" target=\"#b77\">78]</ref>, most mimic squeeze-and-excitate <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: better performance <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b60\">61]</ref>, however this complicates their training process <ref type=  target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>, multiple loss at different depth <ref type=\"bibr\" target=\"#b60\">[61]</ref>, adding components in the architecture <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: echanism have been used successfully to give a global view in conjunction with (local) convolutions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  give a global view in conjunction with (local) convolutions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the joint representations. A recent model, based on Contrastive Language-Image Pre-training (CLIP) <ref type=\"bibr\" target=\"#b33\">[34]</ref>, learns a multi-modal embedding space, which may be used t ction \u2206i.</p><p>From natural language to \u2206t In order to reduce text embedding noise, Radford et al. <ref type=\"bibr\" target=\"#b33\">[34]</ref> utilize a technique called prompt engineering that feeds s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b11\">[12]</ref> have revolutionized image synthesis, with recent style-based generative models <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta hat different StyleGAN layers are responsible for different levels of detail in the generated image <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Consequently, it is common to split the layers into three manipulations in Figure <ref type=\"figure\">7</ref> are performed using StyleGAN2 pretrained on FFHQ <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The inputs are real images, embedded in W+ space using th s, demonstrated on portraits of celebrities. Edits are performed using StyleGAN2 pretrained on FFHQ <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The inputs are real images, embedded in W+ space using th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: (VL) representations <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: image synthesis, with recent style-based generative models <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> boasting some of th re> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We use StyleGAN2<ref type=\"bibr\" target=\"#b18\">[19]</ref> in all our experiments.</note> \t\t</body> \t\t<back> \t\t\t<div . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oder architecture, to disentangle the semantics of both input images and text descriptions. ManiGAN <ref type=\"bibr\" target=\"#b21\">[22]</ref> introduces a novel text-image combination module, which pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: image synthesis, with recent style-based generative models <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> boasting some of th re> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We use StyleGAN2<ref type=\"bibr\" target=\"#b18\">[19]</ref> in all our experiments.</note> \t\t</body> \t\t<back> \t\t\t<div . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (VL) representations <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (VL) representations <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nerated images from StyleGAN2-ada <ref type=\"bibr\" target=\"#b16\">[17]</ref> pretrained on AFHQ dogs <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= ections. Here we use StyleGAN-ada <ref type=\"bibr\" target=\"#b16\">[17]</ref> pretrained on AFHQ wild <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which contains wolves, lions, tigers and foxes. There is a  ections. Here we use StyleGAN-ada <ref type=\"bibr\" target=\"#b16\">[17]</ref> pretrained on AFHQ wild <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which contains wolves, lions, tigers and foxes. There is a  ]</ref>. Right: using StyleGAN2-ada<ref type=\"bibr\" target=\"#b16\">[17]</ref> pretrained on AFHQ dogs<ref type=\"bibr\" target=\"#b4\">[5]</ref>. The target attribute used in the text prompt is indicated a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned by CLIP.</p><p>Text-guided image generation and manipulation The pioneering work of Reed et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> approached text-guided image generation by training a cond ntion mechanism between the text and image features. Additional supervision was used in other works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: shot image classification on a variety of datasets. We refer the reader to OpenAI's Distill article <ref type=\"bibr\" target=\"#b10\">[11]</ref> for an extensive exposition and discussion of the visual c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed image retrieval, image captioning, and visual question answering. Following the success of BERT <ref type=\"bibr\" target=\"#b8\">[9]</ref> in various language tasks, recent VL methods typically use T. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> for a variety of tasks, such as languagebased image retriev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: image synthesis, with recent style-based generative models <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> boasting some of th re> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We use StyleGAN2<ref type=\"bibr\" target=\"#b18\">[19]</ref> in all our experiments.</note> \t\t</body> \t\t<back> \t\t\t<div . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e features. Additional supervision was used in other works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> to further improve . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p://www.tei-c.org/ns/1.0\"><head n=\"1.\">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type=\"bibr\" target=\"#b11\">[12]</ref> have revolutionized image synthesis, with recent style-bas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: udies focus on text-guided image manipulation. Some methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> use a GAN-based enc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b29\">[30]</ref>, conditioned by text embeddings obtained from a pretrained encoder. Zhang et al. <ref type=\"bibr\" target=\"#b53\">[54,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref> improved image qua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: image synthesis, with recent style-based generative models <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> boasting some of th re> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We use StyleGAN2<ref type=\"bibr\" target=\"#b18\">[19]</ref> in all our experiments.</note> \t\t</body> \t\t<back> \t\t\t<div . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed image retrieval, image captioning, and visual question answering. Following the success of BERT <ref type=\"bibr\" target=\"#b8\">[9]</ref> in various language tasks, recent VL methods typically use T. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: training a network that encodes a given image into a latent representation of the manipulated image <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of NAS methods, especially when the search space is huge. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar  are three basic types: accuracy-based, magnitudebased, and angle-based metrics. For example, PCNAS <ref type=\"bibr\" target=\"#b19\">[20]</ref> drop unpromising operators layer by layer using accuracy a ect for elastic depth. The split point space is set to range <ref type=\"bibr\" target=\"#b8\">(9,</ref><ref type=\"bibr\" target=\"#b19\">20)</ref> to handle different complexity constrains. In total we have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b4\">5]</ref> and gradient-based methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Path-based methods s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b33\">[34]</ref>, Dropconnect <ref type=\"bibr\" target=\"#b39\">[40]</ref>, StochDepth <ref type=\"bibr\" target=\"#b14\">[15]</ref>, Shake-Shake <ref type=\"bibr\" target=\"#b8\">[9]</ref> are a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d then share the weights across subnets. They could be further categorized as two types: path-based <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe on constrain. A comparison between the architectures searched by NEAS and other NAS methods such as <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> is presented in Fig \"><head n=\"4.1.\">Implementation Details</head><p>Search Space. Consistent with previous NAS methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe la><p>We then formulate NEAS as a two-stage optimization problem like other one-shot methods (e.g., <ref type=\"bibr\" target=\"#b9\">[10]</ref>). The firststage is to optimize the weight of the supernet  \"table\">1</ref>. Comparison of state-of-the-art NAS methods on ImageNet. \u2020: TPU days, : reported by <ref type=\"bibr\" target=\"#b9\">[10]</ref>, \u2021: searched on CIFAR-10, \"-\" means not reported. \u2666: Tested A.</p><p>Supernet Training. We train the supernet for 120 epochs using the settings similar to SPOS <ref type=\"bibr\" target=\"#b9\">[10]</ref>: SGD optimizer with momentum 0.9 and weight decay 4e-5, ini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b33\">[34]</ref>, Dropconnect <ref type=\"bibr\" target=\"#b39\">[40]</ref>, StochDepth <ref type=\"bibr\" target=\"#b14\">[15]</ref>, Shake-Shake <ref type=\"bibr\" target=\"#b8\">[9]</ref> are a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tasks such as image classification <ref type=\"bibr\" target=\"#b47\">[48]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b22\">[23]</ref>, object detection <ref type=\"bibr\" target=\"#b3\">[4]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: have shown that the design of neural network architecture [ <ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on ability of the architectures found by NEAS, we transfer the architectures to the downstream COCO <ref type=\"bibr\" target=\"#b21\">[22]</ref> object detection task. We use the NEAS-S (pre-trained 500 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plexity constraint. The searched architectures generate new stateof-the-art performance on ImageNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>. For instance, as shown in Fig. <ref type=\"figure\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chieves 77.9% top-1 accuracy on ImageNet, which is 19% smaller and 1.6% better than EfficientNet-B0 <ref type=\"bibr\" target=\"#b36\">[37]</ref>. The architecture discovered by NEAS transfers well to dow ><head>Methods</head><p>Top-1 Top- squeeze-excitation modules to each block following Effi-cientNet <ref type=\"bibr\" target=\"#b36\">[37]</ref> and MobileNetV3 <ref type=\"bibr\" target=\"#b11\">[12]</ref>. ain the discovered architectures for 350 epochs on ImageNet using similar settings as Efficient-Net <ref type=\"bibr\" target=\"#b36\">[37]</ref>: RMSProp optimizer with momentum 0.9 and decay 0.9, weight outperforms the recent MobileNetV3 <ref type=\"bibr\" target=\"#b11\">[12]</ref> and EfficientNet-B0/B1 <ref type=\"bibr\" target=\"#b36\">[37]</ref>. In particular, NEAS-L achieves 80.0% top-1 accuracy with   previous NAS methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, our search space includes a stack of mobile inverted bottl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed 500 epochs on ImageNet) as a dropin replacement for the backbone feature extractor in Reti-naNet <ref type=\"bibr\" target=\"#b20\">[21]</ref> and compare it with other backbone networks. We perform tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hilous settings, their evaluation is often limited to a few graph datasets introduced by Pei et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> that have certain undesirable properties such as small siz  have been proposed to achieve higher performance in low-homophily settings. For instance, Geom-GCN <ref type=\"bibr\" target=\"#b43\">[44]</ref> introduces a geometric aggregation scheme, MixHop <ref typ y used datasets to evaluate non-homophilous graph representation learning methods were presented by <ref type=\"bibr\" target=\"#b43\">[44]</ref> (see our Appendix Table <ref type=\"table\" target=\"#tab_5\"> lic) datasets discussed in <ref type=\"bibr\" target=\"#b47\">[48]</ref>, evaluation on the datasets in <ref type=\"bibr\" target=\"#b43\">[44]</ref> is plagued by high variance across different train/test sp \ud835\udc63 }| |\ud835\udc38| .<label>(1)</label></formula><p>Another related measure is what we call the node homophily <ref type=\"bibr\" target=\"#b43\">[44]</ref>, defined as is the number of neighbors of \ud835\udc65 that have the  ondly, the stability of performance across runs is better for our datasets than those of Pei et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> (see <ref type=\"bibr\" target=\"#b59\">[60]</ref> results). M i-c.org/ns/1.0\"><head>A.2 Previous Non-Homophilous Data</head><p>For the six datasets in Pei et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> often used in evaluation of graph representation learning  d grid is the same as that of APPNP. We use their Personalized PageRank weight initialization.      <ref type=\"bibr\" target=\"#b43\">[44]</ref>. The \"film\" dataset is also to as \"Actor\". Note that there Figure 5 :</head><label>5</label><figDesc>Figure 5: Compatibility matrices of datasets in Pei et al.<ref type=\"bibr\" target=\"#b43\">[44]</ref>. The \"film\" dataset is also to as \"Actor\". Note that there ml:id=\"tab_5\"><head>Table 4 :</head><label>4</label><figDesc>Statistics for datasets from Pei et al.<ref type=\"bibr\" target=\"#b43\">[44]</ref>. #C is the number of node classes.</figDesc><table><row><c  Chameleon than Squirrel and better on Squirrel than Actor <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>. Our measure sugges nd internet relationships, such as in web page connections <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. \u2022 Malicious or fraudulent nodes, such as in auction networ her works in non-homophilous graph learning evaluation, we take a high proportion of training nodes <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tp://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_2\"><head></head><label></label><figDesc>-patents<ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> is a dataset of ut. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, GAT <ref type=\"bibr\" target=\"#b51\">[52]</ref>, jumping knowledge networks (GCN+JK, GAT+JK) <ref type=\"bibr\" target=\"#b53\">[54]</ref>, and APPNP <ref type=\"bibr\" target=\"#b30\">[31]</ref>.</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: features are based on artists liked by each user. Nodes are labeled with reported gender.Facebook100<ref type=\"bibr\" target=\"#b50\">[51]</ref> consists of 100 Facebook friendship network snapshots from. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive models over graphs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. They tend to either sample node feature vectors from a rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: omophily in the data <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar omophily assumptions <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>, and thus face issu  simple, inexpensive models are able to achieve state-of-the-art performance on homophilic datasets <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>.</p></div> <div xm sentation learning work. Also, we include SGC <ref type=\"bibr\" target=\"#b52\">[53]</ref> and C&amp;S <ref type=\"bibr\" target=\"#b23\">[24]</ref> as simple methods that perform well on homophilic datasets t=\"#b56\">[57]</ref>.</p><p>\u2022 Simple methods: SGC <ref type=\"bibr\" target=\"#b52\">[53]</ref>, C&amp;S <ref type=\"bibr\" target=\"#b23\">[24]</ref>, two-hop variants.</p><p>\u2022 General GNNs: GCN <ref type=\"bi s conducted using grid search for most methods. Tuning for C&amp;S is done as in the original paper <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which uses Optuna <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: construction, and even those models that do not may perform poorly in nonhomophilous graph settings <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>. While some GNNs h interaction networks <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> (deezer, FB100, Pokec). \u2022 Biological structures such as in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w-pass graph filters <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref> that smooth features over the graph topology, which produce get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. By leveraging this assumption, several simple, inexpensive chieve state-of-the-art performance on homophilic datasets <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>, and thus face issues when used in non-homophilous settings  they have often been overlooked by recent graph representation learning work. Also, we include SGC <ref type=\"bibr\" target=\"#b52\">[53]</ref> and C&amp;S <ref type=\"bibr\" target=\"#b23\">[24]</ref> as s arget=\"#b57\">58]</ref>, LINK <ref type=\"bibr\" target=\"#b56\">[57]</ref>.</p><p>\u2022 Simple methods: SGC <ref type=\"bibr\" target=\"#b52\">[53]</ref>, C&amp;S <ref type=\"bibr\" target=\"#b23\">[24]</ref>, two-ho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: witch-DE, which has 9,498 nodes, 76,569 edges, edge homophily of .632, and \u0125 of .142.</p><p>YelpChi <ref type=\"bibr\" target=\"#b38\">[39]</ref> is a graph in which the nodes are reviews for hotels and r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. By leveraging this. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he class labels are fraudulent reviews and recommended reviews. The 32 node features are taken from <ref type=\"bibr\" target=\"#b44\">[45]</ref>. We take the topology from <ref type=\"bibr\" target=\"#b15\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are masked auto-encoders, while in vision the recently popular choices are Siamese networks (e.g., <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" tar trastive self-supervised pre-training can outperform their supervised counterparts in certain tasks <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p><p>Contrastive   \"MoCo v3\" framework that facilitates our study. MoCo v3 is an incremental improvement of MoCo v1/2 <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, and we strike for calability. The pseudocode of MoCo v3 is in Alg. 1, described next.</p><p>As common practice (e.g., <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>), we take two crops robing accuracy. In this regime, the larger batch improves accuracy thanks to more negative samples <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. The curve of a 4k  g rate as lr\u00d7BatchSize/256, where lr is a \"base\" learning rate. lr is the hyper-parameter being set <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" tar th masked auto-encoding, we study the frameworks that are based on Siamese networks, including MoCo <ref type=\"bibr\" target=\"#b18\">[20]</ref> and *: equal contribution. framework model params acc. (%) d by two encoders, f q and f k , with output vectors q and k. Intuitively, q behaves like a \"query\" <ref type=\"bibr\" target=\"#b18\">[20]</ref>, and the goal of learning is to retrieve the corresponding , in MoCo v3 we use the keys that naturally co-exist in the same batch. We abandon the memory queue <ref type=\"bibr\" target=\"#b18\">[20]</ref>, which we find has diminishing gain if the batch is suffic ckbone and projection head, but not the prediction head. f k is updated by the movingaverage of f q <ref type=\"bibr\" target=\"#b18\">[20]</ref>, excluding the prediction head.</p><p>As a reference, we e arget=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. The methodology is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  self-supervised ViT models have competitive results vs. the big convolutional ResNets in prior art <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>. On one hand, this MLP heads. The projection head <ref type=\"bibr\" target=\"#b9\">[10]</ref> is a 3-layer MLP, following <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The prediction head <ref type=\"bibr\" target=\"#b16\">[18]</ tation. We evaluate single-crop top-1 accuracy in the validation set.  <ref type=\"table\">1</ref> in <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and BYOL results are from Table <ref type=\"table\">1</ref> ure\" target=\"#fig_3\">8</ref> we compare with the state-of-the-art big ResNets reported by SimCLR v2 <ref type=\"bibr\" target=\"#b10\">[11]</ref> and BYOL <ref type=\"bibr\" target=\"#b16\">[18]</ref>. We not ned with two 224\u00d7224 crops, and are evaluated by linear probing. SimCLR v2 results are from Table1in<ref type=\"bibr\" target=\"#b10\">[11]</ref>, and BYOL results are from Table1in<ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k</head><p>Self-supervised visual representation learning. In computer vision, contrastive learning <ref type=\"bibr\" target=\"#b17\">[19]</ref> has become increasingly successful for self-supervised lea s to retrieve the corresponding \"key\". This is formulated as minimizing a contrastive loss function <ref type=\"bibr\" target=\"#b17\">[19]</ref>. We adopt the form of InfoNCE <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecent progress on Vision Transformers (ViT) <ref type=\"bibr\">[16]</ref>. In contrast to prior works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\">16]</ref> that train self-supervised Transfo sed learning in computer vision.</p><p>Self-supervised Transformers for vision. In pioneering works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\">16]</ref>, training self-supervised Transfor t=\"#b18\">[20]</ref> and *: equal contribution. framework model params acc. (%) linear probing: iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> iGPT-L 1362M 69.0 iGPT <ref type=\"bibr\" target=\"#b8\">[9]</re params acc. (%) linear probing: iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> iGPT-L 1362M 69.0 iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> iGPT-XL 6801M 72.0 MoCo v3</p><p>ViT-B 86M 76.7 MoCo v3</p>< cation, evaluated by linear probing (top panel) or end-to-end fine-tuning (bottom panel). Both iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> and masked patch prediction <ref type=\"bibr\">[16]</ref> belo t=\"#b34\">[36,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> (Table <ref type=\"table\">1</ref>). iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> masks and reconstructs pixels, and the self-supervised varia bone. It leads to \u223c1% improvement consistently (see Fig. <ref type=\"figure\">8</ref>).</p><p>In iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref>, accuracy can be improved by using longer sequences in the p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>We have also tried BatchNorm (BN) <ref type=\"bibr\" target=\"#b22\">[24]</ref>, WeightNorm (WN) <ref type=\"bibr\" target=\"#b37\">[39]</ref>, or gradient clip on patch projection. We observe that BN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us efforts on generalizing Transformers to computer vision <ref type=\"bibr\" target=\"#b41\">[43,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">[18]</ref> is a 2-layer MLP. The hidden layers of both MLPs are 4096-d and are with ReLU <ref type=\"bibr\" target=\"#b29\">[31]</ref>; the output layers of both MLPs are 256-d, without ReLU. I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b17\">[19]</ref> has become increasingly successful for self-supervised learning, e.g., <ref type=\"bibr\" target=\"#b42\">[44,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" ta f k 's outputs from other images, known as q's negative samples. \u03c4 is a temperature hyper-parameter <ref type=\"bibr\" target=\"#b42\">[44]</ref> for 2 -normalized q, k.</p><p>Following <ref type=\"bibr\" t results, even when it is potentially unstable. To reveal the instability, we monitor the kNN curves <ref type=\"bibr\" target=\"#b42\">[44]</ref> (see appendix) during training. In Sec. 4.1, we study how   The saturation can also be caused by the limited power of the existing instance-based pretext task <ref type=\"bibr\" target=\"#b42\">[44]</ref>. It may be desired to design more difficult pretext tasks.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the output layers of both MLPs are 256-d, without ReLU. In MoCo v3, all layers in both MLPs have BN <ref type=\"bibr\" target=\"#b21\">[23]</ref>, following SimCLR <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us efforts on generalizing Transformers to computer vision <ref type=\"bibr\" target=\"#b41\">[43,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the output layers of both MLPs are 256-d, without ReLU. In MoCo v3, all layers in both MLPs have BN <ref type=\"bibr\" target=\"#b21\">[23]</ref>, following SimCLR <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le, reordering algorithms often obtain large speedups on graphs with power-law degree distributions <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref>, which near neighbor graphs do not have. I lished in a single pass through the graph.</p><p>Degree-Based Grouping. Degree-Based Grouping (DBG) <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref> is an extension of hub clustering to multi escending degree order, but the order of the nodes within each group is not changed. The authors of <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref> use logarithmically spaced thresholds to e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, information retrieval <ref type=\"bibr\" target=\"#b16\">(Huang et al., 2013)</ref>, computer vision <ref type=\"bibr\" target=\"#b13\">(Frome et al., 2013)</ref> and other application domains. Beyond mach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ighbor Graphs. Graph-based algorithms such as HNSW, pruned approximate near neighbor graphs (PANNG) <ref type=\"bibr\" target=\"#b17\">(Iwasaki, 2016)</ref>, and optimized near neighbor graphs (ONNG) <ref  and ONNG. PANNG prunes a k-NN graph to remove edges when alternative paths exist between two nodes <ref type=\"bibr\" target=\"#b17\">(Iwasaki, 2016)</ref>. The PANNG algorithm first adds edges to the k-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  brute force <ref type=\"bibr\" target=\"#b22\">(Johnson et al., 2019)</ref> to spacepartitioning trees <ref type=\"bibr\" target=\"#b4\">(Beygelzimer et al., 2006)</ref>. Graph algorithms have emerged as a m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le, reordering algorithms often obtain large speedups on graphs with power-law degree distributions <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref>, which near neighbor graphs do not have. I lished in a single pass through the graph.</p><p>Degree-Based Grouping. Degree-Based Grouping (DBG) <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref> is an extension of hub clustering to multi escending degree order, but the order of the nodes within each group is not changed. The authors of <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref> use logarithmically spaced thresholds to e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the top five search libraries on the well-established ANN-benchmarks leader board use a graph index <ref type=\"bibr\" target=\"#b1\">(Aum\u00fcller et al., 2020)</ref>. Because these libraries have been integ rprise that the resulting graphs have approximately the same performance on real-world search tasks <ref type=\"bibr\" target=\"#b1\">(Aum\u00fcller et al., 2020)</ref>. As previously mentioned, we expect thes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: space. The engine recommends the products whose embeddings are nearest to the embedded search query <ref type=\"bibr\" target=\"#b32\">(Nigam et al., 2019)</ref>.</p><p>Since the search occurs for every q  function under strict latency requirements: a max-imum search time of 20 ms is a common constraint <ref type=\"bibr\" target=\"#b32\">(Nigam et al., 2019)</ref>. A 20% improvement in search time allows t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een intra-aware features based on Graph Neural Network (GNN) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>.</p><p>Inter-RM learns a set of relation messages and estim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  similar to that extracted from another basic feature. To achieve this, inspired by the center loss <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we develop a compactness loss. The compactness loss L C l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: riety of FER methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> have been proposed  earning method to tackle the disturbance caused by facial identity and pose variations. Ruan et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> propose a novel Disturbance-Disentangled Learning (DDL) me 26\">[27]</ref> 97.37 \u2021 73.23 88.00 FN2EN <ref type=\"bibr\" target=\"#b7\">[8]</ref> 98.60 \u2020 -87.71 DDL <ref type=\"bibr\" target=\"#b19\">[20]</ref> 99. <ref type=\"bibr\" target=\"#b15\">16</ref>   <ref type=\"b bibr\" target=\"#b23\">[24]</ref> 86.90 56.40 SCN <ref type=\"bibr\" target=\"#b22\">[23]</ref> 87.01 -DDL <ref type=\"bibr\" target=\"#b19\">[20]</ref> 87 the baseline method are not easily distinguishable for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b12\">[13]</ref> 84.13 51.05 IPA2LT <ref type=\"bibr\" target=\"#b27\">[28]</ref> 86.77 58.29 SPDNet <ref type=\"bibr\" target=\"#b0\">[1]</ref> 87.00 58.14 RAN <ref type=\"bibr\" target=\"#b23\">[24]</ref> 86. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed in our experiment. RAF-DB involves 12,271 images for training and 3,068 images for testing. SFEW <ref type=\"bibr\" target=\"#b5\">[6]</ref> is created by selecting static frames from Acted Facial Expr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: quence to construct the training set and the test set, thus resulting in a total of 981 images. MMI <ref type=\"bibr\" target=\"#b18\">[19]</ref> is also a lab-controlled database, containing 205 video se. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> adopt Principal Component Analysis (PCA) or Linear Discrimina. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of latent features associated with their corresponding importance weights. Traditional FER methods <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms, health-care, etc. <ref type=\"bibr\" target=\"#b28\">[29]</ref>. According to psychological studies <ref type=\"bibr\" target=\"#b8\">[9]</ref>, the FER task is to classify an input facial image into one . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to construct the training set and the test set, thus resulting in a total of 615 images. Oulu-CASIA <ref type=\"bibr\" target=\"#b29\">[30]</ref> contains videos captured in controlled lab conditions. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: riety of FER methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> have been proposed  earning method to tackle the disturbance caused by facial identity and pose variations. Ruan et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> propose a novel Disturbance-Disentangled Learning (DDL) me 26\">[27]</ref> 97.37 \u2021 73.23 88.00 FN2EN <ref type=\"bibr\" target=\"#b7\">[8]</ref> 98.60 \u2020 -87.71 DDL <ref type=\"bibr\" target=\"#b19\">[20]</ref> 99. <ref type=\"bibr\" target=\"#b15\">16</ref>   <ref type=\"b bibr\" target=\"#b23\">[24]</ref> 86.90 56.40 SCN <ref type=\"bibr\" target=\"#b22\">[23]</ref> 87.01 -DDL <ref type=\"bibr\" target=\"#b19\">[20]</ref> 87 the baseline method are not easily distinguishable for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> adopt Principal Component Analysis (PCA) or Linear Discrimina. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to construct the training set and the test set, thus resulting in a total of 615 images. Oulu-CASIA <ref type=\"bibr\" target=\"#b29\">[30]</ref> contains videos captured in controlled lab conditions. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> have been proposed to learn holistic expression features by ss separability and intra-class compactness.</head><p>A few FER methods employ attention mechanisms <ref type=\"bibr\" target=\"#b25\">[26]</ref> to improve the discriminative ability of expression featur \" target=\"#b25\">[26]</ref> to improve the discriminative ability of expression features. Xie et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> design an attention layer to focus on salient regions of a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: total). Similarly to <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, we employ the subj [13]</ref> 95.78 \u2020 78.46 -IPA2LT <ref type=\"bibr\" target=\"#b27\">[28]</ref> 92.45 \u2021 65.61 61.49 DeRL <ref type=\"bibr\" target=\"#b26\">[27]</ref> 97.37 \u2021 73.23 88.00 FN2EN <ref type=\"bibr\" target=\"#b7\">[8. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ), fear (FE), happy (HA), sad (SA), surprise (SU), and neutral (NE).</p><p>A variety of FER methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: riety of FER methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> have been proposed  earning method to tackle the disturbance caused by facial identity and pose variations. Ruan et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> propose a novel Disturbance-Disentangled Learning (DDL) me 26\">[27]</ref> 97.37 \u2021 73.23 88.00 FN2EN <ref type=\"bibr\" target=\"#b7\">[8]</ref> 98.60 \u2020 -87.71 DDL <ref type=\"bibr\" target=\"#b19\">[20]</ref> 99. <ref type=\"bibr\" target=\"#b15\">16</ref>   <ref type=\"b bibr\" target=\"#b23\">[24]</ref> 86.90 56.40 SCN <ref type=\"bibr\" target=\"#b22\">[23]</ref> 87.01 -DDL <ref type=\"bibr\" target=\"#b19\">[20]</ref> 87 the baseline method are not easily distinguishable for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of latent features associated with their corresponding importance weights. Traditional FER methods <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to construct the training set and the test set (consisting of 1,440 images in total). Similarly to <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ </ref> 92.45 \u2021 65.61 61.49 DeRL <ref type=\"bibr\" target=\"#b26\">[27]</ref> 97.37 \u2021 73.23 88.00 FN2EN <ref type=\"bibr\" target=\"#b7\">[8]</ref> 98.60 \u2020 -87.71 DDL <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, we employ the subject-independent ten-fold cross-validatio div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Methods</head><p>Accuracy (%) CK+ MMI Oulu-CASIA PPDN <ref type=\"bibr\" target=\"#b31\">[32]</ref> 97.30 \u2020 -72.40 IACNN <ref type=\"bibr\" target=\"#b16\">[17]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: prise (SU), and neutral (NE).</p><p>A variety of FER methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar class similarities and enhance intra-class compactness for expression feature extraction. Li et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose a deep locality-preserving loss based method, whic ent ten-fold cross-validation protocol for evaluation on all the three in-the-lab databases. RAF-DB <ref type=\"bibr\" target=\"#b12\">[13]</ref> is a real-world FER database, which contains 30,000 images 1\">[32]</ref> 97.30 \u2020 -72.40 IACNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> 95.37 \u2021 71.55 -DLP-CNN <ref type=\"bibr\" target=\"#b12\">[13]</ref> 95.78 \u2020 78.46 -IPA2LT <ref type=\"bibr\" target=\"#b27\">[28]<  -87.71 DDL <ref type=\"bibr\" target=\"#b19\">[20]</ref> 99. <ref type=\"bibr\" target=\"#b15\">16</ref>   <ref type=\"bibr\" target=\"#b12\">[13]</ref> 84.13 51.05 IPA2LT <ref type=\"bibr\" target=\"#b27\">[28]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: quence to construct the training set and the test set, thus resulting in a total of 981 images. MMI <ref type=\"bibr\" target=\"#b18\">[19]</ref> is also a lab-controlled database, containing 205 video se. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b19\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Wang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b12\">Feng et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Lv et al., 202 enges. We first encode the QA context using an LM, and retrieve a KG subgraph following prior works <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref>. Our QA-GNN has two key insights: (i) Relev  compare with prior works, KagNet <ref type=\"bibr\" target=\"#b19\">(Lin et al., 2019)</ref> and MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref> in Table 1. As we handle edges of different tions and linear with respect to the number of nodes. We achieve the same space complexity as MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org  RN <ref type=\"bibr\" target=\"#b35\">(Santoro et al., 2017)</ref> 74.57 (\u00b10.91) 69.08 (\u00b10.21) + MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref> 74   <ref type=\"bibr\" target=\"#b22\">(Lv et   et al., 2020)</ref> 74   <ref type=\"bibr\" target=\"#b22\">(Lv et al., 2020)</ref> 75.3 RoBERTa+MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref> 75.4 Albert+PG <ref type=\"bibr\" target=\"#b4 bibr\">KagNet (Lin et al., 2019)</ref>, and ( <ref type=\"formula\" target=\"#formula_5\">5</ref>) MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref>. ( <ref type=\"formula\" target=\"#formula_1\"> toRoBERTa + PG <ref type=\"bibr\" target=\"#b45\">(Wang et al., 2020b)</ref> 80.2 AristoRoBERTa + MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref> 80.6 Albert + KB 81.0 T5 * <ref type=\"bibr\" the QA context (z LM = f enc (text(z))), and each node on the G sub using the entity embedding from <ref type=\"bibr\" target=\"#b12\">Feng et al. (2020)</ref>. In the subsequent sections, we will reason  nswer choice), we retrieve the subgraph G sub from G following the pre-processing step described in <ref type=\"bibr\" target=\"#b12\">Feng et al. (2020)</ref>, with hop size k = 2. Henceforth, in this se alt water\"). While prior KG reasoning models <ref type=\"bibr\" target=\"#b19\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Feng et al., 2020)</ref> enumerate individual paths in the KG for mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: m reasoning on the working graph G W , our GNN module builds on the graph attention framework (GAT) <ref type=\"bibr\" target=\"#b43\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, which induces node representations v contrast to these works, QA-GNN jointly models the language and KG. Graph Attention Networks (GATs) <ref type=\"bibr\" target=\"#b43\">(Veli\u010dkovi\u0107 et al., 2018)</ref> perform attention-based message passi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  al., 2019)</ref>, or explicitly represented in structured knowledge graphs (KGs), such as Freebase <ref type=\"bibr\" target=\"#b5\">(Bollacker et al., 2008)</ref> and ConceptNet <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e explicit and interpretable knowledge, several works integrate structured knowledge (KGs) into LMs <ref type=\"bibr\" target=\"#b25\">(Mihaylov and Frank, 2018;</ref><ref type=\"bibr\" target=\"#b19\">Lin et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onally, existing LM+KG methods for reasoning <ref type=\"bibr\" target=\"#b19\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Wang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b12\">Feng et al.,  target=\"#b25\">(Mihaylov and Frank, 2018;</ref><ref type=\"bibr\" target=\"#b19\">Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Wang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b49\">Yang et al.,  s the facts corresponding to each question, prepared by <ref type=\"bibr\">Clark et al. (2019)</ref>  <ref type=\"bibr\" target=\"#b46\">(Wang et al., 2019a)</ref> 72.61( \u00b10.39) 68.59 (\u00b10.96) + KagNet <ref  17)</ref>, (2) RGCN <ref type=\"bibr\" target=\"#b36\">(Schlichtkrull et al., 2018)</ref>, (3) GconAttn <ref type=\"bibr\" target=\"#b46\">(Wang et al., 2019a)</ref>, ( <ref type=\"formula\" target=\"#formula_4\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Methods Test</head><p>Careful Selection <ref type=\"bibr\" target=\"#b1\">(Banerjee et al., 2019)</ref> 72.0 AristoRoBERTa 77.8 KF + SIR <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e explicit and interpretable knowledge, several works integrate structured knowledge (KGs) into LMs <ref type=\"bibr\" target=\"#b25\">(Mihaylov and Frank, 2018;</ref><ref type=\"bibr\" target=\"#b19\">Lin et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b31\">(Rajpurkar et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Joshi et al., 2017;</ref><ref type=\"bibr\" target=\"#b50\">Yang et al., 2018)</ref>, and KBQA, where systems perform semantic pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing tasks, e.g., passagebased QA, where systems identify answers using given or retrieved documents <ref type=\"bibr\" target=\"#b31\">(Rajpurkar et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Joshi et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b42\">Toutanova et al., 2015;</ref><ref type=\"bibr\" target=\"#b48\">Xiong et al., 2019;</ref><ref type=\"bibr\" target=\"#b39\">Sun et al., 2019;</ref><ref type=\"bibr\" target=\"#b47\">Wang et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2019)</ref>, or explicitly represented in structured knowledge graphs (KGs), such as Freebase <ref type=\"bibr\" target=\"#b5\">(Bollacker et al., 2008)</ref> and ConceptNet <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to be effective for modeling graphbased data. Several works use GNNs to model the structure of text <ref type=\"bibr\" target=\"#b52\">(Yasunaga et al., 2017;</ref><ref type=\"bibr\" target=\"#b56\">Zhang et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e explicit and interpretable knowledge, several works integrate structured knowledge (KGs) into LMs <ref type=\"bibr\" target=\"#b25\">(Mihaylov and Frank, 2018;</ref><ref type=\"bibr\" target=\"#b19\">Lin et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: des/paths rely on graph-based metrics such as PageRank, centrality, and off-the-shelf KG embeddings <ref type=\"bibr\" target=\"#b28\">(Paul and Frank, 2019;</ref><ref type=\"bibr\" target=\"#b11\">Fadnis et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Methods Test</head><p>Careful Selection <ref type=\"bibr\" target=\"#b1\">(Banerjee et al., 2019)</ref> 72.0 AristoRoBERTa 77.8 KF + SIR <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ructure of the KGs to perform joint reasoning over these two sources of information. Previous works <ref type=\"bibr\" target=\"#b2\">(Bao et al., 2016;</ref><ref type=\"bibr\" target=\"#b40\">Sun et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b19\">(Lin et al., 2019)</ref> be noisy <ref type=\"bibr\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b13\">Guu et al., 2015)</ref>. How to reason effectively with both sources . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to be effective for modeling graphbased data. Several works use GNNs to model the structure of text <ref type=\"bibr\" target=\"#b52\">(Yasunaga et al., 2017;</ref><ref type=\"bibr\" target=\"#b56\">Zhang et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2020b)</ref> 75.6 Albert <ref type=\"bibr\">(Lan et al., 2020) (ensemble)</ref> 76.5 UnifiedQA * <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref> 79.1</p><p>RoBERTa + QA-GNN (Ours) 76.1 6 Albert + KB 81.0 T5 * <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref> 83.2 UnifiedQA * <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref> 87.2</p><p>AristoRoBERTa + QA-GNN (Ours y, the top two systems, T5 <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref> and UnifiedQA <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref>, are trained with more data and use 8x . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b38\">(Srivastava et al., 2014)</ref>. The parameters of the model are optimized by RAdam <ref type=\"bibr\" target=\"#b20\">(Liu et al., 2020)</ref>, with batch size 128, gradient clipping 1.0 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2020b)</ref> 75.6 Albert <ref type=\"bibr\">(Lan et al., 2020) (ensemble)</ref> 76.5 UnifiedQA * <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref> 79.1</p><p>RoBERTa + QA-GNN (Ours) 76.1 6 Albert + KB 81.0 T5 * <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref> 83.2 UnifiedQA * <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref> 87.2</p><p>AristoRoBERTa + QA-GNN (Ours y, the top two systems, T5 <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref> and UnifiedQA <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref>, are trained with more data and use 8x . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: noisy labels are involved.</p><p>Motivated by this reason, Negative Learning for Noisy Labels; NLNL <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which is an indirect learning method for training CNNs, h ibly clean data and trains the other network with this data. Use of complementary labels Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed a noise-robust learning method where instead of m rpreted as a probability vector p \u2208 \u2206 c\u22121 , where \u2206 c\u22121 denotes the c-dimensional simplex.</p><p>NL <ref type=\"bibr\" target=\"#b11\">[12]</ref> is an indirect learning method for training CNNs with nois  <ref type=\"bibr\" target=\"#b29\">[30]</ref>, APL <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and NLNL <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Dataset We conduct the experiments on CIFAR10, CI-FAR100  a. We provide 110 y to each data in order to match the training speed to when training with CIFAR10 <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eta-learning to obtain weights that can be easily fine-tuned to a given noisy dataset. Zhang et al. <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed to learn confidence scores of each samples from t arget=\"#b6\">[7]</ref> 71.39 Joint-Optim <ref type=\"bibr\" target=\"#b25\">[26]</ref> 72.16 MetaCleaner <ref type=\"bibr\" target=\"#b33\">[34]</ref> 72.5 MLNT <ref type=\"bibr\" target=\"#b15\">[16]</ref> 73.47 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eta-learning to obtain weights that can be easily fine-tuned to a given noisy dataset. Zhang et al. <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed to learn confidence scores of each samples from t arget=\"#b6\">[7]</ref> 71.39 Joint-Optim <ref type=\"bibr\" target=\"#b25\">[26]</ref> 72.16 MetaCleaner <ref type=\"bibr\" target=\"#b33\">[34]</ref> 72.5 MLNT <ref type=\"bibr\" target=\"#b15\">[16]</ref> 73.47 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Arazo et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>, fits pe=\"bibr\" target=\"#b33\">[34]</ref> 72.5 MLNT <ref type=\"bibr\" target=\"#b15\">[16]</ref> 73.47 PENCIL <ref type=\"bibr\" target=\"#b31\">[32]</ref> 73.49</p><p>Ours 74.15 It is shown that Co-teaching and Jo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: many approaches regarding this issue. For example, there are meth-ods that design noise-robust loss <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target amily of studies aims to design novel loss functions that are tolerant of label noise. Ghosh et al. <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> showed that the mean a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Arazo et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>, fits pe=\"bibr\" target=\"#b33\">[34]</ref> 72.5 MLNT <ref type=\"bibr\" target=\"#b15\">[16]</ref> 73.47 PENCIL <ref type=\"bibr\" target=\"#b31\">[32]</ref> 73.49</p><p>Ours 74.15 It is shown that Co-teaching and Jo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: correction methods <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or example, there are meth-ods that design noise-robust loss <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" targe oss functions that are tolerant of label noise. Ghosh et al. <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> showed that the mean absolute error (MAE) loss is theoretical. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. They assume that prior knowledge like noise rate or noisy . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to obtain. Some other works used CNN with additional layer <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, and noise transition. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 4}. Models For CIFAR10 and CIFAR100 experiments, we used ResNet34. For Clothing1M, we used ResNet50 <ref type=\"bibr\" target=\"#b7\">[8]</ref>, pretrained on ImageNet. Hyperparameters We used stochastic . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph neural networks on graphs is still a nascent research topic, a few recent works have emerged <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020;</ref><ref type=\"bibr\" target=\"#b18\">Vu &amp; Thai, >, and Gradient <ref type=\"bibr\" target=\"#b15\">(Pope et al., 2019)</ref>, since previous explainers <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Ying et al., 2 rable amount of explanations for providing a global view of explanations. For this end, PGExplainer <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020)</ref> learns a multilayer perceptron (MLP) to expl 2017)</ref>: GNNExplainer <ref type=\"bibr\" target=\"#b22\">(Ying et al., 2019)</ref> and PG-Explainer <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020)</ref> <ref type=\"foot\" target=\"#foot_1\">3</ref> .  </p><p>For fair comparisons, we report the results of PGExplainer following its setting reported in <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020)</ref> and compare them with the results of GNNExpl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"1.\">Introduction</head><p>Many problems in scientific domains, ranging from social networks <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2020)</ref> to biology <ref type=\"bibr\" target=\"#b26\">(Z. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ip between this edge/node and its corresponding prediction. Based on the insights from neuroscience <ref type=\"bibr\" target=\"#b0\">(Biswal et al., 1997)</ref>, we extend the notion of Granger causality at the structural connectivity local to a certain area somehow dictates the function of that  piece <ref type=\"bibr\" target=\"#b0\">(Biswal et al., 1997)</ref>. Due to the inherent property of GNNs, the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: its flexibility to choose the predictive model, which is commonly referred to as \"modelagnosticism\" <ref type=\"bibr\" target=\"#b16\">(Ribeiro et al., 2016)</ref>. Guided by the first principles of Grang. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -world datasets. For graph classification, we use two benchmark datasets from bioinformatics -Mutag <ref type=\"bibr\" target=\"#b2\">(Debnath et al., 1991)</ref> and NCI1 <ref type=\"bibr\" target=\"#b19\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dient-based method <ref type=\"bibr\" target=\"#b22\">(Ying et al., 2019)</ref>, graph attention method <ref type=\"bibr\" target=\"#b17\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, and Gradient <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns, ranging from social networks <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2020)</ref> to biology <ref type=\"bibr\" target=\"#b26\">(Zitnik et al., 2018)</ref> and chemistry <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ress prefixes in the large-scale and dynamic Internet. Tag-based inter-AS source address validation <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> shows high accur  packets and provide sufficient benefits for deployers.</p><p>However, existing tag-based solutions <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> are subject to t ch other, which is vulnerable to Man-in-the-Middle (MitM) attacks. And for solutions similar to SPM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, whose tags are fixed random strings. If attackers sniff le lgorithms for tag generation which is of low efficiency or constant tag which is of low security T1 <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. In this section scheme <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The third type is the tag-based method, such as SPM <ref type=\"bibr\" target=\"#b9\">[10]</ref> and Passport <ref type=\"bibr\" target=\"#b10\">[11]</ref>. As  tions filer packets with spoofed source IP addresses by stamping and verifying tags in packets. SPM <ref type=\"bibr\" target=\"#b9\">[10]</ref> applies fixed random strings as tags, which is simple and e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  They provide low deployment incentives to deployers' ASes <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Therefore, a number of solutions <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  488.3 KB extra memory. We show the above results in Table <ref type=\"table\">.</ref>I. According to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, the SRAM size of ASIC can reach 100 MB, which is enough t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  possibility of packets replay, but in the same time introduce vulnerability of information leakage <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Similarly, the choice of field length should deal with th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs hard to accurately trace their sources and effectively defeat them. According to a recent survey <ref type=\"bibr\" target=\"#b0\">[1]</ref>, about 31.5% of tested 5,178 Autonomous Systems (ASes) have  Ses do not apply the filtering mechanism. They provide low deployment incentives to deployers' ASes <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Therefore, a numbe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a function of key, source address, and timestamp G12 G2. Our method is inspired by previous studies <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. It can ensure . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ofed traffic that may enter deployers' ASes. They either filter packets based on the number of hops <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> or the mapping betw ning accurate mappings for all input interfaces from route information is difficult. HCF and NetHCF <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> extracts the number. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g SEC will directly forward the negotiation packets instead of trying to understand what they carry <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Moreover, SEC places tags in the option fields of the IP . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  They provide low deployment incentives to deployers' ASes <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Therefore, a number of solutions <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of the experimental path parameters, they can still forward these modified BGP messages normally G3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Because the UPDATE messages are carried over stateful TCP. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  They provide low deployment incentives to deployers' ASes <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Therefore, a number of solutions <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lenge, we propose a novel nonlinear feature decorrelation approach based on Random Fourier Features <ref type=\"bibr\" target=\"#b43\">[45]</ref> with linear computational complexity. As for the second ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion has been demonstrated to be effective in improving the generalization ability of linear models. <ref type=\"bibr\" target=\"#b28\">[29]</ref> proposes a sample weighting approach with the goal of deco f type=\"bibr\" target=\"#b51\">[53]</ref>.</p><p>Learning sample weights for decorrelation Inspired by <ref type=\"bibr\" target=\"#b28\">[29]</ref>, we propose to eliminate the dependence between features i een correlation and model stability under misspecification <ref type=\"bibr\" target=\"#b49\">[51,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, and propose to address such a problem via a sample reweigh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ively studied in the domain generalization (DG) literature <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar rce domains. A common approach is to extract domain-invariant features over multiple source domains <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ  approaches that exploit regularization with meta-learning <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and Invariant Risk Minimization (IRM) framework <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , some methods are proposed to implicitly learn latent domains from data <ref type=\"bibr\">[44,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b58\">60]</ref>, but they implicitl ence according to them only, thus generalize better. For adversarially trained methods like DG-MMLD <ref type=\"bibr\" target=\"#b38\">[39]</ref>, the supervision from minor domains is insufficient and th ses in the dataset to We follow the experimental protocol of <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> for both the datasets and utilize three domains as source d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ough these four settings, namely PACS [31], VLCS <ref type=\"bibr\" target=\"#b56\">[58]</ref>, MNIST-M <ref type=\"bibr\" target=\"#b14\">[15]</ref> and NICO <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Introd. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on (DG) literature <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\">31, act domain-invariant features over multiple source domains <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar  so that irrelevant features vary across different domains while relevant features remain invariant <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: all h A \u2208 H A and h B \u2208 H B . Then, the independence can be determined by the following proposition <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><formula xml:id=\"formula_1\">Proposition 3.1 If the pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  have \u03a3 AB = 0 \u21d0\u21d2 A \u22a5 B<label>(2)</label></formula><p>Hilbert-Schmidt Independence Criterion (HSIC) <ref type=\"bibr\" target=\"#b17\">[18]</ref>, which requires that the squared Hilbert-Schmidt norm of \u03a3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g RKHS is denoted by H A . If k B and H B are similarly defined, the cross-covariance operator \u03a3 AB <ref type=\"bibr\" target=\"#b12\">[13]</ref> from H B to H A is as follows:</p><formula xml:id=\"formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. The basic idea of DG is to divide a category into multiple arget=\"#b61\">63]</ref>. There are several approaches that exploit regularization with meta-learning <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and Invariant Risk . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: irrelevant features and relevant features (i.e. the features that are relevant to a given category) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. The basic idea of DG is to divide a category into multiple arget=\"#b61\">63]</ref>. There are several approaches that exploit regularization with meta-learning <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and Invariant Risk . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng process. Some pioneering works based on Lasso framework <ref type=\"bibr\" target=\"#b54\">[56,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref> propose to decorrelate features by adding a regularizer that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a given category) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. Taking the recogniti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n Appendix B.2. Actually, Frobenius norm corresponds to the Hilbert-Schmidt norm in Euclidean space <ref type=\"bibr\" target=\"#b51\">[53]</ref>, so that the independent testing statistic can be based on ically, setting both n A and n B to 5 is solid enough to judge the independence of random variables <ref type=\"bibr\" target=\"#b51\">[53]</ref>.</p><p>Learning sample weights for decorrelation Inspired . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ivided and labeled <ref type=\"bibr\" target=\"#b59\">[61,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b44\">46,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng process. Some pioneering works based on Lasso framework <ref type=\"bibr\" target=\"#b54\">[56,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref> propose to decorrelate features by adding a regularizer that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: all h A \u2208 H A and h B \u2208 H B . Then, the independence can be determined by the following proposition <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><formula xml:id=\"formula_1\">Proposition 3.1 If the pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: machine learning models fail to make trustworthy predictions <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b49\">51]</ref>. To address this issue, out-of-distribution (OOD) generaliz [29]</ref> proposes a sample weighting approach with the goal of decorrelating input variables, and <ref type=\"bibr\" target=\"#b49\">[51]</ref> theoretically proves why such sample weighting can make a  theoretically bridge the connections between correlation and model stability under misspecification <ref type=\"bibr\" target=\"#b49\">[51,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, and propose to ad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep neural models can easily overfit the noisy labels, leading to severe degradation of performance <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Zhang et al., g supervised learning models for entity-centric IE tasks. Our method is motivated by recent studies <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">Toneva et al. e well-represented patterns of data in early steps, while needing much more steps to memorize noise <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017)</ref>.</p><p>Moreover, learned noisy examples ten reases, being consistent with the delayed learning curves the neural models have on noisy instances <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Our method is motivated by recent studies <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">Toneva et al., 2019)</ref> showing that noisy labels often have delay 2017)</ref>.</p><p>Moreover, learned noisy examples tend to be frequently forgotten in later epochs <ref type=\"bibr\" target=\"#b28\">(Toneva et al., 2019)</ref>, since they conflict with the general ind. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b6\">Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020)</ref>  <ref type=\"bibr\" target=\"#b29\">(Wang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: abuncu, 2018;</ref><ref type=\"bibr\" target=\"#b31\">Wang et al., 2019b)</ref>, noise filtering layers <ref type=\"bibr\" target=\"#b25\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Goldberg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ire an additional clean and sufficiently large reference dataset to develop a noise filtering model <ref type=\"bibr\" target=\"#b17\">(Qin et al., 2018)</ref>. Accordingly, those methods may not be gener isily labeled instances from D without using external resources such as a clean development dataset <ref type=\"bibr\" target=\"#b17\">(Qin et al., 2018)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ lized learning resources that may not exist in a general supervised setting. Reinforcement learning <ref type=\"bibr\" target=\"#b17\">(Qin et al., 2018;</ref><ref type=\"bibr\" target=\"#b32\">Wang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b6\">Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020)</ref>  <ref type=\"bibr\" target=\"#b29\">(Wang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b39\">(Zeng et al., 2015;</ref><ref type=\"bibr\" target=\"#b11\">Lin et al., 2016;</ref><ref type=\"bibr\" target=\"#b8\">Ji et al., 2017)</ref> groups noisy training instances and assumes at . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2019b)</ref>, noise filtering layers <ref type=\"bibr\" target=\"#b25\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Goldberger and Ben-Reuven, 2017)</ref>, and sample selection <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he language model will be sent into a token-level softmax classifier. We use the BIO tagging scheme <ref type=\"bibr\" target=\"#b18\">(Ramshaw and Marcus, 1995)</ref> and output the tag with the maximum . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: labeling errors. This problem has even drastically affected widely used benchmarks, such as CoNLL03 <ref type=\"bibr\" target=\"#b27\">(Tjong Kim Sang, 2002)</ref> and TA-CRED <ref type=\"bibr\" target=\"#b4 e conducted based on TA-CRED <ref type=\"bibr\" target=\"#b42\">(Zhang et al., 2017b)</ref> and CoNLL03 <ref type=\"bibr\" target=\"#b27\">(Tjong Kim Sang, 2002)</ref>. TACRED is a crowdsourced dataset for re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: abuncu, 2018;</ref><ref type=\"bibr\" target=\"#b31\">Wang et al., 2019b)</ref>, noise filtering layers <ref type=\"bibr\" target=\"#b25\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Goldberg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st in specialized application scenarios <ref type=\"bibr\" target=\"#b26\">(Surdeanu et al., 2012;</ref><ref type=\"bibr\" target=\"#b39\">Zeng et al., 2015;</ref><ref type=\"bibr\" target=\"#b19\">Ratner et al., th labels, whereas much effort has been devoted to reducing labeling noise. Multi-instance learning <ref type=\"bibr\" target=\"#b39\">(Zeng et al., 2015;</ref><ref type=\"bibr\" target=\"#b11\">Lin et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: abuncu, 2018;</ref><ref type=\"bibr\" target=\"#b31\">Wang et al., 2019b)</ref>, noise filtering layers <ref type=\"bibr\" target=\"#b25\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Goldberg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eading to severe degradation of performance <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Zhang et al., 2017a)</ref>. Unfortunately, labeling on large corpora, k can memorize noisy labels, and its generalizability will severely degrade when trained with noise <ref type=\"bibr\" target=\"#b40\">(Zhang et al., 2017a)</ref>. In computer vision, much investigation h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: benchmarks, such as CoNLL03 <ref type=\"bibr\" target=\"#b27\">(Tjong Kim Sang, 2002)</ref> and TA-CRED <ref type=\"bibr\" target=\"#b42\">(Zhang et al., 2017b)</ref>, where a notable portion of incorrect lab s a sentence-level classification problem. Accordingly, we first apply the entity masking technique <ref type=\"bibr\" target=\"#b42\">(Zhang et al., 2017b)</ref>  put sentence and replace the subject and ww.tei-c.org/ns/1.0\"><head n=\"5.1\">Datasets</head><p>The experiments are conducted based on TA-CRED <ref type=\"bibr\" target=\"#b42\">(Zhang et al., 2017b)</ref> and CoNLL03 <ref type=\"bibr\" target=\"#b27. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e work, we plan to extend the use of the proposed framework to other tasks such as event-centric IE <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2021)</ref> and coreference resolution <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y hindered the performance of SOTA systems <ref type=\"bibr\" target=\"#b21\">(Reiss et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Alt et al., 2020)</ref>. Hence, developing a robust learning method th  Kim Sang, 2002)</ref>. TACRED is a crowdsourced dataset for relation extraction. A recent study by <ref type=\"bibr\" target=\"#b0\">Alt et al. (2020)</ref> found a large portion of examples to be mislab o so, we extract the 2,526 noisy examples from the TACRED dev and test sets where the relabeling by <ref type=\"bibr\" target=\"#b0\">Alt et al. (2020)</ref> disagree with their original labels. According. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b6\">Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020)</ref>  <ref type=\"bibr\" target=\"#b29\">(Wang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y hindered the performance of SOTA systems <ref type=\"bibr\" target=\"#b21\">(Reiss et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Alt et al., 2020)</ref>. Hence, developing a robust learning method th  Kim Sang, 2002)</ref>. TACRED is a crowdsourced dataset for relation extraction. A recent study by <ref type=\"bibr\" target=\"#b0\">Alt et al. (2020)</ref> found a large portion of examples to be mislab o so, we extract the 2,526 noisy examples from the TACRED dev and test sets where the relabeling by <ref type=\"bibr\" target=\"#b0\">Alt et al. (2020)</ref> disagree with their original labels. According. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 15;</ref><ref type=\"bibr\" target=\"#b5\">Goldberger and Ben-Reuven, 2017)</ref>, and sample selection <ref type=\"bibr\" target=\"#b13\">(Malach and Shalev-Shwartz, 2017;</ref><ref type=\"bibr\" target=\"#b9\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Configurations</head><p>For base models C-GCN <ref type=\"bibr\">(Zhang et al., 2018)</ref> and LUKE <ref type=\"bibr\" target=\"#b36\">(Yamada et al., 2020)</ref>, we rerun the officially released impleme 6.9 BERT LARGE <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref> 70.9 70.2 78.3 77.9 LUKE \u2663 <ref type=\"bibr\" target=\"#b36\">(Yamada et al., 2020)</ref> 71 Table <ref type=\"table\">3</ref>: F 1 s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he language model will be sent into a token-level softmax classifier. We use the BIO tagging scheme <ref type=\"bibr\" target=\"#b18\">(Ramshaw and Marcus, 1995)</ref> and output the tag with the maximum . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs and enlarge the distance between negative sample pairs. As suggested by distance metric learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>, a sufficient number of negative sample pairs are required t imilarity between inter-class samples.</p><p>Triplet loss. Triplet loss was proposed by Ding et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and Schroff et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref> ance comparison <ref type=\"bibr\" target=\"#b50\">[50]</ref>, triplet loss was proposed by Ding et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and Schroff et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Triplet loss was proposed by Ding et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and Schroff et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref> independently for person reidentification and face recogni triplet loss was proposed by Ding et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and Schroff et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref> independently for person re-identification and face recogn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b51\">51,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, unsupervised representation learning is critical to visual. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  different task, i.e., person re-identification (re-ID), which is fundamental in video surveillance <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Re-ID refers to the problem of re-identifying individuals. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  different task, i.e., person re-identification (re-ID), which is fundamental in video surveillance <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Re-ID refers to the problem of re-identifying individuals. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he valuable triplets <ref type=\"bibr\" target=\"#b22\">[23]</ref>, to perform cross-batch triplet loss <ref type=\"bibr\" target=\"#b41\">[42]</ref>, and to apply to weakly supervised scenario <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b28\">28]</ref> (see \"goal line\" in Figure <ref type=\"figure\" target=\"#fig_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mantic information in an efficient fashion. As reported by <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, this overclustering problem can lead to unnecessary harmful   just memorizes the data instead of learning from the data <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>To overcome the over-clustering problem, recent works  k just memorizes the data instead of learning from the data<ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.Ideally, we would like to use just the right amount of negati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvised training protocols, and the trained network weights serve as the initialization of Mask-RCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> with C4. We finetune all layers on the train2017 set of CO. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed data.</p><p>Our unsupervised SimCSE simply predicts the input sentence itself, with only dropout <ref type=\"bibr\" target=\"#b44\">(Srivastava et al., 2014)</ref> used as noise (Figure <ref type=\"figu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y to alleviate the problem is postprocessing, either to eliminate the dominant principal components <ref type=\"bibr\" target=\"#b5\">(Arora et al., 2017;</ref><ref type=\"bibr\" target=\"#b35\">Mu and Viswan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly effective in learning sentence embeddings, coupled with pre-trained language models such as BERT <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref> and RoBERTa <ref type=\"bibr\" target=\"#b29 /formula><p>In this work, we encode input sentences using a pre-trained language model such as BERT <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref> or RoBERTa <ref type=\"bibr\" target=\"#b29\" nally, we introduce one more optional variant which adds a masked language modeling (MLM) objective <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref> as an auxiliary loss to Eq. 1: +\u03bb\u2022 mlm (\u03bb ume that they take the same evaluation and just take BERT-whitening in experiments here. \u2022 For BERT <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref> and RoBERTa <ref type=\"bibr\" target=\"#b29. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e <ref type=\"bibr\" target=\"#b26\">(Kiros et al., 2015;</ref><ref type=\"bibr\">Hill et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Conneau et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Logeswaran   the recent success of leveraging natural language inference (NLI) datasets for sentence embeddings <ref type=\"bibr\" target=\"#b12\">(Conneau et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">Reimers an sed datasets to provide better training signals for improving alignment of our approach. Prior work <ref type=\"bibr\" target=\"#b12\">(Conneau et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">Reimers an tening <ref type=\"bibr\" target=\"#b45\">(Su et al., 2021)</ref>. Supervised methods include InferSent <ref type=\"bibr\" target=\"#b12\">(Conneau et al., 2017)</ref>, Universal Sentence Encoder <ref type=\"b r average GloVe embedding <ref type=\"bibr\" target=\"#b40\">(Pennington et al., 2014)</ref>, InferSent <ref type=\"bibr\" target=\"#b12\">(Conneau et al., 2017)</ref> and Universal Sentence Encoder <ref type mpared to unsupervised approaches, supervised sentence embeddings demonstrate stronger performance. <ref type=\"bibr\" target=\"#b12\">Conneau et al. (2017)</ref> propose to fine-tune a Siamese model on N. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b37\">Pagliardini et al. (2018)</ref> show that simply augmenting the idea of word2vec <ref type=\"bibr\" target=\"#b34\">(Mikolov et al., 2013)</ref> with n-gram embeddings leads to strong r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: irs is through the use of independently sampled dropout masks. In standard training of Transformers <ref type=\"bibr\" target=\"#b46\">(Vaswani et al., 2017)</ref>, there is a dropout mask placed on fully. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rning with a dual-encoder approach, by forming (current sentence, next sentence) as (x i , x + i ). <ref type=\"bibr\" target=\"#b57\">Zhang et al. (2020)</ref> consider global sentence representations an l., 2014)</ref>, Skipthought <ref type=\"bibr\" target=\"#b26\">(Kiros et al., 2015)</ref>, and IS-BERT <ref type=\"bibr\" target=\"#b57\">(Zhang et al., 2020)</ref>. We also compare our models to 9 There is   with n-gram embeddings leads to strong results. Several recent models adopt contrastive objectives <ref type=\"bibr\" target=\"#b57\">(Zhang et al., 2020;</ref><ref type=\"bibr\" target=\"#b55\">Wu et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b21\">(Henderson et al., 2017;</ref><ref type=\"bibr\" target=\"#b18\">Gillick et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b27\">Lee et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Wiebe et al., 2005)</ref>, SST-2 <ref type=\"bibr\">(Socher et al., 2013), TREC (Voorhees and</ref><ref type=\"bibr\" target=\"#b47\">Tice, 2000)</ref> and MRPC <ref type=\"bibr\" target=\"#b14\">(Dolan and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and supervised sentence embedding methods. Unsupervised methods include averaging GloVe embeddings <ref type=\"bibr\" target=\"#b40\">(Pennington et al., 2014)</ref>, Skipthought <ref type=\"bibr\" target=  elaborate on how we obtain different baselines for comparison:</p><p>\u2022 For average GloVe embedding <ref type=\"bibr\" target=\"#b40\">(Pennington et al., 2014)</ref>, InferSent <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpts have been made to integrate multimodal contents into graph-based recommendation systems. MMGCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> constructs modality-specific user-item interaction graphs  tures as the content information to predict the interactions between users and items.</p><p>\u2022 MMGCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> is one of the state-of-the-art multimodal recommendation m \">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. MMGCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> constructed modal-specific graph and refine modal-specific mmendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. MMGCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> constructe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -item interaction graphs to model user preferences specific to each modality. Following MMGCN, GRCN <ref type=\"bibr\" target=\"#b36\">[37]</ref>  Visual structure &lt; l a t e x i t s h a 1 _ b a s e 6 4 ecific representations to obtain the representations of users or items for prediction.</p><p>\u2022 GRCN <ref type=\"bibr\" target=\"#b36\">[37]</ref> is also one of the state-of-the-arts multimodal recommenda onstructed modal-specific graph and refine modal-specific representations for users and items. GRCN <ref type=\"bibr\" target=\"#b36\">[37]</ref> refined user-item interaction graph by identifying the fal 39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. MMGCN <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mmerce, instant video platforms and social media platforms <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. For example, VBPR  e conduct experiments on three categories of widely used Amazon dataset introduced by McAuley et al.<ref type=\"bibr\" target=\"#b24\">[25]</ref>:</figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: c.org/ns/1.0\"><head n=\"3.1.4\">Implementation details.</head><p>We implemente our method in Py-Torch <ref type=\"bibr\" target=\"#b26\">[27]</ref> and the embedding dimension \ud835\udc51 is fixed to 64 for all model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplied to many applications, such as e-commerce, instant video platforms and social media platforms <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b11\">[12]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>, extends the vanilla CF framework by incorporating multimoda m visual representations for learning style features of items and sensing preferences of users. ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref> introduced item-level and component-level attention model fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  model for inferring the underlying users' preferences encoded in the implicit user feedbacks. VECF <ref type=\"bibr\" target=\"#b3\">[4]</ref> modeled users' various attentions on different image regions. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b11\">[12]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>, extends the vanilla CF framework by incorporating multimoda m visual representations for learning style features of items and sensing preferences of users. ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref> introduced item-level and component-level attention model fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s into the embedding process to learn better representations. These graph-based recommender systems <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" ta nal layers, the high-order item-item relationships are captured and aggregated. Following He et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Wu et al. <ref type=\"bibr\" target=\"#b38\">[39]</ref>, we e r to harvest the collaborative signals as well as high-order connectivity signals.</p><p>\u2022 LightGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> argues the unnecessarily complicated design of GCNs (i.e. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o jointly train the image representation as well as the parameters in a recommender model. Sherlock <ref type=\"bibr\" target=\"#b9\">[10]</ref> incorporates categorical information for recommendation bas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 34\">[35,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and usually require a perfect graph structure that are hard to construct in real-world applications <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Since GNNs recursively aggregate information from neighborh type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, probabilistic modeling <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ter representations. These graph-based recommender systems <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> achieve great succe ks (GNNs) have been introduced into recommendation systems <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b42\">43]</ref>, and direct optimization approaches <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. For example, IDGL . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eractions, their expressiveness is confined.</p><p>Inspired by the success of graph neural networks <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ter representations. These graph-based recommender systems <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> achieve great succe ks (GNNs) have been introduced into recommendation systems <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  explicitly model item relationships, which have been proved to be important in recommender systems <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Specifically, the majority of previous work concentrates . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b31\">32]</ref>. Traditional work on multimedia recommendation, e.g., VBPR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and  y consists of two essential components: light graph convolution and layer combination.</p><p>\u2022 VBPR <ref type=\"bibr\" target=\"#b11\">[12]</ref>: Based upon the BPR model, it integrates the visual featur <ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. For example, VBPR <ref type=\"bibr\" target=\"#b11\">[12]</ref> extended Matrix Factorization by incorporating visual feat s.</p><p>For Clothing dataset where visual features are very important in revealing item attributes <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, VBPR, MMGCN, and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 34\">[35,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eractions, their expressiveness is confined.</p><p>Inspired by the success of graph neural networks <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 34\">[35,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 34\">[35,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as context representation learning <ref type=\"bibr\" target=\"#b3\">[4]</ref>, machine translation <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and language modeling <ref type=\"bibr\" target=\"#b15\">[16] arget=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> or generated by pre-defined function <ref type=\"bibr\" target=\"#b20\">[21]</ref> were added to context representations. The other line of w f trainable vectors p i \u2208 {p t } L t=1 , where L is the maximum sequence length. On the other hand, <ref type=\"bibr\" target=\"#b20\">[21]</ref> has proposed to generate p i using the sinusoidal function >) under the self-attention setting in eq. ( <ref type=\"formula\">2</ref>), which is originally from <ref type=\"bibr\" target=\"#b20\">[21]</ref>. They share the same nature that the position information  =\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Properties of RoPE</head><p>Long-term decay: Following <ref type=\"bibr\" target=\"#b20\">[21]</ref>, we choose \u03b8 i = 10000 \u22122i/d . One can prove that this set. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar ey remove the position information in the value term by setting f v (x j ) := W v x j . Later works <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targ e middle two terms of ?? and found little correlations between absolute positions and words. Follow <ref type=\"bibr\" target=\"#b16\">[17]</ref>, they have proposed to model a pair of words or positions . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: position encoding from the perspective with Neural ODE <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and <ref type=\"bibr\" target=\"#b21\">[22]</ref> has proposed to model the position information in complex . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  added to context representations. The other line of works <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: recurrence and no convolution, and the self-attention architecture is shown to be position-agnostic <ref type=\"bibr\" target=\"#b25\">[26]</ref>, different approaches have been proposed to inject positio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as shown on various natural language processing (NLP) tasks such as context representation learning <ref type=\"bibr\" target=\"#b3\">[4]</ref>, machine translation <ref type=\"bibr\" target=\"#b20\">[21]</re ed RoPE. For cross-comparison with other pre-trained Transformer-based models in Chinese, i.e. BERT <ref type=\"bibr\" target=\"#b3\">[4]</ref>, WoBERT <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and NEZHA coding, where absolute position encoding which are trainable <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targe bel></formula><p>Where p i \u2208 R d is a d-dimensional vector depending of the position of token x i . <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ ompared with the pre-trained BERT and WoBERT model on the same pre-training data, as shown in table <ref type=\"bibr\" target=\"#b3\">(4)</ref>. With short text cut-offs, i.e. 512, RoFormer achieves compa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b20\">[21]</ref> were added to context representations. The other line of works <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: recurrence and no convolution, and the self-attention architecture is shown to be position-agnostic <ref type=\"bibr\" target=\"#b25\">[26]</ref>, different approaches have been proposed to inject positio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r by recursively computing a hidden state along the time dimension. Convolution-based models (CNNs) <ref type=\"bibr\" target=\"#b4\">[5]</ref> were typically considered position-agnostic, but recent work f works focuses on absolute position encoding, where absolute position encoding which are trainable <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: recurrence and no convolution, and the self-attention architecture is shown to be position-agnostic <ref type=\"bibr\" target=\"#b25\">[26]</ref>, different approaches have been proposed to inject positio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mbedding method use by other works, i.e. eqs. ( <ref type=\"formula\" target=\"#formula_3\">3</ref>) to <ref type=\"bibr\" target=\"#b9\">(10)</ref>, our approach is multiplicative. Moreover, our RoPE natural r product of query and key for every pair of tokens, which has quadratic complexity O(N 2 ). Follow <ref type=\"bibr\" target=\"#b9\">[10]</ref> , linear attentions reformulate equation 17 as</p><formula   n )v n N n=1 \u03c6(q m ) \u03d5(k n ) (18)</formula><p>where \u03c6(\u2022), \u03d5(\u2022) are usually non-negative functions. <ref type=\"bibr\" target=\"#b9\">[10]</ref> has proposed \u03c6(x) = \u03d5(x) = elu(x) + 1 and first computed th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng high scalability and efficiency. We also find recently emerging scalable algorithms, such as SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SIGN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, are spec sage. However, the GMLP-GU still allows a variety of graph aggregators provided in Section 3.2. SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> can be taken as a special case of this version with the no NP <ref type=\"bibr\" target=\"#b14\">[15]</ref>, AP-GCN <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref>, SIGN <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which are t could compromise performance on a range of benchmark tasks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and suggest separating GCN from the aggregation scheme. We. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cently emerging scalable algorithms, such as SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SIGN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, are special variants of our GMLP framework.</p><p>To valida ale messages are indiscriminately aggregated, without effectively exploring their correlation. SIGN <ref type=\"bibr\" target=\"#b6\">[7]</ref> can be viewed as a special case of using the message aggrega e researches show that such entanglement could compromise performance on a range of benchmark tasks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and suggest separat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: evant nodes and unnecessary messages when increasing the number of hops (i.e., over-smoothing issue <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, link prediction <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref>, recommendation <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, and knowledge graphs <ref type=\"bibr\" target=\"#b15\">[16,</ ph sampling strategies <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> to alleviate the sc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b31\">[32]</ref>, Res-GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, APPNP <ref type=\"bibr\" target=\"#b14\">[15]</ref>, AP-GCN <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGC <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: number of layers. For large graphs that can not be entirely stored on each worker's local   storage <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta  and bottleneck of a two-layer Graph-SAGE along with the increased workers on Reddit dataset.storage<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta  and bottleneck of a two-layer Graph-SAGE along with the increased workers on Reddit dataset.storage<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref>, gathering the required neighborhood neural messages leads to massive data communication costs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta /ref>, gathering the required neighborhood neural messages leads to massive data communication costs<ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta /ref>, gathering the required neighborhood neural messages leads to massive data communication costs<ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: epresentation learning scenarios such as node classification <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targe s been an ever-growing interest in graph sampling strategies <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" targe  The majority of these GNNs can be described in terms of the neural message passing (NMP) framework <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which is based on the core idea of recursive neighborhood a is widely adopted by mainstream GNNs, like GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and GraphSAI tions provided by DGL <ref type=\"bibr\" target=\"#b1\">[2]</ref> to test the scalability of Graph-SAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref> (batch size = 8192). We partition the Reddit dataset across  ith the gating message aggregators. In the inductive settings, the compared baselines are GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, FastGCN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ClusterGCN nctions provided by DGL<ref type=\"bibr\" target=\"#b1\">[2]</ref> to test the scalability of Graph-SAGE<ref type=\"bibr\" target=\"#b8\">[9]</ref> (batch size = 8192). We partition the Reddit dataset across  nctions provided by DGL<ref type=\"bibr\" target=\"#b1\">[2]</ref> to test the scalability of Graph-SAGE<ref type=\"bibr\" target=\"#b8\">[9]</ref> (batch size = 8192). We partition the Reddit dataset across . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> to alleviate the scalability and efficiency issues, samplin f type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and GraphSAINT <ref type=\"bibr\" target=\"#b33\">[34]</ref>, where each layer adopts a neighborhood and an updating fu er workers.   <ref type=\"bibr\" target=\"#b11\">[12]</ref>, two social networks (Flickr and Reddit) in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, four co-authorship graphs (Amazon and Coauthor) in <ref t pe=\"bibr\" target=\"#b4\">[5]</ref>, ClusterGCN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and GraphSAINT <ref type=\"bibr\" target=\"#b33\">[34]</ref>. The detailed introduction of these baselines are shown in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: evant nodes and unnecessary messages when increasing the number of hops (i.e., over-smoothing issue <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Coauthor) in <ref type=\"bibr\" target=\"#b22\">[23]</ref>, the copurchasing network (ogbn-products) in <ref type=\"bibr\" target=\"#b9\">[10]</ref> and the tencent video dataset from our industry partner -Te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: evant nodes and unnecessary messages when increasing the number of hops (i.e., over-smoothing issue <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd resource constraints. While there has been an ever-growing interest in graph sampling strategies <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target ive settings, the compared baselines are GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, FastGCN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ClusterGCN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d knowledge graphs <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. The majority of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd resource constraints. While there has been an ever-growing interest in graph sampling strategies <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target ive settings, the compared baselines are GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, FastGCN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ClusterGCN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: N <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and GraphSAINT <ref type=\"bibr\" target=\"#b33\">[34]</ref>,  the transductive settings, we compare GMLP with GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GAT <ref type=\"bibr\" target=\"#b26\">[27]</ref>, JK-Net <ref type=\"bibr\" target=\"#b31\">[32]</ref>, Res-GCN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en increasing the number of hops (i.e., over-smoothing issue <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>). Moreover, no corr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: epresentation learning scenarios such as node classification <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targe s been an ever-growing interest in graph sampling strategies <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" targe  The majority of these GNNs can be described in terms of the neural message passing (NMP) framework <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which is based on the core idea of recursive neighborhood a is widely adopted by mainstream GNNs, like GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and GraphSAI tions provided by DGL <ref type=\"bibr\" target=\"#b1\">[2]</ref> to test the scalability of Graph-SAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref> (batch size = 8192). We partition the Reddit dataset across  ith the gating message aggregators. In the inductive settings, the compared baselines are GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, FastGCN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ClusterGCN nctions provided by DGL<ref type=\"bibr\" target=\"#b1\">[2]</ref> to test the scalability of Graph-SAGE<ref type=\"bibr\" target=\"#b8\">[9]</ref> (batch size = 8192). We partition the Reddit dataset across  nctions provided by DGL<ref type=\"bibr\" target=\"#b1\">[2]</ref> to test the scalability of Graph-SAGE<ref type=\"bibr\" target=\"#b8\">[9]</ref> (batch size = 8192). We partition the Reddit dataset across . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d knowledge graphs <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. The majority of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Coauthor) in <ref type=\"bibr\" target=\"#b22\">[23]</ref>, the copurchasing network (ogbn-products) in <ref type=\"bibr\" target=\"#b9\">[10]</ref> and the tencent video dataset from our industry partner -Te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> to alleviate the scalability and efficiency issues, samplin f type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and GraphSAINT <ref type=\"bibr\" target=\"#b33\">[34]</ref>, where each layer adopts a neighborhood and an updating fu er workers.   <ref type=\"bibr\" target=\"#b11\">[12]</ref>, two social networks (Flickr and Reddit) in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, four co-authorship graphs (Amazon and Coauthor) in <ref t pe=\"bibr\" target=\"#b4\">[5]</ref>, ClusterGCN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and GraphSAINT <ref type=\"bibr\" target=\"#b33\">[34]</ref>. The detailed introduction of these baselines are shown in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>. Most recently, <ref type=\"bibr\" target=\"#b24\">[25]</ref> achieved impressive performance on several image recogniti re. VATT borrows the exact architecture from BERT <ref type=\"bibr\" target=\"#b22\">[23]</ref> and ViT <ref type=\"bibr\" target=\"#b24\">[25]</ref> except the layer of tokenization and linear projection res <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, image recognition <ref type=\"bibr\" target=\"#b24\">[25]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b107\">[10 #b56\">57]</ref>. However these methods still rely on the feature extracted by CNNs.</p><p>Recently, <ref type=\"bibr\" target=\"#b24\">[25]</ref> proposes a set of convolution-free vision Transformers whi mance with CNNs. <ref type=\"bibr\" target=\"#b85\">[86]</ref> improves the training data efficiency of <ref type=\"bibr\" target=\"#b24\">[25]</ref> by using stronger data augmentations and knowledge distill ]</ref>. Recently, <ref type=\"bibr\" target=\"#b17\">[18]</ref> conduct contrastive learning using ViT <ref type=\"bibr\" target=\"#b24\">[25]</ref> and achieve impressive results. As for the video domain, i  R t\u2022h\u2022w\u20223\u00d7d</formula><p>. This can be seen as a 3D extension of the patching mechanism proposed in <ref type=\"bibr\" target=\"#b24\">[25]</ref>. To encode the position of these patches, we define a dime ecture <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which has been widely used in NLP. Similar to ViT <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we do not tweak the architecture so that our weights can  n, we still achieve competitive results to the supervised pre-training using large-scale image data <ref type=\"bibr\" target=\"#b24\">[25]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  ustrated in Figure <ref type=\"figure\" target=\"#fig_0\">1</ref> middle panel) and refer the reader to <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for more details o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b87\">[88]</ref> have become the de facto model architecture for NLP tasks <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b69\">70,</ref><ref type=\"bibr\" ta > without supervised pre-training.</p><p>Our VATT results, along with others reported for NLP tasks <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, image recognition  m with large-scale, unlabeled visual data? To answer this question, we draw insights from NLP. BERT <ref type=\"bibr\" target=\"#b22\">[23]</ref> and GPT <ref type=\"bibr\" target=\"#b69\">[70,</ref><ref type target=\"#fig_0\">1</ref> illustrates the architecture. VATT borrows the exact architecture from BERT <ref type=\"bibr\" target=\"#b22\">[23]</ref> and ViT <ref type=\"bibr\" target=\"#b24\">[25]</ref> except t former Architecture</head><p>For simplicity, we adopt the most established Transformer architecture <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which has been widely used in NLP. Similar to ViT <ref ty get=\"#fig_0\">1</ref> middle panel) and refer the reader to <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for more details of the standard Transformer architecture. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b81\">[82]</ref>, motion and appearance statistics <ref type=\"bibr\" target=\"#b89\">[90]</ref>, speed <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b90\">91]</ref> and encodings <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, sorting frames or video clips <ref type=\"bibr\" target=\"#b53\">[54,</ref><ref type=\"bibr\" target=\"#b96\">97,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b90\">91]</ref> and encodings <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, sorting frames or . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b90\">91]</ref> and encodings <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, sorting frames or . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b90\">91]</ref> and encodings <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, sorting frames or . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b53\">[54,</ref><ref type=\"bibr\" target=\"#b96\">97,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Recently, <ref type=\"bibr\" target=\"#b67\">[68]</ref> apply . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ised learning can be achieved by predicting whether a video has correspondence with an audio stream <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target n=\"3.4\">Multimodal Contrastive Learning</head><p>Inspired by <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, we use Noise Contras. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g</head><p>Inspired by <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, we use Noise Contrastive Estimation (NCE) to align video-a ei-c.org/ns/1.0\"><head>A.1.1 Pre-training</head><p>Following <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, we use HowTo100M <ref type=\"bibr\" target=\"#b57\">[58]</ref> tch size 8192 and longer pre-training for 6 epochs, arriving at exactly the same results as MIL-NCE <ref type=\"bibr\" target=\"#b58\">[59]</ref> on YouCook2 and the R@10 of 29.2 and MedR of 42 on MSR-VTT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d using a large-scale, human-curated image dataset. Furthermore, we set new records on Kinetics-400 <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Kinetics-600 <ref type=\"bibr\" target=\"#b14\">[15]</ref>, M ype=\"bibr\" target=\"#b80\">[81]</ref>, HMDB51 <ref type=\"bibr\" target=\"#b51\">[52]</ref>, Kinetics-400 <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Kinetics-600 <ref type=\"bibr\" target=\"#b14\">[15]</ref>, a  videos), HMDB51 <ref type=\"bibr\" target=\"#b51\">[52]</ref> (51 classes, 6,766 videos), Kinetics-400 <ref type=\"bibr\" target=\"#b13\">[14]</ref> (400 classes, 234,584 videos), Kinetics-600 <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, patch location prediction <ref type=\"bibr\" target=\"#b23\">[24]</ref>, solving jigsaw puzzles <ref type=\"bibr\" target=\"#b62\">[63]</ref>, and image rotation prediction <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b10\">[11]</ref> and multimodal video understanding <ref type=\"bibr\" target=\"#b83\">[84,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>. However these meth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[25]</ref> achieved impressive performance on several image recognition tasks, including ImageNet <ref type=\"bibr\" target=\"#b21\">[22]</ref>, using a pre-trained Transformer with minimal architecture ion, and zero-shot text-to-video retrieval. Fine-tuning the vision-modality Transformer on ImageNet <ref type=\"bibr\" target=\"#b21\">[22]</ref> obtains the top-1 accuracy of 78.7%, which is comparable t we evaluate the transferability of the vision backbone by fine-tuning it on ImageNet classification <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Since HMDB51, UCF101, and ESC50 are very small datasets c nsformer in the image domain. We finetune the last checkpoint of the vision Transformer on ImageNet <ref type=\"bibr\" target=\"#b21\">[22]</ref> with no modification to our architecture or the tokenizati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b14\">[15]</ref>, Moments in Time <ref type=\"bibr\" target=\"#b60\">[61]</ref>, and AudioSet <ref type=\"bibr\" target=\"#b32\">[33]</ref> without supervised pre-training.</p><p>Our VATT results, a c.org/ns/1.0\"><head n=\"4.1\">Experimental Setup</head><p>Pre-train: we use a combination of AudioSet <ref type=\"bibr\" target=\"#b32\">[33]</ref> and HowTo100M <ref type=\"bibr\" target=\"#b57\">[58]</ref> da > for video action recognition. We use ESC50 <ref type=\"bibr\" target=\"#b65\">[66]</ref> and AudioSet <ref type=\"bibr\" target=\"#b32\">[33]</ref> for audio event classification, and we evaluate the qualit r\" target=\"#b58\">59]</ref>, we use HowTo100M <ref type=\"bibr\" target=\"#b57\">[58]</ref> and AudioSet <ref type=\"bibr\" target=\"#b32\">[33]</ref> to pre-train VATT. The former contains 1.2M unique videos,  We use ESC50 <ref type=\"bibr\" target=\"#b65\">[66]</ref> (50 classes, 2000 audio clips) and AudioSet <ref type=\"bibr\" target=\"#b32\">[33]</ref> (527 classes, \u223c2M audio clips) to evaluate our audio Trans. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref> propose a novel instance discrimination objective. The recent trend of contrastive learning <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b38\">39]</ref>, sorting frames or video clips <ref type=\"bibr\" target=\"#b53\">[54,</ref><ref type=\"bibr\" target=\"#b96\">97,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b81\">[82]</ref>, motion and appearance statistics <ref type=\"bibr\" target=\"#b89\">[90]</ref>, speed <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b90\">91]</ref> and encodings <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e change makes our text model's weights directly transferable to the state-of-the-art text model T5 <ref type=\"bibr\" target=\"#b71\">[72]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to use specialized hardware to accelerate these operations <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar  its generality.</p><p>The most relevant works to ours are <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar =\"bibr\" target=\"#b80\">81]</ref>) or require multiple instances to support different data structures <ref type=\"bibr\" target=\"#b44\">[45]</ref>, all of which lack generality and efficiency. Second, many ware. Previous works only focus on accelerating a specific operation on a particular data structure <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b78\">79,</ref><ref type=\"bibr\" ta  how should the accelerator be integrated into the CPU?</p><p>We first consider an intuitive scheme <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" ta or takes such a hybrid scheme and balances every aspect of a design.</p><p>Fully integrated designs <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ine learning workloads to a GPU or a dedicated accelerator <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b61\">62]</ref>. Communication only happens at the kernel initialization an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  itself.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. Benchmarks</head><p>DPDK. DPDK <ref type=\"bibr\" target=\"#b37\">[38]</ref> is a popular networking application development library fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  itself.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. Benchmarks</head><p>DPDK. DPDK <ref type=\"bibr\" target=\"#b37\">[38]</ref> is a popular networking application development library fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are written in managed languages such as Java, garbage collection is a major consumer of CPU cycles <ref type=\"bibr\" target=\"#b54\">[55]</ref>.</p><p>Trie. We distinguish trie from tree because they ha rators for operations like Malloc <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and garbage collection <ref type=\"bibr\" target=\"#b54\">[55]</ref>, even if the end-to-end performance improvement is not ama ef type=\"bibr\" target=\"#b38\">[39]</ref> and previous works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>, as summarized in Fig. <ref type=\"figure\" target=\"#fig_1\">1 get=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es of such network functions are virtual switch <ref type=\"bibr\" target=\"#b62\">[63]</ref>, firewall <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and load balancer <ref type=\"bibr\" target=\"#b20\">[21]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b62\">[63]</ref>, firewall <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and load balancer <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Hash tables are also used for in-memory key-value stores . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: garbage collector in Java Virtual Machine (JVM) maintains the live objects in a tree data structure <ref type=\"bibr\" target=\"#b0\">[1]</ref>. A garbage collection event causes a traversal of the object n will trigger the idle CFA to issue memory requests for both the queried key and the starting node <ref type=\"bibr\" target=\"#b0\">( 1 )</ref>. Depending on the order in which the results of these two . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are written in managed languages such as Java, garbage collection is a major consumer of CPU cycles <ref type=\"bibr\" target=\"#b54\">[55]</ref>.</p><p>Trie. We distinguish trie from tree because they ha rators for operations like Malloc <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and garbage collection <ref type=\"bibr\" target=\"#b54\">[55]</ref>, even if the end-to-end performance improvement is not ama ef type=\"bibr\" target=\"#b38\">[39]</ref> and previous works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>, as summarized in Fig. <ref type=\"figure\" target=\"#fig_1\">1 get=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sign, as well as to avoid private cache pollution.</p><p>We evaluate QEI using the Sniper simulator <ref type=\"bibr\" target=\"#b10\">[11]</ref> with five representative cloud data center workloads. The  iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Simulator</head><p>We implement QEI in Sniper <ref type=\"bibr\" target=\"#b10\">[11]</ref>, a multi-core x86 simulator. We configure the simulator to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sign, as well as to avoid private cache pollution.</p><p>We evaluate QEI using the Sniper simulator <ref type=\"bibr\" target=\"#b10\">[11]</ref> with five representative cloud data center workloads. The  iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Simulator</head><p>We implement QEI in Sniper <ref type=\"bibr\" target=\"#b10\">[11]</ref>, a multi-core x86 simulator. We configure the simulator to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntify which PVPs perform well. To address this, we use a strategy similar to knowledge distillation <ref type=\"bibr\" target=\"#b14\">(Hinton et al., 2015)</ref>. First, we define a set P of PVPs that in ing.</p><p>We transform the above scores into a probability distribution q using softmax. Following <ref type=\"bibr\" target=\"#b14\">Hinton et al. (2015)</ref>, we use a temperature of T = 2 to obtain a (2019)</ref>  Temperature We choose a temperature of 2 when training the final classifier following <ref type=\"bibr\" target=\"#b14\">Hinton et al. (2015)</ref>.</p><p>Auxiliary language modeling To find. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ence2: b\", and the PLM is asked to predict strings like \"not entailment\". and commonsense knowledge <ref type=\"bibr\" target=\"#b38\">(Trinh and Le, 2018;</ref><ref type=\"bibr\" target=\"#b25\">Petroni et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b25\">Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Wang et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Sakaguchi et al., 2020)</ref>, linguistic capabilities <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e for a subset of classes (e.g., <ref type=\"bibr\" target=\"#b31\">Romera-Paredes and Torr, 2015;</ref><ref type=\"bibr\" target=\"#b41\">Veeranna et al., 2016;</ref><ref type=\"bibr\" target=\"#b47\">Ye et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ;</ref><ref type=\"bibr\" target=\"#b19\">Kassner and Sch\u00fctze, 2020)</ref>, understanding of rare words <ref type=\"bibr\" target=\"#b33\">(Schick and Sch\u00fctze, 2020)</ref>, and ability to perform symbolic rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b3\">(Brin, 1999;</ref><ref type=\"bibr\" target=\"#b0\">Agichtein and Gravano, 2000;</ref><ref type=\"bibr\" target=\"#b1\">Batista et al., 2015)</ref>, parsing <ref type=\"bibr\" target=\"#b22\">(M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref> and argumentative relation classification <ref type=\"bibr\" target=\"#b23\">(Opitz, 2019)</ref>. <ref type=\"bibr\" target=\"#b35\">Srivastava et al. (2018)</ref> use task descriptions for zero-shot cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sks; we report mean accuracy and standard deviation for three training runs. a 12-layer Transformer <ref type=\"bibr\" target=\"#b40\">(Vaswani et al., 2017)</ref>. Both <ref type=\"bibr\" target=\"#b45\">Xie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ord sense disambiguation <ref type=\"bibr\" target=\"#b46\">(Yarowsky, 1995)</ref>, relation extraction <ref type=\"bibr\" target=\"#b3\">(Brin, 1999;</ref><ref type=\"bibr\" target=\"#b0\">Agichtein and Gravano,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>Other approaches for few-shot learning in NLP include exploiting examples from related tasks <ref type=\"bibr\" target=\"#b49\">(Yu et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Gu et al., 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Sakaguchi et al., 2020)</ref>, linguistic capabilities <ref type=\"bibr\" target=\"#b10\">(Ettinger, 2020;</ref><ref type=\"bibr\" target=\"#b19\">Kassner and Sch\u00fc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng generally requires huge metadata storage to be effective.</p><p>Fetch-Directed Prefetching (FDP) <ref type=\"bibr\" target=\"#b8\">[8]</ref>- <ref type=\"bibr\" target=\"#b14\">[12]</ref>, instead, leverag \" target=\"#b16\">[14]</ref>, although the baseline has a decoupled frontend and basic FDP capability <ref type=\"bibr\" target=\"#b8\">[8]</ref>, its shallow 12-instruction FTQ limited the runahead capabil >.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. Fetch Directed Prefetch</head><p>FDP <ref type=\"bibr\" target=\"#b8\">[8]</ref> was proposed to exploit the existing branch prediction mecha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n cache (I-cache) misses and frequent pipeline stalls in modern processors. Instruction prefetching <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b21\">[18]</ref> is a well-known t ig_2\"><head>direction history = (direction history 1 )</head><label>1</label><figDesc>| branch taken<ref type=\"bibr\" target=\"#b0\">(1)</ref> </figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns/1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uding a next-line, the top-3 prefetchers, D-JOLT <ref type=\"bibr\" target=\"#b33\">[30]</ref>, FNL+MMA <ref type=\"bibr\" target=\"#b34\">[31]</ref>, and EIP <ref type=\"bibr\" target=\"#b21\">[18]</ref>, from I  (NL1): prefetch the next line upon a cache miss.</p><p>\u2022 The top-3 prefetchers from IPC-1: FNL+MMA <ref type=\"bibr\" target=\"#b34\">[31]</ref>,</p><p>D-JOLT <ref type=\"bibr\" target=\"#b33\">[30]</ref>, E ne stalls until the BTB miss is resolved. However, PFC can speculatively progress without 3 FNL+MMA <ref type=\"bibr\" target=\"#b34\">[31]</ref> implemented such a filter, but it still requires 1.5x more ne such that the destination would be prefetched when the source cache line is encountered. FNL+MMA <ref type=\"bibr\" target=\"#b34\">[31]</ref> combines efficient, aggressive next line prefetcher (Footp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b43\">[40]</ref>. The branch predictors are sized based on recently announced commercial processors <ref type=\"bibr\" target=\"#b29\">[26]</ref>, <ref type=\"bibr\" target=\"#b30\">[27]</ref>, <ref type=\"bib 3. The branch predictor bandwidth is double the fetch bandwidth to support the run-ahead capability <ref type=\"bibr\" target=\"#b29\">[26]</ref>. We evaluated 2-entry (16instruction) FTQ to disable FDP b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on predictors <ref type=\"bibr\" target=\"#b24\">[21]</ref>- <ref type=\"bibr\" target=\"#b26\">[23]</ref>, <ref type=\"bibr\" target=\"#b36\">[33]</ref> predict the direction (taken or not taken) of branches. Ea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b34\">[31]</ref>, and EIP <ref type=\"bibr\" target=\"#b21\">[18]</ref>, from IPC-1, and perfect <ref type=\"bibr\" target=\"#b35\">[32]</ref> with and without FDP. In this experiment, all mechanisms u  27KB 8-way Entangled Table <ref type=\"table\">)</ref>. \u2022 Perfect prefetching (Perfect): proposed in <ref type=\"bibr\" target=\"#b35\">[32]</ref>, where we assume that a prefetch brings the data into the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b23\">[20]</ref>, that prefetch adjacent I-cache lines, temporal prefetching <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[6]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d the storage overhead of temporal history by sharing it across cores. Call graph prefetching (CGP) <ref type=\"bibr\" target=\"#b45\">[42]</ref> analyzed call graphs to prefetch instructions of the funct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly reduced for discontinuous control flows due to various branches. Temporal prefetching mechanisms <ref type=\"bibr\" target=\"#b3\">[4]</ref>- <ref type=\"bibr\" target=\"#b7\">[7]</ref>, <ref type=\"bibr\" t which is based on FDP and comprises three predictors: selectivenext-four-line (SN4L), discontinuity <ref type=\"bibr\" target=\"#b3\">[4]</ref> (Dis), and BTB prefetching. SN4L improves next-four-line pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctures.</p><p>Software prefetching mechanisms also exist <ref type=\"bibr\" target=\"#b17\">[15]</ref>- <ref type=\"bibr\" target=\"#b20\">[17]</ref>, <ref type=\"bibr\" target=\"#b46\">[43]</ref>. pTask <ref typ ot faithfully model a decoupled frontend. AsmDB <ref type=\"bibr\" target=\"#b17\">[15]</ref> and I-SPY <ref type=\"bibr\" target=\"#b20\">[17]</ref> are recent profile-guided compiler-based software instruct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mages with convnets <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. They typically sha get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. However, our metho harpening of the teacher output to avoid collapse, while other popular components such as predictor <ref type=\"bibr\" target=\"#b27\">[28]</ref>, advanced normalization <ref type=\"bibr\" target=\"#b9\">[10] n unsupervised features without discriminating between images. Of particular interest, Grill et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> propose a metric-learning formulation called BYOL, where f ule is \u03b8 t \u2190 \u03bb\u03b8 t + (1 \u2212 \u03bb)\u03b8 s , with \u03bb following a cosine schedule from 0.996 to 1 during training <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Originally the momentum encoder has been introduced as a  nstraints <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, predictor <ref type=\"bibr\" target=\"#b27\">[28]</ref> or batch normalizations <ref type=\"bibr\" target=\"#b27\">[28 m-up for \u03c4 t from 0.04 to 0.07 during the first 30 epochs. We follow the data augmentations of BYOL <ref type=\"bibr\" target=\"#b27\">[28]</ref> (color jittering, Gaussian blur and solarization) and mult ncoder with the multicrop augmentation and the cross-entropy loss. We also report results with BYOL <ref type=\"bibr\" target=\"#b27\">[28]</ref>, MoCo-v2 <ref type=\"bibr\" target=\"#b13\">[14]</ref> and SwA 2 <ref type=\"bibr\" target=\"#b13\">[14]</ref>, SwAV <ref type=\"bibr\" target=\"#b9\">[10]</ref> and BYOL <ref type=\"bibr\" target=\"#b27\">[28]</ref> when using convnet or ViT. In Tab. 13, we see that when tr performs the corresponding published numbers with ResNet-50. Indeed, we obtain 72.7% for BYOL while <ref type=\"bibr\" target=\"#b27\">[28]</ref> report 72.5% in this 300-epochs setting. We obtain 71.1% f self-supervised learning frameworks, namely MoCo-v2 <ref type=\"bibr\" target=\"#b13\">[14]</ref>, BYOL <ref type=\"bibr\" target=\"#b27\">[28]</ref> and SwAV <ref type=\"bibr\" target=\"#b9\">[10]</ref> with  <r rable sizes with significantly reduced compute requirements <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Self-distillation with no labels. We illustrate DINO in th t a momentum encoder, at the cost of a drop of performance <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Several other works echo this direction, showing that one  t (row 6) while it is critical in BYOL to prevent collapse <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. For completeness, we propose in Appendix B an extended ver  not been observed by other frameworks also using momentum <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, nor when the teacher is built from the previous epoch.</p>  collapse (7, 8) which is consistent with previous studies <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Interestingly, we observe that the teacher output centerin ent by providing target features of higher quality. This dynamic was not observed in previous works <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>.</p><p>Network arc ds and this particular design appears to work best for DINO (Appendix C). We do not use a predictor <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, resulting in the  target=\"#b9\">10]</ref>, predictor <ref type=\"bibr\" target=\"#b27\">[28]</ref> or batch normalizations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. While our framewo nvolutional networks of comparable sizes with a significant reduction of computational requirements <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Our code is availa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder <ref type=\"bibr\" target=\"#b30\">[31]</ref>, multi-crop training <ref type=\"bibr\" target=\"#b9\">[10]</r e good performance with k-NN only emerge when combining certain components such as momentum encoder <ref type=\"bibr\" target=\"#b30\">[31]</ref> and multi-crop augmentation <ref type=\"bibr\" target=\"#b9\"> 27\">[28]</ref>, advanced normalization <ref type=\"bibr\" target=\"#b9\">[10]</ref> or contrastive loss <ref type=\"bibr\" target=\"#b30\">[31]</ref> add little benefits in terms of stability or performance.  nterest, using an exponential moving average (EMA) on the student weights, i.e., a momentum encoder <ref type=\"bibr\" target=\"#b30\">[31]</ref>, is particularly well suited for our framework. The update inally the momentum encoder has been introduced as a substitute for a queue in contrastive learning <ref type=\"bibr\" target=\"#b30\">[31]</ref>. However, in our framework, its role differs since we do n get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. They typically share a similar structure but with differen get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. However, our method shares also similarities with knowledg are to either learn a linear classifier on frozen features <ref type=\"bibr\" target=\"#b78\">[79,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or to finetune the features on downstream tasks. For linear nce classification <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, which considers ea consistent with observations made on convolutional networks <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>. Finally, self-supe  In practice, this requires large batches <ref type=\"bibr\" target=\"#b11\">[12]</ref> or memory banks <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>. Several variants  ResNet-50 (Appendix D). This behavior has not been observed by other frameworks also using momentum <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, nor when the teac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r performs a form of model ensembling similar to Polyak-Ruppert averaging with an exponential decay <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>. Using Polyak-Rupp h.</p><p>We propose to interpret the momentum teacher in DINO as a form of Polyak-Ruppert averaging <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref> with an exponentia f>, weight averaging usually produces a better model than the individual models from each iteration <ref type=\"bibr\" target=\"#b48\">[49]</ref>. By aiming a target obtained with a teacher better than th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>: Copy detection. We report the mAP performance in copy detection on Copydays \"strong\" subset <ref type=\"bibr\" target=\"#b19\">[20]</ref>. For reference, we also report the performance of the mult ion task. We report the mean average precision on the \"strong\" subset of the INRIA Copydays dataset <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The task is to recognize images that have been distorted . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the form of clustering <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targe et=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b1\">2)</ref> and <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b8\">9)</ref>). We also observe that adding a predictor has little impact <  collapse without predictor nor batch normalizations in BYOL <ref type=\"bibr\" target=\"#b6\">(7,</ref><ref type=\"bibr\" target=\"#b8\">9)</ref>, though with a significant performance drop which can likely . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 72\">[73]</ref>. When using soft labels, the approach is often referred to as knowledge distillation <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> and has been primari hat the teacher output centering avoids collapse without predictor nor batch normalizations in BYOL <ref type=\"bibr\" target=\"#b6\">(7,</ref><ref type=\"bibr\" target=\"#b8\">9)</ref>, though with a signifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al. <ref type=\"bibr\" target=\"#b69\">[70]</ref> propose to use a noise contrastive estimator (NCE) <ref type=\"bibr\" target=\"#b29\">[30]</ref> to compare instances instead of classifying them. A caveat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b69\">70]</ref>. Several variants allow automatic grouping of instances in the form of clustering <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target  form of hard-distillation in Caron et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> and Asano et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Second, we consider using the student network from the prev MSE, but this significantly alters the performance (see rows <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b1\">2)</ref> and <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: users may be given suboptimal recommendations, we follow the literature on conservative exploration <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2016;</ref><ref type=\"bibr\" target=\"#b17\">Garcelon et al. en baseline. Outside pure exploration, in the regret minimization setting, conservative exploration <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2016)</ref> enforces the anytime average performance to b p index) chosen at round s, this requirement is formalized as a conservative exploration constraint <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2016)</ref>:</p><formula xml:id=\"formula_19\">\u2200t, 1 t t s= se 2: 0 was pulled because \u03be t &lt; 0. Here the proof follows similar steps as that of Theorem 5 in <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2016)</ref>.     Recall that \u03a6(t) = min( K k=1 N k (t \u2212 1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a. They must rely on strong modeling assumptions with methods such as low-rank matrix factorization <ref type=\"bibr\" target=\"#b31\">(Koren et al., 2009)</ref>. This leads to envy when the system misrep ng modeling assumptions and multi-task learning, with methods such as low-rank matrix factorization <ref type=\"bibr\" target=\"#b31\">(Koren et al., 2009)</ref>. The limited capacity of the models (e.g.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y:</p><formula xml:id=\"formula_30\">\u2206 i = max(max k\u2208[G] \u00b5 i k \u2212 \u00b5 i i , 0).</formula><p>In line with <ref type=\"bibr\" target=\"#b10\">(Chevaleyre et al., 2017)</ref>, we consider two ways of measuring th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e perspective of preference-based fairness <ref type=\"bibr\" target=\"#b45\">(Zafar et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">Ustun et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Kim et al., 2 ype=\"bibr\" target=\"#b15\">(Foley, 1967;</ref><ref type=\"bibr\" target=\"#b1\">Balcan et al., 2018;</ref><ref type=\"bibr\" target=\"#b42\">Ustun et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Kim et al., 2 bibr\" target=\"#b45\">(Zafar et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Kim et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Ustun et al., 2019)</ref>. Our work starts from the perspective of en s protected groups rather than individuals <ref type=\"bibr\" target=\"#b45\">(Zafar et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">Ustun et al., 2019)</ref>) to personalized predictions. Second, as we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  also be oriented towards recommended items <ref type=\"bibr\" target=\"#b9\">(Celis et al., 2017;</ref><ref type=\"bibr\" target=\"#b5\">Biega et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Geyik et al.,  zed versions of the criterion and other variants considering constraint averages over user/contexts <ref type=\"bibr\" target=\"#b5\">(Biega et al., 2018;</ref><ref type=\"bibr\" target=\"#b37\">Patro et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 019)</ref>, and equal user satisfaction <ref type=\"bibr\" target=\"#b36\">(Mehrotra et al., 2017;</ref><ref type=\"bibr\" target=\"#b13\">Ekstrand et al., 2018)</ref>, while our audit for envy-freeness focus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b32\">Lambrecht &amp; Tucker, 2019)</ref>, and equal user satisfaction <ref type=\"bibr\" target=\"#b36\">(Mehrotra et al., 2017;</ref><ref type=\"bibr\" target=\"#b13\">Ekstrand . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ions of group envy-freeness in fair division, either considering every subset of individuals (e.g., <ref type=\"bibr\" target=\"#b4\">Berliant et al. (1992)</ref>), or pre-defined groups (e.g., <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re exploration problems in bandits <ref type=\"bibr\" target=\"#b0\">(Audibert &amp; Bubeck, 2010;</ref><ref type=\"bibr\" target=\"#b6\">Bubeck et al., 2009)</ref>. To alleviate the potential negative side e \"bibr\" target=\"#b18\">Geyik et al., 2019)</ref>.</p><p>Multi-arm bandits In pure exploration bandits <ref type=\"bibr\" target=\"#b6\">(Bubeck et al., 2009)</ref>, an agent has to identify a specific set o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ject detection <ref type=\"bibr\" target=\"#b16\">[19]</ref>. Our work is inspired by the recent DetNAS <ref type=\"bibr\" target=\"#b6\">[9]</ref>, but has three fundamental differences. First, the studied t e architectures in search space, the same as previous work <ref type=\"bibr\" target=\"#b17\">[20,</ref><ref type=\"bibr\" target=\"#b6\">9]</ref>, we resort to evolutionary algorithms <ref type=\"bibr\" target tch statistics on one path should be independent of others <ref type=\"bibr\" target=\"#b17\">[20,</ref><ref type=\"bibr\" target=\"#b6\">9]</ref>. Therefore, we need to recalculate batch statistics for each . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: verted bottleneck (MBConv) <ref type=\"bibr\" target=\"#b42\">[45]</ref> with squeeze-excitation module <ref type=\"bibr\" target=\"#b21\">[24,</ref><ref type=\"bibr\" target=\"#b20\">23]</ref> to construct a new enotes mobile inverted bottleneck <ref type=\"bibr\" target=\"#b42\">[45]</ref> with squeeze excitation <ref type=\"bibr\" target=\"#b21\">[24]</ref>.</p><p>volution (DSConv) <ref type=\"bibr\" target=\"#b8\">[11. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the common-used binary cross-entropy loss for foreground-background classification and the IoU loss <ref type=\"bibr\" target=\"#b51\">[54]</ref> for object bounding-box regression.</p><p>Phase 3: Searchi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plexity, while they inevitably bring non-negligible performance degradation due to information loss <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b35\">38]</ref>. On the other hand. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: CO <ref type=\"bibr\" target=\"#b10\">[13]</ref> DaSiam <ref type=\"bibr\" target=\"#b54\">[57]</ref> C-RPN <ref type=\"bibr\" target=\"#b14\">[17]</ref> ATOM <ref type=\"bibr\" target=\"#b11\">[14]</ref> SiamFC++(A). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plexity, while they inevitably bring non-negligible performance degradation due to information loss <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b35\">38]</ref>. On the other hand. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  representation capability. On the other hand, ATOM <ref type=\"bibr\" target=\"#b11\">[14]</ref>, DiMP <ref type=\"bibr\" target=\"#b3\">[6]</ref>, and ROAM <ref type=\"bibr\" target=\"#b50\">[53]</ref> combine  ATOM <ref type=\"bibr\" target=\"#b11\">[14]</ref> TKU <ref type=\"bibr\" target=\"#b45\">[48]</ref> DiMP r <ref type=\"bibr\" target=\"#b3\">[6]</ref> Ocean(off) <ref type=\"bibr\" target=\"#b53\">[56]</ref> Ours  < type=\"bibr\" target=\"#b49\">[52]</ref> Ocean-online <ref type=\"bibr\" target=\"#b53\">[56]</ref> DiMP-50 <ref type=\"bibr\" target=\"#b3\">[6]</ref> Ours  ECO <ref type=\"bibr\" target=\"#b10\">[13]</ref> DaSiam < type=\"bibr\" target=\"#b49\">[52]</ref> SiamRPN++(R) <ref type=\"bibr\" target=\"#b27\">[30]</ref> DiMP-50 <ref type=\"bibr\" target=\"#b3\">[6]</ref> Ours  pared to the trackers with online update, such as ATOM  the trackers with online update, such as ATOM <ref type=\"bibr\" target=\"#b11\">[14]</ref> and DiMP r <ref type=\"bibr\" target=\"#b3\">[6]</ref>, LightTrack-LargeB is also competitive, surpassing them by 5 formance of LightTrack will be further improved. For example, LightTrack-LargeB outperforms DiMP-50 <ref type=\"bibr\" target=\"#b3\">[6]</ref> by 1.2%, while using 8\u00d7 fewer Params (3.1 v.s. 26.1 M).</p>< 4 presents that LightTrack-Mobile achieves better precision (69.5%), being 0.8% higher than DiMP-50 <ref type=\"bibr\" target=\"#b3\">[6]</ref>. Besides, the P norm and AUC of LightTrack-Mobile are compar ype=\"bibr\" target=\"#b53\">[56]</ref> by 1.2% and 2.9%, respectively.  Compared to the online DiMP-18 <ref type=\"bibr\" target=\"#b3\">[6]</ref>, LightTrack-LargeB improves the success score by 2.1%, while  trackers (such as <ref type=\"bibr\" target=\"#b28\">[31,</ref><ref type=\"bibr\" target=\"#b11\">14,</ref><ref type=\"bibr\" target=\"#b3\">6]</ref>) all require ImageNet pre-training for their backbone network. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> take the powerful ResNet-50 <ref type=\"bibr\" target=\"#b19\">[22]</ref> instead of AlexNet <ref type=\"bibr\" target=\"#b26\">[29]</re . For example, the human-designed SiamRPN++ with ResNet-50 backbone has 48.9G FLOPs with 54M Params <ref type=\"bibr\" target=\"#b19\">[22]</ref>, being orders of magnitude more complex than architectures. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ete set of architecture candidates, differentiable methods <ref type=\"bibr\" target=\"#b34\">[37,</ref><ref type=\"bibr\" target=\"#b5\">8]</ref> relax the search space to be continuous, such that the search. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d expensive. For instance, the latest SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> trackers respectively utilize 7.1G and 20.3G model Flops a  type=\"bibr\" target=\"#b27\">[30]</ref>, SiamFC++ <ref type=\"bibr\" target=\"#b49\">[52]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref>, while using much fewer Flops and parameters. Best viewed  n existing methods. On Snapdragon 845 Adreno 630 GPU [3], our LightTrack runs 12\u00d7 faster than Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> (38.4 v.s. 3.2 fps), while using 13\u00d7 fewer parameters (1.9 he localization precision. Meanwhile, SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> take the powerful ResNet-50 <ref type=\"bibr\" target=\"#b19\" orted in <ref type=\"bibr\" target=\"#b25\">[28]</ref>. Ocean(off) denotes the offline version of Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref>. Some values are missing because either the tracker is not <ref type=\"bibr\" target=\"#b45\">[48]</ref> DiMP r <ref type=\"bibr\" target=\"#b3\">[6]</ref> Ocean(off) <ref type=\"bibr\" target=\"#b53\">[56]</ref> Ours  <ref type=\"formula\" target=\"#formula_3\">4</ref>). On n, we train the head and the backbone supernets jointly on tracking data. The same as previous work <ref type=\"bibr\" target=\"#b53\">[56]</ref>, the tracking data consists of Youtube-BB <ref type=\"bibr\" th subsequent search images. The hyper-parameters in testing are selected with the tracking toolkit <ref type=\"bibr\" target=\"#b53\">[56]</ref>, which contains an automated parameter tuning algorithm. O f type=\"bibr\" target=\"#b27\">[30]</ref> ATOM <ref type=\"bibr\" target=\"#b11\">[14]</ref> Ocean-offline <ref type=\"bibr\" target=\"#b53\">[56]</ref> SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> Ocea =\"bibr\" target=\"#b53\">[56]</ref> SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> Ocean-online <ref type=\"bibr\" target=\"#b53\">[56]</ref> DiMP-50 <ref type=\"bibr\" target=\"#b3\">[6]</ref> Ours  ECO  is 1.6% and 1.9% superior than SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> and Ocean(off) <ref type=\"bibr\" target=\"#b53\">[56]</ref>, respectively. Besides, if we loosen the computation const e of 0.555, which surpasses SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> and Ocean-offline <ref type=\"bibr\" target=\"#b53\">[56]</ref> by 1.2% and 2.9%, respectively.  Compared to the online Di va 7 and Xiaomi Mi 8. We observe that SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> cannot run at real-time speed (i.e., &lt; 25 fps) on these ef type=\"bibr\" target=\"#b27\">[30]</ref>, SiamFC++<ref type=\"bibr\" target=\"#b49\">[52]</ref> and Ocean<ref type=\"bibr\" target=\"#b53\">[56]</ref>, while using much fewer Flops and parameters. Best viewed  > (middle). In essence, it is a variant of Siamese tracker <ref type=\"bibr\" target=\"#b27\">[30,</ref><ref type=\"bibr\" target=\"#b53\">56]</ref>. Specifically, it takes a pair of tracking images as the in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trackers that achieve superior performance compared to handcrafted SOTA trackers, such as SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean [56], while using much fewer model Flops and par , tracking models are becoming increasingly heavy and expensive. For instance, the latest SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> tracke ef type=\"bibr\" target=\"#b2\">[5]</ref>, SiamRPN <ref type=\"bibr\" target=\"#b28\">[31]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref>, SiamFC++ <ref type=\"bibr\" target=\"#b49\">[52]</ref> and Oc 30M Flops tracker, which achieves an EAO of 0.33 on VOT-19 benchmark, surpassing the SOTA SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> by 4.6% while reducing its model complexity (48.9G Flops)  for bounding box estimation, which largely improve the localization precision. Meanwhile, SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> take t s than 600M Flops <ref type=\"bibr\" target=\"#b4\">[7]</ref>, i.e., mobile setting. However, SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> with ResNet-50 backbone has 48.9G Flops, which exceeds the =\"bibr\" target=\"#b47\">[50]</ref> SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> SiamRPN++(M) <ref type=\"bibr\" target=\"#b27\">[30]</ref> ATOM <ref type=\"bibr\" target=\"#b11\">[14]</ref> TKU <ref ty -Mobile achieves superior performance compared to existing SOTA offline trackers, such as SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and SiamFC++ <ref type=\"bibr\" target=\"#b49\">[52]</ref>, wh esNet-50 and GoogleNet, respectively. DaSiam <ref type=\"bibr\" target=\"#b54\">[57]</ref> SiamRPN++(R) <ref type=\"bibr\" target=\"#b27\">[30]</ref> ATOM <ref type=\"bibr\" target=\"#b11\">[14]</ref> Ocean-offli =\"bibr\" target=\"#b11\">[14]</ref> SiamFC++(A) <ref type=\"bibr\" target=\"#b49\">[52]</ref> SiamRPN++(R) <ref type=\"bibr\" target=\"#b27\">[30]</ref> DiMP-50 <ref type=\"bibr\" target=\"#b3\">[6]</ref> Ours  pare latforms, , including Apple iPhone 7 Plus, Huawei Nova 7 and Xiaomi Mi 8. We observe that SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> cannot <ref type=\"bibr\" target=\"#b2\">[5]</ref>, SiamRPN<ref type=\"bibr\" target=\"#b28\">[31]</ref>, SiamRPN++<ref type=\"bibr\" target=\"#b27\">[30]</ref>, SiamFC++<ref type=\"bibr\" target=\"#b49\">[52]</ref> and Oce <ref type=\"figure\" target=\"#fig_3\">2</ref> (middle). In essence, it is a variant of Siamese tracker <ref type=\"bibr\" target=\"#b27\">[30,</ref><ref type=\"bibr\" target=\"#b53\">56]</ref>. Specifically, it . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cessful adoptions of diffusion probabilistic models (diffusion models for short) in image synthesis <ref type=\"bibr\" target=\"#b1\">[Ho et al., 2020]</ref> and speech synthesis <ref type=\"bibr\" target=\"  synthesis tasks <ref type=\"bibr\" target=\"#b2\">[Kong et al., 2020]</ref> and image synthesis fields <ref type=\"bibr\" target=\"#b1\">[Ho et al., 2020]</ref>. However, to the best of our knowledge, diffus ion, to provide a basic understanding of diffusion probabilistic models (diffusion model for short) <ref type=\"bibr\" target=\"#b1\">[Ho et al., 2020]</ref>, we first briefly review its formulation.</p>< al lower bound (ELBO) on negative log likelihood and introduce KL divergence and variance reduction <ref type=\"bibr\" target=\"#b1\">[Ho et al., 2020]</ref>:</p><formula xml:id=\"formula_4\">E[\u2212 log p \u03b8 (x <p>Diffusion probabilistic models <ref type=\"bibr\" target=\"#b10\">[Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b1\">Ho et al., 2020]</ref> are a kind of generative models using a Markov  019]</ref>. Then the LR information is fused with the 2D-convolution block output hidden. Following <ref type=\"bibr\" target=\"#b1\">Ho et al. [2020]</ref>, we transform the timestep   </p><formula xml:i ng of the conditional noise predictor uses Eq. ( <ref type=\"formula\">6</ref>) as loss term and Adam <ref type=\"bibr\" target=\"#b1\">[Kingma and Ba, 2014]</ref> as optimizer, with batch size 16 and learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Diffusion models</head><p>Diffusion probabilistic models <ref type=\"bibr\" target=\"#b10\">[Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b1\">Ho e rnel introduced in its original paper for a fair comparison.</p><p>For General SR, we use the DIV2K <ref type=\"bibr\" target=\"#b10\">[Timofte et al., 2018] and</ref><ref type=\"bibr\">Flickr2K [Timofte et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\">[Fookes et al., 2012;</ref><ref type=\"bibr\">Sajjadi et al., 2017]</ref>, remote sensing <ref type=\"bibr\" target=\"#b4\">[Li et al., 2009]</ref>, surveillance monitoring <ref type=\"bibr\" targ nd flow-based methods. PSNR-oriented methods <ref type=\"bibr\" target=\"#b0\">[Dong et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Lim et al., 2017;</ref><ref type=\"bibr\" target=\"#b12\">Zhang et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref>, remote sensing <ref type=\"bibr\" target=\"#b4\">[Li et al., 2009]</ref>, surveillance monitoring <ref type=\"bibr\" target=\"#b0\">[Fang et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Park et al., 20 rized into three main types: PSNR-oriented, GANdriven and flow-based methods. PSNR-oriented methods <ref type=\"bibr\" target=\"#b0\">[Dong et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Lim et al., 201 ef><ref type=\"bibr\" target=\"#b12\">Zhang et al., 2018b;</ref><ref type=\"bibr\">Qiu et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">Guo et al., 2020]</ref> are trained with simple distribution assumptio mation loss. One groundbreaking solution to tackle the over-smoothing problem is GAN-driven methods <ref type=\"bibr\" target=\"#b0\">[Cheon et al., 2018;</ref><ref type=\"bibr\">Kim et al., 2019;</ref><ref [Ho et al., 2020]</ref> and speech synthesis <ref type=\"bibr\" target=\"#b2\">[Kong et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Chen et al., 2021]</ref> witness the power of diffusion models in gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b0\">[Dong et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Lim et al., 2017;</ref><ref type=\"bibr\" target=\"#b12\">Zhang et al., 2018b;</ref><ref type=\"bibr\">Qiu et al., 2019;</ref><re neral SR (4\u00d7) compared with <ref type=\"bibr\">EDSR [Lim et al., 2017]</ref>, RRDB, ESRGAN, RankSRGAN <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> and SRFlow (\u03c4 = 0.9) with their official r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2009]</ref>, surveillance monitoring <ref type=\"bibr\" target=\"#b0\">[Fang et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Park et al., 2020]</ref> and so on. SISR aims to recover high-resoluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b0\">[Dong et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Lim et al., 2017;</ref><ref type=\"bibr\" target=\"#b12\">Zhang et al., 2018b;</ref><ref type=\"bibr\">Qiu et al., 2019;</ref><re neral SR (4\u00d7) compared with <ref type=\"bibr\">EDSR [Lim et al., 2017]</ref>, RRDB, ESRGAN, RankSRGAN <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> and SRFlow (\u03c4 = 0.9) with their official r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2009]</ref>, surveillance monitoring <ref type=\"bibr\" target=\"#b0\">[Fang et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Park et al., 2020]</ref> and so on. SISR aims to recover high-resoluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2009]</ref>, surveillance monitoring <ref type=\"bibr\" target=\"#b0\">[Fang et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Park et al., 2020]</ref> and so on. SISR aims to recover high-resoluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion to the corresponding HR space. In this paper, we choose the RRDB architecture following SRFlow <ref type=\"bibr\" target=\"#b6\">[Lugmayr et al., 2020]</ref>, which employs the residual-in-residual s sts of 162,770 images for training and evaluate on 5000 images from the test split following SRFlow <ref type=\"bibr\" target=\"#b6\">[Lugmayr et al., 2020]</ref>. We central-crop the aligned patches 2 an so evaluate our SRDiff on LPIPS <ref type=\"bibr\" target=\"#b11\">[Zhang et al., 2018a]</ref>, LR-PSNR <ref type=\"bibr\" target=\"#b6\">[Lugmayr et al., 2020]</ref> and the pixel standard deviation \u03c3. LPIPS DB [Wang et al.,  2018],ESRGAN [Wang et al., 2018],ProgFSR [Kim et al.,  2019]  and SRFlow (\u03c4 = 0.8)<ref type=\"bibr\" target=\"#b6\">[Lugmayr et al., 2020]</ref> 3 . RRDB is trained by only L 1 loss and  face SR (8\u00d7) and general SR (4\u00d7) tasks. For face SR, we use Celeb-Faces Attributes Dataset (CelebA) <ref type=\"bibr\" target=\"#b6\">[Liu et al., 2015b]</ref>, which is a large-scale face attributes data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance modeling. State-of-the-art techniques represent hardware dataflow using either compute-centric <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref> or data-centric no ataflow is specified using loop transformation directives including reorder, blocking, and parallel <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. The compute-centr pute-centric notation-based models only analyze data reuse opportunities in a coarse-grained manner <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. For example, Inte n is widely adopted as it can directly represents dataflow in high-level languages using directives <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. In <ref type=\"bib .org/ns/1.0\"><head>Features</head><p>Computation-centric Data-centric STT Relation-Centric Timeloop <ref type=\"bibr\" target=\"#b78\">[39]</ref> Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> MAE ze various hardware metrics, and support for non-systolic array spatial architecture. Both Timeloop <ref type=\"bibr\" target=\"#b78\">[39]</ref> and Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> 3\">[14]</ref>, dataflow is notated using two hyperplanes with polyhedral dependency graph. Timeloop <ref type=\"bibr\" target=\"#b78\">[39]</ref> describes the design space using a concise and unified loo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t. Other spatial architectures, like DySER <ref type=\"bibr\" target=\"#b58\">[19]</ref> and Plasticine <ref type=\"bibr\" target=\"#b81\">[42]</ref>, integrate PEs and their interconnect in a flexible manner [22]</ref>. Mesh NoCs are applied in DySER <ref type=\"bibr\" target=\"#b58\">[19]</ref> and Plasticine <ref type=\"bibr\" target=\"#b81\">[42]</ref>. In a multicast network, PEs are connected through wires t get=\"#b58\">19,</ref><ref type=\"bibr\" target=\"#b75\">36,</ref><ref type=\"bibr\" target=\"#b79\">40,</ref><ref type=\"bibr\" target=\"#b81\">42,</ref><ref type=\"bibr\" target=\"#b87\">48]</ref>. Each PE contains a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: theory is widely used for systolic architectures <ref type=\"bibr\">[4,</ref><ref type=\"bibr\">9,</ref><ref type=\"bibr\" target=\"#b52\">13,</ref><ref type=\"bibr\" target=\"#b66\">27,</ref><ref type=\"bibr\" tar et=\"#b74\">35]</ref>. Recent efforts attempt to apply this theory for automatic FPGA code generation <ref type=\"bibr\" target=\"#b52\">[13,</ref><ref type=\"bibr\" target=\"#b66\">27]</ref>. PolySA <ref type= eneration <ref type=\"bibr\" target=\"#b52\">[13,</ref><ref type=\"bibr\" target=\"#b66\">27]</ref>. PolySA <ref type=\"bibr\" target=\"#b52\">[13]</ref> and AutoSA <ref type=\"bibr\" target=\"#b93\">[54]</ref> compi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b52\">13,</ref><ref type=\"bibr\" target=\"#b66\">27,</ref><ref type=\"bibr\" target=\"#b67\">28,</ref><ref type=\"bibr\" target=\"#b74\">35]</ref>. There are also studies mainly focusing on the arithmetic p c arrays <ref type=\"bibr\">[4,</ref><ref type=\"bibr\">9,</ref><ref type=\"bibr\" target=\"#b67\">28,</ref><ref type=\"bibr\" target=\"#b74\">35]</ref>. Recent efforts attempt to apply this theory for automatic . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s represent hardware dataflow using either compute-centric <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref> or data-centric notations <ref type=\"bibr\" target=\"#b63\">[2 et=\"#b49\">[10,</ref><ref type=\"bibr\" target=\"#b50\">11,</ref><ref type=\"bibr\" target=\"#b63\">24,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. Temporal reuse happens when the same</p><formula xml:id=\"f ation directives including reorder, blocking, and parallel <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. The compute-centric notation provides great flexibility fo nalyze data reuse opportunities in a coarse-grained manner <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. For example, Interstellar <ref type=\"bibr\" target=\"#b95\">[ presents dataflow in high-level languages using directives <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. In <ref type=\"bibr\" target=\"#b53\">[14]</ref>, dataflow is  et=\"#b50\">[11,</ref><ref type=\"bibr\" target=\"#b64\">25,</ref><ref type=\"bibr\" target=\"#b71\">32,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. Eyeriss-v2 <ref type=\"bibr\" target=\"#b50\">[11]</ref> analy c Data-centric STT Relation-Centric Timeloop <ref type=\"bibr\" target=\"#b78\">[39]</ref> Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> MAESTRO <ref type=\"bibr\">[24, 25] [4, 9, 28, 54]</ref> TEN rray spatial architecture. Both Timeloop <ref type=\"bibr\" target=\"#b78\">[39]</ref> and Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> use compute-centric notation. The loop order determines th =\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. For example, Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> calculates data reuse using the product of unroll factors. ives. The mapping directives consist of memory constraints and PE workload assignment. Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> represents the hardware dataflow using Halide's scheduling [25]</ref> applies data-centric notation and analyzes data reuse, energy, and latency. Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> explores the design space using loop operators like loop s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b73\">34,</ref><ref type=\"bibr\" target=\"#b85\">46,</ref><ref type=\"bibr\" target=\"#b86\">47,</ref><ref type=\"bibr\" target=\"#b92\">53]</ref>. In recent years, spatial architectures have emerged as a p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b56\">17,</ref><ref type=\"bibr\" target=\"#b58\">19,</ref><ref type=\"bibr\" target=\"#b75\">36,</ref><ref type=\"bibr\" target=\"#b79\">40,</ref><ref type=\"bibr\" target=\"#b81\">42,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b95\">[56]</ref> represents the hardware dataflow using Halide's scheduling language <ref type=\"bibr\" target=\"#b84\">[45]</ref> for design exploration. Interstellar extends Halide with a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: llions, or even billions, of valid schedules with a wide range of performance and energy efficiency <ref type=\"bibr\" target=\"#b48\">[49]</ref>. Considering the vast range of DNN layer dimensions and ha r cost models <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b70\">[71]</ref>. However, navig </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Brute-force Approaches:</head><p>Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> Brute-force &amp; Random dMazeRunner <ref type=\"bibr\" targ  Buffer space <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b64\">[65]</ref>, <ref type=\"bib ial architecture. Listing 1 shows an example of a schedule. Here, we use a loop-nest representation <ref type=\"bibr\" target=\"#b48\">[49]</ref> to explicitly describe how the computation of a convolutio e the one with the best result for the target metric, and 2) the Timeloop Hybrid mapper in Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> that randomly selects a tiling factorization, prunes super. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rative sampling <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>. However, these schedulers typically require massive train enTuner <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref> FlexFlow <ref type=\"bibr\" target=\"#b37\">[38]</ref> MCMC Gamma <ref type=\"bibr\" target=\"#b39\">[40]</ref> Genet 2\">[3]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[26]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>, <ref type=\"bibr\" target=\"#b60\">[61]</ref>, <ref type=\"bibr\" target=\"#b62\">[63]</ref>, <ref type=\"bibr\" target=\"#b71\">[72]</ref> and cloud appli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rching for the best candidate based on their cost models <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bib s:</head><p>Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> Brute-force &amp; Random dMazeRunner <ref type=\"bibr\" target=\"#b22\">[23]</ref> Brute-force Triton <ref type=\"bibr\" target=\"#b64\">[65]</re <head>Inputs Weights Outputs</head><p>Input Buffer space <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as black-box tuning, beam search, and other machine learning algorithms with iterative sampling <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" et=\"#b14\">[15]</ref> ML-based Iteration Halide <ref type=\"bibr\" target=\"#b55\">[56]</ref> Beamsearch <ref type=\"bibr\" target=\"#b2\">[3]</ref>, OpenTuner <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref typ ent efforts use feedback-driven approaches along with machine learning or other statistical methods <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rching for the best candidate based on their cost models <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bib s:</head><p>Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> Brute-force &amp; Random dMazeRunner <ref type=\"bibr\" target=\"#b22\">[23]</ref> Brute-force Triton <ref type=\"bibr\" target=\"#b64\">[65]</re <head>Inputs Weights Outputs</head><p>Input Buffer space <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8\">[9]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>. Prior work tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8\">[9]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>. Prior work tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ayer dimensions, including: ResNet-50 <ref type=\"bibr\" target=\"#b33\">[34]</ref>, ResNeXt-50 (32x4d) <ref type=\"bibr\" target=\"#b69\">[70]</ref>, and Deepbench <ref type=\"bibr\" target=\"#b23\">[24]</ref> (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b44\">[45]</ref> FlexFlow <ref type=\"bibr\" target=\"#b37\">[38]</ref> MCMC Gamma <ref type=\"bibr\" target=\"#b39\">[40]</ref> Genetic Algorithm Mind Mapping <ref type=\"bibr\" target=\"#b \">[15]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref> to improve the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ich has been applied successfully in some previous studies <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>In transfer learning for ASR, we first train a neur nsfer learning. The first attempt for transfer leaning in the DNN-based ASR system was conducted in <ref type=\"bibr\" target=\"#b14\">[15]</ref>. They used four European languages to build a multilingual. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: at such approaches need large amounts of data for training <ref type=\"bibr\" target=\"#b3\">[4]</ref>- <ref type=\"bibr\" target=\"#b5\">[6]</ref>. This could be a fundamental issue for low-resource language \" target=\"#b13\">[14]</ref> and transfer learning from a highresource language to a low-resource one <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Transfer learning is one of the most straightforward  to deal with the low-resource problem, which has been applied successfully in some previous studies <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>In transfe rsDat corpus <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We use a fully convolutional neural network <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref> as our ASR model,  would be better merely train Softmax layer instead of retraining more layers.</p><p>The authors in <ref type=\"bibr\" target=\"#b5\">[6]</ref> used a fully convolutional end-to-end ASR model. They traine d><p>We used 11 1Dconvolutional layers on top of each other based on the architecture introduced in <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We used zero-  adapt well to Persian speech data. Our results are consistent with the finding of previous work in <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The speed of convergence: As Fig. <ref type=\"figure\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: beled speech data <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>, data augmentation techniques <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation techniques <ref type=\"bibr\" target=\"#b12\">[13]</ref>, pre-training methods using Autoencoders <ref type=\"bibr\" target=\"#b13\">[14]</ref> and transfer learning from a highresource language to a lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: an performs better than the ASR model, which was trained from scratch by only German data.</p><p>In <ref type=\"bibr\" target=\"#b21\">[22]</ref>, a multilingual ASR model with 10 BABEL languages from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: languages. On the other hand, in this field, end-to-end approaches are the current state of the art <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The crucial point . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: languages. On the other hand, in this field, end-to-end approaches are the current state of the art <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The crucial point . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b18\">[19]</ref> as our ASR model, and train it by Connectionist Temporal Classification (CTC) loss <ref type=\"bibr\" target=\"#b19\">[20]</ref>. In the first step, we train the network on 960 hours of t orresponds to one of the Persian phonemes.</p><p>We trained the network using the CTC loss function <ref type=\"bibr\" target=\"#b19\">[20]</ref>. If we denote the input of acoustic features and desired o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the multilingual ASR model is more beneficial than monolingual ASR models.</p><p>A more recent work <ref type=\"bibr\" target=\"#b22\">[23]</ref> proposed a language-adversarial transfer learning method. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the multilingual ASR model is more beneficial than monolingual ASR models.</p><p>A more recent work <ref type=\"bibr\" target=\"#b22\">[23]</ref> proposed a language-adversarial transfer learning method. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding representation learning using self-supervised methods <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, semi-supervised me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b18\">[19]</ref> as our ASR model, and train it by Connectionist Temporal Classification (CTC) loss <ref type=\"bibr\" target=\"#b19\">[20]</ref>. In the first step, we train the network on 960 hours of t orresponds to one of the Persian phonemes.</p><p>We trained the network using the CTC loss function <ref type=\"bibr\" target=\"#b19\">[20]</ref>. If we denote the input of acoustic features and desired o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -hidden Markov model) GMM-HMM and (deep neural network-hidden Markov model) DNN-HMM. The authors in <ref type=\"bibr\" target=\"#b16\">[17]</ref> also attempted to use an endto-end model for the Persian p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvised methods to take advantages of unlabeled speech data <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>, data augmentatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the multilingual ASR model is more beneficial than monolingual ASR models.</p><p>A more recent work <ref type=\"bibr\" target=\"#b22\">[23]</ref> proposed a language-adversarial transfer learning method. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: an performs better than the ASR model, which was trained from scratch by only German data.</p><p>In <ref type=\"bibr\" target=\"#b21\">[22]</ref>, a multilingual ASR model with 10 BABEL languages from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: an performs better than the ASR model, which was trained from scratch by only German data.</p><p>In <ref type=\"bibr\" target=\"#b21\">[22]</ref>, a multilingual ASR model with 10 BABEL languages from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ld, end-to-end approaches are the current state of the art <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The crucial point is that such approaches need large amount. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvised methods to take advantages of unlabeled speech data <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>, data augmentatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: beled speech data <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>, data augmentation techniques <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: languages. On the other hand, in this field, end-to-end approaches are the current state of the art <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The crucial point . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b22\">[25]</ref>, or with pattern-based models such as discrete choice modelling (DCM) <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" targ tability of the trajectories predicted by hand-crafted models, in particular discrete choice models <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>, and the high accura te choice modelling uti-lizes a grid for selecting the next action, but relative to each individual <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50,</ref><ref type=\"bibr\" targ iors for human motion have been described in DCM literature, we follow the formulation described in <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>, which is well adapt all exponential parameters of the chosen hand-designed functions are set to the estimated values in <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>. For synthetic data, atical formulations of the above functions are detailed in <ref type=\"bibr\" target=\"#b47\">[50,</ref><ref type=\"bibr\" target=\"#b4\">7]</ref>. Each person is assumed to select the anchor a k for which th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by hand-crafted models, in particular discrete choice models <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>, and the high accuracy of the neural network-based predicti ed in DCM literature, we follow the formulation described in <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>, which is well adapted to our problem setting. Functions mo n hand-designed functions are set to the estimated values in <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>. For synthetic data, we embed the goals in a 64-dimensional r selecting the next action, but relative to each individual <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50,</ref><ref type=\"bibr\" target=\"#b44\">47]</ref>. The high interpret corresponding functions. The exact mathematical formulations of the above functions are detailed in <ref type=\"bibr\" target=\"#b47\">[50,</ref><ref type=\"bibr\" target=\"#b4\">7]</ref>. Each person is assu  L-MNL for measuring the anchor probabilities, rather than those of the cross-nested logit model in <ref type=\"bibr\" target=\"#b47\">[50]</ref>.</p><p>Training: All the parameters of our model are learn ble the integration of the DCM framework. According to the DCM framework for pedestrian forecasting <ref type=\"bibr\" target=\"#b47\">[50]</ref>, the anchor set A at each time-step is defined dynamically. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nterpretable decisions. To this end, we resort to a novel hybrid and interpretable framework in DCM <ref type=\"bibr\" target=\"#b52\">[55]</ref>, where knowledge-based hand-crafted functions can be augme pe=\"bibr\" target=\"#b20\">23]</ref>. In this paper, we utilize the Learning Multinomial Logit (L-MNL) <ref type=\"bibr\" target=\"#b52\">[55]</ref>, as our base DCM model.</p></div> <div xmlns=\"http://www.t pture. The inclusion of NN-based terms helps to alleviate this issue.</p><p>Recently proposed L-MNL <ref type=\"bibr\" target=\"#b52\">[55]</ref> architecture allows having both NN-based and knowledge-bas keep the knowledge-based functions and the parameters interpretable after adding the neural network <ref type=\"bibr\" target=\"#b52\">[55,</ref><ref type=\"bibr\" target=\"#b20\">23]</ref>. In this paper, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: istributions. Gupta et al. <ref type=\"bibr\" target=\"#b18\">[21]</ref> utilize Winner-takes-all (WTA) <ref type=\"bibr\" target=\"#b48\">[51]</ref> loss, in addi-Figure <ref type=\"figure\">2</ref>: At each t ts a unimodal trajectory distribution. 2. Winner-Takes-All (WTA): this architecture was proposed in <ref type=\"bibr\" target=\"#b48\">[51]</ref> to encourage the network to output diverse trajectories. 3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: osed in literature <ref type=\"bibr\" target=\"#b45\">[48,</ref><ref type=\"bibr\" target=\"#b51\">54,</ref><ref type=\"bibr\" target=\"#b9\">12,</ref><ref type=\"bibr\" target=\"#b18\">21,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pattern-based models such as discrete choice modelling (DCM) <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" target=\"#b5\">8]</ref>. These models, based  arget=\"#b5\">[8,</ref><ref type=\"bibr\" target=\"#b37\">40]</ref>, and critical or emergency situations <ref type=\"bibr\" target=\"#b17\">[20,</ref><ref type=\"bibr\" target=\"#b42\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . DCMs have often been applied in fields of economy <ref type=\"bibr\" target=\"#b0\">[3]</ref>, health <ref type=\"bibr\" target=\"#b49\">[52]</ref> and transportation <ref type=\"bibr\" target=\"#b8\">[11]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b63\">66,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" target=\"#b35\">38,</ref><ref type=\"bibr\" target=\"#b53\">56]</ref>. To provide different weights to neighbours that affect the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b25\">28,</ref><ref type=\"bibr\" target=\"#b18\">21,</ref><ref type=\"bibr\" target=\"#b3\">6,</ref><ref type=\"bibr\" target=\"#b34\">37]</ref>. Generative models implicitly model the probability distrib ework. Several works <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b3\">6,</ref><ref type=\"bibr\" target=\"#b34\">37]</ref> utilize generative adversarial networks to model trajectory. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: osed in literature <ref type=\"bibr\" target=\"#b45\">[48,</ref><ref type=\"bibr\" target=\"#b51\">54,</ref><ref type=\"bibr\" target=\"#b9\">12,</ref><ref type=\"bibr\" target=\"#b18\">21,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rent weights to neighbours that affect the trajectory of the pedestrian of interest, multiple works <ref type=\"bibr\" target=\"#b59\">[62,</ref><ref type=\"bibr\" target=\"#b15\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b61\">64,</ref><ref type=\"bibr\" target=\"#b63\">66,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" target=\"#b35\">38,</ref><ref type=\"bibr\" target=\"#b53\">56]</ref>. To provide differe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based perspectives <ref type=\"bibr\" target=\"#b54\">[57,</ref><ref type=\"bibr\" target=\"#b46\">49,</ref><ref type=\"bibr\" target=\"#b2\">5]</ref>. While the hand-crafted functions of these methods lead to in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of this random term leads to different types of DCM models <ref type=\"bibr\" target=\"#b56\">[59,</ref><ref type=\"bibr\" target=\"#b39\">42]</ref>. While many works incorporate data-driven methods into the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Social LSTM, neural networks have become the de-facto choice for designing human trajectory models <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b61\">64,</ref><ref type=\"bibr\" ta o use Gaussian mixture model (GMM) on top of the recurrent decoder in cVAE framework. Several works <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b3\">6,</ref><ref type=\"bibr\" targ get=\"#b45\">[48,</ref><ref type=\"bibr\" target=\"#b51\">54,</ref><ref type=\"bibr\" target=\"#b9\">12,</ref><ref type=\"bibr\" target=\"#b18\">21,</ref><ref type=\"bibr\" target=\"#b61\">64,</ref><ref type=\"bibr\" tar nerative modelling <ref type=\"bibr\" target=\"#b30\">[33,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" target=\"#b18\">21,</ref><ref type=\"bibr\" target=\"#b3\">6,</ref><ref type=\"bibr\" targe \">37]</ref> utilize generative adversarial networks to model trajectory distributions. Gupta et al. <ref type=\"bibr\" target=\"#b18\">[21]</ref> utilize Winner-takes-all (WTA) <ref type=\"bibr\" target=\"#b rget=\"#b48\">[51]</ref> to encourage the network to output diverse trajectories. 3. SGAN: Social GAN <ref type=\"bibr\" target=\"#b18\">[21]</ref>, a popular generative model to tackle multimodal trajector. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b0\">[3]</ref>, health <ref type=\"bibr\" target=\"#b49\">[52]</ref> and transportation <ref type=\"bibr\" target=\"#b8\">[11]</ref>, where interpretation of parameters that capture human beha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based perspectives <ref type=\"bibr\" target=\"#b54\">[57,</ref><ref type=\"bibr\" target=\"#b46\">49,</ref><ref type=\"bibr\" target=\"#b2\">5]</ref>. While the hand-crafted functions of these methods lead to in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oice modelling (DCM) <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" target=\"#b5\">8]</ref>. These models, based on domain knowledge, were successful in  f type=\"bibr\" target=\"#b43\">[46,</ref><ref type=\"bibr\" target=\"#b60\">63]</ref>, collision avoidance <ref type=\"bibr\" target=\"#b5\">[8,</ref><ref type=\"bibr\" target=\"#b37\">40]</ref>, and critical or eme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>. Human social interactions have also been modelled using other knowledge-based perspectives <ref type=\"bibr\" target=\"#b54\">[57,</ref><ref type=\"bibr\" target=\"#b46\">49,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e trajectory of the pedestrian of interest, multiple works <ref type=\"bibr\" target=\"#b59\">[62,</ref><ref type=\"bibr\" target=\"#b15\">18,</ref><ref type=\"bibr\" target=\"#b32\">35,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  leading to mode collapse.</p><p>Another recently popular approach is based on generative modelling <ref type=\"bibr\" target=\"#b30\">[33,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" ta  on the past scene, thereby naturally offering a possibility to output multiple samples. Lee et al. <ref type=\"bibr\" target=\"#b30\">[33]</ref> propose a recurrent encoder-decoder architecture within co ational Auto-Encoder architectures has been shown recently <ref type=\"bibr\" target=\"#b25\">[28,</ref><ref type=\"bibr\" target=\"#b30\">33]</ref> to successfully predict multi-modal trajectories by learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  leading to mode collapse.</p><p>Another recently popular approach is based on generative modelling <ref type=\"bibr\" target=\"#b30\">[33,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" ta  on the past scene, thereby naturally offering a possibility to output multiple samples. Lee et al. <ref type=\"bibr\" target=\"#b30\">[33]</ref> propose a recurrent encoder-decoder architecture within co ational Auto-Encoder architectures has been shown recently <ref type=\"bibr\" target=\"#b25\">[28,</ref><ref type=\"bibr\" target=\"#b30\">33]</ref> to successfully predict multi-modal trajectories by learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b41\">44,</ref><ref type=\"bibr\" target=\"#b33\">36]</ref> propose to utilize attention mechanisms <ref type=\"bibr\" target=\"#b55\">[58,</ref><ref type=\"bibr\" target=\"#b6\">9]</ref>. The attention weigh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  leading to mode collapse.</p><p>Another recently popular approach is based on generative modelling <ref type=\"bibr\" target=\"#b30\">[33,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" ta  on the past scene, thereby naturally offering a possibility to output multiple samples. Lee et al. <ref type=\"bibr\" target=\"#b30\">[33]</ref> propose a recurrent encoder-decoder architecture within co ational Auto-Encoder architectures has been shown recently <ref type=\"bibr\" target=\"#b25\">[28,</ref><ref type=\"bibr\" target=\"#b30\">33]</ref> to successfully predict multi-modal trajectories by learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pattern-based models such as discrete choice modelling (DCM) <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" target=\"#b5\">8]</ref>. These models, based  arget=\"#b5\">[8,</ref><ref type=\"bibr\" target=\"#b37\">40]</ref>, and critical or emergency situations <ref type=\"bibr\" target=\"#b17\">[20,</ref><ref type=\"bibr\" target=\"#b42\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>. Human social interactions have also been modelled using other knowledge-based perspectives <ref type=\"bibr\" target=\"#b54\">[57,</ref><ref type=\"bibr\" target=\"#b46\">49,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b39\">42]</ref>. While many works incorporate data-driven methods into the DCM framework <ref type=\"bibr\" target=\"#b7\">[10,</ref><ref type=\"bibr\" target=\"#b23\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rent weights to neighbours that affect the trajectory of the pedestrian of interest, multiple works <ref type=\"bibr\" target=\"#b59\">[62,</ref><ref type=\"bibr\" target=\"#b15\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o many topics such as pedestrian flows <ref type=\"bibr\" target=\"#b36\">[39]</ref>, walking in groups <ref type=\"bibr\" target=\"#b43\">[46,</ref><ref type=\"bibr\" target=\"#b60\">63]</ref>, collision avoidan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>. Human social interactions have also been modelled using other knowledge-based perspectives <ref type=\"bibr\" target=\"#b54\">[57,</ref><ref type=\"bibr\" target=\"#b46\">49,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e trajectory of the pedestrian of interest, multiple works <ref type=\"bibr\" target=\"#b59\">[62,</ref><ref type=\"bibr\" target=\"#b15\">18,</ref><ref type=\"bibr\" target=\"#b32\">35,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b39\">42]</ref>. While many works incorporate data-driven methods into the DCM framework <ref type=\"bibr\" target=\"#b7\">[10,</ref><ref type=\"bibr\" target=\"#b23\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el classification for an input image, which is normally formulated as semantic segmentation problem <ref type=\"bibr\" target=\"#b53\">(Ronneberger et al., 2015;</ref><ref type=\"bibr\" target=\"#b1\">Badrina  The different types of layers are common in DCNNs developed for semantic segmentation. Please refer<ref type=\"bibr\" target=\"#b53\">(Ronneberger et al., 2015;</ref><ref type=\"bibr\" target=\"#b7\">Chai et the calculation efficiency. Among various DCNNs based on deep semantic segmentation networks, U-Net <ref type=\"bibr\" target=\"#b53\">(Ronneberger et al., 2015)</ref> performs well in segmenting differen head><p>The employed DCNN for semantic segmentation is an encoderdecoder network adapted from U-Net <ref type=\"bibr\" target=\"#b53\">(Ronneberger et al., 2015)</ref>. The architecture of the adapted U-N. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b38\">(Matsuda et al., 2001;</ref><ref type=\"bibr\" target=\"#b62\">Son et al., 2013;</ref><ref type=\"bibr\" target=\"#b63\">Soontranon et al., 2015;</ref><ref type=\"bibr\" target=\"#b82\">Zhou et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic information for many tasks such as crop growing condition monitoring, yield prediction and so on <ref type=\"bibr\" target=\"#b38\">(Matsuda et al., 2001;</ref><ref type=\"bibr\" target=\"#b62\">Son et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n applications, such as Cband RADARSAT-2 <ref type=\"bibr\" target=\"#b65\">(Staples et al., 2017;</ref><ref type=\"bibr\" target=\"#b25\">Jia et al., 2013)</ref> and Sentinel-1 <ref type=\"bibr\" target=\"#b79\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ula\">7</ref>) speckle filter, multi-temporal filter with averaged structure Lee filter presented in <ref type=\"bibr\" target=\"#b6\">(Caves et al., 2011)</ref> is used to suppress the speckle noise to re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic information for many tasks such as crop growing condition monitoring, yield prediction and so on <ref type=\"bibr\" target=\"#b38\">(Matsuda et al., 2001;</ref><ref type=\"bibr\" target=\"#b62\">Son et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b15\">(Dong et al., 2016;</ref><ref type=\"bibr\" target=\"#b43\">Oliphant et al., 2019;</ref><ref type=\"bibr\" target=\"#b50\">Qiu et al., 2017;</ref><ref type=\"bibr\" target=\"#b66\">Teluguntla et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch on crop mapping and proved its reliability <ref type=\"bibr\" target=\"#b73\">(Xu et al., 2020;</ref><ref type=\"bibr\" target=\"#b21\">Hao et al., 2020)</ref>. Hence, CDL data sets, which corresponded to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b62\">Son et al., 2013;</ref><ref type=\"bibr\" target=\"#b63\">Soontranon et al., 2015;</ref><ref type=\"bibr\" target=\"#b82\">Zhou et al., 2017)</ref>. In the past decades, crop classification ha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b77\">Zhang et al., 2017)</ref>, Landsat <ref type=\"bibr\" target=\"#b15\">(Dong et al., 2016;</ref><ref type=\"bibr\" target=\"#b43\">Oliphant et al., 2019;</ref><ref type=\"bibr\" target=\"#b50\">Qiu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd extensively in both scientific and commercial workloads <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Unfortunately, considerable state is required to memorize c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e at most 10 Sim-Points for each SPEC benchmark.</p><p>We present multi-core results for CloudSuite <ref type=\"bibr\" target=\"#b42\">[43]</ref> and multi-programmed SPEC benchmarks. For CloudSuite, we u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onchip caches <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, but the metadata storage requirements for these prefetche. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Somogyi et al., build on STMS to combine spatial and temporal streams with their STeMs prefetcher <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>While the GHB improves significantly over table-bas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ome prefetchers do store their metadata in onchip caches <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, but the metada ar memory accesses. Some irregular memory accesses can be prefetched by exploiting spatial locality <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b29\">[29]</ref>, <ref type=\"bib tial patterns can be prefetched across different regions in memory. For example, the SMS prefetcher <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses on-chip tables to correlate spatial footprints with t ate Triage against two state-of-the-art on-chip prefetchers, namely, Spatial Memory Streaming (SMS) <ref type=\"bibr\" target=\"#b16\">[17]</ref> and the Best Offset Prefetcher (BO) <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nces of correlated address pairs, are found extensively in both scientific and commercial workloads <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Unfortunately, con  because temporal streams can have highly variable lengths <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Their STMS prefetcher <ref type=\"bibr\" target=\"#b0\">[1]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: use novel ways to reduce the overhead of off-2. This paper improves upon the original Triage design <ref type=\"bibr\" target=\"#b2\">[3]</ref> in two ways. First, it employs a new dynamic partitioning sc  of Triage, denoted as Triage and Triage-ISR. The former reproduces the design from our MICRO paper <ref type=\"bibr\" target=\"#b2\">[3]</ref> and does not include the integrated stream representation. I resentation improves Triage's performance by 2.7 percent compared to the baseline version of Triage <ref type=\"bibr\" target=\"#b2\">[3]</ref> that does not distinguish between regular and irregular acce. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or SPEC benchmarks we use the reference input set. For all single-core benchmarks, we use SimPoints <ref type=\"bibr\" target=\"#b41\">[42]</ref> to find representative regions. Each Sim-Point is warmed u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, <ref type=\"bibr\" target=\"#b20\">[21]</ref> and strided <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .</p><p>Early solutions amortized the cost of off-chip metadata accesses across multiple prefetches <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In 2013, the Irreg <p>A second class of prefetchers exploit address correlation by storing metadata in off-chip memory <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t ot ideal for organizing off-chip metadata because temporal streams can have highly variable lengths <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Their STMS prefetc  type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Their STMS prefetcher <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref> instead uses a glob eal choice for organizing on-chip metadata. In particular, compared to other metadata organizations <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t ge against existing off-chip temporal prefetchers, namely, Sampled Temporal Memory Streaming (STMS) <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Domino <ref type=\"bibr\" target=\"#b43\">[44]</ref>, and MISB   metadata prefetcher.</p><p>Unfortunately, all of these solutions still have important limitations: <ref type=\"bibr\" target=\"#b0\">(1)</ref> Even MISB has metadata traffic of 260.8 percent, so its perf ificantly over table-based solutions, it suffers from three drawbacks that are addressed by Triage: <ref type=\"bibr\" target=\"#b0\">(1)</ref> The GHB makes it infeasible to combine address correlation w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e status register to identify a candidate way for eviction. For example, for a 4-way LLC with SRRIP <ref type=\"bibr\" target=\"#b40\">[40]</ref> replacement policy (2-3 bits per entry storing RRPV values. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and cannot achieve a compromise between them. Yu et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref> utilized semantic branch and detail branch to extract two  t we use GTX 1660Ti during the evaluation phase). For training, we adopt minibatch gradient descent <ref type=\"bibr\" target=\"#b41\">[42]</ref> with a batch size of 4. The Adam optimizer is selected to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: adds too many padding operations and requires a large kernel to capture context. Inspired by CARAFE <ref type=\"bibr\" target=\"#b42\">[43]</ref>, we designed concentration-aware guided upsampling. Our CA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the accuracy of location information while extracting context information: (i) Multiscale inference <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref> inputs images o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l value will change drastically at the class boundary. The method based on probability graph models <ref type=\"bibr\" target=\"#b26\">[27]</ref>- <ref type=\"bibr\" target=\"#b28\">[29]</ref> uses high-order. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ruments such as hyperspectral and synthetic aperture radar <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, increasingly more computer vision methods have achieved rem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> realizes long-range dependencies, which helps to focus on what networks want to model. Fu et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> proposed the DANet based on a self-attention mechanism tha is selected to optimize the network, and the initial learning rate is set as 0.001. Following DANet <ref type=\"bibr\" target=\"#b37\">[38]</ref>, we employ the ''poly'' learning rate policy where the pow. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: image classification, object detection and target tracking <ref type=\"bibr\" target=\"#b7\">[8]</ref>- <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, the fully convolutional network (FCN) <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e representational power of CNNs, Yu et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> and Hu et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> added an attention mechanism to enhance the relationship b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ffic situations, urban and rural construction, and the environmental monitoring of forest resources <ref type=\"bibr\" target=\"#b2\">[3]</ref>- <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>The associat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 4]</ref>, have achieved state-of-theart performance on benchmark dataset. The deep residual network <ref type=\"bibr\" target=\"#b14\">[15]</ref> solves exploding and vanishing gradient problems. The deep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r, and the key of this method is the selection of similarity criteria. Lakshmi and Sankaranarayanan <ref type=\"bibr\" target=\"#b25\">[26]</ref> summarized the edge detection method, the basic idea of wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ruments such as hyperspectral and synthetic aperture radar <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, increasingly more computer vision methods have achieved rem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: del the correlation of different dimensions of features. A Nonlocal network designed by Wang et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> realizes long-range dependencies, which helps to focus on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: computer hardware and remote sensing instruments such as hyperspectral and synthetic aperture radar <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, increasingly more . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the accuracy of location information while extracting context information: (i) Multiscale inference <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref> inputs images o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad><p>Different backbones (EfficientNet <ref type=\"bibr\" target=\"#b44\">[45]</ref>, ResNet, DenseNet <ref type=\"bibr\" target=\"#b46\">[47]</ref>) are used in our proposed method to guarantee the generali. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tionships between object classes. Super-pixel algorithms <ref type=\"bibr\" target=\"#b29\">[30]</ref>- <ref type=\"bibr\" target=\"#b31\">[32]</ref> adopt the similarity of the features between pixels to gro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: image classification, object detection and target tracking <ref type=\"bibr\" target=\"#b7\">[8]</ref>- <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, the fully convolutional network (FCN) <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: end learning framework of a single encoder. Hence, we instead adopt the studentteacher-like network <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b29\">29]</ref> in which only the st get encoder. At the same time, the target encoder is updated based on momentum-based moving average <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target o bootstrap the representations by providing enhanced but consistent targets to the online encoders <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>. Figure <ref type=\"fig d (i.e., \ud835\udf0f = 1). This observation is consistent with previous work on momentum-based moving average <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target pproximate the online encoder, we investigate \ud835\udf0f in the range of [0.9, 1.0], as done in previous work<ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>.</note> \t\t</body> \t\t<b sed a bootstrapping-based self-supervised learning framework <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b6\">6]</ref>, which is capable of avoiding the collapsed solution without  l recent work on bootstrapping-based representation learning <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b6\">6]</ref> empirically demonstrated that this kind of dynamics (i.e., up paper, the term \"bootstrapping\" is not used in the statistical meaning, but in the idiomatic meaning<ref type=\"bibr\" target=\"#b6\">[6]</ref>. Strictly speaking, it refers to using estimated values (i.e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  advantage of these neighbors as input features of users and items, we use a neighbor-based encoder <ref type=\"bibr\" target=\"#b10\">[10,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" ta  networks (GCN). It can consider multi-hop neighbors as well based on a stack of GCN layers. \u2022 LGCN <ref type=\"bibr\" target=\"#b10\">[10]</ref>: The state-of-the-art method that further tailors the GCN- k based on the LGCN encoder. It computes the user/item representations by using the lightweight GCN <ref type=\"bibr\" target=\"#b10\">[10]</ref> that adopts the proposed neighbor augmentation technique (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: distribution <ref type=\"bibr\" target=\"#b26\">[26]</ref>.</p><p>On the other hand, generative methods <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" targ utoencoder (VAE) <ref type=\"bibr\" target=\"#b18\">[18]</ref> or generative adversarial networks (GAN) <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" targ licit feedback, we evaluate the performance of each method by using two widely-used ranking metrics <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" targ  latent distribution to approximate the prior, assumed to be the normal distribution.</p><p>\u2022 CFGAN <ref type=\"bibr\" target=\"#b1\">[1]</ref>: The state-of-the-art GAN-based OCCF method.</p><p>The discr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, which explicitly aims to distinguish positive user-item in ype=\"bibr\" target=\"#b27\">27]</ref> to deep neural networks <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, a variety of techniques have been studied to effectively m personalized ranking <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> defines the similarity of a user and an item by the inner p hbor-based encoder <ref type=\"bibr\" target=\"#b10\">[10,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> which additionally takes a given set of users (or items) as et=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> which provide the minimum count of user-item interactions f get=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b33\">33]</ref> train their model s xploit the neighborhood information of users and items to compute the representations.</p><p>\u2022 NGCF <ref type=\"bibr\" target=\"#b32\">[32]</ref>: A neighbor-based method which encodes a user's (and item'. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: For better recommendation performance and faster convergence, advanced negative sampling strategies <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref> are also proposed to butions for negative sampling. For example, sampling negative pairs from a non-uniform distribution <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref> (e.g., the multinomi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b26\">[26]</ref>.</p><p>On the other hand, generative methods <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" tar rs indicating their interacted items. They employ the architecture of variational autoencoder (VAE) <ref type=\"bibr\" target=\"#b18\">[18]</ref> or generative adversarial networks (GAN) <ref type=\"bibr\"  tem) encoder for the OCCF task. It simplifies the GCN by using the light graph convolution. \u2022 M-VAE <ref type=\"bibr\" target=\"#b18\">[18]</ref>: The OCCF method based on a variational autoencoder that r used ranking metrics <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b18\">18]</ref>: Precision (P@\ud835\udc3e) and Normalized Discounted Cumulative Gain . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve metric learning <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b25\">25]</ref> directly learns the latent space by modeling their similari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd faster convergence, advanced negative sampling strategies <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref> are also proposed to sample from non-uniform distributions. ple, sampling negative pairs from a non-uniform distribution <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref> (e.g., the multinomial distribution which models the probab us the convergence speed and final performance largely depend on the negative sampling distribution <ref type=\"bibr\" target=\"#b26\">[26]</ref>.</p><p>On the other hand, generative methods <ref type=\"bi of {2 0 , 2 1 , 2 2 , 2 3 , 2 4 }, and consider three different distributions for negative sampling <ref type=\"bibr\" target=\"#b26\">[26]</ref>: 1) uniform sampling, 2) static-and-global sampling which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , generative methods <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b31\">31]</ref> aim to learn the un t=\"#b18\">[18]</ref> or generative adversarial networks (GAN) <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b31\">31]</ref>, in order to infer . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b31\">31]</ref> aim to learn the underlying latent distribution of users, u arial networks (GAN) <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b31\">31]</ref>, in order to infer the users' preference on each item based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iments, we use three real-world datasets: CiteULike <ref type=\"bibr\" target=\"#b30\">[30]</ref>, Ciao <ref type=\"bibr\" target=\"#b28\">[28]</ref>, and FourSquare <ref type=\"bibr\" target=\"#b20\">[20]</ref>.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \ud835\udf19 is a scoring function to facilitate the optimization. For example, Bayesian personalized ranking <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n=\"1\">INTRODUCTION</head><p>Over the past decade, one-class collaborative filtering (OCCF) problems <ref type=\"bibr\" target=\"#b13\">[13,</ref><ref type=\"bibr\" target=\"#b24\">24]</ref> have been extensiv y (e.g., inner product) between the representation of a user and an item. From matrix factorization <ref type=\"bibr\" target=\"#b13\">[13,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref> to deep neural net dle the real-world recommendation scenario where only positive user-item interaction can be labeled <ref type=\"bibr\" target=\"#b13\">[13,</ref><ref type=\"bibr\" target=\"#b24\">24]</ref> as a form of users optimize the scores by using the pointwise prediction loss <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b13\">13]</ref> or the pairwise ranking loss <ref type=\"bibr\" target=\"#b12\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by bootstrapping methods in deep reinforcement learning <ref type=\"bibr\" target=\"#b21\">[21,</ref><ref type=\"bibr\" target=\"#b22\">22]</ref>, it directly bootstraps the representation of images by usi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" tar et=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" tar f their representations, and collaborative metric learning <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b25\">25]</ref> directly learns the ance of each method by using two widely-used ranking metrics <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b18\">18]</ref>: Precision (P@\ud835\udc3e) an zes the Euclidean distance between a user and an item based on the pairwise hinge loss.</p><p>\u2022 SML <ref type=\"bibr\" target=\"#b17\">[17]</ref>: The state-of-the-art OCCF method based on metric learning. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd faster convergence, advanced negative sampling strategies <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref> are also proposed to sample from non-uniform distributions. ple, sampling negative pairs from a non-uniform distribution <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref> (e.g., the multinomial distribution which models the probab us the convergence speed and final performance largely depend on the negative sampling distribution <ref type=\"bibr\" target=\"#b26\">[26]</ref>.</p><p>On the other hand, generative methods <ref type=\"bi of {2 0 , 2 1 , 2 2 , 2 3 , 2 4 }, and consider three different distributions for negative sampling <ref type=\"bibr\" target=\"#b26\">[26]</ref>: 1) uniform sampling, 2) static-and-global sampling which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b31\">31]</ref> aim to learn the underlying latent distribution of users, u arial networks (GAN) <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b31\">31]</ref>, in order to infer the users' preference on each item based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, which explicitly a get=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" tar llow previous work <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> which provide the m sentation of a user and an item. From matrix factorization <ref type=\"bibr\" target=\"#b13\">[13,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref> to deep neural networks <ref type=\"bibr\" target=\"#b11\">[11, bibr\" target=\"#b13\">13]</ref> or the pairwise ranking loss <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref> to discriminate between positive and negative interactions. der in the end.</p><p>Existing discriminative OCCF methods <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref> have tried to optimize the latent space where the user-item thods in the first category directly optimize the embedding vectors of users and items.</p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b27\">[27]</ref>: The Bayesian personalized ranking method for OCCF. It opt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by bootstrapping methods in deep reinforcement learning <ref type=\"bibr\" target=\"#b21\">[21,</ref><ref type=\"bibr\" target=\"#b22\">22]</ref>, it directly bootstraps the representation of images by usi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, which explicitly aims to distinguish positive user-item in ype=\"bibr\" target=\"#b27\">27]</ref> to deep neural networks <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, a variety of techniques have been studied to effectively m personalized ranking <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> defines the similarity of a user and an item by the inner p hbor-based encoder <ref type=\"bibr\" target=\"#b10\">[10,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> which additionally takes a given set of users (or items) as et=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> which provide the minimum count of user-item interactions f get=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b33\">33]</ref> train their model s xploit the neighborhood information of users and items to compute the representations.</p><p>\u2022 NGCF <ref type=\"bibr\" target=\"#b32\">[32]</ref>: A neighbor-based method which encodes a user's (and item'. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: em in our framework. Motivated by the recent success of self-supervised learning in various domains <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b4\">4]</ref>, we exploit augmented  earning approach has achieved a great success in computer vision and natural language understanding <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b4\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd faster convergence, advanced negative sampling strategies <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref> are also proposed to sample from non-uniform distributions. ple, sampling negative pairs from a non-uniform distribution <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref> (e.g., the multinomial distribution which models the probab us the convergence speed and final performance largely depend on the negative sampling distribution <ref type=\"bibr\" target=\"#b26\">[26]</ref>.</p><p>On the other hand, generative methods <ref type=\"bi of {2 0 , 2 1 , 2 2 , 2 3 , 2 4 }, and consider three different distributions for negative sampling <ref type=\"bibr\" target=\"#b26\">[26]</ref>: 1) uniform sampling, 2) static-and-global sampling which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b3\">[Guo et al., 2008;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2019]</ref> make noticeable progress, existing methods su ificant performance degradation when tested on unseen dataset. Take the state-of-the-art model PIPR <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref> as an example, compared tested on trainset-h v xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Protein feature encoding</head><p>Previous work <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref> has proved that protein features based on th earning based: We choose three representative deep learning (DL) algorithms in PPI prediction, PIPR <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref>, DNN-PPI <ref type=\"bibr\" target=\"#b4\">[Li e ibition, catalysis, and expression. Each pair of interacting proteins contains at least one of them.<ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref> randomly select 1,690 and 5,189 proteins fro br\" target=\"#b4\">[Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Sun et al., 2017. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: accumulated PPI data to predict the unknown PPIs accurately.</p><p>Despite long-term research works <ref type=\"bibr\" target=\"#b3\">[Guo et al., 2008;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al ssification can be summarized into two stages. The early research is based on Machine Learning (ML) <ref type=\"bibr\" target=\"#b3\">[Guo et al., 2008;</ref><ref type=\"bibr\">Wong et al., 2015;</ref><ref  ately.</p><p>Despite long-term research works <ref type=\"bibr\" target=\"#b3\">[Guo et al., 2008;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al PPI prediction and classification. These works <ref type=\"bibr\" target=\"#b4\">[Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al r\" target=\"#b3\">Hashemifar et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Sun et al., 2017]</ref> typically use Convolution Neural Networks or R ) are the keys to different GNN architectures. In this paper we use Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b3\">[Xu et al., 2018]</ref>, where the sum of the neighbor node features i thod to represent each amino acid (Details in the Supplementary Section 3). We adopt Adam algorithm <ref type=\"bibr\" target=\"#b3\">[Kingma and Ba, 2014]</ref> to optimize all trainable parameters. The  ><p>1. Machine Learning based: We choose three representative machine learning (ML) algorithms, SVM <ref type=\"bibr\" target=\"#b3\">[Guo et al., 2008]</ref>, RF <ref type=\"bibr\">[Wong et al., 2015]</ref ref>. The input feature of the algorithms uniformly selects common handcrafted protein features, AC <ref type=\"bibr\" target=\"#b3\">[Guo et al., 2008]</ref> and CTD <ref type=\"bibr\" target=\"#b3\">[Du et  mon handcrafted protein features, AC <ref type=\"bibr\" target=\"#b3\">[Guo et al., 2008]</ref> and CTD <ref type=\"bibr\" target=\"#b3\">[Du et al., 2017]</ref>, of which CTD use seven attributes for the div \">[Chen et al., 2019]</ref>, DNN-PPI <ref type=\"bibr\" target=\"#b4\">[Li et al., 2018]</ref> and DPPI <ref type=\"bibr\" target=\"#b3\">[Hashemifar et al., 2018]</ref>. We construct the same architecture as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uence of the protein.</p><p>More recent work has focused on the feature representation of proteins. <ref type=\"bibr\" target=\"#b5\">[Saha and others, 2020]</ref> proposes a novel deep multi-modal archit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uence of the protein.</p><p>More recent work has focused on the feature representation of proteins. <ref type=\"bibr\" target=\"#b5\">[Saha and others, 2020]</ref> proposes a novel deep multi-modal archit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms due to its powerful expressive ability, including PPI prediction and classification. These works <ref type=\"bibr\" target=\"#b4\">[Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al. algorithms in PPI prediction, PIPR <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref>, DNN-PPI <ref type=\"bibr\" target=\"#b4\">[Li et al., 2018]</ref> and DPPI <ref type=\"bibr\" target=\"#b3\">[Hashem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: more importantly, even if a single experiment has detected PPI, it cannot fully interpret its types <ref type=\"bibr\" target=\"#b2\">[De Las Rivas and Fontanillo, 2010]</ref>. Evidently, we urgently need Transformer based neural network to generate proteins pretrained embedding. In the latest research, <ref type=\"bibr\" target=\"#b2\">[Yang et al., 2020]</ref> considers the correlation of PPIs and first  problems of inter-novel-protein interactions. However, In the field of Drug-drug Interaction (DDI), <ref type=\"bibr\" target=\"#b2\">[Deng et al., 2020]</ref> mentioned that the testset is divided accord. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uence of the protein.</p><p>More recent work has focused on the feature representation of proteins. <ref type=\"bibr\" target=\"#b5\">[Saha and others, 2020]</ref> proposes a novel deep multi-modal archit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Work</head><p>The primary amino acid sequences are confirmed to contain all the protein information <ref type=\"bibr\" target=\"#b0\">[Anfinsen, 1972]</ref> and are extremely easy to obtain. Thus, there i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: isease states, which in turn facilitate the therapeutic target identification and novel drug design <ref type=\"bibr\" target=\"#b6\">[Skrabanek et al., 2008]</ref>. There are many experimental methods to (ML) <ref type=\"bibr\" target=\"#b3\">[Guo et al., 2008;</ref><ref type=\"bibr\">Wong et al., 2015;</ref><ref type=\"bibr\" target=\"#b6\">Silberberg et al., 2014;</ref><ref type=\"bibr\" target=\"#b6\">Shen et al ref type=\"bibr\">Wong et al., 2015;</ref><ref type=\"bibr\" target=\"#b6\">Silberberg et al., 2014;</ref><ref type=\"bibr\" target=\"#b6\">Shen et al., 2007]</ref>. These methods provide feasible solutions, bu \"bibr\" target=\"#b3\">[Guo et al., 2008]</ref>, RF <ref type=\"bibr\">[Wong et al., 2015]</ref>, and LR <ref type=\"bibr\" target=\"#b6\">[Silberberg et al., 2014]</ref>. The input feature of the algorithms u  and hydrophobicity among amino acids, where 20 natural amino acids can be clustered into 7 classes <ref type=\"bibr\" target=\"#b6\">[Shen et al., 2007]</ref>, shown in Table <ref type=\"table\">7</ref>. F. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uence of the protein.</p><p>More recent work has focused on the feature representation of proteins. <ref type=\"bibr\" target=\"#b5\">[Saha and others, 2020]</ref> proposes a novel deep multi-modal archit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  participate in signal relay mechanisms, or jointly contribute toward specific organismal functions <ref type=\"bibr\" target=\"#b7\">[Szklarczyk et al., 2016]</ref>. It can be said that the study of PPIs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: x instead of adjacency matrix <ref type=\"bibr\" target=\"#b15\">[16]</ref> and simplified model design <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Also, there has been a growing interest in making the mod able for large graph datasets with millions of nodes. We compare the accuracy of our model with SGC <ref type=\"bibr\" target=\"#b28\">[29]</ref>, Node2Vec <ref type=\"bibr\" target=\"#b10\">[11]</ref> and SI. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ode classification <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, link prediction <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y datasets which are more likely to have nodes with different labels connected together. Zhu et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> highlight this problem and propose node's ego-embedding an s of the datasets are presented in Table <ref type=\"table\" target=\"#tab_1\">1</ref>. Homophily ratio <ref type=\"bibr\" target=\"#b34\">[35]</ref> denotes the fraction of edges which connects two nodes of  ts as the best performance of these models. GCNII <ref type=\"bibr\" target=\"#b6\">[7]</ref> and H2GCN <ref type=\"bibr\" target=\"#b34\">[35]</ref> have proposed multiple variants of their model. We have ch de classification task. Results for GCN, GAT, GraphSAGE, Cheby+JK, MixHop and H2GCN-1 are taken from<ref type=\"bibr\" target=\"#b34\">[35]</ref>. For GEOM-GCN and GCNII results are taken from the respect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y datasets which are more likely to have nodes with different labels connected together. Zhu et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> highlight this problem and propose node's ego-embedding an s of the datasets are presented in Table <ref type=\"table\" target=\"#tab_1\">1</ref>. Homophily ratio <ref type=\"bibr\" target=\"#b34\">[35]</ref> denotes the fraction of edges which connects two nodes of  ts as the best performance of these models. GCNII <ref type=\"bibr\" target=\"#b6\">[7]</ref> and H2GCN <ref type=\"bibr\" target=\"#b34\">[35]</ref> have proposed multiple variants of their model. We have ch de classification task. Results for GCN, GAT, GraphSAGE, Cheby+JK, MixHop and H2GCN-1 are taken from<ref type=\"bibr\" target=\"#b34\">[35]</ref>. For GEOM-GCN and GCNII results are taken from the respect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he prediction capabilities. Some of the techniques used in these variants include neighbor sampling <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, attention mechanism ed propagation scheme along all edges, which is sometimes not scalable for larger graphs. GraphSAGE <ref type=\"bibr\" target=\"#b11\">[12]</ref> and FastGCN <ref type=\"bibr\" target=\"#b5\">[6]</ref> introd. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, graph classification <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, prediction of mol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, link prediction <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n, Cornell, Texas <ref type=\"bibr\" target=\"#b21\">[22]</ref> represent links between webpages, Actor <ref type=\"bibr\" target=\"#b25\">[26]</ref> represent actor cooccurrence in Wikipedia pages, Chameleon. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eatures that are aggregated from the neighbors. APPNP <ref type=\"bibr\" target=\"#b15\">[16]</ref>, JK <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Geom-GCN <ref type=\"bibr\" target=\"#b21\">[22]</ref> aim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b17\">18]</ref>, natural language processing <ref type=\"bibr\" target=\"#b18\">[19]</ref>, node ranking <ref type=\"bibr\" target=\"#b19\">[20]</ref> and so on.</p><p>In this work, we focus on the node classi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata management support, OS, core, and memory system.</p><p>Compared to the closest prior-work, XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MetaSys uses a similar tagged memory-based metadata manag cate a diverse set of metadata at runtime and (ii) lightweight and low-overhead metadata management <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Even small overheads imposed as a result of the system's  agged architectures. MetaSys is inspired by the metadata management and interfaces proposed in XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref> and the large body of work on tagged memory <ref type=\"bib s table is allocated by the OS for each process and is saved in memory. In MetaSys (similar to XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cheri <ref type=\"bibr\" target=\"#b19\">[20]</ref>), we t >To associate memory address ranges with an ID, we provide the MAP/UNMAP interface (similar to XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>). MAP and UNMAP are implemented as new RISC-V instructions work</head><p>MetaSys implements a tagged-memory-based system with a metadata cache similar to XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>. MetaSys however has three major benefits. First, MetaSys  UCA systems. MetaSys can flexibly implement the range of crosslayer optimizations supported by XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>. MetaSys's dynamic interface for metadata communication en udies were conducted with our baseline 128-entry MMC with a tagging granularity of 512B (as in XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>). Fig. <ref type=\"figure\">9</ref> plots the corresponding  es with a single change to the hardware-software interface <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and consolidating common metadata management support; thus, nagement, data placement, thread scheduling, memory scheduling, data compression, and approximation <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. For QoS optimizat nagement support across multiple optimizations. Such systems were recently proposed for performance <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, memory protection. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ther applications <ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref><ref type=\"bibr\" target=\"#b53\">[54]</ref>. In this section,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>, program annotati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b26\">[27]</ref><ref type=\"bibr\" target=\"#b27\">[28]</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref> and capability-ba gged memory-based <ref type=\"bibr\" target=\"#b26\">[27]</ref><ref type=\"bibr\" target=\"#b27\">[28]</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref> implementation of gged memory-based <ref type=\"bibr\" target=\"#b26\">[27]</ref><ref type=\"bibr\" target=\"#b27\">[28]</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref> metadata manageme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: istic-based hardware prefetchers, e.g., stride <ref type=\"bibr\" target=\"#b54\">[55]</ref> and stream <ref type=\"bibr\" target=\"#b55\">[56]</ref>, or even sophisticated prefetchers that rely on repeated p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: niques were quick to implement with MetaSys, each only requiring an additional ~100 lines of Chisel <ref type=\"bibr\" target=\"#b22\">[23]</ref> code. In comparison, the hardware components of MetaSys re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ck. Existing defenses such as ExecShield <ref type=\"bibr\" target=\"#b84\">[85]</ref>   stack canaries <ref type=\"bibr\" target=\"#b85\">[86]</ref> do not protect against sophisticated attack techniques <re =\"bibr\" target=\"#b82\">[83]</ref> benchmarks: (i) the Baseline system with no overheads; (ii) canary <ref type=\"bibr\" target=\"#b85\">[86]</ref> stack protection in the GCC RISC-V compiler; and (iii) Met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b101\">102,</ref><ref type=\"bibr\" target=\"#b102\">103]</ref>. MetaSys allows communicating an applications' QoS requir. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , pointer-chasing <ref type=\"bibr\" target=\"#b45\">[46]</ref><ref type=\"bibr\" target=\"#b46\">[47]</ref><ref type=\"bibr\" target=\"#b47\">[48]</ref><ref type=\"bibr\" target=\"#b48\">[49]</ref>, linear algebra c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b55\">[56]</ref>, or even sophisticated prefetchers that rely on repeated patterns <ref type=\"bibr\" target=\"#b56\">[57]</ref><ref type=\"bibr\" target=\"#b57\">[58]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref><ref type=\"bibr\" target=\"#b53\">[54]</ref>. In this section, we demonstrate how MetaSys can be flexib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l's generalization <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for the unseen target domains in an episodic training parad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ning-to-learn) that can improve the model's generalization <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for the unseen targ with meta-learning <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>: These methods adopted the episodic training paradigm to sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> on the mixture of experts <ref type=\"bibr\" target=\"#b27\">[28]</ref> (MoE) show that MoE can improve the overall model's capabi  domains are usually totally different from source domains.</p><p>Mixture of Experts. Jacobs et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> first introduced the mixture of experts (MoE). MoE aims to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r better generalization on target domains, as mentioned in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b66\">67]</ref>. <ref type=\"bibr\" t ptimizing Domain-specific Experts</head><p>As mentioned in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b66\">67]</ref>, exploiting the com type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Recently, works <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> on the mixture of experts <ref type=\"bibr\" target=\"#b27\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>: These methods augment the source domain data to increase t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  domains. Inspired by meta-learning (learning-to-learn) that can improve the model's generalization <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta ource-trained model will be more robust to unseen target domains. (3) Optimizing with meta-learning <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uming to finetune the model on these unlabeled samples. As a result, domain generalizable (DG) ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta  are devoted to the problem of DG ReID in this paper.</p><p>Almost all the existing DG ReID methods <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta less relevant domains. However, such relevance is often not explicitly considered by existing works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta .</p><p>Very recently, researchers started to study the topic of domain generalization (DG) in ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta ets and Evaluation Settings.</head><p>Datasets and Evaluation Metrics. Following the previous works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta rce domains without using any target training data, and tests on unseen target domains. Song et al. <ref type=\"bibr\" target=\"#b44\">[45]</ref> proposed the problem of domain generalization in ReID and  ere exist two evaluation protocols for DG ReID, as shown in Tab. 1. Under the setting of Protoco1-1 <ref type=\"bibr\" target=\"#b44\">[45]</ref>, all the images in these datasets M+D+C2+C3+CS (including  etting of protocol-1 and protocol-2. We report the performances of the methods marked by \" * \" from <ref type=\"bibr\" target=\"#b44\">[45]</ref>. The best results are highlighted with bold. Comparison un. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y and encourage source domain experts to provide more complementary and discriminative information. <ref type=\"bibr\" target=\"#b2\">(3)</ref> To make the model more generalizable to target domains, we p enting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>]. To handle the problem of domain biases <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref> in ReID, researche. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: into three aspects. (1) Learning domain-invariant features <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>: These methods assu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: deep metric learning <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, part-based methods < stage, we use the meta-test D u to compute the domain loss and relation alignment loss with Eq. (3) <ref type=\"bibr\" target=\"#b6\">(7)</ref>, which is formulated as follows:</p><formula xml:id=\"formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e to provide complementary information for better generalization on target domains, as mentioned in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta ://www.tei-c.org/ns/1.0\"><head n=\"3.2.\">Optimizing Domain-specific Experts</head><p>As mentioned in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in image recognition <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, machine translation <ref type=\"bibr\" target=\"#b43\">[44]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y and encourage source domain experts to provide more complementary and discriminative information. <ref type=\"bibr\" target=\"#b2\">(3)</ref> To make the model more generalizable to target domains, we p enting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ifferent camera views. Many works on fully supervised ReID <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> have achieved quite p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alizable (DG) ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> has been appealing to researchers recently. Generally, DG R  by existing works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Recently, works <ref type=\"bibr\" target=\"#b43\">[44,< ation (DG) in ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, which learns the generalizable ReID models on multi-source the previous works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> on DG ReID, we conduct our experiments on the public ReID o ng DG ReID methods <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30</ref>] follow the same pipeline, where they collect all source dom alization <ref type=\"bibr\" target=\"#b48\">[49]</ref> to learn a more generalizable model. Jin et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposed Style Normalization and Restitution modules to di For data augmentation, we perform random cropping, random flipping, and color jittering. Similar to <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we discard random erasing (REA) because REA will degenera e evaluated on the average of 10 repeated random splits of gallery and probe sets. Under Protocol-2 <ref type=\"bibr\" target=\"#b29\">[30]</ref>, all the images in M+D+C3+MT (including the training and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: part-based methods <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, and attention netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eID models to other domains, the performance often drops significantly because of the domain biases <ref type=\"bibr\" target=\"#b51\">[52]</ref>. To tackle this problem, some researchers have studied uns <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CUHK03 <ref type=\"bibr\" target=\"#b35\">[36]</ref>, MSMT17 <ref type=\"bibr\" target=\"#b51\">[52]</ref>, Table <ref type=\"table\">1</ref>. Different evaluation pro t=\"#b58\">59</ref>]. To handle the problem of domain biases <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref> in ReID, researchers proposed unsupervised domain adaptatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alizable (DG) ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> has been appealing to researchers recently. Generally, DG R  by existing works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Recently, works <ref type=\"bibr\" target=\"#b43\">[44,< ation (DG) in ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, which learns the generalizable ReID models on multi-source the previous works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> on DG ReID, we conduct our experiments on the public ReID o ng DG ReID methods <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30</ref>] follow the same pipeline, where they collect all source dom alization <ref type=\"bibr\" target=\"#b48\">[49]</ref> to learn a more generalizable model. Jin et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposed Style Normalization and Restitution modules to di For data augmentation, we perform random cropping, random flipping, and color jittering. Similar to <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we discard random erasing (REA) because REA will degenera e evaluated on the average of 10 repeated random splits of gallery and probe sets. Under Protocol-2 <ref type=\"bibr\" target=\"#b29\">[30]</ref>, all the images in M+D+C3+MT (including the training and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b60\">61]</ref>. ) and meta-train (e.g., D2, ..., DK ). A meta-test image c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ID <ref type=\"bibr\" target=\"#b25\">[26]</ref>, GRID <ref type=\"bibr\" target=\"#b36\">[37]</ref>, VIPeR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and iLIDs <ref type=\"bibr\" target=\"#b52\">[53]</ref>.</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ication loss L cls , triplet loss <ref type=\"bibr\" target=\"#b24\">[25]</ref> L tri , and center loss <ref type=\"bibr\" target=\"#b53\">[54]</ref> L cent to optimize K domain-specific experts {M \u03c6 k } K k= f D s , C s is the prototypes set of D s , and \u03b8 is the parameter of the voting network. Similar to <ref type=\"bibr\" target=\"#b53\">[54]</ref>, prototypes can be updated with the center loss in Eq. (1). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oE methods have shown their superiority in image recognition <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, machine translatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rn domain-invariant features which are robust for unseen target domains. (2) Augmenting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[37]</ref>, VIPeR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and iLIDs <ref type=\"bibr\" target=\"#b52\">[53]</ref>.</p><p>For CUHK03, we use the \"labelled\" dataset for train. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in image recognition <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, machine translation <ref type=\"bibr\" target=\"#b43\">[44]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rson-Search datasets, including Market1501 <ref type=\"bibr\" target=\"#b61\">[62]</ref>, DukeMTMC-reID <ref type=\"bibr\" target=\"#b62\">[63]</ref>, CUHK02 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CUHK03 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: decorrelation loss to decorrelate all these domain experts' features.</p><p>Metric Loss. Similar to <ref type=\"bibr\" target=\"#b37\">[38]</ref>, we use the classification loss L cls , triplet loss <ref  sNet50 <ref type=\"bibr\" target=\"#b23\">[24]</ref> pretrained on ImageNet as our backbone. Similar to <ref type=\"bibr\" target=\"#b37\">[38]</ref>, the last residual layer's stride size is set as 1. After  ref>, we discard random erasing (REA) because REA will degenerate the cross-domain ReID performance <ref type=\"bibr\" target=\"#b37\">[38]</ref>. The batch size is set to 64, including 16 identities and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  have studied unsupervised domain adaptation (UDA) methods <ref type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" tar hers proposed unsupervised domain adaptation (UDA) methods <ref type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, which utilize the unl get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ication loss L cls , triplet loss <ref type=\"bibr\" target=\"#b24\">[25]</ref> L tri , and center loss <ref type=\"bibr\" target=\"#b53\">[54]</ref> L cent to optimize K domain-specific experts {M \u03c6 k } K k= f D s , C s is the prototypes set of D s , and \u03b8 is the parameter of the voting network. Similar to <ref type=\"bibr\" target=\"#b53\">[54]</ref>, prototypes can be updated with the center loss in Eq. (1). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alizable (DG) ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> has been appealing to researchers recently. Generally, DG R  by existing works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Recently, works <ref type=\"bibr\" target=\"#b43\">[44,< ation (DG) in ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, which learns the generalizable ReID models on multi-source the previous works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> on DG ReID, we conduct our experiments on the public ReID o ng DG ReID methods <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30</ref>] follow the same pipeline, where they collect all source dom alization <ref type=\"bibr\" target=\"#b48\">[49]</ref> to learn a more generalizable model. Jin et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposed Style Normalization and Restitution modules to di For data augmentation, we perform random cropping, random flipping, and color jittering. Similar to <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we discard random erasing (REA) because REA will degenera e evaluated on the average of 10 repeated random splits of gallery and probe sets. Under Protocol-2 <ref type=\"bibr\" target=\"#b29\">[30]</ref>, all the images in M+D+C3+MT (including the training and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y and encourage source domain experts to provide more complementary and discriminative information. <ref type=\"bibr\" target=\"#b2\">(3)</ref> To make the model more generalizable to target domains, we p enting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: into three aspects. (1) Learning domain-invariant features <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>: These methods assu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">Implementation Details</head><p>We use ResNet50 <ref type=\"bibr\" target=\"#b23\">[24]</ref> pretrained on ImageNet as our backbone. Similar to <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b60\">61]</ref>. ) and meta-train (e.g., D2, ..., DK ). A meta-test image c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b62\">[63]</ref>, CUHK02 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CUHK03 <ref type=\"bibr\" target=\"#b35\">[36]</ref>, MSMT17 <ref type=\"bibr\" target=\"#b51\">[52]</ref>, Table <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y and encourage source domain experts to provide more complementary and discriminative information. <ref type=\"bibr\" target=\"#b2\">(3)</ref> To make the model more generalizable to target domains, we p enting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ication loss L cls , triplet loss <ref type=\"bibr\" target=\"#b24\">[25]</ref> L tri , and center loss <ref type=\"bibr\" target=\"#b53\">[54]</ref> L cent to optimize K domain-specific experts {M \u03c6 k } K k= f D s , C s is the prototypes set of D s , and \u03b8 is the parameter of the voting network. Similar to <ref type=\"bibr\" target=\"#b53\">[54]</ref>, prototypes can be updated with the center loss in Eq. (1). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ze the relevance between 0 and 1. Softmax-triplet function <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> has been shown to be a powerful tool to measure the metric . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe et=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe ion (e.g., sigmoid or softmax) to normalize the relevance between 0 and 1. Softmax-triplet function <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> has been shown to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b46\">47]</ref>, part-based methods <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b61\">[62]</ref>, DukeMTMC-reID <ref type=\"bibr\" target=\"#b62\">[63]</ref>, CUHK02 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CUHK03 <ref type=\"bibr\" target=\"#b35\">[36]</ref>, MSMT17 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe et=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe ion (e.g., sigmoid or softmax) to normalize the relevance between 0 and 1. Softmax-triplet function <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> has been shown to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eID models to other domains, the performance often drops significantly because of the domain biases <ref type=\"bibr\" target=\"#b51\">[52]</ref>. To tackle this problem, some researchers have studied uns <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CUHK03 <ref type=\"bibr\" target=\"#b35\">[36]</ref>, MSMT17 <ref type=\"bibr\" target=\"#b51\">[52]</ref>, Table <ref type=\"table\">1</ref>. Different evaluation pro t=\"#b58\">59</ref>]. To handle the problem of domain biases <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref> in ReID, researchers proposed unsupervised domain adaptatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bust for unseen target domains. (2) Augmenting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ifferent camera views. Many works on fully supervised ReID <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> have achieved quite p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>.</p><p>For CUHK03, we use the \"labelled\" dataset for training and adopt the protocol used in <ref type=\"bibr\" target=\"#b63\">[64]</ref> for testing. For simplicity, in the next sections we denot. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ID <ref type=\"bibr\" target=\"#b25\">[26]</ref>, GRID <ref type=\"bibr\" target=\"#b36\">[37]</ref>, VIPeR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and iLIDs <ref type=\"bibr\" target=\"#b52\">[53]</ref>.</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oE methods have shown their superiority in image recognition <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, machine translatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ifferent camera views. Many works on fully supervised ReID <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> have achieved quite p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: deep metric learning <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, part-based methods < stage, we use the meta-test D u to compute the domain loss and relation alignment loss with Eq. (3) <ref type=\"bibr\" target=\"#b6\">(7)</ref>, which is formulated as follows:</p><formula xml:id=\"formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion (UDA) methods <ref type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar tion (UDA) methods <ref type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: part-based methods <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, and attention netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: into three aspects. (1) Learning domain-invariant features <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>: These methods assu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eID, we conduct our experiments on the public ReID or Pearson-Search datasets, including Market1501 <ref type=\"bibr\" target=\"#b61\">[62]</ref>, DukeMTMC-reID <ref type=\"bibr\" target=\"#b62\">[63]</ref>, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eID, we conduct our experiments on the public ReID or Pearson-Search datasets, including Market1501 <ref type=\"bibr\" target=\"#b61\">[62]</ref>, DukeMTMC-reID <ref type=\"bibr\" target=\"#b62\">[63]</ref>, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ication loss L cls , triplet loss <ref type=\"bibr\" target=\"#b24\">[25]</ref> L tri , and center loss <ref type=\"bibr\" target=\"#b53\">[54]</ref> L cent to optimize K domain-specific experts {M \u03c6 k } K k= f D s , C s is the prototypes set of D s , and \u03b8 is the parameter of the voting network. Similar to <ref type=\"bibr\" target=\"#b53\">[54]</ref>, prototypes can be updated with the center loss in Eq. (1). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion (UDA) methods <ref type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar tion (UDA) methods <ref type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, which utilize the unl get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rson-Search datasets, including Market1501 <ref type=\"bibr\" target=\"#b61\">[62]</ref>, DukeMTMC-reID <ref type=\"bibr\" target=\"#b62\">[63]</ref>, CUHK02 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CUHK03 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: deep metric learning <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, part-based methods < stage, we use the meta-test D u to compute the domain loss and relation alignment loss with Eq. (3) <ref type=\"bibr\" target=\"#b6\">(7)</ref>, which is formulated as follows:</p><formula xml:id=\"formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ID <ref type=\"bibr\" target=\"#b25\">[26]</ref>, GRID <ref type=\"bibr\" target=\"#b36\">[37]</ref>, VIPeR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and iLIDs <ref type=\"bibr\" target=\"#b52\">[53]</ref>.</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o unseen target domains. (3) Optimizing with meta-learning <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>: These methods adop. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oth source side and target side. The base architecture of mCOLT is the state-of-the-art Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al. (2017b)</ref>). A little different from previous work. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b18\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Misra and Maaten, 2020)</ref>. Researchers in the NLP domain have als. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b17\">Ha et al., 2017;</ref><ref type=\"bibr\" target=\"#b16\">Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Currey and Heafield, 2019)</ref>. <ref type=\"bibr\" target=\"#b20\">John. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b17\">(Ha et al., 2017;</ref><ref type=\"bibr\" target=\"#b16\">Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Ji et al., 2020)</ref>. Despite these benefits, challenges still rema t=\"#b37\">Zhang et al. (2020)</ref> improved the zero-shot translation with online back translation. <ref type=\"bibr\" target=\"#b19\">Ji et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b23\">Liu et al. (20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad><p>While initial research on NMT starts with building translation systems between two languages, <ref type=\"bibr\" target=\"#b12\">Dong et al. (2015)</ref> extends the bilingual NMT to one-to-many tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ot translation with online back translation. <ref type=\"bibr\" target=\"#b19\">Ji et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b23\">Liu et al. (2020)</ref> shows that large scale monolingual data can i EU score after removing Romanian dialects. (**) BLEU scores for Transformer and mBART are cited from<ref type=\"bibr\" target=\"#b23\">(Liu et al., 2020)</ref> gual data. The total number of sentences in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o AA .We divide the experiments into two scenarios: First we evaluate our method on Tatoeba dataset <ref type=\"bibr\" target=\"#b3\">(Artetxe and Schwenk (2019)</ref>), which is English-centric. Then we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b17\">Ha et al., 2017;</ref><ref type=\"bibr\" target=\"#b16\">Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Currey and Heafield, 2019)</ref>. <ref type=\"bibr\" target=\"#b20\">John. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ot translation with online back translation. <ref type=\"bibr\" target=\"#b19\">Ji et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b23\">Liu et al. (2020)</ref> shows that large scale monolingual data can i EU score after removing Romanian dialects. (**) BLEU scores for Transformer and mBART are cited from<ref type=\"bibr\" target=\"#b23\">(Liu et al., 2020)</ref> gual data. The total number of sentences in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o AA .We divide the experiments into two scenarios: First we evaluate our method on Tatoeba dataset <ref type=\"bibr\" target=\"#b3\">(Artetxe and Schwenk (2019)</ref>), which is English-centric. Then we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o AA .We divide the experiments into two scenarios: First we evaluate our method on Tatoeba dataset <ref type=\"bibr\" target=\"#b3\">(Artetxe and Schwenk (2019)</ref>), which is English-centric. Then we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b17\">Ha et al., 2017;</ref><ref type=\"bibr\" target=\"#b16\">Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Currey and Heafield, 2019)</ref>. <ref type=\"bibr\" target=\"#b20\">John. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Hence, there has been a massive increase in work on MT systems that involve more than two languages <ref type=\"bibr\" target=\"#b7\">(Chen et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Choi et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  multilingual machine translation aims to create a single unified model to translate many languages <ref type=\"bibr\" target=\"#b20\">(Johnson et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Aharoni et  tive for any language pairs, while most previous work focus on improving English-centric directions <ref type=\"bibr\" target=\"#b20\">(Johnson et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Aharoni et  n models are appealing for two reasons. First, they are model efficient, enabling easier deployment <ref type=\"bibr\" target=\"#b20\">(Johnson et al., 2017)</ref>. Further, parameter sharing across diffe arget=\"#b16\">Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Currey and Heafield, 2019)</ref>. <ref type=\"bibr\" target=\"#b20\">Johnson et al. (2017)</ref> shows that a multilingual NMT system enab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p model, we apply Layer Normalization for word embedding and pre-norm residual connection following <ref type=\"bibr\" target=\"#b34\">Wang et al. (2019a)</ref> for both encoder and decoder. Therefore, ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p model, we apply Layer Normalization for word embedding and pre-norm residual connection following <ref type=\"bibr\" target=\"#b34\">Wang et al. (2019a)</ref> for both encoder and decoder. Therefore, ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b38\">(Zhuang et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b18\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sh-centric. Then we conduct similar similarity search task on non-English language pairs. Following <ref type=\"bibr\" target=\"#b31\">Tran et al. (2020)</ref>, we construct a multi-way parallel testset (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b4\">(Bapna et al., 2019;</ref><ref type=\"bibr\" target=\"#b21\">Kim et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Wang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b13\">Escolano et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b18\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Misra and Maaten, 2020)</ref>. Researchers in the NLP domain have als. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b17\">(Ha et al., 2017;</ref><ref type=\"bibr\" target=\"#b16\">Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Ji et al., 2020)</ref>. Despite these benefits, challenges still rema t=\"#b37\">Zhang et al. (2020)</ref> improved the zero-shot translation with online back translation. <ref type=\"bibr\" target=\"#b19\">Ji et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b23\">Liu et al. (20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rsation and recommendation for developing an effective CRS <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><p>Different pr by reinforcement learning (RL) methods for policy learning <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. As shown in Figure gue actions. (4) Multi-round Conversational Recommendation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Under this problem model. In order to reduce the action space in policy learning, another state-of-the-art method SCPR <ref type=\"bibr\" target=\"#b13\">[14]</ref> only considers learning the policy of when to ask or recom  <ref type=\"bibr\" target=\"#b25\">[26]</ref>, EAR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and SCPR <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>domains or applications, since there are three diff hough the connectivity of the graph can also be used to eliminate invalid actions by path reasoning <ref type=\"bibr\" target=\"#b13\">[14]</ref>, there are still a large number of candidates left for act  target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> and (ii) when to ask or recommend <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>In order to simplify the overall framework of MCR w  user-specified attribute \ud835\udc5d 0 , i.e., cand to ask attributes. Following the path reasoning approach <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we have</p><formula xml:id=\"formula_3\">\ud835\udc60 0 = [[{\ud835\udc5d 0 }, {} d><p>A large action search space will harm the performance of the policy learning to a great extent <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Thus, it attaches great importance to handle the overwhel tter eliminate the uncertainty of candidate items, but also encode the user preference. Inspired by <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we adopt weighted entropy as the criteria to prune candid d build a 2-layer taxonomy with 29 first-layer categories for Yelp. \u2022 LastFM* and Yelp*. Lei et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> consider that it is not realistic to manually merge attrib een the conversation and recommendation components with a similar RL framework as CRM.</p><p>\u2022 SCPR <ref type=\"bibr\" target=\"#b13\">[14]</ref>. This is the state-of-the-art method on MCR setting, which <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2.4\">Implementation Details.</head><p>Following <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we split the E-Commerce dataset by 7:1.5:1.5 for training CRM<ref type=\"bibr\" target=\"#b25\">[26]</ref>, EAR<ref type=\"bibr\" target=\"#b11\">[12]</ref>, and SCPR<ref type=\"bibr\" target=\"#b13\">[14]</ref>.</figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns res extra efforts to train an offline recommendation model <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> or pretrain the policy network with synthetic dialogue hist e multi-round conversational recommendation (MCR) scenario <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, the most realistic conversational recommendation setting p  updated by P 4.1.4 Reward. Following previous MCR studies <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, our environment contains five kinds of rewards, namely, (1 tion history in the current state. Unlike previous studies <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> that adopt heuristic features for conversation history mode evious studies on multiround conversational recommendation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, we adopt success rate at the turn \ud835\udc61 (SR@\ud835\udc61) <ref type=\"bibr isodes. To maintain a fair comparison with other baselines <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, we adopt the same reward settings to train the proposed mo many successful applications of graphbased RL methods on different scenarios of recommender systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and Zhou et al. <ref type=\"bibr\" target=\"#b48\">[49]</ref> employ graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> for state representation learning to enhance the performanc s, and attributes from the connectivity of the graph, we employ a graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b9\">[10]</ref> to refine the node representations with structural and rela ent (\ud835\udc5d &lt; 0.01) over all baselines. hDCG stands for hDCG@<ref type=\"bibr\" target=\"#b14\">(15,</ref><ref type=\"bibr\" target=\"#b9\">10)</ref>. SR and hDCG are the higher the better, while AT is the lowe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b44\">45]</ref>. Recent works on sequential recommendation <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and interactive recommendation <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd a recommendation agent, and employs RL algorithms to learn the optimal recommendation strategies <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> or interactive recommender systems (IRS) <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, which mainly focu br\" target=\"#b36\">37]</ref> and interactive recommendation <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> adopt RL to capture get=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. For example, Xian et al. <ref type=\"bibr\" target=\"#b35\">[3 ibr\" target=\"#b41\">[42]</ref>. Lei et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> and Zhou et al. <ref type=\"bibr\" target=\"#b48\">[49]</ref> employ graph convolutional network (GCN) <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fying question\". (3) Dialogue Understanding and Generation <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. For example, Xian  mmendation, which is enhanced with adversarial actor-critic for demonstration-guided path reasoning <ref type=\"bibr\" target=\"#b41\">[42]</ref>. Lei et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: has the maximum expected reward achievable by the optimal policy \ud835\udf0b * , follows the Bellman equation <ref type=\"bibr\" target=\"#b0\">[1]</ref> as: Then, the agent will receive the reward \ud835\udc5f \ud835\udc61 from the use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  algorithms to learn the optimal recommendation strategies <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: has the maximum expected reward achievable by the optimal policy \ud835\udf0b * , follows the Bellman equation <ref type=\"bibr\" target=\"#b0\">[1]</ref> as: Then, the agent will receive the reward \ud835\udc5f \ud835\udc61 from the use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tive conversations <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Since it has the natural advantage of explicitly acquiring tance of interactivity of asking clarifying questions in CRS <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. More importantly,  nal recommendation scenarios. (2) Question-driven Approaches <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> aim at asking quest randomly chosen from P \ud835\udc63 . Then the session follows the process of \"System Ask, User</p><p>Respond\" <ref type=\"bibr\" target=\"#b40\">[41]</ref> as described in Section 4.</p></div> <div xmlns=\"http://ww. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: training makes the model easily overfit. These problems make the transfer-based methods inefficient <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, the ada r module was proposed for parameter-efficient fine-tuning in multilingual or cross-lingual settings <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, which can mitiga stigate the performance of multiple adapters on cross-lingual ASR tasks.</p><p>In our previous work <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we proposed MetaAdapter to learn general and transferable s tive improvement. This paper is substantially an extended version of our previously published paper <ref type=\"bibr\" target=\"#b8\">[9]</ref> at ICASSP 2021. Compared to the previous version, we make he  to find a proper initialization for rapid adaptation have also been explored for cross-lingual ASR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Hsu et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> proposed ages to learn language-agnostic information in the multilingual data. On the other hand, Hou et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> investigates the possibility of applying adapters to cross-l lingual transfer learning tasks as shown in previous works <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D. e=\"bibr\" target=\"#b42\">[43]</ref> for fast adaptation to the new target tasks. In our previous work <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we investigated two meta-learning algorithms: Model-Agnosti pter-based ASR cross-lingual adaptation.</p><p>In the first stage, different from the previous work <ref type=\"bibr\" target=\"#b8\">[9]</ref>, SimAdapter trains the language-specific heads for each sour. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  language identifier. Some attempts take a step towards realizing language-universal ASR. Li et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> proposed to replace the characters with the Unicode bytes . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to learn general and transferable speech representations using model-agnostic meta-learning (MAML) <ref type=\"bibr\" target=\"#b12\">[13]</ref> and achieved promising results on extremely low-resource l <p>Besides learning the language-agnostic features, the optimization-based meta-learning approaches <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref> that aim to fin . <ref type=\"bibr\" target=\"#b7\">[8]</ref> proposed to apply the model-agnostic meta-learning (MAML) <ref type=\"bibr\" target=\"#b12\">[13]</ref> as the pre-training method and achieved significant improv =\"#b8\">[9]</ref>, we investigated two meta-learning algorithms: Model-Agnostic Meta-Learning (MAML) <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Reptile <ref type=\"bibr\" target=\"#b25\">[26]</ref>. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Data Set</head><p>We adopt the Common Voice 5.1 <ref type=\"bibr\" target=\"#b43\">[44]</ref> corpus for our experiments. We follow the official data sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: asing attention over the years that handles multiple languages with a single model. Watanabe et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposed a language-independent architecture based on hybr y training on 16,000 hours speech data of 51 languages with up to 1 billion parameters. Inspired by <ref type=\"bibr\" target=\"#b16\">[17]</ref>, Hou et al. presented LID-42 <ref type=\"bibr\" target=\"#b3\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g/ns/1.0\"><head>C. Implementation Details</head><p>We implement the E2E methods based on the ESPnet <ref type=\"bibr\" target=\"#b48\">[49]</ref> codebase. The subword-based LID-42 model proposed in <ref   training. The weight of the CTC module \u03bb is set to 0.3 throughout the experiments following ESPnet <ref type=\"bibr\" target=\"#b48\">[49]</ref>. Beam size 10 is employed for joint decoding.</p></div> <d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ty of parameters contained in the Transformer-based models <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>- <ref type=\"bibr\" target=\"#b29\">[30]</ref>, recent literat type=\"bibr\" target=\"#b31\">[32]</ref> for parameter-efficient adaptation of pre-trained Transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> on various down tion <ref type=\"bibr\" target=\"#b36\">[37]</ref> of large-scale pre-trained language models like BERT <ref type=\"bibr\" target=\"#b27\">[28]</ref> and XLM <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Li et a o-sequence ASR task while they experiment on text classification tasks based on the pretrained BERT <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p><p>Some researchers have proposed to apply the Adapter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>- <ref type=\"bibr\" target=\"#b29\">[30]</ref>, recent literature proposed the Adapter structure <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref> for parameter-e sformer model, fine-tuning the adapters is significantly efficient with acceptable performance loss <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Therefore, adapters have been adopted as a fine-tuning te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s made remarkable progress by training on large-scale data <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We can use a single E2E ASR system for a large number of la  target=\"#b0\">(1)</ref> We propose a parallel new algorithm called SimAdapter for crosslingual ASR. <ref type=\"bibr\" target=\"#b1\">(2)</ref> We investigate the difference and integration between the Me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ltilingual acoustic Transformer model trained on 11 mixed corpora of 42 languages.</p><p>Cho et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> validated the effectiveness of cross-lingual transfer lear. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ty of parameters contained in the Transformer-based models <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>- <ref type=\"bibr\" target=\"#b29\">[30]</ref>, recent literat type=\"bibr\" target=\"#b31\">[32]</ref> for parameter-efficient adaptation of pre-trained Transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> on various down tion <ref type=\"bibr\" target=\"#b36\">[37]</ref> of large-scale pre-trained language models like BERT <ref type=\"bibr\" target=\"#b27\">[28]</ref> and XLM <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Li et a o-sequence ASR task while they experiment on text classification tasks based on the pretrained BERT <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p><p>Some researchers have proposed to apply the Adapter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hese are distinct from the generalized experience of stress.</p><p>The most similar work to ours is <ref type=\"bibr\" target=\"#b37\">Turcan and McKeown (2019)</ref>, our prior work publishing a dataset  \">Baselines</head><p>We present a re-implementation of the same BERTbased fine-tuning model used in <ref type=\"bibr\" target=\"#b37\">Turcan and McKeown (2019)</ref>, where this model performed best on D selected in the top 10 LIME explanations. Dreaddit is Dr, and GoEmotions is GE.</p><p>(which echoes <ref type=\"bibr\" target=\"#b37\">Turcan and McKeown (2019)</ref>'s finding that stressful data is typi a is labeled stress, with the remaining 47.4% labeled nonstress. We use the train-dev-test split of <ref type=\"bibr\" target=\"#b37\">Turcan and McKeown (2019)</ref> into 2,562 train, 276 development, an ble <ref type=\"table\" target=\"#tab_0\">1</ref>. The primary dataset we use for this work is Dreaddit <ref type=\"bibr\" target=\"#b37\">(Turcan and McKeown, 2019)</ref>, a dataset of 3,553 segments of Redd. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., <ref type=\"bibr\" target=\"#b10\">Felbo et al. (2017)</ref>, who first fine-tuned on emoji detection an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  stress has been shown to interact with emotion <ref type=\"bibr\" target=\"#b23\">(Lazarus, 2006;</ref><ref type=\"bibr\" target=\"#b36\">Thoern et al., 2016;</ref><ref type=\"bibr\" target=\"#b24\">Levenson, 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the language model itself on language from the target domain has been shown to improve performance <ref type=\"bibr\" target=\"#b17\">(Howard and Ruder, 2018;</ref><ref type=\"bibr\" target=\"#b3\">Chakrabar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs in a model are shared between or used to inform multiple different tasks. Hard parameter sharing <ref type=\"bibr\" target=\"#b2\">(Caruana, 1993)</ref>, the variant we employ, uses some set of paramet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: access to some physiological signals (e.g., <ref type=\"bibr\" target=\"#b42\">Zuo et al. (2012)</ref>; <ref type=\"bibr\" target=\"#b1\">Allen et al. (2014)</ref>; <ref type=\"bibr\" target=\"#b0\">Al-Shargie et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  stress has been shown to interact with emotion <ref type=\"bibr\" target=\"#b23\">(Lazarus, 2006;</ref><ref type=\"bibr\" target=\"#b36\">Thoern et al., 2016;</ref><ref type=\"bibr\" target=\"#b24\">Levenson, 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs in a model are shared between or used to inform multiple different tasks. Hard parameter sharing <ref type=\"bibr\" target=\"#b2\">(Caruana, 1993)</ref>, the variant we employ, uses some set of paramet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b17\">(Howard and Ruder, 2018;</ref><ref type=\"bibr\" target=\"#b3\">Chakrabarty et al., 2019;</ref><ref type=\"bibr\" target=\"#b15\">Gururangan et al., 2020)</ref> (also note <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: experiments, so we use a single template. The inputs and outputs are modeled in the standard format <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>.</p><p>We fine-tune pretrained models to  place=\"foot\" n=\"2\" xml:id=\"foot_2\">Since the original pretraining corpus is not available, we follow<ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> and recreate the dataset by crawling http:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"bibr\" target=\"#b1\">Bansal et al., 2014;</ref><ref type=\"bibr\" target=\"#b20\">Mao et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Shang et al., 2020)</ref>. In this work, we propose an approach that  ith between 125 and 452 terms. Following <ref type=\"bibr\" target=\"#b20\">Mao et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b31\">Shang et al. (2020)</ref>, we used the medium-sized trees from <ref t >Bansal et al. (2014)</ref>   <ref type=\"bibr\" target=\"#b20\">Mao et al. (2018)</ref> 37.9 37.9 37.9 <ref type=\"bibr\" target=\"#b31\">Shang et al. (2020)</ref>  rate siblinghood information. <ref type=\"b ataset, which contain hundreds of terms <ref type=\"bibr\" target=\"#b6\">(Bordea et al., 2016b)</ref>. <ref type=\"bibr\" target=\"#b31\">Shang et al. (2020)</ref> apply graph neural networks and show that t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  larger taxonomies from the SemEval-2016 Task 13 benchmark dataset, which contain hundreds of terms <ref type=\"bibr\" target=\"#b6\">(Bordea et al., 2016b)</ref>. <ref type=\"bibr\" target=\"#b31\">Shang et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nually curated taxonomies provide useful information, they are incomplete and expensive to maintain <ref type=\"bibr\" target=\"#b14\">(Hovy et al., 2009)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Hearst, 1992)</ref> to large corpora, and use corpus statistics to construct taxonomic trees (e.g., <ref type=\"bibr\" target=\"#b32\">Snow et al., 2005;</ref><ref type=\"bibr\" target=\"#b17\">Kozareva and H infer hypernym relations from large corpora (e.g. <ref type=\"bibr\" target=\"#b13\">Hearst, 1992;</ref><ref type=\"bibr\" target=\"#b32\">Snow et al., 2005;</ref><ref type=\"bibr\" target=\"#b17\">Kozareva and H Snow et al., 2005;</ref><ref type=\"bibr\" target=\"#b17\">Kozareva and Hovy, 2010)</ref>. For example, <ref type=\"bibr\" target=\"#b32\">Snow et al. (2005)</ref> propose a system that extracts pattern-based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: guages because of the additional resources devoted to the development of English models. (See e.g., <ref type=\"bibr\" target=\"#b2\">Bender, 2011;</ref><ref type=\"bibr\" target=\"#b21\">Mielke, 2016;</ref><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d n=\"3.3\">Models</head><p>In our experiments, we use pretrained models from the Huggingface library <ref type=\"bibr\" target=\"#b37\">(Wolf et al., 2019)</ref>. For the English dataset we experiment with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hypernymy detection tasks, pattern-based approaches outperform those based on distributional models <ref type=\"bibr\" target=\"#b29\">(Roller et al., 2018)</ref>. Subsequent work pointed out the sparsity. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s the analog of MST for directed graphs, and finds the highest scoring arborescence in O(n 2 ) time <ref type=\"bibr\" target=\"#b10\">(Chu, 1965)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: commonsense knowledge. Other work has focused on extracting knowledge of relations between entities <ref type=\"bibr\" target=\"#b26\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b15\">Jiang et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 30\">(Sagot, 2008)</ref>, Italian <ref type=\"bibr\" target=\"#b19\">(Magnini et al., 1994)</ref>, Dutch <ref type=\"bibr\" target=\"#b28\">(Postma et al., 2016)</ref>, Polish <ref type=\"bibr\" target=\"#b27\">(P. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: solution of only 96 \u00d7 96.</p><p>The framework of Vector Quantized Variational AutoEncoders (VQ-VAE) <ref type=\"bibr\" target=\"#b45\">[46]</ref> alleviates this problem. VQ-VAE trains an encoder to compr ns.</p><p>The image tokenizer is a discrete Auto-Encoder, which is similar to the stage 1 of VQ-VAE <ref type=\"bibr\" target=\"#b45\">[46]</ref> or d-VAE <ref type=\"bibr\" target=\"#b38\">[39]</ref>. More s e codebook is updated periodically during training as the mean of the vectors recently mapped to it <ref type=\"bibr\" target=\"#b45\">[46]</ref>. \u2022 The nearest-neighbor mapping, fixed codebook, where the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dels, when Mansimov et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> added text information to DRAW <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Then Generative Adversarial Nets <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b53\">[54]</ref> decomposed the generation into a sketch-refinement process. AttnGAN <ref type=\"bibr\" target=\"#b50\">[51]</ref> used attention on words to focus on the corresponding subr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b32\">[33]</ref>. Auto-regressive model is not nascent in CV. PixelCNN, PixelRNN <ref type=\"bibr\" target=\"#b46\">[47]</ref> and Image Transformer <ref type=\"bibr\" target=\"#b35\">[36]< the hidden variable in the stage 1. Then in the stage 2, an auto-regressive model (such as PixelCNN <ref type=\"bibr\" target=\"#b46\">[47]</ref>) learns to fit the prior of hidden variables. This discret. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b32\">[33]</ref>. Auto-regressive model is not nascent in CV. PixelCNN, PixelRNN <ref type=\"bibr\" target=\"#b46\">[47]</ref> and Image Transformer <ref type=\"bibr\" target=\"#b35\">[36]< the hidden variable in the stage 1. Then in the stage 2, an auto-regressive model (such as PixelCNN <ref type=\"bibr\" target=\"#b46\">[47]</ref>) learns to fit the prior of hidden variables. This discret. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1\">[42]</ref> fed the text embeddings to both generator and discriminator as extra inputs. StackGAN <ref type=\"bibr\" target=\"#b53\">[54]</ref> decomposed the generation into a sketch-refinement process. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ity and accuracy for text-image generation at a finer granularity than FID and Inception Score (IS) <ref type=\"bibr\" target=\"#b42\">[43]</ref>.</p><p>\u2022 We proposed PB-relaxation and Sandwich-LN to stab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rg/ns/1.0\"><head n=\"1\">Introduction</head><p>Self attention mechanisms, represented by Transformers <ref type=\"bibr\" target=\"#b0\">[1]</ref>, have driven the advancement of various machine learning pro ref type=\"bibr\" target=\"#b29\">[30]</ref>, and follow a standard warmup learning rate schedule as in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We use an initial learning rate of 3 \u00d7 10 \u22123 a weight decay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  attention\" works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref>. The difference i nearized attention <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, which is formulated as</p><formula xml:id=\"formula_8\">Y t  g the dot product. <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose to approximate the exponential kernel with inner pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atterns to attend <ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" t n with different Transformers: Reformer <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Linear Transformer <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Performer <ref type=\"bibr\" target=\"#b12\">[13]</ref> (only r <ref type=\"bibr\" target=\"#b11\">[12]</ref> (its best performing dense version), Linear Transformer <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Performer <ref type=\"bibr\" target=\"#b12\">[13]</ref> no  target=\"#b7\">[8]</ref>, Synthesizer <ref type=\"bibr\" target=\"#b11\">[12]</ref> , Linear Transformer <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Performer <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</ e rearranged computational ordering of Q, K, V is also found in recent \"linearized attention\" works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" t lified to element-wise operations and global pooling. AFT-simple is similar to linearized attention <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta t transformers, see <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>Approximating the dot product. <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xt compression.</head><p>Other approaches try to learn context patterns. Adaptive-Span Transformers <ref type=\"bibr\" target=\"#b22\">[23]</ref> learn a range for each attention head within which to atte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xt compression.</head><p>Other approaches try to learn context patterns. Adaptive-Span Transformers <ref type=\"bibr\" target=\"#b22\">[23]</ref> learn a range for each attention head within which to atte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atterns to attend <ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref>. The difference is that AFT combines k and v in an element  h=3) and \"small\" (L=12, d=384, h=6) configurations, respectively. We also consider Lambda Networks <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which is closely related to the linearized attention line ter linear implementation). We also apply BatchNorm to the query, key projections as recommended by <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Our result is shown in Table <ref type=\"table\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" t n with different Transformers: Reformer <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Linear Transformer <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Performer <ref type=\"bibr\" target=\"#b12\">[13]</ref> (only r <ref type=\"bibr\" target=\"#b11\">[12]</ref> (its best performing dense version), Linear Transformer <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Performer <ref type=\"bibr\" target=\"#b12\">[13]</ref> no  target=\"#b7\">[8]</ref>, Synthesizer <ref type=\"bibr\" target=\"#b11\">[12]</ref> , Linear Transformer <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Performer <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</ e rearranged computational ordering of Q, K, V is also found in recent \"linearized attention\" works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" t lified to element-wise operations and global pooling. AFT-simple is similar to linearized attention <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta t transformers, see <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>Approximating the dot product. <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  attention\" works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref>. The difference i nearized attention <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, which is formulated as</p><formula xml:id=\"formula_8\">Y t  g the dot product. <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose to approximate the exponential kernel with inner pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atterns to attend <ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: node aggregates the representations of its neighbors and combines them with its own representation. <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> pioneered the use of attentionbased ne ms (Definitions 3.1 and 3.2), and derive our claims theoretically (Theorem 1) from the equations of <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref>. Empirically, we use a synthetic probl /head><p>Although the scoring function e can be defined in various ways, the original definition of <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> (Equation ( <ref type=\"formula\" target here different curves denote different queries (h i ).</p><p>Generalization to multi-head attention <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> found it beneficial to employ H separa O (|V|dd + |E|d ). However, by merging its linear layers, GAT can be computed faster than stated by <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref>. For a detailed time-and parametric-co >Setup All models use skip connections <ref type=\"bibr\" target=\"#b23\">(He et al., 2016)</ref> as in <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref>. When previous results exist, we take  ich could not fit this dataset, is provided in Appendix F.1.</p><p>The role of multi-head attention <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> found the role of multi-head attention tional networks <ref type=\"bibr\" target=\"#b48\">(Santoro et al., 2017)</ref>. The GAT formulation of <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> rose as the most popular framework for \u03b2 \u0177 K (39) = q Q x K + \u03b2 q Q \u0177 K<label>(40)</label></formula><p>C.1 Time Complexity GAT As noted by <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref>, the time complexity of a single GAT h 0.09 0.06 0.07 0.07 0.11 0.08 0.30 0.07 k0 k1 k2 k3 k4 k5 k6 k7 k8 k9 (a) Attention in standard GAT <ref type=\"bibr\" target=\"#b53\">(Veli\u010dkovi\u0107 et al. (2018)</ref>) k0 k1 k2 k3 k4 k5 k6 k7 k8 k9 q0 q1  N i with equal importance (e.g., mean or max-pooling as AGGREGATE). To address this limitation, GAT <ref type=\"bibr\" target=\"#b53\">(Veli\u010dkovi\u0107 et al., 2018)</ref> instantiates Equation ( <ref type=\"fo foot_0\">1</ref> effectively applying an MLP to compute the score for each query-key pair:</p><p>GAT <ref type=\"bibr\" target=\"#b53\">(Veli\u010dkovi\u0107 et al., 2018)</ref>:</p><formula xml:id=\"formula_5\">e (h  \"><head>C Complexity Analysis</head><p>We repeat the definitions of GAT, GATv2 and DPGAT:</p><p>GAT <ref type=\"bibr\" target=\"#b53\">(Veli\u010dkovi\u0107 et al., 2018)</ref>:</p><p>GATv2 (our fixed version):</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b34\">(Leshno et al., 1993;</ref><ref type=\"bibr\" target=\"#b44\">Pinkus, 1999;</ref><ref type=\"bibr\" target=\"#b43\">Park et al., 2021)</ref>, and it was chosen only for consistency with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  implementation of \"graph attention network\" in all popular GNN libraries such as PyTorch Geometric <ref type=\"bibr\" target=\"#b15\">(Fey and Lenssen, 2019)</ref>, DGL <ref type=\"bibr\" target=\"#b56\">(Wa >, and is now the standard implementation of \"graph attention network\" in all popular GNN libraries <ref type=\"bibr\" target=\"#b15\">(Fey and Lenssen, 2019;</ref><ref type=\"bibr\" target=\"#b56\">Wang et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g pairwise interactions between elements in graph-structured data goes back to interaction networks <ref type=\"bibr\" target=\"#b4\">(Battaglia et al., 2016;</ref><ref type=\"bibr\" target=\"#b25\">Hoshen, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b47\">Rong et al., 2020b)</ref>, and other tricks <ref type=\"bibr\" target=\"#b58\">(Wang, 2021;</ref><ref type=\"bibr\" target=\"#b28\">Huang et al., 2021)</ref> are orthogonal to the contribution of the G. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b57\">Wang et al., 2019c;</ref><ref type=\"bibr\" target=\"#b27\">Huang and Carley, 2019;</ref><ref type=\"bibr\" target=\"#b40\">Ma et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Kosaraju et al.,  target=\"#b57\">Wang et al., 2019c;</ref><ref type=\"bibr\" target=\"#b27\">Huang and Carley, 2019;</ref><ref type=\"bibr\" target=\"#b40\">Ma et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Kosaraju et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b34\">(Leshno et al., 1993;</ref><ref type=\"bibr\" target=\"#b44\">Pinkus, 1999;</ref><ref type=\"bibr\" target=\"#b43\">Park et al., 2021)</ref>, and it was chosen only for consistency with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g pairwise interactions between elements in graph-structured data goes back to interaction networks <ref type=\"bibr\" target=\"#b4\">(Battaglia et al., 2016;</ref><ref type=\"bibr\" target=\"#b25\">Hoshen, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wn representation as the query. This generalizes the standard averaging or max-pooling of neighbors <ref type=\"bibr\" target=\"#b31\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b21\">Hamilton. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b40\">Ma et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Kosaraju et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Nathani et al., 2019;</ref><ref type=\"bibr\" target=\"#b61\">Wu et al.,  ibr\" target=\"#b40\">Ma et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Kosaraju et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Nathani et al., 2019;</ref><ref type=\"bibr\" target=\"#b61\">Wu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: main idea is that we can define an appropriate function that GATv2 will be a universal approximator <ref type=\"bibr\" target=\"#b9\">(Cybenko, 1989;</ref><ref type=\"bibr\" target=\"#b24\">Hornik, 1991)</ref  from the universal approximation theorem <ref type=\"bibr\" target=\"#b24\">(Hornik et al., 1989;</ref><ref type=\"bibr\" target=\"#b9\">Cybenko, 1989;</ref><ref type=\"bibr\" target=\"#b16\">Funahashi, 1989;</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Recognizing emotion using RL <ref type=\"bibr\" target=\"#b45\">[45]</ref> and knowledge-based system <ref type=\"bibr\" target=\"#b46\">[46]</ref> are not new things actually. Liu et al. <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: real-time performance, RL is applied as the main module in this paper. Recognizing emotion using RL <ref type=\"bibr\" target=\"#b45\">[45]</ref> and knowledge-based system <ref type=\"bibr\" target=\"#b46\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n while recognizing the emotion of the current state by utilizing the dueling deep-Q-network (DDQN) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The structure of DDQN in this RL module is depicted in Fi to compute the loss function Loss(t) as below. q eval (s(t + 1), a(t + 1)) = Q (s(t + 1), a(t + 1)) <ref type=\"bibr\" target=\"#b11\">(12)</ref> q ex pect (s(t + 1), a(t + 1)) = R + \u03b3 max a(t +1) q eval . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref> utilizes its proposed model based on audio features into health psychology field. Huang et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> focuses on nonverbal sounds which naturally exists in our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vantages in processing sequence information <ref type=\"bibr\" target=\"#b39\">[39]</ref>. Zhang et al. <ref type=\"bibr\" target=\"#b40\">[40]</ref> builds a quantum-like multimodal network (QMN), which uses. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  approaches ERC include works that focus on audio <ref type=\"bibr\" target=\"#b18\">[19]</ref>, visual <ref type=\"bibr\" target=\"#b19\">[20]</ref> and electroencephalogram (EEG) <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  unique characteristics of ERC are summarized as context dependence, persistence and contagiousness <ref type=\"bibr\" target=\"#b6\">[7]</ref> that the static and dynamic flows in a daily dialogue are bo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng the conversation information clearly and continuously <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Peng et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpetitive experiments results. Some other unimodal approaches ERC include works that focus on audio <ref type=\"bibr\" target=\"#b18\">[19]</ref>, visual <ref type=\"bibr\" target=\"#b19\">[20]</ref> and elec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vantages in processing sequence information <ref type=\"bibr\" target=\"#b39\">[39]</ref>. Zhang et al. <ref type=\"bibr\" target=\"#b40\">[40]</ref> builds a quantum-like multimodal network (QMN), which uses. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Peng et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a text based model which fuses the word-level and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b25\">[26]</ref> and have better readability <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Tao et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> proposes a two-stage module to find the low-dimensional te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  other relative subjects knowledge <ref type=\"bibr\" target=\"#b41\">[41]</ref>. Plaza-del Arco et al. <ref type=\"bibr\" target=\"#b42\">[42]</ref> recognizes emotions by integrating different affective lex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is also meaningful because the emotion of speakers is relatively following regular emotion inertia <ref type=\"bibr\" target=\"#b51\">[51]</ref>. However, few of them try to combine RL and domain knowled. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpetitive experiments results. Some other unimodal approaches ERC include works that focus on audio <ref type=\"bibr\" target=\"#b18\">[19]</ref>, visual <ref type=\"bibr\" target=\"#b19\">[20]</ref> and elec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> utilizes its proposed model based on audio features into health psychology field. Huang et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> focuses on nonverbal sounds which naturally exists in our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  other relative subjects knowledge <ref type=\"bibr\" target=\"#b41\">[41]</ref>. Plaza-del Arco et al. <ref type=\"bibr\" target=\"#b42\">[42]</ref> recognizes emotions by integrating different affective lex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rkers in positive emotions during conversations. Emotion-related features are encoded by Ryu et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref> that the robustness of edge patterns in the edge region ar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Peng et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a text based model which fuses the word-level and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fier.</p><p>RL has its unique advantages on imitating the real-time conversion of the conversations <ref type=\"bibr\" target=\"#b49\">[49]</ref> and the domain knowledge in conversations <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"><head>A. Datasets</head><p>Our model is evaluated on two widely accepted public datasets, IEMOCAP <ref type=\"bibr\" target=\"#b54\">[54]</ref> and MELD <ref type=\"bibr\" target=\"#b55\">[55]</ref>. These . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: be eight that each speaker memories four sentences before recognition the next sentence. Lai et al. <ref type=\"bibr\" target=\"#b44\">[44]</ref> proposes a different contextual window sizes based recurre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: be eight that each speaker memories four sentences before recognition the next sentence. Lai et al. <ref type=\"bibr\" target=\"#b44\">[44]</ref> proposes a different contextual window sizes based recurre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vantages in processing sequence information <ref type=\"bibr\" target=\"#b39\">[39]</ref>. Zhang et al. <ref type=\"bibr\" target=\"#b40\">[40]</ref> builds a quantum-like multimodal network (QMN), which uses. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: time ERC, even less using different modalities as inputs <ref type=\"bibr\" target=\"#b52\">[52]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are designed to track the flow of emotion states of each interlocutor throughout the conversations <ref type=\"bibr\" target=\"#b9\">[10]</ref>. However, rare researches pay the same attention on the rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> utilizes its proposed model based on audio features into health psychology field. Huang et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> focuses on nonverbal sounds which naturally exists in our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on two widely accepted public datasets, IEMOCAP <ref type=\"bibr\" target=\"#b54\">[54]</ref> and MELD <ref type=\"bibr\" target=\"#b55\">[55]</ref>. These two datasets are multimodal datasets recording text. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng the conversation information clearly and continuously <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Peng et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b25\">[26]</ref> and have better readability <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Tao et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> proposes a two-stage module to find the low-dimensional te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: edge-based system <ref type=\"bibr\" target=\"#b46\">[46]</ref> are not new things actually. Liu et al. <ref type=\"bibr\" target=\"#b47\">[47]</ref> proposes a Reinforcement Online Learning (ROL) method for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e during actual research processes, majority studies choose to build models on whole dialogue range <ref type=\"bibr\" target=\"#b8\">[9]</ref> and extra layers are designed to track the flow of emotion s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the ROL to least square (LS) and support vector regression (SVR) for emotion prediction. Li and Xu <ref type=\"bibr\" target=\"#b48\">[48]</ref> proposes a RL model for pre-selecting (RLPS) useful images. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b30\">[31]</ref> with self-attention or parameterized node similarity with input node features <ref type=\"bibr\" target=\"#b18\">[19]</ref>. These models do not suit our task for two reasons. First, s and the input graph topology for node embedding learning <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Here, the graph structure is constructed by human experts . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: raph based recommendation models, such as GC-MC <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Pin-Sage <ref type=\"bibr\" target=\"#b37\">[38]</ref>, NGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, LR-GCCF < m correlation graph with item content features as input, and learned graph-smoothed item embeddings <ref type=\"bibr\" target=\"#b37\">[38]</ref>. LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Light et=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Therefore, we also use GCN as encoder and feed the enhance. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndations by collectively learning users' preference from user-item historical interaction behaviors <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta x as input, and learn free user and item latent embeddings based on matrix factorization techniques <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Recently, researc ommendations by collectively analyzing user-item behaviors <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Learning accurate . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Amazon-Video Games <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, and Pinterest <ref type=\"bibr\" target=\"#b7\">[8]</ref>. For ry is viewed as 1 only when rating equals 5. For Amazon-Video Games, we adopt the processed dataset <ref type=\"bibr\" target=\"#b36\">[37]</ref> which each user have at least 5 records. For Pinterest, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  learning, such to avoid fake edges or adversarial attacks <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. In contrast, we consider the scenario of sparse links in C hree graph learning based models, including GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, DropEdge <ref type=\"bibr\" target=\"#b26\">[27]</ref> and GLCN <ref type=\"bibr\" target=\"#b15\">[16]</ref>. GAT is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e recommendation, due to the flexibility, and relatively high performance of these embedding models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ llaborative filtering models have been proposed to better model user-item bipartite graph structure <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ ation and feature transformation are unnecessary and bring unnecessary complexity in graph based CF <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. Therefore, we only  rocess when the performance decreases in the validation data. Similar to many graph based CF models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ  <ref type=\"bibr\" target=\"#b37\">[38]</ref>, NGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref> and LightGCN <ref type=\"bibr\" target=\"#b11\">[12]</ref> have  nput, and learned graph-smoothed item embeddings <ref type=\"bibr\" target=\"#b37\">[38]</ref>. LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref> and LightGCN <ref type=\"bibr\" target=\"#b11\">[12]</ref> simpl CF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, BiGI <ref type=\"bibr\" target=\"#b2\">[3]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref> and LightGCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Earlier works treated user-item implicit feedback as a use tite graph structure <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. These neural graph models perform graph convolution by upd raph based CF models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we set the embedding propagation depth \ud835\udc3e in range of {0,1, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Pin-Sage <ref type=\"bibr\" target=\"#b37\">[38]</ref>, NGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Light der collaborative signals for better user/item embedding learning with iterative graph convolutions <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Pin-Sage took item-item correlation graph with item conte \">[26]</ref>. The second is neural graph based CF models with fixed graph structure, including NGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, BiGI <ref type=\"bibr\" target=\"#b2\">[3]</ref>, LR-GCCF <re et=\"#b17\">[18]</ref>. Extensive works show the effectiveness of GCNs on graph based recommendations <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graph structure to facilitate downstream graph based tasks <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. These approaches l larity calculation is based on input node features, such as the covariance matrix of input features <ref type=\"bibr\" target=\"#b19\">[20]</ref>, feature weight vector <ref type=\"bibr\" target=\"#b15\">[16]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eraction behaviors <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. In most recommenda  architecture for modern recommender systems due to its flexibility and relatively high performance <ref type=\"bibr\" target=\"#b27\">[28]</ref>. After that, the preference of a user to an item can be pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: preference from user-item historical interaction behaviors <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar ese embedding models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Earlier works trea ved preference set <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Therefore, how to learn users' preferences from implicit f zation techniques for the user and item embedding learning <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Bayesian Personalized Ranking (BPR) is a popular pairwise  ser-item behaviors <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Learning accurate user and item embeddings is the default  latent embeddings based on matrix factorization techniques <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Recently, researchers argued that user behaviors can be na  prediction, we employ the pairwise ranking based BPR loss <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, which assumes that the observed items' prediction values s esigns a pairwise ranking goal, assuming that a user prefers an observed item to an unobserved item <ref type=\"bibr\" target=\"#b25\">[26]</ref>. As users' behaviors are naturally represented as a user-i ould be categorized into three groups. The first is classical matrix factorization based method BPR <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The second is neural graph based CF models with fixed gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Amazon-Video Games <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, and Pinterest <ref type=\"bibr\" target=\"#b7\">[8]</ref>. For ry is viewed as 1 only when rating equals 5. For Amazon-Video Games, we adopt the processed dataset <ref type=\"bibr\" target=\"#b36\">[37]</ref> which each user have at least 5 records. For Pinterest, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e. However, we find that the multiplexer-based coverage proposed by the state-of-the-art RTL fuzzer <ref type=\"bibr\" target=\"#b13\">[14]</ref>, has critical limitations due to following two reasons: 1) Intel experienced before. Particularly comparing the fuzzing performance of DIFUZZRTL against RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref> (i.e., the state of the art RTL fuzzer), DIFUZZRTL showed  ut each approach has its own limitation.</p><p>Focusing the discussion on fuzzing techniques, RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed the mux-coverage guided fuzzing technique. The co rageguided feature; 2) mux-cov, which utilizes the mux-coverage guided fuzzing as proposed by RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref>; and 3) reg-cov, which utilizes the register-coverage guid  number of line almost twice because of the wiring cost of monitoring per each mux. Moreover, RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref> was not able to instrument Boom core due to the resource c nch coverage. As mentioned, branch coverage on RTL code has fundamental limitation. Recently, RFUZZ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, proposes mux coverage which can be synthesized into FPGA  the most widely used coverage is the functional coverage which is manually defined by the designers <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Static RTL Verification. Along with dynamic verification, mmarize, register-coverage of DIFUZZRTL has three fundamental differences from muxcoverage of RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref>, making DIFUZZRTL's simulation more efficient. First, inst. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b26\">[28]</ref><ref type=\"bibr\" target=\"#b27\">[29]</ref><ref type=\"bibr\" target=\"#b28\">[30]</ref><ref type=\"bibr\" target=\"#b29\">[31]</ref><ref type=\"bibr\" target=\"#b30\">[32]</ref><ref type=\"bibr\" t =\"#b26\">[28]</ref><ref type=\"bibr\" target=\"#b27\">[29]</ref><ref type=\"bibr\" target=\"#b28\">[30]</ref><ref type=\"bibr\" target=\"#b29\">[31]</ref><ref type=\"bibr\" target=\"#b30\">[32]</ref>. For example, tai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iding method. Two distributions are considered significantly different if p-value is less than 0.05 <ref type=\"bibr\" target=\"#b47\">[51]</ref>.</p><p>no-cov mux-cov reg-cov no-cov</p><formula xml:id=\"f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  where the fuzzer identifies a bug by comparing the output of multiple programs of the same purpose <ref type=\"bibr\" target=\"#b33\">[35,</ref><ref type=\"bibr\" target=\"#b34\">36]</ref>. In fact, such dif  differential fuzzing to find the inconsistent behaviors across Java Virtual Machines (JVMs). Nezha <ref type=\"bibr\" target=\"#b33\">[35]</ref> defined the notion of \u03b4-diversity, which represents the as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rocessors in deeper aspects.</p><p>Other CPU fuzzing works <ref type=\"bibr\" target=\"#b55\">[59,</ref><ref type=\"bibr\" target=\"#b56\">60]</ref> aim at finding hardware flaws as well as undocumented instr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b29\">[31]</ref><ref type=\"bibr\" target=\"#b30\">[32]</ref><ref type=\"bibr\" target=\"#b31\">[33]</ref><ref type=\"bibr\" target=\"#b32\">[34]</ref>, mostly because 1) it has strong security implications; 2). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iding method. Two distributions are considered significantly different if p-value is less than 0.05 <ref type=\"bibr\" target=\"#b47\">[51]</ref>.</p><p>no-cov mux-cov reg-cov no-cov</p><formula xml:id=\"f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n discovered. Focusing on the cases in open source CPUs, it is reported that OpenSparc had 296 bugs <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Proprietary CPUs such as Intel CPUs also suffered from the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" t =\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" t type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> and symbolic execution <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> techniques have be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iscover semantic bugs by observing inconsistent behaviors across similar applications. For example, <ref type=\"bibr\" target=\"#b60\">[64,</ref><ref type=\"bibr\" target=\"#b61\">65]</ref> leverages differen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: using an RTL simulator (i.e., Verilator <ref type=\"bibr\" target=\"#b44\">[47]</ref> or icarus Verilog <ref type=\"bibr\" target=\"#b45\">[48]</ref>). This prototype includes 1.5 k lines of python codes for  xity increases. Mor1kx is slow because it can only be simulated using slow Icarus Verilog simulator <ref type=\"bibr\" target=\"#b45\">[48]</ref>. The decreased simulation speed due to the register-covera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l recommendations <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>Despite the success of sequential models in Natural L proposed embedding smoothing method with a state-of-the-art sequential recommendation model, SASRec <ref type=\"bibr\" target=\"#b3\">[4]</ref>, on three public datasets. Results of extensive experiments  i-c.org/ns/1.0\"><head n=\"2.2\">SASRec</head><p>Here we concisely introduce the backbone model SASRec <ref type=\"bibr\" target=\"#b3\">[4]</ref>. SASRec (short for Self-Attention based Sequential Recommend e performances of the models on sequential recommendation, which has been widely used in literature <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Given a user's i o-layer hierarchical attention network to couple user's long-term and short-term preference. SASRec <ref type=\"bibr\" target=\"#b3\">[4]</ref> and BERT4Rec <ref type=\"bibr\" target=\"#b34\">[35]</ref> adopt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aph convolution in it. By using sparse matrices, the original form of GCN takes O\u00f0jEjdK \u00fe jIjd 2 K\u00de <ref type=\"bibr\" target=\"#b20\">[21]</ref>, where jEj is the number of edges in the item graph, jIj i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: clude, embedding smoothing through graph convolutions is mainly based on the second-order proximity <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>In addition, from the perspective of graph signal p lations in real-world datasets are often sparse and many similar items may not be explicitly linked <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Therefore, considering the similarity of neighborhood str. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . 2 This is a product-review dataset crawled from Amazon.com, a well-known online shopping platform <ref type=\"bibr\" target=\"#b21\">[22]</ref>. The entire data is split into several subsets according t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b37\">[38]</ref>, NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>. 2). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation network or the knowledge graph to generate knowledge-aware recommendations, such as RippleNet <ref type=\"bibr\" target=\"#b45\">[46]</ref>, KGCN-LS <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and KG. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b17\">[18]</ref>, GC-SAN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and PinSage <ref type=\"bibr\" target=\"#b44\">[45]</ref>. 4) Heterogeneous information network/knowledge graph. The. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[37]</ref>, SpectralCF <ref type=\"bibr\" target=\"#b37\">[38]</ref>, NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and Li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: owledge-aware recommendations, such as RippleNet <ref type=\"bibr\" target=\"#b45\">[46]</ref>, KGCN-LS <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and KGAT <ref type=\"bibr\" target=\"#b47\">[48]</ref>.</p></. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ommendation models, neural models are not necessarily capturing the multi-hop transitivity of items <ref type=\"bibr\" target=\"#b8\">[9]</ref>. That is, they are poor at maintaining the global structure . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed to introduce privileged distillation into recommendations <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Selective Distillation Network <ref type=\"bibr\" target=\"#b as the teacher model, so that the student model can distill effective review information. Xu et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> proposed Privileged Features Distillation (PFD) to distill. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ine recommendations, researchers proposed to introduce privileged distillation into recommendations <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Selective Distillat ibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Selective Distillation Network <ref type=\"bibr\" target=\"#b5\">[6]</ref> was proposed to use a review process framework as the teache. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the attribute similarity for new user (item) recommendations <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. However, they do n ences conventionally <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Social data based  ata, and then generate the new user's embedding with the connection between new users and old users <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Despite the achievements they have made, most of these mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: laborative filtering and alleviate the data sparsity problem <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. For example, Chen . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs and items to enhance the modeling of preference embedding <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targe s usually learn a mapping function to transform the content representation into collaborative space <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which solve the problem of limited equipment resources and e the problem of limited equipment resources and reduce the running time. For example, Zhang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> constructed an embedding based model to distill user's met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which solve the problem of limited equipment resources and e the problem of limited equipment resources and reduce the running time. For example, Zhang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> constructed an embedding based model to distill user's met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For example, DeepMusic <ref type=\"bibr\" target=\"#b27\">[28]  collaborative space <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and leverage deep cross-network structure to capture highe \" target=\"#b29\">30]</ref>. For example, DeepMusic <ref type=\"bibr\" target=\"#b27\">[28]</ref> and CDL <ref type=\"bibr\" target=\"#b29\">[30]</ref> were proposed to incorporate content data into deep neural. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: laborative filtering and alleviate the data sparsity problem <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. For example, Chen . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us, inductive learning methods of graph are proposed to tackle unseen nodes' representation problem <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, intermediate layers <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, and relation-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns have shown huge success for solving data sparsity problem <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Since the user-ite. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For example, DeepM ransform the content representation into collaborative space <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and leverage deep  type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For example, DeepMusic <ref type=\"bibr\" target=\"#b27\">[28]</ref> and CDL <ref type=\"bibr\" target=\"#b29\">[30]</ref> were pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ays: logits output <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, intermediate layers <ref type=\"bibr\" target=\"#b23\">[24,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For example, DeepMusic <ref type=\"bibr\" target=\"#b27\">[28]  collaborative space <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and leverage deep cross-network structure to capture highe \" target=\"#b29\">30]</ref>. For example, DeepMusic <ref type=\"bibr\" target=\"#b27\">[28]</ref> and CDL <ref type=\"bibr\" target=\"#b29\">[30]</ref> were proposed to incorporate content data into deep neural. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: we select three suitable and public available datasets to evaluate all the models, i.e., Yelp, XING <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and Amazon-Video Games <ref type=\"bibr\" target=\"#b10\">[11]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ization to capture the new users' preferences conventionally <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>. Recently, graph based recommendations have shown huge success for solving data sparsity problem <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based on past user behaviors, such as purchasing a product <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. Recently, graph ba which are widely applied in various recommendation systems <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. These methods leve ion to obtain low-dimensional representations of users and items. For example, Salakhutdinov et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> proposed Bayesian Personalized Ranking (BPR), which learne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which solve the problem of limited equipment resources and e the problem of limited equipment resources and reduce the running time. For example, Zhang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> constructed an embedding based model to distill user's met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us, inductive learning methods of graph are proposed to tackle unseen nodes' representation problem <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which solve the problem of limited equipment resources and e the problem of limited equipment resources and reduce the running time. For example, Zhang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> constructed an embedding based model to distill user's met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs and items to enhance the modeling of preference embedding <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targe s usually learn a mapping function to transform the content representation into collaborative space <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dge distillation is presented in three ways: logits output <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, intermediate layer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: laborative filtering and alleviate the data sparsity problem <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. For example, Chen . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: formation of the existing entities and the attribute similarity for new user (item) recommendations <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ age social data and basic matrix factorization to capture the new users' preferences conventionally <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, and relation-based distillation <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, and relation-based distillation <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ure to capture higher-order relationships between features <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. For example, xDeepFM was proposed to model cross interacti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, intermediate layers <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, and relation-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs and items to enhance the modeling of preference embedding <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targe s usually learn a mapping function to transform the content representation into collaborative space <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based on past user behaviors, such as purchasing a product <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. Recently, graph ba which are widely applied in various recommendation systems <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. These methods leve ion to obtain low-dimensional representations of users and items. For example, Salakhutdinov et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> proposed Bayesian Personalized Ranking (BPR), which learne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>. Recently, graph based recommendations have shown huge success for solving data sparsity problem <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nals learned by the generator and the discriminator in order to obtain a better ranking model. DESA <ref type=\"bibr\" target=\"#b28\">[29]</ref> leveraged both document novelty and subtopic coverage base h R-LTR and PAMM, denoted as R-LTR-NTN and PAMM-NTN, respectively.</p><p>(4) Ensemble methods. DESA <ref type=\"bibr\" target=\"#b28\">[29]</ref> and DVGAN <ref type=\"bibr\" target=\"#b20\">[21]</ref> are tw wing previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, the relevance scor  the previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar 45]</ref>. Consistent with previous diversification models <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar ns on the intent graph, which is the same as previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. The number of GCN  stent with that in <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>.</p><formula xml:id=\"formula_12\">\ud835\udc53 rel (\ud835\udc51 \ud835\udc56 ) is produced ( loss function of the model. We follow the previous studies <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and utilize the list-pairwise loss function for optimizatio e as previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. The ClueWeb09 contains 200 queries of Web Track dataset fr get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Note that our proposed Graph4DIV does not use subtopics an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  intent graph. Motivated by the powerful aggregating capability of the graph convolutional networks <ref type=\"bibr\" target=\"#b17\">[18]</ref> (GCN), we adapt GCN to this dynamic intent graph for learn  representation of query and items in the product search.</p><p>Graph convolutional networks (GCNs) <ref type=\"bibr\" target=\"#b17\">[18]</ref> can collect the neighbors' information by generalizing tra  fields, such as computer vision <ref type=\"bibr\" target=\"#b10\">[11]</ref>, social network analysis <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and natural langua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oothing problem, which is often observed in multi-layer GCNs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. According to the e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in multi-layer GCNs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. According to the experimental results, it is suitable for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: explicit approaches <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Yue and Joachims <ref type=\"bibr\" target=\"#b42\">[43]</ref> \" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Inspired by this, we train a classifier to explicitly judg ndancy. Besides, we also use the diversity measure Subtopic Recall (denoted as S-rec, a.k.a. I-rec) <ref type=\"bibr\" target=\"#b44\">[45]</ref>. Consistent with previous diversification models <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mon and natural way to present the relationship of documents, queries, and intents in IR literature <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us on the document's novelty based on the similarity between documents and do not rely on subtopics <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As subtopic mining itself is a very challenging task, the i they use.</p><p>Implicit Diversification Approaches Most implicit methods obey the framework of MMR <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which balances the relevance and novelty of the document wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mon and natural way to present the relationship of documents, queries, and intents in IR literature <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in multi-layer GCNs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. According to the experimental results, it is suitable for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \ud835\udc51 \ud835\udc57 share the same subtopic. To mine the subtopic information from the documents, we leverage BERT <ref type=\"bibr\" target=\"#b29\">[30]</ref> to extract the representation of documents \ud835\udc51 \ud835\udc56 and \ud835\udc51 \ud835\udc57 . H. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. Other advanced tex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rage graph structure to enhance the representation of documents and queries. For example, Li et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> learned text representation with graph structure that cont. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. Other advanced tex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: G <ref type=\"bibr\" target=\"#b6\">[7]</ref>, ERR-IA <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and NRBP <ref type=\"bibr\" target=\"#b7\">[8]</ref>, which are official diversity evaluation metrics used in Web. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , and the common way to measure diversity is based on intent <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target= used by lots of previous research, including \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b6\">[7]</ref>, ERR-IA <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and NRBP <ref type=\"bibr\" target=\"#b7\">[8]</ref>, which are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eat performance on several natural language processing tasks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are several graph neural networks, we also try other ones, such as Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b39\">[40]</ref>, in Graph4DIV. All hyper-parameters are kept the same with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b34\">35]</ref> and natural language processing <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tution of real intents to measure diversity of each document <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, while implicit approaches focus on the document's novelty . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and natural language processing <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ether in the ranking process, which could be categorized into explicit methods. For instance, DVGAN <ref type=\"bibr\" target=\"#b20\">[21]</ref> combined ranking signals learned by the generator and the   respectively.</p><p>(4) Ensemble methods. DESA <ref type=\"bibr\" target=\"#b28\">[29]</ref> and DVGAN <ref type=\"bibr\" target=\"#b20\">[21]</ref> are two ensemble methods that use both explicit (subtopic) same relevance features R \ud835\udc56 as those used in previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><formula xml:id  based on relevance and diversity. Following previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar le\" target=\"#tab_8\">3</ref> and is consistent with that in <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>.</p><formula xml:id  query \ud835\udc5e. To make a fair comparison with the previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar target=\"#b1\">[2]</ref>, which is the same as previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. The ClueWeb09 cont rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Note that our prop he notation redundancy, we omit the query \ud835\udc5e in all equations.Session 3E: Diversity and Novelty SIGIR<ref type=\"bibr\" target=\"#b20\">'21,</ref>  </note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" plac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: G <ref type=\"bibr\" target=\"#b6\">[7]</ref>, ERR-IA <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and NRBP <ref type=\"bibr\" target=\"#b7\">[8]</ref>, which are official diversity evaluation metrics used in Web. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  The preliminary experiment starts with a representative graph SSL scheme, Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b45\">[46]</ref>, for encoding the global information into node representat ws of nodes (sampled ego-networks). Motivated by DIM <ref type=\"bibr\" target=\"#b14\">[15]</ref>, DGI <ref type=\"bibr\" target=\"#b45\">[46]</ref> and InfoGraph <ref type=\"bibr\" target=\"#b42\">[43]</ref> ha  + (1 \u2212 y i ) \u2022 log (1 \u2212 p i )) , (5)</formula><p>Basic L S S L . We adopt Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b45\">[46]</ref>, a stateof-the-art graph SSL scheme, to instantiate the SS pture transferable structural patterns across graphs, over-emphasizes the structural homophily. DGI <ref type=\"bibr\" target=\"#b45\">[46]</ref> contrasts the whole graph with the node in it to encode th aph Contrastive Coding (GCC) <ref type=\"bibr\" target=\"#b33\">[34]</ref> and Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b45\">[46]</ref> perform contrastive learning between node-node pairs or gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: neural networks (GNNs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar ed by the recent progress in self-supervised learning (SSL) <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar ds. The state-ofthe-art GNN models include GCN <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>, GAT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, GIN <ref t ]</ref> propagates messages based on the graph Laplacian matrix in a transductive manner. GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposes an inductive learning framework, in which the agg pts of unsupervised graph learning such as GAE <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSage <ref type=\"bibr\" target=\"#b10\">[11]</ref>, node2vec <ref type=\"bibr\" target=\"#b9\">[10]</ref>, deepwa es offer limited benefits. For example, GAE <ref type=\"bibr\" target=\"#b19\">[20]</ref> and GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> reconstruct the adjacency matrix following the neighborhoo rget=\"#b19\">[20]</ref> and Random walk-based objective (RW) <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> reconstruct the one-hop or the multi-hop adjacency informat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -supervised learning (pre-training) is a common and effective scheme in the area of computer vision <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ mes, contrastive learning (CL), raises a recent surge of interest in visual representation learning <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. On a parallel note,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n. We construct four separate datasets from Reddit with different \u03b7. Specifically, we perform METIS <ref type=\"bibr\" target=\"#b18\">[19]</ref>, a graph partition algorithm, to partition the the origina =\"#b22\">[23]</ref>, which contains 256,059 users, 74, 258 products and 560,804 interactions. We use <ref type=\"bibr\" target=\"#b18\">[19]</ref> to partition the original Amazon dataset into 20 sub-graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b16\">[17]</ref> learns node attributes by a generative SSL scheme. You et al. <ref type=\"bibr\" target=\"#b56\">[57]</ref> and Hassani et al. <ref type=\"bibr\" target=\"#b11\">[12]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mmendation systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>, computational biology <ref type=\"bibr\" target=\"#b38\">[39,<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b1\">[2]</ref>.</p><p>Recently, driven by the advances of graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ese models have been widely used in various real-world applications, such as recommendation systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b1\">[2]</ref>.</p><p>Recently, driven by the advances of graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>, as graphs effectively describe the correlations among inte ibr\" target=\"#b58\">59]</ref> or belief propagation on graphs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Nevertheless, these early attempts usually rely on the hum. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>. The main idea of GNN-based anomaly detection is to leverag. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b22\">(Rashkin et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Majumder et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Zandie and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: driven models that leverage such corpora <ref type=\"bibr\" target=\"#b20\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Zhang et al., 2020;</ref><ref type=\"bibr\" target=\"#b23\">Roller et al. rms the model performance and response coherence.</p><p>DialoGPT We additionally evaluated DialoGPT <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2020)</ref>, which is a GPT-2-based model pre-trained . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and work through the challenges that they face <ref type=\"bibr\" target=\"#b1\">(Burleson, 2003;</ref><ref type=\"bibr\" target=\"#b10\">Langford et al., 1997;</ref><ref type=\"bibr\" target=\"#b3\">Heaney and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esearch has also shown that people prefer dialog systems that can provide more supportive responses <ref type=\"bibr\" target=\"#b21\">(Rains et al., 2020)</ref>.</p><p>Research has shown that providing e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b39\">Zhou and Wang, 2018;</ref><ref type=\"bibr\" target=\"#b8\">Huber et al., 2018;</ref><ref type=\"bibr\" target=\"#b7\">Huang et al., 2020)</ref>. As a notable work of emotional conversation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: at relate to emotional chatting <ref type=\"bibr\">(Zhou et al., 2018)</ref> or empathetic responding <ref type=\"bibr\" target=\"#b22\">(Rashkin et al., 2019)</ref> return messages that are examples of emo elements of emotional chatting <ref type=\"bibr\">(Zhou et al., 2018)</ref> and empathetic responding <ref type=\"bibr\" target=\"#b22\">(Rashkin et al., 2019)</ref>.</p><p>support through social interactio cal for previous emotional chatting <ref type=\"bibr\">(Zhou et al., 2018)</ref> or empathetic dialog <ref type=\"bibr\" target=\"#b22\">(Rashkin et al., 2019)</ref> datasets.</p><p>We also present the stat s, existing online conversation datasets <ref type=\"bibr\" target=\"#b24\">(Sharma et al., 2020a;</ref><ref type=\"bibr\" target=\"#b22\">Rashkin et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Zhong et al that dialog systems are expected to be equipped with. Another related task is empathetic responding <ref type=\"bibr\" target=\"#b22\">(Rashkin et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Lin et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and work through the challenges that they face <ref type=\"bibr\" target=\"#b1\">(Burleson, 2003;</ref><ref type=\"bibr\" target=\"#b10\">Langford et al., 1997;</ref><ref type=\"bibr\" target=\"#b3\">Heaney and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ioned and then responding with a reflection from a template or a message from a pre-defined lexicon <ref type=\"bibr\" target=\"#b31\">(Welch et al., 2020)</ref>. Few studies have focused on generating su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and work through the challenges that they face <ref type=\"bibr\" target=\"#b1\">(Burleson, 2003;</ref><ref type=\"bibr\" target=\"#b10\">Langford et al., 1997;</ref><ref type=\"bibr\" target=\"#b3\">Heaney and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b24\">Sharma et al., 2020a;</ref><ref type=\"bibr\" target=\"#b36\">Zhong et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Zheng et al., 2021)</ref>, which aims at understanding users' feeling. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n et al., 2017)</ref> or matrix factorization <ref type=\"bibr\" target=\"#b4\">(Cao et al., 2015;</ref><ref type=\"bibr\" target=\"#b51\">Wang et al., 2016)</ref> objectives, GNNs explicitly derive proximity. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iological function) prediction benchmarks.</p><p>In particular, the Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b54\">(Xu et al., 2019)</ref> pre-trained by the proposed method outperform =\"bibr\" target=\"#b24\">Hu et al. (2019)</ref>, we adopt a five-layer Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b54\">(Xu et al., 2019)</ref> with 300-dimensional hidden units and a mean  ton et al., 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b49\">(Velickovic et al., 2018)</ref> and GIN <ref type=\"bibr\" target=\"#b54\">(Xu et al., 2019)</ref>).</p><p>We can observe that GraphLoG outperfo bibr\" target=\"#b57\">Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b60\">Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b54\">Xu et al., 2019)</ref> sought to improve the effectiveness of these t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the idea of stochastic EM algorithm <ref type=\"bibr\" target=\"#b7\">(Celeux &amp; Govaert, 1992;</ref><ref type=\"bibr\" target=\"#b38\">Nielsen et al., 2000)</ref> and draw a sample \u1e91Gn \u223c p(z Gn |G n , \u03b8 t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b26\">Ji et al., 2019)</ref>, especially in a hierarchical way for graph-structured data <ref type=\"bibr\" target=\"#b1\">(Ashburner et al., 2000;</ref><ref type=\"bibr\" target=\"#b9\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b60\">Zhang et al., 2018)</ref>.</p><p>General EM Algorithm. The basic objective of EM algorithm <ref type=\"bibr\" target=\"#b11\">(Dempster et al., 1977;</ref><ref type=\"bibr\" target=\"#b31\">Krishnan  he expectation of complete-data likelihood through the EM algorithm.</p><p>The vanilla EM algorithm <ref type=\"bibr\" target=\"#b11\">(Dempster et al., 1977;</ref><ref type=\"bibr\" target=\"#b31\">Krishnan . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b27\">Jiang et al., 2017)</ref>, and predicting the properties of circuits in circuit design <ref type=\"bibr\" target=\"#b59\">(Zhang et al., 2019)</ref>. Recently, Graph Neural Networks (GNNs) ha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b40\">Qiu et al., 2020;</ref><ref type=\"bibr\">You et al., 2020a)</ref> and meta-learning <ref type=\"bibr\" target=\"#b34\">(Lu et al., 2021)</ref>. However, all these methods are able to model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e for some social or chemical graphs. Finally, another important aspect mentioned by a recent paper <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref> concerns the spectral ability of GNN mod cted nodes messages and one that updates the concerned node representation.</p><p>In a recent paper <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref>, it was explicitly shown that both spati pectral properties of the graph signal such as in image/signal processing applications. As shown in <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref>, the vast majority of existing MPNNs ope w-pass filters which limits their capacity. To lead this analysis, we use the datasets presented in <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref>. First, we evaluate if the models can le et from <ref type=\"bibr\" target=\"#b9\">(Chen et al., 2020)</ref>, 2D-Grid and Band-Pass dataset from <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref>, Zinc12K from <ref type=\"bibr\" target=\"# ef>, Mnist-75 dataset from online source<ref type=\"foot\" target=\"#foot_7\">5</ref> which was used in <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref> with exactly the same procedure, PROTEIN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Sato et al., 2020)</ref> or to add a unique label <ref type=\"bibr\" target=\"#b29\">(Murphy et al., 2019)</ref> in order to have the ability to distingui. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e tests using a first order logic <ref type=\"bibr\" target=\"#b22\">(Immerman &amp; Lander, 1990;</ref><ref type=\"bibr\" target=\"#b3\">Barcel\u00f3 et al., 2019)</ref>. Consider two unlabeled and undirected gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e, it has also been shown that many other combinatorial problems on graph cannot be solved by MPNNs <ref type=\"bibr\" target=\"#b31\">(Sato et al., 2019)</ref>.</p><p>In <ref type=\"bibr\" target=\"#b26\">(M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the first generation of Graph Neural Networks (GNNs) called Message Passing Neural Networks (MPNNs) <ref type=\"bibr\" target=\"#b17\">(Gilmer et al., 2017)</ref>. These algorithms spread each node featur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Defferrard et al., 2016)</ref>, to the connected nodes features (GAT for graph attention network) <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and/or to edge features <ref type=\"bi upports can be written thanks to operations from L + 1 . C = A + \u00d7 diag(1) and C mlp = diag(1). GAT <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> can be expressed in Eq.( <ref type=\"f in <ref type=\"bibr\" target=\"#b40\">(Xu et al., 2019)</ref>.</p><p>Graph attention networks (GATs) in <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> proposes to transpose the attention m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -able weights. These weights can be shared with respect to the distance between nodes (Chebnet GNN) <ref type=\"bibr\" target=\"#b12\">(Defferrard et al., 2016)</ref>, to the connected nodes features (GAT 1 .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B.2. Theorem.2</head><p>Proof. Chebnet <ref type=\"bibr\" target=\"#b12\">(Defferrard et al., 2016)</ref> uses desired number k of convolution  scaled graph Laplacian. The number of convolution supports C (k) can be chosen. They are defined by <ref type=\"bibr\" target=\"#b12\">(Defferrard et al., 2016)</ref> as follows:</p><p>C (1) = I, C (2) = . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing operations in L + 1 . According to the universal approximation theory of multi layer perceptron <ref type=\"bibr\" target=\"#b21\">(Hornik et al., 1989)</ref>, if we have enough layers, we can impleme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Defferrard et al., 2016)</ref>, to the connected nodes features (GAT for graph attention network) <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and/or to edge features <ref type=\"bi upports can be written thanks to operations from L + 1 . C = A + \u00d7 diag(1) and C mlp = diag(1). GAT <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> can be expressed in Eq.( <ref type=\"f in <ref type=\"bibr\" target=\"#b40\">(Xu et al., 2019)</ref>.</p><p>Graph attention networks (GATs) in <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> proposes to transpose the attention m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b35\">(Park et al., 2019;</ref><ref type=\"bibr\" target=\"#b39\">Salha et al., 2019)</ref>. <ref type=\"bibr\" target=\"#b46\">(Velickovic et al., 2019)</ref> suggested an unsupervised learning ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b7\">Bruna, 2020;</ref><ref type=\"bibr\" target=\"#b5\">Bueno &amp; Hylton, 2020)</ref>. In <ref type=\"bibr\" target=\"#b41\">(Santoro et al., 2018)</ref> the authors study generalization in abst. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ther papers <ref type=\"bibr\">(Velickovic et al., arXiv:2010.08853v3 [cs.</ref>LG] 15 Jul 2021 2019; <ref type=\"bibr\" target=\"#b23\">Khalil et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Joshi et al. ractical side, <ref type=\"bibr\" target=\"#b21\">(Joshi et al., 2019;</ref><ref type=\"bibr\">2020;</ref><ref type=\"bibr\" target=\"#b23\">Khalil et al., 2017)</ref>, study the Traveling Salesman Problem (TSP. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b35\">(Park et al., 2019;</ref><ref type=\"bibr\" target=\"#b39\">Salha et al., 2019)</ref>. <ref type=\"bibr\" target=\"#b46\">(Velickovic et al., 2019)</ref> suggested an unsupervised learning ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b1\">(Barab\u00e1si &amp; Albert, 1999)</ref>, which captures graph structure in social networks <ref type=\"bibr\" target=\"#b2\">(Barab\u00e2si et al., 2002)</ref>, biological networks <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: different sizes. Unfortunately, such training procedures cannot be easily applied to general tasks. <ref type=\"bibr\" target=\"#b26\">(Knyazev et al., 2019)</ref> studied the relationship between general. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ther papers <ref type=\"bibr\">(Velickovic et al., arXiv:2010.08853v3 [cs.</ref>LG] 15 Jul 2021 2019; <ref type=\"bibr\" target=\"#b23\">Khalil et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Joshi et al. ractical side, <ref type=\"bibr\" target=\"#b21\">(Joshi et al., 2019;</ref><ref type=\"bibr\">2020;</ref><ref type=\"bibr\" target=\"#b23\">Khalil et al., 2017)</ref>, study the Traveling Salesman Problem (TSP. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>Datasets. We use datasets from <ref type=\"bibr\" target=\"#b34\">(Morris et al., 2020)</ref> and <ref type=\"bibr\" target=\"#b38\">(Rozemberczki et al., 2020)</ref> (Twitch egos and Deezer egos).</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ther papers <ref type=\"bibr\">(Velickovic et al., arXiv:2010.08853v3 [cs.</ref>LG] 15 Jul 2021 2019; <ref type=\"bibr\" target=\"#b23\">Khalil et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Joshi et al. ractical side, <ref type=\"bibr\" target=\"#b21\">(Joshi et al., 2019;</ref><ref type=\"bibr\">2020;</ref><ref type=\"bibr\" target=\"#b23\">Khalil et al., 2017)</ref>, study the Traveling Salesman Problem (TSP. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 002)</ref>, biological networks <ref type=\"bibr\" target=\"#b11\">(Eisenberg &amp; Levanon, 2003;</ref><ref type=\"bibr\" target=\"#b29\">Light et al., 2005)</ref> and internet link data <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  et al., 2020)</ref>: Nodewise sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b26\">Markowitz et a ying message passing implementation. GAS revisits and generalizes the idea of historical embeddings <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>, which are defined as node embeddings acqui  approximate their embeddings via historical embeddings acquired in previous iterations of training <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>, denoted by h( ) w . After each step of tra e model weights are kept fixed, h( ) v eventually equals h ( ) v after a fixed amount of iterations <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org  an affordable approximation. The idea of historical embeddings was originally introduced in VR-GCN <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>. VR-GCN aims to reduce the variance in esti >(Chen et al., 2018a)</ref>, LADIES <ref type=\"bibr\" target=\"#b53\">(Zou et al., 2019)</ref>, VR-GCN <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>, MVS-GNN <ref type=\"bibr\" target=\"#b6\">(Con 2\">(Wu et al., 2019)</ref>, SIGN <ref type=\"bibr\" target=\"#b11\">(Frasca et al., 2020)</ref> and GBP <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2020a)</ref>. Since results are hard to compare across d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: u et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Huang et al., 2018)</ref>; In subgraph sampling <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019;</ref><ref type=\"bibr\" target=\"#b49\">Zeng et al., story access, and increases closeness and reduces staleness in return.</p><p>Similar to CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019)</ref>, we make use of graph clustering technique chniques for minibatch selection, as first introduced in the subgraph sampling approach CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019)</ref>. CLUSTER-GCN leverages clustering in order d mini-batch GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019)</ref> training, cf. Table <ref type=\"table\" targ  et al., 2018b)</ref>, MVS-GNN <ref type=\"bibr\" target=\"#b6\">(Cong et al., 2020)</ref>, CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019)</ref>, GRAPHSAINT <ref type=\"bibr\" target=\"#b49\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: wide adoption in industrial and social applications is the difficulty to scale them to large graphs <ref type=\"bibr\" target=\"#b11\">(Frasca et al., 2020)</ref>.</p><p>While the full-gradient in a GNN i \"#b49\">(Zeng et al., 2020b)</ref>, SGC <ref type=\"bibr\" target=\"#b42\">(Wu et al., 2019)</ref>, SIGN <ref type=\"bibr\" target=\"#b11\">(Frasca et al., 2020)</ref> and GBP <ref type=\"bibr\" target=\"#b3\">(Ch r\" target=\"#b42\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Klicpera et al., 2019a;</ref><ref type=\"bibr\" target=\"#b11\">Frasca et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Yu et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es based on the concept of dropping edges <ref type=\"bibr\" target=\"#b25\">(Ma &amp; Tang, 2020;</ref><ref type=\"bibr\" target=\"#b32\">Rong et al., 2020)</ref>: Nodewise sampling <ref type=\"bibr\" target=\" \"formula_20\">h(L) v ) = c (L) v for all v \u2208 V.</formula><p>Theorem 5 extends the insights of Lemma  <ref type=\"bibr\" target=\"#b32\">(Rong et al., 2020)</ref> are still applicable for data augmentation . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  clustering techniques, e.g., METIS <ref type=\"bibr\" target=\"#b20\">(Karypis &amp; Kumar, 1998;</ref><ref type=\"bibr\" target=\"#b8\">Dhillon et al., 2007)</ref>, to achieve this goal. It aims to construc ake use of graph clustering methods <ref type=\"bibr\" target=\"#b20\">(Karypis &amp; Kumar, 1998;</ref><ref type=\"bibr\" target=\"#b8\">Dhillon et al., 2007)</ref> in order to minimize the inter-connectivit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ented techniques in practice. <ref type=\"foot\" target=\"#foot_4\">2</ref> PyGAS is built upon PYTORCH <ref type=\"bibr\" target=\"#b30\">(Paszke et al., 2019)</ref> and utilizes the PYTORCH GEOMET-Fast Hist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: exponentially increasing dependency of nodes over layers; a phenomenon framed as neighbor explosion <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>. Due to neighbor explosion and since th hen et al., 2018b)</ref>. VR-GCN aims to reduce the variance in estimation during neighbor sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, and avoids the need to sample a large   memory usage of GCN+GAS training with the memory usage of full-batch GCN, and mini-batch GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and CLUSTER-GCN <ref type=\"bibr\" target as they will run out of memory on common GPUs. We compare with 10 scalable GNN baselines: GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, FASTGCN <ref type=\"bibr\" target=\"#b2\"> a &amp; Tang, 2020;</ref><ref type=\"bibr\" target=\"#b32\">Rong et al., 2020)</ref>: Nodewise sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: highly scalable, as it is in the heart of many large-scale distributed graph storage layers such as <ref type=\"bibr\" target=\"#b51\">(Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b50\">Zheng et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: exponentially increasing dependency of nodes over layers; a phenomenon framed as neighbor explosion <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>. Due to neighbor explosion and since th hen et al., 2018b)</ref>. VR-GCN aims to reduce the variance in estimation during neighbor sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, and avoids the need to sample a large   memory usage of GCN+GAS training with the memory usage of full-batch GCN, and mini-batch GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and CLUSTER-GCN <ref type=\"bibr\" target as they will run out of memory on common GPUs. We compare with 10 scalable GNN baselines: GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, FASTGCN <ref type=\"bibr\" target=\"#b2\"> a &amp; Tang, 2020;</ref><ref type=\"bibr\" target=\"#b32\">Rong et al., 2020)</ref>: Nodewise sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ented techniques in practice. <ref type=\"foot\" target=\"#foot_4\">2</ref> PyGAS is built upon PYTORCH <ref type=\"bibr\" target=\"#b30\">(Paszke et al., 2019)</ref> and utilizes the PYTORCH GEOMET-Fast Hist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ue to the universal approximation theorem <ref type=\"bibr\" target=\"#b16\">(Hornik et al., 1989;</ref><ref type=\"bibr\" target=\"#b15\">Hornik, 1991)</ref>. However, the theory behind Lemma 4 holds for any. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of GAS into a distributed training algorithm <ref type=\"bibr\" target=\"#b24\">(Ma et al., 2019;</ref><ref type=\"bibr\" target=\"#b52\">Zhu et al., 2016;</ref><ref type=\"bibr\" target=\"#b37\">Tripathy et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: highly scalable, as it is in the heart of many large-scale distributed graph storage layers such as <ref type=\"bibr\" target=\"#b51\">(Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b50\">Zheng et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: her line of work is based on the idea of decoupling propagations from predictions, either as a pre- <ref type=\"bibr\" target=\"#b42\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Klicpera et al. iang et al., 2019)</ref>, GRAPHSAINT <ref type=\"bibr\" target=\"#b49\">(Zeng et al., 2020b)</ref>, SGC <ref type=\"bibr\" target=\"#b42\">(Wu et al., 2019)</ref>, SIGN <ref type=\"bibr\" target=\"#b11\">(Frasca . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: highly scalable, as it is in the heart of many large-scale distributed graph storage layers such as <ref type=\"bibr\" target=\"#b51\">(Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b50\">Zheng et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: following tasks:</p><p>1. classifying academic papers in citation networks (CORA, CITESEER, PUBMED) <ref type=\"bibr\" target=\"#b33\">(Sen et al., 2008;</ref><ref type=\"bibr\" target=\"#b45\">Yang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: following tasks:</p><p>1. classifying academic papers in citation networks (CORA, CITESEER, PUBMED) <ref type=\"bibr\" target=\"#b33\">(Sen et al., 2008;</ref><ref type=\"bibr\" target=\"#b45\">Yang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ented techniques in practice. <ref type=\"foot\" target=\"#foot_4\">2</ref> PyGAS is built upon PYTORCH <ref type=\"bibr\" target=\"#b30\">(Paszke et al., 2019)</ref> and utilizes the PYTORCH GEOMET-Fast Hist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  clustering techniques, e.g., METIS <ref type=\"bibr\" target=\"#b20\">(Karypis &amp; Kumar, 1998;</ref><ref type=\"bibr\" target=\"#b8\">Dhillon et al., 2007)</ref>, to achieve this goal. It aims to construc ake use of graph clustering methods <ref type=\"bibr\" target=\"#b20\">(Karypis &amp; Kumar, 1998;</ref><ref type=\"bibr\" target=\"#b8\">Dhillon et al., 2007)</ref> in order to minimize the inter-connectivit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nverge to the same optimum since we explicitely consider arbitrary GNNs solving non-convex problems <ref type=\"bibr\" target=\"#b6\">(Cong et al., 2020)</ref>.</p><p>It is well known that the most powerf >(Zou et al., 2019)</ref>, VR-GCN <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>, MVS-GNN <ref type=\"bibr\" target=\"#b6\">(Cong et al., 2020)</ref>, CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">( (Hamilton et al., 2017)</ref>, and avoids the need to sample a large amount of neighbors in return. <ref type=\"bibr\" target=\"#b6\">Cong et al. (2020)</ref> further simplified this scheme into a one-sho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ented techniques in practice. <ref type=\"foot\" target=\"#foot_4\">2</ref> PyGAS is built upon PYTORCH <ref type=\"bibr\" target=\"#b30\">(Paszke et al., 2019)</ref> and utilizes the PYTORCH GEOMET-Fast Hist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d GNN, which exist for a wide range of models <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Corso et al., xists a wide range of maximally powerful GNNs <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Corso et al., bibr\" target=\"#b47\">(Zaheer et al., 2017;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">Maron et al. h (L) w in case c (L) v = c</formula><p>(L) w <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019)</ref>, where c</p><formula xml:id=\"formula_12\">( , we know that such a function needs to exist <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019)</ref>. Therefore, it is sufficient to show that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: = w \u2208 V (4) Furthermore, we know that there exists Message is injective for all countable multisets <ref type=\"bibr\" target=\"#b47\">(Zaheer et al., 2017;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: exponentially increasing dependency of nodes over layers; a phenomenon framed as neighbor explosion <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>. Due to neighbor explosion and since th hen et al., 2018b)</ref>. VR-GCN aims to reduce the variance in estimation during neighbor sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, and avoids the need to sample a large   memory usage of GCN+GAS training with the memory usage of full-batch GCN, and mini-batch GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and CLUSTER-GCN <ref type=\"bibr\" target as they will run out of memory on common GPUs. We compare with 10 scalable GNN baselines: GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, FASTGCN <ref type=\"bibr\" target=\"#b2\"> a &amp; Tang, 2020;</ref><ref type=\"bibr\" target=\"#b32\">Rong et al., 2020)</ref>: Nodewise sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GNNs and label propagation <ref type=\"bibr\" target=\"#b36\">(Shi et al., 2020)</ref>, graph diffusion <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2019b)</ref>, or random wiring <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: quency of individual words is naturally much higher. Inspired by recent advances in text generation <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>, we propose a new  roposed concept name generation in taxonomies. Meng et al. <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> applied Seq2Seq to generate keyphrases from scientific arti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es have been widely used to enhance the performance of many applications such as question answering <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> and personalized r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: od of each existing concept in the taxonomy being its hypernym and then added it as a new leaf node <ref type=\"bibr\" target=\"#b23\">[24]</ref>. <ref type=\"bibr\">Manzoor et al.</ref> extended the measur sly improve the taxonomy completeness. We choose TaxoExpan as the extraction-based method GenTaxo++ <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>The details of GenTaxo++ are as follows. We start w ific concepts and more than 700 thousand taxonomic relations. We follow the processing in TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> to select a partial taxonomy under the \"computer science\"  p>Acc Acc-Uni Acc-Multi HiExpan <ref type=\"bibr\" target=\"#b24\">[25]</ref> 9.89 18.28 8.49 TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> 16.32 \u2022 MeSH <ref type=\"bibr\" target=\"#b13\">[14]</ref>: It ations and then organize the extracted concept pairs into a DAG as the output taxonomy. \u2022 TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> adopts GNNs to encode the positional information and uses   al. proposed a position-enhanced graph neural network to encode the relative position of each term <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Yu et al. applied a mini-path-based classifier instead of e input of the taxonomy completion task includes two parts <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b2\">[3]</ref>, optimal branching <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and minimum-cost flow <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Mao et al. used reinforcement learning to organize the hype. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: od of each existing concept in the taxonomy being its hypernym and then added it as a new leaf node <ref type=\"bibr\" target=\"#b23\">[24]</ref>. <ref type=\"bibr\">Manzoor et al.</ref> extended the measur sly improve the taxonomy completeness. We choose TaxoExpan as the extraction-based method GenTaxo++ <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>The details of GenTaxo++ are as follows. We start w ific concepts and more than 700 thousand taxonomic relations. We follow the processing in TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> to select a partial taxonomy under the \"computer science\"  p>Acc Acc-Uni Acc-Multi HiExpan <ref type=\"bibr\" target=\"#b24\">[25]</ref> 9.89 18.28 8.49 TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> 16.32 \u2022 MeSH <ref type=\"bibr\" target=\"#b13\">[14]</ref>: It ations and then organize the extracted concept pairs into a DAG as the output taxonomy. \u2022 TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> adopts GNNs to encode the positional information and uses   al. proposed a position-enhanced graph neural network to encode the relative position of each term <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Yu et al. applied a mini-path-based classifier instead of e input of the taxonomy completion task includes two parts <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: od of each existing concept in the taxonomy being its hypernym and then added it as a new leaf node <ref type=\"bibr\" target=\"#b23\">[24]</ref>. <ref type=\"bibr\">Manzoor et al.</ref> extended the measur sly improve the taxonomy completeness. We choose TaxoExpan as the extraction-based method GenTaxo++ <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>The details of GenTaxo++ are as follows. We start w ific concepts and more than 700 thousand taxonomic relations. We follow the processing in TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> to select a partial taxonomy under the \"computer science\"  p>Acc Acc-Uni Acc-Multi HiExpan <ref type=\"bibr\" target=\"#b24\">[25]</ref> 9.89 18.28 8.49 TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> 16.32 \u2022 MeSH <ref type=\"bibr\" target=\"#b13\">[14]</ref>: It ations and then organize the extracted concept pairs into a DAG as the output taxonomy. \u2022 TaxoExpan <ref type=\"bibr\" target=\"#b23\">[24]</ref> adopts GNNs to encode the positional information and uses   al. proposed a position-enhanced graph neural network to encode the relative position of each term <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Yu et al. applied a mini-path-based classifier instead of e input of the taxonomy completion task includes two parts <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tep. The second step was often considered as graph optimization and solved by maximum spanning tree <ref type=\"bibr\" target=\"#b2\">[3]</ref>, optimal branching <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b36\">37]</ref> and embedding-based methods <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> were widely used in the first step. The second step was oft. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al. used a hybrid method to combine lexico-syntactic patterns, semantic web, and neural networks <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Manzoor et al. proposed a joint-learning framework to simul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: omies from two different domains we use to evaluate the taxonomy completion methods:</p><p>\u2022 MAG-CS <ref type=\"bibr\" target=\"#b25\">[26]</ref>: The Microsoft Academic Graph (MAG) taxonomy has over 660 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce drop.</p><p>To tackle this issue, a socially-aware SSL framework which combines the tri-training <ref type=\"bibr\" target=\"#b46\">[47]</ref> (multi-view co-training) with SSL is proposed in this pape t perspectives and also provide us with a scenario to fuse tri-training and SSL.</p><p>Tri-training <ref type=\"bibr\" target=\"#b46\">[47]</ref> is a popular semi-supervised learning algorithm which expl g, in this paper, matrices appear in bold capital letters and vectors appear in bold lower letters. <ref type=\"bibr\" target=\"#b46\">[47]</ref> is a popular semi-supervised learning algorithm which deve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  progress in SSL seeks to harness this flexible learning paradigm for graph representation learning <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al models, GNNs also empower other recommendation methods working on specific graphs such as SR-GNN <ref type=\"bibr\" target=\"#b31\">[32]</ref> and DHCN <ref type=\"bibr\" target=\"#b34\">[35]</ref> over th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b1\">[2]</ref>, NGCF <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and LightGCN <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The basic idea of these GCN-based models is to exploit th recommendation models. But for a concrete framework which can be easily followed, we adopt LightGCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> as the basic structure of the encoders due to its simplici  and \ud835\udc3b \ud835\udc53 and \ud835\udc3b \ud835\udc60 be the auxiliary encoders.</p><p>Theoretically, given a concrete \ud835\udc3b \ud835\udc5f like LightGCN <ref type=\"bibr\" target=\"#b10\">[11]</ref>, there should be the optimal structures of \ud835\udc3b \ud835\udc53 and \ud835\udc3b \ud835\udc60 . H  to test the effectiveness of the self-supervised tri-training for recommendation:</p><p>\u2022 LightGCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> is a GCN-based general recommendation model that leverages plex correlations among users with hyperedges to improve recommendation performance.</p><p>LightGCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> is the basic encoder in SEPT. Investigating the performanc DCG@10 DiffNet++ Pr ec @ 10 R ec @ 10 N D C G @ 10  redundant and useless parameters and operations <ref type=\"bibr\" target=\"#b10\">[11]</ref>. On both LastFM and Douban-Book, SEPT outperforms \ud835\udc46 2 -MHC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b40\">[41]</ref><ref type=\"bibr\" target=\"#b41\">[42]</ref><ref type=\"bibr\" target=\"#b42\">[43]</ref>. Owing to the pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and language modeling <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d the neighbor-discrimination based contrastive learning. Let L \ud835\udc5f be the BPR pairwise loss function <ref type=\"bibr\" target=\"#b23\">[24]</ref> which is defined as:</p><formula xml:id=\"formula_8\">L \ud835\udc5f =  CF <ref type=\"bibr\" target=\"#b27\">[28]</ref>, GCMC <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and BPR <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Two strong social recommendation models are also compared. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. As the basic idea of SSL is to learn with the automaticall target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> for model pretraining. The recent progress in SSL seeks to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentation learning <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> has identified an effective training scheme for graph-based rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. The common types of stochastic augmentations include but a nlabeled example set is required. We follow existing works <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> to perturb the raw graph with edge dropout at a certain pro ons of the same node but learned from different views, which is known as graph contrastive learning <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Inspired by its effectiveness, a few studies <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve learning <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Inspired by its effectiveness, a few studies <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta alk.</p><p>Inspired by the success of graph contrastive learning, there have been some recent works <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta tecture with uniform feature masking and dropout for self-supervised item recommendation. Ma et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> mine extra signals for supervision by looking at the longe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: social network is often known as a reflection of homophily <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> (i.e., users who have similar preferences are more likely t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: racting information of the image-style feature map. Hence, our work is also related to the study of <ref type=\"bibr\" target=\"#b8\">[Liu et al., 2020]</ref>, who formulated incomplete utterance rewritin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Zhang et al., 2021b;</ref><ref type=\"bibr\" target=\"#b30\">Zhang et al., 2021a]</ref>. Previous works <ref type=\"bibr\" target=\"#b25\">[Zeng et al., 2015;</ref><ref type=\"bibr\" target=\"#b2\">Feng et al., 2 ies <ref type=\"bibr\" target=\"#b21\">[Yao et al., 2019;</ref><ref type=\"bibr\">Tang et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Zeng et al., 2020;</ref><ref type=\"bibr\" target=\"#b15\">Wang et al., 2  type=\"bibr\">Christopoulou et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Nan et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Zeng et al., 2020;</ref><ref type=\"bibr\" target=\"#b15\">Wang et al., 2 bles relational reasoning across sentences by automatically inducing a latent document-level graph. <ref type=\"bibr\" target=\"#b25\">Zeng et al. [2020]</ref> proposed the graph aggregation-and-inference \" target=\"#tab_1\">1</ref>.</p><p>\u2022 DocRED <ref type=\"bibr\" target=\"#b21\">[Yao et al., 2019]</ref>   <ref type=\"bibr\" target=\"#b25\">[Zeng et al., 2020]</ref> 59.14 61.22 59.00 61.24 HeterGSAN-BERT base \" target=\"#b10\">[Nan et al., 2020]</ref>, <ref type=\"bibr\">GLRE [Wang et al., 2020a]</ref> and GAIN <ref type=\"bibr\" target=\"#b25\">[Zeng et al., 2020]</ref>, HeterGSAN <ref type=\"bibr\" target=\"#b20\">[ /p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.6\">Case Study</head><p>We follow GAIN <ref type=\"bibr\" target=\"#b25\">[Zeng et al., 2020]</ref> to select the same example and conduct a ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref> 65.9 83.1 GLRE <ref type=\"bibr\" target=\"#b15\">[Wang et al., 2020a]</ref> 68.5 -SciBERT base <ref type=\"bibr\" target=\"#b0\">[Beltagy et al., 2019]</ref> 65.1 82.5 ATLOP-SciBERT base <ref type=\"b ed on Pytorch. We used cased BERT-base, or RoBERTa-large as the encoder on Do-cRED and SciBERT-base <ref type=\"bibr\" target=\"#b0\">[Beltagy et al., 2019]</ref> on CDR and GDA. We optimize our model wit >. Following ATLOP <ref type=\"bibr\" target=\"#b31\">[Zhou et al., 2021]</ref>, we utilize the SciBERT <ref type=\"bibr\" target=\"#b0\">[Beltagy et al., 2019]</ref> which</p><formula xml:id=\"formula_10\">[1]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cted document-level graph module based on heuristics, structured attention or dependency structures <ref type=\"bibr\" target=\"#b11\">[Peng et al., 2017;</ref><ref type=\"bibr\">Christopoulou et al., 2019; ction between relations, and there have been few studies on RE. On the other hand, as these studies <ref type=\"bibr\" target=\"#b11\">[Nguyen and Grishman, 2015;</ref><ref type=\"bibr\">Shen and Huang, 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s to study the RE problem from a computer vision perspective. In this study, we leveraged the U-Net <ref type=\"bibr\" target=\"#b12\">[Ronneberger et al., 2015]</ref>, which consists of a contracting pat the largest number of entities, counted from all the dataset samples. To this end, we utilize U-Net <ref type=\"bibr\" target=\"#b12\">[Ronneberger et al., 2015]</ref>, which is a famous semantic segmenta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Zhang et al., 2021a]</ref>. Previous works <ref type=\"bibr\" target=\"#b25\">[Zeng et al., 2015;</ref><ref type=\"bibr\" target=\"#b2\">Feng et al., 2018]</ref> focused on identifying relations within a sin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  they need large datasets for training and do not adequately capture person-specific idiosyncrasies <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Personalized models like ours and NVP <ref type=\"bibr\" targ ed based on input audio and a reference frame. Among efforts on full-frame synthesis, Video Rewrite <ref type=\"bibr\" target=\"#b4\">[5]</ref> was a pioneering work.</p><p>It represented speech with phon. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ntirely using a single video of the speaker.</p><p>3D pose normalization: We use a 3D face detector <ref type=\"bibr\" target=\"#b19\">[20]</ref> to obtain the pose and 3D landmarks of the speaker's face   on the audio spectrogram.</p><p>The face in the video is tracked using a 3D face landmark detector <ref type=\"bibr\" target=\"#b19\">[20]</ref>, resulting in 468 facial features, with the depth (z-compo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al viewpoint and lighting variations. Another body of research predicts 3D facial meshes from audio <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta erate 3D face models driven by input audio or text, but do not necessarily aim for photorealism. In <ref type=\"bibr\" target=\"#b37\">[38]</ref>, the authors learn a Hidden Markov Model (HMM) to map Mel-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  audio signal and can synthesize videos of unrestricted length.</p><p>Actor-driven Video Synthesis: <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> present techniques slation: while 'actor-driven' video translation techniques <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> generally require a professional actor to record the entire e-versa. Notably, in contrast to narratordriven techniques <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, our approach for video dubbing does not require a human ac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: consistency of synthesized results. We can train this AR network satisfactorily via Teacher Forcing <ref type=\"bibr\" target=\"#b40\">[41]</ref>, using previous ground truth atlases. The resulting networ ve model, since that would entail a recursion in the network. Instead we follow the Teacher Forcing <ref type=\"bibr\" target=\"#b40\">[41]</ref> paradigm of training the network with the ground truth pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: egress blendshapes of a 3D face using the combined audio-visual embedding from a deep network. VOCA <ref type=\"bibr\" target=\"#b11\">[12]</ref> pre-registers subject-specific 3D mesh models using FLAME . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. These approaches are directly suitable for VR and gaming a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ee' approach does not need video, and can be driven by either recorded audio, TTS, or voice cloning <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Voice controlled Avatars: Our model's blendshapes output   videos into different languages. In conjunction with appropriate video re-timing and voice-cloning <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the resulting videos look fairly convincing. We have empl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of d(\u2022) including the 1 loss, Structural Similarity Loss (SSIM), and Gradient Difference Loss (GDL) <ref type=\"bibr\" target=\"#b26\">[27]</ref> and found SSIM to perform the best.</p><p>Blendshapes deco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cess to ground truth facial information. We show experiments with three talking head datasets: GRID <ref type=\"bibr\" target=\"#b9\">[10]</ref>, TCD-TIMIT <ref type=\"bibr\" target=\"#b15\">[16]</ref> and CR e a LipNet model <ref type=\"bibr\" target=\"#b1\">[2]</ref> pretrained for lip-reading on GRID dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Observations: We report the metrics in Figure <ref t  GRID, CREMA-D and TCD-TIMIT</head><p>For self-reenactment studies we performed experiments on GRID <ref type=\"bibr\" target=\"#b9\">[10]</ref>, TCD TMIT <ref type=\"bibr\" target=\"#b15\">[16]</ref> and CRE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the training video, we stabilize the rigid head motion (see <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>) to provide a registered 3D mesh suitable for training our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erging Convolution and Self-Attention</head><p>For convolution, we mainly focus on the MBConv block <ref type=\"bibr\" target=\"#b26\">[27]</ref> which employs depthwise convolution <ref type=\"bibr\" targe ref> are popular in mobile platforms due to its lower computational cost and smaller parameter size <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>Recent works show that an improved inverted residua t=\"#b26\">[27]</ref>.</p><p>Recent works show that an improved inverted residual bottlenecks (MBConv <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>), which is built u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: which only has a linear complexity w.r.t. the spatial size <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>.</p><p>We briefly e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [14]</ref> alone. More importantly, when pre-trained on large-scale weakly labeled JFT-300M dataset <ref type=\"bibr\" target=\"#b14\">[15]</ref>, ViT achieves comparable results to state-of-the-art (SOTA  dataset JFT-3B<ref type=\"bibr\" target=\"#b25\">[26]</ref> for pre-training, while others use JFT-300M<ref type=\"bibr\" target=\"#b14\">[15]</ref>. See Appendix A.2 for the size details of CoAtNet-5/6/7. \u2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b45\">[46]</ref>, and three common techniques, including stochastic depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>, label smoothing <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n techniques, including stochastic depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>, label smoothing <ref type=\"bibr\" target=\"#b47\">[48]</ref> and weight decay <ref type=\"bibr\" target=\"#b48\">[49]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b45\">[46]</ref>, and three common techniques, including stochastic depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>, label smoothing <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e been the dominating model architecture for computer vision <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= tectures for many computer vision tasks. Traditionally, regular convolutions, such as ResNet blocks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, are popular in large-scale ConvNets; in contrast, depthwise. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b45\">[46]</ref>, and three common techniques, including stochastic depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>, label smoothing <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: _0\">1</ref> only vanilla Transformer layers, one could obtain reasonable performance on ImageNet-1K <ref type=\"bibr\" target=\"#b13\">[14]</ref> alone. More importantly, when pre-trained on large-scale w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l cost and hence are often regarded as an add-on to the ConvNets, similar to squeeze-and-excitation <ref type=\"bibr\" target=\"#b41\">[42]</ref> module. In comparison, after the success of ViT and ResNet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raditional methods like <ref type=\"bibr\" target=\"#b32\">[33]</ref> borrow the idea from PageRank and <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposes to construct label groups by minimizing the diver  the threshold are discarded and the remaining ones are kept as candidates.</p><p>Adapting based on <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we rank our candidate labels following three criteria:</p text. While C t is selected from (\u2022) in our task, this term can also be ignored, which is same with <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Then the ranking score can be defined as</p><formula xml: t=\"#tab_6\">5</ref>.</p><p>The results show that our proposed method (\u00b5 = 0.8, = 0.1), extended from <ref type=\"bibr\" target=\"#b33\">[34]</ref>, outperforms the baseline algorithm for both inverse label. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s, including k-means and spectral clustering, have been proven to be special cases in kernel kmeans <ref type=\"bibr\" target=\"#b30\">[31]</ref>, where semi-supervised constraints can be added as well. T into different evolution tracks. Kernel k-means, which has been proven to be a generalized solution <ref type=\"bibr\" target=\"#b30\">[31]</ref> for both k-means and spectral clustering, is more flexible. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref> and matrix factorization based like NetSMF <ref type=\"bibr\" target=\"#b27\">[28]</ref> and ProNE <ref type=\"bibr\" target=\"#b28\">[29]</ref>, largely focus on the neighborhood characteristics. Neural f> to encode publication contents and then adapt the spectral propagation process proposed in ProNE <ref type=\"bibr\" target=\"#b28\">[29]</ref> to incorporate structure information as shown in Figure <r owing experiments.  For encoding the structure of G, we take spectral propagation proposed in ProNE <ref type=\"bibr\" target=\"#b28\">[29]</ref> to incorporate both neighborhood and global features. For  3 For ProNE, the embedding dimension is 32 and the order of Chebyshev expansion is 10, according to <ref type=\"bibr\" target=\"#b28\">[29]</ref>. 4 For node2vec, the embedding dimension is 32. Walk lengt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hind scientific publications and some previous works share similar objectives with us. For example, <ref type=\"bibr\" target=\"#b20\">[21]</ref> try to detect topic evolution in scientific literature by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  information, concept extraction and taxonomy construction <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> have been studied for identifying field structures. Algorith. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: F, focus on the statistical attributes based on words or phrases and later topic models such as LDA <ref type=\"bibr\" target=\"#b21\">[22]</ref> extract latent features behind documents. While word embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e technique of automatic labeling for clusters has also been well studied previously. Works such as <ref type=\"bibr\" target=\"#b31\">[32]</ref>, use external knowledge sources like Wikipedia or WordNet  s. For example, \"They either rely on pattern-based methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> which extract hierarchical relation leveraging linguistic f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el>(17)</label></formula><p>which is also known as Jaccard index.</p><p>Like other similarity tests <ref type=\"bibr\" target=\"#b43\">[44]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>, we compared th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le for novice researchers and intelligence analysts to entering new fields quickly, as mentioned in <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>To help readers find the main topics among such massi \"bibr\" target=\"#b3\">[4]</ref> have been studied for identifying field structures. Algorithm Roadmap <ref type=\"bibr\" target=\"#b1\">[2]</ref> is also proposed to sketch the dynamics of algorithms in spe ://www.tei-c.org/ns/1.0\"><head>Lack of Ground Truth</head><p>Unlike the comparison of algorithms in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, the relations between references and the citing work are no nd hierarchical clustering to construct topic taxonomy and describe the relations between concepts. <ref type=\"bibr\" target=\"#b1\">[2]</ref> proposes to use algorithm roadmap to sketch the dynamics of  cks, they need to be connected into the evolution roadmap. Similar to algorithm roadmap proposed in <ref type=\"bibr\" target=\"#b1\">[2]</ref> where temporal order is used to connect algorithms, we sort   like DBLP are integrated. This is also a notable difference compared with many prior works such as <ref type=\"bibr\" target=\"#b1\">[2]</ref>, where only papers from the same conference or dataset are c =\"#b41\">42]</ref>, which cluster concepts to induce an implicit hierarchy.\" is a piece of text from <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Co-mentions inside the same boxes, marked as strong co-ment ies potential connections, especially under the context of the citing publication.</p><p>Similar to <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we use pdftotext provided from xpdf 4 to extract the strong. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g work are not explicitly disclosed. The intentions of citations have been studied in several works <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, where intentio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: handle data in unsupervised ways.</p><p>Problems for Importance Identification While previous works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> attempt to quan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: formation should be considered during analyzing relationships. Latest embedding methods on contents <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graphs <ref typ wledge, pre-trained language models including ELMo <ref type=\"bibr\" target=\"#b8\">[9]</ref> and BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, incorporate contexts during encoding word sequences, demons d settings. Sentence-BERT <ref type=\"bibr\" target=\"#b25\">[26]</ref> presents a modification of BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref> by finetuning under siamese and triplet networks, which deri  with existing embedding techniques.</p><p>\u2022 Recent natural language embedding techniques like BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref> need to be fine-tuned on downstream supervised tasks, which  v xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Case Study</head><p>Influential works like BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref> attract discussions from different fields and have profound . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ontents <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graphs <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, either concate ods, including skip-gram based, such as LINE <ref type=\"bibr\" target=\"#b26\">[27]</ref> and node2vec <ref type=\"bibr\" target=\"#b12\">[13]</ref> and matrix factorization based like NetSMF <ref type=\"bibr b13\">[14]</ref> rely on labeled data for supervised training. Skip-gram based methods like node2vec <ref type=\"bibr\" target=\"#b12\">[13]</ref> or matrix factorization based methods mainly produce struc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y, which has been addressed in several very recent works <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Even so, they still heavily rely on the fine-tuning proce questionable for encoding long texts. Those recent works <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref> targeting on long texts also need labeled data for downstr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  information, concept extraction and taxonomy construction <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> have been studied for identifying field structures. Algorith. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: handle data in unsupervised ways.</p><p>Problems for Importance Identification While previous works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> attempt to quan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: their desired topics or related publications, but it is hard for researchers to reason and word2vec <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Two critical methods in BERT, Masked Language Model and Ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: F, focus on the statistical attributes based on words or phrases and later topic models such as LDA <ref type=\"bibr\" target=\"#b21\">[22]</ref> extract latent features behind documents. While word embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y, which has been addressed in several very recent works <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Even so, they still heavily rely on the fine-tuning proce questionable for encoding long texts. Those recent works <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref> targeting on long texts also need labeled data for downstr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: handle data in unsupervised ways.</p><p>Problems for Importance Identification While previous works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> attempt to quan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: F, focus on the statistical attributes based on words or phrases and later topic models such as LDA <ref type=\"bibr\" target=\"#b21\">[22]</ref> extract latent features behind documents. While word embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  The selection of N l labels in one label group also considers Maximal Marginal Relevance criterion <ref type=\"bibr\" target=\"#b39\">[40]</ref>, which shares a similar formula as described above. The ge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rediction, can be traced back to MaskGAN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Skip-thought <ref type=\"bibr\" target=\"#b11\">[12]</ref> as well. Some of such kind of information could be relativ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: their desired topics or related publications, but it is hard for researchers to reason and word2vec <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Two critical methods in BERT, Masked Language Model and Ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tentions of citations have been studied in several works <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, where intentions are categorized by coarse functionalitie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b21\">[22]</ref> extract latent features behind documents. While word embeddings like GloVe <ref type=\"bibr\" target=\"#b22\">[23]</ref> produce transferable representations, enabling the use of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hind scientific publications and some previous works share similar objectives with us. For example, <ref type=\"bibr\" target=\"#b20\">[21]</ref> try to detect topic evolution in scientific literature by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lems recently <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. Compared to traditional neural recommendation models, RL-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ontents <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graphs <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, either concate ods, including skip-gram based, such as LINE <ref type=\"bibr\" target=\"#b26\">[27]</ref> and node2vec <ref type=\"bibr\" target=\"#b12\">[13]</ref> and matrix factorization based like NetSMF <ref type=\"bibr b13\">[14]</ref> rely on labeled data for supervised training. Skip-gram based methods like node2vec <ref type=\"bibr\" target=\"#b12\">[13]</ref> or matrix factorization based methods mainly produce struc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  characteristics. Neural models such as GCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and GraphSage <ref type=\"bibr\" target=\"#b29\">[30]</ref>, can capture more complex features, but the training of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: their desired topics or related publications, but it is hard for researchers to reason and word2vec <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Two critical methods in BERT, Masked Language Model and Ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: handle data in unsupervised ways.</p><p>Problems for Importance Identification While previous works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> attempt to quan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  due to the over-smoothing and gradient vanishing problems <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. A famous solution to mitigate this problem in computer vis g is the concatenation of  embeddings from all the layers. This adapted version is similar to JKNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s on handling web-scale graphs via graph sampling strategy <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, the datasets used in its paper (&gt; 10,000,000 nodes) are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b11\">[12]</ref>, HGT <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and HetSANN <ref type=\"bibr\" target=\"#b16\">[17]</ref> were developed within the last two years.</p><p>Despite va producing mixed results when compared to GAT (See Table <ref type=\"table\" target=\"#tab_3\">3</ref>). <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Attention-based graph neural network for heterogeneous st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n application for Heterogeneous GNNs, but most related works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s on handling web-scale graphs via graph sampling strategy <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, the datasets used in its paper (&gt; 10,000,000 nodes) are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y attention to their potential on heterogeneous graphs (a.k.a., Heterogeneous Information Networks) <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ ressive capacity. Specifically, we perform \ud835\udc3e independent attention mechanisms according to Equation <ref type=\"bibr\" target=\"#b7\">(8)</ref>, and concatenate their results as the final representation.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: label></formula><p>where \ud835\udc50 \ud835\udc56,\ud835\udc5f is a normalization constant and \ud835\udc4a 0 ,\ud835\udc4a \ud835\udc5f s are learnable parameters. <ref type=\"bibr\" target=\"#b4\">[5]</ref>. General attributed multiplex heterogeneous network embeddin  test set.</p><p>\u2022 Amazon is an online purchasing platform. We use the subset preprocessed by GATNE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, containing electronics category products with co-viewing an greater than those in Table <ref type=\"table\" target=\"#tab_4\">4</ref> by a large margin. Most works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s on handling web-scale graphs via graph sampling strategy <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, the datasets used in its paper (&gt; 10,000,000 nodes) are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: label></formula><p>where \ud835\udc50 \ud835\udc56,\ud835\udc5f is a normalization constant and \ud835\udc4a 0 ,\ud835\udc4a \ud835\udc5f s are learnable parameters. <ref type=\"bibr\" target=\"#b4\">[5]</ref>. General attributed multiplex heterogeneous network embeddin  test set.</p><p>\u2022 Amazon is an online purchasing platform. We use the subset preprocessed by GATNE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, containing electronics category products with co-viewing an greater than those in Table <ref type=\"table\" target=\"#tab_4\">4</ref> by a large margin. Most works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ies in an open knowledge graph. In this paper, we mainly survey and benchmark models on this topic. <ref type=\"bibr\" target=\"#b33\">[34]</ref> and KGNN-LS <ref type=\"bibr\" target=\"#b32\">[33]</ref>. KGC 4]</ref>  experiments. We use sum aggregator because it has best overall performance as reported in <ref type=\"bibr\" target=\"#b33\">[34]</ref>.</p><p>C.14 KGAT C.14.1 Recommendation. We set \ud835\udc51 (0) = 64, ter low-frequency nodes. To align items to knowledge graph entities, we adopt the same procedure as <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>.</p></div> <div xm mance deteriorates when we do that, which is also found in <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>  experiments. We use sum aggregator because it has best ove. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nection</head><p>GNNs are hard to be deep due to the over-smoothing and gradient vanishing problems <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. A famous solution. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ore we propose to improve the quality of the embeddings E of candidate negatives. Inspired by \ud835\udc5a\ud835\udc56\ud835\udc65\ud835\udc62\ud835\udc5d <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, we introduce the  of its distribution on Light-GCN. We explore different choices according to the relevant researches <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>:</p><p>\u2022 Beta dist lent distributions in machine learning. Here we fix \ud835\udf07 = 0.5, \ud835\udf0e = 0.1. \u2022 Uniform distribution. MoCHi <ref type=\"bibr\" target=\"#b17\">[18]</ref> serves for self-supervised learning approaches, and propos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g the most promising techniques for this problem has been the usage of collaborative filtering (CF) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, which models users' historical interactions with items to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g the most promising techniques for this problem has been the usage of collaborative filtering (CF) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, which models users' historical interactions with items to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. In doing so, the m et=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>, MixGCF proposes to  improve the quality of negative candidates (Section 3.1). <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> samples the negativ  type=\"bibr\" target=\"#b48\">[49]</ref> samples the negatives based on their PageRank scores and MCNS <ref type=\"bibr\" target=\"#b47\">[48]</ref> re-designs both positive and negative sampling distributio gely unexplored. Notably, early attempts-PinSage <ref type=\"bibr\" target=\"#b48\">[49]</ref> and MCNS <ref type=\"bibr\" target=\"#b47\">[48]</ref>-focus on improving the sampling distributions at the discr each layer \ud835\udc59. Notably, a recent study on negative sampling for graph representation learning (MCNS) <ref type=\"bibr\" target=\"#b47\">[48]</ref> theoretically shows that the expected risk of the optimal  ><p>Dataset Description. In this paper, we evaluate our method on three benchmark datasets: Alibaba <ref type=\"bibr\" target=\"#b47\">[48]</ref>, Yelp2018 <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and A ype=\"bibr\" target=\"#b47\">[48]</ref>, Yelp2018 <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and Amazon <ref type=\"bibr\" target=\"#b47\">[48]</ref>, which are publicly available. Each dataset consists of th ibr\" target=\"#b48\">49]</ref> samples the negative based on the graph information. For example, MCNS <ref type=\"bibr\" target=\"#b47\">[48]</ref> derives a theory to quantify the impact of the negative sa  which vary in terms of domain, size, and sparsity to evaluate our proposed MixGCF.</p><p>\u2022 Alibaba <ref type=\"bibr\" target=\"#b47\">[48]</ref> is collected from the Alibaba online shopping platform. Th authors applied the 10core setting to filter out the nodes with less than 10 interactions. \u2022 Amazon <ref type=\"bibr\" target=\"#b47\">[48]</ref> is first released in <ref type=\"bibr\" target=\"#b24\">[25]</ \"#tab_1\">1</ref>. We follow the same settings described in <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> to split the datasets into training, validation, and testin e previous studies <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> that conduct the sampled metrics, we compute Recall@\ud835\udc41 and N. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>, into account. For instance, GraphRec <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er-item pair (as positive) to unobserved ones (as negative pairs). Take the widely-adopted BPR loss <ref type=\"bibr\" target=\"#b30\">[31]</ref> for example, for each user and one of her positive items,  y in \ud835\udc42(|V | 2 )), the learning objective is usually simplified by negative sampling as the BPR loss <ref type=\"bibr\" target=\"#b30\">[31]</ref>:</p><formula xml:id=\"formula_8\">max \ud835\udc63 + ,\ud835\udc63 \u2212 \u223c\ud835\udc53 S (\ud835\udc62) \ud835\udc43 \ud835\udc62  egative (DNS), GAN-based (IRGAN and AdvIR), and graph-based (MCNS) sampler, as follows:</p><p>\u2022 RNS <ref type=\"bibr\" target=\"#b30\">[31]</ref>: Random negative sampling (RNS) strategy applies uniform d >31]</ref> samples the negative item from a fixed distribution. Bayesian Personalized Ranking (BPR) <ref type=\"bibr\" target=\"#b30\">[31]</ref> applies uniform distribution to sample negative, which is  \" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, we extend the pairwise BPR loss <ref type=\"bibr\" target=\"#b30\">[31]</ref> to \ud835\udc3e-pair loss aiming to enhance the performance of the mo et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>.</p></div> <div xml tead of sampling real items from the data as negative ones <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar ethods roughly fall into four groups.</p><p>Static Sampler <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> samples the negative item from a fixed distribution. Bayesi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cent years, negative sampling has been studied in recommendation task for solving one-class problem <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. To be specific, mos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. To provide personalized information accurately, the recomm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplications as diverse as online shopping <ref type=\"bibr\" target=\"#b52\">[53]</ref>, social network <ref type=\"bibr\" target=\"#b28\">[29]</ref>, advertising <ref type=\"bibr\" target=\"#b14\">[15]</ref>, an  as \ud835\udc53 uniform (\ud835\udc62)) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar evious works take the side information, e.g., social network <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and knowledge graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: studied in recommendation task for solving one-class problem <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. To be specific, most interactions between users and items . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b28\">[29]</ref>, advertising <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and Web search <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Its goal is to provide users with personalized informatio  = {e (\ud835\udc59)</p><formula xml:id=\"formula_9\">\ud835\udc63 \ud835\udc5a } of size \ud835\udc40 \u00d7 (\ud835\udc3f + 1).</formula><p>A very recent study <ref type=\"bibr\" target=\"#b16\">[17]</ref> suggests that recommendation models usually operate on an   studies have attempted to design new sampling distributions for prioritizing informative negatives <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta bedding e \ud835\udc63 \u2212 in Figure <ref type=\"figure\" target=\"#fig_1\">2</ref>, we first follows the convention <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> to select \ud835\udc40 negati \" target=\"#b51\">[52]</ref>: Dynamic negative sampling (DNS) strategy is the stateof-the-art sampler <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, which adaptively  . \u2022 DNS performs as the strongest baseline in most cases, which is consistent with previous studies <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta s to improve the optimization of general recommender systems <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar /1.0\"><head>GAN-based Sampler</head><p>Hard Negative Sampler <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as been supported by empirical studies focusing on different concrete measures of semantic distance <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b;</ref><ref type=\"bibr\" target=\"#b0\">Bryson et al. im-  plementation in KG exploration which has been empirically demonstrated by case or user studies <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b;</ref><ref type=\"bibr\" target=\"#b0\">Bryson et al. by a user study where a concrete measure of semantic distance was implemented based on entity types <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b]</ref>. By comparing pairs of answers, users pref owed depth of search to prevent large subgraphs (i.e., diam &gt; 2d) which are not favored by users <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b]</ref>.</p><p>T opt denotes the optimal answer fo v) min v \u2208V PageRank(v ) \uf8f6 \uf8f8 .<label>(17)</label></formula><p>For semantic distance sd, we followed <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b]</ref> to compute the Jaccard distance between se e cohesive than standard GSTs. It partially justified the advantage of cohesive answers reported in <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b]</ref>. Similar to our efficiency experiment, in  n of cohesive answers, which is not considered in <ref type=\"bibr\">[Cheng and Kharlamov, 2017;</ref><ref type=\"bibr\" target=\"#b3\">Cheng et al., 2017b]</ref>. Our algorithm computes an optimum solution n Algorithm 1. Following common practice <ref type=\"bibr\" target=\"#b9\">[Kacholia et al., 2005;</ref><ref type=\"bibr\" target=\"#b3\">Cheng et al., 2017b]</ref>, we introduce an extra input d representing. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  KG by easily submitting a set of keywords expressing an information need. State-of-the-art methods <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2020]</ref> efficiently find and present an optimum subg e=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref>, aka a group Steiner tree (GST) <ref type=\"bi e=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref>, or as a variant of this problem <ref type=\"b  Recent faster algorithms for the GST problem <ref type=\"bibr\" target=\"#b10\">[Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref> were not compared because they were designed  sider more scalable approximation algorithms <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n ly we have extended it to calculate the total weight of graph elements and their semantic distances <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2021]</ref>. This extended optimization problem is refer  cost. This optimization problem is referred to as the quadratic group Steiner tree problem (QGSTP) <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2021]</ref>. It extends the cost function of the vertex-  solution to QGSTP while the algorithms in <ref type=\"bibr\" target=\"#b0\">[Bryson et al., 2020;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2021]</ref> cannot guarantee to find an optimum solution. is the main concern. For such applications, one may consider more scalable approximation algorithms <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref>, aka a group Steiner tree (GST) <ref type=\"bibr\" target=\"#b8\">[Ihler, 1991]</ref>. The underlying assumption is: an aggregation of s ite hits(k i ) as K i for 1 \u2264 i \u2264 g. We call K i keyword vertices.</p><p>Similar to the GST problem <ref type=\"bibr\" target=\"#b8\">[Ihler, 1991]</ref>, given G = V, E we define a feasible answer to Q a et=\"#b13\">[Shi et al., 2021]</ref>. It extends the cost function of the vertex-weighted GST problem <ref type=\"bibr\" target=\"#b8\">[Ihler, 1991;</ref><ref type=\"bibr\" target=\"#b9\">Klein and Ravi, 1995]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f entities <ref type=\"bibr\">[Cheng, 2020;</ref><ref type=\"bibr\" target=\"#b11\">Li et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Cheng et al., 2020]</ref>. The above transformationbased methods are t l KG that has been popularly used for evaluating keyword querying. We reused 50 queries provided by <ref type=\"bibr\" target=\"#b5\">[Coffman and Weaver, 2014]</ref> but removed those containing unmatcha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he keyword query into a formal query and then execute the formal query over a standard query engine <ref type=\"bibr\" target=\"#b14\">[Tran et al., 2009;</ref><ref type=\"bibr\" target=\"#b12\">Pound et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: While various AI-empowered methods have been devised to support this task, such as KG summarization <ref type=\"bibr\" target=\"#b1\">[Cheng et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Cheng et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: While various AI-empowered methods have been devised to support this task, such as KG summarization <ref type=\"bibr\" target=\"#b1\">[Cheng et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Cheng et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b2\">Cheng et al., 2017a]</ref> and relevance-based entity recommendation <ref type=\"bibr\" target=\"#b7\">[Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Zhou et al., 202 vertices with the largest degrees and all the edges between them. We reused 429 queries provided by <ref type=\"bibr\" target=\"#b7\">[Hasibi et al., 2017]</ref> but removed those containing unmatchable k. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref>, aka a group Steiner tree (GST) <ref type=\"bibr\" target=\"#b8\">[Ihler, 1991]</ref>. The underlying assumption is: an aggregation of s ite hits(k i ) as K i for 1 \u2264 i \u2264 g. We call K i keyword vertices.</p><p>Similar to the GST problem <ref type=\"bibr\" target=\"#b8\">[Ihler, 1991]</ref>, given G = V, E we define a feasible answer to Q a et=\"#b13\">[Shi et al., 2021]</ref>. It extends the cost function of the vertex-weighted GST problem <ref type=\"bibr\" target=\"#b8\">[Ihler, 1991;</ref><ref type=\"bibr\" target=\"#b9\">Klein and Ravi, 1995]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the keywords in a query as an optimum answer <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020 ee that contains all the keywords in a query <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020 sity (L1) and four universities (L4). We generated 200 synthetic queries following the procedure in <ref type=\"bibr\" target=\"#b10\">[Li et al., 2016]</ref>. Specifically, we varied two parameters: the  ices in an answer but is unaware of semantic distance. Recent faster algorithms for the GST problem <ref type=\"bibr\" target=\"#b10\">[Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the keywords in a query as an optimum answer <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020 ee that contains all the keywords in a query <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020 sity (L1) and four universities (L4). We generated 200 synthetic queries following the procedure in <ref type=\"bibr\" target=\"#b10\">[Li et al., 2016]</ref>. Specifically, we varied two parameters: the  ices in an answer but is unaware of semantic distance. Recent faster algorithms for the GST problem <ref type=\"bibr\" target=\"#b10\">[Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: studies of \"cohesiveness\" in graph search and exploration <ref type=\"bibr\">[Dass et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Zhu et al., 2018]</ref>, their definitions are orthogonal to ours.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: attern, e.g., exploring relationships between a set of entities <ref type=\"bibr\">[Cheng, 2020;</ref><ref type=\"bibr\" target=\"#b11\">Li et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Cheng et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: support this task, such as KG summarization <ref type=\"bibr\" target=\"#b1\">[Cheng et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Cheng et al., 2017a]</ref> and relevance-based entity recommendation <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> and relevance-based entity recommendation <ref type=\"bibr\" target=\"#b7\">[Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Zhou et al., 2020]</ref>, the most commonly adopted user interface is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a minimum-weight connected subgraph that contains all the keywords in a query as an optimum answer <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 201 mulated as a GST problem to extract a minimum-weight tree that contains all the keywords in a query <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 201 ad><p>We are the first to propose an exact algorithm for QGSTP. We compared our algorithm with DPBF <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007]</ref>, a state-of-the-art exact algorithm for the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he keyword query into a formal query and then execute the formal query over a standard query engine <ref type=\"bibr\" target=\"#b14\">[Tran et al., 2009;</ref><ref type=\"bibr\" target=\"#b12\">Pound et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f entities <ref type=\"bibr\">[Cheng, 2020;</ref><ref type=\"bibr\" target=\"#b11\">Li et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Cheng et al., 2020]</ref>. The above transformationbased methods are t l KG that has been popularly used for evaluating keyword querying. We reused 50 queries provided by <ref type=\"bibr\" target=\"#b5\">[Coffman and Weaver, 2014]</ref> but removed those containing unmatcha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hi et al., 2020]</ref>, or as a variant of this problem <ref type=\"bibr\">[Kargar and An, 2011;</ref><ref type=\"bibr\" target=\"#b9\">Le et al., 2014;</ref><ref type=\"bibr\" target=\"#b15\">Yang et al., 2019 e cost function of the vertex-weighted GST problem <ref type=\"bibr\" target=\"#b8\">[Ihler, 1991;</ref><ref type=\"bibr\" target=\"#b9\">Klein and Ravi, 1995]</ref> by introducing a quadratic term sd(v i , v longer paths.</p><p>4.2 Algorithm B 3 F B 3 F is detailed in Algorithm 1. Following common practice <ref type=\"bibr\" target=\"#b9\">[Kacholia et al., 2005;</ref><ref type=\"bibr\" target=\"#b3\">Cheng et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the keywords in a query as an optimum answer <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020 ee that contains all the keywords in a query <ref type=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020 sity (L1) and four universities (L4). We generated 200 synthetic queries following the procedure in <ref type=\"bibr\" target=\"#b10\">[Li et al., 2016]</ref>. Specifically, we varied two parameters: the  ices in an answer but is unaware of semantic distance. Recent faster algorithms for the GST problem <ref type=\"bibr\" target=\"#b10\">[Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: While various AI-empowered methods have been devised to support this task, such as KG summarization <ref type=\"bibr\" target=\"#b1\">[Cheng et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Cheng et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, dependency-based cores <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, dataflow inspired operands of consumer instructions depending on the same register with a linked list of SSR pointers <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> (dashed arrows in  zed wakeup issue of SSR and boosting performance by composing multiple execution units (scaling-up) <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta w></table></figure> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\"><ref type=\"bibr\" target=\"#b11\">[12]</ref> compares Forwardflow against a baseline OoO core with 32-e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is only 0.6%. Similar performance result on the OoO core is reported on the original paper of NoSQ <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Considering that the performance difference is limited, w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, dataflow inspired techniques <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, and in-order base truction format to express dataflow dependencies of programs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. To gain benefit on units (scaling-up) <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. This paper opens another way that enhancing performance by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esses the destination pointer array and retrieves the successor of I5's destination register, I7.R3 <ref type=\"bibr\" target=\"#b5\">( 6 )</ref>, which aims to speculatively wake up I7.src3 in the next c g, dataflow architectures adopt new instruction format to express dataflow dependencies of programs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esses the destination pointer array and retrieves the successor of I5's destination register, I7.R3 <ref type=\"bibr\" target=\"#b5\">( 6 )</ref>, which aims to speculatively wake up I7.src3 in the next c g, dataflow architectures adopt new instruction format to express dataflow dependencies of programs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpare the performance of Omegaflow with NoSQ against Omegaflow with a traditional LSQ and store set <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The average performance improvement is only 0.6%. Similar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpare the performance of Omegaflow with NoSQ against Omegaflow with a traditional LSQ and store set <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The average performance improvement is only 0.6%. Similar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  energy efficiency <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is only 0.6%. Similar performance result on the OoO core is reported on the original paper of NoSQ <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Considering that the performance difference is limited, w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>The Transformer <ref type=\"bibr\" target=\"#b45\">[45]</ref> is well acknowledged as the most powerful neural network i f>.</p><p>Transformer. The Transformer architecture consists of a composition of Transformer layers <ref type=\"bibr\" target=\"#b45\">[45]</ref>.</p><p>Each Transformer layer has two parts: a self-attent sequential data, one can either give each position an embedding (i.e., absolute positional encoding <ref type=\"bibr\" target=\"#b45\">[45]</ref>) as the input or encode the relative distance of any two p r. Graphormer is built upon the original implementation of classic Transformer encoder described in <ref type=\"bibr\" target=\"#b45\">[45]</ref>. In addition, we apply the layer normalization (LN) before. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ode, its associated edges' features will be used together with the node features in the aggregation <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b49\">49,</ref><ref type=\"bibr\" ta  we also find an interesting connection between using self-attention and the virtual node heuristic <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" ta section, various graph pooling functions are proposed to represent the graph embedding. Inspired by <ref type=\"bibr\" target=\"#b15\">[15]</ref>, in Graphormer, we add a special node called [VNode] to th /ref> and GIN <ref type=\"bibr\" target=\"#b49\">[49]</ref>, and their variants with virtual node (-VN) <ref type=\"bibr\" target=\"#b15\">[15]</ref>. They achieve the state-of-the-art valid and test mean abs rget=\"#b49\">[49]</ref> 3.8M 0.1203 0.1537 (0.1536*) GCN-VN <ref type=\"bibr\" target=\"#b26\">[26,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref> 4.9M 0.1225 0.1485 (0.1510*) GIN-VN <ref type=\"bibr\" target arget=\"#b15\">15]</ref> 4.9M 0.1225 0.1485 (0.1510*) GIN-VN <ref type=\"bibr\" target=\"#b49\">[49,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref> 6.7M 0.1150 0.1395 (0.1396*) GINE-VN <ref type=\"bibr\" targe target=\"#b15\">15]</ref> 6.7M 0.1150 0.1395 (0.1396*) GINE-VN <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref> 13.2M 0.1248 0.1430 DeeperGCN-VN <ref type=\"bibr\" target=\"# \" target=\"#b15\">15]</ref> 13.2M 0.1248 0.1430 DeeperGCN-VN <ref type=\"bibr\" target=\"#b30\">[30,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref> 25.5M 0.1059 0.1398 GT <ref type=\"bibr\" target=\"#b13\">[13]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: owledged as the most powerful neural network in modelling sequential data, such as natural language <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b35\">35,</ref><ref type=\"bibr\" ta  of the entire graph h G would be the node feature of [VNode] in the final layer. In the BERT model <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b35\">35]</ref>, there is a simila. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the input or encode the relative distance of any two positions (i.e., relative positional encoding <ref type=\"bibr\" target=\"#b41\">[41,</ref><ref type=\"bibr\" target=\"#b43\">43]</ref>) in the Transforme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (LN) before the multi-head self-attention (MHA) and the feed-forward blocks (FFN) instead of after <ref type=\"bibr\" target=\"#b48\">[48]</ref>. This modification has been unanimously adopted by all cur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the input or encode the relative distance of any two positions (i.e., relative positional encoding <ref type=\"bibr\" target=\"#b41\">[41,</ref><ref type=\"bibr\" target=\"#b43\">43]</ref>) in the Transforme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion scheme on graph is proposed in <ref type=\"bibr\" target=\"#b53\">[53]</ref>; it has been proved in <ref type=\"bibr\" target=\"#b32\">[32]</ref> that adopting distance encoding (i.e., one-hot feature of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Graphormer, AP (%) DeeperGCN-VN+FLAG <ref type=\"bibr\" target=\"#b30\">[30]</ref> 5.6M 28.42\u00b10.43 DGN <ref type=\"bibr\" target=\"#b2\">[2]</ref> 6.7M 28.85\u00b10.30 GINE-VN <ref type=\"bibr\" target=\"#b5\">[5]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b23\">23,</ref><ref type=\"bibr\" target=\"#b47\">47,</ref><ref type=\"bibr\" target=\"#b55\">55,</ref><ref type=\"bibr\" target=\"#b42\">42,</ref><ref type=\"bibr\" target=\"#b13\">13]</ref>. Therefore, it is s more related to our Graphormer. For example, several parts of the transformer layer are modified in <ref type=\"bibr\" target=\"#b42\">[42]</ref>, including an additional GNN employed in attention sub-lay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GNNs, e.g., Weisfeiler-Lehman-PE (WL-PE) <ref type=\"bibr\" target=\"#b55\">[55]</ref> and Laplacian PE <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. We report the perfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ortant factors in predicting the trend of a social network <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b37\">37]</ref>. Such information is neglected in the current attention cal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GNNs, e.g., Weisfeiler-Lehman-PE (WL-PE) <ref type=\"bibr\" target=\"#b55\">[55]</ref> and Laplacian PE <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. We report the perfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o have a huge number of followers are important factors in predicting the trend of a social network <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b37\">37]</ref>. Such information . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GNNs, e.g., Weisfeiler-Lehman-PE (WL-PE) <ref type=\"bibr\" target=\"#b55\">[55]</ref> and Laplacian PE <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. We report the perfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aborating the works by adapting attention mechanism to GNNs<ref type=\"bibr\" target=\"#b33\">[33,</ref><ref type=\"bibr\" target=\"#b54\">54,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aborating the works by adapting attention mechanism to GNNs<ref type=\"bibr\" target=\"#b33\">[33,</ref><ref type=\"bibr\" target=\"#b54\">54,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the input or encode the relative distance of any two positions (i.e., relative positional encoding <ref type=\"bibr\" target=\"#b41\">[41,</ref><ref type=\"bibr\" target=\"#b43\">43]</ref>) in the Transforme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s natural language <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b35\">35,</ref><ref type=\"bibr\" target=\"#b6\">6]</ref> and speech <ref type=\"bibr\" target=\"#b17\">[17]</ref>. Model v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b36\">36]</ref> and programming language <ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\" target=\"#b40\">40]</ref>. However, to the be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the input or encode the relative distance of any two positions (i.e., relative positional encoding <ref type=\"bibr\" target=\"#b41\">[41,</ref><ref type=\"bibr\" target=\"#b43\">43]</ref>) in the Transforme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b36\">36]</ref> and programming language <ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fore, in self-supervised tasks, the ground truth labels are not available. Motivated by DeepCluster <ref type=\"bibr\" target=\"#b39\">[40]</ref> which uses the cluster assignments of learned features as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 1\">[42]</ref>. In this work, we employ the covariance matrix adaptation evolution strategy (CMA-ES) <ref type=\"bibr\" target=\"#b42\">[43]</ref>, a state-of-the-art optimizer for continuous black-box fun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s w.r.t. hyperparameters, which have been widely used in solving bi-level problems in meta learning <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. To obtain meta-gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b26\">[27]</ref><ref type=\"bibr\" target=\"#b27\">[28]</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref>. Specifically, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ure search and loss function search. Among them, our work is highly related to loss function search <ref type=\"bibr\" target=\"#b32\">[33]</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> such as DGI <ref type=\"bibr\" target=\"#b12\">[13]</ref>, PAR/CLU <ref type=\"bibr\" target=\"#b13\">[14]</ref> and MVG in graph neural networks to provide additional supervision <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t asks, we use 5 such tasks including 1 contrastive learning method and 4 predictive methods -(1) DGI <ref type=\"bibr\" target=\"#b12\">[13]</ref>: it is a contrastive learning method maximizing the mutual en calculating pseudo-homophily, we set the number of clusters to 5 for all datasets. Following DGI <ref type=\"bibr\" target=\"#b12\">[13]</ref> and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b13\">[14]</ref>, it predicts partition labels from Metis graph partition <ref type=\"bibr\" target=\"#b46\">[47]</ref>; (3) PAR <ref type=\"bibr\" target=\"#b13\">[14]</ref>, it pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s function search <ref type=\"bibr\" target=\"#b32\">[33]</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" target=\"#b35\">[36]</ref>. However, these m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b3\">4]</ref>, node clustering <ref type=\"bibr\" target=\"#b4\">[5]</ref>, recommender systems <ref type=\"bibr\" target=\"#b5\">[6]</ref> and drug discovery <ref type=\"bibr\" target=\"#b6\">[7]</ref>.< \"#b51\">[52]</ref>, friend ranking <ref type=\"bibr\" target=\"#b52\">[53]</ref> and item recommendation <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, which impact many e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b3\">4]</ref>, node clustering <ref type=\"bibr\" target=\"#b4\">[5]</ref>, recommender systems <ref type=\"bibr\" target=\"#b5\">[6]</ref> and drug discovery <ref type=\"bibr\" target=\"#b6\">[7]</ref>.< \"#b51\">[52]</ref>, friend ranking <ref type=\"bibr\" target=\"#b52\">[53]</ref> and item recommendation <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, which impact many e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of SSL tasks including DGI, PAR, CLU, PAIRDIS <ref type=\"bibr\" target=\"#b15\">[16]</ref> and PAIRSIM <ref type=\"bibr\" target=\"#b16\">[17]</ref> perform over 3 datasets. Their node clustering and node cl b13\">[14]</ref>, it predicts clustered labels from k-means clustering on node features; (4) PAIRSIM <ref type=\"bibr\" target=\"#b16\">[17]</ref>, it predicts pairwise feature similarity between node pair. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b13\">[14]</ref>, it predicts partition labels from Metis graph partition <ref type=\"bibr\" target=\"#b46\">[47]</ref>; (3) PAR <ref type=\"bibr\" target=\"#b13\">[14]</ref>, it pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s function search <ref type=\"bibr\" target=\"#b32\">[33]</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" target=\"#b35\">[36]</ref>. However, these m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ther SSL tasks.</p><p>We perform experiments on 7 real-world datasets widely used in the literature <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta o use the training and test data. For WikiCS <ref type=\"bibr\" target=\"#b48\">[49]</ref> and Citeseer <ref type=\"bibr\" target=\"#b47\">[48]</ref>, we use the public data splits provided by the authors. Fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: also been recently proposed for use in large-scale social platforms for tasks including forecasting <ref type=\"bibr\" target=\"#b51\">[52]</ref>, friend ranking <ref type=\"bibr\" target=\"#b52\">[53]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s function search <ref type=\"bibr\" target=\"#b32\">[33]</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" target=\"#b35\">[36]</ref>. However, these m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b12\">[13]</ref>, PAR/CLU <ref type=\"bibr\" target=\"#b13\">[14]</ref> and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Given graph and node attribute data, they construct prete =\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b26\">[27]</ref><ref type=\"bibr\" t efully mediate their influence. We also note that (1) the recent contrastive learning method, MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref>, needs to deal with a dense diffusion matrix and is prohib ikiCS, Citeseer and CoraFull. To demonstrate the effectiveness of the proposed framework, we follow <ref type=\"bibr\" target=\"#b14\">[15]</ref> and evaluate all methods on two different downstream tasks f clusters to 5 for all datasets. Following DGI <ref type=\"bibr\" target=\"#b12\">[13]</ref> and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we we use a simple one-layer GCN <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b3\">4]</ref>, node clustering <ref type=\"bibr\" target=\"#b4\">[5]</ref>, recommender systems <ref type=\"bibr\" target=\"#b5\">[6]</ref> and drug discovery <ref type=\"bibr\" target=\"#b6\">[7]</ref>.< \"#b51\">[52]</ref>, friend ranking <ref type=\"bibr\" target=\"#b52\">[53]</ref> and item recommendation <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, which impact many e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b54\">[55]</ref>. Moreover, GNNs can also suffer from degree-related biases <ref type=\"bibr\" target=\"#b55\">[56]</ref>. Self-supervised learning (SSL) is often used to learn hig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Graph neural networks (GNNs) are powerful tools for extracting useful information from graph data <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ther SSL tasks.</p><p>We perform experiments on 7 real-world datasets widely used in the literature <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta o use the training and test data. For WikiCS <ref type=\"bibr\" target=\"#b48\">[49]</ref> and Citeseer <ref type=\"bibr\" target=\"#b47\">[48]</ref>, we use the public data splits provided by the authors. Fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i-task self-supervised learning for computer vision, most of them assign equal weights to SSL tasks <ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" t ulti-Task Self-Supervised Learning. Our work is also related to multi-task self-supervised learning <ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st and transferable. Recently, IB has been applied to GNNs <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>. But IB needs the knowledge of the downstream tasks that ma tion bottleneck has applied to learn graph representations <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>. Specifically, the objective of graph information bottlenec IB robust to adverserial attack and strongly transferrable <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>.</p><p>Unfortunately, GIB requires the knowledge of the cla. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ncoder learnt by IB tends to be more robust and transferable. Recently, IB has been applied to GNNs <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>. But IB needs the   representation is. Recently, the information bottleneck has applied to learn graph representations <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>. Specifically, the ormation also makes GNNs trained w.r.t. GIB robust to adverserial attack and strongly transferrable <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>.</p><p>Unfortunate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d-to-end training <ref type=\"bibr\" target=\"#b10\">[10]</ref><ref type=\"bibr\" target=\"#b11\">[11]</ref><ref type=\"bibr\" target=\"#b12\">[12]</ref><ref type=\"bibr\" target=\"#b13\">[13]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b13\">[13]</ref><ref type=\"bibr\" target=\"#b14\">[14]</ref><ref type=\"bibr\" target=\"#b15\">[15]</ref><ref type=\"bibr\" target=\"#b16\">[16]</ref>, where a large number of task-specific labels are needed.  e but crucial structural information, thus failing in many tasks including node-role classification <ref type=\"bibr\" target=\"#b16\">[16,</ref><ref type=\"bibr\" target=\"#b35\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b13\">[13]</ref><ref type=\"bibr\" target=\"#b14\">[14]</ref><ref type=\"bibr\" target=\"#b15\">[15]</ref><ref type=\"bibr\" target=\"#b16\">[16]</ref>, where a large number of task-specific labels are needed.  e but crucial structural information, thus failing in many tasks including node-role classification <ref type=\"bibr\" target=\"#b16\">[16,</ref><ref type=\"bibr\" target=\"#b35\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: labels are needed <ref type=\"bibr\" target=\"#b18\">[18,</ref><ref type=\"bibr\" target=\"#b20\">[20]</ref><ref type=\"bibr\" target=\"#b21\">[21]</ref><ref type=\"bibr\" target=\"#b22\">[22]</ref><ref type=\"bibr\" t e expected to be reconstructed based on the output of GNNs <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b21\">21,</ref><ref type=\"bibr\" target=\"#b36\">36]</ref>. Experiments showed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ween AD-GCL and JOAO for the tasks investigated in Sec. 5 is given in Appendix H.</p><p>Tian et al. <ref type=\"bibr\" target=\"#b71\">[71]</ref> has recently proposed the InfoMin principle that shares so rased in our notation, the optimal augmentation T IM (G) given by InfoMin (called the sweet spot in <ref type=\"bibr\" target=\"#b71\">[71]</ref>) needs to satisfy</p><formula xml:id=\"formula_15\">I(t IM (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b59\">59,</ref><ref type=\"bibr\" target=\"#b60\">60,</ref><ref type=\"bibr\" target=\"#b63\">[63]</ref><ref type=\"bibr\" target=\"#b64\">[64]</ref><ref type=\"bibr\" target=\"#b65\">[65]</ref> was initially pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ole classification <ref type=\"bibr\" target=\"#b16\">[16,</ref><ref type=\"bibr\" target=\"#b35\">35,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> and graph classific. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: has recently shown great potential in many applications in biochemistry, physics and social science <ref type=\"bibr\" target=\"#b1\">[1]</ref><ref type=\"bibr\" target=\"#b2\">[2]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[4,</ref><ref type=\"bibr\" target=\"#b5\">5]</ref>, have become the almost de facto encoders for GRL <ref type=\"bibr\" target=\"#b6\">[6]</ref><ref type=\"bibr\" target=\"#b7\">[7]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inciple to match traditional network-embedding requirement <ref type=\"bibr\" target=\"#b32\">[32]</ref><ref type=\"bibr\" target=\"#b33\">[33]</ref><ref type=\"bibr\" target=\"#b34\">[34]</ref><ref type=\"bibr\" t br\" target=\"#b76\">[76]</ref><ref type=\"bibr\" target=\"#b77\">[77]</ref> and network embedding methods <ref type=\"bibr\" target=\"#b33\">[33,</ref><ref type=\"bibr\" target=\"#b34\">34,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b39\">[39,</ref><ref type=\"bibr\" target=\"#b59\">59,</ref><ref type=\"bibr\" target=\"#b60\">60,</ref><ref type=\"bibr\" target=\"#b63\">[63]</ref><ref type=\"bibr\" target=\"#b64\">[64]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b60\">60,</ref><ref type=\"bibr\" target=\"#b63\">[63]</ref><ref type=\"bibr\" target=\"#b64\">[64]</ref><ref type=\"bibr\" target=\"#b65\">[65]</ref> was initially proposed to train CNNs for image representat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dding requirement <ref type=\"bibr\" target=\"#b32\">[32]</ref><ref type=\"bibr\" target=\"#b33\">[33]</ref><ref type=\"bibr\" target=\"#b34\">[34]</ref><ref type=\"bibr\" target=\"#b35\">[35]</ref>, where the edges  br\" target=\"#b77\">[77]</ref> and network embedding methods <ref type=\"bibr\" target=\"#b33\">[33,</ref><ref type=\"bibr\" target=\"#b34\">34,</ref><ref type=\"bibr\" target=\"#b78\">78,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[4,</ref><ref type=\"bibr\" target=\"#b5\">5]</ref>, have become the almost de facto encoders for GRL <ref type=\"bibr\" target=\"#b6\">[6]</ref><ref type=\"bibr\" target=\"#b7\">[7]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  use datasets from Open Graph Benchmark (OGB) <ref type=\"bibr\" target=\"#b52\">[52]</ref>, TU Dataset <ref type=\"bibr\" target=\"#b73\">[73]</ref> and ZINC <ref type=\"bibr\" target=\"#b74\">[74]</ref> for gra e evaluate AD-GCL on semi-supervised learning for graph classification on the benchmark TU datasets <ref type=\"bibr\" target=\"#b73\">[73]</ref>. We follow the setting in <ref type=\"bibr\" target=\"#b24\">[ :</head><label>3</label><figDesc>Semi-supervised learning performance with 10% labels on TU datasets<ref type=\"bibr\" target=\"#b73\">[73]</ref> (10-Fold Accuracy (%)\u00b1 std over 5 runs). Bold/Bold \u22c6 indic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  [0, 1] and utilize the Gumbel-Max reparametrization trick <ref type=\"bibr\" target=\"#b56\">[56,</ref><ref type=\"bibr\" target=\"#b57\">57]</ref>. Specifically, p e = Sigmoid((log \u03b4 \u2212 log(1 \u2212 \u03b4) + \u03c9 e )/\u03c4 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b39\">[39,</ref><ref type=\"bibr\" target=\"#b59\">59,</ref><ref type=\"bibr\" target=\"#b60\">60,</ref><ref type=\"bibr\" target=\"#b63\">[63]</ref><ref type=\"bibr\" target=\"#b64\">[64]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cs and social science <ref type=\"bibr\" target=\"#b1\">[1]</ref><ref type=\"bibr\" target=\"#b2\">[2]</ref><ref type=\"bibr\" target=\"#b3\">[3]</ref>. Graph neural networks (GNNs), inheriting the power of neura la><p>For design choices regarding aggregation, update and pooling functions we refer the reader to <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b9\">[9]</ref>. GNNs have been mostly studied in cases with supervised end-to-end training <ref type=\"bibr\" target=\"#b10\">[10]</ref><ref type=\"bibr\" target=\"#b11\">[11]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y information, i.e. the three-dimensional (3D) spatial structure of a molecule. Although some works <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> take the atomic di  extend graph attention mechanism in order to learn aggregation weights. Furthermore, several works <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> start to take the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  tasks to pretrain GeoGNN. For the geometry-level tasks, we utilize the Merck molecular force field <ref type=\"bibr\" target=\"#b19\">[20]</ref> function from the RDKit <ref type=\"foot\" target=\"#foot_1\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: v <ref type=\"bibr\" target=\"#b33\">[34]</ref>, ESOL <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and Lipo <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we use Root Mean Square Error (RMSE), and for QM7 <ref ty cules in water. The values are all obtained through molecular dynamics simulations. \u2022 Lipophilicity <ref type=\"bibr\" target=\"#b14\">[15]</ref> is collected from the ChEMBL database <ref type=\"bibr\" tar ns. \u2022 Lipophilicity <ref type=\"bibr\" target=\"#b14\">[15]</ref> is collected from the ChEMBL database <ref type=\"bibr\" target=\"#b14\">[15]</ref>, containing the experimental results of the octanol or wat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, including both classification and regression tasks <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar rovides the qualitative (binary label) binding results on inhibitors of human \u03b2-secretase 1. \u2022 BBBP <ref type=\"bibr\" target=\"#b32\">[33]</ref> is a dataset that contains molecules with measured permeai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ith various competitive baselines. D-MPNN <ref type=\"bibr\" target=\"#b55\">[56]</ref> and AttentiveFP <ref type=\"bibr\" target=\"#b53\">[54]</ref> are the GNNs without pre-training, while N-Gram <ref type= esigned for molecular representation, D-MPNN <ref type=\"bibr\" target=\"#b55\">[56]</ref>, AttentiveFP <ref type=\"bibr\" target=\"#b53\">[54]</ref>, and GTransformer <ref type=\"bibr\" target=\"#b39\">[40]</ref olecules by taking each atom as a node and each chemical bond as an edge. For example, Attentive FP <ref type=\"bibr\" target=\"#b53\">[54]</ref> proposes to extend graph attention mechanism in order to l .852 (0.053) 0.919 (0.030) 0.897 (0.040) 0.632 (0.023) 0.826 (0.023) 0.718 (0.011) 0.807 AttentiveFP<ref type=\"bibr\" target=\"#b53\">[54]</ref> 0.863 (0.015) 0.908 (0.050) 0.933 (0.020) 0.605 (0.060) 0.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> take SMILES strings <ref type=\"bibr\" target=\"#b50\">[51]</ref> that describe the molecules by strings as inputs, and leve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to obtain the graph representation. Hyper-parameters and Evaluation Metrics. We use Adam Optimizer <ref type=\"bibr\" target=\"#b25\">[26]</ref> with learning rate of 0.001 for all our models. For each d  vocabulary size for discretizing atomic distance as 30, and the mask ratio as 0.15. Adam optimizer <ref type=\"bibr\" target=\"#b25\">[26]</ref> with learning rate of 0.005 is utilized and we train 20 ep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [12]</ref>, are commonly used for molecular representations by traditional machine learning methods <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, we use Root Mean Square Error (RMSE), and for QM7 <ref type=\"bibr\" target=\"#b2\">[3]</ref>, QM8 <ref type=\"bibr\" target=\"#b35\">[36]</ref>, and QM9 <ref type=\"bibr\" target=\"#b40\">[41]</ref>, we use as HOMO and LUMO determined by ab-initio density function theory (DFT), and atomization energy.\u2022 QM8<ref type=\"bibr\" target=\"#b35\">[36]</ref> uses a variety of quantum mechanics methods to calculate t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to obtain the graph representation. Hyper-parameters and Evaluation Metrics. We use Adam Optimizer <ref type=\"bibr\" target=\"#b25\">[26]</ref> with learning rate of 0.001 for all our models. For each d  vocabulary size for discretizing atomic distance as 30, and the mask ratio as 0.15. Adam optimizer <ref type=\"bibr\" target=\"#b25\">[26]</ref> with learning rate of 0.005 is utilized and we train 20 ep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hods used in molecular property prediction datasets, including random splitting, scaffold splitting <ref type=\"bibr\" target=\"#b1\">[2]</ref> and random scaffold splitting. Scaffold splitting and random. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\">17,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" tar , recent trackers use either one or multi-stage BBR (e.g., <ref type=\"bibr\" target=\"#b16\">[18,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\">54, on (mBBR). As a single step of regression usually works well (as observed in recent studies as well <ref type=\"bibr\" target=\"#b30\">[32,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref>), mBBR works effic chanism <ref type=\"bibr\" target=\"#b16\">[18,</ref><ref type=\"bibr\">54]</ref> and deeper architecture <ref type=\"bibr\" target=\"#b30\">[32,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>.</p><p>Our work is MP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, C-RPN <ref type=\"bibr\" target=\"#b16\">[18]</ref>, SiamDW < ref> with 0.523 success score, we obtain considerable gains by 4.8%. Our MART outperforms SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref> by 7.5% in success plot. In addition, compared to multi-st re our MART to 9 state-of-the-art trackers (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref>, SiamFC <ref oposed MART to 12 state-ofthe-art trackers (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref>, C-RPN <ref  ef> (c), our approach achieves competitive result with success score of 0.678 compared to SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref> and DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>. In compa nificantly outperforms ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref> with EAO of 0.292 and SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref> with EAO of 0.285.</p></div> <div xmlns=\"http://www.tei-c.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\">17,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" tar th many extensions <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b56\">58]</ref>. Notably, by integr i-stage BBR (e.g., <ref type=\"bibr\" target=\"#b16\">[18,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\">54,</ref><ref type=\"bibr\" target=\"#b60\">62,  usually works well (as observed in recent studies as well <ref type=\"bibr\" target=\"#b30\">[32,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref>), mBBR works efficiently most of time, with very few iterat tegrating with region proposal network (RPN) <ref type=\"bibr\" target=\"#b43\">[45]</ref>, the work of <ref type=\"bibr\" target=\"#b31\">[33]</ref> greatly improves Siamese tracker in dealing with scale cha  type=\"bibr\" target=\"#b16\">[18]</ref>, DaSiamRPN <ref type=\"bibr\" target=\"#b61\">[63]</ref>, SiamRPN <ref type=\"bibr\" target=\"#b31\">[33]</ref>, Grad-Net <ref type=\"bibr\" target=\"#b33\">[35]</ref>, SA-Si. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e displacement attention (CADA) module and an RNN. The RNN component, implemented based on Con-vGRU <ref type=\"bibr\" target=\"#b0\">[1]</ref>, aims at long-term representation by learning to aggregate t \"#b38\">[40,</ref><ref type=\"bibr\" target=\"#b58\">60]</ref>.</p><p>Our approach is closely related to <ref type=\"bibr\" target=\"#b0\">[1]</ref> that proposes convolutional gated recurrent unit networks (C  frame to frame. Considering the importance of spatial information in 2D images/videos, the work of <ref type=\"bibr\" target=\"#b0\">[1]</ref> replaces linear product operation in GRU-RNN <ref type=\"bibr aSOT and TC-128 and performs favorably against many trackers on OTB-2015. adding RNN (i.e., ConvGRU <ref type=\"bibr\" target=\"#b0\">[1]</ref>) for temporal representation, the performance is improved to el>3</label><figDesc>Figure 3. Comparison between feature aggregation in existing RNN (e.g., ConvGRU<ref type=\"bibr\" target=\"#b0\">[1]</ref>) and our MA-RNN. Instead of directly aggregating features, w ect segmentation <ref type=\"bibr\" target=\"#b49\">[51]</ref>, video action classification/recognition <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, video object detect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng to its capacity in modeling long-term feature representation including video object segmentation <ref type=\"bibr\" target=\"#b49\">[51]</ref>, video action classification/recognition <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ments on five popular benchmarks including GOT-10k <ref type=\"bibr\" target=\"#b23\">[25]</ref>, LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref>, TC-128 <ref type=\"bibr\" target=\"#b36\">[38]</ref>, OTB-15  at a speed of around 31 fps. We train the target localization branch using training splits of LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref>, GOT-10k <ref type=\"bibr\" target=\"#b23\">[25]</ref> and VID k and \u03b1 are set to 5, 1 and 0.8, respectively. For BBR-IoU network, we use training splits of LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref>, GOT-10k <ref type=\"bibr\" target=\"#b23\">[25]</ref> and VID p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.\">Experiment on LaSOT</head><p>LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a large-scale dataset consisting of 1,400 sequences. Fo ead><p>To validate the effect of different components, we conduct ablation experiments on LaSOT tst <ref type=\"bibr\" target=\"#b15\">[16]</ref> regarding the target localization and target estimation.</ re <ref type=\"figure\">5</ref>. Comparisons of our MART and other state-of-the-art trackers on LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref>, TC-128 <ref type=\"bibr\" target=\"#b36\">[38]</ref> and OTB-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b40\">42,</ref><ref type=\"bibr\" target=\"#b53\">55,</ref><ref type=\"bibr\" target=\"#b55\">57,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>. Despite considerab he-art performance <ref type=\"bibr\" target=\"#b40\">[42,</ref><ref type=\"bibr\" target=\"#b53\">55,</ref><ref type=\"bibr\" target=\"#b55\">57]</ref>. One of recent trends is to integrate correlation filter tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b42\">44,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" target=\"#b47\">49,</ref><ref type=\"bibr\" tar arget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b46\">48]</ref>, the classification model is learned by minimizing the squa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ort from historical frames, even when difficult challenges occur. A recent representative effort in <ref type=\"bibr\" target=\"#b62\">[64]</ref> develops such a spatial-temporal representation for tracki nt tracking inference. <ref type=\"bibr\" target=\"#b1\">(2)</ref> Long-term representation. In tracker <ref type=\"bibr\" target=\"#b62\">[64]</ref>, spatial-temporal representation is achieved by warping an ype=\"bibr\" target=\"#b47\">49,</ref><ref type=\"bibr\" target=\"#b62\">64]</ref>. Especially, the work of <ref type=\"bibr\" target=\"#b62\">[64]</ref> leverages temporal information for tracking. Despite robus [32,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>.</p><p>Our work is related to but different from <ref type=\"bibr\" target=\"#b62\">[64]</ref> that uses a large FlowNet <ref type=\"bibr\" target=\"#b13\">[ le for motion dynamics. Besides, we model a long-term representation using RNNs, which differs from <ref type=\"bibr\" target=\"#b62\">[64]</ref> with a short-term representation. Our work is relevant to  get=\"#b42\">44,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" target=\"#b47\">49,</ref><ref type=\"bibr\" target=\"#b62\">64]</ref>. Especially, the work of <ref type=\"bibr\" target=\"#b62\">[64. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iciency, the work of <ref type=\"bibr\" target=\"#b1\">[2]</ref> has been improved with many extensions <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" ta ref type=\"bibr\" target=\"#b1\">[2]</ref>, StructSiam <ref type=\"bibr\" target=\"#b59\">[61]</ref>, DSiam <ref type=\"bibr\" target=\"#b19\">[21]</ref>, ECO <ref type=\"bibr\" target=\"#b8\">[9]</ref>, STRCF <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b40\">42,</ref><ref type=\"bibr\" target=\"#b53\">55,</ref><ref type=\"bibr\" target=\"#b55\">57,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>. Despite considerable advancement, most existing trackers f 8,</ref><ref type=\"bibr\">54]</ref> and deeper architecture <ref type=\"bibr\" target=\"#b30\">[32,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>.</p><p>Our work is related to but different from <ref type= ype=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\">54,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\" target=\"#b61\">63]</ref>) or IoUNet (e.g., <  <ref type=\"bibr\" target=\"#b30\">[32]</ref>, C-RPN <ref type=\"bibr\" target=\"#b16\">[18]</ref>, SiamDW <ref type=\"bibr\" target=\"#b60\">[62]</ref>, MDNet <ref type=\"bibr\" target=\"#b40\">[42]</ref>, SiamFC <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nv layer, MA-RNN and classification network are jointly end-to-end trained using the ADAM optimizer <ref type=\"bibr\" target=\"#b26\">[28]</ref>. In inference phase, only the classification network is up  the transformation conv layer, MA-RNNs and classification are end-to-end trained using ADAM method <ref type=\"bibr\" target=\"#b26\">[28]</ref> on videos. To reduce redundancy, we sparsely sample a fram 23\">[25]</ref> and VID <ref type=\"bibr\" target=\"#b44\">[46]</ref>. We train for 50 epochs using ADAM <ref type=\"bibr\" target=\"#b26\">[28]</ref>. The learning rate starts from 10 \u22123 with a decay of 0.1 e 44\">[46]</ref> and COCO <ref type=\"bibr\" target=\"#b37\">[39]</ref>. We train for 50 epochs with ADAM <ref type=\"bibr\" target=\"#b26\">[28]</ref> using learning rate of 10 \u22123 with a decay of 0.1 every 10 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b14\">15]</ref>, video object detection <ref type=\"bibr\" target=\"#b38\">[40,</ref><ref type=\"bibr\" target=\"#b58\">60]</ref>.</p><p>Our approach is closely related to <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  flow has been greatly improved (e.g., FlowNet/FlowNet 2.0 <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b24\">26]</ref>). Despite this, it is time-consuming to obtain such motion . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acker in dealing with scale change, which is further enhanced by incorporating distractor detection <ref type=\"bibr\" target=\"#b61\">[63]</ref>, multi-stage mechanism <ref type=\"bibr\" target=\"#b16\">[18, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, C-RPN <ref type=\"bibr\" target=\"#b16\">[18]</ref>, DaSiamRPN <ref type=\"bibr\" target=\"#b61\">[63]</ref>, SiamRPN <ref type=\"bibr\" target=\"#b31\">[33]</ref>, Grad-N ype=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\">54,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\" target=\"#b61\">63]</ref>) or IoUNet (e.g., <ref type=\"bibr\" target=\"#b2\">[3,</ref><r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a backbone network (i.e., ResNet <ref type=\"bibr\" target=\"#b21\">[23]</ref> pre-trained on Ima-geNet <ref type=\"bibr\" target=\"#b12\">[13]</ref>) to extract initial feature representation x t . Consideri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wing we discuss the most related work and refer readers to <ref type=\"bibr\" target=\"#b34\">[36,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b45\">47]</ref> for recent tracking. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>Inspired by discriminative correlation filter trackers <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acker in dealing with scale change, which is further enhanced by incorporating distractor detection <ref type=\"bibr\" target=\"#b61\">[63]</ref>, multi-stage mechanism <ref type=\"bibr\" target=\"#b16\">[18, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, C-RPN <ref type=\"bibr\" target=\"#b16\">[18]</ref>, DaSiamRPN <ref type=\"bibr\" target=\"#b61\">[63]</ref>, SiamRPN <ref type=\"bibr\" target=\"#b31\">[33]</ref>, Grad-N ype=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\">54,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\" target=\"#b61\">63]</ref>) or IoUNet (e.g., <ref type=\"bibr\" target=\"#b2\">[3,</ref><r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 26\">[28]</ref> on videos. To reduce redundancy, we sparsely sample a frame with a random interval \u2208 <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> to form a new sequen t <ref type=\"bibr\" target=\"#b33\">[35]</ref>, SA-Siam <ref type=\"bibr\" target=\"#b20\">[22]</ref>, ACT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SiamFC <ref type=\"bibr\" target=\"#b1\">[2]</ref>, ECO-HC <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nv layer, MA-RNN and classification network are jointly end-to-end trained using the ADAM optimizer <ref type=\"bibr\" target=\"#b26\">[28]</ref>. In inference phase, only the classification network is up  the transformation conv layer, MA-RNNs and classification are end-to-end trained using ADAM method <ref type=\"bibr\" target=\"#b26\">[28]</ref> on videos. To reduce redundancy, we sparsely sample a fram 23\">[25]</ref> and VID <ref type=\"bibr\" target=\"#b44\">[46]</ref>. We train for 50 epochs using ADAM <ref type=\"bibr\" target=\"#b26\">[28]</ref>. The learning rate starts from 10 \u22123 with a decay of 0.1 e 44\">[46]</ref> and COCO <ref type=\"bibr\" target=\"#b37\">[39]</ref>. We train for 50 epochs with ADAM <ref type=\"bibr\" target=\"#b26\">[28]</ref> using learning rate of 10 \u22123 with a decay of 0.1 every 10 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, the work of <ref type=\"bibr\" target=\"#b0\">[1]</ref> replaces linear product operation in GRU-RNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> with convolution and proposes ConvGRU for action recognition. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a similarity measurement for tracking using Siamese networks <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b48\">50]</ref>. Owing to balanced accuracy and efficiency, the work of <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> contains 60 sequences which are developed by replacing 12 less representative videos in VOT-2018 <ref type=\"bibr\" target=\"#b27\">[29]</ref> with more challenging ones. Similar to VOT-2018, each trac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  flow has been greatly improved (e.g., FlowNet/FlowNet 2.0 <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b24\">26]</ref>). Despite this, it is time-consuming to obtain such motion . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\" target=\"#b61\">63]</ref>) or IoUNet (e.g., <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>) to estimate target sc tains AO of 0.628, SR 0.50 of 0.732 and SR 0.75 of 50.4, outperforming the second best tracker DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref> with AO of 0.611, SR 0.50 of 0.717 and SR 0.75 of 0.492 by 1 ng and the rest 280 for testing. We compare MART with 12 state-of-the-art tracking algorithms (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref>, SiamRPN++ <re our MART achieves the best performance with 0.571 success score, outperforming the second best DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref> by 0.3%. In comparison to ATOM <ref type=\"bibr\" target=\"#b7\"  one-pass evaluation (OPE) for evaluation. We compare our MART to 9 state-of-the-art trackers (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ATOM < , our MART achieves the best result with success score of 0.621, outperforming the second best DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref> with 0.609 success score by 1.2%. In comparison with ATOM th E to assess different algorithms. We compare our proposed MART to 12 state-ofthe-art trackers (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ATOM < ith success score of 0.678 compared to SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref> and DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>. In comparison with ATOM that applies only spatial feature f ith several recent topperformance trackers from VOT-2019, and Tab. 2 demonstrates the results. DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref> achieves the best EAO of 0.379. Our MART obtains promising r mlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_2\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b2\">3</ref> shows the ablation experiments on target localization. Without. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ely studied in recent years. In the following we discuss the most related work and refer readers to <ref type=\"bibr\" target=\"#b34\">[36,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b42\">44,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" target=\"#b47\">49,</ref><ref type=\"bibr\" tar arget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b46\">48]</ref>, the classification model is learned by minimizing the squa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>Inspired by discriminative correlation filter trackers <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etc.</p><p>Early approaches usually leverage various hand-crafted representation (e.g., pixel value <ref type=\"bibr\" target=\"#b3\">[4]</ref>, HoG <ref type=\"bibr\" target=\"#b22\">[24]</ref>) for tracking. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b56\">58]</ref>. Notably, by integrating with region proposal network (RPN). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etc.</p><p>Early approaches usually leverage various hand-crafted representation (e.g., pixel value <ref type=\"bibr\" target=\"#b3\">[4]</ref>, HoG <ref type=\"bibr\" target=\"#b22\">[24]</ref>) for tracking. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance <ref type=\"bibr\" target=\"#b36\">[37]</ref>, as well as graph theory algorithms like widest path <ref type=\"bibr\" target=\"#b3\">[4]</ref> and most reliable path <ref type=\"bibr\" target=\"#b3\">[4]</re h theory algorithms like widest path <ref type=\"bibr\" target=\"#b3\">[4]</ref> and most reliable path <ref type=\"bibr\" target=\"#b3\">[4]</ref>, are special instances of this path formulation with differe e show that such a formulation can be efficiently solved via the generalized Bellman-Ford algorithm <ref type=\"bibr\" target=\"#b3\">[4]</ref> under mild conditions and scale up to large graphs.</p><p>Th maximal path length of 3. A more scalable solution is to use the generalized Bellman-Ford algorithm <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Specifically, assuming the operators \u2295, \u2297 satisfy a semirin ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Graph Theory Algorithms</head><p>Widest Path <ref type=\"bibr\" target=\"#b3\">[4]</ref> min(w q (e i ), w q (e j )) max(h q (P i ), h q (P j )) \u2212\u221e,  b3\">[4]</ref> min(w q (e i ), w q (e j )) max(h q (P i ), h q (P j )) \u2212\u221e, +\u221e w e Most Reliable Path <ref type=\"bibr\" target=\"#b3\">[4]</ref> w q (e i ) \u00d7 w q (e j ) max(h q (P i ), h q (P j )) 0, 1 w e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 10\">7</ref>). The negative samples are generated according to Partial Completeness Assumption (PCA) <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which corrupts one of the entities in a positive triplet . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ented here, as one can always borrow MESSAGE and AGGREGATE from the arsenal of message-passing GNNs <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  multiplication corresponds to scaling. Such transformations correspond to the relational operators <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> in knowledge graph  link prediction variants, e.g., complex logical queries with conjunctions (\u2227) and disjunctions (\u2228) <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. In the future, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b3\">[4]</ref>. Specifically, assuming the operators \u2295, \u2297 satisfy a semiring system <ref type=\"bibr\" target=\"#b20\">[21]</ref> with summation identity 0 q and multiplication identity 1 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: AGE and AGGREGATE from the arsenal of message-passing GNNs <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch transformations correspond to the relational operators <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> in knowledge graph embeddings <ref type=\"bibr\" target=\"#b5\" logical queries with conjunctions (\u2227) and disjunctions (\u2228) <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. In the future, we would like to how NBFNet approximates th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the ubiquitous existence of graphs, such a task has many applications, such as recommender system <ref type=\"bibr\" target=\"#b33\">[34]</ref>, knowledge graph completion <ref type=\"bibr\" target=\"#b40\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, such as recommender system <ref type=\"bibr\" target=\"#b33\">[34]</ref>, knowledge graph completion <ref type=\"bibr\" target=\"#b40\">[41]</ref> and drug repurposing <ref type=\"bibr\" target=\"#b26\">[27]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tence embeddings from BERT. <ref type=\"foot\" target=\"#foot_0\">1</ref> In practice, previous studies <ref type=\"bibr\" target=\"#b29\">(Reimers and Gurevych, 2019;</ref><ref type=\"bibr\" target=\"#b21\">Li e verse sentence-related tasks. Following <ref type=\"bibr\" target=\"#b10\">Conneau et al. (2017)</ref>, <ref type=\"bibr\" target=\"#b29\">Reimers and Gurevych (2019)</ref> (SBERT) propose to compute sentence  might not be desirable to derive sentence embeddings directly from BERT without fine-tuning. While <ref type=\"bibr\" target=\"#b29\">Reimers and Gurevych (2019)</ref> attempt to alleviate this problem w r multilingual datasets. We also employ RoBERTa <ref type=\"bibr\">(Liu et al., 2019)</ref> and SBERT <ref type=\"bibr\" target=\"#b29\">(Reimers and Gurevych, 2019)</ref> in some cases to evaluate the gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ys on contrastive learning, refer to <ref type=\"bibr\" target=\"#b20\">Le-Khac et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b18\">Jaiswal et al. (2020)</ref>.</p><p>Fine-tuning BERT with Supervision.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentations from them, which are broadly applicable across diverse sentence-related tasks. Following <ref type=\"bibr\" target=\"#b10\">Conneau et al. (2017)</ref>, <ref type=\"bibr\" target=\"#b29\">Reimers a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uter vision community <ref type=\"bibr\" target=\"#b39\">(Chen et al. (2020)</ref>; Chen and He (2020); <ref type=\"bibr\" target=\"#b16\">He et al. (2020)</ref>, inter alia). We improve the framework of <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er and easier to use than existing methods <ref type=\"bibr\" target=\"#b12\">(Fang and Xie, 2020;</ref><ref type=\"bibr\" target=\"#b38\">Xie et al., 2020)</ref>. Moreover, we customize the NT-Xent loss <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  our approach. However, they generally require data augmentation techniques, e.g., back-translation <ref type=\"bibr\" target=\"#b30\">(Sennrich et al., 2016)</ref>, or prior knowledge on training data su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2017)</ref>, SICK-R <ref type=\"bibr\" target=\"#b23\">(Marelli et al., 2014)</ref>, and STS12-16 <ref type=\"bibr\" target=\"#b4\">(Agirre et al., 2012</ref><ref type=\"bibr\" target=\"#b5\">(Agirre et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ter vision and natural language processing <ref type=\"bibr\" target=\"#b20\">(Gomez et al., 2017;</ref><ref type=\"bibr\" target=\"#b65\">Xie et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Bai et al., 2018  2019)</ref> and grouped convolutions <ref type=\"bibr\" target=\"#b36\">(Krizhevsky et al., 2012;</ref><ref type=\"bibr\" target=\"#b65\">Xie et al., 2017)</ref>, we generalize reversible residual connection. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b20\">(Gomez et al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Kitaev et al., 2019)</ref> and grouped convolutions <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imizers as the baseline models. The implementation of all the reversible models is based on PyTorch <ref type=\"bibr\" target=\"#b51\">(Paszke et al., 2019)</ref> and supports both PyTorch Geometric (PyG). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed node features.</p><p>To construct a stable or contractive GNN block, we mimic the design of MDEQ <ref type=\"bibr\" target=\"#b2\">(Bai et al., 2020)</ref>. We build a GNN block as follows:</p><formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed node features.</p><p>To construct a stable or contractive GNN block, we mimic the design of MDEQ <ref type=\"bibr\" target=\"#b2\">(Bai et al., 2020)</ref>. We build a GNN block as follows:</p><formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: crosoft Academic Graph (MAG) <ref type=\"bibr\" target=\"#b61\">(Wang et al., 2020)</ref>. Recent works <ref type=\"bibr\" target=\"#b40\">(Li et al., 2019;</ref><ref type=\"bibr\">2020;</ref><ref type=\"bibr\" t  due to the over-smoothing <ref type=\"bibr\">(Li et al., 2018)</ref> and vanishing gradient problems <ref type=\"bibr\" target=\"#b40\">(Li et al., 2019;</ref><ref type=\"bibr\">2021)</ref>   <ref type=\"bibr ttp://www.tei-c.org/ns/1.0\"><head n=\"3.2.\">Over-parameterized GNNs</head><p>Deep GNNs. Recent works <ref type=\"bibr\" target=\"#b40\">(Li et al., 2019;</ref><ref type=\"bibr\">2020)</ref> show how adding r t al., 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b60\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, and DeepGCN <ref type=\"bibr\" target=\"#b40\">(Li et al., 2019)</ref>. We can also combine the proposed techniques . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: djacency matrices or graph diffusion matrices <ref type=\"bibr\" target=\"#b64\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Klicpera et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Bojchevski  Cai et al., 2020)</ref>. Another line of work <ref type=\"bibr\" target=\"#b64\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Klicpera et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Bojchevski . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Recent works <ref type=\"bibr\" target=\"#b40\">(Li et al., 2019;</ref><ref type=\"bibr\">2020;</ref><ref type=\"bibr\" target=\"#b11\">Chen et al., 2020)</ref> have successfully trained deep models with a ual connections aid in training deeper GNNs <ref type=\"bibr\" target=\"#b21\">(Gong et al., 2020;</ref><ref type=\"bibr\" target=\"#b11\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b68\">Xu et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n techniques such as DropEdge <ref type=\"bibr\" target=\"#b55\">(Rong et al., 2020)</ref>, DropConnect <ref type=\"bibr\" target=\"#b24\">(Hasanzadeh et al., 2020)</ref>, PairNorm <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion is to use implicit differentiation <ref type=\"bibr\" target=\"#b56\">(Scarselli et al., 2008;</ref><ref type=\"bibr\" target=\"#b1\">Bai et al., 2019)</ref>, which assumes that the network can reach an e  the initial node features and Z in is initialized to zeros for the first iteration. Similar to DEQ <ref type=\"bibr\" target=\"#b1\">(Bai et al., 2019)</ref>, the forward pass of DEQ-GNN is implemented w target=\"#b37\">(Leemput et al., 2019)</ref>. The deep equilibrium module is implemented based on DEQ <ref type=\"bibr\" target=\"#b1\">(Bai et al., 2019)</ref>.  Multi-view Inference on Ogbn-proteins. To f tates Z are set to zero. We implement our DEQ-GNN based on the original DEQ implementation for CNNs <ref type=\"bibr\" target=\"#b1\">(Bai et al., 2019</ref>). Broyden's root-finding method is used <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: djacency matrices or graph diffusion matrices <ref type=\"bibr\" target=\"#b64\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Klicpera et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Bojchevski  Cai et al., 2020)</ref>. Another line of work <ref type=\"bibr\" target=\"#b64\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Klicpera et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Bojchevski . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n techniques such as DropEdge <ref type=\"bibr\" target=\"#b55\">(Rong et al., 2020)</ref>, DropConnect <ref type=\"bibr\" target=\"#b24\">(Hasanzadeh et al., 2020)</ref>, PairNorm <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nother approach is efficient propagation via K-power adjacency matrices or graph diffusion matrices <ref type=\"bibr\" target=\"#b64\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Klicpera et al. )</ref>, and GraphNorm <ref type=\"bibr\" target=\"#b8\">(Cai et al., 2020)</ref>. Another line of work <ref type=\"bibr\" target=\"#b64\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Klicpera et al. lation Studies</head><p>Comparison with SGC &amp; SIGN on Ogbn-products. We compare RevGNN with SGC <ref type=\"bibr\" target=\"#b64\">(Wu et al., 2019)</ref> and SIGN <ref type=\"bibr\" target=\"#b17\">(Fras. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: patial neighbors. Many subsequent works <ref type=\"bibr\" target=\"#b23\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b47\">Monti et al., 2017;</ref><ref type=\"bibr\" target=\"#b49\">Niepert et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Radford et al.)</ref>, BERT (340M) <ref type=\"bibr\" target=\"#b14\">(Devlin et al., 2019)</ref>,   <ref type=\"bibr\" target=\"#b5\">(Brown et al., 2020)</ref>, Gshard-M4 (600B) <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arking GNNs <ref type=\"bibr\" target=\"#b15\">(Dwivedi et al., 2020)</ref>, Open Graph Benchmark (OGB) <ref type=\"bibr\" target=\"#b27\">(Hu et al., 2020;</ref><ref type=\"bibr\">2021)</ref> and Microsoft Aca rs. Our model RevGNN-Deep, outperforms all state-of-the-art approaches on the ogbn-proteins dataset <ref type=\"bibr\" target=\"#b27\">(Hu et al., 2020)</ref> with an ROC-AUC of 87.74%, while only consumi rain a 112-layer residual GNN and achieve SOTA performance on the large-scale ogbn-proteins dataset <ref type=\"bibr\" target=\"#b27\">(Hu et al., 2020)</ref>. Other works also demonstrate that residual c efficiency and parameter efficiency on the ogbnproteins dataset from the Open Graph Benchmark (OGB) <ref type=\"bibr\" target=\"#b27\">(Hu et al., 2020)</ref>. We use the same GNN operator <ref type=\"bibr d Deep GNNs</head><p>We conduct experiments on several datasets from the Open Graph Benchmark (OGB) <ref type=\"bibr\" target=\"#b27\">(Hu et al., 2020)</ref>. We first show state-of-the art results on se sible connections, which reach new SOTA results on the ogbn-proteins dataset of the OGB leaderboard <ref type=\"bibr\" target=\"#b27\">(Hu et al., 2020)</ref> (Table <ref type=\"table\" target=\"#tab_3\">1</r sis is an interesting avenue for future investigation. We conduct experiments on three OGB datasets <ref type=\"bibr\" target=\"#b27\">(Hu et al., 2020)</ref> including ogbn-proteins, ogbn-arxiv and ogbnp batch training to further optimize memory. We conduct an ablation study on the ogbn-products dataset<ref type=\"bibr\" target=\"#b27\">(Hu et al., 2020)</ref> with full-batch training and a simple random-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ible GNNs. Inspired by reversible networks <ref type=\"bibr\" target=\"#b20\">(Gomez et al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Kitaev et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b47\">Monti et al., 2017;</ref><ref type=\"bibr\" target=\"#b49\">Niepert et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Gao et al., 2018;</ref><ref type=\"bibr\" target=\"#b67\">Xu et al., 2019. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dient problems <ref type=\"bibr\" target=\"#b40\">(Li et al., 2019;</ref><ref type=\"bibr\">2021)</ref>   <ref type=\"bibr\" target=\"#b25\">(He et al., 2016;</ref><ref type=\"bibr\" target=\"#b29\">Huang et al., 2 get=\"#b40\">(Li et al., 2019;</ref><ref type=\"bibr\">2020)</ref> show how adding residual connections <ref type=\"bibr\" target=\"#b25\">(He et al., 2016)</ref> to vertex features (X = f w (X, A, U ) + X) e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  lead to significant performance loss compared to the homophilous regime due to assumption mismatch <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ t against structural attacks. Inspired by recent works on improving GNNs for heterophilous settings <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ  node classification, several GNN designs for handling heterophilous connections have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f attack scenarios <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, node features <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, or combinations o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f attack scenarios <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, node features <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, or combinations o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b32\">[33]</ref>). This issue is also known in classical semi-supervised learning <ref type=\"bibr\" target=\"#b23\">[24]</ref>. To address this issue in semi-supervised node classificat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: respect to features or class labels) to create connections <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. Datasets displaying such tendencies are thus typically cal e inability of existing GNNs to exploit heterophilous data <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> can be exploited by an attacker by inserting heterophilous   assumption mismatch <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe terophilous settings <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe s have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe ed heterophilous. A homophilous or heterophilous edge is a special case with k = 1.</p><p>Following <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, we define homophi ized in existing GNN models including GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref> and H 2 GCN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and has been shown to boost the representation power of G >[40]</ref>, and has been shown to boost the representation power of GNNs under natural heterophily <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>  naturally-occurring heterophily which aim to aggregate information from higher-order neighborhoods <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p><p>Intuition To deal with naturally-occurring heteroph  useful strategy is to exploit the heterophilous patterns by considering higher-order neighborhoods <ref type=\"bibr\" target=\"#b39\">[40]</ref> or GNN formulations leveraging class compatibility matrice  with heterophily-inspired designs only: GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref>, H 2 GCN <ref type=\"bibr\" target=\"#b39\">[40]</ref> and CPGNN <ref type=\"bibr\" target=\"#b38\">[39]</ref>;</p><p terophilous datasets where GCN can exhibit significantly inferior accuracy to models like GraphSAGE <ref type=\"bibr\" target=\"#b39\">[40]</ref>. We use an attack budget equal to its node degree and allo tails on how we incorporate the low-rank approximation vaccination into the formulations of H 2 GCN <ref type=\"bibr\" target=\"#b39\">[40]</ref> and GraphSAGE <ref type=\"bibr\" target=\"#b8\">[9]</ref> in o 9]</ref> in order to form the hybrid methods, H 2 GCN-SVD and GraphSAGE-SVD.</p><p>H 2 GCN-SVD From <ref type=\"bibr\" target=\"#b39\">[40]</ref>, each layer in the neighborhood aggregation stage of H 2 G. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b19\">[20]</ref>: FB100 <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Snap Patents <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. We report the dat erable and thus not identifiable. Also, no offensive content is found within the data. Snap Patents <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> is a utility paten Y| \u2212 1) + |Y| \u2212 1) 2<label>(26)</label></formula><p>This term is denoted as \u2206L CM,direct atk in Eq. <ref type=\"bibr\" target=\"#b14\">(15)</ref>.</p><p>The amount of increase in CM-type attack loss \u2206L CM k term in Eq. <ref type=\"bibr\" target=\"#b15\">(16)</ref>.</p><p>For the \u2206L CM,direct atk term in Eq. <ref type=\"bibr\" target=\"#b14\">(15)</ref> and the \u2206L CM,indirect atk term in Eq. ( <ref type=\"formul and \u03b4 2 = 0, or \u03b4 1 = 0 and \u03b4 2 = 1) when h &gt; 1 |Y| ; replcaing the \u2206L CM,direct atk term in Eq. <ref type=\"bibr\" target=\"#b14\">(15)</ref> to the equivalent term \u2206L CM,f (2)   s atk in Eq. <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mitigate the perturbations introduced by Nettack <ref type=\"bibr\" target=\"#b40\">[41]</ref>; Pro-GNN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which aims to learn the unperturbed graph structure in tr #b38\">[39]</ref>;</p><p>(2) State-of-the-art architectures designed with robustness in mind: ProGNN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GNNGuard <ref type=\"bibr\" target=\"#b36\">[37]</ref> and GC as existing works have shown that low-rank based approach does not adapt well to untargeted attacks <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Nevertheless, methods combining both designs and low-rank milar to state-of-the-art models like GNNGuard <ref type=\"bibr\" target=\"#b36\">[37]</ref> and ProGNN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, they may remain vulnerable to other types of attacks such . D.4 for details). For evaluation, we follow the setup in <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>: per dataset, we split the nodes into training (10%), valid r increasing the CE-type attack loss, which is the same as the range for CM-type attack loss as Eq. <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p><p>For the case \u03b4 1 = 0 and \u03b4 2 = \u00b11, we have</p><form. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>.</p><p>A seemingly  t=\"#b21\">22]</ref>, or combinations of these perturbations <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>.</p><p>On the defen ttacks have been proposed, including: adversarial training <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>; low-rank approximati o the surrogate model are transferable to the attacked GNN <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. For node classification, the attack loss L atk quantifies  s, such as the amount of perturbations and unnoticeability <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>T bel y v \u2208 Y, the loss L atk could be the cross-entropy loss (CE-type) used for training the network <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>:</p><formula xml:i ted attacks (cf. \u00a72), generated by NET-TACK <ref type=\"bibr\" target=\"#b40\">[41]</ref> and Metattack <ref type=\"bibr\" target=\"#b41\">[42]</ref>, respectively. As we focus on robustness against structura ber of edges in each dataset, and we use the Meta-Self variant as it shows the most destructiveness <ref type=\"bibr\" target=\"#b41\">[42]</ref>.</p><p>Hardware Specifications For fair runtime measuremen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .e., the tendency of similar nodes (with respect to features or class labels) to create connections <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. Datasets displayi oof in App. C.1. Intuitively, the relative inability of existing GNNs to exploit heterophilous data <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> can be exploited b </ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, recent works <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta ompared to the homophilous regime due to assumption mismatch <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar by recent works on improving GNNs for heterophilous settings <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar ns for handling heterophilous connections have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: versarial training <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>; low-rank approximation of graph adjacency <ref type=\"bibr\" t e to certain types of structural and feature perturbations <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. Interested readers a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f attack scenarios <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, node features <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, or combinations o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ckled by ML models in different contexts, and our goal is to build on the recent ML efforts for MCG <ref type=\"bibr\" target=\"#b33\">[Mansimov et al., 2019</ref><ref type=\"bibr\" target=\"#b46\">, Simm and thods <ref type=\"bibr\" target=\"#b46\">[Simm and Hernandez-Lobato, 2020]</ref> or not captured at all <ref type=\"bibr\" target=\"#b33\">[Mansimov et al., 2019</ref>]. \u2022 It explicitly models and predicts es ref type=\"bibr\" target=\"#b13\">[Gilmer et al., 2017]</ref> and self-attention networks. Thus, unlike <ref type=\"bibr\" target=\"#b33\">[Mansimov et al., 2019]</ref>, we are not affected by MPNNs' pitfalls </ref>. Statistics and other details are in fig. <ref type=\"figure\" target=\"#fig_9\">10</ref> and in <ref type=\"bibr\" target=\"#b33\">Mansimov et al. [2019]</ref>. Datasets are preprocessed as described  =\"#fig_9\">10</ref>, but point the interested reader other resources describing these datasets, e.g. <ref type=\"bibr\" target=\"#b33\">Mansimov et al. [2019]</ref>, Axelrod and Gomez-Bombarelli [2020a], <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ccurately sample equilibrium structures, but quickly become prohibitively slow for larger molecules <ref type=\"bibr\" target=\"#b45\">[Shim and MacKerell Jr, 2011</ref><ref type=\"bibr\" target=\"#b3\">, Bal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Force Field <ref type=\"bibr\" target=\"#b39\">[Rapp\u00e9 et al., 1992]</ref>, Merck Molecular Force Field <ref type=\"bibr\" target=\"#b16\">[Halgren, 1996]</ref>). However, FFs are crude approximations of the  ter additional fine-tuning with FF energy minimization. We used MMFF94s Merck Molecular Force Field <ref type=\"bibr\" target=\"#b16\">[Halgren, 1996</ref><ref type=\"bibr\" target=\"#b17\">[Halgren, , 1999]]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  unknown a priori and can vary between one and several thousand conformations for a single molecule <ref type=\"bibr\" target=\"#b6\">[Chan et al., 2021]</ref>. Nevertheless, various facets of the curse o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  unknown a priori and can vary between one and several thousand conformations for a single molecule <ref type=\"bibr\" target=\"#b6\">[Chan et al., 2021]</ref>. Nevertheless, various facets of the curse o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ess on modeling protein folding dynamics <ref type=\"bibr\" target=\"#b25\">[Ingraham et al., 2018</ref><ref type=\"bibr\" target=\"#b0\">, AlQuraishi, 2019</ref><ref type=\"bibr\" target=\"#b35\">, No\u00e9 et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bitively slow for larger molecules <ref type=\"bibr\" target=\"#b45\">[Shim and MacKerell Jr, 2011</ref><ref type=\"bibr\" target=\"#b3\">, Ballard et al., 2015</ref><ref type=\"bibr\" target=\"#b8\">, De Vivo et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tential energy, docking poses <ref type=\"bibr\" target=\"#b34\">[McGann, 2011]</ref>, shape similarity <ref type=\"bibr\" target=\"#b30\">[Kumar and Zhang, 2018]</ref>, pharmacophore searching <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the corresponding 3D atom coordinates are learned to approximately match these predicted distances <ref type=\"bibr\" target=\"#b21\">[Havel et al., 1983b</ref><ref type=\"bibr\" target=\"#b7\">,a, Crippen e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b32\">, Liu et al., 2021]</ref> such as charge distribution, potential energy, docking poses <ref type=\"bibr\" target=\"#b34\">[McGann, 2011]</ref>, shape similarity <ref type=\"bibr\" target=\"#b30\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b0\">, AlQuraishi, 2019</ref><ref type=\"bibr\" target=\"#b35\">, No\u00e9 et al., 2019</ref><ref type=\"bibr\" target=\"#b44\">, Senior et al., 2020]</ref>, where crystallized 3D structures are pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tional drug discovery because conformations determine biological, chemical, and physical properties <ref type=\"bibr\" target=\"#b15\">[Guimaraes et al., 2012</ref><ref type=\"bibr\" target=\"#b41\">, Sch\u00fctt . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b13\">[Gilmer et al., 2017</ref><ref type=\"bibr\" target=\"#b5\">, Battaglia et al., 2018</ref><ref type=\"bibr\" target=\"#b53\">, Yang et al., 2019]</ref> computes node embeddings h v \u2208 R d , \u2200v \u2208 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b7\">,a, Crippen et al., 1988</ref><ref type=\"bibr\" target=\"#b19\">, Havel, 1998</ref><ref type=\"bibr\" target=\"#b31\">, Lagorce et al., 2009</ref><ref type=\"bibr\" target=\"#b40\">, Riniker . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bitively slow for larger molecules <ref type=\"bibr\" target=\"#b45\">[Shim and MacKerell Jr, 2011</ref><ref type=\"bibr\" target=\"#b3\">, Ballard et al., 2015</ref><ref type=\"bibr\" target=\"#b8\">, De Vivo et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st popular open-source software, a stochastic DG-based method developed in the RDKit package. OMEGA <ref type=\"bibr\" target=\"#b24\">[Hawkins et al., 2010</ref><ref type=\"bibr\" target=\"#b23\">, Hawkins a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b0\">, AlQuraishi, 2019</ref><ref type=\"bibr\" target=\"#b35\">, No\u00e9 et al., 2019</ref><ref type=\"bibr\" target=\"#b44\">, Senior et al., 2020]</ref>, where crystallized 3D structures are pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the corresponding 3D atom coordinates are learned to approximately match these predicted distances <ref type=\"bibr\" target=\"#b21\">[Havel et al., 1983b</ref><ref type=\"bibr\" target=\"#b7\">,a, Crippen e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s root-mean-square deviation of atomic positions (RMSD), computed by the Kabsch alignment algorithm <ref type=\"bibr\" target=\"#b26\">[Kabsch, 1976]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1 matrices of 3D coordinates after being SE(3)-aligned a priori (using the Kabsch alignment algorithm <ref type=\"bibr\" target=\"#b26\">[Kabsch, 1976]</ref>). Next, we introduce four types of metrics to co bling is done using all nodes except one, in turn. We align all S i together using Kabsch algorithm <ref type=\"bibr\" target=\"#b26\">[Kabsch, 1976]</ref> <ref type=\"foot\" target=\"#foot_2\">7</ref> , foll. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b7\">,a, Crippen et al., 1988</ref><ref type=\"bibr\" target=\"#b19\">, Havel, 1998</ref><ref type=\"bibr\" target=\"#b31\">, Lagorce et al., 2009</ref><ref type=\"bibr\" target=\"#b40\">, Riniker . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[Friedrich et al., 2017]</ref> with <ref type=\"bibr\">OMEGA [Hawkins et al., 2010, Hawkins and</ref><ref type=\"bibr\" target=\"#b23\">Nicholls, 2012]</ref> being a popular example. They usually process a hod developed in the RDKit package. OMEGA <ref type=\"bibr\" target=\"#b24\">[Hawkins et al., 2010</ref><ref type=\"bibr\" target=\"#b23\">, Hawkins and Nicholls, 2012</ref><ref type=\"bibr\" target=\"#b10\">, Fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of NAS methods, especially when the search space is huge. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar  are three basic types: accuracy-based, magnitudebased, and angle-based metrics. For example, PCNAS <ref type=\"bibr\" target=\"#b19\">[20]</ref> drop unpromising operators layer by layer using accuracy a ect for elastic depth. The split point space is set to range <ref type=\"bibr\" target=\"#b8\">(9,</ref><ref type=\"bibr\" target=\"#b19\">20)</ref> to handle different complexity constrains. In total we have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> is essential to the performance for varied tasks in compute. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b44\">45]</ref> or evolution algorithms <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. These methods have demonstrated that NAS can find architec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: forcement learning <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> or evolution algorithms <ref type=\"bibr\" target=\"#b30\">[31,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: adient-based methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Path-based methods sample paths in each iteration to optim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chieves 77.9% top-1 accuracy on ImageNet, which is 19% smaller and 1.6% better than EfficientNet-B0 <ref type=\"bibr\" target=\"#b36\">[37]</ref>. The architecture discovered by NEAS transfers well to dow idual blocks (MBConv). We also add squeeze-excitation modules to each block following Effi-cientNet <ref type=\"bibr\" target=\"#b36\">[37]</ref> and MobileNetV3 <ref type=\"bibr\" target=\"#b11\">[12]</ref>. ain the discovered architectures for 350 epochs on ImageNet using similar settings as Efficient-Net <ref type=\"bibr\" target=\"#b36\">[37]</ref>: RMSProp optimizer with momentum 0.9 and decay 0.9, weight outperforms the recent MobileNetV3 <ref type=\"bibr\" target=\"#b11\">[12]</ref> and EfficientNet-B0/B1 <ref type=\"bibr\" target=\"#b36\">[37]</ref>. In particular, NEAS-L achieves 80.0% top-1 accuracy with   previous NAS methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, our search space includes a stack of mobile inverted bottl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tasks such as image classification <ref type=\"bibr\" target=\"#b47\">[48]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b22\">[23]</ref>, object detection <ref type=\"bibr\" target=\"#b3\">[4]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  have shown that the design of neural network architecture <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: antify the diversity across operators inspired by fixed-size determinantal point processing (K-DPP) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, a popular sampling model with great ability to measure th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plexity constraint. The searched architectures generate new stateof-the-art performance on ImageNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>. For instance, as shown in Fig. <ref type=\"figure\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Multimodal Transformer (MulT) approach to fuse crossmodal information from unaligned data sequences <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Their approach introduces the modality reinforcement unit lly determine the passed proportions of the reinforced features. Compared with the prior MulT model <ref type=\"bibr\" target=\"#b17\">[18]</ref>, the advantage of our approach lies in two aspects. First, al. propose the crossmodal attention mechanism to learn the inherent correlations across modalities <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Their approach repeatedly reinforces one modality with in with information from a source modality by learning the directional pairwise attention between them <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Denote by X s \u2208 R Ts\u00d7ds the data sequence from the source 1.0\"><head n=\"3.3.\">Model overview</head><p>Our model is trained in an end-to-end manner. Following <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we use a 1D temporal convolutional layer to process the i  \ud835\udc3f\u2192\ud835\udc36 \ud835\udc4d [\ud835\udc56]   \ud835\udc49\u2192\ud835\udc36 \ud835\udc4d [\ud835\udc56+1]   \ud835\udc36 MUM [\ud835\udc56]   CA mul   where C\u2192 * unit. Compared with the prior MulT model <ref type=\"bibr\" target=\"#b17\">[18]</ref> which reinforces the target modality by repeatedly attendi Translation Network (MCTN) <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Multimodal Transformer (MulT) <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Of these, MulT and LF-LSTM can be applied directly to the ns/1.0\"><head n=\"4.1.\">Experimental setup</head><p>We follow the common protocol of the prior works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ss modalities (see Fig. <ref type=\"figure\">1(a)</ref>), based on the recent advances of transformer <ref type=\"bibr\" target=\"#b19\">[20]</ref>. By exploring the crossmodal interaction between elements  ter information produced by incorrect crossmodal interactions. Finally, as in the transformer model <ref type=\"bibr\" target=\"#b19\">[20]</ref>, a position-wise feed-forward layer with skip connection w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ared to multimodal fusion from static modalities like images <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ared to multimodal fusion from static modalities like images <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ognizing the sentiment attitude of humans from video clips <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar to infer the sentiment attitude of humans from video clips <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar e late-fusion strategy by combining the high-level information learnt from each individual modality <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. This task involves time-series * Corresponding authors. data et=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. The crucial point lies in multimodal fusion from data sequen ropose to infer the joint representations of different modalities by probabilistic graphical models <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Although these prior works obtain better performance than l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igh-level information learnt from each individual modality <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Furthermore, Gan e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odalities like images <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, this task requires t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mply adopt the early-fusion strategy by concatenating the input sequences from different modalities <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> or the late-fusion s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rt the video transcripts into the pre-trained Glove model to obtain 300-dimensional word embeddings <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For the visual modality, we process the video frames by F. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ared to multimodal fusion from static modalities like images <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n reconstruction over high-level visual concepts, referred to as visual words. For instance, BoWNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> derived a teacherstudent learning scheme following this pr ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Among them, BoWNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> was the first work to use BoWs as reconstruction targets f arget=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>. In self-supervised learning, BowNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> trains a student to match the BoW representations produced  a key ingredient of several recent deep learning approaches <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: taset. We implement the full solution of our method (as described in \u00a7 4.1) using the ResNet50 (v1) <ref type=\"bibr\" target=\"#b33\">[34]</ref> architecture. We evaluate the learned representations on I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b67\">68,</ref><ref type=\"bibr\" target=\"#b69\">70,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b78\">79]</ref>. Many works rely on pretext reconstruction tasks <ref type= get=\"#b66\">67,</ref><ref type=\"bibr\" target=\"#b75\">76,</ref><ref type=\"bibr\" target=\"#b76\">77,</ref><ref type=\"bibr\" target=\"#b78\">79]</ref>, where the reconstruction target is defined at image pixel . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>, typically implemented in a contrastive learning framework  get=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b68\">69,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref> have shown great results. Among them, contrastive-based ins get=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b70\">71</ref>] is the most prominent example. In this case, a convnet is t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  typically implemented in a contrastive learning framework <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. The primary focus here is to learn low-dimensional image /. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b64\">65]</ref>.</p><p>Teacher-student approaches. This paradigm has a long research history <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref> and it is frequent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b56\">[57]</ref>, Places205 <ref type=\"bibr\" target=\"#b77\">[78]</ref> and VOC07 <ref type=\"bibr\" target=\"#b20\">[21]</ref> classification datasets as well as on the V0C07+12 detecti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  classes using as input a limited set of training examples <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>. The advantages of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b19\">20]</ref>, as well as clustering-based approaches <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>.</p><p>Several recent m iction task while SwAV uses an image-cluster prediction task <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. BoW targets are much r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inal prediction. Gradient-based methods <ref type=\"bibr\" target=\"#b16\">(Simonyan et al., 2013;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016)</ref> are widely used for the saliency computation. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rove the generalization of neural networks. Mixup approaches are categorized into input-level mixup <ref type=\"bibr\" target=\"#b26\">(Yun et al., 2019;</ref><ref type=\"bibr\">Kim et al.,</ref> Figure <re s with a span in another text, which is inspired from CutMix arXiv:2106.08062v1 [cs.CL] 15 Jun 2021 <ref type=\"bibr\" target=\"#b26\">(Yun et al., 2019)</ref>, to preserves the locality of two source tex  different span length would be too complex. This same-size replacement strategy is also adopted in <ref type=\"bibr\" target=\"#b26\">Yun et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b20\">Uddin et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" target=\"#b21\">(Verma et al., 2019)</ref> depending on the location of the mix opera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" target=\"#b21\">(Verma et al., 2019)</ref> depending on the location of the mix opera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ain the magnitude of a gradient vector, which becomes a saliency of each token similar to PuzzleMix <ref type=\"bibr\" target=\"#b9\">(Kim et al., 2020)</ref>.</p><p>Mixing text Text data x A and x B are . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uate mixup methods in sentence classification <ref type=\"bibr\" target=\"#b5\">(Guo et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Thulasidasan et al., 2019)</ref>. We use two different versions of TR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uate mixup methods in sentence classification <ref type=\"bibr\" target=\"#b5\">(Guo et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Thulasidasan et al., 2019)</ref>. We use two different versions of TR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  from HuggingFace datasets library. <ref type=\"foot\" target=\"#foot_1\">2</ref>For GLUE, we use SST-2 <ref type=\"bibr\" target=\"#b17\">(Socher et al., 2013)</ref>, MNLI <ref type=\"bibr\" target=\"#b25\">(Wil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Park et al., 2021)</ref>. On the other hand, mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> interpolates input texts and labels for th of each sample and combine them by weighted sum, which is similar to the original implementation of <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>.<ref type=\"foot\" target=\"#foot_0\">1</ref> T point, with a learning rate of 1e-5 for five epochs. This two-step training, which also utilized by <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>, speeds up the model convergence. We report  architecture (e.g., [CLS], [SEP ]) when conducting a mixup to preserve special signs. As stated by <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>, giving too high values for mixup ratio may aracteristic that benefits more from cross-label mixup than mixup with the same label, as stated at <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>. <ref type=\"foot\" target=\"#foot_4\">5</ref>  transformers</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"5\" xml:id=\"foot_4\"><ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref> states that mixing random pairs from all cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: purkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b0\">(Bentivogli et al., 2009)</ref>, MRPC <ref type=\"bibr\" target=\"#b2\">(Dolan and Brockett, 2005)</ref>, and QQP<ref type=\"foot\" target=\"#foo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" target=\"#b21\">(Verma et al., 2019)</ref> depending on the location of the mix opera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" target=\"#b21\">(Verma et al., 2019)</ref> depending on the location of the mix opera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the span length is 2 out of 10. 2020; <ref type=\"bibr\" target=\"#b22\">Walawalkar et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" tar  replacement strategy is also adopted in <ref type=\"bibr\" target=\"#b26\">Yun et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b20\">Uddin et al. (2021)</ref>. In situations where span length is the sam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Park et al., 2021)</ref>. On the other hand, mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> interpolates input texts and labels for th of each sample and combine them by weighted sum, which is similar to the original implementation of <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>.<ref type=\"foot\" target=\"#foot_0\">1</ref> T point, with a learning rate of 1e-5 for five epochs. This two-step training, which also utilized by <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>, speeds up the model convergence. We report  architecture (e.g., [CLS], [SEP ]) when conducting a mixup to preserve special signs. As stated by <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>, giving too high values for mixup ratio may aracteristic that benefits more from cross-label mixup than mixup with the same label, as stated at <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>. <ref type=\"foot\" target=\"#foot_4\">5</ref>  transformers</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"5\" xml:id=\"foot_4\"><ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref> states that mixing random pairs from all cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: purkar et al., 2016)</ref>, RTE <ref type=\"bibr\" target=\"#b0\">(Bentivogli et al., 2009)</ref>, MRPC <ref type=\"bibr\" target=\"#b2\">(Dolan and Brockett, 2005)</ref>, and QQP<ref type=\"foot\" target=\"#foo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Park et al., 2021)</ref>. On the other hand, mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> interpolates input texts and labels for th of each sample and combine them by weighted sum, which is similar to the original implementation of <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>.<ref type=\"foot\" target=\"#foot_0\">1</ref> T point, with a learning rate of 1e-5 for five epochs. This two-step training, which also utilized by <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>, speeds up the model convergence. We report  architecture (e.g., [CLS], [SEP ]) when conducting a mixup to preserve special signs. As stated by <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>, giving too high values for mixup ratio may aracteristic that benefits more from cross-label mixup than mixup with the same label, as stated at <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>. <ref type=\"foot\" target=\"#foot_4\">5</ref>  transformers</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"5\" xml:id=\"foot_4\"><ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref> states that mixing random pairs from all cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ain the magnitude of a gradient vector, which becomes a saliency of each token similar to PuzzleMix <ref type=\"bibr\" target=\"#b9\">(Kim et al., 2020)</ref>.</p><p>Mixing text Text data x A and x B are . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the span length is 2 out of 10. 2020; <ref type=\"bibr\" target=\"#b22\">Walawalkar et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" tar  replacement strategy is also adopted in <ref type=\"bibr\" target=\"#b26\">Yun et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b20\">Uddin et al. (2021)</ref>. In situations where span length is the sam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" target=\"#b21\">(Verma et al., 2019)</ref> depending on the location of the mix opera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re, most previous attempts on mixup for texts <ref type=\"bibr\" target=\"#b5\">(Guo et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020)</ref> apply mixup on hidden vectors like embeddings. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b23\">(Wang et al., 2018)</ref>, TREC <ref type=\"bibr\" target=\"#b11\">(Li and Roth, 2002;</ref><ref type=\"bibr\" target=\"#b7\">Hovy et al., 2001)</ref>, and ANLI <ref type=\"bibr\" target=\"#b13\">(Nie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" target=\"#b21\">(Verma et al., 2019)</ref> depending on the location of the mix opera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er-item interactions <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. A large model with e real-time platform <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><p>To tackle th arget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar arget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> (Figure <ref type=\" arget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> have focused on mat od effectively utilizing the teacher's predictions. First, <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> distill the knowledge of the items with high scores in the  distilling knowledge of a few top-ranked items is effective to discover the user's preferable items <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Most recently, <ref type=\"bibr\" target=\"#b9\">[10]</ref> u opology (Section 3.3). Note that we do not include the methods distilling the predictions (e.g., RD <ref type=\"bibr\" target=\"#b21\">[22]</ref>, CD <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and RRD <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sophisticated model architectures to better understand the complex nature of user-item interactions <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ e latency. For this reason, it is challenging to adopt such a large model to the real-time platform <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ =\"#b25\">26]</ref>.</p><p>To tackle this problem, Knowledge Distillation (KD) has been adopted to RS <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe l size.</p><p>Most existing KD methods for RS transfer the knowledge from the teacher's predictions <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe s another recent approach that transfers the latent knowledge from the teacher's intermediate layer <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, pointing out that t que. However, due to their restricted capability, the loss of recommendation accuracy is inevitable <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ only applicable to specific models or easy to fall into a local optimum because of the local search <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>.</p><p>Knowledge Dis <ref type=\"bibr\" target=\"#b4\">[5]</ref> that matches the class distributions, most existing methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe owledge of the items ranked highly by the teacher but ranked lowly by the student. On the one hand, <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> focus on distilling  ledge. Pointing out that the predictions incompletely reveal the teacher's knowledge, a few methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> <ref type=\"foot\" tar gression enables e \ud835\udc60 to capture compressed information that can restore detailed information in e \ud835\udc61 <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. <ref type=\"bibr\" ta longing to the different groups <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>The existing methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> based on the hint re nteractions (FourSquare) and remove items having fewer than 10 interactions (FourSquare) as done in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Table <ref type=\"ta ure\" target=\"#fig_0\">1b</ref>) that can restore more detailed preference information in the teacher <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>However, the existing hint regression-based methods f te layer. Note that the two groups of methods can be utilized together to fully improve the student <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>(1) KD by the predictions. Motivated by <ref type=\"bi 1]</ref> adopts this original hint regression to improve the student.</p><p>The most recent work DE <ref type=\"bibr\" target=\"#b7\">[8]</ref> further elaborates this approach for RS. DE argues that usin via the same network without being mixed with the representations belonging to the different groups <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>The existing methods <ref type=\"bibr\" target=\"#b7\">[8 ence in an end-to-end manner considering both the teacher and the student, we borrow the idea of DE <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Formally, let there exist \ud835\udc3e preference groups in the teache r\" target=\"#b5\">[6]</ref>. This group assignment is evolved during the training via backpropagation <ref type=\"bibr\" target=\"#b7\">[8]</ref>. This will be explained in Section 3.3.4.</p><p>With the ass the topology-preserving loss, the second term corresponds to the hint regression loss adopted in DE <ref type=\"bibr\" target=\"#b7\">[8]</ref> that makes the group assignment process differentiable. We p onding group, which makes the entities having strong correlations get distilled by the same network <ref type=\"bibr\" target=\"#b7\">[8]</ref>. \ud835\udefe is a hyperparameter balancing the two terms. In this work .org/ns/1.0\"><head n=\"4.1\">Experimental Setup</head><p>We closely follow the experiment setup of DE <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, for a thorough evaluation, we make two changes in  2\">[3]</ref>, which is the state-of-the-art top-\ud835\udc41 recommendation method, as a base model. 2) unlike <ref type=\"bibr\" target=\"#b7\">[8]</ref> that samples negative items for evaluation, we adopt the ful \">[20]</ref>: A KD method utilizing the original hint regression.</p><p>\u2022 Distillation Experts (DE) <ref type=\"bibr\" target=\"#b7\">[8]</ref>: The state-of-the-art KD method distilling the latent knowle RD <ref type=\"bibr\" target=\"#b21\">[22]</ref>, CD <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and RRD <ref type=\"bibr\" target=\"#b7\">[8]</ref>) in the competitors, because they are not competing with the in the competitors, because they are not competing with the methods distilling the latent knowledge <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Instead, we provide experiment results when they are combin on-based KD method. We report the results with the state-of-the-art prediction KD method (i.e., RRD <ref type=\"bibr\" target=\"#b7\">[8]</ref>) on CiteU-Like with \ud835\udf19 = 0.1 in Figure <ref type=\"figure\" tar distilling the latent knowledge (i.e., DE and HTD). This result aligns with the results reported in <ref type=\"bibr\" target=\"#b7\">[8]</ref> and shows the importance of distilling the latent knowledge. ><table /></figure> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\"><ref type=\"bibr\" target=\"#b7\">[8]</ref> proposes two KD methods: one by prediction and the other by   each user are held out for testing/validation, and the rest are used for training. However, unlike <ref type=\"bibr\" target=\"#b7\">[8]</ref> that samples a predefined number (e.g., 499) of unobserved i \u22125 }.</p><p>For the hint regression-related setup, we closely follow the setup reported in DE paper <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Specifically, two-layer MLP with [\ud835\udc51 \ud835\udc60 \u2192 (\ud835\udc51 \ud835\udc60 + \ud835\udc51 \ud835\udc61 )/2 \u2192 \ud835\udc51 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: igure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_2\"><head>\u2022</head><label></label><figDesc>NeuMF<ref type=\"bibr\" target=\"#b3\">[4]</ref>: A deep model that combines MF and Multi-Layer Perceptron (M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have distilled know >[20]</ref> proposes \"hint regression\" that matches the intermediate representations. Subsequently, <ref type=\"bibr\" target=\"#b27\">[28]</ref> matches the gram matrices of the representations, <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is time-consuming, it enables a more thorough evaluation compared to the sampling-based evaluation <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. We evaluate all met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns alone is insufficient because meaningful intermediate information is ignored, subsequent methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have distilled know >[20]</ref> proposes \"hint regression\" that matches the intermediate representations. Subsequently, <ref type=\"bibr\" target=\"#b27\">[28]</ref> matches the gram matrices of the representations, <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y. Several methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> have utilized the binary representations of users and items. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have distilled knowledge from the teacher's intermediate la uently, <ref type=\"bibr\" target=\"#b27\">[28]</ref> matches the gram matrices of the representations, <ref type=\"bibr\" target=\"#b28\">[29]</ref> matches the attention maps from the networks, and <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: serving transformations <ref type=\"bibr\" target=\"#b0\">[1]</ref>, pruning and compression techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns alone is insufficient because meaningful intermediate information is ignored, subsequent methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have distilled know >[20]</ref> proposes \"hint regression\" that matches the intermediate representations. Subsequently, <ref type=\"bibr\" target=\"#b27\">[28]</ref> matches the gram matrices of the representations, <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [8]</ref>. However, for a thorough evaluation, we make two changes in the setup. 1) we add LightGCN <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which is the state-of-the-art top-\ud835\udc41 recommendation method,  odel that combines MF and Multi-Layer Perceptron (MLP) to learn the user-item interaction.\u2022 LightGCN<ref type=\"bibr\" target=\"#b2\">[3]</ref>: The state-of-the-art model which adopts simplified Graph Co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms having fewer than 10 interactions (FourSquare) as done in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Table <ref type=\"table\">5</ref> summarizes the statistics . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us approaches have been proposed for reducing the model size and inference latency. Several methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y. Several methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> have utilized the binary representations of users and items. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y. Several methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> have utilized the binary representations of users and items. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y. Several methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> have utilized the binary representations of users and items. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: itectures and learning strategies, which are widely used for top-\ud835\udc41 recommendation task.</p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b18\">[19]</ref>: A learning-to-rank model that models user-item interactio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: serving transformations <ref type=\"bibr\" target=\"#b0\">[1]</ref>, pruning and compression techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: itectures and learning strategies, which are widely used for top-\ud835\udc41 recommendation task.</p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b18\">[19]</ref>: A learning-to-rank model that models user-item interactio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_2\"><head>\u2022</head><label></label><figDesc>NeuMF<ref type=\"bibr\" target=\"#b3\">[4]</ref>: A deep model that combines MF and Multi-Layer Perceptron (M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ification of the attention weights of BERT alongside BSR sparsity optimizations in the TVM compiler <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2018]</ref>. We show how algorithms and compiler optimiz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esearch area dedicated to writing libraries to accelerate sparse neural networks on these platforms <ref type=\"bibr\" target=\"#b7\">[Elsen et al., 2020]</ref> and next generation hardware has native spa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>We use the official BERT model from Google as the starting point. Following the notation from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>, we denote the number of layers (i.e., tran blocks are our pruning target.</p><p>Data: In pre-training, we use the same pre-training corpora as <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>: BookCorpus (800M words) <ref type=\"bibr\" t </ref> .</p><p>Input/Output representations: We follow the input/output representation setting from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref> for both pre-training and fine-tuning. We u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esearch area dedicated to writing libraries to accelerate sparse neural networks on these platforms <ref type=\"bibr\" target=\"#b7\">[Elsen et al., 2020]</ref> and next generation hardware has native spa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>We use the official BERT model from Google as the starting point. Following the notation from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>, we denote the number of layers (i.e., tran blocks are our pruning target.</p><p>Data: In pre-training, we use the same pre-training corpora as <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>: BookCorpus (800M words) <ref type=\"bibr\" t </ref> .</p><p>Input/Output representations: We follow the input/output representation setting from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref> for both pre-training and fine-tuning. We u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esearch area dedicated to writing libraries to accelerate sparse neural networks on these platforms <ref type=\"bibr\" target=\"#b7\">[Elsen et al., 2020]</ref> and next generation hardware has native spa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>We use the official BERT model from Google as the starting point. Following the notation from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>, we denote the number of layers (i.e., tran blocks are our pruning target.</p><p>Data: In pre-training, we use the same pre-training corpora as <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>: BookCorpus (800M words) <ref type=\"bibr\" t </ref> .</p><p>Input/Output representations: We follow the input/output representation setting from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref> for both pre-training and fine-tuning. We u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  CoLA<ref type=\"bibr\" target=\"#b23\">[Warstadt et al., 2018]</ref>, Stanford Sentiment Treebank (SST)<ref type=\"bibr\" target=\"#b19\">[Socher et al., 2013]</ref>, Microsoft Research Paragraph Corpus (MRP. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b17\">[Rajpurkar et al., 2016]</ref>, Recognizing Textual Entailment (RTE) and Winograd NLI(WNLI)<ref type=\"bibr\" target=\"#b14\">[Levesque et al., 2012]</ref>.</note> \t\t</body> \t\t<back>  \t\t\t<div typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on with input tensor. The BSR format and sparse multiplication operator implementation follow SciPy <ref type=\"bibr\" target=\"#b21\">[Virtanen et al., 2020]</ref>.</p><p>\u2022 The TVM task scheduler is able. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>We use the official BERT model from Google as the starting point. Following the notation from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>, we denote the number of layers (i.e., tran blocks are our pruning target.</p><p>Data: In pre-training, we use the same pre-training corpora as <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>: BookCorpus (800M words) <ref type=\"bibr\" t </ref> .</p><p>Input/Output representations: We follow the input/output representation setting from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref> for both pre-training and fine-tuning. We u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s.</p><p>Some work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> proposes different strategy of adding or removing edges to  verages these insights to improve performance in GNN-based node classification via edge prediction. <ref type=\"bibr\" target=\"#b25\">[26]</ref> present the Node-Parallel Augmentation scheme, that create. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ginal GCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> and input vectors are row-normalized accordingly <ref type=\"bibr\" target=\"#b7\">[8]</ref>. For our model, we train nine 2-layer GCNs with the same hid. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce Criterion(HSIC) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, a widely used dependency measurement <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, as a penalty term. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. <ref type=\"bibr\" target=\"#b31\">[32]</ref> introduce data augmentation on graphs and present two heur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sets: Citeseer <ref type=\"bibr\" target=\"#b9\">[10]</ref> is research paper citation network, UAI2010 <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a dataset for community detection, ACM <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y works have proposed data augmentation technologies on different types of features, such as images <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ures, such as images <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, texts <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: years, due to the superior performance on graph data mining <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>For graph-ba etwork, UAI2010 <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a dataset for community detection, ACM <ref type=\"bibr\" target=\"#b23\">[24]</ref> is research paper coauthor network extracted from ACM data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: x sparse, too. So before the process of data augmentation, we first use the update rule proposed in <ref type=\"bibr\" target=\"#b2\">[3]</ref> through the original adjacency matrix A to build new edges b tures contain more useful information than original graph features and help to node classification. <ref type=\"bibr\" target=\"#b2\">(3)</ref> We noticed that two graph augmentation methods GAug and MCGL. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y works have proposed data augmentation technologies on different types of features, such as images <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ocal graph structure <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. <ref type=\"bibr\" target=\"#b31\">[32]</ref> introduce data aug. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and RoBERTa <ref type=\"bibr\" target=\"#b25\">(Liu et al., 2019b)</ref>) to be factual knowledge bases <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bouraoui et  paradigms, as shown in Figure <ref type=\"figure\" target=\"#fig_0\">1:</ref> \u2022 Prompt-based retrieval <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Jiang et a eratures have shown that such paradigms can achieve decent performance on some benchmarks like LAMA <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019)</ref>.</p><p>Despite some reported success, cu rent answer distribution from the widely-used LAMA<ref type=\"foot\" target=\"#foot_2\">4</ref> dataset <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019)</ref>. However, we find that the prediction di Models (PLMs) raises the question of whether PLMs can be directly used as reliable knowledge bases. <ref type=\"bibr\" target=\"#b30\">Petroni et al. (2019)</ref> propose the LAMA benchmark, which probes  e regarded as the answer. We consider three kinds of prompts: the manually prompts T man created by <ref type=\"bibr\" target=\"#b30\">Petroni et al. (2019)</ref>, the mining-based prompts T mine by <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: another representative category of language models -the generative pre-trained models (e.g., GPT2/3 <ref type=\"bibr\" target=\"#b32\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Brown et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 9b)</ref>) to be factual knowledge bases <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bouraoui et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Jiang et al Ettinger, 2020)</ref> and world knowledge <ref type=\"bibr\" target=\"#b5\">(Davison et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bouraoui et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">Forbes et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sult, Chicago, Big City and City will all be introduced into ETG. Then we apply topological sorting <ref type=\"bibr\" target=\"#b4\">(Cook, 1985)</ref> to ETG to obtain a Fine-to-Coarse entity type seque. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: A. There are many studies focus on probing specific knowledge in PLMs, such as linguistic knowledge <ref type=\"bibr\" target=\"#b23\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b41\">Tenney et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: diction tasks such as NER <ref type=\"bibr\" target=\"#b20\">(Lample et al., 2016)</ref> and factoid QA <ref type=\"bibr\" target=\"#b14\">(Iyyer et al., 2014)</ref>. Moreover, based on the mechanism of incor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b33\">Roberts et al., 2020;</ref><ref type=\"bibr\">Lin et al., 2020;</ref><ref type=\"bibr\" target=\"#b40\">Tamborrino et al., 2020)</ref>. Recently, some studies doubt the reli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b23\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b41\">Tenney et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Liu et al., 2019a;</ref><ref type=\"bibr\">Htut et al., 2019;</ref><ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: UNI, we first collect all the triples which belong to the same 41 relations with LAMA from Wikidata <ref type=\"bibr\" target=\"#b42\">(Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014)</ref>, then we randomly sample 50K tri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: diction tasks such as NER <ref type=\"bibr\" target=\"#b20\">(Lample et al., 2016)</ref> and factoid QA <ref type=\"bibr\" target=\"#b14\">(Iyyer et al., 2014)</ref>. Moreover, based on the mechanism of incor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t et al., 2019)</ref>, semantic knowledge <ref type=\"bibr\" target=\"#b41\">(Tenney et al., 2019;</ref><ref type=\"bibr\" target=\"#b43\">Wallace et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Ettinger, 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d event representations are also natural language words, we adopt the pre-trained language model T5 <ref type=\"bibr\" target=\"#b35\">(Raffel et al., 2020)</ref> as our transformer-based encoder-decoder . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: xploit the event schema knowledge, we propose to employ a trie-based constrained decoding algorithm <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b3\">Cao et al., 20 o form event records via constrained decoding <ref type=\"bibr\" target=\"#b3\">(Cao et al., 2021;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a)</ref>, which allows TEXT2EVENT to handle various e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: N, Arg1: the man}.</p><p>Currently, most event extraction methods employ the decomposition strategy <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b33\">Nguyen and Ngu arget=\"#b12\">Hong et al., , 2018;;</ref><ref type=\"bibr\" target=\"#b15\">Huang and Riloff, 2012;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b36\">Sha et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r decision interactions and information sharing. The neural encoder-decoder generation architecture <ref type=\"bibr\" target=\"#b39\">(Sutskever et al., 2014;</ref><ref type=\"bibr\" target=\"#b1\">Bahdanau . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a trie-based constrained decoding algorithm <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b3\">Cao et al., 2021)</ref> for event generation. During constrained decod EVENT directly generate event schemas and text spans to form event records via constrained decoding <ref type=\"bibr\" target=\"#b3\">(Cao et al., 2021;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b5\">(Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b33\">Nguyen and Nguyen, 2019;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Zhang et al. d preprocessing step as the previous work <ref type=\"bibr\" target=\"#b54\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Du and Cardie e used the same criteria in previous work <ref type=\"bibr\" target=\"#b54\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Lin et al.,  2019); DYGIE++ is a BERT-based model which captures both within-sentence and cross-sentence context <ref type=\"bibr\" target=\"#b43\">(Wadden et al., 2019)</ref>; GAIL is an inverse reinforcement learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b21\">Liao and Grishman, 2010;</ref><ref type=\"bibr\" target=\"#b11\">Hong et al., 2011</ref><ref type=\"bibr\" target=\"#b12\">Hong et al., , 2018;;</ref><ref type=\"bibr\" target=\"#b15\">Huang and R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b15\">Huang and Riloff, 2012;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b36\">Sha et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">Lin et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b5\">Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b36\">Sha et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">Lin et al., 2018;</ref><ref type=\"bibr\" target=\"#b48\">Yang et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: v\u00e1 et al., 2019)</ref>, relation extraction <ref type=\"bibr\" target=\"#b50\">(Zeng et al., 2018;</ref><ref type=\"bibr\" target=\"#b53\">Zhang et al., 2020b)</ref>, and aspect term extraction <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a trie-based constrained decoding algorithm <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b3\">Cao et al., 2021)</ref> for event generation. During constrained decod EVENT directly generate event schemas and text spans to form event records via constrained decoding <ref type=\"bibr\" target=\"#b3\">(Cao et al., 2021;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b29\">(McClosky et al., 2011;</ref><ref type=\"bibr\" target=\"#b20\">Li et al., 2013</ref><ref type=\"bibr\" target=\"#b19\">Li et al., , 2014;;</ref><ref type=\"bibr\" target=\"#b47\">Yang and Mitc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ptimized our model using label smoothing <ref type=\"bibr\" target=\"#b40\">(Szegedy et al., 2016;</ref><ref type=\"bibr\" target=\"#b31\">M\u00fcller et al., 2019)</ref> and AdamW <ref type=\"bibr\" target=\"#b26\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: benchmarks: ACE05-EN + and ERE-EN, using the same split and preprocessing step in the previous work <ref type=\"bibr\" target=\"#b23\">(Lin et al., 2020)</ref>. Compared to ACE05-EN, ACE05-EN + and ERE-EN IE system which employs global feature and beam search to extract globally optimal event structures <ref type=\"bibr\" target=\"#b23\">(Lin et al., 2020)</ref>.</p><p>Implementations. We optimized our mod \" target=\"#b54\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Lin et al., 2020)</ref>. Since TEXT2EVENT is a text generation model,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b5\">(Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b33\">Nguyen and Nguyen, 2019;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Zhang et al. d preprocessing step as the previous work <ref type=\"bibr\" target=\"#b54\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Du and Cardie e used the same criteria in previous work <ref type=\"bibr\" target=\"#b54\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Lin et al.,  2019); DYGIE++ is a BERT-based model which captures both within-sentence and cross-sentence context <ref type=\"bibr\" target=\"#b43\">(Wadden et al., 2019)</ref>; GAIL is an inverse reinforcement learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b47\">Yang and Mitchell, 2016;</ref><ref type=\"bibr\" target=\"#b32\">Nguyen et al., 2016;</ref><ref type=\"bibr\" target=\"#b25\">Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b52\">Zhang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b19\">Li et al., , 2014;;</ref><ref type=\"bibr\" target=\"#b47\">Yang and Mitchell, 2016;</ref><ref type=\"bibr\" target=\"#b32\">Nguyen et al., 2016;</ref><ref type=\"bibr\" target=\"#b25\">Liu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b9\">Du and Cardie, 2020;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 2020;</ref><ref type=\"bibr\" target=\"#b24\">Liu et al., 2020)</ref>.</p><p>Compared with previous methods, we mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on methods employ the decomposition strategy <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b33\">Nguyen and Nguyen, 2019;</ref><ref type=\"bibr\" target=\"#b43\">Wadden e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b9\">Du and Cardie, 2020;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 20 l., 2020a)</ref>, and 4) question-answering <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2020b;</ref><ref type=\"bibr\" target=\"#b9\">Du and Cardie, 2020;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 20 \" target=\"#b54\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Du and Cardie, 2020)</ref>, and we denote it as ACE05-EN.</p><p>In add 021)</ref>; Multi-task TANL extends TANL by transferring structure knowledge from other tasks; EEQA <ref type=\"bibr\" target=\"#b9\">(Du and Cardie, 2020)</ref> and MQAEE <ref type=\"bibr\" target=\"#b18\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ptimized our model using label smoothing <ref type=\"bibr\" target=\"#b40\">(Szegedy et al., 2016;</ref><ref type=\"bibr\" target=\"#b31\">M\u00fcller et al., 2019)</ref> and AdamW <ref type=\"bibr\" target=\"#b26\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b22\">Lin et al., 2018;</ref><ref type=\"bibr\" target=\"#b48\">Yang et al., 2019;</ref><ref type=\"bibr\" target=\"#b45\">Wang et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Ma et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rks (GANs) <ref type=\"bibr\">[Sabokrou et al., 2018;</ref><ref type=\"bibr\">Perera et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020a]</ref> have been a common choice for novelty detect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nation between the downscaled feature A z and z as an input pair. \u03b2 is a hyperparameter. Similar to <ref type=\"bibr\" target=\"#b4\">[Hjelm et al., 2019]</ref>, the optimization of Eq. ( <ref type=\"formu f face detection is from the video. To this end, we first extract the optical flow using FlowNet2.0 <ref type=\"bibr\" target=\"#b4\">[Ilg et al., 2017]</ref> from each video with 30 fps.</p><p>Following . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lty classes. Recently, self-supervised learning <ref type=\"bibr\">[Komodakis and Gidaris, 2018;</ref><ref type=\"bibr\" target=\"#b5\">Ji et al., 2019]</ref> holds great for improving representations when   discriminative ability of features that fails to effectively detect novelty samples. Besides, some <ref type=\"bibr\" target=\"#b5\">[Lim et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Sinha et al., 20 ved by our unsupervised method significantly lower than supervision methods except auxiliary method <ref type=\"bibr\" target=\"#b5\">[Liu et al., 2018]</ref>. This is probably due to the help of the addi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resentation by solving some specialized pretext tasks, such as geometric transformations prediction <ref type=\"bibr\" target=\"#b3\">[Hendrycks et al., 2019]</ref>. During inference, the model is transfe tion prediction <ref type=\"bibr\">[Komodakis and Gidaris, 2018]</ref> and transformations prediction <ref type=\"bibr\" target=\"#b3\">[Hendrycks et al., 2019]</ref> could capture the semantic information   on the pretext tasks (e.g., <ref type=\"bibr\">RotNet [Komodakis and Gidaris, 2018]</ref>, Geometric <ref type=\"bibr\" target=\"#b3\">[Hendrycks et al., 2019]</ref>) achieves higher performance, compared  rning based methods (e.g. <ref type=\"bibr\">RotNet [Komodakis and Gidaris, 2018]</ref> and Geometric <ref type=\"bibr\" target=\"#b3\">[Hendrycks et al., 2019]</ref>) shows worse performance on DCASE. This. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctively detect novelty samples. Besides, some <ref type=\"bibr\" target=\"#b5\">[Lim et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Sinha et al., 2021]</ref> use data augmentation as an additional sourc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lty classes. Recently, self-supervised learning <ref type=\"bibr\">[Komodakis and Gidaris, 2018;</ref><ref type=\"bibr\" target=\"#b5\">Ji et al., 2019]</ref> holds great for improving representations when   discriminative ability of features that fails to effectively detect novelty samples. Besides, some <ref type=\"bibr\" target=\"#b5\">[Lim et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Sinha et al., 20 ved by our unsupervised method significantly lower than supervision methods except auxiliary method <ref type=\"bibr\" target=\"#b5\">[Liu et al., 2018]</ref>. This is probably due to the help of the addi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nation between the downscaled feature A z and z as an input pair. \u03b2 is a hyperparameter. Similar to <ref type=\"bibr\" target=\"#b4\">[Hjelm et al., 2019]</ref>, the optimization of Eq. ( <ref type=\"formu f face detection is from the video. To this end, we first extract the optical flow using FlowNet2.0 <ref type=\"bibr\" target=\"#b4\">[Ilg et al., 2017]</ref> from each video with 30 fps.</p><p>Following . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of the margins p(x)p(y). According to the definition of the variational estimation of JS divergence <ref type=\"bibr\" target=\"#b8\">[Nowozin et al., 2016]</ref>, the maximization of the mutual informati ion error is larger than a predefined threshold T , and a normal instance otherwise. We use PyTorch <ref type=\"bibr\" target=\"#b8\">[Paszke et al., 2019]</ref> to implement our method. For training para. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >), only learn the partial modes of target distributions, which causes the problem of mode dropping <ref type=\"bibr\" target=\"#b0\">[Arora et al., 2018]</ref>. Second, the imbalance capacity of generato ion is, the more similar two variables have. Given two random variables x and y, mutual information <ref type=\"bibr\" target=\"#b0\">[Belghazi et al., 2018]</ref> can be estimated by the JS divergence be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b4\">[Ilg et al., 2017]</ref> from each video with 30 fps.</p><p>Following <ref type=\"bibr\" target=\"#b10\">[Tu et al., 2020]</ref>, the detection models select the training set. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sing a high quality autoencoding DNN <ref type=\"bibr\" target=\"#b2\">(Bourlard and Kamp [1988]</ref>, <ref type=\"bibr\" target=\"#b20\">Wang et al. [2014]</ref>) which first compresses the given image into. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b9\">(LeCun et al. [2010]</ref>, <ref type=\"bibr\" target=\"#b8\">Krizhevsky [2009]</ref>, <ref type=\"bibr\" target=\"#b15\">Russakovsky et al. [2015]</ref>). For each data set, we approximated . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ope et al. [2021]</ref>). We can approximate this manifold by using a high quality autoencoding DNN <ref type=\"bibr\" target=\"#b2\">(Bourlard and Kamp [1988]</ref>, <ref type=\"bibr\" target=\"#b20\">Wang e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>For the ImageNet experiments we used a pre-trained ResNet50 network from the pytorch package <ref type=\"bibr\" target=\"#b6\">(He et al. [2016]</ref>).</p><p>For the ImageNet autoencoder we used t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ope et al. [2021]</ref>). We can approximate this manifold by using a high quality autoencoding DNN <ref type=\"bibr\" target=\"#b2\">(Bourlard and Kamp [1988]</ref>, <ref type=\"bibr\" target=\"#b20\">Wang e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ical details</head><p>The synthetic experiments were conducted with GPU K80 using Pytorch framework <ref type=\"bibr\" target=\"#b12\">(Paszke et al. [2019]</ref>) and Numpy python-library <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b9\">(LeCun et al. [2010]</ref>, <ref type=\"bibr\" target=\"#b8\">Krizhevsky [2009]</ref>, <ref type=\"bibr\" target=\"#b15\">Russakovsky et al. [2015]</ref>). For each data set, we approximated . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ope et al. [2021]</ref>). We can approximate this manifold by using a high quality autoencoding DNN <ref type=\"bibr\" target=\"#b2\">(Bourlard and Kamp [1988]</ref>, <ref type=\"bibr\" target=\"#b20\">Wang e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  n=\"1\">Introduction</head><p>In 2013 <ref type=\"bibr\" target=\"#b19\">Szegedy et al. [2013]</ref> and <ref type=\"bibr\" target=\"#b1\">Biggio et al. [2013]</ref> independently demonstrated the surprising f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ope et al. [2021]</ref>). We can approximate this manifold by using a high quality autoencoding DNN <ref type=\"bibr\" target=\"#b2\">(Bourlard and Kamp [1988]</ref>, <ref type=\"bibr\" target=\"#b20\">Wang e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>For the ImageNet experiments we used a pre-trained ResNet50 network from the pytorch package <ref type=\"bibr\" target=\"#b6\">(He et al. [2016]</ref>).</p><p>For the ImageNet autoencoder we used t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" targ rget=\"#b10\">11]</ref> have been two fast-rising fields, boosted by lottery tickets hypothesis (LTH) <ref type=\"bibr\" target=\"#b9\">[10]</ref> and singleshot network pruning (SNIP) <ref type=\"bibr\" targ performance of the subnetworks discovered by GraNet. The authors of Lottery Ticket Hypothesis (LTH) <ref type=\"bibr\" target=\"#b9\">[10]</ref> introduced a retraining technique, even if they did not eva m accuracy drops. This finding makes a connection to the success of the iterative magnitude pruning <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" tar inference. The pruning criterion includes weight magnitude <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, gradient <ref type=\"bibr\" target=\"#b60\">[61]</ref> Hessian . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [61]</ref> Hessian <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, Taylor expansion <ref type=\"bibr\" target=\"#b46\">[47,</ref> limited to Hessian <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, Taylor expansion <ref type=\"bibr\" target=\"#b46\">[47,</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th the off-the-shelf hardware. In particular, we choose the filter pruning method used in Li et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Unstructured sparsity is a more promising direction not o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b30\">[31]</ref>, Gradient Flow <ref type=\"bibr\" target=\"#b66\">[67]</ref>, Neural Tangent Kernel <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>.</p><p>One-shot pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urons, to the best of our knowledge, were proposed in <ref type=\"bibr\" target=\"#b23\">[24]</ref> and <ref type=\"bibr\" target=\"#b49\">[50]</ref>. After that, various pruning methods have emerged to provi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  achieving a lasting functional recovery. Such mechanism is closely related to the brain plasticity <ref type=\"bibr\" target=\"#b50\">[51]</ref>, and we borrow this concept to developing a computational . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> is another class of methods that prune models during traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b66\">67,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> have been two fast-rising fields, boosted by lottery ticket the existing methods fail to match the performance achieved by the magnitude pruning after training <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Compared with the above-mentioned two classes of pruning, re the possibility of obtaining a trainable sparse neural network before the main training process. <ref type=\"bibr\" target=\"#b10\">[11]</ref> demonstrates that the existing methods for pruning at init es are in line with the baselines reported in the references <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eating the sparsity pattern) which recently has received an upsurge of interest in machine learning <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targ ified as dense-to-sparse training as they start from a dense network. Dynamic Sparse Training (DST) <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targ ng to GMP <ref type=\"bibr\" target=\"#b76\">[77,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> and DST <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ e.g., GMP <ref type=\"bibr\" target=\"#b76\">[77,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> and DST <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ  regeneration scheme is to randomly activate new connections <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. However, it would take a lot of time for random regenerati sparse-to-sparse training. Different from the existing sparse-to-sparse training methods, i.e., SET <ref type=\"bibr\" target=\"#b43\">[44]</ref>, RigL <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and ITOP <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing training via L 0 and L 1 regularization, respectively. <ref type=\"bibr\" target=\"#b59\">[60,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ound 2/3 of the training FLOPs is owing to the backward pass <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b71\">72]</ref>.</p><p>Let us denote r as the ratio of the number of the re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed to give different weights to frame level speaker information. Finally, a statistic pooling layer <ref type=\"bibr\" target=\"#b29\">[30]</ref> is used to aggregate weighted frame level speaker informat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y one utterance of an unseen speaker, we propose a deep discriminative speaker encoder. Inspired by <ref type=\"bibr\" target=\"#b27\">[28]</ref>, first residual network and squeezeand-excitation network  atial translation invariance, which make convolution layer suitable to extract frame level features <ref type=\"bibr\" target=\"#b27\">[28]</ref>. SE block expands the temporal context of the frame level  nel interdependence in features, which has been verified to be helpful in speaker verification task <ref type=\"bibr\" target=\"#b27\">[28]</ref>. The framework of SE block is shown in Figure <ref type=\"f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b27\">[28]</ref>, first residual network and squeezeand-excitation network <ref type=\"bibr\" target=\"#b28\">[29]</ref> are integrated to extract discriminative frame level speak . Increasing the depth of a neural network can significantly improve the quality of representations <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Additionally, batch normalization helps to improve the st /ns/1.0\"><head n=\"2.2.\">Squeeze-and-excitation network</head><p>Squeeze-and-excitation (SE) network <ref type=\"bibr\" target=\"#b28\">[29]</ref> is first introduced to model channel interdependence in fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b9\">10]</ref>, and neural network based methods <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urce speaker to make it sound like that of a target speaker without changing the linguistic content <ref type=\"bibr\" target=\"#b0\">[1]</ref>. This technique has many applications, including expressive . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  limits their use in the real application scenarios. Recently, one-shot voice conversion approaches <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: speaker information extraction, such as i-vector <ref type=\"bibr\" target=\"#b21\">[22]</ref>, dvector <ref type=\"bibr\" target=\"#b22\">[23]</ref>, or x-vector <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b3\">4]</ref>, frequency warping approaches <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, exemplar based methods. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b3\">4]</ref>, frequency warping approaches <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, exemplar based methods. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: exemplar based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, and neural network based methods <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, exemplar based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic features as input and outputs frame level features. It can be done via recurrent neural networks <ref type=\"bibr\" target=\"#b25\">[26]</ref> and convolutional neural networks <ref type=\"bibr\" target= egates variable-length frame level features into utterance level speaker embedding. Average pooling <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> is a popular metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hieve voice conversion, such as Gaussian mixture model (GMM) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, frequency warping appr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with a conversion model or a pre-trained model for speaker information extraction, such as i-vector <ref type=\"bibr\" target=\"#b21\">[22]</ref>, dvector <ref type=\"bibr\" target=\"#b22\">[23]</ref>, or x-v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  limits their use in the real application scenarios. Recently, one-shot voice conversion approaches <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p neural network and avoids the vanishing gradient problem <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Increasing the depth of a neural network can significantly. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic features as input and outputs frame level features. It can be done via recurrent neural networks <ref type=\"bibr\" target=\"#b25\">[26]</ref> and convolutional neural networks <ref type=\"bibr\" target= egates variable-length frame level features into utterance level speaker embedding. Average pooling <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> is a popular metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with a conversion model or a pre-trained model for speaker information extraction, such as i-vector <ref type=\"bibr\" target=\"#b21\">[22]</ref>, dvector <ref type=\"bibr\" target=\"#b22\">[23]</ref>, or x-v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with a conversion model or a pre-trained model for speaker information extraction, such as i-vector <ref type=\"bibr\" target=\"#b21\">[22]</ref>, dvector <ref type=\"bibr\" target=\"#b22\">[23]</ref>, or x-v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with a conversion model or a pre-trained model for speaker information extraction, such as i-vector <ref type=\"bibr\" target=\"#b21\">[22]</ref>, dvector <ref type=\"bibr\" target=\"#b22\">[23]</ref>, or x-v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. While these works . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> are proposed. Compared with previous methods, source and ta ameworks of available one-shot voice conversion approaches <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> generally consist of a speaker encoder, a content encoder, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with a conversion model or a pre-trained model for speaker information extraction, such as i-vector <ref type=\"bibr\" target=\"#b21\">[22]</ref>, dvector <ref type=\"bibr\" target=\"#b22\">[23]</ref>, or x-v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with a conversion model or a pre-trained model for speaker information extraction, such as i-vector <ref type=\"bibr\" target=\"#b21\">[22]</ref>, dvector <ref type=\"bibr\" target=\"#b22\">[23]</ref>, or x-v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  limits their use in the real application scenarios. Recently, one-shot voice conversion approaches <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urce speaker to make it sound like that of a target speaker without changing the linguistic content <ref type=\"bibr\" target=\"#b0\">[1]</ref>. This technique has many applications, including expressive . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: volutional neural network based residual network (ResNet) is a powerful speaker embedding extractor <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The utterance lev rmance for both longduration and short-duration utterances <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>.</p><p>The architecture of ResNet based speaker embedding e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he content encoder extracts frame level content representation from   database and VCC 2016 dataset <ref type=\"bibr\" target=\"#b34\">[35]</ref> respectively. For CMU-ARCTIC database, we select clb (fema. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p neural network and avoids the vanishing gradient problem <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Increasing the depth of a neural network can significantly. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, frequency warping approaches <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he content encoder extracts frame level content representation from   database and VCC 2016 dataset <ref type=\"bibr\" target=\"#b34\">[35]</ref> respectively. For CMU-ARCTIC database, we select clb (fema. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g to the two entities, and L is the max length of all input sequences. Following standard practices <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref>, we add two special tokens to mark the beg d it soon becomes the backbone of many following LMs. By pre-training on a large-scale corpus, BERT <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref> obtains the ability to capture a notable a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rding the strategy for selecting instances inside the bag, the soft attention mechanism proposed by <ref type=\"bibr\" target=\"#b12\">Lin et al. (2016)</ref> is widely used for its better performance tha results in data noise and causes classical supervised RE models hard to train. To solve this issue, <ref type=\"bibr\" target=\"#b12\">Lin et al. (2016)</ref>   <ref type=\"bibr\" target=\"#b5\">(Defferrard e K is the bag size. As for the choice of weight \u03b1 i , we follow the soft attention mechanism used in <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2016)</ref>, where \u03b1 i is the normalized attention score ttp://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Evaluation Metrics</head><p>Following previous literature <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Vashishth et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: (2020)</ref> trained a discriminative model for language representation learning. Recent literature <ref type=\"bibr\" target=\"#b17\">(Peng et al., 2020)</ref> has also attempted to relate the contrastiv elate the contrastive pre-training scheme to classical supervised RE task. Different from our work, <ref type=\"bibr\" target=\"#b17\">Peng et al. (2020)</ref> aims to utilize abundant DS data and help cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s (i.e.negative pairs) should be far away.</p><p>Experiments on three public DSRE benchmarks -NYT10 <ref type=\"bibr\" target=\"#b20\">(Riedel et al., 2010;</ref><ref type=\"bibr\" target=\"#b10\">Hoffmann et d the dataset statistics are listed in Table <ref type=\"table\" target=\"#tab_0\">1</ref>.</p><p>NYT10 <ref type=\"bibr\" target=\"#b20\">(Riedel et al., 2010)</ref> aligns Freebase entity relations with New. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <head n=\"3.3\">Baseline Models</head><p>We choose six recent methods as baseline models.</p><p>Mintz <ref type=\"bibr\" target=\"#b15\">(Mintz et al., 2009</ref> </p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dford et al., 2018)</ref>, XL-Net <ref type=\"bibr\" target=\"#b24\">(Yang et al., 2019)</ref> and GPT2 <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref> are also well-known pre-trained represen ll-known pre-trained representatives with excellent transfer learning ability. Moreover, some works <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref> found that considerably increasing the s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inning and the end of two entities.</p><p>Position Embedding In the Transformer attention mechanism <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref>, positional encodings are injected to ma </p><p>Pre-trained LM Recently pre-trained language models achieved great success in the NLP field. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref> proposed a self-attention based architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s (i.e.negative pairs) should be far away.</p><p>Experiments on three public DSRE benchmarks -NYT10 <ref type=\"bibr\" target=\"#b20\">(Riedel et al., 2010;</ref><ref type=\"bibr\" target=\"#b10\">Hoffmann et d the dataset statistics are listed in Table <ref type=\"table\" target=\"#tab_0\">1</ref>.</p><p>NYT10 <ref type=\"bibr\" target=\"#b20\">(Riedel et al., 2010)</ref> aligns Freebase entity relations with New. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Hoffmann et al., 2011)</ref>, GDS <ref type=\"bibr\" target=\"#b11\">(Jat et al., 2018)</ref> and KBP <ref type=\"bibr\" target=\"#b13\">(Ling and Weld, 2012)</ref> demonstrate the effectiveness of our prop tity pair, and this dataset assures that the at-least-one assumption of MIL always holds.</p><p>KBP <ref type=\"bibr\" target=\"#b13\">(Ling and Weld, 2012)</ref> uses Wikipedia articles annotated with Fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d relation extraction <ref type=\"bibr\">(DSRE)</ref>.</p><p>With awareness of the existing DS noise, <ref type=\"bibr\" target=\"#b27\">Zeng et al. (2015)</ref> introduces the multi-instance learning (MIL). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b12\">(Lin et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Vashishth et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Alt et al., 2019)</ref>, we first conduct a held-out evaluation to mea highly informative instance and label embeddings by exploiting BERT pre-trained model.</p><p>DISTRE <ref type=\"bibr\" target=\"#b0\">(Alt et al., 2019)</ref> A transformer-based model, GPT fine-tuned for c information from the text and improves DSRE models with additional side information from KBs. (4) <ref type=\"bibr\" target=\"#b0\">Alt et al. (2019)</ref> extended the GPT to the DSRE, and finetuned it. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trics</head><p>Following previous literature <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Vashishth et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Alt et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sides, to inherit the ability of language understanding from BERT and avoid catastrophic forgetting <ref type=\"bibr\" target=\"#b14\">(McCloskey and Cohen, 1989)</ref>, we also add the masked language mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\">Dai and Lin (2017)</ref> proposed to use contrastive learning for image caption, and <ref type=\"bibr\" target=\"#b4\">Clark et al. (2020)</ref> trained a discriminative model for language . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inning and the end of two entities.</p><p>Position Embedding In the Transformer attention mechanism <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref>, positional encodings are injected to ma </p><p>Pre-trained LM Recently pre-trained language models achieved great success in the NLP field. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref> proposed a self-attention based architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Hoffmann et al., 2011)</ref>, GDS <ref type=\"bibr\" target=\"#b11\">(Jat et al., 2018)</ref> and KBP <ref type=\"bibr\" target=\"#b13\">(Ling and Weld, 2012)</ref> demonstrate the effectiveness of our prop tity pair, and this dataset assures that the at-least-one assumption of MIL always holds.</p><p>KBP <ref type=\"bibr\" target=\"#b13\">(Ling and Weld, 2012)</ref> uses Wikipedia articles annotated with Fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Hoffmann et al., 2011)</ref>, GDS <ref type=\"bibr\" target=\"#b11\">(Jat et al., 2018)</ref> and KBP <ref type=\"bibr\" target=\"#b13\">(Ling and Weld, 2012)</ref> demonstrate the effectiveness of our prop tity pair, and this dataset assures that the at-least-one assumption of MIL always holds.</p><p>KBP <ref type=\"bibr\" target=\"#b13\">(Ling and Weld, 2012)</ref> uses Wikipedia articles annotated with Fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d relation extraction <ref type=\"bibr\">(DSRE)</ref>.</p><p>With awareness of the existing DS noise, <ref type=\"bibr\" target=\"#b27\">Zeng et al. (2015)</ref> introduces the multi-instance learning (MIL). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b16\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">He et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b23\">Wu et al. (2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers entity tokens e 1 and e 2 as equivalent to other common word tokens t i , which has been proven <ref type=\"bibr\" target=\"#b1\">(Baldini Soares et al., 2019)</ref> to be unsuitable for RE tasks. To . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <head n=\"3.3\">Baseline Models</head><p>We choose six recent methods as baseline models.</p><p>Mintz <ref type=\"bibr\" target=\"#b15\">(Mintz et al., 2009</ref> </p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b16\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">He et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b23\">Wu et al. (2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s (i.e.negative pairs) should be far away.</p><p>Experiments on three public DSRE benchmarks -NYT10 <ref type=\"bibr\" target=\"#b20\">(Riedel et al., 2010;</ref><ref type=\"bibr\" target=\"#b10\">Hoffmann et d the dataset statistics are listed in Table <ref type=\"table\" target=\"#tab_0\">1</ref>.</p><p>NYT10 <ref type=\"bibr\" target=\"#b20\">(Riedel et al., 2010)</ref> aligns Freebase entity relations with New. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sides, to inherit the ability of language understanding from BERT and avoid catastrophic forgetting <ref type=\"bibr\" target=\"#b14\">(McCloskey and Cohen, 1989)</ref>, we also add the masked language mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trics</head><p>Following previous literature <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Vashishth et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Alt et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers entity tokens e 1 and e 2 as equivalent to other common word tokens t i , which has been proven <ref type=\"bibr\" target=\"#b1\">(Baldini Soares et al., 2019)</ref> to be unsuitable for RE tasks. To . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b16\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">He et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b23\">Wu et al. (2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: models hard to train. To solve this issue, <ref type=\"bibr\" target=\"#b12\">Lin et al. (2016)</ref>   <ref type=\"bibr\" target=\"#b5\">(Defferrard et al., 2016)</ref> to encode syntactic information from t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">He et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b23\">Wu et al. (2018)</ref> proposed to use the non-parametric instance-le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b8\">(Hadsell et al., 2006;</ref><ref type=\"bibr\" target=\"#b16\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">He et al., 2020). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b8\">(Hadsell et al., 2006;</ref><ref type=\"bibr\" target=\"#b16\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">He et al., 2020). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ord-powered algorithms such as fastText <ref type=\"bibr\" target=\"#b4\">[5]</ref>, Byte-Pair Encoding <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and WordPiece <ref type=\"bibr\" target=\"#b35\">[36]</ref> b i-c.org/ns/1.0\"><head n=\"3.1\">Anchor Selection</head><p>Subword tokenization algorithms such as BPE <ref type=\"bibr\" target=\"#b36\">[37]</ref> employ deterministic strategies to create tokens and const. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rking datasets based on Freebase <ref type=\"bibr\" target=\"#b39\">[40]</ref> (~15K nodes) and WordNet <ref type=\"bibr\" target=\"#b8\">[9]</ref> (~40K nodes), training on larger graphs (e.g.,  of 120K node et=\"#b37\">[38]</ref>, tensor factorization <ref type=\"bibr\" target=\"#b17\">[18]</ref>, convolutional <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and hyperbolic <ref type=\"bibr\" target=\"#b6\">[7]</ref>. The. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hallow word embedding popularized with word2vec <ref type=\"bibr\" target=\"#b22\">[23]</ref> and GloVe <ref type=\"bibr\" target=\"#b27\">[28]</ref> that learned a vocabulary of 400K-2M most frequent words, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in sparse high-dimensional feature vectors of existing entities in a vocabulary. Recent approaches <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> employ anchor-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: these new edges are then utilized as contextual information to compute its embedding. Previous work <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs of existing entities in a vocabulary. Recent approaches <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> employ anchor-based hashing techniques to factorize sparse . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > 1000 <ref type=\"bibr\" target=\"#b37\">[38]</ref> 1000 <ref type=\"bibr\" target=\"#b37\">[38]</ref> 200 <ref type=\"bibr\" target=\"#b48\">[49]</ref> 512 <ref type=\"bibr\" target=\"#b45\">[46]</ref> 200 <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sary but relation types are more important (than considered before) supports the recent findings of <ref type=\"bibr\" target=\"#b38\">[39]</ref> that based its reasoning process only on relations seen in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: style license), PyKEEN <ref type=\"bibr\" target=\"#b2\">[3]</ref> (MIT License), and PyTorch-Geometric <ref type=\"bibr\" target=\"#b11\">[12]</ref> (MIT License). We ran experiments on a machine with one RT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: level objective, which is inspired by the well-studied Canonical Correlation Analysis (CCA) methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" tar Correlation Analysis. CCA is a classical multivariate analysis method, which is first introduced in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. For two random variables X 2 R m and Y 2 R n , their cova. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >[58]</ref>. We also compare with supervised learning models, including MLP, Label Propagation (LP) <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and supervised baselines GCN <ref type=\"bibr\" target=\"#b2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 9\">[20]</ref> for both stages. The graph encoder f \u2713 is specified as a standard two-layer GCN model <ref type=\"bibr\" target=\"#b21\">[22]</ref> for all the datasets except citeseer (where we empirically MLP, Label Propagation (LP) <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and supervised baselines GCN <ref type=\"bibr\" target=\"#b21\">[22]</ref> and GAT <ref type=\"bibr\" target=\"#b46\">[47]</ref> <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mparison with Peer Methods</head><p>We compare CCA-SSG with classical unsupervised models, Deepwalk <ref type=\"bibr\" target=\"#b31\">[32]</ref> and GAE <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and sel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and self-defined signals, among which contrastive methods <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar learning in vision <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targe ead><p>Contrastive Learning on Graphs. Contrastive methods <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar resentation learning, respectively. MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> generalizes CMC <ref type=\"bibr\" target=\"#b39\">[40]</ref> to graph-structured data by introducing graph diffusion <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mparison with Peer Methods</head><p>We compare CCA-SSG with classical unsupervised models, Deepwalk <ref type=\"bibr\" target=\"#b31\">[32]</ref> and GAE <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and sel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch. All experiments are conducted on a NVIDIA V100 GPU with 16 GB memory. We use the Adam optimizer <ref type=\"bibr\" target=\"#b19\">[20]</ref> for both stages. The graph encoder f \u2713 is specified as a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mparison with Peer Methods</head><p>We compare CCA-SSG with classical unsupervised models, Deepwalk <ref type=\"bibr\" target=\"#b31\">[32]</ref> and GAE <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and sel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 9\">[20]</ref> for both stages. The graph encoder f \u2713 is specified as a standard two-layer GCN model <ref type=\"bibr\" target=\"#b21\">[22]</ref> for all the datasets except citeseer (where we empirically MLP, Label Propagation (LP) <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and supervised baselines GCN <ref type=\"bibr\" target=\"#b21\">[22]</ref> and GAT <ref type=\"bibr\" target=\"#b46\">[47]</ref> <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y similar but nonequivalent samples. To address this issue, we employ smoothed linear interpolation <ref type=\"bibr\" target=\"#b3\">(Bowman et al., 2016;</ref><ref type=\"bibr\" target=\"#b59\">Zheng et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e, we employ smoothed linear interpolation <ref type=\"bibr\" target=\"#b3\">(Bowman et al., 2016;</ref><ref type=\"bibr\" target=\"#b59\">Zheng et al., 2019)</ref> between sentences in the embedding space to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  &amp; Lample, 2019)</ref> have been made for learning cross-lingual representation. More recently, <ref type=\"bibr\" target=\"#b10\">Conneau et al. (2020)</ref> present XLM-R to study the effects of tra =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">DATA AND MODEL</head><p>During pre-training, we follow <ref type=\"bibr\" target=\"#b10\">Conneau et al. (2020)</ref> to build a Common-Crawl Corpus using the  e=\"bibr\" target=\"#b32\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Lan et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Conneau et al., 2020)</ref>. For the latter, <ref type=\"bibr\" target= antic discrepancy among cross-lingual sentences.</p><p>The HICTL is conducted on the basis of XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref> and experiments are performed on several ned a transformer-based model on multilingual Wikipedia which covers various languages, while XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref> studied the effects of training unsuperv e , and 24 layers and 1024 hidden units for HICTL. We initialize the parameters of HICTL with XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref>. Hyperparameters for pre-training and fi s for pre-training HICTL. We use the same vocabulary as well as the sentence-piece model with XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref>. During finetuning on XTREME, we search  n et al., 2019)</ref>, XLM<ref type=\"bibr\" target=\"#b8\">(Conneau &amp; Lample, 2019)</ref> and XLM-R<ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref> are from XTREME<ref type=\"bibr\" target=\" ; Lample, 2019)</ref>, Unicoder <ref type=\"bibr\" target=\"#b22\">(Huang et al., 2019)</ref> and XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref>. Results of \u2021 are from our in-house repl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to-text problem and pre-trained a denoising sequence-to-sequence model at scale. Concurrently, BART <ref type=\"bibr\" target=\"#b29\">(Lewis et al., 2020)</ref> pre-trained a denoising sequence-to-sequen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: titions. For Indic languages, we fine-tune on Hi\u2192En translation (1.56M sentence pairs are from IITB <ref type=\"bibr\" target=\"#b27\">(Kunchukuttan et al., 2018b</ref>)), and test on {Ro, It, Cs, Nl}\u2192En . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b16\">Feng et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Chi et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b25\">Kong et al. (2020)</ref> improved language representation learning by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b52\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b55\">Ye et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">He et al., 2019; y contains the hard negative samples which are beneficial for learning high-quality representations <ref type=\"bibr\" target=\"#b55\">(Ye et al., 2019)</ref>. Specifically, the word-level contrastive los. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: titions. For Indic languages, we fine-tune on Hi\u2192En translation (1.56M sentence pairs are from IITB <ref type=\"bibr\" target=\"#b27\">(Kunchukuttan et al., 2018b</ref>)), and test on {Ro, It, Cs, Nl}\u2192En . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: strated in Eq. (1), contrastive losses can also be based on other forms, such as margin-based loses <ref type=\"bibr\" target=\"#b18\">(Hadsell et al., 2006)</ref> and variants of NCE losses <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediction includes POS tagging and NER. We use POS tagging data from the Universal Dependencies v2.5 <ref type=\"bibr\" target=\"#b37\">(Nivre et al., 2018)</ref> treebanks. Each word is assigned one of 17. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to-text problem and pre-trained a denoising sequence-to-sequence model at scale. Concurrently, BART <ref type=\"bibr\" target=\"#b29\">(Lewis et al., 2020)</ref> pre-trained a denoising sequence-to-sequen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b14\">(Eisele &amp; Yu, 2010)</ref> for French, Spanish, Arabic and Chinese, the IIT Bombay corpus <ref type=\"bibr\" target=\"#b26\">(Kunchukuttan et al., 2018a)</ref> for Hindi, the OpenSubtitles 2018  \"#b14\">(Eisele &amp; Yu, 2010)</ref> for French, Spanish, Arabic and Chinese, the IIT Bombay corpus <ref type=\"bibr\" target=\"#b26\">(Kunchukuttan et al., 2018a)</ref> for Hindi, the OpenSubtitles 2018 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b17\">Guo et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b61\">Zhu et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b49\">Weng et al. (2020)</ref> proposed a BERTfused NMT model, in which the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b13\">(Edunov et al., 2019;</ref><ref type=\"bibr\" target=\"#b56\">Zhang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b17\">Guo et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b61\">Zhu et al. (2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b20\">He et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Tian et al., 2020)</ref> to natural language processing <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediction includes POS tagging and NER. We use POS tagging data from the Universal Dependencies v2.5 <ref type=\"bibr\" target=\"#b37\">(Nivre et al., 2018)</ref> treebanks. Each word is assigned one of 17. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , 2020;</ref><ref type=\"bibr\" target=\"#b46\">Tian et al., 2020)</ref> to natural language processing <ref type=\"bibr\" target=\"#b35\">(Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b36\">Mnih &amp;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  treebanks. Each word is assigned one of 17 universal POS tags. For NER, we use the Wikiann dataset <ref type=\"bibr\" target=\"#b39\">(Pan et al., 2017)</ref>. (iii) Question answering includes three tas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b16\">Feng et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Chi et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b25\">Kong et al. (2020)</ref> improved language representation learning by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediction includes POS tagging and NER. We use POS tagging data from the Universal Dependencies v2.5 <ref type=\"bibr\" target=\"#b37\">(Nivre et al., 2018)</ref> treebanks. Each word is assigned one of 17. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to-text problem and pre-trained a denoising sequence-to-sequence model at scale. Concurrently, BART <ref type=\"bibr\" target=\"#b29\">(Lewis et al., 2020)</ref> pre-trained a denoising sequence-to-sequen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (see appendix A) shows the statistics of the parallel data.</p><p>We adopt the Transformer-Encoder <ref type=\"bibr\" target=\"#b47\">(Vaswani et al., 2017)</ref> as the backbone with 12 layers and 768 h anguage corpus at the second step. The initial learning rate is 2e-5 and inverse sqrt learning rate <ref type=\"bibr\" target=\"#b47\">(Vaswani et al., 2017)</ref> scheduler is also adopted. For WMT'14 En. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b13\">(Edunov et al., 2019;</ref><ref type=\"bibr\" target=\"#b56\">Zhang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b17\">Guo et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b61\">Zhu et al. (2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .45, 2.8, 6.37, 4.4, and 3.4</ref>. In addition, our approach also outperforms the BERT-fused model <ref type=\"bibr\" target=\"#b53\">(Yang et al., 2020)</ref>, a method treats BERT as an extra context W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b52\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b55\">Ye et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">He et al., 2019; y contains the hard negative samples which are beneficial for learning high-quality representations <ref type=\"bibr\" target=\"#b55\">(Ye et al., 2019)</ref>. Specifically, the word-level contrastive los. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediction includes POS tagging and NER. We use POS tagging data from the Universal Dependencies v2.5 <ref type=\"bibr\" target=\"#b37\">(Nivre et al., 2018)</ref> treebanks. Each word is assigned one of 17. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gh to model sentence-level representations <ref type=\"bibr\" target=\"#b24\">(Joshi et al., 2020;</ref><ref type=\"bibr\" target=\"#b54\">Yang et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Liu et al., 20 l. (2019)</ref> extended BERT-style models by jointly training the encoder-decoder framework. XLNet <ref type=\"bibr\" target=\"#b54\">(Yang et al., 2019)</ref> trained by predicting masked tokens auto-re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b14\">(Eisele &amp; Yu, 2010)</ref> for French, Spanish, Arabic and Chinese, the IIT Bombay corpus <ref type=\"bibr\" target=\"#b26\">(Kunchukuttan et al., 2018a)</ref> for Hindi, the OpenSubtitles 2018  \"#b14\">(Eisele &amp; Yu, 2010)</ref> for French, Spanish, Arabic and Chinese, the IIT Bombay corpus <ref type=\"bibr\" target=\"#b26\">(Kunchukuttan et al., 2018a)</ref> for Hindi, the OpenSubtitles 2018 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .45, 2.8, 6.37, 4.4, and 3.4</ref>. In addition, our approach also outperforms the BERT-fused model <ref type=\"bibr\" target=\"#b53\">(Yang et al., 2020)</ref>, a method treats BERT as an extra context W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wledge learned from large-scale unlabeled data to downstream NLP tasks, such as text classification <ref type=\"bibr\" target=\"#b44\">(Socher et al., 2013)</ref> and natural language inference <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to-text problem and pre-trained a denoising sequence-to-sequence model at scale. Concurrently, BART <ref type=\"bibr\" target=\"#b29\">(Lewis et al., 2020)</ref> pre-trained a denoising sequence-to-sequen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , 2020;</ref><ref type=\"bibr\" target=\"#b46\">Tian et al., 2020)</ref> to natural language processing <ref type=\"bibr\" target=\"#b35\">(Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b36\">Mnih &amp;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b55\">Ye et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">He et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Tian et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b52\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b55\">Ye et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">He et al., 2019; y contains the hard negative samples which are beneficial for learning high-quality representations <ref type=\"bibr\" target=\"#b55\">(Ye et al., 2019)</ref>. Specifically, the word-level contrastive los. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s-lingual representations on a very large scale.</p><p>For sequence-to-sequence pre-training, UniLM <ref type=\"bibr\" target=\"#b12\">(Dong et al., 2019)</ref> fine-tuned BERT with an ensemble of masks, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Lan et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Conneau et al., 2020)</ref>. For the latter, <ref type=\"bibr\" target=\"#b22\">(Huang et al., 2019)</ref> defined the cross-lingual paraphrase class et al., 2019)</ref>, XLM <ref type=\"bibr\" target=\"#b8\">(Conneau &amp; Lample, 2019)</ref>, Unicoder <ref type=\"bibr\" target=\"#b22\">(Huang et al., 2019)</ref> and XLM-R <ref type=\"bibr\" target=\"#b10\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Lan et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Conneau et al., 2020)</ref>. For the latter, <ref type=\"bibr\" target=\"#b22\">(Huang et al., 2019)</ref> defined the cross-lingual paraphrase class et al., 2019)</ref>, XLM <ref type=\"bibr\" target=\"#b8\">(Conneau &amp; Lample, 2019)</ref>, Unicoder <ref type=\"bibr\" target=\"#b22\">(Huang et al., 2019)</ref> and XLM-R <ref type=\"bibr\" target=\"#b10\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b42\">Raffel et al., 2019)</ref> or using pre-trained models to initialize encoders <ref type=\"bibr\" target=\"#b13\">(Edunov et al., 2019;</ref><ref type=\"bibr\" target=\"#b56\">Zhang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b56\">Zhang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b17\">Guo et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b61\">Zhu et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b49\">Weng et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  R X X (\u03c4 ) can be calculated by Fast Fourier Transforms (FFT) based on the Wiener-Khinchin theorem <ref type=\"bibr\" target=\"#b41\">[37]</ref>:</p><formula xml:id=\"formula_8\">S X X (f ) = F (X t ) F * . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ssing of historical series before predicting future series <ref type=\"bibr\" target=\"#b19\">[15,</ref><ref type=\"bibr\" target=\"#b6\">2]</ref>, such as Prophet <ref type=\"bibr\" target=\"#b37\">[33]</ref> wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b32\">28,</ref><ref type=\"bibr\" target=\"#b27\">23,</ref><ref type=\"bibr\" target=\"#b33\">29,</ref><ref type=\"bibr\" target=\"#b23\">19,</ref><ref type=\"bibr\" target=\"#b39\">35]</ref> have achieved great es autoregressive methods and RNNs to model the probabilistic distribution of future series. LSTNet <ref type=\"bibr\" target=\"#b23\">[19]</ref> introduces convolutional neural networks (CNNs) with recur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h of predicted time series. Recent deep forecasting models <ref type=\"bibr\" target=\"#b45\">[41,</ref><ref type=\"bibr\" target=\"#b21\">17,</ref><ref type=\"bibr\" target=\"#b24\">20,</ref><ref type=\"bibr\" tar ence length. Previous Transformer-based forecasting models <ref type=\"bibr\" target=\"#b45\">[41,</ref><ref type=\"bibr\" target=\"#b21\">17,</ref><ref type=\"bibr\" target=\"#b24\">20]</ref> mainly focus on imp ng the exponentially increasing intervals, which reduces the complexity to O(L(log L) 2 ). Reformer <ref type=\"bibr\" target=\"#b21\">[17]</ref> presents the local-sensitive hashing (LSH) attention and r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b11\">7]</ref>, audio processing <ref type=\"bibr\" target=\"#b18\">[14]</ref> and even computer vision <ref type=\"bibr\" target=\"#b16\">[12,</ref><ref type=\"bibr\" target=\"#b25\">21]</ref>. However, applying. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tention-based RNNs <ref type=\"bibr\" target=\"#b43\">[39,</ref><ref type=\"bibr\" target=\"#b34\">30,</ref><ref type=\"bibr\" target=\"#b35\">31]</ref> introduce the temporal attention to explore the long-range . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es <ref type=\"bibr\" target=\"#b19\">[15,</ref><ref type=\"bibr\" target=\"#b6\">2]</ref>, such as Prophet <ref type=\"bibr\" target=\"#b37\">[33]</ref> with trend-seasonality decomposition and N-BEATS <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  characterizing itself by the large length of predicted time series. Recent deep forecasting models <ref type=\"bibr\" target=\"#b45\">[41,</ref><ref type=\"bibr\" target=\"#b21\">17,</ref><ref type=\"bibr\" ta cause of the quadratic complexity of sequence length. Previous Transformer-based forecasting models <ref type=\"bibr\" target=\"#b45\">[41,</ref><ref type=\"bibr\" target=\"#b21\">17,</ref><ref type=\"bibr\" ta ents the local-sensitive hashing (LSH) attention and reduces the complexity to O(L log L). Informer <ref type=\"bibr\" target=\"#b45\">[41]</ref> extends Transformer with KL-divergence based ProbSparse at , weather and disease.</p><p>Datasets Here is a description of the six experiment datasets: (1) ETT <ref type=\"bibr\" target=\"#b45\">[41]</ref> dataset contains the data collected from electricity trans ng-term future, such as Transformer <ref type=\"bibr\" target=\"#b39\">[35]</ref> predict-720, Informer <ref type=\"bibr\" target=\"#b45\">[41]</ref> predict-336.</p><p>Auto-Correlation vs. self-attention fam ation between scattered points. Though some selfattentions <ref type=\"bibr\" target=\"#b24\">[20,</ref><ref type=\"bibr\" target=\"#b45\">41]</ref> consider the local information, they only utilize this to h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tage in modeling long-term dependencies for sequential data, which enables more powerful big models <ref type=\"bibr\" target=\"#b11\">[7,</ref><ref type=\"bibr\" target=\"#b15\">11]</ref>.</p><p>However, the er in sequential data, such as natural language processing <ref type=\"bibr\" target=\"#b15\">[11,</ref><ref type=\"bibr\" target=\"#b11\">7]</ref>, audio processing <ref type=\"bibr\" target=\"#b18\">[14]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: position, which is a standard method in time series analysis <ref type=\"bibr\" target=\"#b5\">[1,</ref><ref type=\"bibr\" target=\"#b31\">27]</ref>. It can be used to process the complex time series and extr rd method in time series analysis, time series decomposition <ref type=\"bibr\" target=\"#b5\">[1,</ref><ref type=\"bibr\" target=\"#b31\">27]</ref> deconstructs a time series into several components, each re -term forecasting context, we take the idea of decomposition <ref type=\"bibr\" target=\"#b5\">[1,</ref><ref type=\"bibr\" target=\"#b31\">27]</ref>, which can separate the series into trend-cyclical and seas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nge, inspired by the previous graph sparsification studies <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we propose a relation-based personalized PageRank (PPR) to  by previous studies <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, personalized PageRank can be utilized to preserve more eff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n the graph. To address the second challenge, inspired by the previous graph sparsification studies <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we propose a rela t important edges in the graph. Inspired by previous studies <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, personalized PageR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: C. We compute the correlation between graphs using a series of commonly used graph property metrics <ref type=\"bibr\" target=\"#b23\">[24]</ref> listed in Table <ref type=\"table\">6</ref> in Appendix C, s head>C STRUCTURAL PROPERTY OF GRAPHS</head><p>We extract eight commonly used graph property metrics <ref type=\"bibr\" target=\"#b23\">[24]</ref> as shown in Table <ref type=\"table\">6</ref>, each of which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e reliance on task-specific labeled data, inspired by pre-training techniques from computer version <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and natural language  chniques in natural language processing <ref type=\"bibr\" target=\"#b5\">[6]</ref> and computer vision <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, recent studies <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndle the heterogeneous objects and interactions, various complex semantic patterns (e.g., meta-path <ref type=\"bibr\" target=\"#b26\">[27]</ref> and meta-graph <ref type=\"bibr\" target=\"#b6\">[7]</ref>) ha g. In contrast, as a unique high-order structure that defines a heterogeneous graph, network schema <ref type=\"bibr\" target=\"#b26\">[27]</ref> is convenient to sample its instances, and schema instance th and metagraph relies on domain knowledge. However, for a heterogeneous graph, its network schema <ref type=\"bibr\" target=\"#b26\">[27]</ref> is a unique defining structure that captures both high-ord utilize high-order semantic patterns such as meta-structures <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> and motifs. However, there are three major weakness of thes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ffline sparsification to retain the most important edges in the graph. Inspired by previous studies <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ on matrix multiplication, which is infeasible for a large-scale graph. Inspired by previous studies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we obtain the personalized PageRank for a relation \ud835\udc45 using   allows us to simply truncate small values of \u03a0 \ud835\udc45 and recover sparsity. Similar to previous studies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we use the top-k entries with the highest mass per column a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1\">INTRODUCTION</head><p>In recent years, as an emerging tool for learning on graph-structured data <ref type=\"bibr\" target=\"#b32\">[33]</ref>, graph neural networks (GNNs) learn powerful graph represe networks have received significant research interests due to the prevalence of graph-structure data <ref type=\"bibr\" target=\"#b32\">[33]</ref>. They utilize neural networks to learn node representation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e reliance on task-specific labeled data, inspired by pre-training techniques from computer version <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and natural language  chniques in natural language processing <ref type=\"bibr\" target=\"#b5\">[6]</ref> and computer vision <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, recent studies <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, recent studies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar raining strategies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndle the heterogeneous objects and interactions, various complex semantic patterns (e.g., meta-path <ref type=\"bibr\" target=\"#b26\">[27]</ref> and meta-graph <ref type=\"bibr\" target=\"#b6\">[7]</ref>) ha g. In contrast, as a unique high-order structure that defines a heterogeneous graph, network schema <ref type=\"bibr\" target=\"#b26\">[27]</ref> is convenient to sample its instances, and schema instance th and metagraph relies on domain knowledge. However, for a heterogeneous graph, its network schema <ref type=\"bibr\" target=\"#b26\">[27]</ref> is a unique defining structure that captures both high-ord utilize high-order semantic patterns such as meta-structures <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> and motifs. However, there are three major weakness of thes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> and sparsification <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Sampling of neighboring nodes is often conducted online dur e graph, online sampling still incurs a significant overhead in computing the distribution of nodes <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Thus, it is less desirable than sparsification, which does   graphs, online sampling still incurs a significant overhead in computing the distribution of nodes <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Thus, we resort to offline sparsification to retain the mos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ffline sparsification to retain the most important edges in the graph. Inspired by previous studies <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ on matrix multiplication, which is infeasible for a large-scale graph. Inspired by previous studies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we obtain the personalized PageRank for a relation \ud835\udc45 using   allows us to simply truncate small values of \u03a0 \ud835\udc45 and recover sparsity. Similar to previous studies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we use the top-k entries with the highest mass per column a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ethod PT-HGNN and other baselines. We implement the base model with PyTorch Geometric (PyG) package <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We set the hidden dimension as 400, the number of heads as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tly on a large-scale heterogeneous graph. For the first challenge, inspired by contrastive learning <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we design a contrastive pre-training strategy to model th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: transferable knowledge from the unlabeled graph structures <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. In particular, the raph during pre-training? Existing pre-training strategies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar ef><ref type=\"bibr\" target=\"#b9\">10]</ref>, recent studies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar knowledge. In contrast to previous pre-training strategies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar e node-and graph-level self-supervision on multiple graphs for pre-training the GNNs, while GPT-GNN <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduces a selfsupervised attributed graph generation ta entations to construct positive/negative samples for conducting contrastive learning, while GPT-GNN <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduces a self-supervised attributed graph generation t ode dropping and subgraphs are employed as the pre-training strategies in the experiment. \u2022 GPT-GNN <ref type=\"bibr\" target=\"#b11\">[12]</ref> is generative pre-training model for GNNs, which reconstru -training procedure. For fair comparison, the above parameters follow the setting of previous study <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For the implementation of our pre-training framework PT-H .</p><p>In the sampling stage, we follow the settings of the HGSampling sampler in previous studies <ref type=\"bibr\" target=\"#b11\">[12]</ref> for fair comparison. The parameter settings of the baselin per-Venue, and Author Name Disambiguation (Author ND) as three downstream tasks used in prior works <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. The model perform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e pairwise influence between all pairs of nodes in relation \ud835\udc45, which typically are highly localized <ref type=\"bibr\" target=\"#b22\">[23]</ref>. Spatial localization allows us to simply truncate small v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trated to benefit a wide variety of graph mining tasks from node classification and link prediction <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> to graph generation  o ensure scalability to large graphs, existing work often considers two lines of approach: sampling <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> and sparsification < ype=\"bibr\" target=\"#b4\">5]</ref> and message passing architectures to aggregate neighbors' features <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Besides, some studi in method trains a GNN model for the downstream tasks on the fine-tuning graph directly. \u2022 EdgePred <ref type=\"bibr\" target=\"#b8\">[9]</ref> predicts whether there exists a link between two nodes, whic ef> for fair comparison. The parameter settings of the baseline models are as follows: (1) EdgePred <ref type=\"bibr\" target=\"#b8\">[9]</ref> is simply used to predict whether there exists a link betwee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: transferable knowledge from the unlabeled graph structures <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. In particular, the raph during pre-training? Existing pre-training strategies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar ef><ref type=\"bibr\" target=\"#b9\">10]</ref>, recent studies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar knowledge. In contrast to previous pre-training strategies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar e node-and graph-level self-supervision on multiple graphs for pre-training the GNNs, while GPT-GNN <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduces a selfsupervised attributed graph generation ta entations to construct positive/negative samples for conducting contrastive learning, while GPT-GNN <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduces a self-supervised attributed graph generation t ode dropping and subgraphs are employed as the pre-training strategies in the experiment. \u2022 GPT-GNN <ref type=\"bibr\" target=\"#b11\">[12]</ref> is generative pre-training model for GNNs, which reconstru -training procedure. For fair comparison, the above parameters follow the setting of previous study <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For the implementation of our pre-training framework PT-H .</p><p>In the sampling stage, we follow the settings of the HGSampling sampler in previous studies <ref type=\"bibr\" target=\"#b11\">[12]</ref> for fair comparison. The parameter settings of the baselin per-Venue, and Author Name Disambiguation (Author ND) as three downstream tasks used in prior works <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. The model perform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: complex semantic patterns (e.g., meta-path <ref type=\"bibr\" target=\"#b26\">[27]</ref> and meta-graph <ref type=\"bibr\" target=\"#b6\">[7]</ref>) have been proposed to capture the heterogeneous semantic an erogeneous graph, a natural idea is to utilize high-order semantic patterns such as meta-structures <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> and motifs. However,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> and sparsification <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Sampling of neighboring nodes is often conducted online dur e graph, online sampling still incurs a significant overhead in computing the distribution of nodes <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Thus, it is less desirable than sparsification, which does   graphs, online sampling still incurs a significant overhead in computing the distribution of nodes <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Thus, we resort to offline sparsification to retain the mos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and natural language processing <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, recent works propos led data for pre-training a GNN. Inspired by pre-training techniques in natural language processing <ref type=\"bibr\" target=\"#b5\">[6]</ref> and computer vision <ref type=\"bibr\" target=\"#b3\">[4,</ref><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1\">INTRODUCTION</head><p>In recent years, as an emerging tool for learning on graph-structured data <ref type=\"bibr\" target=\"#b32\">[33]</ref>, graph neural networks (GNNs) learn powerful graph represe networks have received significant research interests due to the prevalence of graph-structure data <ref type=\"bibr\" target=\"#b32\">[33]</ref>. They utilize neural networks to learn node representation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct a synonym prediction sub-task to fine-tune a pre-trained language model which is the ERNIE model <ref type=\"bibr\" target=\"#b23\">[23]</ref> in this paper. As ERNIE is a continual pre-training framew ation discovery rather than relation automatic verification. Pre-trained language models like ERNIE <ref type=\"bibr\" target=\"#b23\">[23]</ref>, BERT <ref type=\"bibr\" target=\"#b7\">[7]</ref>, XLNet <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: stion and the corresponding evidence document <ref type=\"bibr\" target=\"#b10\">[10]</ref>. Fei et al. <ref type=\"bibr\" target=\"#b8\">[8]</ref> propose a hierarchical multi-task word embedding model to le get=\"#b30\">30]</ref> are mainly focus on the medical multiple choices questionanswering, Fei et al. <ref type=\"bibr\" target=\"#b8\">[8]</ref> mainly focus on synonym prediction, which is different from   synonym pairs, we construct additional medical synonym pairs from the existing corpus. Inspired by <ref type=\"bibr\" target=\"#b8\">[8]</ref>, we use two methods for the synonym pairs construction: a ru. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: RNIE <ref type=\"bibr\" target=\"#b23\">[23]</ref>, BERT <ref type=\"bibr\" target=\"#b7\">[7]</ref>, XLNet <ref type=\"bibr\" target=\"#b28\">[28]</ref> and OpenAI GPT <ref type=\"bibr\" target=\"#b19\">[19]</ref> c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b11\">11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" tar V as a natural language inference (NLI) <ref type=\"bibr\" target=\"#b0\">[1]</ref> task. Thorne et al. <ref type=\"bibr\" target=\"#b24\">[24]</ref> mainly feed the concatenated evidence and the given claim . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, which aims to verify given claims with the evidence retrie ions of the claim, the evidence, and the evidence metadata. Then, different from previous work like <ref type=\"bibr\" target=\"#b32\">[32]</ref>, which computes the attention by the claim and the evidenc  infer the relationship between evidence and claims, which achieves better performance. Zhou et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref> propose the graph-based evidence aggregating and reasoning for 2 iterations. We train our system with the paddlepaddle 3 deep learning framework. For the GEAR <ref type=\"bibr\" target=\"#b32\">[32]</ref> and KGAT <ref type=\"bibr\" target=\"#b14\">[14]</ref>, we use  sentences are retrieved, the system will predict the claim being true. \u2022 GEAR. We utilize the GEAR <ref type=\"bibr\" target=\"#b32\">[32]</ref> model as one of the competitive baseline models, which ach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erification. Pre-trained language models like ERNIE <ref type=\"bibr\" target=\"#b23\">[23]</ref>, BERT <ref type=\"bibr\" target=\"#b7\">[7]</ref>, XLNet <ref type=\"bibr\" target=\"#b28\">[28]</ref> and OpenAI   information. The relation-aware attention coefficient \ud835\udf36 is learned by the evidence detector in Eq. <ref type=\"bibr\" target=\"#b7\">(7)</ref>.</p><formula xml:id=\"formula_8\">\ud835\udc7b \ud835\udc90 = \ud835\udc41 \ud835\udc58=1 \ud835\udefc \ud835\udc58 \ud835\udc7b (\ud835\udc58) \ud835\udc63 (10). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eworks for fact verification, which are less effective in the medical domain verification. MedTruth <ref type=\"bibr\" target=\"#b6\">[6]</ref> proposes a truth discovery method for medical knowledge cond. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b15\">[15]</ref>. There are also a few of studies <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b29\">29]</ref> that adopt the enha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on synonym prediction, which is different from the medical entity relation verification. Niu et al. <ref type=\"bibr\" target=\"#b17\">[17]</ref> present an iterative method to generate soft evidence labe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, which aims to verify given claims with the evidence retrie ions of the claim, the evidence, and the evidence metadata. Then, different from previous work like <ref type=\"bibr\" target=\"#b32\">[32]</ref>, which computes the attention by the claim and the evidenc  infer the relationship between evidence and claims, which achieves better performance. Zhou et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref> propose the graph-based evidence aggregating and reasoning for 2 iterations. We train our system with the paddlepaddle 3 deep learning framework. For the GEAR <ref type=\"bibr\" target=\"#b32\">[32]</ref> and KGAT <ref type=\"bibr\" target=\"#b14\">[14]</ref>, we use  sentences are retrieved, the system will predict the claim being true. \u2022 GEAR. We utilize the GEAR <ref type=\"bibr\" target=\"#b32\">[32]</ref> model as one of the competitive baseline models, which ach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b29\">29]</ref> that adopt the enhanced sequential inference model (ESIM) <ref type=\"bibr\" target=\"#b4\">[5]</ref> to infer the relationship between evidence and claims, which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b15\">[15]</ref>. There are also a few of studies <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b29\">29]</ref> that adopt the enha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b11\">11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" tar V as a natural language inference (NLI) <ref type=\"bibr\" target=\"#b0\">[1]</ref> task. Thorne et al. <ref type=\"bibr\" target=\"#b24\">[24]</ref> mainly feed the concatenated evidence and the given claim . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  reading comprehension model SeaReader for question-answering task on clinical medicine is proposed <ref type=\"bibr\" target=\"#b30\">[30]</ref>. Chen et al. <ref type=\"bibr\">[4]</ref> devise a knowledge em to medical synonym prediction. Moreover, the authors of <ref type=\"bibr\" target=\"#b10\">[10,</ref><ref type=\"bibr\" target=\"#b30\">30]</ref> are mainly focus on the medical multiple choices questionan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b11\">11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" tar V as a natural language inference (NLI) <ref type=\"bibr\" target=\"#b0\">[1]</ref> task. Thorne et al. <ref type=\"bibr\" target=\"#b24\">[24]</ref> mainly feed the concatenated evidence and the given claim . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is built upon ElasticSearch, which is a search engine based on the Lucence library followed by BM25 <ref type=\"bibr\" target=\"#b21\">[21]</ref>. We parse dozens of clinical textbooks and medical encyclo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eworks for fact verification, which are less effective in the medical domain verification. MedTruth <ref type=\"bibr\" target=\"#b6\">[6]</ref> proposes a truth discovery method for medical knowledge cond. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ering Dataset (SQuAD) consisting of over 10k questions posed by crowd workers on Wikipedia articles <ref type=\"bibr\" target=\"#b20\">[20]</ref>. Chen et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> propo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b15\">[15]</ref>. There are also a few of studies <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b29\">29]</ref> that adopt the enha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
