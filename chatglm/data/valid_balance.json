{"content": "The context is: acoustics and first-pass text hypotheses for second-pass decoding based on the deliberation network <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The deliberation model has been used in state-of-the-art  lation <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our deliberation model has a similar structure as <ref type=\"bibr\" target=\"#b15\">[16]</ref>: An RNN-T model generates the first-pass hypotheses, and d head><p>A deliberation model is typically trained from scratch by jointly optimizing all components <ref type=\"bibr\" target=\"#b15\">[16]</ref>. However, we find training a two-pass model from scratch t get=\"#b0\">[1]</ref>, and a deliberation decoder, similar to <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. The shared encoder takes log-mel filterbank energies, x = . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, and here we employ Simple Graph Convolution (SGC) <ref typ 5,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, and here we employ Simple Graph Convolution (SGC) <ref type=\"bibr\" target=\"#b38\">[39]</ref> in our implementation. Abnormality Valuator. Afterwards, t lgorithm. Implementation Details. Regarding the proposed GDN model, we use Simple Graph Convolution <ref type=\"bibr\" target=\"#b38\">[39]</ref> to build the network encoder with degree \ud835\udc3e = 2 (two layers. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b0\">[1]</ref>, we use the randomly initialized LeNet for all experiments. L-BFGS <ref type=\"bibr\" target=\"#b10\">[11]</ref> with learning rate 1 is used as the optimizer. For fast tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eration for paraphrasing <ref type=\"bibr\" target=\"#b15\">(Liu et al., 2020)</ref> and summa-rization <ref type=\"bibr\" target=\"#b30\">(Schumann et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to-text problem and pre-trained a denoising sequence-to-sequence model at scale. Concurrently, BART <ref type=\"bibr\" target=\"#b29\">(Lewis et al., 2020)</ref> pre-trained a denoising sequence-to-sequen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: //www.tei-c.org/ns/1.0\"><head>Introduction</head><p>Large-scale knowledge graphs (KGs) such as YAGO <ref type=\"bibr\" target=\"#b10\">(Suchanek, Kasneci, and Weikum 2007)</ref>, NELL <ref type=\"bibr\" tar e=\"bibr\" target=\"#b0\">(Carlson et al. 2010), and</ref><ref type=\"bibr\">Wikidata (Vrande\u010di\u0107 and</ref><ref type=\"bibr\" target=\"#b10\">Kr\u00f6tzsch 2014)</ref> usually represent facts in the form of relations =\"bibr\" target=\"#b9\">Socher et al. 2013;</ref><ref type=\"bibr\" target=\"#b11\">Yang et al. 2015;</ref><ref type=\"bibr\" target=\"#b10\">Trouillon et al. 2016;</ref><ref type=\"bibr\" target=\"#b8\">Schlichtkru sume available, sufficient training instances for all relations.</p><p>In light of the above issue, <ref type=\"bibr\" target=\"#b10\">Xiong et al. (2018)</ref> proposed GMatching which introduces a local )</ref> have been proposed to learn entity embeddings by using relational information, Xiong et al. <ref type=\"bibr\" target=\"#b10\">(Xiong et al. 2018</ref>) demonstrated that explicitly encoding graph spectively. In order to measure the similarity between two vectors, we employ a recurrent processor <ref type=\"bibr\" target=\"#b10\">(Vinyals et al. 2016</ref>) f \u00b5 to perform multiple steps matching. T. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: raining dataset is small, or because the quality of the dataset should be improved?</p><p>Recently, <ref type=\"bibr\" target=\"#b31\">Ning et al. (2018c)</ref> introduced a new dataset called Multi-Axis  lity or neural methods inherently do not work well for this task.</p><p>A recent annotation scheme, <ref type=\"bibr\" target=\"#b31\">Ning et al. (2018c)</ref>, introduced the notion of multi-axis to rep raction prop-3 Between experts: Kohen's \uf8ff \u21e1 0.84. Among crowdsourcers: accuracy 88%. More details in<ref type=\"bibr\" target=\"#b31\">Ning et al. (2018c)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fective, but combined with an LSTM brought marginal improvement and greater interpretability, while <ref type=\"bibr\" target=\"#b8\">[9]</ref> did not find any notable improvement using the Transformer i e Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Though self-attention has provided various benefits such as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b11\">[12]</ref> have revolutionized image synthesis, with recent style-based generative models <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta hat different StyleGAN layers are responsible for different levels of detail in the generated image <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Consequently, it is common to split the layers into three manipulations in Figure <ref type=\"figure\">7</ref> are performed using StyleGAN2 pretrained on FFHQ <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The inputs are real images, embedded in W+ space using th s, demonstrated on portraits of celebrities. Edits are performed using StyleGAN2 pretrained on FFHQ <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The inputs are real images, embedded in W+ space using th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b11\">[12]</ref>, which also proposed a combinatorial algorithm to compute embeddings. Ganea et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> and Gulcehre et al. <ref type=\"bibr\" target=\"#b20\">[21]</r  taking its outputs. An alternative to prevent such collapse would be to introduce bias terms as in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Importantly, when applying the non-linearity directly on   )y 1 + 2 x, y + x 2 y 2<label>(8)</label></formula><p>Similar to the Euclidean case, and following <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we use x = x 0 . On the Poincar\u00e9 ball, we employ pointwis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b15\">Gao et al., 2018;</ref><ref type=\"bibr\">2021)</ref>. Currently, the message passing scheme <ref type=\"bibr\" target=\"#b19\">(Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Sanchez-Gon .1\">MESSAGE PASSING SCHEME</head><p>Currently, the class of message passing neural networks (MPNNs) <ref type=\"bibr\" target=\"#b19\">(Gilmer et al., 2017)</ref> are one of the most widely used architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  predict execution time of programs by using a set of hand-crafted features of high level programs. <ref type=\"bibr\" target=\"#b9\">Dubach et al. (2007)</ref> uses neural networks with hand-crafted feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l Captions, a dataset of 3.3 million text-image pairs that was developed as an extension to MS-COCO <ref type=\"bibr\" target=\"#b21\">(Lin et al., 2014)</ref>.</p><p>To scale up to 12-billion parameters,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Bayesian active learning algorithm for deep learning in image data is proposed based on the idea in <ref type=\"bibr\" target=\"#b41\">[42]</ref>.</p><p>Most works that apply active learning to recommende  basic equation of Variational Inference <ref type=\"bibr\" target=\"#b51\">[52]</ref>.</p><p>Following <ref type=\"bibr\" target=\"#b41\">[42]</ref>, we use the distribution of the network parameter with dro nsidered as the smoothed version of hinge loss.</p><p>As for the second term in (2), it's proved in <ref type=\"bibr\" target=\"#b41\">[42]</ref> that it can be approximated by L2 regularization term</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b41\">[42]</ref>, we use the distribution of the network parameter with dropout <ref type=\"bibr\" target=\"#b52\">[53]</ref> as q(\u03c9). Consider a neural network with only one layer, wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: NNs). However, recent literature has successfully used the idea of defining a sequence over a graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bib  'structured' graph, <ref type=\"bibr\">Mao et al.</ref> showed that graph models can outperform RNNs <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Motivated by these recent successes and in the pursuit of ref>, object detection <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and video representation learning <ref type=\"bibr\" target=\"#b10\">[11]</ref>. GCNs have been used to address skeleton-based action reco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: most downstream tasks. Additionally, while BERT and ALBERT are trained on Wikipedia and BooksCorpus <ref type=\"bibr\" target=\"#b39\">(Zhu et al., 2015)</ref>, RoBERTa is also trained on OpenWebText <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: this weakness. The recent works in SSL are diverse but those that are based on consistency training <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\" target=\"#b32\">Rasmus et a Sajjadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b6\">Clark et al., 2018)</ref> or hidden states <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016)</  works in the consistency training family mostly differ in how the noise is defined: Pseudoensemble <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> to address the lack of highquality, large- eters et al., 2018)</ref>, GPT <ref type=\"bibr\" target=\"#b25\">(Radford et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, unsupervised pretraining of language mode s=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Methods</head><p>Background The BERT model architecture <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> is based on a multilayer bidirectional Tra ead n=\"3.3\">Pretrained BERT Variants</head><p>BERT-Base We use the pretrained weights for BERT-Base <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> released with the original BERT code. <ref 9\">(Howard and Ruder, 2018)</ref> which is equivalent to the linear warmup followed by linear decay <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>. For each dataset and BERT variant, we pic h using AllenNLP <ref type=\"bibr\" target=\"#b8\">(Gardner et al., 2017)</ref>.</p><p>Casing We follow <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref> in using the cased models for NER and the u T</head><p>We mostly follow the same architecture, optimization, and hyperparameter choices used in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>. For text classification (i.e. CLS and REL). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: F = 8 features and K = 8 attention heads (total 64 features). All models are implemented in PyTorch <ref type=\"bibr\" target=\"#b39\">(Paszke et al., 2019)</ref> and PyTorch Geometric <ref type=\"bibr\" ta ><p>For GAT and SuperGAT, we use our implementation (including hyperparameter settings) in Py-Torch <ref type=\"bibr\" target=\"#b39\">(Paszke et al., 2019)</ref>. For GAM, we adopt the code in TensorFlow. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph-based semi-supervised learning, which can be used for graphs exhibiting homophily or heterophily <ref type=\"bibr\" target=\"#b18\">[19]</ref> and has fast linearized versions <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b87\">[88]</ref> have become the de facto model architecture for NLP tasks <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b69\">70,</ref><ref type=\"bibr\" ta > without supervised pre-training.</p><p>Our VATT results, along with others reported for NLP tasks <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, image recognition  m with large-scale, unlabeled visual data? To answer this question, we draw insights from NLP. BERT <ref type=\"bibr\" target=\"#b22\">[23]</ref> and GPT <ref type=\"bibr\" target=\"#b69\">[70,</ref><ref type target=\"#fig_0\">1</ref> illustrates the architecture. VATT borrows the exact architecture from BERT <ref type=\"bibr\" target=\"#b22\">[23]</ref> and ViT <ref type=\"bibr\" target=\"#b24\">[25]</ref> except t former Architecture</head><p>For simplicity, we adopt the most established Transformer architecture <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which has been widely used in NLP. Similar to ViT <ref ty get=\"#fig_0\">1</ref> middle panel) and refer the reader to <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for more details of the standard Transformer architecture. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ly effective in learning sentence embeddings, coupled with pre-trained language models such as BERT <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref> and RoBERTa <ref type=\"bibr\" target=\"#b29 /formula><p>In this work, we encode input sentences using a pre-trained language model such as BERT <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref> or RoBERTa <ref type=\"bibr\" target=\"#b29\" nally, we introduce one more optional variant which adds a masked language modeling (MLM) objective <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref> as an auxiliary loss to Eq. 1: +\u03bb\u2022 mlm (\u03bb ume that they take the same evaluation and just take BERT-whitening in experiments here. \u2022 For BERT <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref> and RoBERTa <ref type=\"bibr\" target=\"#b29. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ule</head><p>This module is a bi-direction recurrent neural network with self-attention as shown in <ref type=\"bibr\" target=\"#b10\">Lin et al. (2017)</ref>. Given an input text x = (w 1 , w 2 , ..., w . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b46\">36,</ref><ref type=\"bibr\" target=\"#b55\">[45]</ref><ref type=\"bibr\" target=\"#b56\">[46]</ref><ref type=\"bibr\" target=\"#b57\">[47]</ref><ref type=\"bibr\" t et=\"#b23\">[13,</ref><ref type=\"bibr\" target=\"#b26\">16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b56\">46]</ref>. However, explanations formulated in this fashion are often get=\"#b26\">16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b36\">26,</ref><ref type=\"bibr\" target=\"#b56\">46]</ref>. To the best of our knowledge, this is the first attempt in lain fact veracity <ref type=\"bibr\" target=\"#b26\">[16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b56\">46]</ref>. Extractive explainable fact checking approaches <ref type= get=\"#b26\">16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b36\">26,</ref><ref type=\"bibr\" target=\"#b56\">46]</ref> (e.g baseline FACE-KEG-linear enc. in Section 4). We find t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: propose an adaptive feature selection (AFS) module to deal with the shortage. Previously, Li et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposed a selective kernel (SK) unit by fusing multiple b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targe >19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> for GP. Very recently, the authors of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> find that the vertex-cut methods can achieve better performa rtex-cut has attracted more and more attention from DGC research community. For example, PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> adopts a random vertex-cut method and two greedy variants fo sly guarantee good workload balance. \u2022 DBH can be implemented as an execution engine for PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and hence all PowerGraph applications can be seamlessly sup hines. Hence, |A(v)| is the number of replicas of v among different machines. Similar to PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>, one of the replicas of a vertex is chosen as the master and  the \u03b1 is, the more skewed a graph will be. This power-law degree distribution makes GP challenging <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Although vertex-cut methods can achieve better performance  though vertex-cut methods can achieve better performance than edge-cut methods for power-law graphs <ref type=\"bibr\" target=\"#b5\">[6]</ref>, existing vertex-cut methods, such as random method in Power ysis for our DBH method. For comparison, the random vertex-cut method (called Random) of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> and the grid-based constrained solution (called Grid) of Gra ly to the p machines via a randomized hash function. The result can be directly got from PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Lemma 1. Assume that we have a sequence of n vertices {v i   theorem says that our DBH method has smaller expected replication factor than Random of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Next we turn to the analysis of the balance constrain \"5.2\">Baselines and Evaluation Metric</head><p>In our experiment, we adopt the Random of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> and the Grid of GraphBuilder [8]<ref type=\"foot\" target=\"#fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lty classes. Recently, self-supervised learning <ref type=\"bibr\">[Komodakis and Gidaris, 2018;</ref><ref type=\"bibr\" target=\"#b5\">Ji et al., 2019]</ref> holds great for improving representations when   discriminative ability of features that fails to effectively detect novelty samples. Besides, some <ref type=\"bibr\" target=\"#b5\">[Lim et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Sinha et al., 20 ved by our unsupervised method significantly lower than supervision methods except auxiliary method <ref type=\"bibr\" target=\"#b5\">[Liu et al., 2018]</ref>. This is probably due to the help of the addi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l's generalization <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for the unseen target domains in an episodic training parad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: similar from every other image in the dataset <ref type=\"bibr\" target=\"#b50\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020)</ref>. We propose to train feature representations  \" target=\"#b26\">Misra &amp; Maaten, 2020;</ref><ref type=\"bibr\" target=\"#b15\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020)</ref> which aims to learn representations by consid main. We use a state-of-the-art self-supervised loss function based on contrastive learning: SimCLR <ref type=\"bibr\" target=\"#b1\">(Chen et al., 2020)</ref>. The SimCLR loss encourages two augmentation seline, SimCLR that uses the novel domain unlabeled data D u to train a representation using SimCLR <ref type=\"bibr\" target=\"#b1\">(Chen et al., 2020)</ref>, and then uses the resulting representation . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  semantic traces and patterns derived from large, generalpurpose knowledge ontologies (e.g. DBPedia <ref type=\"bibr\" target=\"#b13\">[3]</ref>); (ii) formal logic rules; and (iii) attributed relations f help predict and justify the veracity of the facts under consideration. We use the DBPedia ontology <ref type=\"bibr\" target=\"#b13\">[3]</ref> (and its associated Wikipedia text corpus) as the knowledge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ections, background, and reflection images, respectively. Here, \u03b1 and \u03b2 are the mixing coefficients <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" targ  generation results and clearer separation results. Moreover, we introduce the gradient constraints <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> to make the model le ><head n=\"4.1.\">Framework of the Proposed Scheme</head><p>In contrast to the conventional pipelines <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" targ ird columns in Figure <ref type=\"figure\" target=\"#fig_3\">4</ref> 1 ) than previous linear functions <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ n, and the background edge map (E) concurrently. Instead of one-toone framework in previous methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, our separator learn . Recently, deep learning based reflection removal methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> with better generalization ability have been proposed to addr h previous methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, that heavily rely on the simplified model in Equation 1 and  learn the edge features of the reflections with the light field camera. The framework introduced in <ref type=\"bibr\" target=\"#b4\">[5]</ref> exploited the edge information when training the whole netwo and reflection, and three discriminator networks to produce the adversarial losses. Existing method <ref type=\"bibr\" target=\"#b4\">[5]</ref> can be treated as a special instance of our method when the   type=\"bibr\" target=\"#b28\">[29]</ref>, CycleGAN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The yellow boxes highlight some noticeable differences.</p> 5\">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type=\"bibr\" target=\"#b29\">[3 <ref type=\"bibr\" target=\"#b28\">[29]</ref>, CycleGAN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, WS16 <ref typ able\" xml:id=\"tab_3\"><head>Table 3 .</head><label>3</label><figDesc>Efficiency comparisons with FY17<ref type=\"bibr\" target=\"#b4\">[5]</ref>, Zhang18<ref type=\"bibr\" target=\"#b28\">[29]</ref> and Wan18<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mmender systems <ref type=\"bibr\" target=\"#b70\">(Ying et al., 2018)</ref>, hyperlinked Web documents <ref type=\"bibr\" target=\"#b26\">(Kleinberg, 1999)</ref>, knowledge graphs (KGs) <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: echniques to mitigate this issue include multi-task learning <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and pre-trained components <ref type=\"bibr\" target=\"#b10\">[1  both fully supervised data and also weakly supervised data, <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> use multi-task learning to train the ST model jointly with t g and multi-task learning as proposed in previous literature <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ation study (see Appendix A.3). During inference, a MIPS search is conducted with the ScaNN library <ref type=\"bibr\" target=\"#b11\">(Guo et al., 2020)</ref> to efficiently find the top K documents; we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  k with d \u2032 input channels and d \u2032\u2032 output channels, the total number of multiply add required is h <ref type=\"bibr\" target=\"#b0\">(1)</ref> this expression has an extra term, as indeed we have an extr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , the widespread use of such tools raises legitimate privacy concerns. For instance, Mislove et al. <ref type=\"bibr\" target=\"#b22\">[24]</ref> demonstrated how, by analysing Facebook's social network s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es a relational graph convolutional network to link prediction task and entity classification task. <ref type=\"bibr\" target=\"#b35\">[36]</ref> propose a heterogeneous graph neural network model which c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: have shown that the design of neural network architecture [ <ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and typically require pixel-level correspondences of sens  better correspondence, MMF <ref type=\"bibr\" target=\"#b25\">[26]</ref> adopts continuous convolution <ref type=\"bibr\" target=\"#b13\">[14]</ref> to build dense LiDAR BEV feature maps and do point-wise fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e for a subset of classes (e.g., <ref type=\"bibr\" target=\"#b31\">Romera-Paredes and Torr, 2015;</ref><ref type=\"bibr\" target=\"#b41\">Veeranna et al., 2016;</ref><ref type=\"bibr\" target=\"#b47\">Ye et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ome prefetchers do store their metadata in onchip caches <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, but the metada ar memory accesses. Some irregular memory accesses can be prefetched by exploiting spatial locality <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b29\">[29]</ref>, <ref type=\"bib tial patterns can be prefetched across different regions in memory. For example, the SMS prefetcher <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses on-chip tables to correlate spatial footprints with t ate Triage against two state-of-the-art on-chip prefetchers, namely, Spatial Memory Streaming (SMS) <ref type=\"bibr\" target=\"#b16\">[17]</ref> and the Best Offset Prefetcher (BO) <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref>  Register traffic characteristics. We collect a number of characteristics concerning registers <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Our first characteristic is the average number of input ope. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b32\">(Ren et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Yu et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Rodr\u00edguez et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b3\">(Han et al. 2004;</ref><ref type=\"bibr\" target=\"#b4\">Huang, Ertekin, and Giles 2006;</ref><ref type=\"bibr\" target=\"#b12\">Yoshida et al. 2010</ref>) usually leverage supervised learning algor t=\"#b4\">Huang, Ertekin, and Giles 2006;</ref><ref type=\"bibr\" target=\"#b8\">Louppe et al. 2016;</ref><ref type=\"bibr\" target=\"#b12\">Yoshida et al. 2010)</ref>, which usually solve the problem in a disc name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=\"bibr\" target=\"#b12\">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: abled critical predictions in downstream applications-e.g., predicting protein-protein interactions <ref type=\"bibr\" target=\"#b12\">[Gainza et al., 2020</ref><ref type=\"bibr\">, Huang et al., 2020]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral clustering for the graph coarsening, similarly to existing spectral-based graph pooling methods <ref type=\"bibr\" target=\"#b25\">(Ma et al., 2019;</ref><ref type=\"bibr\">Wang et al., 2019)</ref> to c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  has not interacted before. We use the widely-used protocols <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>: Precision@K, Recall@K, and NDCG@K to evaluate the performa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  leading to mode collapse.</p><p>Another recently popular approach is based on generative modelling <ref type=\"bibr\" target=\"#b30\">[33,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" ta  on the past scene, thereby naturally offering a possibility to output multiple samples. Lee et al. <ref type=\"bibr\" target=\"#b30\">[33]</ref> propose a recurrent encoder-decoder architecture within co ational Auto-Encoder architectures has been shown recently <ref type=\"bibr\" target=\"#b25\">[28,</ref><ref type=\"bibr\" target=\"#b30\">33]</ref> to successfully predict multi-modal trajectories by learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the network needs to have a high radix and a low diameter. This is achieved with a HyperX topology <ref type=\"bibr\" target=\"#b6\">[7]</ref>, with all-toall connections on each level. Links on the high. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=\"bibr\" target=\"#b11\">[14]</ref> . It could obtain transformation rules automatically durin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and regularised filter learning <ref type=\"bibr\" target=\"#b4\">[5]</ref>- <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Most existing DCF  light the most discriminative and important visual information of a target. To this end, both fixed <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and adaptive spatia colour-histogram-based image segmentation to suppress the background area. In the same spirit, BACF <ref type=\"bibr\" target=\"#b4\">[5]</ref> employs a predefined binary matrix to crop valid training sa realised by a spatial regularisation have been widely studied in recent advanced DCF-based trackers <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t F <ref type=\"bibr\" target=\"#b15\">[16]</ref>, GFSDCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, BACF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SRDCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Staple <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" target=\"#b24\">[25]</ref><ref type=\"bibr\" target=\"#b25\">[26]</ref><ref type=\"bibr\" target=\"#b26\">[27]</ref>. For example, Li . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons. Multimodal Feature Extraction. In the experiments, we use the pre-trained Inception-v4 network <ref type=\"bibr\" target=\"#b31\">[32]</ref> to extract the visual features of each image. Then, we ave. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  When the input distribution to a learning system changes, it is said to experience covariate shift <ref type=\"bibr\" target=\"#b17\">(Shimodaira, 2000)</ref>. This is typically handled via domain adapta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: domain. Although being just in its infancy, the field has already achieved quite remarkable success <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The prime example is perhaps constituted by data naturally  ises the formalization of learning of data embeddings as functions defined on non-Euclidean domains <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Hyperbolic manifolds, for example, constitute an important  hat in our framework, this hyperparameter controls the amount of parameter sharing in the PHM layer <ref type=\"bibr\" target=\"#b2\">(3)</ref>. In all our experiments, we report the test performance eval. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /p><p>Inspired by discriminative correlation filter trackers <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r ype=\"bibr\" target=\"#b19\">[20]</ref>, Attention U-Net <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acher and the student on the test set. We adopt ResNet-80 as the teacher and ResNet-8 as the student<ref type=\"bibr\" target=\"#b3\">[4]</ref> and train them separately for the image classification task . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on social media given users' posts, connections among users, and a small number of labelled users. <ref type=\"bibr\" target=\"#b34\">Rahimi et al. (2018)</ref> apply GCNs with highway connections on thi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  learning, such to avoid fake edges or adversarial attacks <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. In contrast, we consider the scenario of sparse links in C hree graph learning based models, including GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, DropEdge <ref type=\"bibr\" target=\"#b26\">[27]</ref> and GLCN <ref type=\"bibr\" target=\"#b15\">[16]</ref>. GAT is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntify which PVPs perform well. To address this, we use a strategy similar to knowledge distillation <ref type=\"bibr\" target=\"#b14\">(Hinton et al., 2015)</ref>. First, we define a set P of PVPs that in ing.</p><p>We transform the above scores into a probability distribution q using softmax. Following <ref type=\"bibr\" target=\"#b14\">Hinton et al. (2015)</ref>, we use a temperature of T = 2 to obtain a (2019)</ref>  Temperature We choose a temperature of 2 when training the final classifier following <ref type=\"bibr\" target=\"#b14\">Hinton et al. (2015)</ref>.</p><p>Auxiliary language modeling To find. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b6\">7]</ref>. This important finding attracts great interests in edge partitioning recently <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target xisting partitioners, METIS gives the lowest replication factor which is consistent with literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. However, METIS runs titioning algorithm METIS <ref type=\"bibr\" target=\"#b7\">[8]</ref> is extended for edge partitioning <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which makes full access to the graph structure by partition alanced if max i \u2208[p] {|E i |} \u2264 \u2308 \u03b1 |E | /p\u2309.<label>(1)</label></formula><p>The replication factor <ref type=\"bibr\" target=\"#b2\">[3]</ref> of a partitioning is defined as</p><formula xml:id=\"formula_ <head n=\"2.3\">NP-Hardness</head><p>The p-edge partitioning problem has been proved to be NP-hard in <ref type=\"bibr\" target=\"#b2\">[3]</ref> when p grows with n = |V |. To our best knowledge, it has no weight. One can turn a vertex-partitioner into an edge-partitioner while preserving its performance <ref type=\"bibr\" target=\"#b2\">[3]</ref>. To transform METIS to an edge-partitioner, we first call ME </table></figure> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">In<ref type=\"bibr\" target=\"#b2\">[3]</ref>, the NP-hardness is proved by a reduction from 3-partition p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: atasets and continuously set new stateof-the-art performance <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar  \u2212 L)X is understood as features averaging and propagation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. In graph signal pr rast to the recent design principle of graph neural networks <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, our results sugges 9]</ref>. Started with the early success of ChebNet <ref type=\"bibr\" target=\"#b5\">[6]</ref> and GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> at vertex classification, many variants of GNN have been p e observe that the parameters of a graph convolutional layer in a Graph Convolutional Network (GCN) <ref type=\"bibr\" target=\"#b15\">[16]</ref> only contribute to overfitting. Similar observations have   problem and provide insights to the mechanism underlying the most commonly used baseline model GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and its simplified variant SGC <ref type=\"bibr\" target=\"# onding NNs using true features.</p><p>Theorem 7 implies that, under Assumption 1, both gfNN and GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> have similar high performance. Since gfNN does not require Network model by removing nonlinearity in the neural network and only averaging features.</p><p>GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> Graph Convolutional Neural Network ($) is the most commonl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: assignment, non-maximum suppression (NMS) post-processing. They are not fully end-to-end. Recently, <ref type=\"bibr\" target=\"#b2\">Carion et al. (2020)</ref> proposed DETR to eliminate the need for suc d attention module suffers from a quadratic complexity growth with the feature map size. DETR. DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref> is built upon the Transformer encoder-deco ng different feature levels. Other hyper-parameter setting and training strategy mainly follow DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref>, except that Focal Loss <ref type=\"bibr\" t or 50 epochs and the learning rate is decayed at the 40-th epoch by a factor of 0.1. Following DETR <ref type=\"bibr\" target=\"#b2\">(Carion et al., 2020)</ref>, we train our models using Adam optimizer . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: enerating the entire sequence at once <ref type=\"bibr\" target=\"#b24\">[25]</ref> or in small batches <ref type=\"bibr\" target=\"#b19\">[20]</ref>. However, this introduces a lag in the generation process,  to capture temporal dependencies but requires fixed length videos. This limitation was overcome in <ref type=\"bibr\" target=\"#b19\">[20]</ref> but constraints need to be imposed in the latent space to  ght-forward adaptations of GANs for videos are proposed in <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, replacing the 2D convolutional layers with 3D convolutiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tation tasks before the DL revolution, including amplitude segmentation based on histogram features <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the region based segmentation method <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raditional methods like <ref type=\"bibr\" target=\"#b32\">[33]</ref> borrow the idea from PageRank and <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposes to construct label groups by minimizing the diver  the threshold are discarded and the remaining ones are kept as candidates.</p><p>Adapting based on <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we rank our candidate labels following three criteria:</p text. While C t is selected from (\u2022) in our task, this term can also be ignored, which is same with <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Then the ranking score can be defined as</p><formula xml: t=\"#tab_6\">5</ref>.</p><p>The results show that our proposed method (\u00b5 = 0.8, = 0.1), extended from <ref type=\"bibr\" target=\"#b33\">[34]</ref>, outperforms the baseline algorithm for both inverse label. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: requests. Way prediction reduces energy consumption by avoiding searching all ways to match the tag <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>Software prefetching is an appealing solution to re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch result diversification are unsupervised and they are based on handcrafted features and functions <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe  the more diversified it will be. The most typical implicit model is the MMR (Max Margin Relevance) <ref type=\"bibr\" target=\"#b4\">[5]</ref> model:</p><formula xml:id=\"formula_0\">Score MMR = \ud835\udf06score(\ud835\udc51 \ud835\udc56  reduce result redundancy by comparing document-document similarity regardless the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt, of the text. In contrast, an end-to-end (E2E) SLU system <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd extensively in both scientific and commercial workloads <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Unfortunately, considerable state is required to memorize c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tch stalls when an independent instruction cannot be steered to an empty queue. Salverda and Zilles <ref type=\"bibr\" target=\"#b13\">[14]</ref> evaluate CESP in the context of a realistic baseline and p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: studies of \"cohesiveness\" in graph search and exploration <ref type=\"bibr\">[Dass et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Zhu et al., 2018]</ref>, their definitions are orthogonal to ours.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: veloped Transformers, a new neural architecture for even more effective natural language processing <ref type=\"bibr\" target=\"#b42\">(Vaswani et al., 2017)</ref>. Transformers overcome a major drawback  al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some Transformers uses positional encoding <ref type=\"bibr\" target=\"#b42\">(Vaswani et al., 2017)</ref> or positional embedding <ref type=\"bibr\" presented in Sec 2.3.1. For other details (especially on the multi-head attention), please refer to <ref type=\"bibr\" target=\"#b42\">Vaswani et al. (2017)</ref> and in particular, <ref type=\"bibr\">GPT-2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed somewhat implicit until residual networks <ref type=\"bibr\" target=\"#b5\">(He et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>) explicitly introduced a reparameterization of  ead><p>Since the advent of residual networks <ref type=\"bibr\" target=\"#b5\">(He et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>), most state-of-the-art networks for image clas d of a sequence of such residual blocks. In comparison with the full pre-activation architecture in <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>, we remove two batch normalization layers and o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b31\">32]</ref>. Traditional work on multimedia recommendation, e.g., VBPR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and  y consists of two essential components: light graph convolution and layer combination.</p><p>\u2022 VBPR <ref type=\"bibr\" target=\"#b11\">[12]</ref>: Based upon the BPR model, it integrates the visual featur <ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. For example, VBPR <ref type=\"bibr\" target=\"#b11\">[12]</ref> extended Matrix Factorization by incorporating visual feat s.</p><p>For Clothing dataset where visual features are very important in revealing item attributes <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, VBPR, MMGCN, and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al viewpoint and lighting variations. Another body of research predicts 3D facial meshes from audio <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta erate 3D face models driven by input audio or text, but do not necessarily aim for photorealism. In <ref type=\"bibr\" target=\"#b37\">[38]</ref>, the authors learn a Hidden Markov Model (HMM) to map Mel-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that can be used as encoding backbone, such as VGG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, ResNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the encoding networks. PSPNet is a scene segmentation mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b11\">[12]</ref>, RegNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> and EfficientNet <ref type=\"bibr\" target=\"#b27\">[28]</ref>. The results are shown in Table <ref type=\"table\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>The maximum length limit in BERT naturally reminds us the limited capacity of Working Memory <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a human cognitive system storing information for logical re ttentional system capable of selecting and operating control processes and strategies\", as Baddeley <ref type=\"bibr\" target=\"#b1\">[2]</ref> pointed out in his 1992 classic. Later research detailed tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rous natural phenomenon, posing huge threats to human life, aviation and electrical infrastructures <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The great harm of lightning has driven significant interest ef> propose a new parameterization by combining PR92 and cloud droplet concentration. Mccaul et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> propose two approaches based on the upward fluxes of precipi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b23\">[24]</ref> and modeling with uniform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  loss from the image-space to a higher-level feature space of an object recognition system like VGG <ref type=\"bibr\" target=\"#b48\">[49]</ref>, resulting in sharper results despite lower PSNR values.</ etwork once to get the result. The exclusive use of 3\u00d73 filters is inspired by the VGG architecture <ref type=\"bibr\" target=\"#b48\">[49]</ref> and allows for deeper models at a low number of parameters  map \u03c6, we use a pre-trained implementation of the popular VGG-19 network <ref type=\"bibr\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. It consists of stacked convolutions coupled with pooling l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g.</p><p>Setup We apply our method to convert instances from the binary Stanford Sentiment Treebank <ref type=\"bibr\" target=\"#b23\">(Socher et al., 2013</ref>, SST-2) into prompts, using the standard t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oposal extends such a solution to all memory hierarchy, which is getting deeper recently.</p><p>D2M <ref type=\"bibr\" target=\"#b26\">[27]</ref> finds the actual location of a block in the memory hierarc p><p>Cache level prediction can achieve high accuracy with a simple table. Also, in contrast to D2M <ref type=\"bibr\" target=\"#b26\">[27]</ref>, the overall system design can remain untouched because it. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  models. The disagreement can be defined in several ways, here we take the vote entropy proposed by <ref type=\"bibr\" target=\"#b8\">(Dagan and Engelson, 1995)</ref>. Given a committee consist of C model m, 2005;</ref><ref type=\"bibr\" target=\"#b19\">Kim et al., 2006)</ref> and committee-based approaches <ref type=\"bibr\" target=\"#b8\">(Dagan and Engelson, 1995)</ref>   <ref type=\"formula\">2017</ref>) fur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve 3D information. To this end, we conduct formal analyses in the spherical coordinate system (SCS) <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2019)</ref>, and show that relative location of each ato. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we are the first to discuss their importance under heterop  COMBINE functions that leverage each representation separately-e.g., concatenation, LSTM-attention <ref type=\"bibr\" target=\"#b37\">[38]</ref>. This design is introduced in jumping knowledge networks <  <ref type=\"bibr\" target=\"#b37\">[38]</ref>. This design is introduced in jumping knowledge networks <ref type=\"bibr\" target=\"#b37\">[38]</ref> and shown to increase the representation power of GCNs und ons from two rounds with the embedded ego-representation (following the jumping knowledge framework <ref type=\"bibr\" target=\"#b37\">[38]</ref>), GCN's accuracy increases to 58.93%\u00b13.17 for h = 0.1, a 2 label>(7)</label></formula><p>where we empirically find concatenation works better than max-pooling <ref type=\"bibr\" target=\"#b37\">[38]</ref> as the COMBINE function.</p><p>In the classification stage e compare GraphSAGE, GCN-Cheby and GCN to their corresponding variants enhanced with JK connections <ref type=\"bibr\" target=\"#b37\">[38]</ref>. GCN and GCN-Cheby benefit significantly from D3 in hetero with and without JK connections is similar (gaps mostly less than 2%), matching the observations in <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p><p>While other design choices and implementation detai K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=\"bibr\" target=\"#b37\">[38]</ref> without changing the number of layers or other hyperparame K, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat <ref type=\"bibr\" target=\"#b37\">[38]</ref> without changing the number of layers or other hyperparame. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  large-scale ST dataset, multitask learning <ref type=\"bibr\" target=\"#b32\">(Weiss et al. 2017;</ref><ref type=\"bibr\" target=\"#b6\">B\u00e9rard et al. 2018</ref>) and pretraining techniques <ref type=\"bibr\"  ficantly increases the learning difficulty.</p><p>\u2022 Non-pre-trained Attention Module: Previous work <ref type=\"bibr\" target=\"#b6\">(B\u00e9rard et al. 2018)</ref> trains attention modules for ASR, MT and ST. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b17\">[18]</ref>, GC-SAN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and PinSage <ref type=\"bibr\" target=\"#b44\">[45]</ref>. 4) Heterogeneous information network/knowledge graph. The. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g/ns/1.0\"><head n=\"4.1\">The Base Model: BLSTM-CRF</head><p>Many recent sequence labeling frameworks <ref type=\"bibr\" target=\"#b25\">(Ma and Hovy, 2016b;</ref><ref type=\"bibr\" target=\"#b27\">Misawa et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: where permutation equivariance is either learned from data <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> or obtained by design <ref type=\"bibr\" target=\"#b23\">[24]</ ]</ref>. With node features acting as identifiers, MPNN were shown to become universal in the limit <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which implies that they can solve the graph isomorphism t d evidence that the power of MPNN grows as a function of depth and width for certain graph problems <ref type=\"bibr\" target=\"#b22\">[23]</ref>, showing that (both anonymous and non-anonymous) MPNN cann ether depth and width needs to grow with the number of nodes solely in the worst-case (as proven in <ref type=\"bibr\" target=\"#b22\">[23]</ref>) or with certain probability over the input distribution.< apacity is an effective generalization of the previously considered product between depth and width <ref type=\"bibr\" target=\"#b22\">[23]</ref>, being able to consolidate more involved properties, as we lower bounds rely on a new technique which renders them applicable not only to worst-case instances <ref type=\"bibr\" target=\"#b22\">[23]</ref>, but in expectation over the input distribution.</p><p>An  previous theoretical findings that non-anonymous MPNN are universal and can solve graph isomorphism <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, as well as that the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e target problem <ref type=\"bibr\" target=\"#b14\">(Qi et al., 2018)</ref>. The approaches proposed by <ref type=\"bibr\" target=\"#b18\">Snell et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b20\">Sung et  ication</head><p>Few-shot classification <ref type=\"bibr\" target=\"#b21\">(Vinyals et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Snell et al., 2017)</ref> is a task in which a classifier must be ada ng Networks <ref type=\"bibr\" target=\"#b21\">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017)</ref> 68.17 Graph Network <ref type=\"bibr\" targe </p><p>\u2022 Prototypical Networks: a deep metric-based method using sample average as class prototypes <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017)</ref>.</p><p>\u2022 Graph Network: a graph-based few- the performance by few-shot classification accuracy following previous studies in few-shot learning <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017;</ref><ref type=\"bibr\" target=\"#b20\">Sung et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The main concept behind these approaches is to interpret  s successfully transferred from NLP to protein sequences <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, with the excep f their surrounding context (residues next to it). As previously established for another protein LM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, the t-SNE projections (e.g. ProtBert Fig. <ref type=\"figu \">[20]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, we might expect an upper limit for what protein LMs can l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b13\">[14]</ref> of machine translation and the neural aggregation networks <ref type=\"bibr\" target=\"#b14\">[15]</ref> of video face recognition, we propose the Frame Attention . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ive clothing (Safety smocks and blankets) has been designed to be worn by actively suicidal inmates <ref type=\"bibr\" target=\"#b7\">(Hayes, 2013)</ref>. A top door alarm <ref type=\"bibr\" target=\"#b4\">(C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b40\">41]</ref> and modelling short-text similarities <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar e=\"bibr\" target=\"#b30\">31]</ref> or representation-focused <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" ta  the distributed model improves drastically in the presence of more data. Unlike some previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta nt. Our n-graph based input encoding is motivated by the trigraph encoding proposed by Huang et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, but unlike their approach we don't limit our input repres  on the retrieval task, as a baseline in this paper. Both the deep structured semantic model (DSSM) <ref type=\"bibr\" target=\"#b15\">[16]</ref> and its convolutional variant CDSSM <ref type=\"bibr\" targe lated papers that use short text such as title, for document ranking or related tasks. Huang et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> learn a distributed representation of query and title, for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <ref type=\"bibr\" target=\"#b20\">(Ranganath et al., 2014)</ref>, to include BERT representations. Our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n technique for achieving truthfulness in online auctions is based on the concept of a supply curve <ref type=\"bibr\" target=\"#b21\">[22]</ref>, as applied by Zhang et al. <ref type=\"bibr\" target=\"#b3\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ices but it may incur the loss of information during the training process. To this end, inspired by <ref type=\"bibr\" target=\"#b3\">[Dai et al., 2016]</ref>, we transform the binary optimization problem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ad coverage of the protein universe, as found in the 17929 families of the recent Pfam 32.0 release <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Recent work that applies deep learning is either restrict database is carefully curated, at least 25% of sequences have no experimentally validation function <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and additional experimental functional characterization o of the art models including profile HMMs we use the highly curated Protein families (Pfam) database <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. The 17929 familie otKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Many domains have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the activations of deep neural networks. For this, we consider the \"natural pre-image\" technique of <ref type=\"bibr\" target=\"#b20\">[21]</ref>, whose goal is to characterize the invariants learned by a n untrained deep convolutional generator can be used to replace the surrogate natural prior used in <ref type=\"bibr\" target=\"#b20\">[21]</ref> (the TV norm) with dramatically improved results. Since th antic segmentation) is highly detrimental.</p><p>Natural pre-image. The natural pre-image method of <ref type=\"bibr\" target=\"#b20\">[21]</ref> is a diagnostic tool to study the invariances of a lossy f e obtained by restricting the pre-image to a set X of natural images, called a natural pre-image in <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>In practice, finding points in the natural pre-imag inding points in the natural pre-image can  Inversion with deep image prior Inversion with TV prior <ref type=\"bibr\" target=\"#b20\">[21]</ref> Pre-trained deep inverting network <ref type=\"bibr\" target  on ImageNet ISLVRC) using three different regularizers: the Deep Image prior, the TV norm prior of <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and the network trained to invert representations on a ho ne by regularizing the data term similarly to the other inverse problems seen above. The authors of <ref type=\"bibr\" target=\"#b20\">[21]</ref> prefer to use the TV norm, which is a weak natural image p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ne for person name recognition and the other for publication recognition. Specifically, we use LSTM <ref type=\"bibr\" target=\"#b6\">[7]</ref> as the RNN unit:</p><formula xml:id=\"formula_0\">N = LST M (S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: een proposed and achieved state-of-the-art results in SISR <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar  have been proposed for a better performance. Lim et al. proposed a very deep and wide network EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref> by stacking modified residual blocks in which the batch no the performance. Fig. <ref type=\"figure\">3</ref>(Left) depicts a basic residual module used in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ESRGAN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The  ion, we investigate the combination of our RFA framework with the basic residual block used in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Different from the original residual block used in image  N <ref type=\"bibr\" target=\"#b14\">[15]</ref>, MemNet <ref type=\"bibr\" target=\"#b24\">[25]</ref>, EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>, SRMD <ref type=\"bibr\" target=\"#b35\">[36]</ref>, NLRN <ref n the top row, which can ease the training difficulty to some extent (e.g. residual scaling in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>).</p><p>(2) Feature maps after the attention mechanism ten rchitectures. Here we introduce one of the basic architecture used by some state-of-the-art methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -cache miss sequences directly. Our key observation, inspired by recent studies of data prefetching <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target  prefetching approaches that only retrieve a constant number of blocks in response to a miss (e.g., <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" targ sign is based on recent proposals for addresscorrelated prefetch of recurring temporal data streams <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t to three prior approaches: AttnGAN <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, DM-GAN <ref type=\"bibr\" target=\"#b52\">(Zhu et al., 2019)</ref>, and DF-GAN <ref type=\"bibr\" target=\"#b44\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ls and normalisation that are equally valid <ref type=\"bibr\" target=\"#b51\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Tsai et al., 2019)</ref>.</p><formula xml:id=\"formula_10\">M SA(X) [f  t=\"#b6\">(Cohen &amp; Welling, 2016b;</ref><ref type=\"bibr\" target=\"#b55\">Worrall et al., 2017;</ref><ref type=\"bibr\" target=\"#b46\">Thomas et al., 2018;</ref><ref type=\"bibr\">Kondor et al., 2018;</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and thus results in the use of more sophisticated features <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and regularised filter learning <ref type=\"bibr\" target=\"#b4 on, spatial attention mechanisms and discriminative data fitting, have not been adequately explored <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" features maps, which may exceed thousands of channels, include irrelevant and redundant information <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The filters trai  such feature maps often contain negligible energy and may degrade the performance of a DCF tracker <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>. So far there are ually use multiple features, such as HOG, CN and CNN, collaboratively for robust feature extraction <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\"  CSRDCF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref type=\"bibr\" target=\"#b55\">[56]</ref>, MCPF <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d the storage overhead of temporal history by sharing it across cores. Call graph prefetching (CGP) <ref type=\"bibr\" target=\"#b45\">[42]</ref> analyzed call graphs to prefetch instructions of the funct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and it was used in <ref type=\"bibr\" target=\"#b1\">[2]</ref>  nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=\"bibr\" target=\"#b0\">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=\"bibr\" target=\"#b0\">(1)</ref>, and so now we generate results by performing a second pass . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecent progress on Vision Transformers (ViT) <ref type=\"bibr\">[16]</ref>. In contrast to prior works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\">16]</ref> that train self-supervised Transfo sed learning in computer vision.</p><p>Self-supervised Transformers for vision. In pioneering works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\">16]</ref>, training self-supervised Transfor t=\"#b18\">[20]</ref> and *: equal contribution. framework model params acc. (%) linear probing: iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> iGPT-L 1362M 69.0 iGPT <ref type=\"bibr\" target=\"#b8\">[9]</re params acc. (%) linear probing: iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> iGPT-L 1362M 69.0 iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> iGPT-XL 6801M 72.0 MoCo v3</p><p>ViT-B 86M 76.7 MoCo v3</p>< cation, evaluated by linear probing (top panel) or end-to-end fine-tuning (bottom panel). Both iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> and masked patch prediction <ref type=\"bibr\">[16]</ref> belo t=\"#b34\">[36,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> (Table <ref type=\"table\">1</ref>). iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> masks and reconstructs pixels, and the self-supervised varia bone. It leads to \u223c1% improvement consistently (see Fig. <ref type=\"figure\">8</ref>).</p><p>In iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref>, accuracy can be improved by using longer sequences in the p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplied to the task of single image super-resolution (SISR) <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar of image super-resolution. Thus, in recent image SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, batch normalizatio rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar f type=\"bibr\" target=\"#b13\">[14]</ref>, SRResNet <ref type=\"bibr\" target=\"#b16\">[17]</ref> and EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>). The increasing of depth brings benefits to representatio ding blocks for image super-resolution networks. Compared with vanilla residual blocks used in EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce WDSR-A which has a slim identity mapping pat 5]</ref> leads to better accuracy for deep super-resolution networks. Previous works including EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, BTSRN <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RDN <re e for training SR networks. However, with the increasing depth of neural networks for SR (e.g. MDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> has depth around 180), the networks without batch normaliz ing deeper and deeper (from 3-layer SRCNN <ref type=\"bibr\" target=\"#b3\">[4]</ref> to 160-layer MDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>), training becomes more difficult. Batch normalization lay f type=\"figure\">1</ref>. Two-layer residual blocks are specifically studied following baseline EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Assume the width of identity mapping pathway (Fig. <ref t <p>Figure <ref type=\"figure\">2</ref>: Demonstration of our simplified SR network compared with EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>In this part, we overview the WDSR network architec his part, we overview the WDSR network architectures. We made two major modifications based on EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> super-resolution network.</p><p>Global residual pathway Fi set <ref type=\"bibr\" target=\"#b34\">[35]</ref>  In this part, we show results of baseline model EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> and our proposed WDSR-A and WDSR-B for the task of image b e results suggest that our proposed WDSR-A and WDSR-B have better accuracy and efficiency than EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>. WDSR-B with wider activation also has better or similar p curacy drop with our simpler form.</p><p>Upsampling layer Different from previous state-of-the-arts <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> where one or more  lips and rotations following common data augmentation methods<ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. During training, the input images are also subtracted with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: deep metric learning <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, part-based methods < stage, we use the meta-test D u to compute the domain loss and relation alignment loss with Eq. (3) <ref type=\"bibr\" target=\"#b6\">(7)</ref>, which is formulated as follows:</p><formula xml:id=\"formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction based models <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b31\">21,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>. Interaction based models thrive with encoding word-word tr to ranking signals <ref type=\"bibr\" target=\"#b20\">[11,</ref><ref type=\"bibr\" target=\"#b22\">13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>. Learned end-to-end from user feedbacks <ref type=\"bibr\" ta t=\"#b39\">29]</ref>. Learned end-to-end from user feedbacks <ref type=\"bibr\" target=\"#b33\">[23,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>, the word embeddings can encode so matches tailored for rel nce than score-based ones like mean-pooling or max-pooling <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>.</p><p>Kernel-pooling is applied to each M h q ,h d matrix  hes are more e ective than weight-summing the similarities <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>-\"similarity does not necessarily mean relevance\" <ref type= lored for relevance ranking, which has signi cant advantages over traditional feature-based methods <ref type=\"bibr\" target=\"#b39\">[29,</ref><ref type=\"bibr\" target=\"#b40\">30]</ref>. ese initial succe d learningto-rank techniques are then used to combine the n-gram somatches to the nal ranking score <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>e CNN is the key to modeling n-grams. Typical IR ap [30]</ref>.</p><p>K-NRM uni ed the progress of IR customized embeddings and interaction based model <ref type=\"bibr\" target=\"#b39\">[29]</ref>. It rst embeds words and builds the translation matrix usi  log, K-NRM outperforms both neural IR methods and feature-based learning-to-rank by a large margin <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>ough the so matching of n-grams in information retr -torank layer to calculate the ranking score using the n-gram translations M. is part extends K-NRM <ref type=\"bibr\" target=\"#b39\">[29]</ref> to n-grams. Kernel-pooling is a pooling technique that use ead><p>Conv-KNRM adds the ability of so matching n-grams to the recent state-of-the-art K-NRM model <ref type=\"bibr\" target=\"#b39\">[29]</ref> with convolutional neural networks (CNNs). Without CNNs, C g Conv-KNRM requires large-scale training data, for example, user clicks in a commercial search log <ref type=\"bibr\" target=\"#b39\">[29]</ref> or industryscale annotations <ref type=\"bibr\" target=\"#b31 raining data. ey are then used in the target domain to generate so -TF features \u03a6(M). Xiong, et al. <ref type=\"bibr\" target=\"#b39\">[29]</ref> showed that kernel-pooled so -TF features reveal di erent  rnel is of low importance in search logs as all candidate documents already contain the query words <ref type=\"bibr\" target=\"#b39\">[29]</ref>; however, synonyms can be a strong signal in a recall-orie Log: Sogou.com is a major Chinese commercial search engine.</p><p>e same se ings as K-NRM were used <ref type=\"bibr\" target=\"#b39\">[29]</ref>. e same sample of Sogou log and training-testing splits ar IR baselines for stronger baseline performance. Body texts of training documents were not available <ref type=\"bibr\" target=\"#b39\">[29]</ref>. e Chinese text was segmented by ICTCLASS <ref type=\"bibr\" ad><p>Training and testing labels on Sogou-Log and Bing-Log were generated following prior research <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> ref type=\"bibr\" target=\"#b33\">[23]</ref>, DRMM <ref type=\"bibr\" target=\"#b22\">[13]</ref>, and K-NRM <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>CDSSM <ref type=\"bibr\" target=\"#b36\">[26]</ref> is  ry and document representations on their words' le er-tri-grams (or Chinese characters in Sogou-Log <ref type=\"bibr\" target=\"#b39\">[29]</ref>). e ranking scores are calculated by the similarity betwee  erwards.</p><p>K-NRM is a state-of-the-art neural model previously tested on the Sogou-Log dataset <ref type=\"bibr\" target=\"#b39\">[29]</ref>. It uses kernel-pooling instead of DRMM's histogram poolin earch logs, 5-fold cross validation were used to be consistent with the previous study on Sogou-Log <ref type=\"bibr\" target=\"#b39\">[29]</ref>. On ClueWeb09-B, the 10-fold cross validation splits from  n Sogou-Log, traditional IR methods used both title and body, and neural IR methods only used title <ref type=\"bibr\" target=\"#b39\">[29]</ref>, as discussed in section 5.1. On Bing-Log, all methods use were all learned end-to-end using the query logs. For Sogou-log, we set embedding dimension L = 300 <ref type=\"bibr\" target=\"#b39\">[29]</ref> . For Bing-Log, we set L = 100 because our pilot study sho ers were: \u00b5 1 = 0.9, \u00b5 2 = 0.7, ..., \u00b5 10 = \u22120.9.</p><p>e \u03c3 of the so match bins were set to be 0.1 <ref type=\"bibr\" target=\"#b39\">[29]</ref>. Model Implementation and E ciency: e model was implemente bout 12 hours on an AWS GPU machine. e training time is similar with prior work using only unigrams <ref type=\"bibr\" target=\"#b39\">[29]</ref>. Most computation time was spent on the embedding layer; t rrent neural IR methods to provide additional improvements <ref type=\"bibr\" target=\"#b32\">[22,</ref><ref type=\"bibr\" target=\"#b39\">29,</ref><ref type=\"bibr\" target=\"#b40\">30]</ref> Comparing the two s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: h as the element-wise mean E p [z] = [\u03c0 1 , ..., \u03c0 k ] of these vectors.</p><p>The Gumbel-Max trick <ref type=\"bibr\" target=\"#b8\">(Gumbel, 1954;</ref><ref type=\"bibr\" target=\"#b12\">Maddison et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  consider the degrees of importance of items based on their rankings.</p><p>Recently, Tang and Wang <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposed a KD model to address the ranking problem, called ledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> and present rank distillation (RD) <ref type=\"bibr\" target=\"#b10\">[11]</ref> that applies knowledge distillation to recommender models. ommendation problem because Fig. <ref type=\"figure\">1</ref>. Illustration of rank distillation (RD) <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The teacher model transfers manipulated top-k items as th t may have worse performance than the original student model. Rank distillation (RD). Tang and Wang <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposed ranking distillation (RD) that applies KD for ran e rated by less than 5 users. Table I reports the detailed statistics of these datasets.</p><p>\u2022 RD <ref type=\"bibr\" target=\"#b10\">[11]</ref>: To define the KD loss in equation ( <ref type=\"formula\" t een \u03bb that appears in RD and CD. Specifically, we used the following parameter settings.</p><p>\u2022 RD <ref type=\"bibr\" target=\"#b10\">[11]</ref> and RD-Rank: We set \u03c1 to be 0.5. For CDAE, the number of i er, we used the public PyTorch implementation <ref type=\"foot\" target=\"#foot_5\">6</ref> provided in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. All experiments were conducted on a desktop with 128 GB m KD. Also, the gain indicates how additional accuracy achieved by the proposed model over that of RD <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>Based on this evaluation, we found several interest rform RD over all datasets. Note that the improvement gap for RD is somewhat different from that in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. It is because we used leave-one-out evaluation while <ref  in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. It is because we used leave-one-out evaluation while <ref type=\"bibr\" target=\"#b10\">[11]</ref> used cross-validation evaluation. Our models are consisten el size and efficiency. The model size is proportional to the accuracy of our model, as observed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> as well. The same tendency consistently holds in different oot\" n=\"4\" xml:id=\"foot_3\">http://dawenl.github.io/data/gowalla pro.zip Competitive models. Since RD<ref type=\"bibr\" target=\"#b10\">[11]</ref> is the state-of-the-art KD model for top-N recommendation,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to perform Chinese NER is to first perform word segmentation and then apply word sequence labeling <ref type=\"bibr\" target=\"#b7\">[Yang et al., 2016;</ref><ref type=\"bibr\">He and Sun, 2017]</ref>.</p> rd information for NER has attracted research attention <ref type=\"bibr\">[Passos et al., 2014;</ref><ref type=\"bibr\" target=\"#b7\">Zhang and Yang, 2018]</ref>. In particular, to exploit explicit word i ational efficiency <ref type=\"bibr\">[Strubell et al., 2017]</ref>.</p><p>Specifically, lattice LSTM <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref> employs double recurrent transition compu hinese word segmentation, character-based name taggers can outperform their word-based counterparts <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. <ref type=\"bibr\" target=\"#b7\">Zhang and  ilation. This method achieves great performance in the English NER task. Lattice LSTM. Lattice LSTM <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref> can model the characters in sequence and  ></head><label></label><figDesc>, Weibo NER[Peng and Dredze, 2015; He and Sun, 2016], and Resume NER<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<re NER<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>.The OntoNotes and MSRA are the newswire d utperform their word-based counterparts <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. <ref type=\"bibr\" target=\"#b7\">Zhang and Yang [2018]</ref> exploit an RNNbased lattice structure to s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: till very effective Bag of Embeddings model <ref type=\"bibr\" target=\"#b45\">(White et al. 2015;</ref><ref type=\"bibr\" target=\"#b0\">Arora, Liang, and Ma 2017)</ref> showing that, even in this case, A re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ces a channel-attention mechanism by adaptively recalibrating the channel feature responses. SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref> brings the feature-map attention across two network branch /ref>: Comparing our ResNeSt block with SE-Net <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref>. A detailed view of Split-Attention unit is shown in Figur -Net operates on top of the entire block regardless of multiple groups. Previous models like SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref> introduced feature attention between two network branches,  type=\"bibr\" target=\"#b28\">[29]</ref>, ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref> and SKNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Remarkably, our ResNeSt-50 achieves 80.64 top-1 accuracy, ach group is Split Attention in Cardinal Groups. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, a combined representation for each cardinal group can be o obal average pooling across spatial dimensions s k \u2208 R C/K <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Here the c-th component is calculated as:</p><formula xml: Our method generalizes prior work on feature-map attention <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> within a cardinal group setting <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  popular recently with the boom of recurrent neural networks <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target t al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> proposed the GRU4REC, which applies a multi layer GRU <ref type=\"bibr\" target=\"#b1\">[2]</ref> to simply treat the data as time series. Based on the RNN mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or feature matrix <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>) are not compatibl denoising autoencoder to disturb the structure information. To build a symmetric graph autoencoder, <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes Laplacian sharpening as the counterpart of Laplac ing the latent representations to match a prior distribution for robust node embeddings.</p><p>GALA <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes a symmetric graph convolutional autoencoder recov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n or essential characteristics. However, there is no such wide-accepted formal definition. Paulheim <ref type=\"bibr\" target=\"#b5\">[6]</ref> defined four criteria for knowledge graphs. Ehrlinger and W  statistical relational learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>, knowledge graph refinement <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Chinese knowledge graph construction <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><p>Inspired by the success of augmentation methods in ASR <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, as a remedy to avoid overfitting while using lowresource t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ional networks, have gained much attention and improved the state of the art in node classification <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mal preprocessing scheme.</p><p>The spectrograms are processed by a deep bidirectional LSTM network <ref type=\"bibr\" target=\"#b10\">(Graves et al., 2013)</ref> with a Connectionist Temporal Classificat M is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM <ref type=\"bibr\" target=\"#b10\">(Graves et al., 2013)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ate a variable number of features to a fixed-length vector. Fisher Vectors were first introduced in <ref type=\"bibr\" target=\"#b15\">(Jaakkola and Haussler 1999)</ref> to combine the advantages of gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the previous methods, our Fig. <ref type=\"figure\">1</ref>: Comparing our ResNeSt block with SE-Net <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SK-Net <ref type=\"bibr\" target=\"#b37\">[38]</ref>. A de then the intermediate representation of each group is Split Attention in Cardinal Groups. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, a combined repres ead of the 1 \u00d7 1 layer to better preserve such information <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Convolutional layers require handling featuremap boundarie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  progress in SSL seeks to harness this flexible learning paradigm for graph representation learning <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arthquakes increase the intensity more than small earthquakes. For more on the ETAS model, see e.g. <ref type=\"bibr\" target=\"#b11\">Ogata (1988</ref><ref type=\"bibr\" target=\"#b12\">Ogata ( , 1998))</ref <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Residual analysis</head><p>Residual analysis <ref type=\"bibr\" target=\"#b11\">(Ogata, 1988)</ref> is a type of model checking for point processes s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g the most promising techniques for this problem has been the usage of collaborative filtering (CF) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, which models users' historical interactions with items to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b29\">[30]</ref>, conditioned by text embeddings obtained from a pretrained encoder. Zhang et al. <ref type=\"bibr\" target=\"#b53\">[54,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref> improved image qua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: adds too many padding operations and requires a large kernel to capture context. Inspired by CARAFE <ref type=\"bibr\" target=\"#b42\">[43]</ref>, we designed concentration-aware guided upsampling. Our CA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: i-c.org/ns/1.0\"><head n=\"1.\">Introduction</head><p>Transformer models were originally introduced by <ref type=\"bibr\" target=\"#b36\">Vaswani et al. (2017)</ref> in the context of neural machine translat p><p>Initially, in \u00a7 3.1, we introduce a formulation for the transformer architecture introduced in <ref type=\"bibr\" target=\"#b36\">(Vaswani et al., 2017)</ref>. Subsequently, in \u00a7 3.2 and \u00a7 3.3 we pre the memory consumption with respect to the self attention layer. In all experiments, we use softmax <ref type=\"bibr\" target=\"#b36\">(Vaswani et al., 2017)</ref> to refer to the standard transformer arc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nterpretable decisions. To this end, we resort to a novel hybrid and interpretable framework in DCM <ref type=\"bibr\" target=\"#b52\">[55]</ref>, where knowledge-based hand-crafted functions can be augme pe=\"bibr\" target=\"#b20\">23]</ref>. In this paper, we utilize the Learning Multinomial Logit (L-MNL) <ref type=\"bibr\" target=\"#b52\">[55]</ref>, as our base DCM model.</p></div> <div xmlns=\"http://www.t pture. The inclusion of NN-based terms helps to alleviate this issue.</p><p>Recently proposed L-MNL <ref type=\"bibr\" target=\"#b52\">[55]</ref> architecture allows having both NN-based and knowledge-bas keep the knowledge-based functions and the parameters interpretable after adding the neural network <ref type=\"bibr\" target=\"#b52\">[55,</ref><ref type=\"bibr\" target=\"#b20\">23]</ref>. In this paper, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b56\">58]</ref>. Notably, by integrating with region proposal network (RPN). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >1</ref> depicts an example of 3-shot link prediction in KGs.</p><p>To do few-shot link prediction, <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref> made the first trial and proposed GMatching ed parameters, it's like \"a gradient through a gradient\".</p><p>As far as we know, work proposed by <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref> is the first research on few-shot learning  and Evaluation Metrics</head><p>We use two datasets, NELL-One and Wiki-One which are constructed by <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref>. NELL-One and Wiki-One are derived from NEL  simple TransE embedding model, denoted as -g -r. The result under the third setting is copied from <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref>. It uses the triples from background graph, e transferring relation meta to incomplete triples during prediction.</p><p>Compared with GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018</ref>) which relies on a background knowledge gra e heavily rely on rich training instances <ref type=\"bibr\" target=\"#b26\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b22\">Xiong et al., 2018)</ref>, thus are limited to do few-shot link predi r\" target=\"#b17\">Vinyals et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Snell et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Xiong et al., 2018)</ref>, which tries to learn a matching metric bet f> is a typical method using symmetric twin networks to compute the metric of two inputs. GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018)</ref>, the first trial on one-shot link predicti own in Table <ref type=\"table\" target=\"#tab_6\">4</ref>. The baseline in our experiment is GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018)</ref>, which made the first trial on few-shot li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hieves better performance than the state-of-theart methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing the depth without adding any parameters,  eNet <ref type=\"bibr\" target=\"#b19\">[21]</ref>, Kim et al. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose two very deep convolutional networks for SR, both s ameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and non-DL <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref typ  testing sets, by citing the results of prior methods from <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. The two DRRN models outperforms all existing methods in al chieves better performance than the state-of-theart methods<ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing the depth without adding any parameters,   the performance and significantly outperforms VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30 <ref type=\"bibr\" target=\"#b16\">[17]</ref> by 0.3  the other hand, to control the model parameters, the Deeply-Recursive Convolutional Network (DRCN) <ref type=\"bibr\" target=\"#b13\">[14]</ref> introduces a very deep recursive layer via a chain structu ><p>(2) Recursive learning of residual units is proposed in DRRN to keep our model compact. In DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, a deep recursive layer (up to 16 convolutional recursions et <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> illustrat type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The blue dashed box refers to a recursive layer, among wh esNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and DRRN. U, d, T, and B are the numbers of residual units cursive block.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3.\">DRCN</head><p>DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> is motivated by the observation that adding more weight la s a given image x as feature maps H 0 . The inference net f 2 (H 0 ) stacks T recursions (T = 16 in <ref type=\"bibr\" target=\"#b13\">[14]</ref>) in a recursive layer, with shared weights among these rec \">[3]</ref>. Very deep models (d \u2265 20) include VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, RED <ref type=\"bibr\" target=\"#b16\">[17]</ref> and DRRN wi es the performance and significantly outperforms VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30<ref type=\"bibr\" target=\"#b16\">[17]</ref> by 0.37  type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref>. The blue dashed box refers to a recursive layer, among wh nt CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar mparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b21\">23]</ref>, we crop pixels nea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which solve the problem of limited equipment resources and e the problem of limited equipment resources and reduce the running time. For example, Zhang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> constructed an embedding based model to distill user's met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y datasets which are more likely to have nodes with different labels connected together. Zhu et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> highlight this problem and propose node's ego-embedding an s of the datasets are presented in Table <ref type=\"table\" target=\"#tab_1\">1</ref>. Homophily ratio <ref type=\"bibr\" target=\"#b34\">[35]</ref> denotes the fraction of edges which connects two nodes of  ts as the best performance of these models. GCNII <ref type=\"bibr\" target=\"#b6\">[7]</ref> and H2GCN <ref type=\"bibr\" target=\"#b34\">[35]</ref> have proposed multiple variants of their model. We have ch de classification task. Results for GCN, GAT, GraphSAGE, Cheby+JK, MixHop and H2GCN-1 are taken from<ref type=\"bibr\" target=\"#b34\">[35]</ref>. For GEOM-GCN and GCNII results are taken from the respect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sk structured prediction (Section 5.1). Recent work has highlighted benefits of multi-task learning <ref type=\"bibr\" target=\"#b4\">(Changpinyo et al., 2018)</ref> and transfer learning <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sociations. Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets  gion embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> for improving accuracy.</p></div> <di ing each word in the text to a word vector (word embedding). We take a more general viewpoint as in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> and consider text region embedding -e w-1 as input, serves as an unsupervised embedding function in the model for text categorization. In <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> unsupervised embeddings obtained this ings. Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: discourse representation structure parsing <ref type=\"bibr\" target=\"#b36\">(Liu et al., 2018)</ref>  <ref type=\"bibr\" target=\"#b28\">(Konstas et al., 2017)</ref>. In these works a structured representat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , and those have been very promising <ref type=\"bibr\" target=\"#b17\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b6\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Gilmer et al b28\">(Shervashidze et al., 2011;</ref><ref type=\"bibr\" target=\"#b17\">Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b6\">Hamilton et al., 2017)</ref>.</p><p>Yet, such aggregation schemes some \u2022 u\u2208 N (v) (deg(v)deg(u)) \u22121/2 h (l\u22121) u (2)</formula><p>where deg(v) is the degree of node v in G. <ref type=\"bibr\" target=\"#b6\">Hamilton et al. (2017)</ref> derived a variant of GCN that also works   and can be viewed as a form of a \"skip connection\" between different layers.For COMBINE, GraphSAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> uses concatenation after a feature trans o select the important neighbors via an attention mechanism. The max-pooling operation in GraphSAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> implicitly selects the important nodes.  ords features for each document (node) and citation links (edges) between documents. (II) On Reddit <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>, the task is to predict the community to ataset contains word vectors as node features. (III) For protein-protein interaction networks (PPI) <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> We compare against three baselines: Grap lutional Networks (GCN) <ref type=\"bibr\" target=\"#b17\">(Kipf &amp; Welling, 2017)</ref>, Graph-SAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> and Graph Attention Networks (GAT) <ref  r gives 6 JK-Net variants. We follow exactly the same setting of GraphSAGE as in the original paper <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>, where the model consists of 2 hidden la  the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>. As a result, this graph is more regular. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stimates between-image labels using the clustering technique <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> or kNN-based methods . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eins of greatest interest for unsupervised structure prediction are likely to have lower depth MSAs <ref type=\"bibr\" target=\"#b41\">(Tetchner et al., 2014)</ref>. This is especially a problem for highe 14)</ref>. This is especially a problem for higher organisms, where there are fewer related genomes <ref type=\"bibr\" target=\"#b41\">(Tetchner et al., 2014)</ref>. The hope is that for low-depth MSAs, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on LR images directly and progressively reconstruct the sub-band residuals of HR images. Tai et al. <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed deep recursive residual network (DRRN) to address the estimated high-quality patch with the same resolution as the input low-quality patch. We follow <ref type=\"bibr\" target=\"#b33\">[34]</ref> to do data augmentation. For each task, we train a single . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to use specialized hardware to accelerate these operations <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar  its generality.</p><p>The most relevant works to ours are <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar =\"bibr\" target=\"#b80\">81]</ref>) or require multiple instances to support different data structures <ref type=\"bibr\" target=\"#b44\">[45]</ref>, all of which lack generality and efficiency. Second, many ware. Previous works only focus on accelerating a specific operation on a particular data structure <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b78\">79,</ref><ref type=\"bibr\" ta  how should the accelerator be integrated into the CPU?</p><p>We first consider an intuitive scheme <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" ta or takes such a hybrid scheme and balances every aspect of a design.</p><p>Fully integrated designs <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fore, in self-supervised tasks, the ground truth labels are not available. Motivated by DeepCluster <ref type=\"bibr\" target=\"#b39\">[40]</ref> which uses the cluster assignments of learned features as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: training makes the model easily overfit. These problems make the transfer-based methods inefficient <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, the ada r module was proposed for parameter-efficient fine-tuning in multilingual or cross-lingual settings <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, which can mitiga stigate the performance of multiple adapters on cross-lingual ASR tasks.</p><p>In our previous work <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we proposed MetaAdapter to learn general and transferable s tive improvement. This paper is substantially an extended version of our previously published paper <ref type=\"bibr\" target=\"#b8\">[9]</ref> at ICASSP 2021. Compared to the previous version, we make he  to find a proper initialization for rapid adaptation have also been explored for cross-lingual ASR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Hsu et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> proposed ages to learn language-agnostic information in the multilingual data. On the other hand, Hou et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> investigates the possibility of applying adapters to cross-l lingual transfer learning tasks as shown in previous works <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D. e=\"bibr\" target=\"#b42\">[43]</ref> for fast adaptation to the new target tasks. In our previous work <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we investigated two meta-learning algorithms: Model-Agnosti pter-based ASR cross-lingual adaptation.</p><p>In the first stage, different from the previous work <ref type=\"bibr\" target=\"#b8\">[9]</ref>, SimAdapter trains the language-specific heads for each sour. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: elated evident paragraphs, and finally give a classification <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In academic field, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the dependency path between the entities might be even more indicative of the relation, as noted by <ref type=\"bibr\" target=\"#b40\">Toutanova et al. (2015)</ref>. It is quite possible that using these . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  true. In particular we demonstrate that the recently proposed AutoAugment data augmentation policy <ref type=\"bibr\" target=\"#b5\">[6]</ref> achieves state-of-the-art results on the CIFAR-10-C benchmar mentation strategies. Towards this end, we investigated the learned augmentation policy AutoAugment <ref type=\"bibr\" target=\"#b5\">[6]</ref>. AutoAugment applies a learned mixture of image transformati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: form single image super-resolution (SISR), and significant improvements over shallow CNN structures <ref type=\"bibr\" target=\"#b1\">[2]</ref> have been observed. One benefit from using deeper networks i [4]</ref>, random forest <ref type=\"bibr\" target=\"#b19\">[20]</ref> and convolutional neural network <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Among them, the CNN-based approaches <ref type=\"bibr\" targe ef> have recently set state of the art for SISR. A network with three layers was first developed in <ref type=\"bibr\" target=\"#b1\">[2]</ref> to learn an end-to-end mapping for SR. Subsequently, a deep  tei-c.org/ns/1.0\"><head n=\"3.2.\">Deconvolution layers</head><p>In previous SR methods such as SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> and VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, bicubic  using other SISR methods, including bicubic, Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref>, SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> and DRCN <re datasets. On average, an increase of about 1.0 dB using the proposed method was achieved over SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR < llowing us to train very deep Dataset Bicubic Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref> SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> DRCN <ref typ formation and gradient through the network, making it easy to train. In addition, in previous works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only high-level fea  also to improve the reconstruction performance. Instead of using interpolation for upscaling as in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, recent studies <ref different types of network structures were studied and compared in our work. As in previous methods <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only the feature ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b28\">[29]</ref>, advertising <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and Web search <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Its goal is to provide users with personalized informatio  = {e (\ud835\udc59)</p><formula xml:id=\"formula_9\">\ud835\udc63 \ud835\udc5a } of size \ud835\udc40 \u00d7 (\ud835\udc3f + 1).</formula><p>A very recent study <ref type=\"bibr\" target=\"#b16\">[17]</ref> suggests that recommendation models usually operate on an   studies have attempted to design new sampling distributions for prioritizing informative negatives <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta bedding e \ud835\udc63 \u2212 in Figure <ref type=\"figure\" target=\"#fig_1\">2</ref>, we first follows the convention <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> to select \ud835\udc40 negati \" target=\"#b51\">[52]</ref>: Dynamic negative sampling (DNS) strategy is the stateof-the-art sampler <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, which adaptively  . \u2022 DNS performs as the strongest baseline in most cases, which is consistent with previous studies <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta s to improve the optimization of general recommender systems <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar /1.0\"><head>GAN-based Sampler</head><p>Hard Negative Sampler <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hod for a special type of graphs (i.e., knowledge graph).</p><p>Our method also connects to PinSage <ref type=\"bibr\" target=\"#b20\">[21]</ref> and GAT <ref type=\"bibr\" target=\"#b14\">[15]</ref>. But not. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t al., 2017)</ref>, with the advantage that specialized GPU kernels are not required. We found that <ref type=\"bibr\" target=\"#b43\">Sun et al. (2020)</ref> had independently developed similar procedure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urbations to the input node features with graph structures unchanged. FLAG leverages \"free\" methods <ref type=\"bibr\" target=\"#b27\">(Shafahi et al., 2019)</ref> to conduct efficient adversarial trainin de feature space.</p><p>Augmentation for \"free\". We leverage the \"free\" adversarial training method <ref type=\"bibr\" target=\"#b27\">(Shafahi et al., 2019)</ref> to craft adversarial data augmentations.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: input and the output proximity. A different type of constraint, a structural constraint, is used in <ref type=\"bibr\" target=\"#b2\">Balakrishnan et al. (2019)</ref> to maintain a valid tree structure. O. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oped for classical ASR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target= nd decoder models using recurrent models with LSTMs <ref type=\"bibr\" target=\"#b5\">[6]</ref> or GRUs <ref type=\"bibr\" target=\"#b3\">[4]</ref>. However, their use of hierarchy in the encoders demonstrate eep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Our best model achieves a WER of 10.53% where our baseline  oder depth of the baseline model without using any convolutional layers. Our baseline model follows <ref type=\"bibr\" target=\"#b3\">[4]</ref> using the skip connection technique in its time reduction. T btained 10.5% WER without a language model, an 8.5% absolute improvement over published best result <ref type=\"bibr\" target=\"#b3\">[4]</ref>. While we demonstrated our results only on the seq2seq task,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: variety of relevance match signals and shows strong performance in various ad-hoc retrieval dataset <ref type=\"bibr\" target=\"#b3\">(Dai and Callan, 2019)</ref>. Recent research also has shown kernels c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sed for assessment in the educational contexts of games, to predict outcomes based on game activity <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and to predict responses to questions of various skills giv isite courses for other courses in the same department as the target course as a filter before step <ref type=\"bibr\" target=\"#b1\">(2)</ref>. For example, assuming that Table <ref type=\"table\" target=\"  rigorousness of evaluation, we applied several filters to the enumerated input courses before step <ref type=\"bibr\" target=\"#b1\">(2)</ref>, which are listed as follows, with the first two filters bei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>, into account. For instance, GraphRec <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch contain knowledge representative of the path as a whole. PLDA+, a scalable implementation of LDA <ref type=\"bibr\" target=\"#b24\">[28]</ref>, allows us to quickly nd topic models in these clouds. Unl truction process, all share common topics. We perform topic modeling on these documents using PLDA+ <ref type=\"bibr\" target=\"#b24\">[28]</ref>. e result is a set of plain text topics which represent di s and PLDA+ is a scalable implementation of this algorithm <ref type=\"bibr\" target=\"#b16\">[20,</ref><ref type=\"bibr\" target=\"#b24\">28]</ref>. Developed by Zhiyuan Liu et al., PLDA+ quickly identi es g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ckled by ML models in different contexts, and our goal is to build on the recent ML efforts for MCG <ref type=\"bibr\" target=\"#b33\">[Mansimov et al., 2019</ref><ref type=\"bibr\" target=\"#b46\">, Simm and thods <ref type=\"bibr\" target=\"#b46\">[Simm and Hernandez-Lobato, 2020]</ref> or not captured at all <ref type=\"bibr\" target=\"#b33\">[Mansimov et al., 2019</ref>]. \u2022 It explicitly models and predicts es ref type=\"bibr\" target=\"#b13\">[Gilmer et al., 2017]</ref> and self-attention networks. Thus, unlike <ref type=\"bibr\" target=\"#b33\">[Mansimov et al., 2019]</ref>, we are not affected by MPNNs' pitfalls </ref>. Statistics and other details are in fig. <ref type=\"figure\" target=\"#fig_9\">10</ref> and in <ref type=\"bibr\" target=\"#b33\">Mansimov et al. [2019]</ref>. Datasets are preprocessed as described  =\"#fig_9\">10</ref>, but point the interested reader other resources describing these datasets, e.g. <ref type=\"bibr\" target=\"#b33\">Mansimov et al. [2019]</ref>, Axelrod and Gomez-Bombarelli [2020a], <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2.\">MODEL</head><p>Our sequence-to-sequence model is an encoder-decoder architecture with attention <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: notations, but lack the ability to generalize to other domains. In contrast, embedding-based models <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b;</ref><ref type=\"bibr\" target=\"#b13\">Hao et al., en executing logical queries on incomplete KBs. Our work follows the line of Embedding-based models <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b;</ref><ref type=\"bibr\" target=\"#b10\">Dong et al. ated to these three OOV relations during the test.</p><p>Several baselines are included here: Embed <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b)</ref> deals with factoid QA over KB by matching. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ering what causes deep networks to be fragile to adversarial examples and how to improve robustness <ref type=\"bibr\" target=\"#b45\">[47,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" ta t has been theoretically shown that decreasing the input dimensionality of data improves robustness <ref type=\"bibr\" target=\"#b45\">[47]</ref>. Adversarial training <ref type=\"bibr\" target=\"#b33\">[35]< ining performance on natural examples.</p><p>Using the first order vulnerability of neural networks <ref type=\"bibr\" target=\"#b45\">[47]</ref>, we theoretically show that increasing output dimensionali ethods can improve the model's robustness without compromising clean accuracy. Simon-Gabriel et al. <ref type=\"bibr\" target=\"#b45\">[47]</ref> conducted a theoretical analysis of the vulnerability of n ref>. We denote the multitask predictor as F and each individual task predictor as F c . Prior work <ref type=\"bibr\" target=\"#b45\">[47]</ref> showed that the norm of gradients captures the vulnerabili rial noise is imperceptible, i.e., r \u2192 0, we can approximate \u2206L with a first-order Taylor expansion <ref type=\"bibr\" target=\"#b45\">[47]</ref>.</p><p>Lemma 1. For a given neural network F that predicts l></formula><p>Remark 1. By increasing the number of output tasks M , the first order vulnerability <ref type=\"bibr\" target=\"#b45\">[47]</ref> of network decreases. In the ideal case, if the model has  s we add more tasks, the norm of the joint gradient decreases, indicating improvement to robustness <ref type=\"bibr\" target=\"#b45\">[47]</ref>. The only exception is the depth estimation task, which we e same dimension for baselines and ours during comparison because input dimension impacts robustness<ref type=\"bibr\" target=\"#b45\">[47]</ref>.</note> \t\t</body> \t\t<back>  \t\t\t<div type=\"acknowledgement\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  compared to DNNs <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Recently, very deep CNNs architectures <ref type=\"bibr\" target=\"#b13\">[14]</ref> have also been shown to be successful in ASR <ref type=\"bi R, recently there have been several advancements in the computer vision community on very deep CNNs <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that have not been TMs.</p><p>We are driven by same motivation that led to the success of very deep networks in vision <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and the whole graphs. With solid graph-based theoretic work <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, these methods exploi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ry is not assumed; for a much more thorough treatment with all the measure theoretical details, see <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> and <ref type=\"bibr\" target=\"#b1\">Da fies the mean number of events in a region conditional on the past. Here we use the notation * from <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> to remind ourselves that this densit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: be enforced for multi-modal pre-training. Thanks to the recent progress of self-supervised learning <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f state-of-the-art models.</p><p>Graph Neural Networks (GNNs) <ref type=\"bibr\">(Micheli, 2009;</ref><ref type=\"bibr\" target=\"#b16\">Scarselli et al., 2008)</ref> have recently become the standard tool   state of neighboring nodes. Thanks to layering <ref type=\"bibr\">(Micheli, 2009)</ref> or recursive <ref type=\"bibr\" target=\"#b16\">(Scarselli et al., 2008)</ref> schemes, these models propagate inform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">[19]</ref> , Semi-supervised multi-view maximum entropy discrimination <ref type=\"bibr\" target=\"#b19\">[20]</ref> , and so on.</p><p>It is worth noting that in recent years. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6\">Hu et al. (2017)</ref> propose to augment text data in an encoder-decoder manner. Very recently, <ref type=\"bibr\" target=\"#b1\">(Anaby-Tavor et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Kobayas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6)</ref> partially motivate the ranking-based variant throught the importance sampling viewpoint of <ref type=\"bibr\" target=\"#b0\">Bengio and Sen\u00e9cal (2008)</ref>. However there are two critical differ 0\">Bengio and Sen\u00e9cal (2008)</ref>. However there are two critical differences: 1) the algorithm of <ref type=\"bibr\" target=\"#b0\">Bengio and Sen\u00e9cal (2008)</ref> does not lead to the same objective L . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -sampling and down-sampling layers to provide an error feedback mechanism for each stage. Jo et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> introduced the dynamic upsampling filters for video super- cally generated from the arbitrary customized prior boxes. In the video super resolution, Jo et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a dynamic upsampling filters. The dynamic upsampl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -to-video generation <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> to text-to-video generation <ref type=\"bibr\" target=\"#b22\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: learn node representations by utilizing information from distant neighbors. GCNs and their variants <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b14\">Veli\u010dkovi sed node classification <ref type=\"bibr\">(Kipf &amp; Welling, 2017)</ref>, inductive node embedding <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>, link prediction <ref type=\"bibr\" targe >Implementation Details</head><p>Training with the CV estimator is similar as with the NS estimator <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>. Particularly, each iteration of the al r all the other multi-class datasets. The model is GCN for the former 4 datasets and GraphSAGE-mean <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref> for the latter 2 datasets, see Appendix orted by <ref type=\"bibr\" target=\"#b2\">Chen et al. (2018)</ref>, while their NS baseline, GraphSAGE <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>, does not implement the preprocessing t sets because of their slow convergence and the requirement to fit the entire dataset in GPU memory. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> make an initial attempt to develop stoch www.tei-c.org/ns/1.0\"><head n=\"2.3.\">Neighbor Sampling</head><p>To reduce the receptive field size, <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> propose a neighbor sampling (NS) algorit id=\"formula_8\">P (l) uv = n(u) D (l) P uv if v \u2208 n(l) (u)</formula><p>, and P (l) uv = 0 otherwise. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> propose to perform an approximate forwar D (l) needs to be large for NS, to keep comparable predictive performance with the exact algorithm. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> choose D (1) = 10 and D (2) = 25, and th r, Cora, PubMed and NELL from <ref type=\"bibr\">Kipf &amp; Welling (2017)</ref> and Reddit, PPI from <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref>, with the same train / validation / test ted to the task nor the model. Our algorithm is applicable to other models including GraphSAGE-mean <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a</ref>) and graph attention networks (GAT) <ref e most GCNs only have two graph convolution layers <ref type=\"bibr\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b4\">Hamilton et al., 2017a)</ref>, this gives a significant reduction of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ts of three convolutions, which just cause finite increasing in additional computational complexity <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The effectiveness of ACB has been verified in the fields  [17]</ref>. The effectiveness of ACB has been verified in the fields including image classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, image denoising <ref type=\"bibr\" target=\"#b17\">[18]</ref>  should be robust to rotation and renders consistent results in different rotations. As reported in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, different asymmetric convolutions are robust with differe  network.</p><p>Based on above-mentioned insight, we modify the asymmetric convolutions proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> and design an asymmetric convolution block (ACB) to captur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: as been supported by empirical studies focusing on different concrete measures of semantic distance <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b;</ref><ref type=\"bibr\" target=\"#b0\">Bryson et al. im-  plementation in KG exploration which has been empirically demonstrated by case or user studies <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b;</ref><ref type=\"bibr\" target=\"#b0\">Bryson et al. by a user study where a concrete measure of semantic distance was implemented based on entity types <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b]</ref>. By comparing pairs of answers, users pref owed depth of search to prevent large subgraphs (i.e., diam &gt; 2d) which are not favored by users <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b]</ref>.</p><p>T opt denotes the optimal answer fo v) min v \u2208V PageRank(v ) \uf8f6 \uf8f8 .<label>(17)</label></formula><p>For semantic distance sd, we followed <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b]</ref> to compute the Jaccard distance between se e cohesive than standard GSTs. It partially justified the advantage of cohesive answers reported in <ref type=\"bibr\" target=\"#b3\">[Cheng et al., 2017b]</ref>. Similar to our efficiency experiment, in  n of cohesive answers, which is not considered in <ref type=\"bibr\">[Cheng and Kharlamov, 2017;</ref><ref type=\"bibr\" target=\"#b3\">Cheng et al., 2017b]</ref>. Our algorithm computes an optimum solution n Algorithm 1. Following common practice <ref type=\"bibr\" target=\"#b9\">[Kacholia et al., 2005;</ref><ref type=\"bibr\" target=\"#b3\">Cheng et al., 2017b]</ref>, we introduce an extra input d representing. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: st and transferable. Recently, IB has been applied to GNNs <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>. But IB needs the knowledge of the downstream tasks that ma tion bottleneck has applied to learn graph representations <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>. Specifically, the objective of graph information bottlenec IB robust to adverserial attack and strongly transferrable <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>.</p><p>Unfortunately, GIB requires the knowledge of the cla. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  also be oriented towards recommended items <ref type=\"bibr\" target=\"#b9\">(Celis et al., 2017;</ref><ref type=\"bibr\" target=\"#b5\">Biega et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Geyik et al.,  zed versions of the criterion and other variants considering constraint averages over user/contexts <ref type=\"bibr\" target=\"#b5\">(Biega et al., 2018;</ref><ref type=\"bibr\" target=\"#b37\">Patro et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" target=\"#b35\">[36]</ref><ref type=\"bibr\" t erage GNNs in user-item bipartite graph to CF. KGAT <ref type=\"bibr\" target=\"#b35\">[36]</ref>, KGCN <ref type=\"bibr\" target=\"#b34\">[35]</ref> and KGCN-LS <ref type=\"bibr\" target=\"#b33\">[34]</ref> Full. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utions can automatically absorb the effects of such unexpected adversarial changes in the variances <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. As a result, using . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2005\" xml:id=\"foot_2\">; Wang &amp; Zhang, 2007;<ref type=\"bibr\" target=\"#b33\">Raghavan et al., 2007;</ref> Gleich &amp;   </note> \t\t</body> \t\t<back. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: problem of the loss of detailed information due to the use of downsampling operations, Sun and Wang <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a maximum fusion scheme, which aims to improve th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: allenge to process massive remote sensing images. In our work, two lightweight attention mechanisms <ref type=\"bibr\" target=\"#b16\">[17]</ref> which contains spatial attention and channel attention are ts a convolution operation with 7 \u00d7 7 kernel size.</p><p>In this study, we follow the method of Woo <ref type=\"bibr\" target=\"#b16\">[17]</ref>to integrate the two attention mechanisms. First, we use ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lso are sensitive to small, worst-case perturbations of the input, so-called \"adversarial examples\" <ref type=\"bibr\" target=\"#b32\">(Szegedy et al., 2014)</ref>. This latter phenomenon has struck many   et al., 2004;</ref><ref type=\"bibr\" target=\"#b3\">Biggio &amp; Roli, 2018)</ref>. Since the work of <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref>, a subfield has focused specifically on t ly on the phenomenon of small adversarial perturbations of the input, or \"adversarial examples.\" In <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref> it was proposed these adversarial example mproved robustness to small perturbations.</p><p>In the introduction we referred to a question from <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref> about why we find errors so close to our  lem for every point in the test set <ref type=\"bibr\" target=\"#b23\">(Katz et al., 2017)</ref>. Since <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref>, hundreds of adversarial defense papers h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <head>Experiment Dataset</head><p>We train the PSSM-Distil framework on the training set of CullPDB <ref type=\"bibr\" target=\"#b30\">(Wang and Dunbrack Jr 2003)</ref>. CullPDB validation set, CB513 <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly discussed in one-class collaborative filtering (OCCF) <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Given user u, I + u = {i \u2208 I|r ui = 1} is the set of item imilarity matrices between users and items to predict drug-target interaction. Moreover, Yao et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed dual regularization by combining the weighted-and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vskaya et al., 2014)</ref> uses a classifier-based approach, which is improved by the latter system <ref type=\"bibr\" target=\"#b44\">(Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based \"bibr\" target=\"#b50\">(Susanto et al., 2014;</ref><ref type=\"bibr\">Chollampatt et al., 2016b,a;</ref><ref type=\"bibr\" target=\"#b44\">Rozovskaya and Roth, 2016;</ref><ref type=\"bibr\" target=\"#b26\">Junczy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  proposed software-based schedulers within a GPU to schedule general workload. For example, Juggler <ref type=\"bibr\" target=\"#b17\">[22]</ref> proposes a framework to dynamically execute a job represen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d learning (SSL) or collective classification <ref type=\"bibr\" target=\"#b27\">(Sen et al. 2008;</ref><ref type=\"bibr\" target=\"#b18\">McDowell, Gupta, and Aha 2007;</ref><ref type=\"bibr\" target=\"#b24\">Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  at the instruction level (to ease functional unit pressure) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, or even at functional levels <ref type=\"bibr\" target=\"#b4\">[ g/ns/1.0\"><head>A. Identifying necessary inputs</head><p>While dataflow analysis techniques such as <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> traverse through the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Speech animation for rigged models. Several related methods produce animation curves for speech <ref type=\"bibr\" target=\"#b12\">[Edwards et al. 2016;</ref><ref type=\"bibr\" target=\"#b59\">Taylor et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: preliminary work of a neural influence Diff usion Network (i.e., DiffNet) for social recommendation <ref type=\"bibr\" target=\"#b36\">[37]</ref>. DiffNet models the recursive social diffusion process for \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>.</p><p>In fact, as users play a central role in social pla \">[45]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. On one hand, given the useritem interest graph, NGCF is p  that the higher-order social structure is directly modeled in the recursive user embedding process <ref type=\"bibr\" target=\"#b36\">[37]</ref>. These graph based models showed superior performance comp ary, our main contributions are listed as follows:</p><p>\u2022 Compared to our previous work of DiffNet <ref type=\"bibr\" target=\"#b36\">[37]</ref>, we revisit the social recommendation problem as predictin d social recommendation model, DiffNet, for modeling the social diffusion process in recommendation <ref type=\"bibr\" target=\"#b36\">[37]</ref>. DiffNet advances classical embedding based models with ca at the up to K-th order social network structure is injected into the social recommendation process <ref type=\"bibr\" target=\"#b36\">[37]</ref>. In this part, we propose DiffNet++, an enhanced model of  t, in order to transform this model for the recommendation task. For our proposed models of DiffNet <ref type=\"bibr\" target=\"#b36\">[37]</ref> and DiffNet++, since both models are flexible and could be 8]</ref> and Normalized Discounted Cummulative Gain (NDCG) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Specifically, HR measures the percentage of hit items in  formance with large itemset, similar as many other works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, to evaluate the performance, for each user, we randomly s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [8]</ref>. However, for a thorough evaluation, we make two changes in the setup. 1) we add LightGCN <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which is the state-of-the-art top-\ud835\udc41 recommendation method,  odel that combines MF and Multi-Layer Perceptron (MLP) to learn the user-item interaction.\u2022 LightGCN<ref type=\"bibr\" target=\"#b2\">[3]</ref>: The state-of-the-art model which adopts simplified Graph Co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  model for inferring the underlying users' preferences encoded in the implicit user feedbacks. VECF <ref type=\"bibr\" target=\"#b3\">[4]</ref> modeled users' various attentions on different image regions. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rted by any of the mainstream toolchains. Some progress in this direction has been made in academia <ref type=\"bibr\" target=\"#b26\">[27]</ref>, but with significant performance penalties and implementa get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">17,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d n=\"1\">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type=\"bibr\" target=\"#b9\">[Liu et al., 2011;</ref><ref type=\"bibr\" target=\"#b8\">Lian et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \u0125 +1 i</formula><p>, \u0125 +1 i denote intermediate representations, and Norm can either be Layer-Norm <ref type=\"bibr\" target=\"#b1\">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n be prone to discrimination and instability <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b2\">, Bose and H get=\"#b13\">, Geisler et al., 2020</ref><ref type=\"bibr\" target=\"#b2\">, Bose and Hamilton, 2019</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b38\">, Zhu et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .0\"><head n=\"4.1\">Fast Fibonacci compression</head><p>In this method, we apply the Fibonacci coding <ref type=\"bibr\" target=\"#b1\">[2]</ref> which uses the Fibonacci numbers; 1, 2, 3, 5, 8, 13, . . . .. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t, thus it's effective to learn information from their interactions, such as low-order interactions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> or high-order interact. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns in a graph <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>. We adopt ProNE <ref type=\"bibr\" target=\"#b48\">[49]</ref>  ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>. We adopt ProNE <ref type=\"bibr\" target=\"#b48\">[49]</ref> to pretrain user embeddings in the friendship network, due e j-th eigenvalue \u03bb j is, the better partition effect it would achieve for dividing into j clusters <ref type=\"bibr\" target=\"#b48\">[49]</ref>. Thus we employ a graph filter g to adjust the eigenvalues tion.</p><p>To avoid explicit eigendecomposition and Fourier transform, we use the same trick as in <ref type=\"bibr\" target=\"#b48\">[49]</ref> to approximate g with a Chebyshev expansion and Bessel fun mpled ego network to 32. For pre-trained user embeddings, we generate 64-dim embeddings using ProNE <ref type=\"bibr\" target=\"#b48\">[49]</ref>. In the user feature smoothing method via graph filter, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nalysis to prove that our transductive model is a more general form than existing models (e.g., MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>). \u2022 Efficient and scalable learning algorithms for GATNE h beds networks with multiple views in a single collaborated embedding using attention mechanism. MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref> uses one common embedding and several additional embedding  r \u2208 R s\u00d7d is a trainable transformation matrix.</p><p>Connection with Previous Work. We choose MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>, a recent representative work for MHEN, as the base model   PMNE <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MVE <ref type=\"bibr\" target=\"#b29\">[30]</ref>, MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>. We denote the three methods of PMNE as PMNE(n), PMNE(r) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lculation.</p><p>Input: PSL Rules R, Prediction \u0177i , and Probability P(y|s i ), i = {1, 2, 3}; BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref>, to derive the sentence representation v i etails</head><p>In the framework of CTRL-PG, any contextualized word embedding method, such as BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref>, ELMo <ref type=\"bibr\" target=\"#b41\">(Pete nd RoBERTa <ref type=\"bibr\" target=\"#b37\">(Liu et al. 2019b)</ref>, can be utilized. We choose BERT <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to derive contextualized sentence embeddin ch tokenized sequence and learns an embedding vector for it. We follow the experimental settings in <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to use 12 Transformer layers and attention #b29\">(Kingma and Ba 2014)</ref> to optimize the parameters. We follow the experimental settings in <ref type=\"bibr\" target=\"#b17\">(Devlin et al. 2018)</ref> to set the dropout rate, and batch size as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and language modeling <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. They assume that prior knowledge like noise rate or noisy . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ta-learning from the perspective of metric learning <ref type=\"bibr\" target=\"#b14\">(Koch 2015;</ref><ref type=\"bibr\" target=\"#b44\">Vinyals et al. 2016;</ref><ref type=\"bibr\" target=\"#b36\">Sung et al. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch that it covers a predefined set of downsampling kernels <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>; using DNNs to capture only a natural-image prior which is  adation model and assumes that the downsampling kernels belong to a certain set of Gaussian filters <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Another approach  estimation methods <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Finally, we wo f>.</p><p>Finally, we would like to highlight major differences be-tween this paper and the work in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, whose \"kernel correction\" approach may be misunderstood a >[13]</ref>, whose \"kernel correction\" approach may be misunderstood as our \"correction filter\". In <ref type=\"bibr\" target=\"#b12\">[13]</ref>, three different DNNs (super-resolver, kernel estimator, a ned existing DNN methods (other than SRMD <ref type=\"bibr\" target=\"#b37\">[38]</ref>) can be used in <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Secondly, their approach is restricted by the offline tra ur approach. Thirdly, the concepts of these works are very different: The (iterative) correction in <ref type=\"bibr\" target=\"#b12\">[13]</ref> modifies the estimated downsampling kernel, while our corr erformed with the official code of each method. Unfortunately, such code has not been available for <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  \"sticker-pasting\" setting: Shen et al. <ref type=\"bibr\" target=\"#b20\">[22]</ref> and Nguyen et al. <ref type=\"bibr\" target=\"#b17\">[19]</ref> proposed using a projector to project the adversarial pert the physicalworld attacks against face recognition systems <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b17\">19]</ref>. These attacks craft adversarial perturbation and then proj. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etc.</p><p>Early approaches usually leverage various hand-crafted representation (e.g., pixel value <ref type=\"bibr\" target=\"#b3\">[4]</ref>, HoG <ref type=\"bibr\" target=\"#b22\">[24]</ref>) for tracking. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e tests using a first order logic <ref type=\"bibr\" target=\"#b22\">(Immerman &amp; Lander, 1990;</ref><ref type=\"bibr\" target=\"#b3\">Barcel\u00f3 et al., 2019)</ref>. Consider two unlabeled and undirected gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atterns make the size of the model continuously increasing <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar required for the inference are also increasing accordingly <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar distillation (KD) in the computer vision field, a few work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have employed KD for RS to reduce the size of models while  eacher, and also has a lower latency due to its small size <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>The core idea behind this process is that the soft l sions from the teacher model, the state-of-the-art methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have achieved comparable or even better performance to the  p>However, there are still limitations in existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. First, the learning of the student is only guided by the t at Figure <ref type=\"figure\">1</ref>: The existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> distill the knowledge only based on the teacher's predictio of ranking orders among items. Unlike the existing methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> that distill the knowledge of an item at a time, RRD formul ation performance compared to the state-of-the-art methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. An unified framework. We propose a novel framework-DE-RRD- uge success of KD in the computer vision field, a few work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillati sting items, we adopt a ranking position importance scheme <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> that places more emphasis on the higher positions in the ra .1, 0.5, 1.0}. Following the notation of the previous work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, we call the student model trained without the help of the  recommendation list would have strong correlations to the items that the user has interacted before <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Also, the soft labels provide guidance for distinguishing e=\"bibr\" target=\"#b24\">25]</ref> have adopted KD to RS. A pioneer work is Ranking Distillation (RD) <ref type=\"bibr\" target=\"#b24\">[25]</ref> which applies KD for the ranking problem; Providing recomm items); the high-ranked items in the recommendation list would have strong correlations to the user <ref type=\"bibr\" target=\"#b24\">[25]</ref>. By using such additional supervisions from the teacher, t s. The proposed framework is compared with the following methods:</p><p>\u2022 Ranking Distillation (RD) <ref type=\"bibr\" target=\"#b24\">[25]</ref>: A KD method for recommender system that uses items with t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ly, a lot of works <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" t  reduced, the statistical fault injection (SFI) experiments are still time-consuming. SmartInjector <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposes an intelligent fault injection framework to ident nly one representative fault is selected to implement fault injection for each group. SmartInjector <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposes an intelligent fault injection framework to ident. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >(Simonyan &amp; Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b44\">Srivastava et al., 2015;</ref><ref type=\"bibr\" target=\"#b18\">He et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Huang et al., 20 et (left) <ref type=\"bibr\" target=\"#b34\">(LeCun et al., 1998)</ref> with a 110-layer ResNet (right) <ref type=\"bibr\" target=\"#b18\">(He et al., 2016)</ref> on the CIFAR-100 dataset. The top row shows t e normalization techniques have enabled the development of very deep architectures, such as ResNets <ref type=\"bibr\" target=\"#b18\">(He et al., 2016)</ref> and DenseNets <ref type=\"bibr\" target=\"#b22\"> he past few years.</p><p>It is now common to see networks with hundreds, if not thousands of layers <ref type=\"bibr\" target=\"#b18\">(He et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Huang et al., 2 ageNet models of 2015 all use an order of magnitude less weight decay than models of previous years <ref type=\"bibr\" target=\"#b18\">(He et al., 2016;</ref><ref type=\"bibr\" target=\"#b41\">Simonyan &amp; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, is that repetitive control flow graph traversals lead to r temporal streaming <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> from prefetching approaches that only retrieve a constant n y counting successful prefetches. Hence, like past designs <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, TIFS uses the next-best option, the Recent heuristic, as i target data accesses <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. TIFS adds three logical structures to the chip: a set of S cts the anatomy of the SVB. Our SVB design is adapted from <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. The SVB contains a small fully-associative buffer for temp arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. These prefetchers target primarily off-chip data reference  prefetcher to retrieve instruction-cache blocks ahead of the fetch unit for the rest of the stream <ref type=\"bibr\" target=\"#b36\">[37]</ref>.</p><p>Figure <ref type=\"figure\">5</ref> shows the cumulat  the L2 cache (see <ref type=\"bibr\">Section 5)</ref>.</p><p>The term temporal stream, introduced in <ref type=\"bibr\" target=\"#b36\">[37]</ref>, refers to extended sequences of data references that recu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >(Defferrard et al., 2016)</ref>, to the connected nodes features (GAT for graph attention network) <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and/or to edge features <ref type=\"bi upports can be written thanks to operations from L + 1 . C = A + \u00d7 diag(1) and C mlp = diag(1). GAT <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> can be expressed in Eq.( <ref type=\"f in <ref type=\"bibr\" target=\"#b40\">(Xu et al., 2019)</ref>.</p><p>Graph attention networks (GATs) in <ref type=\"bibr\" target=\"#b37\">(Veli\u010dkovi\u0107 et al., 2018)</ref> proposes to transpose the attention m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>: These methods augment the source domain data to increase t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" tar e a novel face-focused cross-stream network (FFCSN). Different from the popular two-stream networks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targ d for action recognition in videos and has been popular for many human-centric video analysis tasks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Various improvement. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s/1.0\"><head n=\"2.\">PROPOSED APPROACH</head><p>We use a baseline Tacotron architecture specified in <ref type=\"bibr\" target=\"#b7\">[8]</ref>, where we use a GMM attention <ref type=\"bibr\" target=\"#b8\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ty of parameters contained in the Transformer-based models <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>- <ref type=\"bibr\" target=\"#b29\">[30]</ref>, recent literat type=\"bibr\" target=\"#b31\">[32]</ref> for parameter-efficient adaptation of pre-trained Transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> on various down tion <ref type=\"bibr\" target=\"#b36\">[37]</ref> of large-scale pre-trained language models like BERT <ref type=\"bibr\" target=\"#b27\">[28]</ref> and XLM <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Li et a o-sequence ASR task while they experiment on text classification tasks based on the pretrained BERT <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p><p>Some researchers have proposed to apply the Adapter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>. Human social interactions have also been modelled using other knowledge-based perspectives <ref type=\"bibr\" target=\"#b54\">[57,</ref><ref type=\"bibr\" target=\"#b46\">49,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ode, its associated edges' features will be used together with the node features in the aggregation <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b49\">49,</ref><ref type=\"bibr\" ta  we also find an interesting connection between using self-attention and the virtual node heuristic <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" ta section, various graph pooling functions are proposed to represent the graph embedding. Inspired by <ref type=\"bibr\" target=\"#b15\">[15]</ref>, in Graphormer, we add a special node called [VNode] to th /ref> and GIN <ref type=\"bibr\" target=\"#b49\">[49]</ref>, and their variants with virtual node (-VN) <ref type=\"bibr\" target=\"#b15\">[15]</ref>. They achieve the state-of-the-art valid and test mean abs rget=\"#b49\">[49]</ref> 3.8M 0.1203 0.1537 (0.1536*) GCN-VN <ref type=\"bibr\" target=\"#b26\">[26,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref> 4.9M 0.1225 0.1485 (0.1510*) GIN-VN <ref type=\"bibr\" target arget=\"#b15\">15]</ref> 4.9M 0.1225 0.1485 (0.1510*) GIN-VN <ref type=\"bibr\" target=\"#b49\">[49,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref> 6.7M 0.1150 0.1395 (0.1396*) GINE-VN <ref type=\"bibr\" targe target=\"#b15\">15]</ref> 6.7M 0.1150 0.1395 (0.1396*) GINE-VN <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref> 13.2M 0.1248 0.1430 DeeperGCN-VN <ref type=\"bibr\" target=\"# \" target=\"#b15\">15]</ref> 13.2M 0.1248 0.1430 DeeperGCN-VN <ref type=\"bibr\" target=\"#b30\">[30,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref> 25.5M 0.1059 0.1398 GT <ref type=\"bibr\" target=\"#b13\">[13]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: el classification for an input image, which is normally formulated as semantic segmentation problem <ref type=\"bibr\" target=\"#b53\">(Ronneberger et al., 2015;</ref><ref type=\"bibr\" target=\"#b1\">Badrina  The different types of layers are common in DCNNs developed for semantic segmentation. Please refer<ref type=\"bibr\" target=\"#b53\">(Ronneberger et al., 2015;</ref><ref type=\"bibr\" target=\"#b7\">Chai et the calculation efficiency. Among various DCNNs based on deep semantic segmentation networks, U-Net <ref type=\"bibr\" target=\"#b53\">(Ronneberger et al., 2015)</ref> performs well in segmenting differen head><p>The employed DCNN for semantic segmentation is an encoderdecoder network adapted from U-Net <ref type=\"bibr\" target=\"#b53\">(Ronneberger et al., 2015)</ref>. The architecture of the adapted U-N. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b0\">2,</ref><ref type=\"bibr\" target=\"#b1\">3,</ref><ref type=\"bibr\" target=\"#b2\">4,</ref><ref type=\"bibr\" target=\"#b3\">5]</ref>. This pipeline system usually suffers from time delay, parame nd generates target words from left to right at each step [1, <ref type=\"bibr\" target=\"#b1\">3,</ref><ref type=\"bibr\" target=\"#b3\">5]</ref>. This model has also achieved promising results in ASR fields chitecture for all three tasks (ASR, ST and MT). The model architecture is similar with Transformer <ref type=\"bibr\" target=\"#b3\">[5]</ref>, which is the state-of-art model in MT task. Recently, this . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b20\">(Gomez et al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Kitaev et al., 2019)</ref> and grouped convolutions <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y anomalous edges detection, is then highly needed before the data are fed into the following tasks <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Ranshous et a d model which is inspired by <ref type=\"bibr\" target=\"#b3\">[Liu et al., 2017]</ref> and proposed by <ref type=\"bibr\" target=\"#b1\">[Cui et al., 2017]</ref>. In our framework, we construct short state o l></formula><p>GRU is a variant of LSTM network. It is simpler and more effective than LSTM network <ref type=\"bibr\" target=\"#b1\">[Chung et al., 2014]</ref>. GRU can record long-term information, and  oss entropy to distinguish the existing edges and the generated ones. We then take the same idea in <ref type=\"bibr\" target=\"#b1\">[Bordes et al., 2013]</ref> and use marginbased pairwise loss in train ally build the required datasets because the ground-truth for the test phase is difficult to obtain <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed to give different weights to frame level speaker information. Finally, a statistic pooling layer <ref type=\"bibr\" target=\"#b29\">[30]</ref> is used to aggregate weighted frame level speaker informat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Recognizing emotion using RL <ref type=\"bibr\" target=\"#b45\">[45]</ref> and knowledge-based system <ref type=\"bibr\" target=\"#b46\">[46]</ref> are not new things actually. Liu et al. <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: supervised tasks, e.g., discriminating whether two subsequences come from the same user's behaviors <ref type=\"bibr\" target=\"#b35\">[36]</ref>. We further improve upon CLRec and propose Multi-CLRec, wh  the regular task where \ud835\udc65 is a sequence of clicks and \ud835\udc66 is the next click to be predicted. Task u2u <ref type=\"bibr\" target=\"#b35\">[36]</ref> adds an auxiliary loss where \ud835\udc65 and \ud835\udc66 are both sequences fr /div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B.4 Complex Pretext Tasks</head><p>In task u2u <ref type=\"bibr\" target=\"#b35\">[36]</ref>, \ud835\udc65 and \ud835\udc66 are both sequences from the same user, before and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  attention to decide when to stop and then performing soft attention to calculate, also rather than <ref type=\"bibr\" target=\"#b12\">[13]</ref> which needs a CTC trained model to conduct pre-partition b  shows a clear performance advantage than other soft and monotonic models (e.g. triggered attention <ref type=\"bibr\" target=\"#b12\">[13]</ref>), but also matches or surpasses most of the published resu int CTC-attention model / ESPNet <ref type=\"bibr\" target=\"#b15\">[16]</ref> 27.4 Triggered Attention <ref type=\"bibr\" target=\"#b12\">[13]</ref> 30 where the membrane potential Um is constantly simulated. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -item interaction graphs to model user preferences specific to each modality. Following MMGCN, GRCN <ref type=\"bibr\" target=\"#b36\">[37]</ref>  Visual structure &lt; l a t e x i t s h a 1 _ b a s e 6 4 ecific representations to obtain the representations of users or items for prediction.</p><p>\u2022 GRCN <ref type=\"bibr\" target=\"#b36\">[37]</ref> is also one of the state-of-the-arts multimodal recommenda onstructed modal-specific graph and refine modal-specific representations for users and items. GRCN <ref type=\"bibr\" target=\"#b36\">[37]</ref> refined user-item interaction graph by identifying the fal 39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. MMGCN <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tence embeddings from BERT. <ref type=\"foot\" target=\"#foot_0\">1</ref> In practice, previous studies <ref type=\"bibr\" target=\"#b29\">(Reimers and Gurevych, 2019;</ref><ref type=\"bibr\" target=\"#b21\">Li e verse sentence-related tasks. Following <ref type=\"bibr\" target=\"#b10\">Conneau et al. (2017)</ref>, <ref type=\"bibr\" target=\"#b29\">Reimers and Gurevych (2019)</ref> (SBERT) propose to compute sentence  might not be desirable to derive sentence embeddings directly from BERT without fine-tuning. While <ref type=\"bibr\" target=\"#b29\">Reimers and Gurevych (2019)</ref> attempt to alleviate this problem w r multilingual datasets. We also employ RoBERTa <ref type=\"bibr\">(Liu et al., 2019)</ref> and SBERT <ref type=\"bibr\" target=\"#b29\">(Reimers and Gurevych, 2019)</ref> in some cases to evaluate the gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ys on contrastive learning, refer to <ref type=\"bibr\" target=\"#b20\">Le-Khac et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b18\">Jaiswal et al. (2020)</ref>.</p><p>Fine-tuning BERT with Supervision.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, intermediate layers <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, and relation-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s topic has attracted considerable attention in recent years <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target  to identify the same person in different camera views among a potentially huge number of imposters <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. At the same time, p  Laplacian matrix and D is the degree matrix with each element D ii = j S V (i, j). As discussed in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, minimizing the pairwise constraint will force the similar r  images of a person have a high probability of sharing the similar representation features in re-id <ref type=\"bibr\" target=\"#b8\">[9]</ref>, this will make early active learning schema more suitable f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ble to adversarial attacks both at test time (evasion) as well as training time (poisoning attacks) <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b10\">Dai et al., r, they focus on targeted attacks, i.e. attacks designed to change the prediction of a single node. <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> consider both test-time and training-time   algorithm for global attacks on (deep) node classification models at training time. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>, we explicitly tackle the bilevel optimiza gorithm achieved after training on the data (i.e., graph) modified by our algorithm. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b10\">Dai et  in undiscovered, adversarial attacks should be unnoticeable. To account for this, we largely follow <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>'s attacker capabilities. First, we impose   are very likely to be noticed; to prevent such large changes to the degree distribution, we employ <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>'s unnoticeability constraint on the degree  GCN) to evaluate the performance degradation due to the attack. We use the same surrogate model as <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>, which is a linearized two-layer graph con e diagonal matrix of the node degrees, and \u03b8 = {W } the set of learnable parameters. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> we do not linearize the output (softmax) l raining the model for T steps. We compare against this baseline in our experiments; as also done in <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>. However, unlike the meta-gradient, this a compare our meta-gradient approach as well as its approximations with various baselines and Nettack <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018)</ref>. DICE ('delete internally, connect extern l our experiments, we enforce the unnoticeability constraint on the degree distribution proposed by <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018)</ref>. In Fig. <ref type=\"figure\" target=\"#fig_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b19\">[20]</ref>, TIRAMISU <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Triton <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>In principle, accelerating primitives intends to op. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: exed by nodes of an arbitrary directed or undirected graph <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>. This choice is satisfying in the sense that, when the sig  \u2022 1 is norm 1, and A norm = 1 \u03bbmax A. Other norms could be used to define the total variation, see <ref type=\"bibr\" target=\"#b50\">[51]</ref> <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Using this, graph ustified theoretically that the frequency bases obtained from the shift operator tend to be ordered <ref type=\"bibr\" target=\"#b50\">[51]</ref>.</p><p>Up to this point, we have focused primarily on freq odel makes it possible to detect outliers or abnormal values by highpass filtering and thresholding <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b154\">[155]</ref>, or to build  f type=\"bibr\" target=\"#b4\">[5]</ref>, optimizing the prediction of unknown labels in classification <ref type=\"bibr\" target=\"#b50\">[51]</ref> or semisupervised learning problems <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Amazon-Video Games <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, and Pinterest <ref type=\"bibr\" target=\"#b7\">[8]</ref>. For ry is viewed as 1 only when rating equals 5. For Amazon-Video Games, we adopt the processed dataset <ref type=\"bibr\" target=\"#b36\">[37]</ref> which each user have at least 5 records. For Pinterest, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tp://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_2\"><head></head><label></label><figDesc>-patents<ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> is a dataset of ut. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m . On March 7 , 2012 , he was named one of five finalists for the Naismith Award , which is 0.064  <ref type=\"bibr\" target=\"#b0\">(Baevski and Auli, 2019;</ref><ref type=\"bibr\" target=\"#b19\">Radford e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and frequent subgraph mining (FSM) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. ASAP <ref type rerun it without restrictions. The set of restrictions is cor-rect if ans</cell></row></table><note><ref type=\"bibr\" target=\"#b23\">24</ref> Function no_conflict(perm, res set): 25 g \u2190 an empty directe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead n=\"3.2\">Tucker Decomposition</head><p>Tucker decomposition, named after Ledyard R.</p><p>Tucker <ref type=\"bibr\" target=\"#b26\">(Tucker, 1964)</ref>, decomposes a tensor into a set of matrices and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hods have been developed to fit the underlying Markov Random Field, including meanfield DCA (mfDCA) <ref type=\"bibr\" target=\"#b30\">(Morcos et al., 2011)</ref>, sparse inverse covariance (PSICOV) <ref  ethods have been proposed to extend contact prediction to structure prediction. For example, EVFold <ref type=\"bibr\" target=\"#b30\">(Marks et al., 2011)</ref> and DCAFold <ref type=\"bibr\" target=\"#b44\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: their desired topics or related publications, but it is hard for researchers to reason and word2vec <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Two critical methods in BERT, Masked Language Model and Ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b67\">68,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>, two typical face-specific priors: geometry priors and refe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raining graph to boost accuracy without affecting the inference graph, including auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</re nvergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers <ref type=\"bibr\" target=\"#b18\">[19]</ref>. However, auxiliary classifiers require specific new desig tei-c.org/ns/1.0\"><head n=\"3.1\">Generation of training graph</head><p>Similar to auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we add several new classifier heads into the original net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oth source side and target side. The base architecture of mCOLT is the state-of-the-art Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al. (2017b)</ref>). A little different from previous work. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b24\">[23]</ref> learns two unidirectional language models of forward and backward. BERT <ref type=\"bibr\" target=\"#b8\">[7]</ref> uses a bidirectional transformer encoder to predict the mask. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion problem.</p><p>Learning upscaling filters was briefly suggested in the footnote of Dong et.al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>. However, the importance of integrating it into the CNN as p eration was not fully recognised and the option not explored. Additionally, as noted by Dong et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are no efficient implementations of a convolution lay uate the power of the sub-pixel convolution layer by comparing against SRCNN's standard 9-1-5 model <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Here, we follow the approach in <ref type=\"bibr\" target=\"#b CNN's standard 9-1-5 model <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Here, we follow the approach in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, using relu as the activation function for our models in thi ard comparison with results from previous published results<ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>.</note> \t\t</body> \t\t<back> \t\t\t<div type=\"references\">  \t\t\t\t<l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ead n=\"4.\">Experiments</head><p>For the experiments, we follow the protocol used in previous papers <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta  (2) lifted structured embedding <ref type=\"bibr\" target=\"#b20\">[21]</ref>, (3) N-pairs metric loss <ref type=\"bibr\" target=\"#b19\">[20]</ref>, (4) clustering <ref type=\"bibr\" target=\"#b14\">[15]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ending against the adversarial samples have been analyzed in <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. As stated in our following Theorem 1, the network could be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cs. The proposal stage (e.g., Selective Search <ref type=\"bibr\" target=\"#b33\">[34]</ref>, EdgeBoxes <ref type=\"bibr\" target=\"#b36\">[37]</ref>, DeepMask <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Peters et al., 2018)</ref>, GPT <ref type=\"bibr\" target=\"#b39\">(Radford et al., 2018)</ref>, BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, XLM <ref type=\"bibr\" target=\"#b25\">(Lamp  with changes in data size or composition.</p><p>We present a replication study of BERT pretraining <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, which includes a careful evaluation of t cently published methods. We release our model, pretraining and fine-tuning code.</p><p>Setup: BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> takes as input a concatenation of two seg ive training formats:</p><p>\u2022 SEGMENT-PAIR+NSP: This follows the original input format used in BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, with the NSP loss. Each input has a pair d diverse corpora, such as the ones considered in this work.</p><p>The original BERT implementation <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>  Early experiments revealed only minor di SQUAD RESULTS</head><p>We adopt a much simpler approach for SQuAD compared to past work. While BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> and XLNet <ref type=\"bibr\" target=\"#b56\">  submit RoBERTa to the public SQuAD 2.0 leaderboard. Most of the top systems build upon either BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> or XLNet <ref type=\"bibr\" target=\"#b56\">( )</ref>. This formulation significantly simplifies the task, but is not directly comparable to BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>. Following recent work, we adopt the rank training with large batch sizes.</p><p>We pretrain with sequences of at most T = 512 tokens. Unlike <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>, we do not randomly inject short sequences b4\">(Bowman et al., 2015)</ref>, which require predicting relationships between pairs of sentences. <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> observe that removing NSP hurts performanc sults for the four different settings. We first compare the original SEGMENT-PAIR input format from <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> to the SENTENCE-PAIR format; both formats  that removing the NSP loss matches or slightly improves downstream task performance, in contrast to <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. It is possible that the original BERT imp  and end-task accuracy for BERT BASE as we increase the batch size, while tuning the learning rate. <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> originally trained BERT BASE for 1M steps  alf as many optimization steps, thus seeing four times as many sequences in pretraining compared to <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>.</p><p>To help disentangle the importance  ers). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. We pretrain our model using 1024 V100 GPU et=\"#b56\">Yang et al. (2019)</ref>.</p><p>For SQuAD v1.1 we follow the same finetuning procedure as <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. For SQuAD v2.0, we additionally classify  ead>Table 7 :</head><label>7</label><figDesc>Comparison between the published BERT BASE results from<ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> to our reimplementation with either static ranslation <ref type=\"bibr\" target=\"#b30\">(McCann et al., 2017)</ref>, and masked language modeling <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lample &amp  et al., 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Baevski et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: v-chain based methods <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref> assume that users' be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r training neural networks in the reinforcement learning setting, such as neural fitted Q-iteration <ref type=\"bibr\" target=\"#b18\">24</ref> , these methods involve the repeated training of networks de ters on each game, privy only to the inputs a human player would have. In contrast to previous work <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> , our approach incor e history and the action have been used as inputs to the neural network by some previous approaches <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> . The main drawback . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  complete KGs, extensive research efforts <ref type=\"bibr\" target=\"#b21\">(Nickel et al., 2011;</ref><ref type=\"bibr\" target=\"#b2\">Bordes et al., 2013</ref>  et <ref type=\"bibr\">al., 2014;</ref><ref ty t al., 2011)</ref> is one of the earlier work that models the relationship using tensor operations. <ref type=\"bibr\" target=\"#b2\">Bordes et al. (2013)</ref> proposed to model relationships in the 1-D  local connections in knowledge graph.</p><p>Although the entity embeddings from KG embedding models <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al.,  embedding-based methods: RESCAL <ref type=\"bibr\" target=\"#b21\">(Nickel et al., 2011)</ref>, TransE <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013)</ref>, Dist-Mult <ref type=\"bibr\" target=\"#b38\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by m orthonormal matrices. We refer the readers to <ref type=\"bibr\" target=\"#b24\">(Wong, 1967;</ref><ref type=\"bibr\" target=\"#b0\">Absil et al., 2004)</ref> for details on the Riemannian geometry of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: : we benchmark Cassandra 0.7.3 database with 30 million records. The request is generated by a YCSB <ref type=\"bibr\" target=\"#b14\">[15]</ref> client with a 50:50 ratio of read to update.</p><p>Media S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s w.r.t. hyperparameters, which have been widely used in solving bi-level problems in meta learning <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. To obtain meta-gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Note that Qiu et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref> use Random Walk with Restart (RWR) <ref type=\"bibr\" target=\"#b13\">[14]</ref> to generate sampled ego networks. However, information dif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b17\">Ha et al., 2017;</ref><ref type=\"bibr\" target=\"#b16\">Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Currey and Heafield, 2019)</ref>. <ref type=\"bibr\" target=\"#b20\">John. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: garbage collector in Java Virtual Machine (JVM) maintains the live objects in a tree data structure <ref type=\"bibr\" target=\"#b0\">[1]</ref>. A garbage collection event causes a traversal of the object n will trigger the idle CFA to issue memory requests for both the queried key and the starting node <ref type=\"bibr\" target=\"#b0\">( 1 )</ref>. Depending on the order in which the results of these two . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ations, arranging from recommender system <ref type=\"bibr\" target=\"#b34\">[34]</ref>, drug discovery <ref type=\"bibr\" target=\"#b39\">[39]</ref> to information retrieval <ref type=\"bibr\" target=\"#b43\">[4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p><p>Secondly, note imum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios<ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.Secondly, note that O n machine learning <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dhuber, 1997)</ref> system can readily outperform the previous state-of-the-art system, CogCompTime <ref type=\"bibr\" target=\"#b32\">(Ning et al., 2018d)</ref>, by a large margin. The fact that a standa ead><label>2</label><figDesc>Performances on the MATRES test set (i.e., the PT section). CogCompTime<ref type=\"bibr\" target=\"#b32\">(Ning et al., 2018d)</ref> is the previous state-of-the-art feature-b hat in Table <ref type=\"table\" target=\"#tab_2\">2</ref>, CogCompTime performed slightly different to <ref type=\"bibr\" target=\"#b32\">Ning et al. (2018d)</ref>: Cog-CompTime reportedly had F 1 =65.9 (Tab f type=\"table\" target=\"#tab_2\">2</ref> Line 3 therein) and here we obtained F 1 =66.6. In addition, <ref type=\"bibr\" target=\"#b32\">Ning et al. (2018d)</ref> only reported F 1 scores, while we also use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 112\">114]</ref>, we introduce a graph representation method based on the work of Lov\u00e1sz and Szegedy <ref type=\"bibr\" target=\"#b78\">[79]</ref> and Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"  a sampled graphon W \u223c P(W ), where W : [0, 1] 2 \u2192 [0, 1] is a random symmetric measurable function <ref type=\"bibr\" target=\"#b78\">[79]</ref> sampled (according to some distribution) from D W , the se  G te N te := (A te , X te ).</p><p>Our SCM has a direct connection with graphon random graph model <ref type=\"bibr\" target=\"#b78\">[79]</ref>, and extend it by considering vertex attributes. Now we in  subgraph densities (more precisely, induced homomorphism densities) in graphon random graph models <ref type=\"bibr\" target=\"#b78\">[79]</ref> to learn E-invariant representations for the SCM defined i a have different distributions. By introducing a structural causal model inspired by graphon models <ref type=\"bibr\" target=\"#b78\">[79]</ref>, we defined a representation that is approximately invaria e W is the graphon function as illustrated in Definitions 1 and 2. As defined in Lov\u00e1sz and Szegedy <ref type=\"bibr\" target=\"#b78\">[79]</ref>,</p><formula xml:id=\"formula_23\">t(F k , W ) = [0,1] k ij\u2208 ctation before we observe any vertices. B m is a martingale for unattributed graphs (Theorem 2.5 of <ref type=\"bibr\" target=\"#b78\">[79]</ref>). And since in Definitions 1 and 2 we also use the graphon 9]</ref> and their connection to graphon random graph models <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b78\">79]</ref>:</p><formula xml:id=\"formula_0\">Definition 1 (Training Grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b15\">Huang and Riloff, 2012;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b36\">Sha et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">Lin et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene <ref type=\"bibr\" target=\"#b12\">[13]</ref>. These complex pipelines are slow and hard to optimize bec  Then, classifiers <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> or localizers <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ple vFPGAs over the AXI interconnect. For example, we ported publicly-available TCP and RoCE stacks <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> to Coyote, and the h-performance multi-tenant network stack based on our open-source TCP/IP and RDMA engine for FP-GAs <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Like the memory s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t. (1) Different search space is utilized, e.g., range of macro skeletons of the whole architecture <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref> and a different bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, different regularization <ref type=\"bibr\" target=\"#b10\">[11]</ref>, different scheduler <ref type=\"bibr\" target=\"#b14\">[15]</ ge as inspired by <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. We summarize characteristics of our NATS-Bench and NAS-Be ed NAS algorithms <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. As shown in the middle part of Figure <ref type=\"figure\"  ed NAS algorithms <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Since all cells in an architecture have the same topology ure to set up the hyper-parameters and training strategies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. We train each . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [40,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> and QM9 <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>.</p><p>ZINC is a large dataset of commercially available dr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 20b)</ref>.</p><p>\u2022 Replacing the original residual blocks in DDPM with residual blocks from BigGAN <ref type=\"bibr\" target=\"#b4\">(Brock et al., 2018)</ref>. \u2022 Increasing the number of residual blocks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplications as diverse as online shopping <ref type=\"bibr\" target=\"#b52\">[53]</ref>, social network <ref type=\"bibr\" target=\"#b28\">[29]</ref>, advertising <ref type=\"bibr\" target=\"#b14\">[15]</ref>, an  as \ud835\udc53 uniform (\ud835\udc62)) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar evious works take the side information, e.g., social network <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and knowledge graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ich are \"unnoticeable\" to humans but can cause the learning models to misclassify some target nodes <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., >Limitations of Current Approaches. There are two common issues of existing works on attacking GCNs <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., ch puts a high demand on the scalability of the underlying attack models. However, existing efforts <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., e directly and then treat the newly added nodes as existing nodes and apply the attacks proposed in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Z\u00fcgner and  e attackers can benefit more by additionally manipulating features of nodes (shown in Section 5.4). <ref type=\"bibr\" target=\"#b34\">Z\u00fcgner et al. (2018)</ref> propose Nettack, which manipulates both ed ology</head><p>Different from previous works <ref type=\"bibr\" target=\"#b12\">(Dai et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Z\u00fcgner and G ation performs on the target node v 0 .</p><p>It is noteworthy that the surrogate model proposed in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> is typically used to generate perturbatio atures (e.g., mutually exclusive features), it will be easily detected. For this issue, the work in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> proposes a statistical test based on the  p>In this paper, we consider three methods designed for the traditional scenario, including Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcg tional scenario, including Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> and Metaattack <ref type=\"bibr\" target=\"# ection 3.2 which is designed specifically for the new attack scenario instead of the constraints in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>.</p><p>There are two strategies to adapt  d features. We call this strategy sequential injection.</p><p>Nettack for the new scenario. Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> addresses the bilevel optimization proble cores from the old scores after each iteration in constant time.</p><p>According to the analysis in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, the time complexity of Nettack in terms  ion of features, it needs O (\u2206 e n in f d) just like Nettack.</p><p>FGSM for the new scenario. FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> computes the gradients of L atk w.r.t. ed he detailed statistics of these datasets are shown in Table 1. Following the same attack setting in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, we only consider the largest connected c  one-time injection can be roughly treated as the special case of the attack scenario considered in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> as nodes are added in advance and perturb ous nodes directly because connections to the target node usually lead to better attack performance <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>. However, we do not know the optimal numb  FGSM performs better than Nettack in the new scenario which is quite different from the results in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>. We hypothesize that it may be due to the features. Considering that the results of target nodes with higher degrees are harder to be mislead <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, we attack the target nodes with d v 0 /2 the attack performance significantly. This is in contrast to the findings in the attack scenario of <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, where the authors observe that manip-ula res are not very important for successful attacks. Such contrast exists because, in the scenario of <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, the attacker can only perturb features o of vicious nodes to the target node, which is also consistent with our intuition and the results in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018</ref>) that (more) connections with target node . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n et al., 2017)</ref> or matrix factorization <ref type=\"bibr\" target=\"#b4\">(Cao et al., 2015;</ref><ref type=\"bibr\" target=\"#b51\">Wang et al., 2016)</ref> objectives, GNNs explicitly derive proximity. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eir original image size. The encoder part of FCN consists of visual geometry group network (VGGNet) <ref type=\"bibr\" target=\"#b25\">[26]</ref> that is a famous CNN classification model and the decoder . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ut regard to their distance in the input or output sequences <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In all but a few cases <ref type=\"bibr\" target=\"#b21\">[22]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: x instead of adjacency matrix <ref type=\"bibr\" target=\"#b15\">[16]</ref> and simplified model design <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Also, there has been a growing interest in making the mod able for large graph datasets with millions of nodes. We compare the accuracy of our model with SGC <ref type=\"bibr\" target=\"#b28\">[29]</ref>, Node2Vec <ref type=\"bibr\" target=\"#b10\">[11]</ref> and SI. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[36]</ref> (e) EDSR <ref type=\"bibr\" target=\"#b36\">[36]</ref> (f) DBPN <ref type=\"bibr\" target=\"#b20\">[20]</ref> (g) RDN <ref type=\"bibr\" target=\"#b6\">[6]</ref> (h) Ours</ with more than 16 layers based on residual learning. To further improve the performance, Lim et al. <ref type=\"bibr\" target=\"#b20\">[20]</ref> proposed a very deep and wide network EDSR by stacking mod  <ref type=\"bibr\" target=\"#b14\">[14]</ref>, Mem-Net <ref type=\"bibr\" target=\"#b30\">[30]</ref>, EDSR <ref type=\"bibr\" target=\"#b20\">[20]</ref>, SRMD <ref type=\"bibr\" target=\"#b36\">[36]</ref>, NLRN <ref module, and reconstruction part. Given I LR and I SR as the input and output of SAN. As explored in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>, we apply only one ndencies.</p><p>It has been verified that stacking residual blocks is helpful to form a deep CNN in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>. However, very dee filter are set as 3 \u00d7 3 and C =6 4 , respectively. For upscale part H \u2191 (\u2022), we follow the works in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref> and apply ESPCNN < ments</head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">Setup</head><p>Following <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" targ <ref type=\"bibr\" target=\"#b39\">[39]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" ta 30\">30]</ref>, L 1 <ref type=\"bibr\" target=\"#b14\">[14,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>, perceptual losses . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>.</p><p>The most common paradigm for CF is to learn latent f et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> that typically aggregate extended neighbors and need to han edding learning.</p><p>To deepen the use of subgraph structure with high-hop neighbors, Wang et al. <ref type=\"bibr\" target=\"#b38\">[39]</ref> recently proposes NGCF and achieves state-of-the-art perfo <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">PRELIMINARIES</head><p>We first introduce NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, a representative and state-of-the-art GCN model for recom  the scores of NGCF are directly copied from the Table <ref type=\"table\" target=\"#tab_4\">3</ref> of <ref type=\"bibr\" target=\"#b38\">[39]</ref>. As can be seen, removing feature transformation (i.e., NG >transformation and nonlinear activation. The graph convolution operation (a.k.a., propagation rule <ref type=\"bibr\" target=\"#b38\">[39]</ref>) in LightGCN is defined as:</p><formula xml:id=\"formula_5\" ms) that have overlap on interacted items (users), and higher-layers capture higher-order proximity <ref type=\"bibr\" target=\"#b38\">[39]</ref>. Thus combining them will make the representation more com TS</head><p>We first describe experimental settings, and then conduct detailed comparison with NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, the method that is most relevant with LightGCN but more c e experiment workload and keep the comparison fair, we closely follow the settings of the NGCF work <ref type=\"bibr\" target=\"#b38\">[39]</ref>. We request the experimented datasets (including train/tes ation is that increasing the layer number from 0 (i.e., the matrix factorization model, results see <ref type=\"bibr\" target=\"#b38\">[39]</ref>) to 1 leads to the largest performance gain, and  using a  \" target=\"#b26\">27]</ref>. Motivated by the strength of graph convolution, recent efforts like NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GC-MC <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and PinS ifferent layers. The scores of NGCF on Gowalla and Amazon-Book are directly copied from the Table3of<ref type=\"bibr\" target=\"#b38\">[39]</ref>; the scores of NGCF on Yelp2018 are re-run by us.</figDesc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [14]</ref> alone. More importantly, when pre-trained on large-scale weakly labeled JFT-300M dataset <ref type=\"bibr\" target=\"#b14\">[15]</ref>, ViT achieves comparable results to state-of-the-art (SOTA  dataset JFT-3B<ref type=\"bibr\" target=\"#b25\">[26]</ref> for pre-training, while others use JFT-300M<ref type=\"bibr\" target=\"#b14\">[15]</ref>. See Appendix A.2 for the size details of CoAtNet-5/6/7. \u2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ted numerical cross-checking systems. There are some related systems developed, such as ClaimBuster <ref type=\"bibr\" target=\"#b4\">[5]</ref> and StatCheck <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Cla ade by public figures. Verifying such claims includes detecting whether a statement in check-worthy <ref type=\"bibr\" target=\"#b4\">[5]</ref>, retrieve information from large data source  to provide rel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  consistent results for graph classification (e.g. 2.5 percentage points with GCN on the DD dataset <ref type=\"bibr\" target=\"#b17\">[18]</ref>). We found no improvement for the inductive  <ref type=\"fi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nable parameters, and R (0) = X. We call our method with MLP-based estimator CPGNN-MLP. \u2022 GCN-Cheby <ref type=\"bibr\" target=\"#b3\">(Defferrard, Bresson, and Vandergheynst 2016)</ref>. We instantiate th pervised node classification problems thanks to their ability to learn through end-to-end training. <ref type=\"bibr\" target=\"#b3\">Defferrard, Bresson, and Vandergheynst (2016)</ref> proposed an early . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ccuracies, limitations and pitfalls of the related technique known as negotiated-congestion routing <ref type=\"bibr\" target=\"#b27\">[28]</ref>. In Copyright (c) 2008 IEEE. Personal use of this material 5\">[6]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bib ested regions, often at the cost of increased wirelength.</p><p>Negotiated-congestion Routing (NCR) <ref type=\"bibr\" target=\"#b27\">[28]</ref> was introduced in the mid-1990s for global routing in FPGA  (b e ), added cost reflecting congestion history (h e ), and penalty for current congestion (p e ) <ref type=\"bibr\" target=\"#b27\">[28]</ref>. NCR seeks to minimize e c e .</p><p>To begin negotiated-c re-route is the same for each iteration, but can be chosen arbitrarily, according to the authors of <ref type=\"bibr\" target=\"#b27\">[28]</ref>, because the gradual cost increase in congested areas remo =\"formula_7\">c e = b e + h e \u2022 p e<label>(8)</label></formula><p>which is different than Equation 1 <ref type=\"bibr\" target=\"#b27\">[28]</ref>, but also is more intuitive since it preserves the base co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b0\">, AlQuraishi, 2019</ref><ref type=\"bibr\" target=\"#b35\">, No\u00e9 et al., 2019</ref><ref type=\"bibr\" target=\"#b44\">, Senior et al., 2020]</ref>, where crystallized 3D structures are pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: an performs better than the ASR model, which was trained from scratch by only German data.</p><p>In <ref type=\"bibr\" target=\"#b21\">[22]</ref>, a multilingual ASR model with 10 BABEL languages from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" target=\"#b32\">36,</ref><ref type=\"bibr\" target=\"#b64\">70,</ref><ref type=\"bibr\" target=\"#b63\">69,</ref><ref type=\"bibr\" tar get=\"#b36\">40,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" target=\"#b32\">36,</ref><ref type=\"bibr\" target=\"#b64\">70,</ref><ref type=\"bibr\" target=\"#b63\">69,</ref><ref type=\"bibr\" tar and how chains share packet buffers. Similar to prior work <ref type=\"bibr\" target=\"#b62\">[68,</ref><ref type=\"bibr\" target=\"#b64\">70]</ref>, packet scheduling policies in PANIC are programmable. Furt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n be seen as a special form of low-pass filter <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b9\">Li et al. 2019)</ref>. Some recent studies <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: three comparatively simple U-Net models that contain only minor modifications to the original U-Net <ref type=\"bibr\" target=\"#b5\">[6]</ref>. We omit recently proposed extensions such as for example th  we focus our efforts on designing an automatic training pipeline for these models.</p><p>The U-Net <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a successful encoder-decoder network that has received a  tation framework for the medical domain that directly builds around the original U-Net architecture <ref type=\"bibr\" target=\"#b5\">[6]</ref> and dynamically adapts itself to the specifics of any given . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: udy of the distinguishing power of some GNN variants has been initiated. In two independent studies <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., ee-aware MPNNs that do use degree information. The former class of MPNNs covers the GNNs studied in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., s bounded by the WL algorithm. This result can be seen as a slight generalisation of the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al.,  and degree-aware MPNNs matches that of the WL algorithm.</p><p>For anonymous MPNNs related to GNNs <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., or short) is well understood. Indeed, as we will shortly see, it follows from two independent works <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., L , as is indicated in Figure <ref type=\"figure\" target=\"#fig_1\">1</ref>. Proposition 5.2 (Based on <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., ymous MPNN by using an injection h : A s \u2192 Q. What follows is in fact an adaptation of Lemma 5 from <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> itself based on <ref type=\"bibr\">[Zaheer et a gue that M anon is weaker than M WL . The proof is a trivial adaptation of the proofs of Lemma 2 in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=\"bibr\" target=\"#b1 t) w = (\u2113 \u2113 \u2113 (t) M ) w ,</formula><p>as desired.</p><p>We remark that we cannot use the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et x because the class M anon is more general than the class considered in those papers. The proofs in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et can be written in the form g (t) u\u2208NG(v) h (t) (\u2113 \u2113 \u2113 (t\u22121) u</formula><p>) , based on Lemma 5 from <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref>.</p><p>Suppose that \u03bd \u03bd \u03bd : V \u2192 A s0 . It now. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: of the current work on attacking graph neural networks has concentrated on node classification task <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bib luence attack is limited to a few attacker nodes (usually the neighboring nodes of the target node) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Figure <ref type=\"figure\">1</ref> demonstrates a toy exam s based on gradients of a surrogate model so as to fool the classifiers. In addition, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> propose Nettack, which is capable of perturbing the Fig. < ween nodes with high correlation and connecting edges with low correlation. Moreover, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> study both poisoning attacks and evasion attacks (a.k.a te ies of graphs and ensure the unnoticeability in most cases.</p><p>To bridge this gap, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> enforce the perturbations to ensure its unnoticeability by \"><head n=\"3.2.1\">Vanilla Graph Convolution Network (GCN)</head><p>Since a number of existing works <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bib =\"#b22\">[23]</ref> to modify the graph structure or node features, respectively. Following the work <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we assume that attackers have prior knowledge about the g on task, here we briefly introduce other proposed state-of-the-art targeted attack methods: Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref> and GradArgmax <ref type=\"bibr\" target=\"#b12\">[13]</ref>.<  model SGC, A \u2286 V is the set of attacker nodes and the perturbations are constrained to these nodes <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>In particular, we set A = {t} for direct attack and  q is given. However, there is no solution for estimating q exactly yet. To this end, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> derive an efficient way to check for violations of the deg ula_37\">\u039b (q, q ; G, G ) &lt; \u03c4 \u2248 0.004 ,</formula><p>where \u039b is a discriminate function defined in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, G denotes the original graph and G the perturbed one.</p> er posts and comments on content in different topical communities. We follow the setting of Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref> and only consider the largest connected component of the g ame surrogate model SGC (if necessary), and share the same weights \u03b8. Follow the setting of Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the attack budget \u2206 is set to the degrees of target node   instantiation of the adjacency matrix and computes the gradients of all N 2 edges.</p><p>\u2022 Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Nettack is the strongest baseline that can modify the gra nt attack methods. Nettack, a strong baseline, also yields a significant performance as reported in <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Most remarkably, even in attacking other robust graph neu /www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">. We follow the attack settings of Nettack<ref type=\"bibr\" target=\"#b11\">[12]</ref> which modifies graph structure and node features by flippi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oposed architectures <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Recent work has investigated GNN's ability to capture grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that the cost benefits brought by spot offerings can be realized with intuitive bidding strategies <ref type=\"bibr\" target=\"#b14\">[15]</ref>. However, choosing between spot instances and bid levels a e on-demand price since the user only pays the spot price anyway, as is commonly advocated (e.g. in <ref type=\"bibr\" target=\"#b14\">[15]</ref>) and used in practice. Under the given model, the WSD assu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tasks such as image classification <ref type=\"bibr\" target=\"#b47\">[48]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b22\">[23]</ref>, object detection <ref type=\"bibr\" target=\"#b3\">[4]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly work mostly deals with simple graphs with unlabeled edges, recently proposed relation-aware GNNs <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> consider multi-rel pe=\"bibr\" target=\"#b38\">[39]</ref> consider direction and relation types, respectively. Also, R-GCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> considers direction and relation types simultaneously. Rec e=\"bibr\" target=\"#b44\">45]</ref>. 5) R-GCN. This is a GNN-based method for modeling relational data <ref type=\"bibr\" target=\"#b37\">[38]</ref>. 6) MEAN. 7) LAN. These are GNN models for a out-of-knowle get=\"#b14\">[15]</ref>. 3) R-GCN. The same model used in the entity prediction on KG completion task <ref type=\"bibr\" target=\"#b37\">[38]</ref>. 4) I-GEN. Inductive GEN, which only uses feature represen nds the graph convolutional network to consider multi-relational structure, by Schlichtkrull et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p><p>6) MEAN. This model computes the embedding of entit s W r and W r to prevent the excessive increase in the model size, proposed in Schlichtkrull et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref>: W r = B b=1 a r b V b , where B is the number of basis, a twork based methods <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. While they require ggested by several recent works on multi-relational graphs <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, where directed rel ne in previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we measure the ranks in a filtered setting where we do not. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esults. Our work is partly inspired by the works on generating and refining score maps. Yang et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> adopts pyramid features as inputs of the network in the pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o-Encoder (CAE) to learn the joint representation using Denoising Auto-Encoder (DAE) style learning <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Fig. <ref type=\"figure\" target=\"#fig_4\">5</ref> shows a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: matrix structure, which simplifies the filter optimisation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref> and thus results in the use of more sophisticated features < eature extraction is crucial to advanced DCF training, Henriques et al. adopted HOG features in KCF <ref type=\"bibr\" target=\"#b1\">[2]</ref> and <ref type=\"bibr\">Danelljan et al.</ref> proposed to empl  \u03bb is a regularisation parameter. \u2022 F means Frobenius norm and is the circular convolution operator <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With the circulant structure <ref type=\"bibr\" target=\"#b36\" t=\"#b48\">[49]</ref>, the optimisation of Eqn. (1) can efficiently be solved in the frequency domain <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>To prevent their temporal degradation, the final filt e updated by utilising the same updating strategy as other DCF-based trackers, as presented in Eqn. <ref type=\"bibr\" target=\"#b1\">(2)</ref>. Additionally, similar to many other DCF-based trackers, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rching for the best candidate based on their cost models <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bib s:</head><p>Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> Brute-force &amp; Random dMazeRunner <ref type=\"bibr\" target=\"#b22\">[23]</ref> Brute-force Triton <ref type=\"bibr\" target=\"#b64\">[65]</re <head>Inputs Weights Outputs</head><p>Input Buffer space <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >12]</ref>.</p><p>Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, <ref type=\"bibr\" target=\"#b48\">[49]</ref>), a selfsupervised representation learning technique for r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  a single image has recently received a huge boost in performance using Deep-Learning based methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe  CNN-based SR methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, we only learn the residual between the interpolated LR and i haustively trained for these conditions. In fact, ZSSR is significantly better than the older SRCNN <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and in some cases achieves comparable or better results tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed to introduce privileged distillation into recommendations <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Selective Distillation Network <ref type=\"bibr\" target=\"#b as the teacher model, so that the student model can distill effective review information. Xu et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> proposed Privileged Features Distillation (PFD) to distill. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pular technique has drawn applicability at the instruction level (to ease functional unit pressure) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, or even at functional en short-circuit subsequent occurrences. Such lookup table approaches have been studied in the past <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> in the context of sc le prior works use lookup tables to optimize redundant output generations in other workload domains <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, there is a clear co  To study the effects of short-circuiting the CPU computations alone using prior approaches such as <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Memoization</head><p>Prior works such as <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ ing such repeated computations in the CPU execution contexts for scientific workloads. For example, <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> use hardware table t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: exemplar based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, and neural network based methods <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lkit <ref type=\"bibr\" target=\"#b72\">[72]</ref> to extract visual features and the OpenSmile toolkit <ref type=\"bibr\" target=\"#b73\">[73]</ref> to extract acoustic features which are then fed to an SVM . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re-ranking model's effectiveness and its efficiency. While IR-specific networks are reasonably fast <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ rms in a single interaction match matrix, followed by softhistogram scoring based on kernel-pooling <ref type=\"bibr\" target=\"#b35\">[36]</ref>. This allows us to explain scoring reasons by probing the   of a hard histogram method and the resulting lack of fine-tuned word representations. Xiong et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> improve on the idea and propose the kernel-pooling techniq  qi, dj)<label>(4)</label></formula><p>Then, we transform each entry in M with a set of RBF-kernels <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Each kernel focuses on a specific similarity range with c  similarity range with center \u00b5 k . The size of all ranges is set by \u03c3. In contrast to Xiong et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> we do not employ an exact match kernel -as contextualized  alysis unfeasible.</p><p>The differences of TK to previous kernel-pooling methods are:</p><p>\u2022 KNRM <ref type=\"bibr\" target=\"#b35\">[36]</ref> uses only word embeddings, therefore a match does not have  improves the robustness of PACRR's pooling strategy with randomization during training.</p><p>KNRM <ref type=\"bibr\" target=\"#b35\">[36]</ref> uses a soft-histogram (differentiable Gaussian kernel func. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a trainable and personalized relation scoring func  where GNNs can be used directly, while here we investigate GNNs for heterogeneous KGs. Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC a user-personalized weighted graph, which characterizes user's preferences. To this end, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero \u03bb is better than \u03bb = 0 (the case of Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref>), which justifies our claim that LS regularization can ass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in a relatively small proportion of programs' data variables <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, and by selectively protecting these SDC-prone variables, one ref type=\"bibr\" target=\"#b18\">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and allows fault injections to be performed at specific pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e. However, we find that the multiplexer-based coverage proposed by the state-of-the-art RTL fuzzer <ref type=\"bibr\" target=\"#b13\">[14]</ref>, has critical limitations due to following two reasons: 1) Intel experienced before. Particularly comparing the fuzzing performance of DIFUZZRTL against RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref> (i.e., the state of the art RTL fuzzer), DIFUZZRTL showed  ut each approach has its own limitation.</p><p>Focusing the discussion on fuzzing techniques, RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed the mux-coverage guided fuzzing technique. The co rageguided feature; 2) mux-cov, which utilizes the mux-coverage guided fuzzing as proposed by RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref>; and 3) reg-cov, which utilizes the register-coverage guid  number of line almost twice because of the wiring cost of monitoring per each mux. Moreover, RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref> was not able to instrument Boom core due to the resource c nch coverage. As mentioned, branch coverage on RTL code has fundamental limitation. Recently, RFUZZ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, proposes mux coverage which can be synthesized into FPGA  the most widely used coverage is the functional coverage which is manually defined by the designers <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Static RTL Verification. Along with dynamic verification, mmarize, register-coverage of DIFUZZRTL has three fundamental differences from muxcoverage of RFuzz <ref type=\"bibr\" target=\"#b13\">[14]</ref>, making DIFUZZRTL's simulation more efficient. First, inst. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ang, 2001)</ref>, peer-to-peer systems <ref type=\"bibr\" target=\"#b7\">(Hailong &amp; Jun, 2004;</ref><ref type=\"bibr\" target=\"#b12\">John et al., 2000;</ref><ref type=\"bibr\" target=\"#b21\">Mohan &amp; Ka. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e explicit and interpretable knowledge, several works integrate structured knowledge (KGs) into LMs <ref type=\"bibr\" target=\"#b25\">(Mihaylov and Frank, 2018;</ref><ref type=\"bibr\" target=\"#b19\">Lin et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g to the two entities, and L is the max length of all input sequences. Following standard practices <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref>, we add two special tokens to mark the beg d it soon becomes the backbone of many following LMs. By pre-training on a large-scale corpus, BERT <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref> obtains the ability to capture a notable a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, intermediate layers <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, and relation-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sor T. Note that for the first three convolution layers, after each convolution layer applied, ReLU <ref type=\"bibr\" target=\"#b26\">[27]</ref> is used. Since we have saved the indices of these non-empt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .org/ns/1.0\"><head n=\"2.2\">MAML</head><p>We give an overview of Model-Agnostic Meta-Learning method <ref type=\"bibr\" target=\"#b11\">[12]</ref> which is a representative algorithm of optimization-based   problem, we propose to encode the information from support set into our parameter inspired by MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> and further we can obtain a category-specific model to acc n testing query data.</p><p>\u2022Meta-Learning We select two state-of-the-art meta learning models MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> and Meta-SGD <ref type=\"bibr\" target=\"#b20\">[21]</ref> as  ent based learning procedure for new task quick adaptation. In the optimization-based methods, MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> is a recent promising model which learns a set of model pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cus only on methods closely related to FixMatch. Broader introductions to the field are provided in <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nese poetry generation have been mostly rulebased or template-based. Recurrent Neural Network (RNN) <ref type=\"bibr\" target=\"#b10\">[11]</ref> was recently introduced as it has been proved to be effect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b5\">(Gao et al. 2012</ref>). However, traditional hypergraph learning methods <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref> suffer from their high comput ployed to model high-order correlation among data.</p><p>Hypergraph learning is first introduced in <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref>, as a propagation process on  the hypergraph structure. The task can be formulated as a regularization framework as introduced by <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref>:</p><formula xml:id=\"formula_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\">Uz-Zaman et al., 2013;</ref><ref type=\"bibr\" target=\"#b26\">Minard et al., 2015;</ref><ref type=\"bibr\" target=\"#b22\">Llorens et al., 2015;</ref><ref type=\"bibr\" target=\"#b29\">Ning et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: types. It has been widely adopted in many realworld applications, arranging from recommender system <ref type=\"bibr\" target=\"#b34\">[34]</ref>, drug discovery <ref type=\"bibr\" target=\"#b39\">[39]</ref>  partite graphs closely. They propose various DNNs to solve recommendation tasks. For example, GC-MC <ref type=\"bibr\" target=\"#b34\">[34]</ref> uses one relation-aware graph convolution layer to learn n ointly optimizes explicit and implicit relations in a unified framework. \u2022 Matrix completion: GC-MC <ref type=\"bibr\" target=\"#b34\">[34]</ref> and IGMC <ref type=\"bibr\" target=\"#b42\">[42]</ref>. GC-MC  uction-based works <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b34\">34,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" tar tempt to reconstruct the adjacency matrix by learning different encoders. In particular, some works <ref type=\"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" ta ey mainly focus on how to model local graph structures in the latent space.</p><p>Matrix completion <ref type=\"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> and collaborative . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d to belief propagation (BP) <ref type=\"bibr\" target=\"#b36\">(Yedidia, Freeman, and Weiss 2003;</ref><ref type=\"bibr\" target=\"#b25\">Rossi et al. 2018)</ref>, a message-passing approach where each node . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: earning visual representations in a self-supervised manner <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>. Pushing the embeddings of two transformed versions of the  ations learned in an unsupervised way. It is however shown <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref> that increasing the memory/batch size leads to diminishing  t also use contrastive learning losses. These include MoCo <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b31\">30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=  contain the whole training set, while the recent Momentum Contrast (or MoCo) approach of He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref> keeps a queue with features of the last few batches as mem t of self-supervised representation learning. We delve deeper into learning with a momentum encoder <ref type=\"bibr\" target=\"#b31\">[30]</ref> and show evidence that harder negatives are required to fa ions. a)</head><p>We delve deeper into a top-performing contrastive self-supervised learning method <ref type=\"bibr\" target=\"#b31\">[30]</ref> and observe the need for harder negatives; b) We propose h air, which is contrasted with every feature n in the bank of negatives (Q) also called the queue in <ref type=\"bibr\" target=\"#b31\">[30]</ref>. A popular and highly successful loss function for contras \"bibr\" target=\"#b65\">64,</ref><ref type=\"bibr\" target=\"#b78\">77]</ref>, a queue of the last batches <ref type=\"bibr\" target=\"#b31\">[30]</ref>, or simply be every other image in the current minibatch < computing them as the encoder keeps changing. The Momentum Contrast (or MoCo) approach of He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref> offers a compromise between the two negative sampling extr  and all features in Q are encoded with the key encoder.</p><p>How hard are MoCo negatives? In MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> (resp. SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>) t  helping a lot towards learning the proxy task.</p><p>On the difficulty of the proxy task. For MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, SimCLR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, InfoMin i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> and MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref>. MoC rop testing. For object detection on PASCAL VOC <ref type=\"bibr\" target=\"#b18\">[19]</ref> we follow <ref type=\"bibr\" target=\"#b31\">[30]</ref> and fine-tune a Faster R-CNN <ref type=\"bibr\" target=\"#b55 pe=\"foot\" target=\"#foot_2\">3</ref> code and report the common AP, AP50 and AP75 metrics. Similar to <ref type=\"bibr\" target=\"#b31\">[30]</ref>, we do not perform hyperparameter tuning for the object de tic segmentation on the COCO dataset <ref type=\"bibr\" target=\"#b42\">[41]</ref>. Following He et al. <ref type=\"bibr\" target=\"#b31\">[30]</ref>, we use Mask R-CNN <ref type=\"bibr\" target=\"#b29\">[29]</re nd on the train2017 set (118k images) and evaluate on val2017. We adopt feature normalization as in <ref type=\"bibr\" target=\"#b31\">[30]</ref> when fine-tuning. MoCHi and MoCo use the same hyper-parame llowing our discussion in Section 3.2, we wanted to verify that hardness of the proxy task for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> is directly correlated to the difficulty of the transforma i.e. the percentage of queries where the key is ranked over all negatives, across training for MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>, MoCo-v2 <ref type=\"bibr\" target=\"#b12\">[13]</ref> and som et=\"#b53\">[52]</ref> study the robustness of contrastive self-supervised learning methods like MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref> and PIRL <ref type=\"bibr\" target=\"#b47\">[46]</ref> and saw [11]</ref>, e.g. the addition of a target network whose parameter update is lagging similar to MoCo <ref type=\"bibr\" target=\"#b31\">[30]</ref>  Synthesizing for supervised metric learning. Recently, sy rt in parenthesis the difference to MoCo-v2. * denotes reproduced results. \u2020 results are copied from<ref type=\"bibr\" target=\"#b31\">[30]</ref>. We bold (resp. underline) the highest results overall (re rg/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">In this section we study contrastive learning for MoCo<ref type=\"bibr\" target=\"#b31\">[30]</ref> on ImageNet-100, a subset of ImageNet consisting of 100 cl et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b47\">46,</ref><ref type=\"bibr\" tar d highly successful loss function for contrastive learning <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b65\">64]</ref> is the following:</ 2 -normalized. In a number of recent successful approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">30,</ref><ref type=\"bibr\" target=\"#b47\">46,</ref><ref type=\"bibr\" tar pervised learning papers do not discuss variance ; in fact only papers from highly resourceful labs <ref type=\"bibr\" target=\"#b31\">[30,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b11\">Gleich &amp; Mahoney, 2015;</ref><ref type=\"bibr\">Peel, 2017;</ref><ref type=\"bibr\" target=\"#b5\">Chin et al., 2019)</ref> but have largely been ignored in GNNs. That b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpetitive ILSVRC <ref type=\"bibr\" target=\"#b8\">[9]</ref> competition with 224\u00d7224 or 320\u00d7320 images <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, meta-learning <ref type=\"bibr\" target=\"#b10\">(Garcia &amp; Bruna, 2017)</ref>, social analysis <ref type=\"bibr\" target=\"#b25\">(Qiu et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Li &amp; Goldw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, <ref type=\"bibr\" target=\"#b20\">[21]</ref> and strided <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016)</ref>. However, most existing methods require la \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016)</ref> to define the score of the predicted seque \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Finkel et al. \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Ratinov and  cannot deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTM-CRF <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> into a Fuzzy CRF layer, which allows each kens, such as \"Thus\" and \"by\", are labeled as O.</p><p>Fuzzy-LSTM-CRF. We revise the LSTM-CRF model <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> to the Fuzzy-LSTM-CRF model to support th -Disease datasets, LM-LSTM-CRF <ref type=\"bibr\" target=\"#b13\">(Liu et al., 2018)</ref> and LSTM-CRF <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> achieve the state-of-the-art F 1 scores w cent advances in neural models have freed do-main experts from handcrafting features for NER tasks. <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" target=\"#b31\">31,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, which aims to verify given claims with the evidence retrie ions of the claim, the evidence, and the evidence metadata. Then, different from previous work like <ref type=\"bibr\" target=\"#b32\">[32]</ref>, which computes the attention by the claim and the evidenc  infer the relationship between evidence and claims, which achieves better performance. Zhou et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref> propose the graph-based evidence aggregating and reasoning for 2 iterations. We train our system with the paddlepaddle 3 deep learning framework. For the GEAR <ref type=\"bibr\" target=\"#b32\">[32]</ref> and KGAT <ref type=\"bibr\" target=\"#b14\">[14]</ref>, we use  sentences are retrieved, the system will predict the claim being true. \u2022 GEAR. We utilize the GEAR <ref type=\"bibr\" target=\"#b32\">[32]</ref> model as one of the competitive baseline models, which ach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 10; 11]</ref> and we discuss in more detail how USMPep stands out from these approaches. MHCnuggets <ref type=\"bibr\" target=\"#b9\">[10]</ref> is rather similar to the proposed approach (apart from the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esses of the attention mechanism on various applications <ref type=\"bibr\" target=\"#b31\">[31]</ref>, <ref type=\"bibr\" target=\"#b38\">[38]</ref>- <ref type=\"bibr\" target=\"#b40\">[40]</ref>, we build a sup. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: an knowledge about the extraction. For a general introduction of first-order logic, please refer to <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Complete consistency describes the fact that the va. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ghbors into node representation learning and achieve state-ofthe-art performance for recommendation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta pervision Signal. Most models approach the recommendation task under a supervised learning paradigm <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta n users and items, where \ud835\udc66 \ud835\udc62\ud835\udc56 indicates that user \ud835\udc62 has adopted item \ud835\udc56 before. Most existing models <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta rimental Settings</head><p>We conduct experiments on three widely used benchmark datasets: Yelp2018 <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, Amazon-Book <ref  2018 <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, Amazon-Book <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, and Alibaba-iFash shion <ref type=\"bibr\" target=\"#b5\">[6]</ref> <ref type=\"foot\" target=\"#foot_0\">1</ref> . Following <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, we use the same 1 et=\"#b35\">[36]</ref>, and PinSage <ref type=\"bibr\" target=\"#b47\">[48]</ref> since the previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta  type=\"bibr\" target=\"#b1\">[2]</ref>, to GCN that propagates user and item embeddings over the graph <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta  embedding and item embedding. Here we implement it on a state-of-the-art GCN-based model, LightGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Experimental studies on three benchmark datasets demonstr ibr\" target=\"#b47\">48]</ref>, concatenation <ref type=\"bibr\" target=\"#b43\">[44]</ref>, or summation <ref type=\"bibr\" target=\"#b16\">[17]</ref> over the representations of all layers.</p><p>Supervised L rization coefficient \ud835\udf06 2 and the number of GCN layers within the suggested ranges.</p><p>\u2022 LightGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>. This is the state-of-the-art graph-based CF method which   between LightGCN and SGL-ED.trainable parameters, the space complexity remains the same as LightGCN<ref type=\"bibr\" target=\"#b16\">[17]</ref>. The time complexity of model inference is also the same, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  of a single device.</p><p>Recently, pipeline parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> has been proposed as a promising approach for training lar een those nodes during backpropagation. The other category is asynchronous(async) pipeline training <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This manner inserts mini-batches into pipeline continuous lism, and hybrid approaches combining both. Current state-of-theart pipeline partitioning algorithm <ref type=\"bibr\" target=\"#b11\">[12]</ref> is not able to be applied to synchronous training effectiv evice assignment affects communication efficiency and computing resource utilization. Previous work <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses hierarchical planning and works well for asynchronous D. Contributions over previous work</head><p>Previous works on pipeline planning includes PipeDream <ref type=\"bibr\" target=\"#b11\">[12]</ref> (for asynchronous training) and torchgpipe <ref type=\"bibr it the micro-batch further into 2 even slices, and assign each to a device. An alternative approach <ref type=\"bibr\" target=\"#b11\">[12]</ref> (Fig. <ref type=\"figure\" target=\"#fig_6\">8(b)</ref>) is no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ons of deep clustering <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, deep attractor networks <ref type=\"bibr\" target=\"#b3\">[4,</r et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. However, this usually only leads to small improvements, even proach for supervised speech separation is via T-F masking <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. The proposed approach is expected to produce even better sep  reconstruction, it is necessary to first obtain a good enough magnitude estimate. Our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a novel multi-task learning approach combining the  gs:</p><formula xml:id=\"formula_0\">LDC,classic = V V T \u2212 Y Y T 2 F (1)</formula><p>Our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> suggests that an alternative loss function, which whitens th  be discussed in Section 3.4. Following <ref type=\"bibr\" target=\"#b21\">[22]</ref>, our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a chimera++ network combining the two approaches vi \" target=\"#b13\">[14]</ref> only performs iterative reconstruction for each source independently. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref ty es remain fixed during iterations, while the phase of each source are iteratively reconstructed. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the phase reconstruction was only added as a post-processin tained when applying five iterations of Griffin-Lim on each source independently, as is reported in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Performing end-to-end optimization using LWA improves the r ates directly in the time domain. Our result is 1.1 dB better than the previous state-of-the-art by <ref type=\"bibr\" target=\"#b2\">[3]</ref> in terms of both SI-SDR and SDR.</p></div> <div xmlns=\"http:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b11\">11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b25\">25,</ref><ref type=\"bibr\" tar V as a natural language inference (NLI) <ref type=\"bibr\" target=\"#b0\">[1]</ref> task. Thorne et al. <ref type=\"bibr\" target=\"#b24\">[24]</ref> mainly feed the concatenated evidence and the given claim . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> embedding trained  g we refer to in this paper is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> embedding, which h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\">(Tschannen et al., 2020)</ref>, and S4L -supervised plus semi-supervised learning on ImageNet <ref type=\"bibr\" target=\"#b52\">(Zhai et al., 2019a)</ref>.</p><p>ViT-H/14 outperforms BiT-R152x4, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ;</ref><ref type=\"bibr\">Kitaev et al., 2019, i.a.)</ref>.</p><p>In this context, the GLUE benchmark <ref type=\"bibr\" target=\"#b62\">(Wang et al., 2019a)</ref> has become a prominent evaluation framewor s, we use an average of the metrics. More information on the tasks included in GLUE can be found in <ref type=\"bibr\" target=\"#b62\">Wang et al. (2019a)</ref> and in <ref type=\"bibr\">Warstadt et al. (20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, graph classificati es' influence distribution and random walk <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Recently, SGC <ref type=\"bibr\" target=\"#b30\">[31]</ref> is proposed by reducing unnecessary complexity in GCN. The ormation and propagation processes is also adopted in <ref type=\"bibr\" target=\"#b11\">[12]</ref> and <ref type=\"bibr\" target=\"#b30\">[31]</ref> but for the sake of reducing complexity.</p><p>In this wor E <ref type=\"bibr\" target=\"#b8\">[9]</ref>, APPNP <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and SGC <ref type=\"bibr\" target=\"#b30\">[31]</ref>. We aim to provide a rigorous and fair comparison between . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Recently, in visual representation learning, contrastive learning has renewed a surge of interest <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta recent surge of interest in visual representation learning <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar normalized temperature-scaled cross entropy loss (NT-Xent) <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>.</p><p>During GNN p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b16\">[17]</ref>, BERT <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ERNIE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, as alternatives to RNNs and CNNs in many NLP tasks. Howev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ised graph clustering is often an extremely useful end-goal in itself -whether for data exploration <ref type=\"bibr\" target=\"#b44\">[45]</ref>, visualization <ref type=\"bibr\" target=\"#b10\">[11,</ref><r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>.</p><p>Although consi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b1\">[2]</ref> predicts the nonlinear LR-HR mapping via a fully convolution monality among the above CNN models is that their networks contain fewer than 5 layers, e.g., SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> uses 3 convolutional layers. Their deeper structures with 4  ween the input ILR image and the output HR image. There are three notes for VDSR: (1) Un-like SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> that only uses 3 layers, VDSR stacks 20 weight layers (3 \u00d7 3 arget=\"#b32\">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref> RFL <ref typ e as <ref type=\"bibr\" target=\"#b12\">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, VDSR <ref   PSyCo [20] and IA <ref type=\"bibr\" target=\"#b28\">[30]</ref>. The deep models (d \u2264 8) include SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, DJSR <ref type=\"bibr\" target=\"#b31\">[33]</ref>, CSCN <ref t rsity of images. Shi et al. <ref type=\"bibr\" target=\"#b23\">[25]</ref> observe that the prior models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b30\">32]</ref> increase LR image's  1</ref> shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ , k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ  only. Therefore, the input and output images are of the same size. For fair comparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ  type=\"bibr\" target=\"#b9\">[10]</ref> RFL <ref type=\"bibr\" target=\"#b21\">[23]</ref>   the results of <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\">20,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e linked together. <ref type=\"bibr\" target=\"#b36\">[37]</ref> proposes a method using self-attention <ref type=\"bibr\" target=\"#b35\">[36]</ref> and bi-affine scoring algorithm to predict biological rela 14\">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Attention. We adapt Transformer <ref type=\"bibr\" target=\"#b35\">[36]</ref> to encode word sequences in a paragraph, where we calculat across sentences. We base on recent Transformer architecture <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> to build this module, due to its better performance in enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: erging Convolution and Self-Attention</head><p>For convolution, we mainly focus on the MBConv block <ref type=\"bibr\" target=\"#b26\">[27]</ref> which employs depthwise convolution <ref type=\"bibr\" targe ref> are popular in mobile platforms due to its lower computational cost and smaller parameter size <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>Recent works show that an improved inverted residua t=\"#b26\">[27]</ref>.</p><p>Recent works show that an improved inverted residual bottlenecks (MBConv <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>), which is built u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, and relation-based distillation <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h as documents, a graph is needed to build that represents the relation between sentences. TextRank <ref type=\"bibr\" target=\"#b18\">[19]</ref> makes it possible to form a sentence extraction algorithm,  in the united sentence collection S. The sentence similarity is defined as the same as in TextRank <ref type=\"bibr\" target=\"#b18\">[19]</ref>, to measures the overlapping word ratio between two senten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">[41]</ref>, and graph pattern mining <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. Although many techniques such as search order optimization raphs from data graphs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. Graph mining is essential for several problems involving t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b32\">(Toma 2010;</ref><ref type=\"bibr\" target=\"#b41\">Zhao and Jiang 2011;</ref><ref type=\"bibr\" target=\"#b33\">Tominaga and Hijikata 2015)</ref>. Because the sample sizes are so sm ove less accurate in the case of new users <ref type=\"bibr\" target=\"#b4\">(Bergsma et al. 2013;</ref><ref type=\"bibr\" target=\"#b33\">Tominaga and Hijikata 2015)</ref>. Increased data sources that includ f>, gender <ref type=\"bibr\" target=\"#b9\">(Filho et al. 2016)</ref>, tweets, followers and followees <ref type=\"bibr\" target=\"#b33\">(Tominaga and Hijikata 2015)</ref> are relevant to the creation of us  with different cultural backgrounds might select different types of avatars. Tominaga and Hijikata <ref type=\"bibr\" target=\"#b33\">(Tominaga and Hijikata 2015)</ref> collected the data from 300 users   partition scheme</head><p>A user partition scheme developed from the work of Tominaga and Hijikata <ref type=\"bibr\" target=\"#b33\">(Tominaga and Hijikata 2015)</ref>, Lim et al. <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\">64</ref>] on the ActivityNet1.3 dataset. Th get=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b48\">48,</ref><ref type=\"bibr\" target=\"#b56\">56]</ref> on the THUMOS2014 and ActivityNet1.3 datasets, respectively ream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods usually use average pooling or concatenatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er-item interactions <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. A large model with e real-time platform <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><p>To tackle th arget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar arget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> (Figure <ref type=\" arget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> have focused on mat od effectively utilizing the teacher's predictions. First, <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> distill the knowledge of the items with high scores in the  distilling knowledge of a few top-ranked items is effective to discover the user's preferable items <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Most recently, <ref type=\"bibr\" target=\"#b9\">[10]</ref> u opology (Section 3.3). Note that we do not include the methods distilling the predictions (e.g., RD <ref type=\"bibr\" target=\"#b21\">[22]</ref>, CD <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and RRD <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b13\">(Kingma and Welling 2013)</ref>, or Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b6\">(Goodfellow et al. 2014)</ref>, since they often require a significant \" target=\"#b40\">(Tran et al. 2017;</ref><ref type=\"bibr\" target=\"#b19\">Lee et al. 2019)</ref>, GANs <ref type=\"bibr\" target=\"#b6\">(Goodfellow et al. 2014)</ref>, and VAEs <ref type=\"bibr\" target=\"#b13. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> were proposed. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>  ><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> were proposed. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on  e reconstructed some classic SR models, such as SRCNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>, EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> and SRResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>. During <ref type=\"bibr\" target=\"#b5\">[6]</ref>, SRResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Unfortunately, these models become more and more deeper and CN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, LapSRN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. For fair, we retrain most of these models (except for EDSR  <ref type=\"bibr\" target=\"#b8\">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, the results of EDSR provided by their original papers).</p> t upscaling factors and test-datasets. It can be seen that our results are slightly lower than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. But it is worth noting that EDSR <ref type=\"bibr\" target=\"#  slightly lower than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. But it is worth noting that EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> use  RGB channels for training, meanwhile, the data augment  nwhile, the data augment methods are different.</p><p>To better illustrate the difference with EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we show a comparison of model specifications in Table <ref   show a comparison of model specifications in Table <ref type=\"table\" target=\"#tab_3\">3</ref>. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> is an outstanding model gained amazing results. However, it   memory, space and datasets. In contrast, the specifications of our model is much smaller than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which makes it easier to reproduce and promote.</p><p>In Fi nts the upscaling factor) mixed training method is used in <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and geometric selfensemble method is proposed in <ref type= 4]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and geometric selfensemble method is proposed in <ref type=\"bibr\" target=\"#b8\">[9]</ref>. We believe that these training tricks can also improve our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: opted fusion methods, i.e., early fusion and late fusion <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Early fusion aims to combine features directly and train . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref> processes speech input directly into intent without going th tly the most promising results under-perform or just barely outperform traditional cascaded systems <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. One reason is that d to-end speech-to-intent model, we need intent-labeled speech data, and such data is usually scarce. <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> address this problem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to cosine similarity and L2 distance <ref type=\"bibr\" target=\"#b29\">(Mussmann and Ermon, 2016;</ref><ref type=\"bibr\" target=\"#b34\">Ram and Gray, 2012)</ref>. As our ablation study finds other similari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b65\">66,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b146\">148,</ref><ref type=\"bibr\" target=\"#b100\">102,</ref><ref type=\"bibr\" target=\"#b81\">82,</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings for CiteULike, twenty ratings for Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Data statistics are   We follow the widely used leave-one-out evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we le rmance of each method with widely used three ranking metrics <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>: hit ratio (H@\ud835\udc41 ), nor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> estimate 3D object information by calculating the similarity between 3D objects and CAD models. <ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b18\">[19]</ref> explore usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  data alignment, often involve complicated architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bib  that could only be observed from 3D space. MV3D <ref type=\"bibr\" target=\"#b10\">[11]</ref> and AVOD <ref type=\"bibr\" target=\"#b11\">[12]</ref> project the raw point cloud into bird's eye view (BEV) to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr rkable progresses have been witnessed for object detection <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  typically implemented in a contrastive learning framework <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. The primary focus here is to learn low-dimensional image /. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i-gate Mixture-of-Experts (MMoE) structure, which is inspired by the Mixture-of-Experts (MoE) model <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the recent MoE layer <ref type=\"bibr\" target=\"#b15\">[1 \"4\">MODELING APPROACHES 4.1 Mixture-of-Experts</head><p>The Original Mixture-of-Experts (MoE) Model <ref type=\"bibr\" target=\"#b20\">[21]</ref> can be formulated as:</p><formula xml:id=\"formula_5\">= n i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n, that were applied to short-text summarization. We propose a novel variant of the coverage vector <ref type=\"bibr\" target=\"#b25\">(Tu et al., 2016)</ref> from Neural Machine Translation, which we use d n=\"2.3\">Coverage mechanism</head><p>Repetition is a common problem for sequenceto-sequence models <ref type=\"bibr\" target=\"#b25\">(Tu et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">Mi et al., 2016 erating multi-sentence text (see Figure <ref type=\"figure\">1</ref>). We adapt the coverage model of <ref type=\"bibr\" target=\"#b25\">Tu et al. (2016)</ref> to solve the problem. In our coverage model, w ine Translation <ref type=\"bibr\" target=\"#b10\">(Koehn, 2009)</ref>, coverage was adapted for NMT by <ref type=\"bibr\" target=\"#b25\">Tu et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b14\">Mi et al. (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion is built.</p><p>Objective Function We optimize the parameters of PNet using REINFORCE algorithm <ref type=\"bibr\" target=\"#b24\">(Williams 1992</ref>) and policy gradient methods <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. It aims to hide the user's real search intent among a set  y obfuscation method <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> to hide the user's genuine query intent among a set of gene  key problem of this paper, and more details can be found in <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. Different from recording cover queries into the search log. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" targ periments:</p><p>\u2022 AMiner-AND<ref type=\"foot\" target=\"#foot_0\">1</ref> . The dataset is released by <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, which contains 500 author names for traini  e.g., paper network <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" target=\"#b13\">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature  (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch on crop mapping and proved its reliability <ref type=\"bibr\" target=\"#b73\">(Xu et al., 2020;</ref><ref type=\"bibr\" target=\"#b21\">Hao et al., 2020)</ref>. Hence, CDL data sets, which corresponded to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying <ref type=\"bibr\" target=\"#b22\">[24]</ref>. The node-copying model can be used to produce sample grap type=\"bibr\" target=\"#b21\">[23]</ref> uses a non-parametric model for the graph generative model and <ref type=\"bibr\" target=\"#b22\">[24]</ref> proposes a node copying model to achieve flexibility in th rnative, we use a more general generative model for graphs based on copying nodes, as introduced in <ref type=\"bibr\" target=\"#b22\">[24]</ref>. We demonstrate in the following sections that this model   setting.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Node Copying</head><p>In <ref type=\"bibr\" target=\"#b22\">[24]</ref>, Pal et al. introduce the node copying model for \ud835\udc5d (G). Sa etworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying<ref type=\"bibr\" target=\"#b22\">[24]</ref>. The node-copying model can be used to produce sample grap ode classification when there are very few training labels <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" tar node classification when there are very few training labels<ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" tar s. These limitations were addressed in the follow-up works <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24]</ref>, where <ref type=\"bibr\" target=\"#b21\">[23]</ref> uses a non. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o gather to the requesting core, only the requested elements from the data array.</p><p>Song et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> propose a graph processor based on sparse matrix algebra, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ootstrapping approach to iteratively propose new alignment labels to enhance the performance. MuGNN <ref type=\"bibr\" target=\"#b2\">(Cao et al., 2019)</ref> encodes KGs via multi-channel Graph Neural Ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: able place to look is human cognition <ref type=\"bibr\" target=\"#b7\">(Davis &amp; Marcus, 2015;</ref><ref type=\"bibr\" target=\"#b29\">Lake et al., 2016;</ref><ref type=\"bibr\" target=\"#b42\">Marcus, 2001;<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on from the application -even in cases where it is explicitly visible to user threads, as in Psyche <ref type=\"bibr\" target=\"#b32\">[33]</ref>. So-called \"cooperative multi-tasking systems\" (for exampl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed image retrieval, image captioning, and visual question answering. Following the success of BERT <ref type=\"bibr\" target=\"#b8\">[9]</ref> in various language tasks, recent VL methods typically use T. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y and encourage source domain experts to provide more complementary and discriminative information. <ref type=\"bibr\" target=\"#b2\">(3)</ref> To make the model more generalizable to target domains, we p enting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecommendations based on collaborative filtering principles <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, they have not been. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[9]</ref>.</p><p>Representation-based matching approaches are inspired by the Siamese architecture <ref type=\"bibr\" target=\"#b3\">[4]</ref>. This kind of approach aims at encoding each input text in a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ods (GCN <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[13]</ref>), etc.</p><p>Graph-level embedding. The most intuitive way. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ic information for many tasks such as crop growing condition monitoring, yield prediction and so on <ref type=\"bibr\" target=\"#b38\">(Matsuda et al., 2001;</ref><ref type=\"bibr\" target=\"#b62\">Son et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a function of key, source address, and timestamp G12 G2. Our method is inspired by previous studies <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. It can ensure . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent works have focused on learning deep embeddings that can be used as universal object descriptors <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ  effect of incorporating the CF into the fully-convolutional Siamese framework of Bertinetto et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We find that the CF does not improve results for networks t e performance. For our method, we prefer to build upon the fully-convolutional Siamese architecture <ref type=\"bibr\" target=\"#b2\">[3]</ref>, as it enforces the prior that the appearance similarity fun .\">Fully-convolutional Siamese networks</head><p>Our starting point is a network similar to that of <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which we later modify in order to allow the model to be int t is necessary to combine this with a procedure that describes the logic of the tracker. Similar to <ref type=\"bibr\" target=\"#b2\">[3]</ref>, we employ a simplistic tracking algorithm to assess the uti r during training. We first compare against the symmetric Siamese architecture of Bertinetto et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We then compare the endto-end trained CFNet to a variant wh  random seeds, this would require significantly more resources. Our baseline diverges slightly from <ref type=\"bibr\" target=\"#b2\">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw ). We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Staple <ref type=\"bibr\" target=\"#b1\">[2]</ref> and LCT <ref s=\"http://www.tei-c.org/ns/1.0\"><head>A. Implementation details</head><p>We follow the procedure of <ref type=\"bibr\" target=\"#b2\">[3]</ref> to minimize the loss (equation 2) through SGD, with the Xavi  the following ReLU but not the following pooling layer (if any).Our baseline diverges slightly from<ref type=\"bibr\" target=\"#b2\">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw e xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">Note that this differs from<ref type=\"bibr\" target=\"#b2\">[3]</ref>, in which the target object and search area were instead den ve been introduced <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, raising interest in the tracking community for their simplic rget=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> with CNN features, as proposed in previous work <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e features <ref type=\"bibr\" target=\"#b15\">(Henderson et al., 2011;</ref><ref type=\"bibr\">2012;</ref><ref type=\"bibr\" target=\"#b14\">Hamilton et al., 2017b)</ref>. In our pipeline, we augment features w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t vector machine (SVM) <ref type=\"bibr\" target=\"#b6\">(Chang and Lin 2011)</ref>, random forest (RF) <ref type=\"bibr\" target=\"#b5\">(Breiman 2001</ref>) and gradient boosting decision tree (GBDT) <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b40\">42,</ref><ref type=\"bibr\" target=\"#b53\">55,</ref><ref type=\"bibr\" target=\"#b55\">57,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>. Despite considerab he-art performance <ref type=\"bibr\" target=\"#b40\">[42,</ref><ref type=\"bibr\" target=\"#b53\">55,</ref><ref type=\"bibr\" target=\"#b55\">57]</ref>. One of recent trends is to integrate correlation filter tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hing that often occurs in direct-mapped buffers. A 3-bit performance counter based on Heil's design <ref type=\"bibr\" target=\"#b16\">[17]</ref> tracks the effectiveness of each entry and is used to sele pendence chain. If the generating values are present then ARVI's predictions are near perfect. Heil <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposed another approach that correlates on the differenc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: treated as the appearance features of segments. For the motion features, we follow the operation in <ref type=\"bibr\" target=\"#b50\">[50]</ref> and extract the 400dimensional feature vectors from the TS  means that the NMS is not used. We use the conventional average recall with 100 proposals (AR@100) <ref type=\"bibr\" target=\"#b50\">[50]</ref> to evaluate the performance of the proposal generator. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a technical trend <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> is to develop end-t nt Modeling</head><p>Unlike the previous GNN-based studies <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> that assume no or o sonalized. (2) Unlike the ideas of using the decay factors <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> or regularization t Django Unchained), respectively. However, previous studies <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> only model KG relat et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> are founded upon th eover, KG relations are typically modeled in decay factors <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> of adjacent matrix, in order to control the influences of n recommender models <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> have shown that the neighborhood aggregation scheme is a pr sets. Experimental results show that our KGIN outperforms the state-of-the-art methods such as KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>, KGNN-LS <ref type=\"bibr\" target=\"#b37\">[38]</ref>, and CK , \ud835\udc56) pair indicates that user \ud835\udc62 has interacted with item \ud835\udc56 before. In some previous works like KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>, an additional relation interact-with is introduced to exp type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> or regularization terms <ref type=\"bibr\" target=\"#b40\">[41]</ref> in previous studies, we highlight the role of intent relat endation in the experiments:</p><p>(1) We use the Amazon-Book and Last-FM datasets released by KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref>; And (2) we further introduce the Alibaba-iFashion dataset  so as to generate user-specific item representations. It models relations in decay factors. \u2022 KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref> is a state-of-the-art GNN-based recommender. It applies an presentations. As such, these methods are able to model long-range connectivity. For instance, KGAT <ref type=\"bibr\" target=\"#b40\">[41]</ref> combines user-item interactions and KG as a heterogeneous  intent graph (IG), which differs from the homogeneous collaborative graph adopted in previous works <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>.</p></div> <div xm s and construct triplets with the inverse relations in experiments. Closely Following prior studies <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>, we use the same d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ent obfuscation based defenses have proven vulnerable. In their recent seminal work, Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> presented a suite of strategies for estimating network gradi  range of possible attacks, including those having successfully circumvented many previous defenses <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Under these attacks, we compare the worst-case robustness o b32\">33]</ref>.</p><p>Unfortunately, many of these methods have proven vulnerable by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>, who introduced a set of attacking strategies, including a m .</p><p>Thus far, gradient obfuscation is generally considered vulnerable (and at least incomplete) <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We revisit gradient obfuscation, and our defense demonstrat ased defense mechanisms seem plausible. Yet they are all fragile. As demonstrated by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>, with random input transformation, adversarial examples can  -removal transformation is also ineffective. One can use Backward Pass Differentiable Approximation <ref type=\"bibr\" target=\"#b1\">[2]</ref> to easily construct effective adversarial examples. In short ation (BPDA).</head><p>To circumvent the defense using non-differentiable operators, Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> introduced a strategy called Backward Pass Differentiable Ap  attacking strategy. It causes the adversary to suffer from either exploding or vanishing gradients <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Figure <ref type=\"figure\" target=\"#fig_4\">3</ref>-left show #b32\">33]</ref>. Yet those defenses have been proven vulnerable under a reparameterization strategy <ref type=\"bibr\" target=\"#b1\">[2]</ref>. This strategy aims to find some differentiable function h(\u2022 those defenses, the transformed image g(x) remain similar to the input x. Consequently, as shown in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, those defenses can be easily circumvented by replacing g(\u2022) mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">BPDA Attack and the Variants</head><p>BPDA attack <ref type=\"bibr\" target=\"#b1\">[2]</ref>, as reviewed in Sec. 3.3, is a powerful way to estimate netw ck-box attacks, including the black-box transfer attack <ref type=\"bibr\" target=\"#b29\">[30]</ref>   <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We evaluate other methods using the code provided in the or er all tested attacks. The methods indicated by a star (*) are those circumvented by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We include their results therein as a reference. The other  nsformations to the input. But they have been circumvented by Expectation Over Transformation (EOT) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. EOT attack first esti rsarial examples of f b directly (so that g(h(\u2022)) = h(\u2022)), without solving the optimization problem <ref type=\"bibr\" target=\"#b1\">(2)</ref>. We argue that finding such an h(\u2022) is extremely hard. If h(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: orks is motivated by human vision systems that pay visual attention to particular regions in images <ref type=\"bibr\" target=\"#b37\">[38]</ref> or words in sentences <ref type=\"bibr\" target=\"#b38\">[39]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b37\">[38]</ref>, Nguyen et al. <ref type=\"bibr\" target=\"#b38\">[39]</ref>, and Vashishth et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref> adopt a multiple layer of convolutional neural networks (C s utilized to extract global relationships between entities and relations. 10) InteractE: InteractE <ref type=\"bibr\" target=\"#b39\">[40]</ref> intends to improve link prediction performance by increasi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b35\">Kulmanov &amp; Hoehndorf, 2019)</ref>, protein-compound interaction <ref type=\"bibr\" target=\"#b52\">(Tsubaki et al., 2018)</ref>, or protein fold classification <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: der and decoder are convolutional <ref type=\"bibr\" target=\"#b19\">(LeCun et al., 1998)</ref> ResNets <ref type=\"bibr\" target=\"#b10\">(He et al., 2016)</ref> with bottleneck-style resblocks. The models p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd other conventional semantic models.</p><p>In this study, based on a convolutional neural network <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we present a new Convolutional Deep Structured Semantic Mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e presence of noise from the automatic extraction methods used to populate them. For instance, NELL <ref type=\"bibr\" target=\"#b1\">[Carlson et al., 2010]</ref> is known to contain various kinds of erro e part of the original benchmark test collection.   The NELL subset taken from its 165 th iteration <ref type=\"bibr\" target=\"#b1\">[Carlson et al., 2010]</ref>) has been used for the KG refinement task. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> are proposed. Compared with previous methods, source and ta ameworks of available one-shot voice conversion approaches <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> generally consist of a speaker encoder, a content encoder, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which aims to iden umber of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t ef type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based ones <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Graph-based works  gical features in the academic network to enhance the representation of papers. For instance, GHOST <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructs document graph based on co-authorship. <ref type=  by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.\u2022 GHOST<ref type=\"bibr\" target=\"#b1\">[2]</ref>: GHOST use affinity propagation algorithm for clustering on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .org/ns/1.0\"><head n=\"1.\">Introduction</head><p>Adversarial attacks to image classification systems <ref type=\"bibr\" target=\"#b19\">[20]</ref> add small perturbations to images that lead these systems . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad><p>While initial research on NMT starts with building translation systems between two languages, <ref type=\"bibr\" target=\"#b12\">Dong et al. (2015)</ref> extends the bilingual NMT to one-to-many tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding random crop, random clip, Rand-Augment <ref type=\"bibr\" target=\"#b6\">[7]</ref>, Random Erasing <ref type=\"bibr\" target=\"#b43\">[44]</ref>, Mixup <ref type=\"bibr\" target=\"#b41\">[42]</ref> and CutMi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  sequences or evolutionary features as inputs <ref type=\"bibr\" target=\"#b3\">(AlQuraishi, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Ingraham et al., 2019)</ref>. <ref type=\"bibr\" target=\"#b54\">Xu et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Subject independent approaches have been proposed that transform audio features to video frames <ref type=\"bibr\" target=\"#b4\">[5]</ref> but there is still no method to directly transform raw audio rating natural facial expressions. Some methods generate frames based solely on present information <ref type=\"bibr\" target=\"#b4\">[5]</ref>, without taking into account the facial dynamics. This makes for capturing articulation dynamics and estimating the 3D points of the mesh. Finally, Chung et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> proposed a CNN applied on Mel-frequency cepstral coefficient  works that are closest to ours are those proposed in <ref type=\"bibr\" target=\"#b21\">[22]</ref> and <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The former method is subject dependent and requires a large  static method that produces video frames using a sliding window of audio samples like that used in <ref type=\"bibr\" target=\"#b4\">[5]</ref>. This is a GAN-based method that uses a combination of an L  rator but also due to the use of the conditional Sequence Discriminator. Unlike previous approaches <ref type=\"bibr\" target=\"#b4\">[5]</ref> that prohibit the generation of facial expressions, the adve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ectures, e.g., Siamese networks <ref type=\"bibr\" target=\"#b9\">(He et al., 2016)</ref> and attention <ref type=\"bibr\" target=\"#b26\">(Seo et al., 2017;</ref><ref type=\"bibr\" target=\"#b31\">Tay et al., 20 her than specific term matches. Context-aware representation learning, such as co-attention methods <ref type=\"bibr\" target=\"#b26\">(Seo et al., 2017)</ref>, has been proved effective in many benchmark ng elements in the missing dimen-sions. Softmax col is the column-wise softmax operator. Similar to <ref type=\"bibr\" target=\"#b26\">Seo et al. (2017)</ref>, we perform co-attention from two directions: ; (2) interaction and attention mecha-nisms <ref type=\"bibr\" target=\"#b31\">(Tay et al., 2019b;</ref><ref type=\"bibr\" target=\"#b26\">Seo et al., 2017;</ref><ref type=\"bibr\" target=\"#b21\">Parikh et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing to produce only a single texture and have so far not been applied to SISR. Adversarial networks <ref type=\"bibr\" target=\"#b17\">[18]</ref> have recently been shown to produce sharp results in a num ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.4\">Adversarial training</head><p>Adversarial training <ref type=\"bibr\" target=\"#b17\">[18]</ref> is a recent technique that has proven to be a useful mecha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ked into formulations which can better address heterophily: Before applying label propagation, Peel <ref type=\"bibr\" target=\"#b24\">[25]</ref> transforms the original graph into either a similarity gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a few fine-tuning updates. The proposed learning to pre-train can be deemed a form of meta-learning <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017)</ref>, also known as learning to lear ermore, our strategy is a form of meta-learning, in particular, model agnostic meta-learning (MAML) <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017)</ref>. Meta-learning aims to learn pr hods directly adjust the optimization algorithm to enable quick adaptation with just a few examples <ref type=\"bibr\" target=\"#b12\">(Finn, Abbeel, and Levine 2017;</ref><ref type=\"bibr\" target=\"#b43\">Y. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> brought the success. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the context of deep neural networks, and there is now a quickly growing body of work on this topic <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ e, the \u221e -ball around x has recently been studied as a natural notion for adversarial perturbations <ref type=\"bibr\" target=\"#b10\">[11]</ref>. While we focus on robustness against \u221e -bounded attacks i s. On the attack side, prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) <ref type=\"bibr\" target=\"#b10\">[11]</ref> and multiple variations of it <ref type=\"bibr\" target=\"#b1 rget=\"#b2\">[3]</ref> for an overview of earlier work).</p><p>Adversarial training was introduced in <ref type=\"bibr\" target=\"#b10\">[11]</ref>, however the adversary utilized was quite weak-it relied o arial training discusses the phenomenon of transferability <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>-adversarial example. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  are answerable with short phrases and entities, by leveraging dense retrieval techniques like ORQA <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019)</ref>, REALM <ref type=\"bibr\" target=\"#b13\">(Guu e irming our design decisions.</p><p>Initialization R-Prec. R@5 REALM (pretrained) 6.6 14.9</p><p>ICT <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019)</ref> 9.3 16.5 REALM <ref type=\"bibr\" target=\"#b13 . As a baseline, we initialize our model with ICT, a weaker self-supervised retriever introduced in <ref type=\"bibr\" target=\"#b23\">Lee et al. (2019)</ref>. Both models are trained with minibatch sizes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inning and the end of two entities.</p><p>Position Embedding In the Transformer attention mechanism <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref>, positional encodings are injected to ma </p><p>Pre-trained LM Recently pre-trained language models achieved great success in the NLP field. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref> proposed a self-attention based architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  way that GNNs exchange that information between nodes makes them vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Adversarial attacks on graphs, which carefully rewire the g ning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks (e.g., Nettack <ref type=\"bibr\" target=\"#b7\">[8]</ref>) that perturb the graph in training-time and evasion attacks \"#b28\">[29]</ref>. The former deceives the model to misclassify a specific node (i.e., target node) <ref type=\"bibr\" target=\"#b7\">[8]</ref> while the latter degrades the overall performance of the tra e targeted attack where the attacker only manipulates edges of the target node's neighbors. Nettack <ref type=\"bibr\" target=\"#b7\">[8]</ref> generates perturbations by modifying graph structure (i.e.,  . Also, <ref type=\"bibr\" target=\"#b15\">[16]</ref> is designed specifically for the Nettack attacker <ref type=\"bibr\" target=\"#b7\">[8]</ref> and so is less versatile. Another technique <ref type=\"bibr\" our model to baselines under three kinds of adversarial attacks: direct targeted attack (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>), influence targeted attack (Nettack-In <ref type=\"bibr\" tar  attack (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>), influence targeted attack (Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>), and non-targeted attack (Mettack <ref type=\"bibr\" target=\" or all neighbors. In the targeted attack, we select 40 correctly classified target nodes (following <ref type=\"bibr\" target=\"#b7\">[8]</ref>): 10 nodes with the largest classification margin, 20 random taset into training (10%), validation (10%), and test set (80%) following the experimental setup in <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Ap ef type=\"bibr\" target=\"#b15\">[16]</ref>), and models for generating adversarial attacks (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Met  for generating adversarial attacks (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Mettack <ref type=\"bibr\" target=\"#b25\">[26]</ref>).</p> y loss using Adam optimizer and learning rate of 0.01. For other parameters, we follow the setup in <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p></div>\t\t\t</div> \t\t\t<div type=\"references\">  \t\t\t\t<listBib attacker finds optimal perturbation A through optimization <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>:</p><formula xml:id=\"formula_3\">argmin A \u2208P G \u2206 L attack (f(A s. The attacker aims to destroy prediction for target node u by manipulating the incident edges of u<ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Here, T = A = {u}. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eta-learning to obtain weights that can be easily fine-tuned to a given noisy dataset. Zhang et al. <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed to learn confidence scores of each samples from t arget=\"#b6\">[7]</ref> 71.39 Joint-Optim <ref type=\"bibr\" target=\"#b25\">[26]</ref> 72.16 MetaCleaner <ref type=\"bibr\" target=\"#b33\">[34]</ref> 72.5 MLNT <ref type=\"bibr\" target=\"#b15\">[16]</ref> 73.47 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Our method is motivated by recent studies <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">Toneva et al., 2019)</ref> showing that noisy labels often have delay 2017)</ref>.</p><p>Moreover, learned noisy examples tend to be frequently forgotten in later epochs <ref type=\"bibr\" target=\"#b28\">(Toneva et al., 2019)</ref>, since they conflict with the general ind. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: w method, hence we fail to answer the question.</p><p>Pretrained language models, pioneered by BERT <ref type=\"bibr\" target=\"#b11\">[12]</ref>, have emerged as silver bullets for many NLP tasks, such a ficial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, even if the embeddings for larger positions are . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oder architecture, to disentangle the semantics of both input images and text descriptions. ManiGAN <ref type=\"bibr\" target=\"#b21\">[22]</ref> introduces a novel text-image combination module, which pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f stable feature distillation, which consists of a deep global balancing regression (DGBR) algorithm<ref type=\"bibr\" target=\"#b12\">[13]</ref>, a teacher network and a student network. The DGBR algorit ner is another promising direction.</p><p>Feature-Based Module. The current stable feature approach <ref type=\"bibr\" target=\"#b12\">[13]</ref> needs much time and computing resources. For implementing . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to thank the Python community <ref type=\"bibr\" target=\"#b47\">(Van Rossum &amp; Drake Jr, 1995;</ref><ref type=\"bibr\" target=\"#b37\">Oliphant, 2007)</ref> for developing the tools that enabled this work. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b17\">[18]</ref> such as U-Net <ref type=\"bibr\" target=\"#b23\">[24]</ref>, DeepMedic <ref type=\"bibr\" target=\"#b12\">[13]</ref> and holistically nested networks <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>, more extensive connections <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and more sophisticated forms of convolution <ref type=\"bi ef type=\"bibr\" target=\"#b55\">[56]</ref>, ResNet <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DenseNet <ref type=\"bibr\" target=\"#b32\">[33]</ref>, HRNet <ref type=\"bibr\" target=\"#b61\">[62]</ref>, and Effi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8 0.1372 DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  information, concept extraction and taxonomy construction <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> have been studied for identifying field structures. Algorith. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce words to appear at specific positions <ref type=\"bibr\" target=\"#b15\">(Hokamp and Liu, 2017;</ref><ref type=\"bibr\" target=\"#b39\">Post and Vilar, 2018;</ref><ref type=\"bibr\">Hu et al., 2019)</ref>, w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  methods train image embeddings through the local relationships between images in the form of pairs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> or triplets <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ance on different graph tasks, such as node classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, graph classification <ref type=\"bibr\" target=\"#b38\">[39,</ and graph attention <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Since graph data widely exist in different real-world appl volution networks (GCNs) <ref type=\"bibr\" target=\"#b18\">[19]</ref>, graph attention networks (GATs) <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and graph isomorphism networks (GINs) <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erimental Setup</head><p>For all the experiments, we implement EGATs based on the Pytorch framework <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Because of the large memory usage for both adjacency and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d parameter (Equation <ref type=\"formula\">7</ref>). We train a word piece convolutional LM (ConvLM) <ref type=\"bibr\" target=\"#b17\">[18]</ref> on the text data set described in Section 4.1 using the sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b39\">Zhou and Wang, 2018;</ref><ref type=\"bibr\" target=\"#b8\">Huber et al., 2018;</ref><ref type=\"bibr\" target=\"#b7\">Huang et al., 2020)</ref>. As a notable work of emotional conversation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers entity tokens e 1 and e 2 as equivalent to other common word tokens t i , which has been proven <ref type=\"bibr\" target=\"#b1\">(Baldini Soares et al., 2019)</ref> to be unsuitable for RE tasks. To . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, sorting frames or video clips <ref type=\"bibr\" target=\"#b53\">[54,</ref><ref type=\"bibr\" target=\"#b96\">97,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: their dimensionality 1</p><p>\u221a to prevent small gradient values due to large dot product magnitudes <ref type=\"bibr\" target=\"#b69\">[59]</ref>. denotes the concatenation of independent attention heads . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hese are distinct from the generalized experience of stress.</p><p>The most similar work to ours is <ref type=\"bibr\" target=\"#b37\">Turcan and McKeown (2019)</ref>, our prior work publishing a dataset  \">Baselines</head><p>We present a re-implementation of the same BERTbased fine-tuning model used in <ref type=\"bibr\" target=\"#b37\">Turcan and McKeown (2019)</ref>, where this model performed best on D selected in the top 10 LIME explanations. Dreaddit is Dr, and GoEmotions is GE.</p><p>(which echoes <ref type=\"bibr\" target=\"#b37\">Turcan and McKeown (2019)</ref>'s finding that stressful data is typi a is labeled stress, with the remaining 47.4% labeled nonstress. We use the train-dev-test split of <ref type=\"bibr\" target=\"#b37\">Turcan and McKeown (2019)</ref> into 2,562 train, 276 development, an ble <ref type=\"table\" target=\"#tab_0\">1</ref>. The primary dataset we use for this work is Dreaddit <ref type=\"bibr\" target=\"#b37\">(Turcan and McKeown, 2019)</ref>, a dataset of 3,553 segments of Redd. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b12\">(Lin et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Vashishth et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Alt et al., 2019)</ref>, we first conduct a held-out evaluation to mea highly informative instance and label embeddings by exploiting BERT pre-trained model.</p><p>DISTRE <ref type=\"bibr\" target=\"#b0\">(Alt et al., 2019)</ref> A transformer-based model, GPT fine-tuned for c information from the text and improves DSRE models with additional side information from KBs. (4) <ref type=\"bibr\" target=\"#b0\">Alt et al. (2019)</ref> extended the GPT to the DSRE, and finetuned it. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the span length is 2 out of 10. 2020; <ref type=\"bibr\" target=\"#b22\">Walawalkar et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" tar  replacement strategy is also adopted in <ref type=\"bibr\" target=\"#b26\">Yun et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b20\">Uddin et al. (2021)</ref>. In situations where span length is the sam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b9\">Daum\u00e9 III, 2009;</ref><ref type=\"bibr\" target=\"#b5\">Coke et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">Littell et al., 2017)</ref>.</p><p>In this study, we examine whether  h feature vectors from previous work based on the genetic and geographic distance between languages <ref type=\"bibr\" target=\"#b22\">(Littell et al., 2017)</ref>. Results show that the extracted represe up</head><p>Typology Database: To perform our analysis, we use the URIEL language typology database <ref type=\"bibr\" target=\"#b22\">(Littell et al., 2017)</ref>, which is a collection of binary feature not necessarily require pre-existing knowledge of the typological features in the language at hand, <ref type=\"bibr\" target=\"#b22\">Littell et al. (2017)</ref> have proposed a method for inferring typo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t of user-level influence prediction models, most of which <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> consider complicate k, while external sources are assumed to be not present.</p><p>Problem 1. Social Influence Locality <ref type=\"bibr\" target=\"#b52\">[53]</ref> Social influence locality models the probability of v's ac get=\"#b53\">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The dataset is from <ref type=\"bibr\" target=\"#b52\">[53]</ref> and can be downloaded here. 7 The complete dataset contain  and the social action is defined to be whether a user retweets \"Higgs\" related tweets.</p><p>Weibo <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> Weibo 6 is the mos .</p><p>Data Preparation We process the above four datasets following the practice in existing work <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. More concretely, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: des ELMo <ref type=\"bibr\" target=\"#b2\">[3]</ref>, GPT <ref type=\"bibr\" target=\"#b3\">[4]</ref>, BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ALBERT <ref type=\"bibr\" target=\"#b5\">[6]</ref> and other mo bibr\" target=\"#b5\">[6]</ref> and other models. Among the pre-training methods mentioned above, BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref> has the most outstanding performance. The model uses masked  improving text summarization tasks. Yang Liu <ref type=\"bibr\" target=\"#b19\">[20]</ref> applied BERT <ref type=\"bibr\" target=\"#b4\">[5]</ref> to extractive summaries for the first time, the experimental. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) <ref type=\"bibr\" target=\"#b17\">(Sutskever et al., 2014)</ref> with attention paradigm <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: elow.</p><p>Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN <ref type=\"bibr\" target=\"#b15\">[16]</ref>. 3D-CNN has achieved state-of-the-art results in object cl /ref>. 3D-CNN has achieved state-of-the-art results in object classification on tridimensional data <ref type=\"bibr\" target=\"#b15\">[16]</ref>. 3D-CNN not only extracts features from each image frame, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  they have recently been shown to be particularly unstable to adversarial perturbations of the data <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, very small and often imperceptible perturbations e classifier, it seems that the adversarial perturbations are generalizable across different models <ref type=\"bibr\" target=\"#b17\">[18]</ref>. This can actually become a real concern from a security p of the relevant work. The phenomenon of adversarial instability was first introduced and studied in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The authors estimated adversarial examples by solving pen explaining the presence of adversarial examples. Unfortunately, the optimization method employed in <ref type=\"bibr\" target=\"#b17\">[18]</ref> is time-consuming and therefore does not scale to large da he training procedure that allows to boost the robustness of the classifier. Notably, the method in <ref type=\"bibr\" target=\"#b17\">[18]</ref> was applied in order to generate adversarial perturbations he proposed DeepFool approach to stateof-the-art techniques to compute adversarial perturbations in <ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The method in ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The method in <ref type=\"bibr\" target=\"#b17\">[18]</ref> solves a series of penalized optimization problems to find ver that the proposed approach also yields slightly smaller perturbation vectors than the method in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The proposed approach is hence more accurate in detecting mplexity aspect, the proposed approach is substantially faster than the standard method proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, while the approach <ref type=\"bibr\" target=\"#b17  standard method proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, while the approach <ref type=\"bibr\" target=\"#b17\">[18]</ref> involves a costly minimization of a series of objective fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b19\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b9\">Gehring et al., 2017;</ref><ref type=\"bibr\" target=\"#b36\">Vaswani et al., 2017b;</ref><ref type=\"bibr\" target=\"#b32\">Shen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b1\">[2]</ref> based controller to update the memory, while Working Memory Network (W-MemNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses a multi-head attention <ref type=\"bibr\" target=\"#b25\"  <ref type=\"bibr\" target=\"#b25\">[26]</ref>, which is similar to that used in Working Memory Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Multi-head attention allows the model to jointly attend t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: thms (EAs) and genetic algorithms (GAs) <ref type=\"bibr\" target=\"#b23\">(Nicolaou et al., 2012;</ref><ref type=\"bibr\" target=\"#b4\">Devi et al., 2015;</ref><ref type=\"bibr\" target=\"#b12\">Jensen, 2019;</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\">(PBMT, Wubben et al., 2012;</ref><ref type=\"bibr\" target=\"#b21\">Narayan and Gardent, 2014;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2016)</ref> or neural machine translation (NMT, <ref type= olingual phrase-based machine translation <ref type=\"bibr\" target=\"#b36\">(Wubben et al., 2012;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2016)</ref>. Further, syntactic information was also consi al., 2012)</ref>, which re-ranks sentences generated by PBMT for diverse simplifications; SBMT-SARI <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref>, which uses an external paraphrasing database omatic evaluation on the Newsela and WikiLarge datasets, respectively.</p><p>We use the SARI metric <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref> to measure the simplicity of the generated se s not improve performance, as it is known that WikiLarge does not focus on syntactic simplification <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref>. The best performance for this experiment is  =\"#b24\">(Papineni et al., 2002)</ref> to measure the closeness between a candidate and a reference. <ref type=\"bibr\" target=\"#b38\">Xu et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b32\">Sulem et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , <ref type=\"bibr\" target=\"#b5\">Chen et al., 2017;</ref><ref type=\"bibr\">Yang et al., 2019a,b;</ref><ref type=\"bibr\" target=\"#b30\">Nie et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Min et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>We employ three types of regularization during training:</p><p>Residual Dropout We apply dropout <ref type=\"bibr\" target=\"#b26\">[27]</ref> to the output of each sub-layer, before it is added to the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bib \">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bib \">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the joint representations. A recent model, based on Contrastive Language-Image Pre-training (CLIP) <ref type=\"bibr\" target=\"#b33\">[34]</ref>, learns a multi-modal embedding space, which may be used t ction \u2206i.</p><p>From natural language to \u2206t In order to reduce text embedding noise, Radford et al. <ref type=\"bibr\" target=\"#b33\">[34]</ref> utilize a technique called prompt engineering that feeds s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ss in entity type and new fact predictions <ref type=\"bibr\" target=\"#b16\">[Nickel et al., 2012</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016</ref><ref type=\"bibr\" target=\"#b3\">, Dettmer bedding methods,viz., ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> and ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>, which do not make use of any ontologi raining can be done for the refinement task with a negative log-likelihood loss function as follows <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>.</p><formula xml:id=\"formula_0\">L(G) = ods can also be used to predict type labels of entities (the typeOf relation). We work with ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3 ., subtype and subproperty information-and also shows that state-of-the-art embeddings like ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>, <ref type=\"bibr\">SimplE [Kazemi and P valuate the performance of TypeE-X models in the KG refinement task, and compare them with Com-plEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3 \" target=\"#b15\">[Nickel et al., 2011</ref><ref type=\"bibr\" target=\"#b21\">, Socher et al., 2013</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016]</ref>.</p><p>An important step in learning . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -supervised learning <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b77\">76,</ref><ref type=\"bibr\" tar contrastive learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b46\">45,</ref><ref type=\"bibr\" tar /ref><ref type=\"bibr\" target=\"#b77\">76,</ref><ref type=\"bibr\" target=\"#b79\">78]</ref>. Iscen et al. <ref type=\"bibr\" target=\"#b34\">[33]</ref> mine hard negatives from a large set by focusing on the fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Research shows that most queries issued by users are short <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uestion answering <ref type=\"bibr\" target=\"#b22\">(Yu et al., 2017)</ref>, knowledge base population <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, and biomedical knowledge discovery <ref t  our group compared (1) and ( <ref type=\"formula\" target=\"#formula_1\">2</ref>) with sequence models <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, and we report these results; for (3) we r TM), and showed that it outperforms several CNN and dependency-based models by a substantial margin <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>. We compare with this strong baseline, and etup</head><p>We conduct experiments on two relation extraction datasets: (1) TACRED: Introduced in <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, TACRED contains over 106k mention pairs d f the two entities.</p><p>More recently, <ref type=\"bibr\" target=\"#b0\">Adel et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b25\">Zhang et al. (2017)</ref> have shown that relatively simple neural mo la_1\">2</ref> For fair comparisons on the TACRED dataset, we follow the evaluation protocol used in <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017</ref>) by selecting the model with the median dev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> and FPGAs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><p>Activation unit is a non-linear function that some l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed with the supply and demand for cloud resources. Thus, unlike prior works on bidding optimization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, our model not only explicitly accounts for the interplay b old on 2017 spot data (see Figure <ref type=\"figure\" target=\"#fig_4\">3</ref>).</p><p>The authors of <ref type=\"bibr\" target=\"#b9\">[10]</ref> used a profit-maximization model to understand spot price d y explicitly considering job deadlines <ref type=\"bibr\" target=\"#b15\">[16]</ref>, cost minimization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and task depend n-demand instances and a set B t \u2282 R + of bids from B t = |B t | spot instance requests. Many works <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr f this weak assumption has basis in both previous analyses of spot markets and other auctions (e.g. <ref type=\"bibr\" target=\"#b9\">[10]</ref> assumes bids are drawn from U[\u03c0, \u03c0]) as well in the simple  ese variables by the total number of instances, i.e. define n t = N b t instead of B t ), we follow <ref type=\"bibr\" target=\"#b9\">[10]</ref> and assume that all bids b \u2208 B t are drawn independently fr family. A better strategy would be to consider the collective behavior of the spot prices over time <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which we do in this section by accounting for their tempor bly ill-posed (it tends to \u2212\u221e as b t \u2192 0), and the fact that we have constraints on the state space <ref type=\"bibr\" target=\"#b9\">(10)</ref>. Therefore, we need to resort to algorithms that support mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o achieve outstanding performance in many vision tasks, such as fully convolutional networks (FCNs) <ref type=\"bibr\" target=\"#b3\">[4]</ref>, U-Net <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepLab <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nge, inspired by the previous graph sparsification studies <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we propose a relation-based personalized PageRank (PPR) to  by previous studies <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, personalized PageRank can be utilized to preserve more eff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /p><p>Inspired by discriminative correlation filter trackers <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e strong order constraint of RNN models, CNN-based methods <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> were proposed. Some. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aints. As a result, researchers have explored software-based techniques to tolerate hardware faults <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Softwarebased techniques do not require any modification in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[22]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>, <ref type=\"bibr\" target=\"#b52\">[53]</ref>, <ref type=\"bibr\" target=\"#b53\">[54]</ref>. <ref type=\"bibr\" target=\"#b21\">[22]</ref> enhances the gr #b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>, <ref type=\"bibr\" target=\"#b53\">[54]</ref> in terms of classification accuracy. Moreover, we apply Dr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k should also be considered for a comprehensive evaluation <ref type=\"bibr\">(Lu et al., 2019c;</ref><ref type=\"bibr\" target=\"#b43\">Tan et al., 2019)</ref>. Thus, the computational complexity and the t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>One recent work has proposed a matrix-factorizationbased approach to embed uncertain networks <ref type=\"bibr\" target=\"#b9\">(Hu et al. 2017</ref>). However, it cannot be generalized to embed unc pe=\"bibr\" target=\"#b20\">(Trouillon et al. 2016)</ref>, (ii) an uncertain graph embedding model URGE <ref type=\"bibr\" target=\"#b9\">(Hu et al. 2017)</ref> Here linear stands for linear gain, and exp. st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sophisticated model architectures to better understand the complex nature of user-item interactions <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ e latency. For this reason, it is challenging to adopt such a large model to the real-time platform <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ =\"#b25\">26]</ref>.</p><p>To tackle this problem, Knowledge Distillation (KD) has been adopted to RS <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe l size.</p><p>Most existing KD methods for RS transfer the knowledge from the teacher's predictions <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe s another recent approach that transfers the latent knowledge from the teacher's intermediate layer <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, pointing out that t que. However, due to their restricted capability, the loss of recommendation accuracy is inevitable <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ only applicable to specific models or easy to fall into a local optimum because of the local search <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>.</p><p>Knowledge Dis <ref type=\"bibr\" target=\"#b4\">[5]</ref> that matches the class distributions, most existing methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe owledge of the items ranked highly by the teacher but ranked lowly by the student. On the one hand, <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> focus on distilling  ledge. Pointing out that the predictions incompletely reveal the teacher's knowledge, a few methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> <ref type=\"foot\" tar gression enables e \ud835\udc60 to capture compressed information that can restore detailed information in e \ud835\udc61 <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. <ref type=\"bibr\" ta longing to the different groups <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>The existing methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> based on the hint re nteractions (FourSquare) and remove items having fewer than 10 interactions (FourSquare) as done in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Table <ref type=\"ta ure\" target=\"#fig_0\">1b</ref>) that can restore more detailed preference information in the teacher <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>However, the existing hint regression-based methods f te layer. Note that the two groups of methods can be utilized together to fully improve the student <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>(1) KD by the predictions. Motivated by <ref type=\"bi 1]</ref> adopts this original hint regression to improve the student.</p><p>The most recent work DE <ref type=\"bibr\" target=\"#b7\">[8]</ref> further elaborates this approach for RS. DE argues that usin via the same network without being mixed with the representations belonging to the different groups <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>The existing methods <ref type=\"bibr\" target=\"#b7\">[8 ence in an end-to-end manner considering both the teacher and the student, we borrow the idea of DE <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Formally, let there exist \ud835\udc3e preference groups in the teache r\" target=\"#b5\">[6]</ref>. This group assignment is evolved during the training via backpropagation <ref type=\"bibr\" target=\"#b7\">[8]</ref>. This will be explained in Section 3.3.4.</p><p>With the ass the topology-preserving loss, the second term corresponds to the hint regression loss adopted in DE <ref type=\"bibr\" target=\"#b7\">[8]</ref> that makes the group assignment process differentiable. We p onding group, which makes the entities having strong correlations get distilled by the same network <ref type=\"bibr\" target=\"#b7\">[8]</ref>. \ud835\udefe is a hyperparameter balancing the two terms. In this work .org/ns/1.0\"><head n=\"4.1\">Experimental Setup</head><p>We closely follow the experiment setup of DE <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, for a thorough evaluation, we make two changes in  2\">[3]</ref>, which is the state-of-the-art top-\ud835\udc41 recommendation method, as a base model. 2) unlike <ref type=\"bibr\" target=\"#b7\">[8]</ref> that samples negative items for evaluation, we adopt the ful \">[20]</ref>: A KD method utilizing the original hint regression.</p><p>\u2022 Distillation Experts (DE) <ref type=\"bibr\" target=\"#b7\">[8]</ref>: The state-of-the-art KD method distilling the latent knowle RD <ref type=\"bibr\" target=\"#b21\">[22]</ref>, CD <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and RRD <ref type=\"bibr\" target=\"#b7\">[8]</ref>) in the competitors, because they are not competing with the in the competitors, because they are not competing with the methods distilling the latent knowledge <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Instead, we provide experiment results when they are combin on-based KD method. We report the results with the state-of-the-art prediction KD method (i.e., RRD <ref type=\"bibr\" target=\"#b7\">[8]</ref>) on CiteU-Like with \ud835\udf19 = 0.1 in Figure <ref type=\"figure\" tar distilling the latent knowledge (i.e., DE and HTD). This result aligns with the results reported in <ref type=\"bibr\" target=\"#b7\">[8]</ref> and shows the importance of distilling the latent knowledge. ><table /></figure> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\"><ref type=\"bibr\" target=\"#b7\">[8]</ref> proposes two KD methods: one by prediction and the other by   each user are held out for testing/validation, and the rest are used for training. However, unlike <ref type=\"bibr\" target=\"#b7\">[8]</ref> that samples a predefined number (e.g., 499) of unobserved i \u22125 }.</p><p>For the hint regression-related setup, we closely follow the setup reported in DE paper <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Specifically, two-layer MLP with [\ud835\udc51 \ud835\udc60 \u2192 (\ud835\udc51 \ud835\udc60 + \ud835\udc51 \ud835\udc61 )/2 \u2192 \ud835\udc51 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, take an assumption that there exists strong semantic corre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . The latest advances in this area extend self-supervised learning to graph representation learning <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: at some pairs of classes exhibit homophily, while others exhibit heterophily. In belief propagation <ref type=\"bibr\" target=\"#b39\">[40]</ref>, a message-passing algorithm used for inference on graphic =\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. Belief propagation (BP) <ref type=\"bibr\" target=\"#b39\">[40]</ref> is a classic messagepassing algorithm for graph-based semi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: den state of the last moment, respectively. The detailed calculation steps of (5) were presented in <ref type=\"bibr\" target=\"#b1\">(2)</ref>. Observation Encoder. The observation encoder is in charge o r loading the initial values, the prediction decoder will run following the calculation as shown in <ref type=\"bibr\" target=\"#b1\">(2)</ref>. In particular, the input and the prediction at a moment t & nfiguration of the prediction decoder. In addition, inspired by the method from machine translation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, the prediction for the moment t will be sent as the input o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as bagof-words or n-grams <ref type=\"bibr\" target=\"#b19\">(Wang and Manning, 2012)</ref> or SVMs <ref type=\"bibr\" target=\"#b16\">(Tang et al., 2015)</ref>. The neural network based methods like <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" t b0\">[1]</ref> or a supervised measure such as R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The ex \"#b25\">[26]</ref> only use the top documents in the ideal rankings while other methods such as PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> sample the training rankings by judging it through diversi ll to train the model which may lead to underfit. The quantity of the training dataset used by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> is large enough but the quality of it depends on some hype get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" t robability of optimal rankings. Based on the same score function of R-LTR, Xia et al. proposed PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> in which loss function is designed to directly maximize th uch as \ud835\udefc-NDCG and ERR-IA <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The form of \u2111 is inspired by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> method which aims to maximize the margin between positive  |\ud835\udc5e, \ud835\udc36)] .<label>(11)</label></formula><p>The loss function of the discriminator is inspired by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> method which is aiming to maximize the margin between the   approaches and implicit approaches. The implicit approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar /ref>.</p><p>Studies have shown that supervised approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nstruction. Motif, as the specific local structure involving multiple nodes, is first introduced in <ref type=\"bibr\" target=\"#b24\">[25]</ref>. It has been widely used to describe complex structures in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y and encourage source domain experts to provide more complementary and discriminative information. <ref type=\"bibr\" target=\"#b2\">(3)</ref> To make the model more generalizable to target domains, we p enting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 36]</ref>, and random forest <ref type=\"bibr\" target=\"#b28\">[26]</ref>.</p><p>Recently, Dong et al. <ref type=\"bibr\" target=\"#b9\">[7]</ref> propose a Super-Resolution Convolutional Neural Network (SRC s demonstrate that our method is faster than several CNN based super-resolution models, e.g., SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref>, SCN <ref type=\"bibr\" target=\"#b35\">[33]</ref>, VDSR <ref ty onal neural networks based SR. In contrast to modeling the LR-HR mapping in the patch space, SR-CNN <ref type=\"bibr\" target=\"#b9\">[7]</ref> jointly optimize all the steps and learn the nonlinear mappi R network <ref type=\"bibr\" target=\"#b19\">[17]</ref> demonstrates significant improvement over SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref> by increasing the network depth from 3 to 20 convolutional l er model with a fast Table <ref type=\"table\">1</ref>: Comparisons of CNN based SR algorithms: SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref>, FSRCNN <ref type=\"bibr\" target=\"#b10\">[8]</ref>, SCN <ref t ed LapSRN with 8 state-of-theart SR algorithms: A+ <ref type=\"bibr\" target=\"#b32\">[30]</ref>, SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref>, FSRCNN <ref type=\"bibr\" target=\"#b10\">[8]</ref>, SelfExSR < ge, our method reconstructs the rails without the ringing artifacts.</p><p>Ground-truth HR HR SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref> VDSR <ref type=\"bibr\" target=\"#b19\">[17]</ref> LapSRN (ours) l:id=\"fig_0\"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Network architectures of SRCNN<ref type=\"bibr\" target=\"#b9\">[7]</ref>, FSRCNN<ref type=\"bibr\" target=\"#b10\">[8]</ref>, VDSR<ref ty arget=\"#b19\">[17,</ref><ref type=\"bibr\" target=\"#b28\">26]</ref>    the protocol of existing methods <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\" target=\"#b19\">17]</ref>, we generate the LR   methods using the bicubic upsampling for pre-processing generate results with noticeable artifacts <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\" target=\"#b19\">17,</ref><ref type=\"bibr\" targ pe=\"figure\">5</ref>. For 8\u00d7 SR, it is challenging to predict HR images from bicubicupsampled images <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\" target=\"#b19\">17,</ref><ref type=\"bibr\" targ y to hallucinate the regular structure. This is a common limitation shared by parametric SR methods <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\" target=\"#b10\">8,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )</ref>. In this paper we use Checkpoint Processing and Recovery (CPR) as the baseline architecture <ref type=\"bibr\" target=\"#b1\">[2]</ref> since it has been shown to outperform conventional ROB-based rview</head><p>CPR is a ROB-free proposal for building scalable large instruction window processors <ref type=\"bibr\" target=\"#b1\">[2]</ref>. CPR addresses the scalability and performance limitations o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  parameters needed to specify 3D oriented bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t  is still relatively poor. Methods vary by how they encode and learn features from raw point cloud. <ref type=\"bibr\" target=\"#b4\">[5]</ref> uses voxels to encode the raw point cloud, and 3D CNNs (Conv d bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]</ref>, since raw LiDAR point cloud has very sparse data structure,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ional OoO core because of frequent dispatch stalls. A similar steering policy is used by Kim et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> in their Instruction-Level Distributed Processing (ILDP) wor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  this section, we consider introducing a third source of randomness, the random dropout of features <ref type=\"bibr\" target=\"#b12\">(Srivastava et al., 2014)</ref>, which is adopted in various GCN mode rom dropout (ND) of different estimators.</p><p>Our method is based on the weight scaling procedure <ref type=\"bibr\" target=\"#b12\">(Srivastava et al., 2014)</ref> to approximately compute the mean \u00b5</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oss on labeled data, thus it will face the same vulnerability issue as the standard neural networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and 2) the additional smoothness constraint will exacerba mic regularization technique that proactively simulates the perturbations during the training phase <ref type=\"bibr\" target=\"#b13\">[14]</ref>. It has been empirically shown to be able to stabilize neu ive, and then learn over these adversarial examples by minimizing an additional regularization term <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bib rding the target of the training objective. In supervised learning tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, supervised loss <ref type=\"bibr\" target=\"#b13\">[14]</ref> earning tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, supervised loss <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bib  against perturbations for a wide range of standard classification tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bib o obtain the closedform solution of r g i . Inspired by the linear approximation method proposed in <ref type=\"bibr\" target=\"#b13\">[14]</ref> for standard adversarial training, we also design a linear um players Approximation. For labeled nodes, r \u2032 i can be easily evaluated via linear approximation <ref type=\"bibr\" target=\"#b13\">[14]</ref>, i.e., calculating the gradient of D(f (x i , G| \u0398), \u1ef9i )  aphSGAN) in the training phase. Moreover, the results are consistent with findings in previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rs and enlarge the distance between negative sample pairs. As suggested by distance metric learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>, a sufficient number of negative sample pairs are required t imilarity between inter-class samples.</p><p>Triplet loss. Triplet loss was proposed by Ding et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and Schroff et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref> ance comparison <ref type=\"bibr\" target=\"#b50\">[50]</ref>, triplet loss was proposed by Ding et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> and Schroff et al. <ref type=\"bibr\" target=\"#b32\">[32]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on provides a way for users to continuously control the information that is included in the summary <ref type=\"bibr\" target=\"#b0\">(Bornstein et al., 1999;</ref><ref type=\"bibr\" target=\"#b23\">Leuski et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ations. Following this paradigm, a large number of social recommendation models have been developed <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, are performed to maintain a manageable balance between fo ves, focusing all attention on the hard negative examples.</p><p>Online Hard Example Mining (OHEM): <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposed to improve training of two-stage detectors by con  hard example mining <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>In this paper, we propose a new loss function that a rous extensions to this framework have been proposed, e.g. <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar rget=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> that samples hard e nt performance saturates. (d) FL outperforms the best variants of online hard example mining (OHEM) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> by over 3 points A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ss on non-autoregressive sequence generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019)</ref>. 1  Specifically, the Levenshtein T  maximum number of decoding steps is reached <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019)</ref>.<ref type=\"foot\" target=\"#foot_3\">5 n et al., 2018)</ref> or multi-pass decoding <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Gu et s by iteratively editing the outputs from previous iterations. Edit operations such as substitution <ref type=\"bibr\" target=\"#b20\">(Ghazvininejad et al., 2019)</ref> and insertion-deletion <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: te audio-and symbolic-domain music information retrieval (MIR) techniques such as downbeat tracking <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target rst beat in each bar. The same model is used to track the beat positions to create the Tempo events <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We obtain the tick positions between beats by linear interp limitations.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\"><ref type=\"bibr\" target=\"#b1\">2</ref> Therefore, the model generates the pitch, velocity (dynamics),. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>Graph embedding <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> aims to project nodes in a graph into a low-dimensional vec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b13\">Yang et al. (2017)</ref>  <ref type=\"bibr\" target=\"#b13\">[14]</ref> and <ref type=\"bibr\" target=\"#b12\">Wang et al. (2016)</ref>  <ref type=\"bibr\" target=\"#b12\">[13]</ref> e f type=\"bibr\" target=\"#b13\">[14]</ref> and <ref type=\"bibr\" target=\"#b12\">Wang et al. (2016)</ref>  <ref type=\"bibr\" target=\"#b12\">[13]</ref> employ a two-stage approach,</p><formula xml:id=\"formula_3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  language modeling with an unsupervised training objective has been investigated by multiple groups <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Alley et al., </p><p>A line of work in this emerging field proposes the Transformer for protein language modeling <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019;</ref><ref type=\"bibr\">Rao et al., 2019)</ref>. O ures across a variety of tasks, but were not able to show a benefit in the fully end-to-end setting <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019;</ref><ref type=\"bibr\">Rao et al., 2019;</ref><re learn underlying intrinsic properties of proteins such as structure and function from sequence data <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019)</ref>.</p><p>A line of work in this emerging fie d n=\"4.3\">TRANSFORMERS</head><p>We evaluate several pre-trained Transformer models, including ESM-1 <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019)</ref>, ProtBert-BFD <ref type=\"bibr\" target=\"#b1 g. A similar scaling law has been observed previously for supervised secondary structure prediction <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019)</ref>, and parallels observations in the NLP com ns/1.0\"><head>A.4 ESM-1 IMPLEMENTATION DETAILS</head><p>The original ESM-1 models were described in <ref type=\"bibr\" target=\"#b36\">(Rives et al., 2019)</ref>. ESM-1 is trained on Uniref50 in contrast  t in developing similar models for proteins <ref type=\"bibr\" target=\"#b2\">(Alley et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Rives et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Heinzinger et ref type=\"bibr\">Rao et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Elnaggar et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b36\">Rives et al. (2019)</ref> were the first to study protein Transformer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b29\">[30]</ref> to detect object boundingboxes from each image. We further utilize EfficientNet L2 <ref type=\"bibr\" target=\"#b31\">[32]</ref> to extract image features for computation efficiency. By a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ress prefixes in the large-scale and dynamic Internet. Tag-based inter-AS source address validation <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> shows high accur  packets and provide sufficient benefits for deployers.</p><p>However, existing tag-based solutions <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> are subject to t ch other, which is vulnerable to Man-in-the-Middle (MitM) attacks. And for solutions similar to SPM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, whose tags are fixed random strings. If attackers sniff le lgorithms for tag generation which is of low efficiency or constant tag which is of low security T1 <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. In this section scheme <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The third type is the tag-based method, such as SPM <ref type=\"bibr\" target=\"#b9\">[10]</ref> and Passport <ref type=\"bibr\" target=\"#b10\">[11]</ref>. As  tions filer packets with spoofed source IP addresses by stamping and verifying tags in packets. SPM <ref type=\"bibr\" target=\"#b9\">[10]</ref> applies fixed random strings as tags, which is simple and e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: are control flow checking is to partition the program into basic blocks (branch-free parts of code) <ref type=\"bibr\" target=\"#b14\">[14]</ref>. For each block a deterministic signature is calculated an > and On-line control flow error detection using relationship signatures among basic blocks (RSCFC) <ref type=\"bibr\" target=\"#b14\">[14]</ref>.</p><p>ECCA, firstly, assigns a unique prime number identi rget=\"#b15\">[15]</ref> technique to the original code, \uf06c a safe one, obtained by applying the RSCFC <ref type=\"bibr\" target=\"#b14\">[14]</ref> technique to the original code, \uf06c a safe one, obtained by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s <ref type=\"bibr\" target=\"#b6\">[7]</ref>, videos <ref type=\"bibr\" target=\"#b7\">[8]</ref> and music <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>A. Shirian and T. Guha are with the Department of Com. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ble research attention <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. State-of-the-art GCNs usually follow a \"message-passing\" f ion of the Laplacian matrix is avoided, thus reducing the overall time complexity. Kipf and Welling <ref type=\"bibr\" target=\"#b17\">[18]</ref> further propose to simplify the graph convolution using on  GCN methods have been proposed, here we focus on a representative one proposed by Kipf and Welling <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Here, the (l + 1) t h convolutional layer is defined as:< istributions by using our Gaussian-based Graph Convolutions.</p><p>Following the original GCN model <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we also impose L 2 regularization on parameters of the fi To evaluate the robustness of RGCN, we compare it with two state-of-the-art GCN models:</p><p>\u2022 GCN <ref type=\"bibr\" target=\"#b17\">[18]</ref>: As introduced in Section 3.2 , this is the original GCN m  methods is evaluated on a separate test set of 1000 labels. We adopt the same dataset splits as in <ref type=\"bibr\" target=\"#b17\">[18]</ref> and report the average results of 10 runs. In experiments, ectiveness of our proposed method, we adopt three citation networks commonly used in previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>: Cora, Citeseer an \"table\" target=\"#tab_0\">1</ref>.</p><p>We closely follow the experimental setting in previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Specifically, we  . In experiments, we set the number of layers as two for all methods as suggested by previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For GCN and RGCN,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  based merely on implicit feedbacks, i.e., user clicks, in the current session.</p><p>Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> apply recurrent neural networks (RNN) with Gated Recurrent al Networks (RNN) have been devised to model variable-length sequence data. Recently, Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> apply RNN to sessionbased recommendation and achieve signi ARM. We use a RNN with Gated Recurrent Units (GRU) rather than a standard RNN because Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> demonstrate that GRU can outperform the Long Short-Term Me re S i , To learn the parameters of the model, we do not utilize the proposed training procedure in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, where the model is trained in a session-parallel, sequenc sessions of subsequent week for testing. Because we did not train NARM in a session-parallel manner <ref type=\"bibr\" target=\"#b11\">[12]</ref>, a </p><formula xml:id=\"formula_15\">],V (x 2 ), ([x 1 , x  nt representations when computing recommendation scores. \u2022 GRU-Rec: We denote the model proposed in <ref type=\"bibr\" target=\"#b11\">[12]</ref> as GRU-Rec, which utilizes session-parallel mini-batch tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he transferability <ref type=\"bibr\" target=\"#b11\">[10,</ref><ref type=\"bibr\" target=\"#b20\">19,</ref><ref type=\"bibr\" target=\"#b8\">7]</ref>, an adversarial example is usually generated for a single inp box attacks than FGSM at the cost of worse transferability <ref type=\"bibr\" target=\"#b17\">[16,</ref><ref type=\"bibr\" target=\"#b8\">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) =\"#b20\">[19]</ref> of adversarial examples can be used to attack a black-box model. Several methods <ref type=\"bibr\" target=\"#b8\">[7,</ref><ref type=\"bibr\" target=\"#b38\">37]</ref> have been proposed t rg/ns/1.0\"><head n=\"3.2.\">Translation-Invariant Attack Method</head><p>Although many attack methods <ref type=\"bibr\" target=\"#b8\">[7,</ref><ref type=\"bibr\" target=\"#b38\">37]</ref> can generate adversa ref type=\"bibr\" target=\"#b8\">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type=\"bibr\" target=\"#b8\">[7]</ref> proposes to improve the transferability of adversarial examp formula_13\">)</formula><p>The translation-invariant method can be similarly integrated into MI-FGSM <ref type=\"bibr\" target=\"#b8\">[7]</ref> and DIM <ref type=\"bibr\" target=\"#b38\">[37]</ref> as TI-MI-F ) <ref type=\"bibr\" target=\"#b11\">[10]</ref>, momentum iterative fast gradient sign method (MI-FGSM) <ref type=\"bibr\" target=\"#b8\">[7]</ref>, and diverse inputs method (DIM) <ref type=\"bibr\" target=\"#b target=\"#b6\">[5]</ref> since that they are not good at generating transferable adversarial examples <ref type=\"bibr\" target=\"#b8\">[7]</ref>. We denote the attacks combined with our translation-invaria  more likely to transfer to another black-box model.</p><p>We adopt the ensemble method proposed in <ref type=\"bibr\" target=\"#b8\">[7]</ref>, which fuses the logit activations of different models. We a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar ><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. Wigderson and Xiao <ref type=\"bibr\" target=\"#b52\">[53]</ref> conjectured that Chernoff bounds can be generalized to bot. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b77\">Zhang et al., 2017)</ref>, Landsat <ref type=\"bibr\" target=\"#b15\">(Dong et al., 2016;</ref><ref type=\"bibr\" target=\"#b43\">Oliphant et al., 2019;</ref><ref type=\"bibr\" target=\"#b50\">Qiu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on operation is set to 2, then all the potential words can easily fuse into corresponding positions <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_ tional efficiency. In general, end-to-end CNNs in NLP have mainly been used for text classification <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref>. For sequence labeling tasks, CNNs have been mainly  state \u2190 \u2212 h w i , which are concatenated for the NER prediction.</p><p>CNN. We apply a standard CNN <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref> structure on the character or word sequence to obtai 0\"><head n=\"4.3\">Hyper-Parameter Settings</head><p>For all four of the datasets, we used the Adamax <ref type=\"bibr\" target=\"#b4\">[Kingma and Ba, 2014]</ref> optimization to train our networks. The in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ent-based (GB) search; 2) reinforcement learning (RL) search; and 3) evolutionary algorithms (EAs). <ref type=\"bibr\" target=\"#b38\">Real et al. (2019)</ref> conducted a case study of the different sear arch using deep CNNs is attracting more and more attention in the artificial intelligence community <ref type=\"bibr\" target=\"#b38\">(Real et al., 2019;</ref><ref type=\"bibr\" target=\"#b44\">Wang et al. 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ad><p>We formalize our notion of spatial correlation similar to prior studies of spatial footprints <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. We define a spatial ed in hardware by correlating patterns with the code and/or data address that initiates the pattern <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Whereas existing sp  tracking of spatial correlation. We show that the cache-coupled structures used in previous work ( <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>) are suboptimal for  dex consistently provides the most accurate predictions when correlation table storage is unbounded <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. By combining both q lications, PC+address indexing can be approximated by combining the PC with a spatial region offset <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. The spatial region  =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.\">Indexing</head><p>Prior studies of spatial predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> advocate predictor i  coverage and/or fragment prediction entries, consequently polluting the PHT.</p><p>Past predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> couple the predictor eas existing spatial pattern prefetching designs are effective for desktop/engineering applications <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the only practical implementation evaluated on server workl e highest coverage.</p><p>For scientific applications, we corroborate the conclusions of prior work <ref type=\"bibr\" target=\"#b3\">[4]</ref> that indicate PC+offset indexing generally approaches the pe led sectored cache <ref type=\"bibr\" target=\"#b21\">[22]</ref>, whereas the spatial pattern predictor <ref type=\"bibr\" target=\"#b3\">[4]</ref> provided a logical sectored-cache tag array alongside a trad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#b2\">[3]</ref>, optimal branching <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and minimum-cost flow <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Mao et al. used reinforcement learning to organize the hype. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b91\">94,</ref><ref type=\"bibr\" target=\"#b55\">58,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b60\">63]</ref>. Here we make an attempt to actually find this structure. W arget=\"#b91\">94,</ref><ref type=\"bibr\" target=\"#b55\">58,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b60\">63]</ref>. Unlike multi-task learning, we explicitly model the relati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: roposed solutions for different aspects of the problem <ref type=\"bibr\" target=\"#b119\">[120]</ref>, <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref>. In particu arget=\"#b121\">[122]</ref>. In particular, sampling results have been generalized to directed graphs <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref> and to othe ng set selection from an experiment design perspective <ref type=\"bibr\" target=\"#b123\">[124]</ref>, <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref> setting as  ge-scale graphs. Some techniques require computing and storing the first K basis vectors of the GFT <ref type=\"bibr\" target=\"#b120\">[121]</ref>. For larger graph sizes, where this may not be practical  always lead to performance comparable to those of more complex greedy optimization methods such as <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref>.</p><p>Give. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>: A learing-to-rank recommender that adopts MF <ref type=\"bibr\" target=\"#b12\">[13]</ref> to model the user-item interaction. BPR uses the pair-wise. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvised node classification and graph classification <ref type=\"bibr\" target=\"#b41\">(Zhu, 2005;</ref><ref type=\"bibr\" target=\"#b31\">Shervashidze et al., 2011;</ref><ref type=\"bibr\" target=\"#b24\">L\u00fc &am. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e, we employ smoothed linear interpolation <ref type=\"bibr\" target=\"#b3\">(Bowman et al., 2016;</ref><ref type=\"bibr\" target=\"#b59\">Zheng et al., 2019)</ref> between sentences in the embedding space to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p>Among the studies that report results on this database, Jaiswal et. al. used the OpenFace toolkit <ref type=\"bibr\" target=\"#b72\">[72]</ref> to extract visual features and the OpenSmile toolkit <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: zation (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualizati ted languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" ircle class at runtime, the compiler can convert the indirect call to multiple guarded direct calls <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b47\">48]</ref> target optimizations towards a single component such as CPU <ref type=\"bibr\" target=\"#b48\">[49]</ref><ref type=\"bibr\" target=\"#b49\">[50]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in sparse high-dimensional feature vectors of existing entities in a vocabulary. Recent approaches <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> employ anchor-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  They provide low deployment incentives to deployers' ASes <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Therefore, a number of solutions <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cation, because the attacks don't necessarily interfere with the program execution itself. Meltdown <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a classic example. Meltdown operates as a cache side-ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> are hampered by typically lower input data resolution than v data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> uses PointNets <ref type=\"bibr\" target=\"#b6\">[7]</ref> in an  The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RC is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> publish their training configurations for class pedestrian a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igner. We call this method pronunciation-assisted sub-word modeling (PASM), which adopts fast align <ref type=\"bibr\" target=\"#b8\">[9]</ref> to align a pronunciation lexicon arXiv:1811.04284v2 [cs.CL] . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. The methods based on spectral domain adopt the spectral re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b39\">[44]</ref>, PRETZEL <ref type=\"bibr\" target=\"#b33\">[38]</ref>, Clipper <ref type=\"bibr\" target=\"#b20\">[25]</ref>, TF-serving <ref type=\"bibr\" target=\"#b37\">[42]</ref>, etc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g loops of neural networks, blurring different dimensions of the problem. By contrast -similarly to <ref type=\"bibr\" target=\"#b35\">Parshakova et al. (2019a;</ref><ref type=\"bibr\">b)</ref> in a differe =\"bibr\" target=\"#b23\">(Kim &amp; Bengio, 2016;</ref><ref type=\"bibr\" target=\"#b34\">Owen, 2013;</ref><ref type=\"bibr\" target=\"#b35\">Parshakova et al., 2019a</ref>) SNIS consists in computing:</p><formu et=\"#b4\">Belanger &amp; McCallum, 2016)</ref>. Some current applications to text generation include <ref type=\"bibr\" target=\"#b35\">Parshakova et al. (2019a)</ref> and <ref type=\"bibr\" target=\"#b14\">De. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: onal tools such as <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. During this emulation, we dump the input and outputs consu ssing (across various game execution threads) by instrumenting the emulator to record memory traces <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> along with additio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rove the generalization of neural networks. Mixup approaches are categorized into input-level mixup <ref type=\"bibr\" target=\"#b26\">(Yun et al., 2019;</ref><ref type=\"bibr\">Kim et al.,</ref> Figure <re s with a span in another text, which is inspired from CutMix arXiv:2106.08062v1 [cs.CL] 15 Jun 2021 <ref type=\"bibr\" target=\"#b26\">(Yun et al., 2019)</ref>, to preserves the locality of two source tex  different span length would be too complex. This same-size replacement strategy is also adopted in <ref type=\"bibr\" target=\"#b26\">Yun et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b20\">Uddin et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> or external knowledge graphs (KGs) <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to compensate the  ef type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> and knowledge graphs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to enhance the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trated its effectiveness on folding a single protein chain <ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t l information and pairwise potential between any two columns in a paired MSA. See our previous work <ref type=\"bibr\" target=\"#b19\">[20]</ref> for details.</p></div> <div xmlns=\"http://www.tei-c.org/ns atenated sequences and our training proteins. Please see our previous work on training our DL model <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> 42]</ref>.</p><p>We have developed a deep learning (DL) method for intra-protein contact prediction <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, which greatly out  which predicts the probability of any two residues forming a contact. Please see our previous work <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> for a detailed des. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 34\">[35,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ghborhoods on the graph, naturally capturing both graph structures as well as node or edge features <ref type=\"bibr\" target=\"#b47\">(Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b42\">Wu et  bibr\" target=\"#b42\">(Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b2\">Battaglia et al. 2018;</ref><ref type=\"bibr\" target=\"#b47\">Zhang, Cui, and Zhu 2020;</ref><ref type=\"bibr\" target=\"#b48\">Zhou et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  search algorithms <ref type=\"bibr\" target=\"#b18\">[16,</ref><ref type=\"bibr\" target=\"#b22\">20,</ref><ref type=\"bibr\" target=\"#b19\">17]</ref>. For example, DARTS <ref type=\"bibr\" target=\"#b19\">[17]</re ref type=\"bibr\" target=\"#b22\">20,</ref><ref type=\"bibr\" target=\"#b19\">17]</ref>. For example, DARTS <ref type=\"bibr\" target=\"#b19\">[17]</ref> proposed a differentiable search strategy that does not re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>, typically implemented in a contrastive learning framework  get=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b68\">69,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref> have shown great results. Among them, contrastive-based ins get=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b70\">71</ref>] is the most prominent example. In this case, a convnet is t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tional drug discovery because conformations determine biological, chemical, and physical properties <ref type=\"bibr\" target=\"#b15\">[Guimaraes et al., 2012</ref><ref type=\"bibr\" target=\"#b41\">, Sch\u00fctt . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b20\">[21]</ref> and the introduction of the TAGE predictor by Seznec and Michaud <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In practice, on the traces distributed for the first two  e predictors targeting special categories of branches with a state-of-the-art main predictor (TAGE, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, OGEHL <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Piecewis L <ref type=\"bibr\" target=\"#b7\">[8]</ref>), e.g. a loop predictor with the TAGE predictor in L-TAGE <ref type=\"bibr\" target=\"#b25\">[26]</ref> or the address-branch correlator in <ref type=\"bibr\" targe ns/1.0\"><head n=\"3.\">Background on the TAGE Predictor</head><p>The TAGE predictor was introduced in <ref type=\"bibr\" target=\"#b25\">[26]</ref> and is the core predictor of the L-TAGE predictor that won h a single 4-bit counter USE_ALT_ON_NA was found to allow to (slightly) improve prediction accuracy <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The prediction computation algorithm is as follows:</p><p ts showed that one can use wider tag for long histories for a better tradeoff. Previous experiments <ref type=\"bibr\" target=\"#b25\">[26]</ref> have shown that the TAGE predictor performs efficiently on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b23\">[24]</ref> and neural networks <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. For learning the univariate scoring function, Deep Structu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The latter components stabilizes the evolution of graph s  target=\"#b39\">40]</ref>), GNNGUARD determines importance weights using theory of network homophily <ref type=\"bibr\" target=\"#b13\">[14]</ref>, positing that similar nodes (i.e., nodes with similar fea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: abulous semantic segmentation models such as <ref type=\"bibr\">FCN [Long et al., 2015]</ref>, SegNet <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref>, <ref type=\"bibr\">DeepLab-v3 [Chen work is based on the classic encoderdecoder network architecture without the fully-connected layers <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref> and we improve it by adding the re semantic segmentation networks -FCN <ref type=\"bibr\" target=\"#b7\">[Long et al., 2015]</ref>, SegNet <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref>, and DeepLab-V3 <ref type=\"bibr\" t r network with that of <ref type=\"bibr\">FCN-32s, FCN-16s, FCN-8s [Long et al., 2015]</ref>, Seg-Net <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017], and</ref><ref type=\"bibr\">DeepLab-v3 [C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e proposed method naturally extends our previous work of unsupervised information network embedding <ref type=\"bibr\" target=\"#b26\">[27]</ref> and first learns a low dimensional embedding for words thr e network is embedded into a low dimensional vector space that preserves the second-order proximity <ref type=\"bibr\" target=\"#b26\">[27]</ref> between the vertices in the network. The representation of is suitable for arbitrary types of information networks: undirected or directed, binary or weighted <ref type=\"bibr\" target=\"#b26\">[27]</ref>. The LINE model optimizes an objective function which aims vious work, we introduced the LINE model to learn the embedding of large-scale information networks <ref type=\"bibr\" target=\"#b26\">[27]</ref>. LINE is mainly designed for homogeneous networks, i.e., n l for embedding bipartite networks. The essential idea is to make use of the second-order proximity <ref type=\"bibr\" target=\"#b26\">[27]</ref> between vertices, which assumes vertices with similar neig jective (3) can be optimized with stochastic gradient descent using the techniques of edge sampling <ref type=\"bibr\" target=\"#b26\">[27]</ref> and negative sampling <ref type=\"bibr\" target=\"#b17\">[18]< descent in learning network embeddings. For the detailed optimization process, readers can refer to <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>The embeddings of the word-word, word-document, and  (4) is to merge the all the edges in the three sets Eww, E wd , E wl and then deploy edge sampling <ref type=\"bibr\" target=\"#b26\">[27]</ref>, which samples an edge for model updating in each step, wi 0]</ref>.</p><p>\u2022 LINE: the large-scale information network embedding model proposed by Tang et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref>. We use the LINE model to learn unsupervised embeddings wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: blished work on MCTS, to provide the reader Fig. <ref type=\"figure\">1</ref>. The basic MCTS process <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>with the tools to solve new problems using MCTS and basic MCTS process is conceptually very simple, as shown in Figure <ref type=\"figure\">1</ref> (from <ref type=\"bibr\" target=\"#b16\">[17]</ref>). A tree 1 is built in an incremental and asymmetric manne t two moves and uses LGR-1 if there is no LGR-2 entry for the last two moves.</p><p>Baier and Drake <ref type=\"bibr\" target=\"#b16\">[17]</ref> propose an extension to LGR-1 and LGR-2 called Last Good R ests using the Last Good Reply heuristic (6.1.8) to inform simulations, modified by Baier and Drake <ref type=\"bibr\" target=\"#b16\">[17]</ref> to include the forgetting of bad moves. Most programs use . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b3\">Hawkes (1971b</ref><ref type=\"bibr\" target=\"#b4\">Hawkes ( ,a, 1972))</ref>; <ref type=\"bibr\" target=\"#b5\">Hawkes and Oakes (1974)</ref>.</p><p>qq q q q q q qq q q q q qq q qq q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[28]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib r model with the other five state-of-the-art approaches( <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib arget=\"#b36\">[37]</ref> uses the combination of GoogleNet feature and 3D-CNN feature. S2VT-rgb-flow <ref type=\"bibr\" target=\"#b29\">[30]</ref> uses the two-stream features consisting of RGB feature ext. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: figuration of human body structure. This representation is derived from the principle, published in <ref type=\"bibr\" target=\"#b8\">(Johansson, 1973)</ref>, explaining how humans observe actions. This w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, and knowledge distillation <ref type=\"bibr\" target=\"#b9\">[10 ref>). This structure is very similar to multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, in which different supervised tasks share the same input, as nowledge obtained from each task can be reused by the others <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. However, it is not u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b20\">[21]</ref>, 3D convolution neural networks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b24\">[25]</ref> have shown stro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b30\">[31]</ref> to capture all these information over a long period. Yao et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> at-tempt to dynamically select multiple visual representat ssification. The extracted features {v i } n i=1 form the original video representation. Similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref>, temporal soft-attention Attend is used to select visual i tion Attend is used to select visual information most related to each word. But very different from <ref type=\"bibr\" target=\"#b36\">[37]</ref> using the hidden states from a LSTM decoder, we guide the  ce generator and a paragraph generator. To emphasize the mapping from video to sentence, Yao et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> propose a temporal attention model to align the most relev l></formula><p>where W r , U \u03b1 , b \u03b1 , and w are the parameters to be learned.</p><p>Different from <ref type=\"bibr\" target=\"#b36\">[37]</ref>, here we incorporate the content read from multimodal memo o add masks to both sentences and visual features for the convenience of batch training. Similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref>, the sentences with length larger than 30 in MSVD and the  #b25\">[26]</ref>, respectively. Table <ref type=\"table\">3</ref>. The performance comparison with SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> using different visual features on MSR-VTT. Here V and C d t approaches( <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib 13]</ref> and GoogleNet <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Among these compared methods, SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> is the most similar method to ours, which also has an atte ltiple visual feature fusion, we compare our model with the other five state-of-the-art approaches( <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib rget=\"#b1\">[2]</ref>). The comparison results are shown in Table <ref type=\"table\">2</ref>. SA-G-3C <ref type=\"bibr\" target=\"#b36\">[37]</ref> uses the combination of GoogleNet feature and 3D-CNN featu  pairs. Considering that there are few methods tested on this dataset, we compare our model with SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> which is the most similar work to ours. Similarly, we perf <ref type=\"figure\">2</ref> illustrates several descriptions generated by our M 3 -google, SA-google <ref type=\"bibr\" target=\"#b36\">[37]</ref> and human-annotated ground truth on the test set of MSVD.   method. Fig. <ref type=\"figure\">3</ref> shows the attention shift of our M 3 -google and SA-google <ref type=\"bibr\" target=\"#b36\">[37]</ref> across multiple frames when generating the sentence. There about 40 sentences. So there are about 80,000 video-description pairs. Following the standard split <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, we divide the ori. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tation is based on Z-MERT <ref type=\"bibr\" target=\"#b28\">(Zaidan, 2009)</ref>, and we use FastAlign <ref type=\"bibr\" target=\"#b8\">(Dyer et al., 2013)</ref> for word alignment within the joint refineme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l correlation similar to prior studies of spatial footprints <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. We define a spatial region as a fixed-size portion of the  with the code and/or data address that initiates the pattern <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Whereas existing spatial pattern prefetching designs are e ow that the cache-coupled structures used in previous work ( <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>) are suboptimal for observing spatial correlation. Accesses rate predictions when correlation table storage is unbounded <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. By combining both quantities, which we call PC+address ind pproximated by combining the PC with a spatial region offset <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. The spatial region offset of a data address is the distanc \"4.2.\">Indexing</head><p>Prior studies of spatial predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> advocate predictor indices that include address information tries, consequently polluting the PHT.</p><p>Past predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> couple the predictor training structure to a sectored (i.e. y practical implementation evaluated on server workloads provides less than 20% miss rate reduction <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>In this paper, we reconsider prediction and streami is work demonstrates: \u2022 Effective spatial correlation and prediction. Contrary to previous findings <ref type=\"bibr\" target=\"#b16\">[17]</ref>, address-based correlation is not needed to predict the ac region generation is defined can significantly impact the accuracy and coverage of spatial patterns <ref type=\"bibr\" target=\"#b16\">[17]</ref>. A generation must be defined to ensure that, when SMS str t distinguish among distinct access patterns to different data structures by the same code (e.g.,   <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which indicated that PC+address provides superior coverag  experience worse conflict behavior. To mitigate this disadvantage, the spatial footprint predictor <ref type=\"bibr\" target=\"#b16\">[17]</ref> employed a decoupled sectored cache <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  systems. A more detailed description of the protocol and a proof of its security were presented in <ref type=\"bibr\" target=\"#b12\">[14]</ref>, though the published version of that paper did not presen s modifications to the group in the PVL.</p><p>A proof of the consistency protocol was presented in <ref type=\"bibr\" target=\"#b12\">[14]</ref>, but is beyond the scope of this paper. At a high level, h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b21\">22]</ref>, there are several attempts to adopt GNNs to learn with heterogeneous networks <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta ntly, studies have attempted to extend GNNs for modeling heterogeneous graphs. Schlichtkrull et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> propose the relational graph convolutional networks (RGCN)  heterogeneous GNNs as baselines, including:</p><p>\u2022 Relational Graph Convolutional Networks (RGCN) <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which keeps a different weight for each relationship, i.e  refers to HGT +RT E +H e t e r .</p><p>GNN Models GCN <ref type=\"bibr\" target=\"#b8\">[9]</ref> RGCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> GAT <ref type=\"bibr\" target=\"#b21\">[22]</ref> HetGNN <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing (AT) procedure <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> shows promising res al training (e.g., <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar unreliable for generating adversarial samples during single-step adversarial training. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that models trained using adversarial samples =\"bibr\" target=\"#b8\">9]</ref> accepted to ICLR 2018. In this direction, adversarial training method <ref type=\"bibr\" target=\"#b21\">[22]</ref>, shows promising results for learning robust deep learning ls trained using EAT are still susceptible to multi-step attacks in white-box setting. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that adversarially trained model can be made  show that over-fitting effect is the reason for failure to satisfy the criteria.</p><p>Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that it is possible to learn robust models us rameters (\u03b8) should be updated so as to decrease the loss on such adversarial samples. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> solves the maximization step by generating adversarial sam raining method <ref type=\"bibr\" target=\"#b12\">[13]</ref> and multi-step adversarial training method <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Column-1 of Fig. <ref type=\"figure\" target=\"#fig_0\">1</re  for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by <ref type=\"bibr\" target=\"#b21\">[22]</ref>. At each iteration all the clean samples in the mini-batch d for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by<ref type=\"bibr\" target=\"#b21\">[22]</ref>. At each iteration all the clean samples in the mini-batch s added to the image. In our experiments, we set \u03b1 = /steps.</p><p>Projected Gradient Descent (PGD) <ref type=\"bibr\" target=\"#b21\">[22]</ref>: Initially, a small random noise sampled from Uniform dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> work under the mech s, some baseline methods on GNN, for example, GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref> and GAT <ref type=\"bibr\" target=\"#b32\">[33]</ref>, are demonstrated to be capable of extracting features of  t can be used to generate node embeddings, e.g., GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GAT <ref type=\"bibr\" target=\"#b32\">[33]</ref> and gated graph networks <ref type=\"bibr\" target=\"#b17\">[1 get=\"#b19\">[20]</ref>.</p><p>As suggested in previous work <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, the multi-head attention can help to stabilize the trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Y t as before, with the modification that in this case we have for signal Y s (similarly for Y t ): <ref type=\"bibr\" target=\"#b19\">(20)</ref>. The rest of the proof is similar to Proof 3.</p><formula  imate inference algorithms (e.g., iterative classification <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: understanding started attracting more attentions in some close related fields such as image caption <ref type=\"bibr\" target=\"#b4\">[Li et al., 2019b]</ref>. However, due to the complexity of video unde ibr\" target=\"#b2\">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation classifier <ref type=\"bibr\" target=\"#b4\">[Li et al., 2019b]</ref> on it. We adopt almost the same operation exc \" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2016]</ref>. These methods are effective but they overlook patial-attention in image caption domain <ref type=\"bibr\" target=\"#b0\">[Anderson et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Li et al., 2019a] {h 1 , h 2 , ..., h m } where v \u2208 R n\u00d7d is the global feature extracted by a pre-trained 3D-ConvNet <ref type=\"bibr\" target=\"#b4\">[Tran et al., 2015]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ eo segment in the dataset, we uniformly sample 10 frames. And for each frame, we use a Faster R-CNN <ref type=\"bibr\" target=\"#b4\">[Ren et al., 2015]</ref> detector with ResNeXt-101 backbone to detect . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and PPV <ref type=\"bibr\" target=\"#b27\">[29]</ref>. In ICING, it requires each router to verify the optimized. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ifferent camera views. Many works on fully supervised ReID <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> have achieved quite p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  reported in Appendix A.1.</p><p>End-to-End Entity Linking (EL) For EL, we reproduce the setting of <ref type=\"bibr\" target=\"#b27\">Kolitsas et al. (2018)</ref> using the same in-domain and out-of-doma ntity mentions (a span contained in d j ) and e i \u2208 E its corresponding entity in the KB. Following <ref type=\"bibr\" target=\"#b27\">Kolitsas et al. (2018)</ref>, we considered only mentions that have e tics for 10k steps and we do model selection on the validation set. Again, following previous works <ref type=\"bibr\" target=\"#b27\">(Kolitsas et al., 2018)</ref>, we considered only mentions that have . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l., 2019)</ref> and a number of CPD methods <ref type=\"bibr\" target=\"#b0\">(Anand et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Shroff et al. y % GVP-GNN 44.9 DenseCPD <ref type=\"bibr\" target=\"#b25\">(Qi &amp; Zhang, 2020)</ref> 50.7 ProDCoNN <ref type=\"bibr\" target=\"#b35\">(Zhang et al., 2019)</ref> 40.7 SBROF <ref type=\"bibr\" target=\"#b4\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d GNN, which exist for a wide range of models <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Corso et al., xists a wide range of maximally powerful GNNs <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Corso et al., bibr\" target=\"#b47\">(Zaheer et al., 2017;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">Maron et al. h (L) w in case c (L) v = c</formula><p>(L) w <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019)</ref>, where c</p><formula xml:id=\"formula_12\">( , we know that such a function needs to exist <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Morris et al., 2019)</ref>. Therefore, it is sufficient to show that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ere is a similar problem setting in information theory called information bottleneck (IB) principle <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which aims to juice out a compressed code from the origin ng, attempts to find a short code of the input signal but preserves maximum information of the code <ref type=\"bibr\" target=\"#b23\">[24]</ref>. <ref type=\"bibr\" target=\"#b25\">[26]</ref> firstly bridges. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ted to extract information from some task-relevant regions <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cores for all documents. To get the student predicted rank for this document, we apply Weston et al <ref type=\"bibr\" target=\"#b35\">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t al., 2018)</ref>, OpenAI GPT <ref type=\"bibr\" target=\"#b15\">(Radford et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2018)</ref>. It has been shown that language modeling   target=\"#b2\">(Castro et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">de Araujo et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Fernandes et al., 2018)</ref>. The model is composed of two bidirectio ained word embeddings were explored by <ref type=\"bibr\" target=\"#b2\">Castro et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b4\">Fernandes et al. (2018)</ref> compared it to 3 other architectures. Th mming. During evaluation, the most likely sequence is obtained by Viterbi decoding. As described in <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>, WordPiece tokenization requires prediction ead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>. The resulting architecture resembles the L , we use document context for input examples instead of sentence context. Following the approach of <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref> on the SQuAD dataset, examples larger than  pment set comprised of 10% of the First HAREM training set. We use the customized Adam optimizer of <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>.</p><p>For the feature-based approach, we u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  noisy labeling, previous studies adopt multi-instance learning to consider the noises of instances <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010;</ref><ref type=\"bibr\" target=\"#b5\">H te this issue, many studies formulated relation classification as a multi-instance learning problem <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010;</ref><ref type=\"bibr\" target=\"#b5\">H the framework of reinforcement learning <ref type=\"bibr\" target=\"#b16\">(Sutton and Barto 1998;</ref><ref type=\"bibr\" target=\"#b13\">Narasimhan, Yala, and Barzilay 2016)</ref> and then predicts relation 4</ref> generated by the sentences in NYT<ref type=\"foot\" target=\"#foot_2\">5</ref> and developed by <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010)</ref>. There are 522,611 sentences, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to improve the performance of CNNs for various tasks, such as image and video classification tasks <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b33\">33]</ref>. Wang et al. <ref ty ks. The second one is the way of enhancing discriminative ability of the network. Channel attention <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> has been shown to be rate non-local operations for spatial attention in video classification. On the contrary, Hu et al. <ref type=\"bibr\" target=\"#b9\">[ 9]</ref> proposed SENet to exploit channel-wise relationships to ach N-based SR models do not consider the feature interdependencies. To utilize such information, SENet <ref type=\"bibr\" target=\"#b9\">[9]</ref> was introduced in CNNs to rescale the channelwise features f he aggregated information by global covariance pooling, we apply a gating mechanism. As explored in <ref type=\"bibr\" target=\"#b9\">[9]</ref>, the simple sigmoid function can serve as a proper gating fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , graph classification <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent and separation <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. However, this usuall. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: iological function) prediction benchmarks.</p><p>In particular, the Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b54\">(Xu et al., 2019)</ref> pre-trained by the proposed method outperform =\"bibr\" target=\"#b24\">Hu et al. (2019)</ref>, we adopt a five-layer Graph Isomorphism Network (GIN) <ref type=\"bibr\" target=\"#b54\">(Xu et al., 2019)</ref> with 300-dimensional hidden units and a mean  ton et al., 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b49\">(Velickovic et al., 2018)</ref> and GIN <ref type=\"bibr\" target=\"#b54\">(Xu et al., 2019)</ref>).</p><p>We can observe that GraphLoG outperfo bibr\" target=\"#b57\">Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b60\">Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b54\">Xu et al., 2019)</ref> sought to improve the effectiveness of these t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We generalize deep self-attention distillation in MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> by only using self-attention relation disti den size the same, layer-wisely transferring hidden states and self-attention distributions. MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> proposes deep self-attention distillation,   teacher.</p><p>In this work, we generalize and simplify deep self-attention distillation of MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> by using self-attention relation distillati the student has the same number of layers as its teacher to perform layer-wise distillation. MINILM <ref type=\"bibr\" target=\"#b40\">(Wang et al., 2020)</ref> transfers selfattention knowledge of teache \"bibr\" target=\"#b14\">Jiao et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Sun et al., 2019b;</ref><ref type=\"bibr\" target=\"#b40\">Wang et al., 2020)</ref>. The student models are distilled from large \"bibr\" target=\"#b14\">Jiao et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Sun et al., 2019a;</ref><ref type=\"bibr\" target=\"#b40\">Wang et al., 2020)</ref>  MobileBERT compresses a specially designed  esults of Distil-BERT, TinyBERT 2 , BERT SMALL , Truncated BERT BASE and 6\u00d7768 MINILM are taken from<ref type=\"bibr\" target=\"#b40\">Wang et al. (2020)</ref>. BERT SMALL</figDesc><table><row><cell>Model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  domains. Inspired by meta-learning (learning-to-learn) that can improve the model's generalization <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta ource-trained model will be more robust to unseen target domains. (3) Optimizing with meta-learning <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: directly optimize user's utility instead of any proxies of diversity. Determinantal Point Processes <ref type=\"bibr\" target=\"#b38\">[39]</ref> and Multi-Armed Bandits <ref type=\"bibr\" target=\"#b35\">[36. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al neighborhood <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p><p>More interestingly, Z \u00fcgner et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> focused on the node classification using GCN, and proposed ations based on the obtained embedding vectors could be affected correspondingly.</p><p>Inspired by <ref type=\"bibr\" target=\"#b40\">[41]</ref>, in this paper, we propose a new fast gradient attack (FGA et node, then randomly connect the target node to K \u2212 b nodes of different classes.</p><p>\u2022 NETTACK <ref type=\"bibr\" target=\"#b40\">[41]</ref>. NETTACK generates adversarial network iteratively. In eac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ranslation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequence model <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref>. We examine some of the critical paramet \"6.\">Conclusion</head><p>We presented a broad range of basic experiments with the Transformer model <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref> for English-to-Czech neural machine tran training steps is given but no indication on \"how much converged\" the model was at that point, e.g. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref>. Most probably, the training was run unti r faking the global_step stored in the checkpoint) to make sure the learning rate is not too small. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref> suggest to average the last 20 checkpoint. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: le optimization methods usually can be divided into two groups: supervised and unsupervised methods <ref type=\"bibr\" target=\"#b15\">(Grybas et al., 2017)</ref>. Supervised scale optimization methods ar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons <ref type=\"bibr\" target=\"#b28\">[Haley and</ref><ref type=\"bibr\">Soloway, 1992, Barnard and</ref><ref type=\"bibr\" target=\"#b7\">Wessels, 1992]</ref>. However, recent works show Graph Neural Networks ell <ref type=\"bibr\" target=\"#b28\">[Haley and</ref><ref type=\"bibr\">Soloway, 1992, Barnard and</ref><ref type=\"bibr\" target=\"#b7\">Wessels, 1992]</ref>. We instead show a general pattern of how ReLU ML. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ore we propose to improve the quality of the embeddings E of candidate negatives. Inspired by \ud835\udc5a\ud835\udc56\ud835\udc65\ud835\udc62\ud835\udc5d <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, we introduce the  of its distribution on Light-GCN. We explore different choices according to the relevant researches <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>:</p><p>\u2022 Beta dist lent distributions in machine learning. Here we fix \ud835\udf07 = 0.5, \ud835\udf0e = 0.1. \u2022 Uniform distribution. MoCHi <ref type=\"bibr\" target=\"#b17\">[18]</ref> serves for self-supervised learning approaches, and propos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b52\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b55\">Ye et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">He et al., 2019; y contains the hard negative samples which are beneficial for learning high-quality representations <ref type=\"bibr\" target=\"#b55\">(Ye et al., 2019)</ref>. Specifically, the word-level contrastive los. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the ubiquitous existence of graphs, such a task has many applications, such as recommender system <ref type=\"bibr\" target=\"#b33\">[34]</ref>, knowledge graph completion <ref type=\"bibr\" target=\"#b40\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep neural models can easily overfit the noisy labels, leading to severe degradation of performance <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Zhang et al., g supervised learning models for entity-centric IE tasks. Our method is motivated by recent studies <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">Toneva et al. e well-represented patterns of data in early steps, while needing much more steps to memorize noise <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017)</ref>.</p><p>Moreover, learned noisy examples ten reases, being consistent with the delayed learning curves the neural models have on noisy instances <ref type=\"bibr\" target=\"#b1\">(Arpit et al., 2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  nodes in GCN are inclined to converge to a certain value and thus become indistinguishable. ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> solves a similar problem in computer vision w he -th weight matrix W ( ) . Initial residual connection. To simulate the skip connection in ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref>, <ref type=\"bibr\" target=\"#b16\">(Kipf &amp; W ations for introducing identity mapping into our model.</p><p>\u2022 Similar to the motivation of ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref>, identity mapping ensures that a deep GCNII m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bute this performance degradation to the oversmoothing issue <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, which states that   descriptions of the over-smoothing issue simplify the assumption of non-linear activation function <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> or make approximat , which means that representations of nodes converge to indistinguishable limits. To our knowledge, <ref type=\"bibr\" target=\"#b14\">[15]</ref> is the first attempt to demystify the over-smoothing issue  whole graph when the number of training nodes is limited under a semi-supervised learning setting. <ref type=\"bibr\" target=\"#b14\">[15]</ref> applies co-training and self-training to overcome the limi tations has a slight downward trend as the number of propagation iterations increases. According to <ref type=\"bibr\" target=\"#b14\">[15]</ref>, the node representations suffering from the oversmoothing ervation when building very deep graph neural networks, which aligns with the over-smoothing issue. <ref type=\"bibr\" target=\"#b14\">[15]</ref> and <ref type=\"bibr\" target=\"#b32\">[33]</ref> study the ov layers, are very difficult to be separated.  Several studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> attribute this performance degradation phenomenon to the ov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng to a single point, triplet loss enables documents with the same identity to reside on a manifold <ref type=\"bibr\" target=\"#b28\">[20]</ref>, and at the same time maintain a distance from other docum. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: en done to analyze emotion evoked by natural images <ref type=\"bibr\" target=\"#b6\">[7]</ref>, videos <ref type=\"bibr\" target=\"#b7\">[8]</ref> and music <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>A. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the short-term pattern of nodes, we apply the contextual attention-based model which is inspired by <ref type=\"bibr\" target=\"#b3\">[Liu et al., 2017]</ref> and proposed by <ref type=\"bibr\" target=\"#b1\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b26\">[27]</ref><ref type=\"bibr\" target=\"#b27\">[28]</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref>. Specifically, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, large Transformer based models <ref type=\"bibr\" target=\"#b31\">[32]</ref>, such as BERT <ref type=\"bibr\" target=\"#b5\">[6]</ref>, sho il: markus.zlabinger@tuwien.ac.at 3 TU Wien, Austria, email: hanbury@ifs.tuwien.ac.at former layers <ref type=\"bibr\" target=\"#b31\">[32]</ref> (we evaluate up to three) can effectively contextualize qu  learning -a local contextualization, fixed by the n-gram size hyperparameter.</p><p>Vaswani et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref> proposed the Transformer architecture in the context of la intensity of the contextualization. We calculate the context(t1:n) with a set of Transformer layers <ref type=\"bibr\" target=\"#b31\">[32]</ref>. First, the input sequence is fused with a positional enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s in a video for action localization by learning action prototypes and actions jointly. Buch et al. <ref type=\"bibr\" target=\"#b25\">[25]</ref> employed a temporal segment network (TSN) <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rich and rule-based approaches rely on corpus based gender statistics mined from external resources <ref type=\"bibr\" target=\"#b0\">(Bergsma and Lin, 2006)</ref>. Such lists were generated from large un. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b2\">Cheng et al., 2017a]</ref> and relevance-based entity recommendation <ref type=\"bibr\" target=\"#b7\">[Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Zhou et al., 202 vertices with the largest degrees and all the edges between them. We reused 429 queries provided by <ref type=\"bibr\" target=\"#b7\">[Hasibi et al., 2017]</ref> but removed those containing unmatchable k. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y. Several methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> have utilized the binary representations of users and items. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e standard method for unsupervised contact prediction fits a Potts model energy function to the MSA <ref type=\"bibr\" target=\"#b24\">(Lapedes et al., 1999;</ref><ref type=\"bibr\" target=\"#b42\">Thomas et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b22\">Parmar et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Child et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Ho et al., 201 t loses global information. To compensate, <ref type=\"bibr\" target=\"#b3\">Child et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2  target=\"#b22\">Parmar et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b3\">Child et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b11\">Huang et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b9\">Ho et al. (2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b21\">(Levy et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">McCann et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019)</ref>, we propose a new framework that is capable of  formalized as answering the question \"What is the summary?\". Our work is significantly inspired by <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref>, which formalized the task of entity-relation  sk of entity-relation extraction as a multi-turn question answering task. Different from this work, <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> focused on relation extraction rather than NER  target=\"#b22\">Li et al. (2019)</ref> focused on relation extraction rather than NER. Additionally, <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> utilized a template-based procedure for constr nt influence on the final results. Different ways have been proposed for question generation, e.g., <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> utilized a template-based procedure for constr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: RNA) <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and the recurrent neural network transducer (RNN-T) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. In particular, th nd-to-end models including attention-based models <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RNN-T <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> trained on \u223c12,500 \"http://www.tei-c.org/ns/1.0\"><head n=\"2.\">RNN-TRANSDUCER</head><p>The RNN-T was proposed by Graves <ref type=\"bibr\" target=\"#b12\">[13]</ref> as an extension to the connectionist temporal classificati ure <ref type=\"figure\">1</ref>, consists of an encoder (referred to as the transcription network in <ref type=\"bibr\" target=\"#b12\">[13]</ref>), a prediction network and a joint network; as described i e=\"bibr\" target=\"#b13\">[14]</ref>. The entire network is trained jointly to optimize the RNN-T loss <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which marginalizes over all alignments of target labels w p><p>During inference, the most likely label sequence is computed using beam search as described in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, with a minor alteration which was found to make the algor nsive without degrading performance: we skip summation over prefixes in pref(y) (see Algorithm 1 in <ref type=\"bibr\" target=\"#b12\">[13]</ref>), unless multiple hypotheses are identical.</p><p>Note tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b8\">(Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Fey et al., 2020)</ref>. Graph neural networks have been proven to be  ibr\" target=\"#b0\">Abu-El-Haija et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Loukas, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Fey et al., 2020)</ref>. In the context of molecular property predicti olution on both the input graph and a coarser version of it from which all cycles have been removed <ref type=\"bibr\" target=\"#b6\">(Fey et al., 2020)</ref>. Despite interesting results, this approach g >Fey et al., 2020)</ref>. In the context of molecular property prediction, we highlight the work of <ref type=\"bibr\" target=\"#b6\">Fey et al. (2020)</ref> who proposed to perform message passing betwee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hilous settings, their evaluation is often limited to a few graph datasets introduced by Pei et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> that have certain undesirable properties such as small siz  have been proposed to achieve higher performance in low-homophily settings. For instance, Geom-GCN <ref type=\"bibr\" target=\"#b43\">[44]</ref> introduces a geometric aggregation scheme, MixHop <ref typ y used datasets to evaluate non-homophilous graph representation learning methods were presented by <ref type=\"bibr\" target=\"#b43\">[44]</ref> (see our Appendix Table <ref type=\"table\" target=\"#tab_5\"> lic) datasets discussed in <ref type=\"bibr\" target=\"#b47\">[48]</ref>, evaluation on the datasets in <ref type=\"bibr\" target=\"#b43\">[44]</ref> is plagued by high variance across different train/test sp \ud835\udc63 }| |\ud835\udc38| .<label>(1)</label></formula><p>Another related measure is what we call the node homophily <ref type=\"bibr\" target=\"#b43\">[44]</ref>, defined as is the number of neighbors of \ud835\udc65 that have the  ondly, the stability of performance across runs is better for our datasets than those of Pei et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> (see <ref type=\"bibr\" target=\"#b59\">[60]</ref> results). M i-c.org/ns/1.0\"><head>A.2 Previous Non-Homophilous Data</head><p>For the six datasets in Pei et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> often used in evaluation of graph representation learning  d grid is the same as that of APPNP. We use their Personalized PageRank weight initialization.      <ref type=\"bibr\" target=\"#b43\">[44]</ref>. The \"film\" dataset is also to as \"Actor\". Note that there Figure 5 :</head><label>5</label><figDesc>Figure 5: Compatibility matrices of datasets in Pei et al.<ref type=\"bibr\" target=\"#b43\">[44]</ref>. The \"film\" dataset is also to as \"Actor\". Note that there ml:id=\"tab_5\"><head>Table 4 :</head><label>4</label><figDesc>Statistics for datasets from Pei et al.<ref type=\"bibr\" target=\"#b43\">[44]</ref>. #C is the number of node classes.</figDesc><table><row><c  Chameleon than Squirrel and better on Squirrel than Actor <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>. Our measure sugges nd internet relationships, such as in web page connections <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. \u2022 Malicious or fraudulent nodes, such as in auction networ her works in non-homophilous graph learning evaluation, we take a high proportion of training nodes <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the attribute similarity for new user (item) recommendations <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. However, they do n ences conventionally <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Social data based  ata, and then generate the new user's embedding with the connection between new users and old users <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Despite the achievements they have made, most of these mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tc. There are also studies that leverage other architectures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> for sequential reco xt from both directions for sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n owing previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we apply the leave-oneout strategy for evaluation. Concret model, which uses the multi-head attention mechanism to recommend the next item.</p><p>(7) BERT4Rec <ref type=\"bibr\" target=\"#b22\">[23]</ref> uses a Cloze objective loss for sequential recommendation . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b2\">(Bowman et al., 2016;</ref><ref type=\"bibr\" target=\"#b31\">Vendrov et al., 2015;</ref><ref type=\"bibr\" target=\"#b19\">Mou et al., 2016;</ref><ref type=\"bibr\" target=\"#b16\">Liu et al., 201 ent-wise product are then concatenated with the original vectors, \u0101 and \u00e3, or b and b, respectively <ref type=\"bibr\" target=\"#b19\">(Mou et al., 2016;</ref><ref type=\"bibr\">Zhang et al., 2017)</ref>. T 015)</ref> uses unsupervised \"skip-thoughts\" pre-training in GRU encoders. The approach proposed by <ref type=\"bibr\" target=\"#b19\">Mou et al. (2016)</ref> considers tree-based CNN to capture sentence- pe=\"bibr\" target=\"#b31\">(Vendrov et al., 2015)</ref> 15M 98.8 81.4 (4) 300D tree-based CNN encoders <ref type=\"bibr\" target=\"#b19\">(Mou et al., 2016)</ref> 3.5M 83.3 82.1 (5) 300D SPINN-PI encoders <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: racker <ref type=\"bibr\" target=\"#b0\">[1]</ref> that embeds kernel tricks in the circulant structure <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Considering the importance of robust feature representati circular convolution operator <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With the circulant structure <ref type=\"bibr\" target=\"#b36\">[37]</ref> and Fourier transform <ref type=\"bibr\" target=\"#b48\">[49]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uivalent to factorizing some forms of graph proximity (e.g., transformation of the adjacent matrix) <ref type=\"bibr\" target=\"#b34\">[35]</ref>, which overly emphasize on the structural information enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ome video generation methods have dealt with this problem by generating the entire sequence at once <ref type=\"bibr\" target=\"#b24\">[25]</ref> or in small batches <ref type=\"bibr\" target=\"#b19\">[20]</r d to handle videos <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p><p>Straight-for ibr\" target=\"#b23\">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed in <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, replacing the 2D . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b34\">35]</ref> and natural language processing <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cantly improved image classification <ref type=\"bibr\" target=\"#b13\">[14]</ref> and object detection <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> accuracy. Compared t task that requires more complex methods to solve. Due to this complexity, current approaches (e.g., <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ n this paper, we streamline the training process for stateof-the-art ConvNet-based object detectors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. We propose a single egressors, rather than training a softmax classifier, SVMs, and regressors in three separate stages <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. The components of t  very deep detection network (VGG16 <ref type=\"bibr\" target=\"#b19\">[20]</ref>) 9\u00d7 faster than R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref> and 3\u00d7 faster than SPPnet <ref type=\"bibr\" target=\"#b10\">[11 1.0\"><head n=\"1.1.\">R-CNN and SPPnet</head><p>The Region-based Convolutional Network method (R-CNN) <ref type=\"bibr\" target=\"#b8\">[9]</ref> achieves excellent object detection accuracy by using a deep  k h , for each of the K object classes, indexed by k. We use the parameterization for t k given in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, in which t k specifies a scale-invariant translation and lo eparates localization and classification. OverFeat <ref type=\"bibr\" target=\"#b18\">[19]</ref>, R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and SPPnet <ref type=\"bibr\" target=\"#b10\">[11]</ref> also t tions of the dataset). We use mini-batches of size R = 128, sampling 64 RoIs from each image. As in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we take 25% of the RoIs from object proposals that have int rm non-maximum suppression independently for each class using the algorithm and settings from R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= he first is the CaffeNet (essentially AlexNet <ref type=\"bibr\" target=\"#b13\">[14]</ref>) from R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. We alternatively refer to this CaffeNet as model S, for \"sm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n the MDP state, which consists of the query, the preceding documents, and the remaining candidates <ref type=\"bibr\" target=\"#b32\">[33]</ref>.</p><p>The greedy sequential document selection simpli es  28]</ref> for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking <ref type=\"bibr\" target=\"#b32\">[33]</ref> with the Monte Carlo tree search (MCTS), for alleviating t anism <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Xia et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> proposed to model the dynamics of the document utility wit <ref type=\"bibr\" target=\"#b37\">[38]</ref>, the log-based document re-ranking is modeled as a POMDP. <ref type=\"bibr\" target=\"#b32\">[33]</ref> and <ref type=\"bibr\" target=\"#b29\">[30]</ref> propose to m <p>In our experiments, for e ective training of the model parameters and following the practices in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, we combined four TREC datasets and constructed a new data approach which automatically learns novelty features based on neural tensor networks.</p><p>MDP-DIV <ref type=\"bibr\" target=\"#b32\">[33]</ref>: a state-of-the-art learning approach which uses an MDP fo ning approach which uses an MDP for modeling the diverse ranking process. Following the practice in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, we con gured the reward function in MDP-DIV as \u03b1-DCG and  minary representations of the queries and the documents as their inputs. Following the practices in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, in the experiments we used the query vector and document . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: feedback as a composition of user result examination and relevance judgment. Examination hypothesis <ref type=\"bibr\" target=\"#b7\">[8]</ref>, which is a fundamental assumption in click modeling, postul ns; and among them result examination plays a central role <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Unfortunately, most applications of bandit algorithms simply the selected arm a t . Based on the examination hypothesis <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, when C at = 1, the chosen a t must be relevant to the user's. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: isplays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref>. The gesture annotation is performed usin es is motivated by previous research that has suggested that deceivers' speech has lower complexity <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref>. We use the tool described in <ref type=\" or gaze) and nod (Side-Turn-R) more frequently than truth-tellers. This agrees with the findings in <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref> that liars who are more motivated to get . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rns in our framework to extract useful features from the heterogeneous textrich network. Meta-paths <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref> phs, can offer more flexibility and capture richer network semantics than the widely used meta-path <ref type=\"bibr\" target=\"#b32\">[31]</ref> patterns. Recent studies have shown that incorporating mot of two authors (i.e., \"Jure Leskovec\" and \"Jon Kleinberg\").</p><p>It is worth noting that meta-path <ref type=\"bibr\" target=\"#b32\">[31]</ref> can be viewed as a special case of motif patterns when the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: needs could shift while users are searching for papers in different time slices. Chakraborty et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> focused more on user queries, and their proposed system na. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b27\">[28]</ref>, first residual network and squeezeand-excitation network <ref type=\"bibr\" target=\"#b28\">[29]</ref> are integrated to extract discriminative frame level speak . Increasing the depth of a neural network can significantly improve the quality of representations <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Additionally, batch normalization helps to improve the st /ns/1.0\"><head n=\"2.2.\">Squeeze-and-excitation network</head><p>Squeeze-and-excitation (SE) network <ref type=\"bibr\" target=\"#b28\">[29]</ref> is first introduced to model channel interdependence in fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s function search <ref type=\"bibr\" target=\"#b32\">[33]</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" target=\"#b35\">[36]</ref>. However, these m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ition pattern is more complicated. \u2022 The recent approaches <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, which divide the u  the fairness and the convenience of comparison, we follow <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> to filter out sessi , s,i \u22121 ] with the last item s,i \u22121 as l abel . Following <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, for the Yoochoose  tention on the last item after encoding with a RNN. To alleviate the influence of time order, STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> only utilizes the self-attention mechanism without RNN. SR  to sum up as the session embedding. To further alleviate the bias introduced by time series, STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> entirely replaces the recurrent encoder with an attention  h enables the model to explicitly emphasize on the more important parts of the input.</p><p>\u2022 STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> uses attention layers to replace all RNN encoders in previ ith different lengths because the length varies greatly within one dataset. Following previous work <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, sessions in Yooch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: alance value of F \u2032 H : F \u2032 E so that edges will have a higher chance to show themselves. However,  <ref type=\"bibr\" target=\"#b0\">[1]</ref> 59.5% 60.1% 70.7% SemeiEmb <ref type=\"bibr\" target=\"#b22\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ngual transfer learning, which is a popular approach to address the limited resource problem in ASR <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ETS</head><p>Dilated convolutions were originally proposed for the computation of wavelet transform <ref type=\"bibr\" target=\"#b37\">[38]</ref> and employed in the deep learning context (as an alternati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: label field is one of the most effective way to construct the connection between neighboring pixels <ref type=\"bibr\" target=\"#b4\">[5]</ref> . It decreases with the number of pixels having the same lab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ead><p>For the ImageNet experiments we used a pre-trained ResNet50 network from the pytorch package <ref type=\"bibr\" target=\"#b6\">(He et al. [2016]</ref>).</p><p>For the ImageNet autoencoder we used t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ical, fetch-modify-consistent views of the file system. We have formally specified fork consistency <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and, assuming digital signatures and a collision-resistan hus cannot ever see each other's operations without detecting the attack (as proven in earlier work <ref type=\"bibr\" target=\"#b15\">[16]</ref>).</p><p>One optimization worth mentioning is that SUNDR am cs for an untrusted server. An unimplemented but previously published version of the SUNDR protocol <ref type=\"bibr\" target=\"#b15\">[16]</ref> had no groups and thus did not address write-after-write c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b18\">(Peters et al., 2018)</ref>), and 69% accuracy on a balanced variant of the SICK-E dataset <ref type=\"bibr\" target=\"#b16\">(Marelli et al., 2014)</ref>. Next, we apply AUTOPROMPT to the fact r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2.1.\">Multiscale segmentation used</head><p>A region based multiscale image segmentation named MSEG <ref type=\"bibr\" target=\"#b36\">(Tzotsos and Argialas, 2006</ref>) is used to generate the initial se D have a size of \u00d7 500 500 pixels. The multiscale segmentation used in this study is MSEG algorithm <ref type=\"bibr\" target=\"#b36\">(Tzotsos and Argialas, 2006)</ref>, which is implemented under C++ en ed on multi-resolution segmentation (MSEG) <ref type=\"bibr\" target=\"#b0\">(Benz et al. (2004)</ref>; <ref type=\"bibr\" target=\"#b36\">Tzotsos and Argialas, 2006)</ref>, the proposed method is implemented. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b7\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b23\">Zhang et al., 2015a;</ref><ref type=\"bibr\" target=\"#b20\">Yang et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Wang et al., 2  out which words are useful and which words are useless. Therefore, we apply an attention mechanism <ref type=\"bibr\" target=\"#b20\">(Yang et al., 2016)</ref> to get those important words and assemble t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ication loss L cls , triplet loss <ref type=\"bibr\" target=\"#b24\">[25]</ref> L tri , and center loss <ref type=\"bibr\" target=\"#b53\">[54]</ref> L cent to optimize K domain-specific experts {M \u03c6 k } K k= f D s , C s is the prototypes set of D s , and \u03b8 is the parameter of the voting network. Similar to <ref type=\"bibr\" target=\"#b53\">[54]</ref>, prototypes can be updated with the center loss in Eq. (1). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, including k-means and spectral clustering, have been proven to be special cases in kernel kmeans <ref type=\"bibr\" target=\"#b30\">[31]</ref>, where semi-supervised constraints can be added as well. T into different evolution tracks. Kernel k-means, which has been proven to be a generalized solution <ref type=\"bibr\" target=\"#b30\">[31]</ref> for both k-means and spectral clustering, is more flexible. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 0\"><head>B. FIFO Queue Benchmark</head><p>A simple FIFO queue implementation was written based upon <ref type=\"bibr\" target=\"#b7\">[8]</ref>. The queue is based around a ring buffer, with read and writ re operations are needed (i.e. we do not need CAS). For MIPS64 this has been implemented exactly as <ref type=\"bibr\" target=\"#b7\">[8]</ref>, for Mamba a simple modification was made. The FIFO ring buf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns for diagnosing chest diseases. In the US, more than 35 million chest X-rays are taken every year <ref type=\"bibr\" target=\"#b19\">[20]</ref>. It is primarily used to screen diseases such as lung canc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b27\">[28]</ref>, StructSiam <ref type=\"bibr\" target=\"#b57\">[58]</ref>, LSART <ref type=\"bibr\" target=\"#b58\">[59]</ref>, DRT <ref type=\"bibr\" target=\"#b59\">[60]</ref>, MFT <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, Blind-Dehazing <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and more. While such unsupervised methods can exploit image. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  GPM applications targeting various platforms. For TC, there are parallel solvers on multicore CPUs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\" target=\"#b61\">63]</ref>) or IoUNet (e.g., <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>) to estimate target sc tains AO of 0.628, SR 0.50 of 0.732 and SR 0.75 of 50.4, outperforming the second best tracker DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref> with AO of 0.611, SR 0.50 of 0.717 and SR 0.75 of 0.492 by 1 ng and the rest 280 for testing. We compare MART with 12 state-of-the-art tracking algorithms (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref>, SiamRPN++ <re our MART achieves the best performance with 0.571 success score, outperforming the second best DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref> by 0.3%. In comparison to ATOM <ref type=\"bibr\" target=\"#b7\"  one-pass evaluation (OPE) for evaluation. We compare our MART to 9 state-of-the-art trackers (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ATOM < , our MART achieves the best result with success score of 0.621, outperforming the second best DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref> with 0.609 success score by 1.2%. In comparison with ATOM th E to assess different algorithms. We compare our proposed MART to 12 state-ofthe-art trackers (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ATOM < ith success score of 0.678 compared to SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref> and DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>. In comparison with ATOM that applies only spatial feature f ith several recent topperformance trackers from VOT-2019, and Tab. 2 demonstrates the results. DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref> achieves the best EAO of 0.379. Our MART obtains promising r mlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_2\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b2\">3</ref> shows the ablation experiments on target localization. Without. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [61]</ref> Hessian <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, Taylor expansion <ref type=\"bibr\" target=\"#b46\">[47,</ref> limited to Hessian <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, Taylor expansion <ref type=\"bibr\" target=\"#b46\">[47,</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r decision interactions and information sharing. The neural encoder-decoder generation architecture <ref type=\"bibr\" target=\"#b39\">(Sutskever et al., 2014;</ref><ref type=\"bibr\" target=\"#b1\">Bahdanau . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge of recommendation techniques have been proposed, from classic collaborative filtering techniques <ref type=\"bibr\" target=\"#b29\">[30]</ref> to the recent deep learning models <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d does not incur any storage overhead and is transferable across both hardware and software changes <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Functional warmin t, requires huge storage overhead, and does not allow for software changes. Functional warming (FW) <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> does not incur any on, which is fast but does not allow for changes to the software. Virtualized Fast-Forwarding (VFF) <ref type=\"bibr\" target=\"#b25\">[26]</ref> leverages hardware virtualization to quickly get to the ne 31]</ref> extend on the concept of BLRL using a form of hardware state checkpoints. Sandberg et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> propose a method that uses two parallel simulations, pessi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ther applications <ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref><ref type=\"bibr\" target=\"#b53\">[54]</ref>. In this section,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> adopt Principal Component Analysis (PCA) or Linear Discrimina. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  different hardware efficiency metric (e.g., prioritizes latency or energy). For example, HW-NAS in <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> develops a differentiable neural architecture many works have pointed out that DNNs with fewer FLOPs are not necessarily faster or more efficient <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>. For example, NasNet-A has a comparable compl hardware devices, that are primarily targeted by SOTA HW-NAS works.</p><p>FBNet Search Space. FBNet <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> constructs a layer-wise search space with a f  and the last three layers with the remaining layers to be searched. In this way, networks in FBNet <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> search space have more regular structure than  unique architectures. While HW-NAS researchers can develop their search algorithms on top of FBNet <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> search space, tedious efforts are required to al., 2016)</ref>. Specifically, we adopt SOTA ASIC accelerator performance simulators (1) Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Para he Appendix D.</p><p>Note that to estimated the hardware-cost of networks in the FBNet search space <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> when being executed on the commercial categor in our HW-NAS-Bench, we sum up hardware-cost of all unique blocks (i.e., \"block\" in the FBNet space <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>) within the network architectures. To validat his experiment. As an example to use our HW-NAS-Bench, we use ProxylessNAS to search over the FBNet <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> space on CIFAR-100 <ref type=\"bibr\" target=\"# NG THE PERDITION OF EYERISS USING ACCELERGY+TIMELOOP AND DNN-CHIP-PREDICTOR</head><p>Both Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Para b_6\"><head>Table 6 :</head><label>6</label><figDesc>The differences of estimation given by Accelergy<ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop<ref type=\"bibr\" target=\"#b31\">(Paras head><label>7</label><figDesc>Left: the marco-architecture of the search space proposed in the FBNet<ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref> for ImageNet classification. Right: our modif ormance metrics on Eyeriss, we adopt SOTA performance simulators for DNN accelerators (1) Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Para i) or do not provide hardware-cost metrics on real hardware, limiting their applicability to HW-NAS <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Wan et al., 202 ficiency, achieving promising results yet suffering from prohibitive search time/cost. In parallel, <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Wan et al., 202 o not consider hardware efficiency metrics on real hardware, limiting their applicability to HW-NAS <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Wan et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uto-encoder. Most recently, a Deep Structured Semantic Models (DSSM) for Web search was proposed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, which is reported to outperform significantly semantic hash in an input word sequence into a feature vector using the technique called word hashing proposed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>. For example, the word is represented by a count vector of i hastic gradient ascent. Learning of the C-DSSM is similar to that of learning the DSSM described in <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ated visually by projecting the high-dimensional representations down to two dimensions using t-SNE <ref type=\"bibr\" target=\"#b44\">[45]</ref>. A non-redundant (PIDE&lt;40%) version of the SCOPe databa grammar in NLP, we projected the highdimensional embedding space down to two dimensions using t-SNE <ref type=\"bibr\" target=\"#b44\">[45]</ref> and visualized proteins according to annotated structural,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff and Jones, 1999)</ref>, which uses the prediction of models wi br\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff and Jones, 1999)</ref>. However, in order to prevent the deep . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed node features.</p><p>To construct a stable or contractive GNN block, we mimic the design of MDEQ <ref type=\"bibr\" target=\"#b2\">(Bai et al., 2020)</ref>. We build a GNN block as follows:</p><formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: c.org/ns/1.0\"><head>Pre-training Objective</head><p>We adapt the masked language modeling objective <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> to the MSA setting. The loss for an MSA x,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  various forms of artificial noise during training. We distinct regularization methods like dropout <ref type=\"bibr\" target=\"#b46\">(Srivastava et al., 2014)</ref> and task-specific data augmentation t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b19\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Bouraoui et al., 2019)</ref>, summarization <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h Repositioning (EDITOR), which builds on recent progress on non-autoregressive sequence generation <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad   et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Stern et al., 2018)</ref> or multi-pass decoding <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad  \"bibr\" target=\"#b22\">(Gu et al., 2019)</ref>, or 2) the maximum number of decoding steps is reached <ref type=\"bibr\" target=\"#b26\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Ghazvininejad  s widely used in nonautoregressive generation <ref type=\"bibr\" target=\"#b21\">(Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Gu et al., 2019. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 6\">(Wan et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Alok et al., 2016)</ref>, subpixel mapping <ref type=\"bibr\" target=\"#b41\">(Song et al., 2019)</ref>, sparse unmixing <ref type=\"bibr\" target=\"# s such as image clustering <ref type=\"bibr\" target=\"#b34\">(Ma et al., 2015)</ref>, subpixel mapping <ref type=\"bibr\" target=\"#b41\">(Song et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Ma et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nowledge can be captured for recovering the high-frequency details in HR images.</p><p>Recent works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> have successfully  tional neural network <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Among them, the CNN-based approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> have recently set  ork, making it easy to train. In addition, in previous works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only high-level features at top layers were used in the re formance. Instead of using interpolation for upscaling as in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, recent studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref ere studied and compared in our work. As in previous methods <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only the feature maps at the top layer are used as input f  to learn an end-to-end mapping for SR. Subsequently, a deep network with 20 layers was proposed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> to improve the reconstruction accuracy of CNN. The residua on accuracy of CNN. The residuals between the HR images and the interpolated LR images were used in <ref type=\"bibr\" target=\"#b10\">[11]</ref> to speedup the converging speed in training and also to im yers</head><p>In previous SR methods such as SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> and VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, bicubic interpolation is used to upscale LR images to the plus <ref type=\"bibr\" target=\"#b23\">[24]</ref>, SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> and DRCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The im  <ref type=\"bibr\" target=\"#b1\">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> with 20-layer CNN. It should be mentioned that the most si  Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref> SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> DRCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>   In additi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  t f , of dimension 300 for an input text (transcript), t.</p><p>Audio Feature Extraction openSMILE <ref type=\"bibr\" target=\"#b21\">[22]</ref> is an open-source toolkit used to extract high dimensional. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: side information) of users and items <ref type=\"bibr\" target=\"#b14\">(Jain &amp; Dhillon, 2013;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2013)</ref>. In IMC, a rating is decomposed by r ij = x i . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b66\">65,</ref><ref type=\"bibr\" target=\"#b74\">73]</ref>, heavy data augmentations applied to the same image are cru edding space.</p><p>Measuring the utilization of the embedding space. Very recently, Wang and Isola <ref type=\"bibr\" target=\"#b74\">[73]</ref> presented two losses/metrics for assessing contrastive lea /head><p>The uniformity experiment in Figure <ref type=\"figure\">3c</ref> is based on Wang and Isola <ref type=\"bibr\" target=\"#b74\">[73]</ref>. We follow the same definitions of the losses/metrics as p erstand the underlying mechanism that make it work so well <ref type=\"bibr\" target=\"#b68\">[67,</ref><ref type=\"bibr\" target=\"#b74\">73,</ref><ref type=\"bibr\" target=\"#b60\">59,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mising application area for deep learning <ref type=\"bibr\" target=\"#b11\">(Graves et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Ingraham et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Pereira et aphQA <ref type=\"bibr\" target=\"#b1\">(Baldassarre et al., 2020)</ref> on MQA, Structured Transformer <ref type=\"bibr\" target=\"#b16\">(Ingraham et al., 2019)</ref> on CPD, and ProteinSolver <ref type=\"bi bstantial improvement both in terms of perplexity and sequence recovery over Structured Transformer <ref type=\"bibr\" target=\"#b16\">(Ingraham et al., 2019)</ref>, a GNN method which was trained using t ve model over the space of protein sequences conditioned on the given backbone structure. Following <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref>, we frame this as an autoregressive task  the same training and validation sets (Table <ref type=\"table\" target=\"#tab_3\">3</ref>). Following <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref>, we report evaluation on short (100 or f >Protein design As described in the main text, we use the CATH 4.2 dataset and splits as curated by <ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref> and the TS50 test set as curated by <ref et should bear minimal similarity to the training structures. We use the CATH 4.2 dataset curated by<ref type=\"bibr\" target=\"#b16\">Ingraham et al. (2019)</ref> in which all available structures with 4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f>, or introducing a recurrence mechanism as in Transformer-XL for learning longer-range dependency <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. However, no much  an example of a REMI event sequence.</p><p>In our experiment (see Section 5), we use Transformer-XL <ref type=\"bibr\" target=\"#b10\">[11]</ref> to learn to compose Pop piano music using REMI as the unde xpressive classical piano music with a coherent structure of up to one minute.</p><p>Transformer-XL <ref type=\"bibr\" target=\"#b10\">[11]</ref> extends Transformer by introducing the notion of recurrenc s for the \ud835\udf0f-th segment, R denotes the relative positional encodings designed for the Transformer-XL <ref type=\"bibr\" target=\"#b10\">[11]</ref>, a \ud835\udc5b \ud835\udf0f,\ud835\udc56 indicates the attention features from the \ud835\udc56-th he ents (i.e., the segment length) and the recurrence length (i.e., the length of the segment \"cached\" <ref type=\"bibr\" target=\"#b10\">[11]</ref>) are both set to 512. The total number of learnable parame. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b3\">[Guo et al., 2008;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2019]</ref> make noticeable progress, existing methods su ificant performance degradation when tested on unseen dataset. Take the state-of-the-art model PIPR <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref> as an example, compared tested on trainset-h v xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Protein feature encoding</head><p>Previous work <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref> has proved that protein features based on th earning based: We choose three representative deep learning (DL) algorithms in PPI prediction, PIPR <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref>, DNN-PPI <ref type=\"bibr\" target=\"#b4\">[Li e ibition, catalysis, and expression. Each pair of interacting proteins contains at least one of them.<ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref> randomly select 1,690 and 5,189 proteins fro br\" target=\"#b4\">[Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Sun et al., 2017. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hebyshev polynomials to the graph Laplacian, spatially localized filtering is obtained. Kipf et al. <ref type=\"bibr\" target=\"#b18\">[18]</ref> approximate the polynomials using a re-normalized first-or rmula_4\">E ijp = \u00caijp N j=1 \u00caijp<label>(5)</label></formula><p>or symmetric normalization as in GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref>:</p><formula xml:id=\"formula_5\">E ijp = \u00caijp N i=1 \u00caijp N  e our EGNN(C) layer from the formula of EGNN(A) layer. Indeed, the essential difference between GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref> is wheth nd 20% sized subsets, which is called \"dense\" splitting.</p><p>Following the experiment settings of <ref type=\"bibr\" target=\"#b18\">[18]</ref>[27], we use two layers of EGNN in all of our experiments f nd are penalized more in the loss than a majority class.</p><p>The baseline methods we used are GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>. To inve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ies in an open knowledge graph. In this paper, we mainly survey and benchmark models on this topic. <ref type=\"bibr\" target=\"#b33\">[34]</ref> and KGNN-LS <ref type=\"bibr\" target=\"#b32\">[33]</ref>. KGC 4]</ref>  experiments. We use sum aggregator because it has best overall performance as reported in <ref type=\"bibr\" target=\"#b33\">[34]</ref>.</p><p>C.14 KGAT C.14.1 Recommendation. We set \ud835\udc51 (0) = 64, ter low-frequency nodes. To align items to knowledge graph entities, we adopt the same procedure as <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>.</p></div> <div xm mance deteriorates when we do that, which is also found in <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>  experiments. We use sum aggregator because it has best ove. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs. The design of our architecture is inspired by recent progress in computer vision, in particular <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b6\">He  ch deeper networks <ref type=\"bibr\" target=\"#b12\">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref t erarchical manner. Our architecture can be in fact seen as a temporal adaptation of the VGG network <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>. We have also investigated the s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mages with convnets <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. They typically sha get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. However, our metho harpening of the teacher output to avoid collapse, while other popular components such as predictor <ref type=\"bibr\" target=\"#b27\">[28]</ref>, advanced normalization <ref type=\"bibr\" target=\"#b9\">[10] n unsupervised features without discriminating between images. Of particular interest, Grill et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> propose a metric-learning formulation called BYOL, where f ule is \u03b8 t \u2190 \u03bb\u03b8 t + (1 \u2212 \u03bb)\u03b8 s , with \u03bb following a cosine schedule from 0.996 to 1 during training <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Originally the momentum encoder has been introduced as a  nstraints <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, predictor <ref type=\"bibr\" target=\"#b27\">[28]</ref> or batch normalizations <ref type=\"bibr\" target=\"#b27\">[28 m-up for \u03c4 t from 0.04 to 0.07 during the first 30 epochs. We follow the data augmentations of BYOL <ref type=\"bibr\" target=\"#b27\">[28]</ref> (color jittering, Gaussian blur and solarization) and mult ncoder with the multicrop augmentation and the cross-entropy loss. We also report results with BYOL <ref type=\"bibr\" target=\"#b27\">[28]</ref>, MoCo-v2 <ref type=\"bibr\" target=\"#b13\">[14]</ref> and SwA 2 <ref type=\"bibr\" target=\"#b13\">[14]</ref>, SwAV <ref type=\"bibr\" target=\"#b9\">[10]</ref> and BYOL <ref type=\"bibr\" target=\"#b27\">[28]</ref> when using convnet or ViT. In Tab. 13, we see that when tr performs the corresponding published numbers with ResNet-50. Indeed, we obtain 72.7% for BYOL while <ref type=\"bibr\" target=\"#b27\">[28]</ref> report 72.5% in this 300-epochs setting. We obtain 71.1% f self-supervised learning frameworks, namely MoCo-v2 <ref type=\"bibr\" target=\"#b13\">[14]</ref>, BYOL <ref type=\"bibr\" target=\"#b27\">[28]</ref> and SwAV <ref type=\"bibr\" target=\"#b9\">[10]</ref> with  <r rable sizes with significantly reduced compute requirements <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Self-distillation with no labels. We illustrate DINO in th t a momentum encoder, at the cost of a drop of performance <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Several other works echo this direction, showing that one  t (row 6) while it is critical in BYOL to prevent collapse <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. For completeness, we propose in Appendix B an extended ver  not been observed by other frameworks also using momentum <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, nor when the teacher is built from the previous epoch.</p>  collapse (7, 8) which is consistent with previous studies <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Interestingly, we observe that the teacher output centerin ent by providing target features of higher quality. This dynamic was not observed in previous works <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>.</p><p>Network arc ds and this particular design appears to work best for DINO (Appendix C). We do not use a predictor <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, resulting in the  target=\"#b9\">10]</ref>, predictor <ref type=\"bibr\" target=\"#b27\">[28]</ref> or batch normalizations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. While our framewo nvolutional networks of comparable sizes with a significant reduction of computational requirements <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Our code is availa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  predict execution time of programs by using a set of hand-crafted features of high level programs. <ref type=\"bibr\" target=\"#b9\">Dubach et al. (2007)</ref> uses neural networks with hand-crafted feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e human and model word distributions. This does not appear to be rectified by training on more data <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref>. Recent fixes involve modifying the deco emedied by simply increasing the amount of the training data; e.g. largescale GPT-2 language models <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref> display the same issues.</p><p>Improved  story generation <ref type=\"bibr\" target=\"#b7\">(Fan et al., 2018)</ref>, contextual text completion <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref>, language modeling (for k = 0), and dial  type=\"table\">4</ref>, which</p><p>shows completions from the state-of-the-art GPT-2 language model <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref>. Greedy decoding as well as top-k and nu mprove existing pre-trained language models. We demonstrate this by fine-tuning a pre-trained GPT-2 <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref> language model with sequence-level unlik /ref>: Top: Degenerate repetition in completions from a state-of-the-art large-scale language model <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref>. The examples contain single-word repeti ecture agnostic; we choose this one as a representative of recent large-scale language models, e.g. <ref type=\"bibr\" target=\"#b19\">Radford et al. (2019)</ref>. LUL-token+seq , and German . In the firs or the Naismith Award , which is 0.064  <ref type=\"bibr\" target=\"#b0\">(Baevski and Auli, 2019;</ref><ref type=\"bibr\" target=\"#b19\">Radford et al., 2019)</ref>. We perform experiments at the word level. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targe ng artificial labels <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. We introduce Fix-Mat rget=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. We found that this w version of the same image. Inspired by UDA <ref type=\"bibr\" target=\"#b44\">[45]</ref> and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we leverage CutOut <ref type=\"bibr\" target=\"#b12\">[13]</ref br\" target=\"#b1\">[2]</ref>, we leverage CutOut <ref type=\"bibr\" target=\"#b12\">[13]</ref>, CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> fo <p>curacy on CIFAR-10 with 250 labeled examples compared to the previous state-of-the-art of 93.73% <ref type=\"bibr\" target=\"#b1\">[2]</ref> in the standard experimental setting from <ref type=\"bibr\" t periment with two such variants: RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Note that, unless otherwise stated, we use Cutout <ref type arget=\"#b44\">[45]</ref>.</p><p>Instead of setting the transformation magnitudes randomly, CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref> learns them online over the course of training. To do so, a  how close the model's prediction is to the true label. Further details on CTAugment can be found in <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= thms: Unsupervised Data Augmentation (UDA) <ref type=\"bibr\" target=\"#b44\">[45]</ref> and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>. UDA and ReMixMatch both use a weakly-augmented example to g e=\"bibr\" target=\"#b2\">[3]</ref> Weak Weak Sharpening Averages multiple artificial labels ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref> Weak Strong Sharpening Sums losses for multiple predictions  f type=\"bibr\" target=\"#b2\">[3]</ref>, UDA <ref type=\"bibr\" target=\"#b44\">[45]</ref>, and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With the exception of <ref type=\"bibr\" target=\"#b1\">[2]</re et=\"#b44\">[45]</ref>, and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With the exception of <ref type=\"bibr\" target=\"#b1\">[2]</ref>, previous work has not considered fewer than 25 labels per c 3.13%. Our results also compare favorably to recent state-of-the-art results achieved by ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>, despite the fact that we omit various components such as th ch (RA) uses RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and FixMatch (CTA) uses CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref> for strong-augmentation. All baseline models (\u03a0-Model <ref t f type=\"bibr\" target=\"#b2\">[3]</ref>, UDA <ref type=\"bibr\" target=\"#b44\">[45]</ref>, and ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref>) are tested using the same codebase.  using CTAugment and Ra e=\"table\" target=\"#tab_1\">3</ref>, FixMatch achieves the state-of-the-art performance of ReMixMatch <ref type=\"bibr\" target=\"#b1\">[2]</ref> despite being significantly simpler.</p></div> <div xmlns=\"h FixMatch. Specifically, we chose RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref>, which have been used for state-of-the-art SSL algorithms su A) uses RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and the ones with (CTA) uses CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref> for strong-augmentation. All models are tested using the sam of image transformations used in RandAugment <ref type=\"bibr\" target=\"#b9\">[10]</ref> and CTAugment <ref type=\"bibr\" target=\"#b1\">[2]</ref>. For completeness, we listed all transformation operations f d 2 loss [28, 45, 2], using stronger forms of augmentation <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, and using consistency regularization as a component in a lar tency regularization as a component in a larger SSL pipeline <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Pseudo-labeling leverages the idea that we should use   using strong data augmentation can produce better results <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. These heavily-augmented examples are almost certainly outsid resholding). Using sharpening instead of an arg max introduces a hyper-parameter: the temperature T <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>.</p><p>We study the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, disentangling node representations <ref type=\"bibr\" target=\"#b19\">[20]</ref> and automatically selecting hyper-parameters <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b41\">[42]</ref><ref type=\"bibr\" target=\"#b42\">[43]</ref><ref type=\"bibr\" target=\"#b43\">[44]</ref><ref type=\"bibr\" target=\"#b44\">[45]</ref>.</p><p>Global state. In the description above, all message. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ccurately sample equilibrium structures, but quickly become prohibitively slow for larger molecules <ref type=\"bibr\" target=\"#b45\">[Shim and MacKerell Jr, 2011</ref><ref type=\"bibr\" target=\"#b3\">, Bal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: icit data augmentation. Moreover, to supplement the input graph with more global information, MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposes to augment the input graph using graph diffusion. asure MI between input and representations of both nodes and edges without data augmentation; MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposes to learn both node-level and graph-level represen <ref type=\"bibr\" target=\"#b43\">[44]</ref>, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> in Table <ref type=\"table\" target=\"#tab_0\">1</ref>, where  MI) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Furthermore, we report the performance obtained using a l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n be seen as a special form of low-pass filter <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b9\">Li et al. 2019)</ref>. Some recent studies <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, we employ the subject-independent ten-fold cross-validatio div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Methods</head><p>Accuracy (%) CK+ MMI Oulu-CASIA PPDN <ref type=\"bibr\" target=\"#b31\">[32]</ref> 97.30 \u2020 -72.40 IACNN <ref type=\"bibr\" target=\"#b16\">[17]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing process, which leads to better generalization. We develop an approach based on label smoothness <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, which assumes tha therefore learnable <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Inspired by these methods, we design a module of label smo get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Therefore, more re r the unlabeled nodes l * u (E\\V). To solve the issue, we propose minimizing the leave-one-out loss <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Suppose we hold out a single item v and treat it unlabele. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b10\">(Graves et al., 2013)</ref> with a Connectionist Temporal Classification (CTC) output layer <ref type=\"bibr\" target=\"#b9\">(Graves et al., 2006;</ref><ref type=\"bibr\">Graves, 2012, Chapter 7)</ _8\">15</ref>) can be efficiently evaluated and differentiated using a dynamic programming algorithm <ref type=\"bibr\" target=\"#b9\">(Graves et al., 2006)</ref>. Given a target transcription y * , the ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" t =\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" t type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> and symbolic execution <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> techniques have be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: spectral interpretation and vertex domain localization <ref type=\"bibr\" target=\"#b129\">[130]</ref>, <ref type=\"bibr\" target=\"#b130\">[131]</ref>. Notions of stationarity can help develop probabilistic  essing methods leading to graph-based Wiener filtering <ref type=\"bibr\" target=\"#b131\">[132]</ref>, <ref type=\"bibr\" target=\"#b130\">[131]</ref>.</p><p>A study of vertex/spectral localization and uncer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he relational graph and apply it to both tasks.</p><p>Our entity classification model, similarly to <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref>, uses softmax classifiers at each node  nction or simply a linear transformation g m (h i , h j ) = W h j with a weight matrix W such as in <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref>. This type of transformation has been s tion. While we only consider such a featureless approach in this work, we note that it was shown in <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref> that it is possible for this class of m that operate on local graph neighborhoods <ref type=\"bibr\" target=\"#b8\">(Duvenaud et al. 2015;</ref><ref type=\"bibr\" target=\"#b16\">Kipf and Welling 2017)</ref> to large-scale relational data. These an d et al. 2015;</ref><ref type=\"bibr\" target=\"#b6\">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" target=\"#b16\">Kipf and Welling 2017)</ref> for large-scale and highly multi-relatio <ref type=\"bibr\" target=\"#b8\">(Duvenaud et al. 2015)</ref> and graph-based semi-supervised learning <ref type=\"bibr\" target=\"#b16\">(Kipf and Welling 2017)</ref>.</p><p>Motivated by these architectures. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Normalization (GN) as a simple alternative to BN. We notice that many classical features like SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref> and HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref> are grou ><p>The channels of visual representations are not entirely independent. Classical features of SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and GIST <  more abstract and their behaviors are not as intuitive. However, in addition to orientations (SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref>, or <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  unique characteristics of ERC are summarized as context dependence, persistence and contagiousness <ref type=\"bibr\" target=\"#b6\">[7]</ref> that the static and dynamic flows in a daily dialogue are bo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: put images and the ground-truth labels in CV tasks. To this end, Dosovitskiy et al. develop the ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which paves the way for transferring the success of transfo 2 , \u2022 \u2022 \u2022 , X n ] \u2208 R n\u00d7p\u00d7p\u00d73</formula><p>, where (p, p) is the resolution of each image patch. ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref> just utilizes a standard transformer to process the sequence 0 , Z 2 0 , \u2022 \u2022 \u2022 , Z n 0 ] \u2208 R (n+1)\u00d7d</formula><p>where Z class is the class token similar to ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and all of them are initialized as zero. In each layer, the rk Architecture</head><p>We build our TNT architectures by following the basic configuration of ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DeiT <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The patc that of DeiT <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The recent transformerbased models like ViT <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DeiT <ref type=\"bibr\" target=\"#b30\">[31]</ref> are compa ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and a head width of 64 is recommended for visual tasks <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. We adopt the head w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: educes the effectiveness of prefetching.</p><p>Recent solutions use the Global History Buffer (GHB) <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which organizes correlation information by storing recent uce the GHB as a general structure for prefetching streams of temporally correlated memory requests <ref type=\"bibr\" target=\"#b27\">[28]</ref>. However, when used to record address correlation <ref typ /DC prefetcher, which which learns the deltas, or differences, between consecutive memory addresses <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Delta correlation allows PC/DC to store all meta-data on  http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\">Using Nesbit and Smith's terminology<ref type=\"bibr\" target=\"#b27\">[28]</ref>, in which the name before the slash describes the referenc streams based on the PC of the loading instruction, which is known to improve coverage and accuracy <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta fetchers, SMS <ref type=\"bibr\" target=\"#b38\">[39]</ref>, which exploits spatial locality, and PC/DC <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, which uses delta  long streams. Rather than use address correlation, other GHBbased prefetchers use delta correlation <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, whose space requi -based prefetchers <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar ype=\"bibr\" target=\"#b42\">[43]</ref> or address correlation <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, sacrificing signif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" targe ting <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, sketch2image <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, and other image-to-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mpare the performance of Omegaflow with NoSQ against Omegaflow with a traditional LSQ and store set <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The average performance improvement is only 0.6%. Similar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, with adversarial training <ref type=\"bibr\" target=\"#b20\">[21]</ref> being one of the most effective methods. It formulates tra  Typically, using more attack iterations (higher value of k) produces stronger adversarial examples <ref type=\"bibr\" target=\"#b20\">[21]</ref>. However, each attack iteration needs to compute the gradi ct to traditional PGD-40.</p><p>We apply our technique on Madry's Adversarial Training method (MAT) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref> and e arget=\"#b38\">39]</ref> focuse on analyzing and improving adversarial machine learning. Madry et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> first formulate adversarial training as a min-max optimiza higher value of k (more attack iterations), PGDk can generate adversarial examples with higher loss <ref type=\"bibr\" target=\"#b20\">[21]</ref> els. This property is named as transferability. This prope rbations from previous epochs. To compare the attack strength of two attacks, we use Madry's method <ref type=\"bibr\" target=\"#b20\">[21]</ref> to adversarially train two models on MNIST and CIFAR10 and we integrate ATTA with two popular adversarial training methods: Madry's Adversarial Training (MAT) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref>. By e  efficiency</head><p>We select four state-of-the-art adversarial training methods as baselines: MAT <ref type=\"bibr\" target=\"#b20\">[21]</ref>, TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref>, YOPO <r ed in <ref type=\"bibr\" target=\"#b15\">[16]</ref> and is formulated as a min-max optimization problem <ref type=\"bibr\" target=\"#b20\">[21]</ref>. As one of the most effective defense methods, lots of wor , are widely adopted in various adversarial training methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar arget=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" tar jected back to S, k-step projected gradient descent method <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> (PGDk) has been widely adopted to generate adversarial exam  gradient descent <ref type=\"bibr\" target=\"#b15\">[16]</ref>) is adopted to conduct iterative attack <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Setup</head><p>Following the literature <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta  convolutional layers followed by three full-connected layers which is same architecture as used in <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. The adversarial p  with size = 0.3.</p><p>For the CIFAR10 dataset, we use the wide residual network  which is same as <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. The perturbation   a new perspective (e.g., improving transferability between epochs). which is same with other works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta re, and hyper-parameters used in this work.</p><p>MNIST. We use the same model architecture used in <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta  step size and set decay factor as 1 for M-PGD (momentum PGD).</p><p>CIFAR10. Following other works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: urally, multi-objective molecule design is much more challenging than the single-objective scenario <ref type=\"bibr\" target=\"#b16\">(Jin et al., 2020)</ref>.</p><p>This paper studies the problem of mul b)</ref>. Current state-of-the-art multi-objective molecular generation is a rationale-based method <ref type=\"bibr\" target=\"#b16\">(Jin et al., 2020)</ref>. In this approach, the authors propose to bu . It performs Bayesian optimization (BO) to guide molecules towards desired properties. RationaleRL <ref type=\"bibr\" target=\"#b16\">(Jin et al., 2020)</ref> is a state-of-the-art approach for multi-pro s better. Among them, the current state-of-the-art approach is a rationale-based method proposed by <ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>. In this method, the authors propose to build .org/ns/1.0\"><head n=\"4\">EXPERIMENTS</head><p>4.1 EXPERIMENT SETUP Biological objectives. Following <ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>, we consider the following two properties as  JNK3+QED+SA: Inhibiting GSK3\u03b2 or JNK3 while being drug-like and synthetically accessible. Following <ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>, we adopt QED <ref type=\"bibr\" target=\"#b2\">(  an adversarial loss is incorporated in the fitness evaluation.</p><p>Evaluation metrics. Following <ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>, we generate N = 5000 molecules for each appr re synthetically accessible, and 0.67 is the rescaled threshold corresponding to the one reported in<ref type=\"bibr\" target=\"#b16\">Jin et al. (2020)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ arget=\"#b30\">(Rogers &amp; Hahn, 2010)</ref> as input, and the positive threshold is set as \u03b4 = 0.5 <ref type=\"bibr\" target=\"#b16\">(Jin et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Li et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Recent advances in optimization-based approaches have evoked more interests in meta-learning. MAML <ref type=\"bibr\" target=\"#b3\">(Finn, Abbeel, and Levine 2017</ref>) is a general optimization algori separately, we effectively integrate all networks in a modified Model-Agnostic Meta-Learning (MAML) <ref type=\"bibr\" target=\"#b3\">(Finn, Abbeel, and Levine 2017)</ref> framework. An overview of our le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ble to rate trust better in the form of discrete verbal statements, rather than continuous measures <ref type=\"bibr\" target=\"#b28\">[29]</ref> . Verbal statements must introduce subjective fuzziness. F. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: stMult <ref type=\"bibr\" target=\"#b38\">(Yang et al., 2015)</ref>, as well as neural models like HolE <ref type=\"bibr\" target=\"#b19\">(Nickel et al., 2016)</ref> and ConvE <ref type=\"bibr\" target=\"#b8\">( >(Yang et al., 2015)</ref>, TransD <ref type=\"bibr\" target=\"#b13\">(Ji et al., 2015)</ref>, and HolE <ref type=\"bibr\" target=\"#b19\">(Nickel et al., 2016)</ref>. After extensive hyperparameter tuning, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b22\">22,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> to learn node representations via aggregating features of n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  centre location, other approaches were developed to support more complicated network constructions <ref type=\"bibr\" target=\"#b28\">[29]</ref>- <ref type=\"bibr\" target=\"#b30\">[31]</ref>. To accurately . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b60\">66]</ref>. Emerging programmable (\"smart\") NICs can help overcome this problem <ref type=\"bibr\" target=\"#b28\">[32]</ref>. There are many different types of offloads that can be im r from performance issues because embedded CPU cores add tens of microseconds of additional latency <ref type=\"bibr\" target=\"#b28\">[32]</ref>. Also, no existing manycore NICs provide performant mechan y, this makes them a poor fit for pipeline designs. To overcome this limitation, the Azure SmartNIC <ref type=\"bibr\" target=\"#b28\">[32]</ref> onloads computation from the programmable NIC to a core on -performance chaining. Further, manycore NICs even struggle to drive 100 Gbps and faster line-rates <ref type=\"bibr\" target=\"#b28\">[32]</ref>. Because a single embedded processor is not enough to satu for packets that only need to be processed by a hardware accelerator. For example, Firestone et al. <ref type=\"bibr\" target=\"#b28\">[32]</ref> report that processing a packet in one of the cores on a m  network stack, can reduce load on the general purpose CPU, reduce latency, and increase throughput <ref type=\"bibr\" target=\"#b28\">[32,</ref><ref type=\"bibr\" target=\"#b44\">48,</ref><ref type=\"bibr\" ta oads that run with low latency and at line-rate.</p><p>There exist many different programmable NICs <ref type=\"bibr\" target=\"#b28\">[32,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" ta get=\"#b44\">48,</ref><ref type=\"bibr\" target=\"#b53\">59,</ref><ref type=\"bibr\" target=\"#b38\">42,</ref><ref type=\"bibr\" target=\"#b28\">32,</ref><ref type=\"bibr\" target=\"#b33\">37,</ref><ref type=\"bibr\" tar oard FPGAs located as a \"bump-in-the-wire\" use this design <ref type=\"bibr\" target=\"#b48\">[52,</ref><ref type=\"bibr\" target=\"#b28\">32,</ref><ref type=\"bibr\" target=\"#b27\">31]</ref>, and other NICs use get=\"#b44\">48,</ref><ref type=\"bibr\" target=\"#b53\">59,</ref><ref type=\"bibr\" target=\"#b38\">42,</ref><ref type=\"bibr\" target=\"#b28\">32,</ref><ref type=\"bibr\" target=\"#b33\">37,</ref><ref type=\"bibr\" tar  to enable packets to be processed by a chain of functions <ref type=\"bibr\" target=\"#b48\">[52,</ref><ref type=\"bibr\" target=\"#b28\">32]</ref>. Chaining can be modified in these NICs today but requires  get=\"#b35\">39,</ref><ref type=\"bibr\" target=\"#b13\">15,</ref><ref type=\"bibr\" target=\"#b55\">61,</ref><ref type=\"bibr\" target=\"#b28\">32]</ref>. Each tenant may have its own offload chains that may need . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to obtain the graph representation. Hyper-parameters and Evaluation Metrics. We use Adam Optimizer <ref type=\"bibr\" target=\"#b25\">[26]</ref> with learning rate of 0.001 for all our models. For each d  vocabulary size for discretizing atomic distance as 30, and the mask ratio as 0.15. Adam optimizer <ref type=\"bibr\" target=\"#b25\">[26]</ref> with learning rate of 0.005 is utilized and we train 20 ep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: training data is less, usually pre-training and fine-tuning methods can achieve outstanding results <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In recent years, we have witnessed the impressive results o lassified into feature-based models and fine-tuning-based models according to their characteristics <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Feature-based methods mainly use pre-training models to pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b10\">[11]</ref>. Originally, transformers without convolutions were applied on pixels directly <ref type=\"bibr\" target=\"#b46\">[47]</ref>, even scaling to hundred of layers <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et al., 2019)</ref> of the output. Specific applications encourage the model to cover a given topic <ref type=\"bibr\" target=\"#b48\">(Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">See et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g/ns/1.0\"><head>C. Implementation Details</head><p>We implement the E2E methods based on the ESPnet <ref type=\"bibr\" target=\"#b48\">[49]</ref> codebase. The subword-based LID-42 model proposed in <ref   training. The weight of the CTC module \u03bb is set to 0.3 throughout the experiments following ESPnet <ref type=\"bibr\" target=\"#b48\">[49]</ref>. Beam size 10 is employed for joint decoding.</p></div> <d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> is another class of methods that prune models during traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b9\">[10]</ref>, and Gaussian mixture models <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref>. Recent work has . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: been proposed to this end, which compare networks' internal layers in addition to final predictions <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Sun et al. 202 e teacher layers into m buckets with approximately the same sizes and pick only one layer from each <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\">Sun et al. 2019)</ref>. Ther PKD is not the only model that utilizes internal layers' information. Other models such as TinyBERT <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019)</ref> and MobileBERT <ref type=\"bibr\" target=\"#b21. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type=\"bibr\" target=\"#b12\">[13]</ref>, as the answer sentence fails to be directly retrieved by  re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b31\">[32]</ref>, Res-GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, APPNP <ref type=\"bibr\" target=\"#b14\">[15]</ref>, AP-GCN <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGC <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b45\">35,</ref><ref type=\"bibr\" target=\"#b75\">65]</ref>, multimodally using images <ref type=\"bibr\" target=\"#b62\">[52]</ref>, via reasoning from relational tables <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (WSIs) <ref type=\"bibr\" target=\"#b21\">[22]</ref> has become a hot task in the medical imaging field <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" t  the survival prediction task by conducting a regression model on the WSI for each patient directly <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta ets are used in two modes. On the one hand, we follow the experimental settings of previous methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta c representation, such as CNNbased model <ref type=\"bibr\" target=\"#b21\">[22]</ref>, GCN-based model <ref type=\"bibr\" target=\"#b13\">[14]</ref> and FCN-based model <ref type=\"bibr\" target=\"#b18\">[19]</r ction task, taking the sampled patches as input to build up the graph structure. (4), DeepGraphSurv <ref type=\"bibr\" target=\"#b13\">[14]</ref> employs spectral GCN to take the topological relationships settings are used: replacing the hazard prediction using hypergraph representation by DeepGraphSurv <ref type=\"bibr\" target=\"#b13\">[14]</ref> (line 1); removing the survival rank prediction procedure   hypergraph representation can achieve better performance compared with that of using DeepGraphSurv <ref type=\"bibr\" target=\"#b13\">[14]</ref>. (2) The use of ranking information can improve the perfor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t taken by a student on a particular learning task, predict aspects of their next interaction x t+1 <ref type=\"bibr\" target=\"#b4\">[6]</ref>.</p><p>In the most ubiquitous instantiation of knowledge tra f binary variables, each of which represents understanding or non-understanding of a single concept <ref type=\"bibr\" target=\"#b4\">[6]</ref>. A Hidden Markov Model (HMM) is used to update the probabili. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b9\">Du and Cardie, 2020;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 20 l., 2020a)</ref>, and 4) question-answering <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2020b;</ref><ref type=\"bibr\" target=\"#b9\">Du and Cardie, 2020;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 20 \" target=\"#b54\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b43\">Wadden et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Du and Cardie, 2020)</ref>, and we denote it as ACE05-EN.</p><p>In add 021)</ref>; Multi-task TANL extends TANL by transferring structure knowledge from other tasks; EEQA <ref type=\"bibr\" target=\"#b9\">(Du and Cardie, 2020)</ref> and MQAEE <ref type=\"bibr\" target=\"#b18\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e decay of 0.999 through  all experiments. The entire network is trained end-to-end with TensorFlow <ref type=\"bibr\" target=\"#b38\">[39]</ref> distributed machine learning system on four NVidia V100 GP  x-axis is the iteration number and the y-axis is the value of activations. We use the Ten-sorBoard <ref type=\"bibr\" target=\"#b38\">[39]</ref> to visualize the distributions which shows the percentage . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">34]</ref>, or error detection <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Recent approaches . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 18\">[19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and transductive experimental design methods <ref type=\"bibr\" target=\"#b26\">[27]</ref>. These kinds of active learning algorithms are referred to ion matrix A. The selected samples are therefore considered to be the most representative.</p><p>In <ref type=\"bibr\" target=\"#b26\">[27]</ref>, an early active learning via a Transduction Experimental   problem to solve, thus an approximate solution by a sequential optimization problem is proposed in <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  nformative and representative examples for labeling using the min-max margin-based approach. 4. TED <ref type=\"bibr\" target=\"#b26\">[27]</ref> Active learning via Transduction Experimental Design is an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Caron et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> iteratively improve the learned representations by clusterin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for end-to-end learning to directly optimize ranking metrics <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> by creating a differentiable approximation. ApproxNDCG <ref t </ref><ref type=\"bibr\" target=\"#b7\">8]</ref> by creating a differentiable approximation. ApproxNDCG <ref type=\"bibr\" target=\"#b7\">[8]</ref> revisits the idea proposed in <ref type=\"bibr\" target=\"#b30\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aints that help generate more content rich responses that are based on a model of syntax and topics <ref type=\"bibr\" target=\"#b12\">(Griffiths et al., 2005)</ref> and semantic similarity <ref type=\"bib o estimate these distributions, we leverage the unsupervised model of topics and syntax proposed by <ref type=\"bibr\" target=\"#b12\">Griffiths and Steyvers (2005)</ref>. The second constraint encourages. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ation <ref type=\"bibr\" target=\"#b24\">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type=\"bibr\" target=\"#b5\">(Davison et al., 2019)</ref> and argumentative relation classification. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend our prior work <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and propose a hybrid Transformer-LSTM based architecture. T ot only require external language models but also lead to a slow inference. To tackle this problem, <ref type=\"bibr\" target=\"#b0\">[1]</ref> has proposed long short term memory (LSTM)-based encoderdeco <p>In this work, we propose a hybrid Transformer-LSTM architecture which combines the advantages of <ref type=\"bibr\" target=\"#b0\">[1]</ref> and <ref type=\"bibr\" target=\"#b5\">[6]</ref>. It not only has l.</p><p>The paper is organized as follows. Section 2 describes baseline architectures mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref> and <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Then, the propo res 2.1. LSTM-based encoder-decoder architecture</head><p>A LSTM-based encoder-decoder architecture <ref type=\"bibr\" target=\"#b0\">[1]</ref>, denoted as A1 in the rest of this paper, consists of a Bidi ayers respectively. Figure <ref type=\"figure\">1</ref>: LSTM-based encoder-decoder architecture (A1) <ref type=\"bibr\" target=\"#b0\">[1]</ref>, where the decoder acts as an independent language model.</p ords, the LSTM acts as an independent language model that can be easily updated with text-only data <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= tune the transferred model. This avoids a so-called catastrophic forgetting problem as mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Specifically, at each training iteration, we mix a batch of cond step, the model is further fine-tuned with the labeled data of the target language. Similar to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we empirically found that the second step is necessary to i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: > provide the theoretical grounds on permutation invariant functions for the set encoding. Further, <ref type=\"bibr\" target=\"#b22\">Lee et al. (2019a)</ref> propose Set Transformer, which uses attentio t H defined as follows: \u03c1( h\u2208H f (h)), where f and \u03c1 are mapping functions (see the proof of PMA in <ref type=\"bibr\" target=\"#b22\">Lee et al. (2019a)</ref>).</p><p>Since H is a countable set, there is l), which is inspired by the Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al., 2019a)</ref>, to compress the n nodes into the k typical  n (SelfAtt), inspired by the Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al., 2019a)</ref>, as follows (Figure <ref type=\"figure\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  they have some key limitations.</p><p>The first limitation is known as the \"oversmoothing\" problem <ref type=\"bibr\" target=\"#b10\">(Li et al., 2018)</ref>: the performance of GNNs degrade when stackin Hamilton et al. (2017)</ref>.</p><p>Oversmoothing. The oversmoothing problem was first discussed in <ref type=\"bibr\" target=\"#b10\">(Li et al., 2018)</ref>, which proved that by repeatedly applying Lap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: documents. As for the four types of word representations, we use the neural matching component KNRM <ref type=\"bibr\" target=\"#b45\">[46]</ref> to compute the interactive matching scores, i.e. \ud835\udc39 \ud835\udc43\ud835\udc4a , \ud835\udc39  ines, including neural ranking models, personalized search models and privacy enhanced models. KNRM <ref type=\"bibr\" target=\"#b45\">[46]</ref>: It is a neural model using kernels to extract features fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gnising Textual Entailment <ref type=\"bibr\" target=\"#b10\">(Fyodorov, Winter, and Francez 2000;</ref><ref type=\"bibr\" target=\"#b6\">Bowman et al. 2015)</ref>, and Question Answering <ref type=\"bibr\">(He. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r better generalization on target domains, as mentioned in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b66\">67]</ref>. <ref type=\"bibr\" t ptimizing Domain-specific Experts</head><p>As mentioned in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b66\">67]</ref>, exploiting the com type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Recently, works <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> on the mixture of experts <ref type=\"bibr\" target=\"#b27\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hniques in tasks like gene name recognition <ref type=\"bibr\" target=\"#b9\">(Kuksa and Qi, 2010;</ref><ref type=\"bibr\" target=\"#b23\">Tang et al., 2014;</ref><ref type=\"bibr\" target=\"#b24\">Vlachos and Ga. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s constraints. Similarly, FragFold <ref type=\"bibr\">(Kosciolek &amp; Jones, 2014)</ref> and Rosetta <ref type=\"bibr\" target=\"#b31\">(Ovchinnikov et al., 2016)</ref> incorporate constraints from a Potts. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s from intricate feature engineering. Many researchers managed to apply DNNs to weather predictions <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Among these wor  more robust prediction.</p><p>DNN based weather pattern mining. As a breakthrough work, Shi et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> develop the conventional LSTM and propose convolutional LST nowledge from it. This task fits perfectly with the convolutional long short-term memory (ConvLSTM) <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which has proven successful in processing ST tensors. Foll. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y are unfortunately not the only weak spot in machine learning systems.</p><p>Recently, Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> have demonstrated that data preprocessing used in machine  ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Image-Scaling Attacks</head><p>Recently, Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> have shown that scaling algorithms are vulnerable to attac aling algorithm. Both matrices can be computed in advance and are reusable. We refer to Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> for a description how to calculate L and R.</p><p>Based on  assignment.</p><p>We implement image-scaling attacks in the strong variant proposed by Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref>. We make a slight improvement to the original attacks: Ins  on rectangular blocks instead of columns and rows. As a result, the original attack by Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> is not applicable to this scaling algorithm. To attack are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  are several proposals addressing both source and path validation that fill the void, such as ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref> and OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>. However, fo itecture that separates general service functions from routing nodes, and its design is inspired by <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t Current techniques for both source authentication and path verification have been proposed in ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the Origin and Path Trace (OPT) <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acts important portions of a document <ref type=\"bibr\" target=\"#b2\">(Cheng &amp; Lapata, 2016;</ref><ref type=\"bibr\" target=\"#b31\">Nallapati et al., 2017;</ref><ref type=\"bibr\" target=\"#b32\">Narayan e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: larger group of symmetries have been shown to be more discriminative and powerful in recent studies <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" targ variant approaches that have seen recent progress, extending from mathematical framework derived in <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Specifically, <ref ty \">48]</ref>. Cohen and Welling later extend the domain of 2D CNNs from translation to finite groups <ref type=\"bibr\" target=\"#b4\">[5]</ref> and further to arbitrary compact groups <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b28\">(Navarin, Tran, and Sperduti 2018;</ref><ref type=\"bibr\" target=\"#b18\">Hu et al. 2019</ref><ref type=\"bibr\" target=\"#b17\">Hu et al. , 2020) )</ref> is to learn transferable prior knowledge fr t=\"#b18\">Hu et al. 2019)</ref>, or still require supervised information for graph-level pretraining <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref>). While at the node level, predicting links bet  is to learn a generic initialization for model parameters using readily available graph structures <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref><ref type=\"bibr\" target=\"#b18\">(Hu et al. , 2019 atasets. We conduct experiments on data from two domains: biological function prediction in biology <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020</ref>) and research field prediction in bibliography. ers with three unsupervised tasks to capture different aspects of a graph. More recently, Hu et al. <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> propose different strategies to pre-train grap  subgraph is centered at a paper and contains the associated information of For biology data, as in <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref>, we use 306,925 unlabeled protein ego-networks  that correspond to 40 binary classification tasks. We split the downstream data with species split <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref>, and evaluate the test performance with averag  across the graph's patch representations; (3) Context Prediction strategy (denoted by ContextPred) <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> to explore graph structures and (4) Attribute  2020)</ref> to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020)</ref> to learn the regularities of the node and edge which harms the generalization of the pre-trained GNNs. This finding confirms previous observations <ref type=\"bibr\" target=\"#b17\">(Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b31\">Rosenstein et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sufficient internet infrastructure indicate that this function is fulfilled by radio phone-in shows <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rogeneous data can be constructed by different methods according to their different data modalities <ref type=\"bibr\" target=\"#b45\">[46]</ref>. In this paper, we roughly divide these heterogeneous data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  obtained from meta-training tasks for a newly seen few-shot task such as intention classification, <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> present a relation classification dataset -Few  one layer convolutional neural networks (CNN). For ease of comparison, its details are the same as <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> proposed. Hierarchical Attention In order to g  with the CNN encoder. For the neural networks based baselines, we use the same hyper parameters as <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> proposed.</p><p>For our hierarchical attention or 5 way 5 shot and 10 way 5 shot settings on FewRel test set.</figDesc><table /><note>* reported by<ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> and \u25c7 reported by<ref type=\"bibr\" target=\"#b3\" as achieved excellent performance in few-shot image classification and few-shot text classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Gao et al., 2019 ttp://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Datasets</head><p>FewRel Few-Shot Relation Classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018)</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b1\">2]</ref> and MT <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= ns/1.0\"><head>Fine-tuning set</head><p>In-domain Out-of-domain Real + one-speaker TTS synthetic 59. <ref type=\"bibr\" target=\"#b4\">5</ref> 19.5 Only one-speaker TTS synthetic 38.5 13.8</p><p>Table <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ods include AdaBoost <ref type=\"bibr\" target=\"#b10\">(Freund and Schapire, 1997)</ref> and RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref>, which target at classification and rankin ng model that makes more accurate predictions should receive a higher weight. Inspired by RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref>, we reduce the ranking combination problem  = {q1 = (The Tale of Genji, country, ?t) q2 = (The Tale of Genji, genre, ?t)} Similar to RankBoost <ref type=\"bibr\" target=\"#b9\">(Freund et al., 2004)</ref> Ranking loss. The overall objective of KEn <p>Let the set of all the critical entity pairs from all the validation queries of an entity as P . <ref type=\"bibr\" target=\"#b9\">Freund et al. (2004)</ref> have proved that, when using RankBoost, thi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to assign probabilities to sequences of text; this is similar to using them in a generative fashion <ref type=\"bibr\" target=\"#b35\">(Wang and Cho, 2019)</ref> and has previously been investigated by <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" targ for experiments: \u2022 AMiner-AND<ref type=\"foot\" target=\"#foot_0\">1</ref> . The dataset is released by <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, which contains 500 author names for traini (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rsation and recommendation for developing an effective CRS <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><p>Different pr by reinforcement learning (RL) methods for policy learning <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. As shown in Figure gue actions. (4) Multi-round Conversational Recommendation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Under this problem model. In order to reduce the action space in policy learning, another state-of-the-art method SCPR <ref type=\"bibr\" target=\"#b13\">[14]</ref> only considers learning the policy of when to ask or recom  <ref type=\"bibr\" target=\"#b25\">[26]</ref>, EAR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and SCPR <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>domains or applications, since there are three diff hough the connectivity of the graph can also be used to eliminate invalid actions by path reasoning <ref type=\"bibr\" target=\"#b13\">[14]</ref>, there are still a large number of candidates left for act  target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> and (ii) when to ask or recommend <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>In order to simplify the overall framework of MCR w  user-specified attribute \ud835\udc5d 0 , i.e., cand to ask attributes. Following the path reasoning approach <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we have</p><formula xml:id=\"formula_3\">\ud835\udc60 0 = [[{\ud835\udc5d 0 }, {} d><p>A large action search space will harm the performance of the policy learning to a great extent <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Thus, it attaches great importance to handle the overwhel tter eliminate the uncertainty of candidate items, but also encode the user preference. Inspired by <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we adopt weighted entropy as the criteria to prune candid d build a 2-layer taxonomy with 29 first-layer categories for Yelp. \u2022 LastFM* and Yelp*. Lei et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> consider that it is not realistic to manually merge attrib een the conversation and recommendation components with a similar RL framework as CRM.</p><p>\u2022 SCPR <ref type=\"bibr\" target=\"#b13\">[14]</ref>. This is the state-of-the-art method on MCR setting, which <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2.4\">Implementation Details.</head><p>Following <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we split the E-Commerce dataset by 7:1.5:1.5 for training CRM<ref type=\"bibr\" target=\"#b25\">[26]</ref>, EAR<ref type=\"bibr\" target=\"#b11\">[12]</ref>, and SCPR<ref type=\"bibr\" target=\"#b13\">[14]</ref>.</figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns res extra efforts to train an offline recommendation model <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> or pretrain the policy network with synthetic dialogue hist e multi-round conversational recommendation (MCR) scenario <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, the most realistic conversational recommendation setting p  updated by P 4.1.4 Reward. Following previous MCR studies <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, our environment contains five kinds of rewards, namely, (1 tion history in the current state. Unlike previous studies <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> that adopt heuristic features for conversation history mode evious studies on multiround conversational recommendation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, we adopt success rate at the turn \ud835\udc61 (SR@\ud835\udc61) <ref type=\"bibr isodes. To maintain a fair comparison with other baselines <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, we adopt the same reward settings to train the proposed mo many successful applications of graphbased RL methods on different scenarios of recommender systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: le, reordering algorithms often obtain large speedups on graphs with power-law degree distributions <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref>, which near neighbor graphs do not have. I lished in a single pass through the graph.</p><p>Degree-Based Grouping. Degree-Based Grouping (DBG) <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref> is an extension of hub clustering to multi escending degree order, but the order of the nodes within each group is not changed. The authors of <ref type=\"bibr\" target=\"#b12\">(Faldu et al., 2019)</ref> use logarithmically spaced thresholds to e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nter ))</formula><p>It is worth noticing that to better promote the learning of intra-relationships <ref type=\"bibr\" target=\"#b9\">(Hu et al. 2020;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al. 2018)<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b95\">[56]</ref> represents the hardware dataflow using Halide's scheduling language <ref type=\"bibr\" target=\"#b84\">[45]</ref> for design exploration. Interstellar extends Halide with a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. In standard classification, minimising categorical cross e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b38\">, Zhu et al., 2019</ref><ref type=\"bibr\" target=\"#b37\">, Zhang and Zitnik, 2020]</ref>. To achieve fairness, existing work d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ther relate them to an act of deceit <ref type=\"bibr\" target=\"#b52\">[52]</ref>. Ekman and Rosenberg <ref type=\"bibr\" target=\"#b53\">[53]</ref> developed the Facial Action Coding System (FACS) to taxono. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed in the MSA. These patterns are in turn associated with the structure and function of the protein <ref type=\"bibr\" target=\"#b15\">(G\u00f6bel et al., 1994)</ref>. Unsupervised contact prediction aims to d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vily dependent on the end-to-end data.</p><p>As our second contribution, we apply a two-stage model <ref type=\"bibr\" target=\"#b23\">(Tu et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Kano et al., 20  intermediate representation closely tied to the source text. The architecture has been proposed by <ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref> to realize a reconstruction objective, and a s iliary ASR and MT training data ( \u00a73). This model is similar to the architecture first described by <ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref>. It combines two encoder-decoder models in a c , we apply beam search only for the second stage decoder. We do not use the two-phase beam search of<ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref> because of its prohibitive memory requirements. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oustic model with a \"bottleneck\" layer using a frame based criterion on a large multilingual corpus <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target ilingual models can be adapted to the specific language to improve performance further. The work by <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> presented bottleneck ific softmax layers, which are trained using cross-entropy <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. This architecture ca ) if X \u2208 XL1 softmax(WL2e + bL2) if X \u2208 XL2 . . . softmax(WLne + bLn) if X \u2208 XLn</formula><p>Unlike <ref type=\"bibr\" target=\"#b4\">[5]</ref>, we do not have any bottleneck layer, and the whole model is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: age model pretraining has had a tremendous impact on solving many natural language processing tasks <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Radford, 2018  the parameters of the language model are frozen and a task-specific head is trained on top of them <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018)</ref>. The second approach fine-tunes all model   <ref type=\"bibr\" target=\"#b8\">(Radford, 2018)</ref>. The latter can sometimes yield better results <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2019)</ref>, while the first one usually offers better. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f entities <ref type=\"bibr\">[Cheng, 2020;</ref><ref type=\"bibr\" target=\"#b11\">Li et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Cheng et al., 2020]</ref>. The above transformationbased methods are t l KG that has been popularly used for evaluating keyword querying. We reused 50 queries provided by <ref type=\"bibr\" target=\"#b5\">[Coffman and Weaver, 2014]</ref> but removed those containing unmatcha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvised methods to take advantages of unlabeled speech data <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>, data augmentatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: phs. To address this issue, different sampling-based methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  competitions <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>- <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Nevertheless, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the graph h t evw and updating them analogously to equations 1 and 2. Of the existing MPNNs, only <ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref> has used this idea.</p><p>Convolutional N  T v . Note the original work only defined the model for T = 1.</p><p>Molecular Graph Convolutions, <ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref> This work deviates slightly from other MP ed features we include two existing baseline MPNNs, the Molecular Graph Convolutions model (GC) from<ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref>, and the original GG-NN model<ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s by aggregating information from neighbors, which can be seen as a special form of low-pass filter <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b9\">Li et al. 2019)</ ing process of FAGCN. Here, we formally define the whole architecture of FAGCN. Some recent studies <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019;</ref><ref type=\"bibr\" target=\"#b2\">Cui et al. 2020</ n kernel  <ref type=\"figure\">2</ref>, compared with traditional low-pass filters, e.g., GCN and SGC <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019)</ref></p><formula xml:id=\"formula_7\">F 2 L with g \u03b8  </head><p>We compare FAGCN with two types of representative GNNs: Spectral-based methods, i.e., SGC <ref type=\"bibr\" target=\"#b20\">(Wu et al. 2019)</ref>, GCN <ref type=\"bibr\" target=\"#b7\">(Kipf and W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: highly scalable, as it is in the heart of many large-scale distributed graph storage layers such as <ref type=\"bibr\" target=\"#b51\">(Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b50\">Zheng et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s. Checkpointed Warming (CW) takes a checkpoint of the microarchitecture state prior to each region <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Unfortunately, ch erages functional simulation to get to the next representative region, which is slow. Checkpointing <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> takes a snapshot o s an accuracy/speed/overhead trade-off. As mentioned in the introduction, checkpointed warming (CW) <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> is fast, requires  ons to make checkpoints transferable across cache structures <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Functional Warming ich takes a checkpoint of the microarchitecture state prior to each detailed region. Wenisch et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref> store the state of the caches and other micro-architectura. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: olves these problems using the following techniques:</p><p>\u2022 Reversible layers, first introduced in <ref type=\"bibr\" target=\"#b7\">Gomez et al. (2017)</ref>, enable storing only a single copy of activa  term: the b \u2022 n h \u2022 l \u2022 d k ,</formula><p>RevNets. Reversible residual networks were introduced by <ref type=\"bibr\" target=\"#b7\">Gomez et al. (2017)</ref> where it was shown that they can replace Res mer model's self-attention mechanism <ref type=\"bibr\" target=\"#b21\">(Sukhbaatar et al., 2019a;</ref><ref type=\"bibr\" target=\"#b7\">b)</ref> have also recently been explored.</p><p>In particular, levera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have demonstrated their effectiveness in classifying node l  and employ several popular GNN models including GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref>, GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>, SAGE <ref type=\"bibr\" target=\"#b6\">[7]</ref>, APPNP <ref  s a variant of convolutional neural networks that operates on graphs. Graph Attention Network (GAT) <ref type=\"bibr\" target=\"#b28\">[29]</ref> further employed attention mechanism in the aggregation of ramework can be an arbitrary GNN model such as GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> or GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>. We denote the pretrained classifier in a teacher model as ssification accuracies with teacher models as GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>. \u2022 Citeseer <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a o the number of layers and we employ the most widely-used 2-layer setting in this work.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref> improves GCN by incorporating attention mechanism which as ze, 0.01 as learning rate, 0.8 as dropout probability and 0.001 as learning rate decay.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>: we use 64 as hidden-layer size, 0.01 as learning rate, 0.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s h a 1 _ b a s e 6 4 = \" e A Z 8 7  This paper presents progress in diffusion probabilistic models <ref type=\"bibr\" target=\"#b49\">[50]</ref>. A diffusion probabilistic model (which we will call a \"di onals in p \u03b8 (x t\u22121 |x t ), because both processes have the same functional form when \u03b2 t are small <ref type=\"bibr\" target=\"#b49\">[50]</ref>. A notable property of the forward process is that it admi ing to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance <ref type=\"bibr\" target=\"#b49\">[50]</ref>.</p><p>Second, to represent the mean \u00b5 \u03b8 (x t , t), we pro orithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. <ref type=\"bibr\" target=\"#b49\">[50]</ref>, not yet as a practical compression system. </p></div> <di educed variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. <ref type=\"bibr\" target=\"#b49\">[50]</ref>; we include it here only for completeness.</p><formula xml ments so that the number of neural network evaluations needed during sampling matches previous work <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. We set the forwar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: odels with numerous parameters are shown to have better effectiveness and good representation power <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Recently, with the great impact of neural networks on com. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type=\"bibr\" target=\"#b25\">(Peters et al., 2018)</ref>, GPT-2 <ref type=\"bibr\" target=\"#b26\">(Ra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs of existing entities in a vocabulary. Recent approaches <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> employ anchor-based hashing techniques to factorize sparse . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualization <ref type=\"bibr\" target=\"#b28\">[28]</ref>. This optimization statically converts an indirect branch  ly a subset of indirect branches with a limited number of targets that can be determined statically <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Our proposed VPC prediction mechanism provides the benefi 24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> classify the devirtualization techniques into guarded devi on can overcome this limitation, but it requires an expensive mechanism called on-stack replacement <ref type=\"bibr\" target=\"#b28\">[28]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  tion based on static analysis requires type analysis, which in turn requires whole program analysis <ref type=\"bibr\" target=\"#b28\">[28]</ref>, and unsafe languages like C\u00fe\u00fe also require pointer alias  or large applications. Due to the limited applicability of static devirtualization, Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> report only an average 40 percent reduction in the number  et=\"#b23\">[23]</ref>, and type feedback/devirtualization <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. As we show in Section 6, the benefit of devirtualization . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s in distinguishing geometric graph properties, such as girth and circumference, etc. Other studies <ref type=\"bibr\" target=\"#b25\">(Ingraham et al., 2019;</ref><ref type=\"bibr\" target=\"#b45\">Simm et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inimal group miss ratio) and required the assumption that the individual miss ratio curve be convex <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Our algorithm uses dynamic programming to examine the entir he cache between instructions and data, and for multiprogramming by giving each process a partition <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>In the way of optimizing their algorithm, they proved ss-rate derivative. The allocation is optimal if the miss-rate derivatives are as equal as possible <ref type=\"bibr\" target=\"#b8\">[9]</ref>. The optimality depends on several assumptions. One is that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: fferent applications <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b71\">[72]</ref>, random erasing <ref type=\"bibr\" target=\"#b78\">[79]</ref> and stochastic depth <ref type=\"bibr\" target=\"#b33\">[34]</ref>, but not repeated augmentation <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ty of parameters contained in the Transformer-based models <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>- <ref type=\"bibr\" target=\"#b29\">[30]</ref>, recent literat type=\"bibr\" target=\"#b31\">[32]</ref> for parameter-efficient adaptation of pre-trained Transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> on various down tion <ref type=\"bibr\" target=\"#b36\">[37]</ref> of large-scale pre-trained language models like BERT <ref type=\"bibr\" target=\"#b27\">[28]</ref> and XLM <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Li et a o-sequence ASR task while they experiment on text classification tasks based on the pretrained BERT <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p><p>Some researchers have proposed to apply the Adapter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pwords like of and on. Therefore, a better mechanism for local weights is needed.</p><p>Inspired by <ref type=\"bibr\" target=\"#b59\">[60]</ref>, we propose a local weight network based on distributed wo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, which utilize the unl get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b13\">[14]</ref>, which was shown to be useful in large-scale conditional generation tasks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b32\">34]</ref>.</p><p>The model-agn \"#b17\">[19]</ref>, but replace downsampling and upsampling layers with residual blocks similarly to <ref type=\"bibr\" target=\"#b5\">[6]</ref> (with batch normalization [15] replaced by instance normaliz tional and fully connected layers in all the networks. We also use self-attention blocks, following <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b40\">[42]</ref>. They are inse. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: urveillance systems to automatically detect suicidal behaviors and trigger an alarm. In this sense, <ref type=\"bibr\" target=\"#b9\">(Lee et al., 2014)</ref> presented a method for automatically analyzin ide by hanging attempts <ref type=\"bibr\" target=\"#b1\">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=\"bibr\" target=\"#b9\">(Lee et al., 2014)</ref>, we performed our experiments on a large data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b42\">46,</ref><ref type=\"bibr\" target=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" target=\"#b32\">36,</ref><ref type=\"bibr\" tar get=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b36\">40,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" target=\"#b32\">36,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd pursued in natural language processing <ref type=\"bibr\" target=\"#b9\">[10]</ref>, computer vision <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and other domains. To date, the most powerful solution is e how to define (dis)similar instances.</p><p>Q2: Define (dis)similar instances. In computer vision <ref type=\"bibr\" target=\"#b16\">[17]</ref>, two random data augmentations (e.g., random crop, random  d by the graph encoder, the final d-dimensional output vectors are then normalized by their L2-Norm <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>A running example. We illustrate a running example  ffectively build and maintain the dictionary, such as end-to-end (E2E) and momentum contrast (MoCo) <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We discuss the two strategies as follows.</p><p>E2E sampl propagation. The parameters of f k (denoted by \u03b8 k ) are not updated by gradient descent. He et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> propose a momentum-based update rule for \u03b8 k . More formal  the dictionary, such as memory bank <ref type=\"bibr\" target=\"#b58\">[59]</ref>. Recently, He et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> show that MoCo is a more effective option than memory bank >Contrastive loss mechanisms. The common belief is that MoCo has stronger expression power than E2E <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and a larger dictionary size K always helps. We also obse r, the effect of a large dictionary size is not as significant as reported in computer vision tasks <ref type=\"bibr\" target=\"#b16\">[17]</ref>. For example, MoCo (K = 16384) merely outperforms MoCo (K   in Table <ref type=\"table\" target=\"#tab_6\">5</ref> in the Appendix. Momentum. As mentioned in MoCo <ref type=\"bibr\" target=\"#b16\">[17]</ref>, momentum m plays a subtle role in learning high-quality r tasets. For US-Airport, the best performance is reached by m = 0.999, which is the desired value in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, showing that building a consistent dictionary is importan  brings better performance. Moreover, we do not observe the \"training loss oscillation\" reported in <ref type=\"bibr\" target=\"#b16\">[17]</ref> when setting m = 0. GCC (MoCo) converges well, but the acc ings.</p><p>In computer vision, a large collection of work <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" tar  objectives for graph structured data. Inspired by the recent success of contrastive learning in CV <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref> and NLP <ref type= ats each instance as a distinct class of its own and learns to discriminate between these instances <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>. The promise is th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b26\">Roy et al. (2020)</ref>, where k-means finds out the most related keys. <ref type=\"bibr\" target=\"#b29\">Tay et al. (2020a)</ref> learns block permutation for block-wise spar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: targeted network, which corresponds to a more restrictive black box threat model.</p><p>Recent work <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b1\">Bhagoji et al., =\"bibr\" target=\"#b10\">Ilyas et al., 2017)</ref> provides a number of attacks for this threat model. <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> show how to use a basic primitive of zeroth o (x, ) (x l\u22121 + \u03b7 s l ) with s l = \u03a0 \u2202Bp(0,1) \u2207 x L(x l\u22121 , y)<label>(4)</label></formula><p>Indeed, <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> were the first to use finite differences meth ty d=268,203 and thus this method would require 268,204 queries. (It is worth noting, however, that <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> developed additional methods to, at least par. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to successfully inpaint large regions. Despite using no learning, the results may be comparable to <ref type=\"bibr\" target=\"#b14\">[15]</ref> which does. The choice of hyper-parameters is important (f ng). Yet, it works surprisingly well for other situations. We compare to a learning-based method of <ref type=\"bibr\" target=\"#b14\">[15]</ref> in fig. <ref type=\"figure\" target=\"#fig_5\">6</ref>. The de t to successfully inpaint large regions. Despite using no learning, the results may be comparable to<ref type=\"bibr\" target=\"#b14\">[15]</ref> which does. The choice of hyper-parameters is important (f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cessful adoptions of diffusion probabilistic models (diffusion models for short) in image synthesis <ref type=\"bibr\" target=\"#b1\">[Ho et al., 2020]</ref> and speech synthesis <ref type=\"bibr\" target=\"  synthesis tasks <ref type=\"bibr\" target=\"#b2\">[Kong et al., 2020]</ref> and image synthesis fields <ref type=\"bibr\" target=\"#b1\">[Ho et al., 2020]</ref>. However, to the best of our knowledge, diffus ion, to provide a basic understanding of diffusion probabilistic models (diffusion model for short) <ref type=\"bibr\" target=\"#b1\">[Ho et al., 2020]</ref>, we first briefly review its formulation.</p>< al lower bound (ELBO) on negative log likelihood and introduce KL divergence and variance reduction <ref type=\"bibr\" target=\"#b1\">[Ho et al., 2020]</ref>:</p><formula xml:id=\"formula_4\">E[\u2212 log p \u03b8 (x <p>Diffusion probabilistic models <ref type=\"bibr\" target=\"#b10\">[Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b1\">Ho et al., 2020]</ref> are a kind of generative models using a Markov  019]</ref>. Then the LR information is fused with the 2D-convolution block output hidden. Following <ref type=\"bibr\" target=\"#b1\">Ho et al. [2020]</ref>, we transform the timestep   </p><formula xml:i ng of the conditional noise predictor uses Eq. ( <ref type=\"formula\">6</ref>) as loss term and Adam <ref type=\"bibr\" target=\"#b1\">[Kingma and Ba, 2014]</ref> as optimizer, with batch size 16 and learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ven their complementary strengths and weaknesses <ref type=\"bibr\">(d'Avila Garcez et al. 2015;</ref><ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\" target=\"#b47\">Yang nowledge is compiled into a neural network architecture <ref type=\"bibr\">(Bo\u0161njak et al. 2017;</ref><ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\">Evans and Grefenst retability and generalisation, thereby inheriting the best of both worlds. Among such systems, NTPs <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\" target=\"#b32\">Min div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>End-to-end Differentiable Proving</head><p>NTPs <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017)</ref> recursively build a neural networ l atoms in the body need to be proven, and because Z is a free variable with many possible bindings <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017)</ref>. We consider two problems -given  (F) log[1 \u2212 ntp K \u03b8 ( F, d)]<label>(4)</label></formula><p>NTPs can also learn interpretable rules. <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> show that it is possible to learn tes. Although NTPs can be used for learning interpretable rules from data, the solution proposed by <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> can be quite inefficient, as the  <ref type=\"bibr\" target=\"#b21\">(Kemp et al. 2006</ref>) -following the same evaluation protocols as <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>. Furthermore, since GNTPs allows   Prediction Results. We compare GNTPs and NTPs on a set of link prediction benchmarks, also used in <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>. Results, presented in Table 1, s  predicates, 111 unary predicates, 14 constants and 2565 true facts. We follow the protocol used by <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> and split every dataset into trai ww.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_0\">For consistency, we use the same notation as<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>.</note> \t\t\t<note xmlns=\"http://ww s parallel inference to be implemented very efficiently on GPU. This optimisation is also present in<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>.</note> \t\t\t<note xmlns=\"http://ww  \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"7\" xml:id=\"foot_5\">Results reported in<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> were calculated with an incorrect 158 facts about the neighbourhood of countries, and the location of countries and subregions. As in <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>, we randomly split countries into. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b40\">Zhao et al., 2017;</ref><ref type=\"bibr\" target=\"#b33\">Rudinger et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Gonen and Goldberg, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Bordia a r\" target=\"#b41\">Zhao et al., 2018;</ref><ref type=\"bibr\" target=\"#b32\">Ravfogel et al., 2020;</ref><ref type=\"bibr\" target=\"#b14\">Gonen and Goldberg, 2019)</ref>, many of them being based on predefin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e to the space limit, we will skip the description of the basic chain LSTM and readers can refer to <ref type=\"bibr\" target=\"#b11\">Hochreiter and Schmidhuber (1997)</ref> for details. Briefly, when mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ding parameters) that are estimated to have the least effect on the loss. They build on the work of <ref type=\"bibr\" target=\"#b21\">Molchanov et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b6\">Figur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rding the strategy for selecting instances inside the bag, the soft attention mechanism proposed by <ref type=\"bibr\" target=\"#b12\">Lin et al. (2016)</ref> is widely used for its better performance tha results in data noise and causes classical supervised RE models hard to train. To solve this issue, <ref type=\"bibr\" target=\"#b12\">Lin et al. (2016)</ref>   <ref type=\"bibr\" target=\"#b5\">(Defferrard e K is the bag size. As for the choice of weight \u03b1 i , we follow the soft attention mechanism used in <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2016)</ref>, where \u03b1 i is the normalized attention score ttp://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Evaluation Metrics</head><p>Following previous literature <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Vashishth et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erent KGs. Recent works on multilingual KG embeddings provide support for automated entity matching <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017</ref><ref type=\"bibr\" target=\"#b4\">(Chen et al., ,   extended embedding models to bridge multiple KGs, typically for KGs of multiple languages. MTransE <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref> jointly learns a transformation across two s The embedding learning process jointly trains the knowledge model and the alignment model following <ref type=\"bibr\" target=\"#b5\">Chen et al. (2017)</ref>, while self-learning is added to improve the  G i and G j . \u03bb is a positive hyperparameter that weights the two model components.</p><p>Following <ref type=\"bibr\" target=\"#b5\">Chen et al. (2017)</ref>, instead of directly optimizing J in Eq. ( <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eveloped algorithms to improve the performance of models that have been trained with very few data. <ref type=\"bibr\" target=\"#b3\">Finn et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b23\">Snell et al.  challenge. One of the most general algorithms for meta-learning is the optimizationbased algorithm. <ref type=\"bibr\" target=\"#b3\">Finn et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b7\">Li et al. (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: be enforced for multi-modal pre-training. Thanks to the recent progress of self-supervised learning <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecific sequence generation. A coverage driven test generation technique is presented by Fine et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Shen et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> have us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: timization <ref type=\"bibr\" target=\"#b3\">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type=\"bibr\" target=\"#b11\">(Finn et al., 2017)</ref>. In essence, we turn the gradient-based opt </ref> or initial weights that enable rapid adaptation to new tasks or domains in few-shot learning <ref type=\"bibr\" target=\"#b11\">(Finn et al., 2017)</ref>.</p><p>Meta-gradients (e.g., gradients w.r. s is expensive both from a computational and a memory point-of-view.</p><p>To alleviate this issue, <ref type=\"bibr\" target=\"#b11\">Finn et al. (2017)</ref> propose a first-order approximation, leading thus more knowledge than all competing methods. First-order refers to the approximation proposed by <ref type=\"bibr\" target=\"#b11\">Finn et al. (2017)</ref>, i.e. ignoring all second-order derivatives.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing <ref type=\"bibr\" target=\"#b6\">[7]</ref> and graph data <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar ral networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Similarly, the generative framework GPT-GNN <ref type=\"bibr\" target=\"#b14\">[15]</ref> employs a self-supervised attributed graph generation task. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: of the attention layers are set to 500. In the graph transformer encoder, the FFN block has a PReLU <ref type=\"bibr\" target=\"#b33\">[23]</ref> activation function with an intermediate size of 2000 unit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us approaches have been proposed for reducing the model size and inference latency. Several methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: @{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we apply the leave. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b51\">[52,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. A popular learning paradigm is graphbased / hypergraph-bas ral network f (G, X) <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> (X contains the initial features on the vertices for exampl learning problem on the approximation. While the state-of-the-art hypergraph neural networks (HGNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> approximates each hyperedge by a clique and hence requires  detailed experimentation, we demonstrate their effectiveness compared to the state-of-the art HGNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> and other baselines (Sections 5, and 7). \u2022 We thoroughly d =\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Hypergraph neural networks <ref type=\"bibr\" target=\"#b16\">[17]</ref> and their variants <ref type=\"bibr\" target=\"#b22\">[23,</re 2 . Our approach requires at most a linear number of edges (1 and 2|e| \u2212 3 respectively) while HGNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> requires a quadratic number of edges for each hyperedge. < yperGCN and FastHyperGCN against the following baselines:</p><p>\u2022 Hypergraph neural networks (HGNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses the clique expansion <ref type=\"bibr\" target=\"#b51\">[  [7].Our approach requires at most a linear number of edges (1 and 2|e| \u2212 3 respectively) while HGNN<ref type=\"bibr\" target=\"#b16\">[17]</ref> requires a quadratic number of edges for each hyperedge.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he above problems, being unable to identify path deviation <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Recently, there are several proposals addressing both sour. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> and matrix factorization based like NetSMF <ref type=\"bibr\" target=\"#b27\">[28]</ref> and ProNE <ref type=\"bibr\" target=\"#b28\">[29]</ref>, largely focus on the neighborhood characteristics. Neural f> to encode publication contents and then adapt the spectral propagation process proposed in ProNE <ref type=\"bibr\" target=\"#b28\">[29]</ref> to incorporate structure information as shown in Figure <r owing experiments.  For encoding the structure of G, we take spectral propagation proposed in ProNE <ref type=\"bibr\" target=\"#b28\">[29]</ref> to incorporate both neighborhood and global features. For  3 For ProNE, the embedding dimension is 32 and the order of Chebyshev expansion is 10, according to <ref type=\"bibr\" target=\"#b28\">[29]</ref>. 4 For node2vec, the embedding dimension is 32. Walk lengt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  limits their use in the real application scenarios. Recently, one-shot voice conversion approaches <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\">17,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" tar , recent trackers use either one or multi-stage BBR (e.g., <ref type=\"bibr\" target=\"#b16\">[18,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\">54, on (mBBR). As a single step of regression usually works well (as observed in recent studies as well <ref type=\"bibr\" target=\"#b30\">[32,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref>), mBBR works effic chanism <ref type=\"bibr\" target=\"#b16\">[18,</ref><ref type=\"bibr\">54]</ref> and deeper architecture <ref type=\"bibr\" target=\"#b30\">[32,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>.</p><p>Our work is MP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, C-RPN <ref type=\"bibr\" target=\"#b16\">[18]</ref>, SiamDW < ref> with 0.523 success score, we obtain considerable gains by 4.8%. Our MART outperforms SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref> by 7.5% in success plot. In addition, compared to multi-st re our MART to 9 state-of-the-art trackers (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref>, SiamFC <ref oposed MART to 12 state-ofthe-art trackers (DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref>, C-RPN <ref  ef> (c), our approach achieves competitive result with success score of 0.678 compared to SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref> and DiMP <ref type=\"bibr\" target=\"#b2\">[3]</ref>. In compa nificantly outperforms ATOM <ref type=\"bibr\" target=\"#b7\">[8]</ref> with EAO of 0.292 and SiamRPN++ <ref type=\"bibr\" target=\"#b30\">[32]</ref> with EAO of 0.285.</p></div> <div xmlns=\"http://www.tei-c.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: number of clusters reaches the specified K.</p><p>When K is unknown, we adopt an optimal modularity <ref type=\"bibr\" target=\"#b21\">[22]</ref> partitioning mechanism to determine the partition of publi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  according to the statistical characters of objects in each grid. Graph-based methods, such as SCAN <ref type=\"bibr\" target=\"#b43\">[44]</ref> and spectral clustering <ref type=\"bibr\" target=\"#b36\">[37. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  itself.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. Benchmarks</head><p>DPDK. DPDK <ref type=\"bibr\" target=\"#b37\">[38]</ref> is a popular networking application development library fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[35]</ref>. However, estimating MI is generally intractable in highdimensional continuous settings <ref type=\"bibr\" target=\"#b25\">[25]</ref>. MINE <ref type=\"bibr\" target=\"#b1\">[1]</ref> derives a lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bitively slow for larger molecules <ref type=\"bibr\" target=\"#b45\">[Shim and MacKerell Jr, 2011</ref><ref type=\"bibr\" target=\"#b3\">, Ballard et al., 2015</ref><ref type=\"bibr\" target=\"#b8\">, De Vivo et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tate-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by <ref type=\"bibr\" target=\"#b16\">[16]</ref>.</p></div> \t\t\t</abstract> \t\t</profileDesc> \t</teiHeader> \t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d then share the weights across subnets. They could be further categorized as two types: path-based <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe on constrain. A comparison between the architectures searched by NEAS and other NAS methods such as <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> is presented in Fig \"><head n=\"4.1.\">Implementation Details</head><p>Search Space. Consistent with previous NAS methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe la><p>We then formulate NEAS as a two-stage optimization problem like other one-shot methods (e.g., <ref type=\"bibr\" target=\"#b9\">[10]</ref>). The firststage is to optimize the weight of the supernet  \"table\">1</ref>. Comparison of state-of-the-art NAS methods on ImageNet. \u2020: TPU days, : reported by <ref type=\"bibr\" target=\"#b9\">[10]</ref>, \u2021: searched on CIFAR-10, \"-\" means not reported. \u2666: Tested A.</p><p>Supernet Training. We train the supernet for 120 epochs using the settings similar to SPOS <ref type=\"bibr\" target=\"#b9\">[10]</ref>: SGD optimizer with momentum 0.9 and weight decay 4e-5, ini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  attention\" works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref>. The difference i nearized attention <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, which is formulated as</p><formula xml:id=\"formula_8\">Y t  g the dot product. <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose to approximate the exponential kernel with inner pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: TECTURES</head><p>Inspired by the deep residual model <ref type=\"bibr\" target=\"#b6\">[7]</ref>, RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we p RCL) are performed with respect to the discrete time steps that are expressed according to the RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>. Let's consider the \ud835\udc65 \ud835\udc59 input sample in the \ud835\udc59 \ud835\udc61\u210e layer of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> and speech generation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> tasks. VAE has man ws the good performance of this method.</p><p>We have become aware of recent work by Akuzawa et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> which combines an autoregressive speech synthesis model wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: comes unfeasible. A recent relaxation in language modeling <ref type=\"bibr\" target=\"#b28\">[27,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref> turns the prediction problem on its head. First, instead of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ias by using corpus level constraints, but is only practical for models with specialized structure. <ref type=\"bibr\" target=\"#b13\">Kusner et al. (2017)</ref> propose the method based on causal inferen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mentioning branch predictor warmup is by Haskins and Conte <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> in which they propose memory reference reuse latency (MRRL) mup length per sampling unit. (Note that the MRRL approach <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> corresponds to a zero BHM history length.) We further obser. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: > identified particular hand gesture to be important to identify the act of deception. Cohen et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> found that fewer iconic hand gestures were a sign of a dec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvised methods to take advantages of unlabeled speech data <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>, data augmentatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, which explicitly a get=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" tar llow previous work <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> which provide the m sentation of a user and an item. From matrix factorization <ref type=\"bibr\" target=\"#b13\">[13,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref> to deep neural networks <ref type=\"bibr\" target=\"#b11\">[11, bibr\" target=\"#b13\">13]</ref> or the pairwise ranking loss <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref> to discriminate between positive and negative interactions. der in the end.</p><p>Existing discriminative OCCF methods <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref> have tried to optimize the latent space where the user-item thods in the first category directly optimize the embedding vectors of users and items.</p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b27\">[27]</ref>: The Bayesian personalized ranking method for OCCF. It opt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alizable (DG) ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> has been appealing to researchers recently. Generally, DG R  by existing works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Recently, works <ref type=\"bibr\" target=\"#b43\">[44,< ation (DG) in ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, which learns the generalizable ReID models on multi-source the previous works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> on DG ReID, we conduct our experiments on the public ReID o ng DG ReID methods <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30</ref>] follow the same pipeline, where they collect all source dom alization <ref type=\"bibr\" target=\"#b48\">[49]</ref> to learn a more generalizable model. Jin et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposed Style Normalization and Restitution modules to di For data augmentation, we perform random cropping, random flipping, and color jittering. Similar to <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we discard random erasing (REA) because REA will degenera e evaluated on the average of 10 repeated random splits of gallery and probe sets. Under Protocol-2 <ref type=\"bibr\" target=\"#b29\">[30]</ref>, all the images in M+D+C3+MT (including the training and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ll a challenge for training diversification models.</p><p>To tackle this problem, inspired by IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref ty e Carlo search. It is also used in the traditional information retrieval area, Wang proposed IR-GAN <ref type=\"bibr\" target=\"#b18\">[19]</ref> which consists of two information retrieval models in it.  \ud835\udc5e, \ud835\udc46)) 1 + exp(\ud835\udc53 \ud835\udf19 (\ud835\udc51 |\ud835\udc5e, \ud835\udc46)) .<label>(7)</label></formula><p>Please note that different from IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DVGAN-doc has an additional component \ud835\udc46 to represent the  , it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we generate negative document set \ud835\udc37 \u2032 by selecting the do. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: wise approach.</p><p>\u2022 Combining the proposed model with densityweighted Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we introduce active learning into POLAR <ref type=\"bibr\"  ediction the parameters under the posterior disagree about are selected. Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref> selects the instance that maximizes the expected loss base ected Loss Optimization</head><p>The active learning metric we choose is Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The basic idea is to choose the instance that maximizes t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion of instruction misses uncovered, and hence, there is a substantial opportunity for improvement <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b29\">32,</ref><ref type=\"bibr\" ta act that the sequence of instruction cache accesses or misses is repetitive, and hence, predictable <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b19\">22]</ref>. Consequently, tem it footprint, and a 12-bit pointer to the successor. The 8-bit footprint is derived from prior work <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" ta dvantage of the stream address buffer (SAB), which is previously used by prior temporal prefetchers <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28,</ref><ref type=\"bibr\" ta e lookahead is five, and four SABs are used where each one tracks seven consecutive spatial regions <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28]</ref>.</p><p>MANA: MANA  ng twelve entries. Prior work has shown that this configuration successfully exploits the potential <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b25\">28]</ref>. Moreover, MANA_Ta o have a dedicated bit in the footprint. Some pieces of prior work have used (2, 6) spatial regions <ref type=\"bibr\" target=\"#b18\">[21,</ref><ref type=\"bibr\" target=\"#b29\">32,</ref><ref type=\"bibr\" ta  that showed a hardware instruction prefetcher could eliminate most of the instruction cache misses <ref type=\"bibr\" target=\"#b18\">[21]</ref>. However, the proposed prefetcher is impractical because o ]</ref> records and replays the sequence of misses and offers adequately good results. However, PIF <ref type=\"bibr\" target=\"#b18\">[21]</ref> offers a more significant improvement as compared to TIFS  B comes from the prefetch buffers, and the rest is because of the changes made to the BTB. PIF: PIF <ref type=\"bibr\" target=\"#b18\">[21]</ref> records the sequence of spatial regions in a circular hist tly prefetched spatial regions. Prior work has suggested using four SABs that each one tracks seven <ref type=\"bibr\" target=\"#b18\">[21]</ref> or twelve <ref type=\"bibr\" target=\"#b25\">[28]</ref> spatia hers showed that recently-accessed addresses tend to recur <ref type=\"bibr\" target=\"#b16\">[19,</ref><ref type=\"bibr\" target=\"#b18\">21,</ref><ref type=\"bibr\" target=\"#b19\">22,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e for the 'happy' category while the others not. In this paper, inspired by the attention mechanism <ref type=\"bibr\" target=\"#b13\">[14]</ref> of machine translation and the neural aggregation networks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: us with two intermediate minibatches (g k (x), y) and (g k (x ), y ). Third, we perform Input Mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> on these intermediate minibatches. This pr <p>Here, (y, y ) are one-hot labels, and the mixing coefficient \u03bb \u223c Beta(\u03b1, \u03b1) as proposed in mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>. For instance, \u03b1 = 1.0 is equivalent to sa  to substantial improvements, achieving 2.45% test error on CIFAR-10 when combined with Input Mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>. As AgrLearn is complimentary to Input Mix rs: no regularization, AdaMix, Input Mixup, and Manifold Mixup. We follow the training procedure of <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>, which is to use SGD with momentum, a weig e=\"table\">1</ref>: Classification errors on (a) CIFAR-10 and (b) CIFAR-100. We include results from <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> \u2020 and <ref type=\"bibr\" target=\"#b9\">(Guo e  more detail). In the case where S = {0}, Manifold Mixup reduces to the original mixup algorithm of <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>. While one could try to reduce the variance random interpolations between training examples and perform the same interpolation for their labels <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b24\">Tokozume et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b45\">[46]</ref>, and three common techniques, including stochastic depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>, label smoothing <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: irectly. Two popular neural network sequence models are Connectionist Temporal Classification (CTC) <ref type=\"bibr\" target=\"#b10\">[10]</ref> and recurrent models for sequence generation <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: breakthroughs in both fields. More powerful supercomputers <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref> and advanced libraries <ref type=\"bibr\" target=\"#b2\">[3]</re e SuperMUC-NG <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and through its components, such as TPU pods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, specifically designed to ease large scale neural network tr s the green energy-driven Summit <ref type=\"bibr\" target=\"#b0\">[1]</ref> and Google's cloud TPU Pod <ref type=\"bibr\" target=\"#b1\">[2]</ref>, combined with optimized libraries such as IBM DDL <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b33\">(Lin et al. , 2018;;</ref><ref type=\"bibr\">Dligach et al. 2017;</ref><ref type=\"bibr\" target=\"#b48\">Tourille et al. 2017;</ref><ref type=\"bibr\" target=\"#b34\">Lin et al. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o AA .We divide the experiments into two scenarios: First we evaluate our method on Tatoeba dataset <ref type=\"bibr\" target=\"#b3\">(Artetxe and Schwenk (2019)</ref>), which is English-centric. Then we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> and voxels <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch the scores of anomalies deviate from those of normal nodes.</p><p>According to previous studies <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, Gaussian distribu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the uncertainty inherent in user behavior and the limited information provided by browser sessions <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>Based on existing literature, almost all the RNN-ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rations are used to model the sequential document selection process in search result diversi cation <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and multi-page sea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b9\">10]</ref> 2) handcrafted invariant features w/ and w/o deep learning, e.g. <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b4\">[5]</ref>, however, a spectral relaxation of the problem can be solved efficiently <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Let C \u2208 0, 1 n\u00d7k be the cluster assignment matrix and d b r iteration or Lanczos algorithm.</p><p>One can then obtain clusters by means of spectral bisection <ref type=\"bibr\" target=\"#b36\">[37]</ref> with iterative refinement akin to Kernighan-Lin algorithm . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  on existing transcompilers, to create parallel data. Moreover, they essentially rely on BLEU score <ref type=\"bibr\" target=\"#b37\">[38]</ref> to evaluate their translations <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Algorithms that do consider object articulations <ref type ct's pose and scale relative to a category-specific canonical representation. Recently, Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> extended the object coordinate based approach to perform c re normalized and the orientations are aligned for objects in a given category. Whereas the work by <ref type=\"bibr\" target=\"#b27\">[28]</ref> focuses on pose and size estimation for rigid objects, the NCSH representation is inspired by and closely related to Normalized Object Coordinate Space (NOCS) <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which we briefly review here. NOCS is defined as a 3D spa iefly review here. NOCS is defined as a 3D space contained within a unit cube and was introduced in <ref type=\"bibr\" target=\"#b27\">[28]</ref> to estimate the category-level 6D pose and size of rigid o  closed. In addition to normalizing the articulations, NAOCS applies the same normalization used in <ref type=\"bibr\" target=\"#b27\">[28]</ref> to the objects, including zero-centering, aligning orienta head><p>For each part, NPCS further zero-centers its position and uniformly scales it as is done in <ref type=\"bibr\" target=\"#b27\">[28]</ref>, while at the same time keeps its orientation unchanged as ime keeps its orientation unchanged as in NAOCS. In this respect, NPCS is defined similarly to NOCS <ref type=\"bibr\" target=\"#b27\">[28]</ref> but for individual parts instead of whole objects. NPCS pr s {p i \u2208 S (j) }, we have their corresponding NPCS predictions {c i |p i \u2208 S (j) }. We could follow <ref type=\"bibr\" target=\"#b27\">[28]</ref> to perform pose fitting, where the Umeyama algorithm <ref  j) }, as is commonly done for bundle adjustment <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we also use RANSAC for outlier removal.</p><p>Finally, fo  , t (j) , s (j) and the NPCS {c i |p i \u2208 S (j) } to compute an amodal bounding box, the same as in <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  as matching networks and bi-LSTM ( <ref type=\"bibr\" target=\"#b13\">[14]</ref>) or memory-networks ( <ref type=\"bibr\" target=\"#b9\">[10]</ref>) which are learned using a meta-learning approach: they aim n systems that learns to predict on novel problems based only on few labeled examples. For example, <ref type=\"bibr\" target=\"#b9\">[10]</ref> propose to use the recent memory-augmented neural network,  for now. The work of <ref type=\"bibr\" target=\"#b14\">[15]</ref> propose an extension of the model of <ref type=\"bibr\" target=\"#b9\">[10]</ref>, where the true label of the observed instance is withheld  llow a similar principle to what has been recently presented for one-shot learning problems, e.g in <ref type=\"bibr\" target=\"#b9\">[10]</ref>. It aims at extending the basic principle of training in ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oric traffic speeds and the underlying road graph. DCRNN replaces the fully-connected layers in GRU <ref type=\"bibr\" target=\"#b5\">(Chung et al., 2014)</ref> with the diffusion convolution operator <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ween AD-GCL and JOAO for the tasks investigated in Sec. 5 is given in Appendix H.</p><p>Tian et al. <ref type=\"bibr\" target=\"#b71\">[71]</ref> has recently proposed the InfoMin principle that shares so rased in our notation, the optimal augmentation T IM (G) given by InfoMin (called the sweet spot in <ref type=\"bibr\" target=\"#b71\">[71]</ref>) needs to satisfy</p><formula xml:id=\"formula_15\">I(t IM (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>] in the image. However, little work on adversarial examples  ance image regions <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>]. All of these approaches share a common challenge: They hav get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, has focused on hiding perturbations in image regions with  lowing recent work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, we conduct our experiments on the development set (1000 RG. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ressing these concerns, approaches utilizing a dynamic vocabulary of slot values have been proposed <ref type=\"bibr\" target=\"#b13\">(Rastogi, Gupta, and Hakkani-Tur 2018;</ref><ref type=\"bibr\" target=\" representations for potentially unseen inputs from new services. Recent pretrained models like ELMo <ref type=\"bibr\" target=\"#b13\">(Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b3\">(De. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b13\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b27\">Velickovic et al., 2018;</ref><ref type=\"bibr\" target=\"#b19\">Palm et al., 2018)</ref>, we propose an evidence reasoning network (E. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b25\">(Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Rong et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020)</ref>. Many ideas for new GNN architectures are ada rd, as of October 1, 2020). For Cora, Citeseer and Pubmed, we reuse the top performance scores from <ref type=\"bibr\" target=\"#b4\">Chen et al. (2020)</ref>. For Email and US County, we use GCNII <ref t  from <ref type=\"bibr\" target=\"#b4\">Chen et al. (2020)</ref>. For Email and US County, we use GCNII <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020)</ref>. For Rice31, we use GCN with spectral and no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ize of recommenders <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b37\">[38]</ref>, NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>. 2). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ject detection <ref type=\"bibr\" target=\"#b16\">[19]</ref>. Our work is inspired by the recent DetNAS <ref type=\"bibr\" target=\"#b6\">[9]</ref>, but has three fundamental differences. First, the studied t e architectures in search space, the same as previous work <ref type=\"bibr\" target=\"#b17\">[20,</ref><ref type=\"bibr\" target=\"#b6\">9]</ref>, we resort to evolutionary algorithms <ref type=\"bibr\" target tch statistics on one path should be independent of others <ref type=\"bibr\" target=\"#b17\">[20,</ref><ref type=\"bibr\" target=\"#b6\">9]</ref>. Therefore, we need to recalculate batch statistics for each . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bust for unseen target domains. (2) Augmenting source data <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \uf8f9 \uf8fb . (<label>6</label></formula><formula xml:id=\"formula_10\">)</formula><p>The limiting objective <ref type=\"bibr\" target=\"#b5\">(6)</ref>, which we denote by L Q Debiased , still samples examples x  \">CIFAR10 and STL10</head><p>First, for CIFAR10 <ref type=\"bibr\" target=\"#b22\">[23]</ref> and STL10 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed in our experiment. RAF-DB involves 12,271 images for training and 3,068 images for testing. SFEW <ref type=\"bibr\" target=\"#b5\">[6]</ref> is created by selecting static frames from Acted Facial Expr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tasets, but even with very different training objectives, including supervised image classification <ref type=\"bibr\" target=\"#b9\">(Krizhevsky et al., 2012)</ref>, unsupervised density learning <ref ty rmula></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">Experimental Setup</head><p>Since <ref type=\"bibr\" target=\"#b9\">Krizhevsky et al. (2012)</ref> won the ImageNet 2012 competition, ther. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  try to measure the abnormality of nodes with the reconstruction errors of autoencoder-based models <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> or the residuals of  nt anomaly detection methods:</p><p>\u2022 AUC-ROC is widely used in previous anomaly detection research <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Area under curve (A of network anomaly detection using graph neural networks due to its strong modeling power. DOMINANT <ref type=\"bibr\" target=\"#b5\">[6]</ref> achieves superior performance over other shallow methods by  erizing the residuals of attribute information and its coherence with network structure. \u2022 DOMINANT <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a GCN-based autoencoder framework which computes anomaly . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Active learning is widely studied to solve this kind of sample selection problem. As discussed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>, active learning methods can be divided into two categorie ertain number of labeled samples to evaluate the uncertainty of the unlabeled data or sampling bias <ref type=\"bibr\" target=\"#b17\">[18]</ref> will result. It is therefore recommended that such methods ive learning algorithms are referred to as early active learning or early stage experimental design <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We illustrate the procedures of and example of the tradit a><p>Finding the optimal subset V \u2282 X in Eq. ( <ref type=\"formula\">7</ref>) is NP-hard. Inspired by <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we relax the problem to the following problem by introduc ime, the least squared loss used in Eq. ( <ref type=\"formula\">8</ref>) is sensitive to the outliers <ref type=\"bibr\" target=\"#b17\">[18]</ref>, which makes the algorithm not robust.</p><p>We note that  type=\"bibr\" target=\"#b25\">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is shown in <ref type=\"bibr\" target=\"#b17\">[18]</ref> that the 2,1 -norm is the minimum convex hull of the 2,0 - 1.0\"><head n=\"2.\">K-means</head><p>We use the K-means algorithm as another baseline algorithm as in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In each experiment, samples are ranked by their distances It formulates a regularized linear regression problem which minimizes reconstruction error. 5. RRSS <ref type=\"bibr\" target=\"#b17\">[18]</ref> Early active learning via Robust Representation and Struct ance of the linear methods with our algorithm. This is consistent with the mathematical analysis in <ref type=\"bibr\" target=\"#b17\">[18]</ref> that kernelization produces more discriminative representa ithm not robust.</p><p>We note that in previous researches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, the 2,1 -norm is u s, minimization of A 2,1 will achieve the same result as A 2,0 when A is row-sparse. As analyzed in <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, the 2,1 -norm can. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: solution of only 96 \u00d7 96.</p><p>The framework of Vector Quantized Variational AutoEncoders (VQ-VAE) <ref type=\"bibr\" target=\"#b45\">[46]</ref> alleviates this problem. VQ-VAE trains an encoder to compr ns.</p><p>The image tokenizer is a discrete Auto-Encoder, which is similar to the stage 1 of VQ-VAE <ref type=\"bibr\" target=\"#b45\">[46]</ref> or d-VAE <ref type=\"bibr\" target=\"#b38\">[39]</ref>. More s e codebook is updated periodically during training as the mean of the vectors recently mapped to it <ref type=\"bibr\" target=\"#b45\">[46]</ref>. \u2022 The nearest-neighbor mapping, fixed codebook, where the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uccessful use of CNNs in image tasks, a newly proposed sequential recommender, referred to as Caser <ref type=\"bibr\" target=\"#b28\">[29]</ref>, abandoned RNN structures, proposing instead a convolution la\">3</ref>) ( see <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>).</p><formula xml:i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o AA .We divide the experiments into two scenarios: First we evaluate our method on Tatoeba dataset <ref type=\"bibr\" target=\"#b3\">(Artetxe and Schwenk (2019)</ref>), which is English-centric. Then we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as been achieved by using LSTM-CRF models <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b5\">Chiu and Nichol ad><p>We follow the best English NER model <ref type=\"bibr\" target=\"#b15\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Lample et al.,  concatenated as its representation:</p><p>Integrating character representations Both character CNN <ref type=\"bibr\" target=\"#b27\">(Ma and Hovy, 2016)</ref> and LSTM <ref type=\"bibr\" target=\"#b17\">(La. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by the sequential connections between adjacent clicks. For sequenceaware recommendation tasks, FPMC <ref type=\"bibr\" target=\"#b16\">(Rendle, Freudenthaler, and Schmidt-Thieme 2010)</ref> combines Marko. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ional networks, have gained much attention and improved the state of the art in node classification <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d learning (SSL) or collective classification <ref type=\"bibr\" target=\"#b27\">(Sen et al. 2008;</ref><ref type=\"bibr\" target=\"#b18\">McDowell, Gupta, and Aha 2007;</ref><ref type=\"bibr\" target=\"#b24\">Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: three classic citation network benchmarks <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012) al to 0.01.</p><p>\u2022 Cora, Citseer, Pubmed <ref type=\"bibr\" target=\"#b10\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b9\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b29\">Namata et al., 2012). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ltaneously for better generalization performance in computer vision and natural language processing <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ formation and regularization to each classifier head. In natural language processing, Dual Learning <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposes a learning mechanism that two machine translators t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the effect of affirmative action (see e.g., <ref type=\"bibr\" target=\"#b12\">Keith et al., 1985;</ref><ref type=\"bibr\" target=\"#b11\">Kalev et al., 2006)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (GGNN) <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref type=\"bibr\" target=\"#b7\">(Lample et al., 2016)</ref> (BiLSTM-CRF), our model learns a weighted  are respec-tively 39.70%, 44.75%, 36.10% and 46.05%.</p><p>Models for Comparison. We use BiLSTM-CRF <ref type=\"bibr\" target=\"#b7\">(Lample et al., 2016)</ref> with character+bigram embedding without us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the abbreviation, as a short form of text, is prone to ambiguity. Word sense disambiguation methods <ref type=\"bibr\" target=\"#b22\">[23]</ref> have been studied to disambiguate word senses, however, de be expensive and the collected datasets are normally small in size.</p><p>Word sense disambiguation <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a type of technique used to distinguish ambiguous word  </p><p>Inspired by word sense disambiguation methods that label super sense types for word clusters <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we jointly predict the types for abbreviation candidates . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: private-attribute inference attack can be naturally formulated as a problem of adversarial learning <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In our proposed RAP, there are two components: a Bayesian. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ase of it. The \u03a0-model can also be seen as a simplification of the \u0393-model of the ladder network by <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref>, a previously presented network architectu he data is obtained.</p><p>Our approach is somewhat similar to the \u0393-model of the ladder network by <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref>, but conceptually simpler. In the \u03a0-model, ed the issue by shuffling the input sequences in such a way that stratification is guaranteed, e.g. <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref> (confirmed from the authors). This kind of he ones that are most directly connected to our work.</p><p>\u0393-model is a subset of a ladder network <ref type=\"bibr\" target=\"#b17\">(Rasmus et al., 2015)</ref> that introduces lateral connections into . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b11\">[12]</ref>, HGT <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and HetSANN <ref type=\"bibr\" target=\"#b16\">[17]</ref> were developed within the last two years.</p><p>Despite va producing mixed results when compared to GAT (See Table <ref type=\"table\" target=\"#tab_3\">3</ref>). <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Attention-based graph neural network for heterogeneous st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b17\">17</ref> , face recognition <ref type=\"bibr\" target=\"#b18\">18</ref> , and playing Atari games <ref type=\"bibr\" target=\"#b19\">19</ref> . They use many layers of neurons, each arranged in overlapp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: their desired topics or related publications, but it is hard for researchers to reason and word2vec <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Two critical methods in BERT, Masked Language Model and Ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, because it effectively couples the attention mechanism an  state-of-the-art methods using deep two-stream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oE methods have shown their superiority in image recognition <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, machine translatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[37]</ref>, VIPeR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and iLIDs <ref type=\"bibr\" target=\"#b52\">[53]</ref>.</p><p>For CUHK03, we use the \"labelled\" dataset for train. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f video modeling, motivated by its tremendous success on image analysis, such as object recognition <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Recent progress on action recognition has shown that prop  not affect the prediction block, which can stabilize the training procedure. A shortcut connection <ref type=\"bibr\" target=\"#b19\">[20]</ref> is introduced to connect the input features and the output. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n on these devices is an important topic that has attracted much research attention in recent years <ref type=\"bibr\" target=\"#b18\">[23,</ref><ref type=\"bibr\" target=\"#b23\">28,</ref><ref type=\"bibr\" ta ype=\"bibr\" target=\"#b13\">[18]</ref>. An rOperator can also be optimized by an existing kernel tuner <ref type=\"bibr\" target=\"#b18\">[23]</ref>. Our experience shows that, on top of existing optimizatio o a parallel task, RAMMER relies on external tools to partition an rOperator into rTasks (e.g., TVM <ref type=\"bibr\" target=\"#b18\">[23]</ref>). In another word, RAMMER uses external heuristics to deci ultiple versions of rKernel implementations from dif-  ferent sources, e.g., auto-kernel generators <ref type=\"bibr\" target=\"#b18\">[23]</ref>, hand-tuned kernels, or converted from existing operators  mpilers, including TensorFlow (v1.15.2) representing the state-of-the-art DNN framework, TVM (v0.7) <ref type=\"bibr\" target=\"#b18\">[23]</ref> and TensorFlow-XLA representing the state-of-the-art DNN c works and compilers, e.g., TensorFlow <ref type=\"bibr\" target=\"#b13\">[18]</ref>, Py-Torch [15], TVM <ref type=\"bibr\" target=\"#b18\">[23]</ref>, XLA <ref type=\"bibr\" target=\"#b12\">[17]</ref>, etc. TASO  ne in TensorFlow, employs a technique called kernel fusion <ref type=\"bibr\" target=\"#b12\">[17,</ref><ref type=\"bibr\" target=\"#b18\">23]</ref>, which merges several DNN operators into a single one when . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: asing networks Amazon <ref type=\"bibr\" target=\"#b7\">[8]</ref> and a large social network Friendster <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Table <ref type=\"table\" target=\"#tab_1\">2</ref> summarize le GNNs on a billion-scale graph Friendster. We extracted the top-500 ground-truth communities from <ref type=\"bibr\" target=\"#b33\">[34]</ref> and use the community ids as the labels of each node. Note. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Hence MSA can be generalised to other choices of kernels and normalisation that are equally valid <ref type=\"bibr\" target=\"#b51\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Tsai et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref>.</p><p>In this paper, we propose a novel Bayesian graph con et=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref>.In this paper, we propose a novel Bayesian graph convolutio derived from noisy data. Addressing the uncertainty on the underlying graph was first considered in <ref type=\"bibr\" target=\"#b37\">[39]</ref> for the problem of node classification. In this work, the  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Bayesian Graph Convolutional Networks</head><p>In <ref type=\"bibr\" target=\"#b37\">[39]</ref>, to alleviate the effect of the potential noise in the obs G \ud835\udc5c\ud835\udc4f\ud835\udc60 ) \ud835\udc5d (G|D, G \ud835\udc5c\ud835\udc4f\ud835\udc60 ) \ud835\udc51 G \ud835\udc51\ud835\udf40 .<label>(1)</label></formula><p>Zhang et al. presented this model in <ref type=\"bibr\" target=\"#b37\">[39]</ref> for the node classification task. In their case, the value. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Sato et al., 2020)</ref> or to add a unique label <ref type=\"bibr\" target=\"#b29\">(Murphy et al., 2019)</ref> in order to have the ability to distingui. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  wireless spectrum allocation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and wireless crowdsourcing <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>The celebrated VCG mechanism <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed data.</p><p>Our unsupervised SimCSE simply predicts the input sentence itself, with only dropout <ref type=\"bibr\" target=\"#b44\">(Srivastava et al., 2014)</ref> used as noise (Figure <ref type=\"figu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rent weights to neighbours that affect the trajectory of the pedestrian of interest, multiple works <ref type=\"bibr\" target=\"#b59\">[62,</ref><ref type=\"bibr\" target=\"#b15\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y information, i.e. the three-dimensional (3D) spatial structure of a molecule. Although some works <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> take the atomic di  extend graph attention mechanism in order to learn aggregation weights. Furthermore, several works <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> start to take the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Univariate techniq. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: c.org/ns/1.0\"><head n=\"4.2.2\">Predicting Performance in Solo-Mode.</head><p>Referring to prior work <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we design shadow solo-cycle accounting (SSCA) approach to e prediction method of QoSMT is inspired by PTA. PTA uses MLP correction to achieve higher accuracy <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, we can not get an application's MLP without offl SMT throughput and fairness, but they did not take performance control into account. Eyerman et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed the per-thread cycle accounting (PTA) mechanism t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> to alleviate the scalability and efficiency issues, samplin f type=\"bibr\" target=\"#b8\">[9]</ref>, GAT <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and GraphSAINT <ref type=\"bibr\" target=\"#b33\">[34]</ref>, where each layer adopts a neighborhood and an updating fu er workers.   <ref type=\"bibr\" target=\"#b11\">[12]</ref>, two social networks (Flickr and Reddit) in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, four co-authorship graphs (Amazon and Coauthor) in <ref t pe=\"bibr\" target=\"#b4\">[5]</ref>, ClusterGCN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and GraphSAINT <ref type=\"bibr\" target=\"#b33\">[34]</ref>. The detailed introduction of these baselines are shown in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: years, due to the superior performance on graph data mining <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>For graph-ba etwork, UAI2010 <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a dataset for community detection, ACM <ref type=\"bibr\" target=\"#b23\">[24]</ref> is research paper coauthor network extracted from ACM data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2015) )</ref> and recurrent neural network <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Zhou et al., 2016)</ref>. To automatically obtain a large training da ying relations, we apply an attention mechanism over a BiLSTM Encoder, which is first introduced in <ref type=\"bibr\" target=\"#b23\">(Zhou et al., 2016)</ref> for RC. The model architecture is illustrat g et al., 2015)</ref> is also commonly used for RE with the help of position embeddings. BiLSTM+ATT <ref type=\"bibr\" target=\"#b23\">(Zhou et al., 2016)</ref> adds an attention mechanism into BiLSTM to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ter representations. These graph-based recommender systems <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> achieve great succe ks (GNNs) have been introduced into recommendation systems <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> as well as multimodal recommendation systems <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: space. The engine recommends the products whose embeddings are nearest to the embedded search query <ref type=\"bibr\" target=\"#b32\">(Nigam et al., 2019)</ref>.</p><p>Since the search occurs for every q  function under strict latency requirements: a max-imum search time of 20 ms is a common constraint <ref type=\"bibr\" target=\"#b32\">(Nigam et al., 2019)</ref>. A 20% improvement in search time allows t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"5.1\">Experimental Setup</head><p>Datasets. We use two public real-world datasets: CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref>, Foursquare <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b34\">31]</ref>, ASPP <ref type=\"bibr\" target=\"#b6\">[3]</ref>, and Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref>.</p><p>The Inception block adopts multiple branches with dif all the positions equally, probably leading to confusion between object and context. Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref> learns distinctive resolutions of individual objects, unfort pe=\"bibr\" target=\"#b36\">[33]</ref>, ASPP <ref type=\"bibr\" target=\"#b6\">[3]</ref> and Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref>. For Inception, besides the original version, we change its . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  temporal convolutions. Based on the work of <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Chao et al. <ref type=\"bibr\" target=\"#b26\">[26]</ref> improved receptive field alignment to exploit the temporal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b43\">44]</ref> and the possibility of poor margins <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, leading to reduced generalization performance. In practice. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odalities. The data consists of video clips obtained from real court trials, initially presented in <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>Unlike previous work on this dataset, which focuses o  trials. The dataset description is included here for completeness; further details can be found in <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= l, by carefully identifying and labeling truthful and deceptive video clips from trial's recordings <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>In this work, we focus on deception at the subject le life trial dataset was first presented, together with a video-clip level deception detection system <ref type=\"bibr\" target=\"#b8\">[9]</ref>. iv) Different from the earlier work, our evaluations are co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: has the maximum expected reward achievable by the optimal policy \ud835\udf0b * , follows the Bellman equation <ref type=\"bibr\" target=\"#b0\">[1]</ref> as: Then, the agent will receive the reward \ud835\udc5f \ud835\udc61 from the use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediction includes POS tagging and NER. We use POS tagging data from the Universal Dependencies v2.5 <ref type=\"bibr\" target=\"#b37\">(Nivre et al., 2018)</ref> treebanks. Each word is assigned one of 17. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion. The leftmost bank on Figure <ref type=\"figure\" target=\"#fig_0\">1</ref> is a bimodal predictor <ref type=\"bibr\" target=\"#b4\">[5]</ref>. We refer to this bank as bank 0. It has 4k entries, and is  of groups of consecutive history bits, then it is XORed with the branch PC as in a gshare predictor <ref type=\"bibr\" target=\"#b4\">[5]</ref>. For example, bank 3 is indexed with 40 history bits, and th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b30\">[31]</ref> with self-attention or parameterized node similarity with input node features <ref type=\"bibr\" target=\"#b18\">[19]</ref>. These models do not suit our task for two reasons. First, s and the input graph topology for node embedding learning <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Here, the graph structure is constructed by human experts . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: CRF) <ref type=\"bibr\" target=\"#b5\">[6]</ref> , Decision Tree <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> , Support Vector Machine (SVM) <ref type=\"bibr\">[9~12]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD <ref type=\"bibr\" target=\"#b3\">[4]</ref> focuses on the lip region, we found that lip landmarks can b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecently, a novel density based clustering method, named Fast search-and-find of Density Peaks (FDP) <ref type=\"bibr\" target=\"#b32\">[33]</ref> , was proposed. This algorithm assumes that cluster center iguez and Laio proposed a novel density-based clustering method by finding density peaks called FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> . FDP discovers clusters by a two-phase process. First, lo  SNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> , KNNC <ref type=\"bibr\" target=\"#b39\">[40]</ref> , FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> , 3DC <ref type=\"bibr\" target=\"#b22\">[23]</ref> , STClu <r rget=\"#b7\">[8]</ref> . KNNC <ref type=\"bibr\" target=\"#b39\">[40]</ref> (ii) No other parameters. FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> (i)</p><p>The objects with top C t gamma values are chosen object pairs. We vary \u03b2 value in {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} according to the recommendation in <ref type=\"bibr\" target=\"#b32\">[33]</ref> .</p><p>(ii) Density based on K nearest neighbors. The par ch cluster. The statistical error of the estimated density on such a small set of pictures is large <ref type=\"bibr\" target=\"#b32\">[33]</ref> . Therefore, for datasets consisting of clusters with few . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  attention in several applications, from computer vision to natural language processing (NLP) tasks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" ta stabilize training by keeping activations of the network at zero mean and unit variance. Prior work <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> introduced complex ized the component weight matrices as initially described in complex-and quaternion Neural Networks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Both works start . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: somorphism Networks <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2018)</ref>, and various deep models <ref type=\"bibr\" target=\"#b25\">(Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Rong et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd efficiently segmenting large point clouds <ref type=\"bibr\" target=\"#b45\">[Wang et al., 2018</ref><ref type=\"bibr\" target=\"#b29\">, Li et al., 2019b]</ref>. Recent works have looked at frameworks to  20\">, Huang et al., 2017</ref><ref type=\"bibr\" target=\"#b53\">, Yu and Koltun, 2016]</ref>, DeepGCNs <ref type=\"bibr\" target=\"#b29\">[Li et al., 2019b]</ref> propose to train very deep GCNs (56 layers)   to be either SoftMax_Agg \u03b2 (\u2022) or PowerMean_Agg p (\u2022).</p><p>Better Residual Connections. DeepGCNs <ref type=\"bibr\" target=\"#b29\">[Li et al., 2019b]</ref> show residual connections <ref type=\"bibr\" t ., 2016]</ref> is used in every layer before the activation function ReLU.</p><p>ResGCN. Similar to <ref type=\"bibr\" target=\"#b29\">Li et al. [2019b]</ref>, we construct ResGCN by adding residual conne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b18\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Misra and Maaten, 2020)</ref>. Researchers in the NLP domain have als. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: access to some physiological signals (e.g., <ref type=\"bibr\" target=\"#b42\">Zuo et al. (2012)</ref>; <ref type=\"bibr\" target=\"#b1\">Allen et al. (2014)</ref>; <ref type=\"bibr\" target=\"#b0\">Al-Shargie et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, natural language processing <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, and social networ eural Networks</head><p>We briefly introduce the concepts of supervised graph learning, Transformer <ref type=\"bibr\" target=\"#b52\">[53]</ref>, and GNNs in this section.</p><p>Supervised learning tasks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  is critical to realism.</p><p>In contrast to methods that use a parametric model of the human face <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we directly predict the positions of face mesh vertices in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b18\">[19]</ref>, Hetero-ConvLSTM <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DML <ref type=\"bibr\" target=\"#b20\">[21]</ref> and CoST-Net <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Pr hich introduces spatial graph features and spatial model ensemble on top of the basic ConvLSTM. DML <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposes a meta graph attention module and a meta recurren. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rided convolution at the 3 \u00d7 3 layer instead of the 1 \u00d7 1 layer to better preserve such information <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Convolutional lay vers (64 GPUs in total) in parallel. Our learning rates are adjusted according to a cosine schedule <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. We follow the com  small for j = c, while z c is being pushed to its optimal value \u221e, and this can induce overfitting <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. Rather than assig ><p>Tweaks from ResNet-D. We also adopt two simple yet effective ResNet modifications introduced by <ref type=\"bibr\" target=\"#b25\">[26]</ref>: (1) The first 7 \u00d7 7 convolutional layer is replaced with   including bias units, \u03b3 and \u03b2 in the batch normalization layers.</p><p>#P GFLOPs acc(%) ResNetD-50 <ref type=\"bibr\" target=\"#b25\">[26]</ref>  For example 2s2x40d denotes radix=2, cardinality=2 and wi ixup training, we simply mix each sample from the current mini-batch with its reversed order sample <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Batch Normalization <ref type=\"bibr\" target=\"#b31\">[32]</ /www.tei-c.org/ns/1.0\"><head n=\"5.2\">Ablation Study</head><p>ResNeSt is based on the ResNet-D model <ref type=\"bibr\" target=\"#b25\">[26]</ref>  ResNeSt-fast setting, the effective average downsampling  inality, and d the network width (0s represents the use of a standard residual block as in ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref>). We empirically find that increasing the radix from 0 to  ref type=\"bibr\" target=\"#b59\">[60]</ref>, SENet <ref type=\"bibr\" target=\"#b28\">[29]</ref>, ResNet-D <ref type=\"bibr\" target=\"#b25\">[26]</ref> and SKNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Remar to the weights of convolutional and fully connected layers <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. We do not subject any of the other network parameters to w ng on images that share the same crop size. ResNet variants<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and DeepLab <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> have become the frequently-used schemes. Generally, feature =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 <ref type=\"bibr\" target=\"#b19\">[20]</ref>,  =\"bibr\" target=\"#b11\">[12]</ref>, DeepLab V3 <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DeepLab V3+ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, FC-DenseNet57 (tiramisu) <ref type=\"bibr\" target=\"#b19\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: has recently shown great potential in many applications in biochemistry, physics and social science <ref type=\"bibr\" target=\"#b1\">[1]</ref><ref type=\"bibr\" target=\"#b2\">[2]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, satellite imaging <ref type=\"bibr\" target=\"#b37\">[38]</ref>, face recognition <ref type=\"bibr\" target=\"#b16\">[17]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: intent-labeled speech data, and such data is usually scarce. <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> address this problem using a curriculum and transfer learni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  graph neural networks on graphs is still a nascent research topic, a few recent works have emerged <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020;</ref><ref type=\"bibr\" target=\"#b18\">Vu &amp; Thai, >, and Gradient <ref type=\"bibr\" target=\"#b15\">(Pope et al., 2019)</ref>, since previous explainers <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Ying et al., 2 rable amount of explanations for providing a global view of explanations. For this end, PGExplainer <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020)</ref> learns a multilayer perceptron (MLP) to expl 2017)</ref>: GNNExplainer <ref type=\"bibr\" target=\"#b22\">(Ying et al., 2019)</ref> and PG-Explainer <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020)</ref> <ref type=\"foot\" target=\"#foot_1\">3</ref> .  </p><p>For fair comparisons, we report the results of PGExplainer following its setting reported in <ref type=\"bibr\" target=\"#b14\">(Luo et al., 2020)</ref> and compare them with the results of GNNExpl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b18\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Misra and Maaten, 2020)</ref>. Researchers in the NLP domain have als. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e for some social or chemical graphs. Finally, another important aspect mentioned by a recent paper <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref> concerns the spectral ability of GNN mod cted nodes messages and one that updates the concerned node representation.</p><p>In a recent paper <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref>, it was explicitly shown that both spati pectral properties of the graph signal such as in image/signal processing applications. As shown in <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref>, the vast majority of existing MPNNs ope w-pass filters which limits their capacity. To lead this analysis, we use the datasets presented in <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref>. First, we evaluate if the models can le et from <ref type=\"bibr\" target=\"#b9\">(Chen et al., 2020)</ref>, 2D-Grid and Band-Pass dataset from <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref>, Zinc12K from <ref type=\"bibr\" target=\"# ef>, Mnist-75 dataset from online source<ref type=\"foot\" target=\"#foot_7\">5</ref> which was used in <ref type=\"bibr\" target=\"#b2\">(Balcilar et al., 2021)</ref> with exactly the same procedure, PROTEIN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ions of the defense models compared with normally trained models. We adopt class activation mapping <ref type=\"bibr\" target=\"#b39\">[38]</ref> to visualize the attention maps of three normally trained . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ss this, some recent studies have proposed greedy methods <ref type=\"bibr\">[Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Z\u00fcgner et al., 2018]</ref> to attack the graph-based deep learning sy tures in pixel color space. However, recent explorations in the graph adversarial attack techniques <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Dai et al.,  ce, this process can be trivial as many statistics can be pre-computed or re-computed incrementally <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>.</p><p>Algorithm 1: IG-JSMA -Integrated G prediction score for its ground-truth class. The adversarial graph was constructed by using nettack <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>. Without any defense, the target node is  ifficult to attack than those with less neighbors. This is also consistent with the observations in <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref> that nodes with higher degrees have highe ber of neighbors which have low similarity scores to the target nodes. This also stands for nettack <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>. For example, we enable both feature and   contribute much to the predictive capabilities of GCN models but introduce unnecessary complexity. <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref> uses a simplified surrogate model to achi y, IG-JSMA is quite stable as the classification margins have much less variance. Just as stated in <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>, the vanilla gradient-based methods, such. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: revious work on CycleGANs <ref type=\"bibr\" target=\"#b29\">(Zhu et al., 2017)</ref> and dual learning <ref type=\"bibr\" target=\"#b11\">(He et al., 2016)</ref>, our method takes two initial models in oppos ition to that, we would like to incorporate a language modeling loss during NMT training similar to <ref type=\"bibr\" target=\"#b11\">He et al. (2016)</ref>. Finally, we would like to adapt our approach . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: v\u00e1 et al., 2019)</ref>, relation extraction <ref type=\"bibr\" target=\"#b50\">(Zeng et al., 2018;</ref><ref type=\"bibr\" target=\"#b53\">Zhang et al., 2020b)</ref>, and aspect term extraction <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b16\">[17]</ref>, or generating intermediate representation in speech-to-text translation <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our deliberation model has a similar structure as <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e change makes our text model's weights directly transferable to the state-of-the-art text model T5 <ref type=\"bibr\" target=\"#b71\">[72]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">34]</ref>, and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these app hs, which are hard to tune in practice. (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above t ht for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.</p><p>\u2022 RippleNet <ref type=\"bibr\" target=\"#b23\">[24]</ref> is a representative of hybrid methods, which is a memory-n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: des/paths rely on graph-based metrics such as PageRank, centrality, and off-the-shelf KG embeddings <ref type=\"bibr\" target=\"#b28\">(Paul and Frank, 2019;</ref><ref type=\"bibr\" target=\"#b11\">Fadnis et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  huge boost in performance using Deep-Learning based methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target yer. The network input is interpolated to the output size. As done in previous CNN-based SR methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targe orted numerical were produced using the evaluation script of <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . 2017b) by 37.0%, the Deep3 system (Raychev et al., 2016a) by 29.7%, and an adaptation of Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> for code prediction by 30.0%. These are sig \"bibr\" target=\"#b45\">, Yang and Xiang, 2019)</ref>. We include an adaptation of path-based Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> in our evaluations and show that our models Devanbu, 2017b)</ref>, Deep3 <ref type=\"bibr\" target=\"#b35\">(Raychev et al., 2016a)</ref>, Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref>).</p><p>Fig 3 puts these models in perspect #b35\">(Raychev et al., 2016a</ref>)) vs. TravTrans+ ; \u2022 from 43.6% to 73.6% when comparing Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref>  Thus, we argue that our proposal of using  \">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequences. We also include Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> to compare our efforts against a popular co ven a method body, how well can Code2Seq generate the correct method name? The training proposed in <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> is not well suited for next token predictio tsis et al., 2020</ref><ref type=\"bibr\" target=\"#b28\">, Li et al., 2018)</ref>), to paths in an AST <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a</ref><ref type=\"bibr\">(Alon et al., ,b, 2020))</re iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Code2Seq</head><p>Code2Seq is a model by <ref type=\"bibr\" target=\"#b7\">Alon et al. 2019a</ref> that embeds code snippets by embedding AST pat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b9\">(LeCun et al. [2010]</ref>, <ref type=\"bibr\" target=\"#b8\">Krizhevsky [2009]</ref>, <ref type=\"bibr\" target=\"#b15\">Russakovsky et al. [2015]</ref>). For each data set, we approximated . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b18\">Nie et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b31\">Yoneda et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> have achieved the top three results a laimevidence pair individually and then aggregate all NLI predictions for final verification. Then, <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b31\">Yoned the task. In the document retrieval and sentence selection stages, we simply follow the method from <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> since their method has the highest sc ose noisy evidence.</p><p>In the document retrieval step, we adopt the entity linking approach from <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>. Given a claim, the method first util ent selects the most relevant evidence for the claim from all sentences in the retrieved documents. <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> modify the ESIM 2 https://www.mediawi he OFEVER scores of our model and models from other teams. After running the same model proposed by <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>, we find our OFEVER score is slightly cted to form the final evidence set in the original method.</p><p>In addition to the original model <ref type=\"bibr\" target=\"#b10\">(Hanselowski et al., 2018)</ref>, we add a relevance score filter wit models from the FEVER shared task as our baselines.</p><p>The Athene UKP TU Darmstadt team (Athene) <ref type=\"bibr\" target=\"#b10\">(Hanselowski et al., 2018)</ref> combines five inference vectors from. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Cedar <ref type=\"bibr\" target=\"#b48\">[48]</ref>. The Lisp machine had a real-time garbage collector <ref type=\"bibr\" target=\"#b5\">[5]</ref>.</p><p>A number of research kernels are written in high-leve  or closures awkward <ref type=\"bibr\" target=\"#b26\">[26]</ref>.</p><p>Concurrent garbage collectors <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ee Figure <ref type=\"figure\" target=\"#fig_2\">2</ref>) borrows the idea of intention disentanglement <ref type=\"bibr\" target=\"#b34\">[35]</ref>, which has a built-in routing mechanism for clustering. Le a set of trainable parameters \ud835\udf41 \u210e \u2208 R \ud835\udc51 , \u210e = 1, 2, . . . , \ud835\udc3b , to represent \ud835\udc3b intention prototypes <ref type=\"bibr\" target=\"#b34\">[35]</ref>, based on which each item \ud835\udc66 \ud835\udc61 is being routed into intenti >14,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, Taobao <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" tar here \ud835\udf0c = 0.07 following previous work <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>Some efforts <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" ta d their solutions, such as using multiple interest vectors <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> or multiple interest sub-models <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance <ref type=\"bibr\" target=\"#b36\">[37]</ref>, as well as graph theory algorithms like widest path <ref type=\"bibr\" target=\"#b3\">[4]</ref> and most reliable path <ref type=\"bibr\" target=\"#b3\">[4]</re h theory algorithms like widest path <ref type=\"bibr\" target=\"#b3\">[4]</ref> and most reliable path <ref type=\"bibr\" target=\"#b3\">[4]</ref>, are special instances of this path formulation with differe e show that such a formulation can be efficiently solved via the generalized Bellman-Ford algorithm <ref type=\"bibr\" target=\"#b3\">[4]</ref> under mild conditions and scale up to large graphs.</p><p>Th maximal path length of 3. A more scalable solution is to use the generalized Bellman-Ford algorithm <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Specifically, assuming the operators \u2295, \u2297 satisfy a semirin ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Graph Theory Algorithms</head><p>Widest Path <ref type=\"bibr\" target=\"#b3\">[4]</ref> min(w q (e i ), w q (e j )) max(h q (P i ), h q (P j )) \u2212\u221e,  b3\">[4]</ref> min(w q (e i ), w q (e j )) max(h q (P i ), h q (P j )) \u2212\u221e, +\u221e w e Most Reliable Path <ref type=\"bibr\" target=\"#b3\">[4]</ref> w q (e i ) \u00d7 w q (e j ) max(h q (P i ), h q (P j )) 0, 1 w e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >[19]</ref> as a way to study brain function. We consider the simplest of many types of perceptrons <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a single-layer perceptron consisting of one artificial neur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: level objective, which is inspired by the well-studied Canonical Correlation Analysis (CCA) methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" tar Correlation Analysis. CCA is a classical multivariate analysis method, which is first introduced in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. For two random variables X 2 R m and Y 2 R n , their cova. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b52\">(Yao, 1993;</ref><ref type=\"bibr\" target=\"#b54\">Yao and Liu, 1997;</ref><ref type=\"bibr\" target=\"#b53\">Yao, 1999)</ref> suggested that neuroevolution is a different kind of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2]</ref> have focused on adapting the B + -tree structure to obtain better cache behaviour. Work in <ref type=\"bibr\" target=\"#b1\">[1]</ref> presented buffer-trees, which like the CCB + -tree, use the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b22\">23]</ref>, or tackling posterior collapse <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: by changing the parameters of the optimization algorithm to depend on the network activation values <ref type=\"bibr\" target=\"#b22\">(Wiesler et al., 2014;</ref><ref type=\"bibr\" target=\"#b14\">Raiko et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e features. Additional supervision was used in other works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> to further improve . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inal prediction. Gradient-based methods <ref type=\"bibr\" target=\"#b16\">(Simonyan et al., 2013;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016)</ref> are widely used for the saliency computation. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the goal of humancomputer interaction popular, and it has been a research hotspot in recent decades <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Automatic Speech Recognition (ASR) refers to the task of an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: th the camera images together with the LiDAR point clouds to create the ground truth bounding boxes <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This motivates multi-modal sensor fusion as a way to impro ing boxes with confidence scores, as shown in Fig 2 <ref type=\"figure\">.</ref> In the KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref> only rotation in z axis is considered (yaw angle), while ro ate and confident scores. There are multiple ways to encode the 3D bounding boxes, in KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref>, a 7-digit vector containing 3D dimension (height, width an eriments, we focus on the car class since it has the most training and testing samples in the KITTI <ref type=\"bibr\" target=\"#b9\">[10]</ref> dataset.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\" ad><p>Our fusion system is evaluated on the challenging 3D object detection benchmark KITTI dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref> which has both LiDAR point clouds and camera images. There  \" target=\"#fig_7\">5</ref> shows some qualitative results of our proposed fusion method on the KITTI <ref type=\"bibr\" target=\"#b9\">[10]</ref> test set. Red bounding boxes represent wrong detections (fa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct with only audio or only text input. And there are also some related work studied different tasks <ref type=\"bibr\" target=\"#b31\">(Yang et al. 2003)</ref> or auxiliary tool <ref type=\"bibr\">(Zhang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and self-defined signals, among which contrastive methods <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar learning in vision <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targe ead><p>Contrastive Learning on Graphs. Contrastive methods <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar resentation learning, respectively. MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> generalizes CMC <ref type=\"bibr\" target=\"#b39\">[40]</ref> to graph-structured data by introducing graph diffusion <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ial for training large and complex deep learning architectures. RNN-T models are difficult to train <ref type=\"bibr\" target=\"#b10\">[11]</ref> and also require significantly large amount of data to joi oral classification (CTC) model <ref type=\"bibr\" target=\"#b1\">[2]</ref> or cross entropy (CE) model <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and the prediction network with LSTM language model (LM)  on.</p><p>model for encoder and prediction network in the context of TL the RNN-T model. Authors in <ref type=\"bibr\" target=\"#b10\">[11]</ref> have shown that CE initialized RNN-T models perform better in blocks. gets (necessary for CE training), is obtained from word level alignments as discussed in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. From the word alignments, the start frame, end frame and  ned by using byte pair encoding <ref type=\"bibr\" target=\"#b25\">[26]</ref> algorithm as described in <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>We also report the word error rate (WER) on hybrid . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rson-Search datasets, including Market1501 <ref type=\"bibr\" target=\"#b61\">[62]</ref>, DukeMTMC-reID <ref type=\"bibr\" target=\"#b62\">[63]</ref>, CUHK02 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CUHK03 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  that can be used as encoding backbone, such as VGG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, ResNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and Inception <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a the effectiveness of AFS module, we use PSPNet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and ResNet50 <ref type=\"bibr\" target=\"#b1\">[2]</ref> as the encoding networks. PSPNet is a scene segmentation mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: real-time performance, RL is applied as the main module in this paper. Recognizing emotion using RL <ref type=\"bibr\" target=\"#b45\">[45]</ref> and knowledge-based system <ref type=\"bibr\" target=\"#b46\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t works are no longer limited to model structure, but considers sample-based knowledge distillation <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. In this paper, we tudy rather than the past knowledge distillation approaches such as considering the level of sample <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> and model structur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rnal layers in addition to final predictions <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019;</ref><ref type=\"bibr\" target=\"#b21\">Sun et al. 2020</ref><ref type=\"bibr\">Sun et al. , 2019))</ref>, but  ther models such as TinyBERT <ref type=\"bibr\" target=\"#b15\">(Jiao et al. 2019)</ref> and MobileBERT <ref type=\"bibr\" target=\"#b21\">(Sun et al. 2020</ref>) also found it crucial for training competitiv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: posed to maximize the MI between node embeddings and a global summary embedding. Following DGI, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposes two node-level contrastive objectives to directly  the agreement of node embeddings across two corrupted views of the graph.</p><p>Following DGI, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref> employs two discriminators to directly measure MI between   summary, we provide a brief comparison between the  <ref type=\"bibr\" target=\"#b43\">[44]</ref>, GMI <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and MVGRL <ref type=\"bibr\" target=\"#b14\">[15]</ref> in Ta ax (DGI) <ref type=\"bibr\" target=\"#b43\">[44]</ref>, Graphical Mutual Information Maximization (GMI) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and Multi-View Graph Representation Learning (MVGRL) <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe et=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe ion (e.g., sigmoid or softmax) to normalize the relevance between 0 and 1. Softmax-triplet function <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> has been shown to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms that are applied to object segmentation based on CNNs <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Because the performance of deep learning algorithms depen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es that use Neural Networks have also been applied extensively to the SMB level generation problem. <ref type=\"bibr\" target=\"#b10\">Hoover, Togelius, and Yannakis (2015)</ref> trained a neural network . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: image synthesis, with recent style-based generative models <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> boasting some of th re> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We use StyleGAN2<ref type=\"bibr\" target=\"#b18\">[19]</ref> in all our experiments.</note> \t\t</body> \t\t<back> \t\t\t<div . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a vanilla and Siamese architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Each system is evaluated under specific configurations rega vych <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed to combine BERT with a Siamese architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref> for semantic representations of sentences and their similari ese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. In Siamese networks, two inputs are fed through identical s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: N, Arg1: the man}.</p><p>Currently, most event extraction methods employ the decomposition strategy <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b33\">Nguyen and Ngu arget=\"#b12\">Hong et al., , 2018;;</ref><ref type=\"bibr\" target=\"#b15\">Huang and Riloff, 2012;</ref><ref type=\"bibr\" target=\"#b5\">Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b36\">Sha et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve to automatically generate prompts given the few-shot training data using the generative T5 model <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>. This allows us to cheaply obtain effecti lly from a fixed set of label words M(Y). To address this challenging problem, we propose to use T5 <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>, a large pre-trained text-to-text Transfo e <ref type=\"table\">B</ref>.1.</p><p>For automatic template search with T5, we take the T5-3B model <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref>, which is the largest publicly available . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ures</head><p>There have been a large number of works and debates on NIC offloading of TCP features <ref type=\"bibr\" target=\"#b33\">[35,</ref><ref type=\"bibr\" target=\"#b45\">47,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Yang et al., 2019)</ref>, neighborhood information <ref type=\"bibr\" target=\"#b36\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us efforts on generalizing Transformers to computer vision <ref type=\"bibr\" target=\"#b41\">[43,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  limitations of deep SISR. SISR performance was boosted right after the non-local attention modules <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targ   <ref type=\"bibr\" target=\"#b19\">[20]</ref>, RNAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> and SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, incorporate non-local operation into their networks in orde o the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from <ref type=\"bibr\" target=\"#b1\">[2]</ref> and a deconvolution layer for upscaling the module outputs.  ution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we divide the feature maps into region grids, where the int N <ref type=\"bibr\" target=\"#b17\">[18]</ref>, OISR <ref type=\"bibr\" target=\"#b11\">[12]</ref> and SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Quantitative Evaluations In Table <ref type=\"table\" t f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours Urban100 (4\u00d7): img 078 HR Bicubic LapSRN <ref type=\"bib f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours Urban100 (4\u00d7): img 047 HR Bicubic LapSRN <ref type=\"bib f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours which only needs 20% parameters of RCAN and SAN, but ac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: preprocessing step and performs standard logistic regression to remove redundant computation. PPRGo <ref type=\"bibr\" target=\"#b3\">[4]</ref> uses Personalized PageRank to capture multi-hop neighborhood cency matrix \u00c3 and feature matrix X in the precomputation phase, which requires O(LmF ) time. PPRGo <ref type=\"bibr\" target=\"#b3\">[4]</ref> calculates approximate the Personalized PageRank (PPR) matri in APPNP and PPRGo <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>; 2) w = 0 for = 0, . . . , L \u2212 1 and w L = 1, in which case P br\" target=\"#b36\">[37]</ref>, SGC and PPRGo (linear model) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p><p>We implement GBP in PyTorch and C++, and employ initia. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ultiple Reaction Monitoring (MRM) parameters optimized for Malic acid based on published parameters <ref type=\"bibr\" target=\"#b56\">57</ref> . Electrospray ionization parameters were optimized for 0.8m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b23\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b24\">Radford et al., 2018;</ref><ref type=\"bibr\" target=\"#b5\">Devlin et al., 2019)</ref>, which has improved performances on various ngth of n by the same WordPiece tokenizer <ref type=\"bibr\" target=\"#b35\">(Wu et al., 2016)</ref> in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>. Next, as shown in Fig. <ref type=\"figure\"> s generated by the cross-modality encoder. For the cross-modality output, following the practice in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>, we append a special token [CLS] (denoted a  and each of them only focuses on a single modality (i.e., language or vision). Different from BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, which applies the transformer encoder onl om branch of Fig. <ref type=\"figure\" target=\"#fig_0\">2</ref>, the task setup is almost same to BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>: words are randomly masked with a probabil n image and a sentence match each other. This task is similar to 'Next Sentence Prediction' in BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>.</p><p>Image Question Answering (QA) In or  by the WordPiece tokenizer <ref type=\"bibr\" target=\"#b35\">(Wu et al., 2016)</ref> provided in BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> image to maximize the pre-training compute s. We take Adam (Kingma and Ba, 2014) as the optimizer with a linear-decayed learning-rate schedule <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> and a peak learning rate at 1e \u2212 4. We tra .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">BERT versus LXMERT</head><p>BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019</ref>) is a pre-trained language encoder which im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: les of the image-text pair, and the number of negative samples is expanded based on the latest MoCo <ref type=\"bibr\" target=\"#b15\">[16]</ref> framework to improve the representation ability of the neu ize of D. Our image-text retrieval model leverages contrastive learning and expands the latest MoCo <ref type=\"bibr\" target=\"#b15\">[16]</ref> as the pre-training framework, as illustrated in Figure <r dal embedding space. Implementation Details We utilize the momentumupdated history queue as in MoCo <ref type=\"bibr\" target=\"#b15\">[16]</ref> for contrastive learning. We adopt clip-wise random crops,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rchy which have shown an advantageous performance over paradigms using user-item interactions alone <ref type=\"bibr\" target=\"#b18\">[19]</ref>- <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>Generall representation involves extensive and unscalable computation with the adjacent matrix of the graph. <ref type=\"bibr\" target=\"#b18\">[19]</ref> learns a hierarchical representation of graphs by decompos nd becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. There are some of our proposed method, which fixes the number of user levels to 2. The parameter of CGNN refers to <ref type=\"bibr\" target=\"#b18\">[19]</ref>. \u2022 DIN: A popular deep neural network method without graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: olor prior in generative facial prior allows us to perform color enhancement including colorization <ref type=\"bibr\" target=\"#b72\">[73]</ref>. We believe the generative facial priors also incorporate . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> or head networks <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> by providing the c attention, we follow <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> by including a rela. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on a number of well-known benchmarks, i.e. OTB2015 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018 3 DCF tracker on four benchmarks including OTB2015 <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n single sentence relation extraction with an exception of <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, which focus on general documents while not targeting on a  e extracted dependency parse tree, where the tree roots of different sentences are linked together. <ref type=\"bibr\" target=\"#b36\">[37]</ref> proposes a method using self-attention <ref type=\"bibr\" ta  the self-attention of the words, and use a convolutional layer in self-attention blocks similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref> to alleviate the burden on the model to attend to local fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ally integrates the informations of different levels via upsam- pling and concatenating as HyperNet <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Different from the refinement strategy like stacked hourg mid output from the GlobalNet: 1) Concatenate (Concat) operation is directly attached like HyperNet <ref type=\"bibr\" target=\"#b20\">[21]</ref>,</p><p>2) a bottleneck block is attached first in each lay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b22\">[23]</ref> and end-to-end models <ref type=\"bibr\" target=\"#b23\">[24]</ref>- <ref type=\"bibr\" target=\"#b27\">[28]</ref>). In contrast to these approaches that aim to model tempor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: user packets are not detoured in transmission, numerous network attack surfaces are opened up today <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b3\">[4]</ref>. For example, an at d path validation that fill the void, such as ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref> and OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>. However, for the targeting environment which is adversarial performance of the Click router as the baseline, and compare our PSVM with the-state-of-the-art OPT <ref type=\"bibr\" target=\"#b1\">[2]</ref>.  Method and parameter setup. For fairness, we use the same  ave been proposed in ICING <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the Origin and Path Trace (OPT) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Orthogonal Sequence Verification (OSV) <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mum additional information utility for the current selected document sequence. However, researchers <ref type=\"bibr\" target=\"#b14\">[15]</ref> have already proved that this greedy document selection me al ranking, the model has to search all the ranking space, which is an NP-hard problem. Feng et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed the M2DIV model with Monte-Caro Tree Search (MCTS ult to train since MCTS is so time consuming that the M2DIV propose another raw policy without MCTS <ref type=\"bibr\" target=\"#b14\">[15]</ref> in adaption to some online ranking tasks.</p><p>In this pa . Based on the reinforced learning approach MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Feng <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed the M2DIV model with the Monte-Caro Tree Search ( imal and global optimal rankings. However, M2DIV is difficult to train since MCTS is time consuming <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and M2DIV only models the document novelty, ignoring the  p reinforced learning based models e.g. MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref> and M2DIV <ref type=\"bibr\" target=\"#b14\">[15]</ref> are taking too much time to train, we do not take those mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which solve the problem of limited equipment resources and e the problem of limited equipment resources and reduce the running time. For example, Zhang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> constructed an embedding based model to distill user's met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se performances than a single convolution due to overfitting. To overcome overfitting, Liang and Hu <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses a recurrent layer that takes feed-forward inputs into is in accordance with the limited success of previous methods using at most three recursions so far <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Among many reasons, two severe problems are vanishing and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: raged to launch Distributed Denial-of-Service (DDoS) attacks, in particular, the reflection attacks <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Due to the absence of a method to block packet header modif eans conclude on the filtering policies of the whole AS-they reveal SAV compliance for a part of it <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target ysis of the SAV deployment at the longest matching prefix is another commonly used unit of analysis <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>.</p><p>\u2022 /24 IPv4 ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  different channels is not effectively utilized. We introduce the squeeze-and-excitation operations <ref type=\"bibr\" target=\"#b29\">[30]</ref> to learn the attention weights of different feature channe  and H \u00d7 W is the size of feature map, the c th element of z is calculated via Eq. (3) by following <ref type=\"bibr\" target=\"#b29\">[30]</ref>:</p><formula xml:id=\"formula_3\">Z c = F sq (u c ) = 1 H \u00d7  ntrinsically introduce dynamics conditioned on the input, helping to boost feature discriminability <ref type=\"bibr\" target=\"#b29\">[30]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ly related to our approach are the Defense-GAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> and MagNet <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which first estimate the manifold of clean data to detect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b9\">(Kikuchi et al., 2016;</ref><ref type=\"bibr\" target=\"#b4\">Fan et al., 2017;</ref><ref type=\"bibr\" target=\"#b25\">Scarton and Specia, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Nishiha ummary more focused on a given named entity <ref type=\"bibr\" target=\"#b4\">(Fan et al., 2017)</ref>. <ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref> and <ref type=\"bibr\" target=\"#b17\">Ni t\" n=\"2\" xml:id=\"foot_1\">We did not investigate predicting ratios on a per sentence basis as done by<ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref>, and leave this for future work. End-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  based on artificial design but fails to preserve more complex local structures. Besides, GeniePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> learns the importance of different neighbours and builds a rove the use of graph information based on the smoothness values of different networks. \u2022 GeniePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> is also a GNN model with adaptive receptive fields by the  t=\"#b12\">[13]</ref> 76.13 \u00b1 1.38% 67.51 \u00b1 1.07% 72.38 \u00b1 0.37% 80.03 \u00b1 1.23% 34.12 \u00b1 3.26% GeniePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> 76.59 \u00b1 1.92% 66.95 \u00b1 1.33% 73.   </p></div> <div xmlns=\"h \">[33]</ref> 59.34 \u00b1 1.10% CS-GNN <ref type=\"bibr\" target=\"#b12\">[13]</ref> 59.91 \u00b1 0.22% GeniePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> 62.31 \u00b1 0.34% STAR-GNN (ours) 69.29 \u00b1 1.43% </p></div> <di \">[34]</ref> takes advantage of anchor sets to incorporate position information in GNNs. Ge-niePath <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposes a breadth function to learn the importance of dif  GNNs with adaptive fields, most attention scores are calculated by the similarity of node features <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Some existing mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  are done during internship at Microsoft   <ref type=\"bibr\" target=\"#b7\">(Berard et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Bansal et al., 2019)</ref>, where they leverage the available ASR and  bibr\" target=\"#b38\">Weiss et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Bansal et al., 2018</ref><ref type=\"bibr\" target=\"#b4\">Bansal et al., , 2019;;</ref><ref type=\"bibr\" target=\"#b33\">Sperber et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ow-resolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance <ref type=\"bibr\" target=\"#b53\">[51,</ref><ref type=\"bibr\" target=\"#b28\">26,</ref><ref type=\"bibr\" ta <ref type=\"bibr\" target=\"#b28\">[26]</ref>, DBPN <ref type=\"bibr\" target=\"#b18\">[16]</ref>, and RCAN <ref type=\"bibr\" target=\"#b53\">[51]</ref>. However, these methods still suffer from the large space  nsists of several up-and down-sampling layers to iteratively produce LR and HR images. Zhang et al. <ref type=\"bibr\" target=\"#b53\">[51]</ref> propose the channel attention mechanism to build a deep mo nlike the baseline U-Net, we build each basic block using B residual channel attention block (RCAB) <ref type=\"bibr\" target=\"#b53\">[51]</ref> to improve the model capacity. Following <ref type=\"bibr\"  are 2 dual models for 4\u00d7 SR and 3 dual models for 8\u00d7 SR, respectively. Let B be the number of RCABs <ref type=\"bibr\" target=\"#b53\">[51]</ref> and F be the number of base feature channels. For 4\u00d7 SR, w ctionbased methods <ref type=\"bibr\" target=\"#b18\">[16,</ref><ref type=\"bibr\" target=\"#b27\">25,</ref><ref type=\"bibr\" target=\"#b53\">51]</ref>. Haris et al. <ref type=\"bibr\" target=\"#b18\">[16]</ref> pro ata, and augment the training data following the method in <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b53\">51]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g future lightning occurrences. However, although extrapolationbased methods for weather nowcasting <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref> can be migrated to  re sensitive to different dimensions are assembled to predict mobile events in the city. Shi et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposed convolutional LSTM (ConvLSTM) for precipitation now t\u22121 .</formula><p>The ConvLSTM in this paper does not include peephole connections, as mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The data first enters the CNN modules, where sequentially a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2014</ref><ref type=\"bibr\" target=\"#b20\">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Zhou et al., ef> is a revision of CNN which uses piecewise max-pooling to extract more relation features. BiLSTM <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015)</ref> is also commonly used for RE with the help. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ular and effective approaches for addressing class imbalance <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. However, existing work are overwhelmingly dedicated to i.i .1.3\">Evaluation Metrics.</head><p>Following existing works in evaluating imbalanced classification <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, we adopt three cr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: handle data in unsupervised ways.</p><p>Problems for Importance Identification While previous works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> attempt to quan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een challenged by processors that do not block on loads <ref type=\"bibr\" target=\"#b4\">[ER94]</ref>  <ref type=\"bibr\" target=\"#b1\">[CS95]</ref>. Rather than stalling until a cache miss is satisfied, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oaches have been proposed to reduce the redundant computation in the spatial dimension. The OctConv <ref type=\"bibr\" target=\"#b5\">[6]</ref> reduces the spatial resolution by using low-frequency featur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned using Graph Neural Networks (GNNs). We used message passing neural networks (MPNNs) in practice <ref type=\"bibr\" target=\"#b7\">(Gilmer et al., 2017)</ref>, but other GNNs can fit the framework as w y parameters \u03b8, which has been proven powerful to predict chemical properties with molecular graphs <ref type=\"bibr\" target=\"#b7\">(Gilmer et al., 2017)</ref>. Particularly, given a molecule x, we inpu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: orms the sentiment view. Then, using a framework of multi-view canonical correlation analysis (CCA) <ref type=\"bibr\" target=\"#b18\">[11]</ref>, we calculate a latent embedding space in which correlatio s the linear relationship between random variables. Several nonlinear extensions such as kernel CCA <ref type=\"bibr\" target=\"#b18\">[11]</ref> and Deep CCA <ref type=\"bibr\" target=\"#b29\">[22]</ref> hav ions among multiple views using a framework of the generalization of canonical correlation analysis <ref type=\"bibr\" target=\"#b18\">[11]</ref>. Let X i (i \u2208 {v, t, s}) denote the feature matrix of the  at the distances in the resulting space between each pair of views for the same image are minimized <ref type=\"bibr\" target=\"#b18\">[11]</ref>. The objective function to learn the latent space is as fo  \u03d5 j (X j ), and w ik represents the k-th column of the matrix W i . In the conventional kernel CCA <ref type=\"bibr\" target=\"#b18\">[11]</ref>, kernel trick is used in Eq. (1). To reduce the computatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: results under different topic data by classifying the training set data according to the topic. CGU <ref type=\"bibr\" target=\"#b33\">[34]</ref> is a seq2seq model base on the convolutional gated unit an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b12\">13]</ref>, Inception V3 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, Inception V4 <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and Inception-ResNet V2 <ref type=\"bibr\" target=\"#b33\">[3 =\"#b34\">[35]</ref>, Inception V4 <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and Inception-ResNet V2 <ref type=\"bibr\" target=\"#b33\">[34]</ref>  We also attack the corresponding ensemble model (referred. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing instructions out of order. We believe that a solution similar to those proposed by Stark et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> and Cher et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> ca .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Related Work</head><p>Stark et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed a limited form of out-of-order instruction fetch . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: models hard to train. To solve this issue, <ref type=\"bibr\" target=\"#b12\">Lin et al. (2016)</ref>   <ref type=\"bibr\" target=\"#b5\">(Defferrard et al., 2016)</ref> to encode syntactic information from t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: de classification and link prediction, these methods can be prone to discrimination and instability <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al irness and Stability in GNNs. Recent studies addressed the issues of fairness and stability in GNNs <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b11\">, Fisher et al  biases prevalent in the data, but may also exacerbate them thanks to their message passing schemes <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>. Generally, in graphs such as social networ nodes with similar sensitive attribute (e.g., race, age) values are likely to connect to each other <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>. Since GNNs compute node representations by =|P (\u0177 u =1|y u =1, s=0)\u2212P (\u0177 u =1|y u =1, s=1)|, where probabilities are estimated on the test set <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>. To measure counterfactual fairness, we def t al., 2019]</ref>.  Baseline methods and implementation. We consider two baseline methods: FairGCN <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref> and RobustGCN <ref type=\"bibr\" target=\"#b38 ll><cell>12.41\u00b10.54 12.40\u00b11.62</cell><cell>10.16\u00b10.49 10.09\u00b11.55</cell></row></table><note>, FairGCN<ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>) and stability (i.e., RobustGCN<ref type=\"b bility in GNNs as independent problems and proposed standalone solutions for the same. For example, <ref type=\"bibr\" target=\"#b7\">Dai and Wang [2021]</ref> proposed FairGNN to promote fairness in GNNs  work de-biases embeddings with respect to sensitive attributes via adversarial learning frameworks <ref type=\"bibr\" target=\"#b7\">[Dai and</ref><ref type=\"bibr\">Wang, 2021, Bose and</ref><ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion as suggested by Velickovic et al. <ref type=\"bibr\" target=\"#b48\">[49]</ref> and Vaswani et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref>. The multi-head attention mechanism performs K independent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  capitalize on FlowNet-s <ref type=\"bibr\" target=\"#b58\">[59]</ref> to produce optical flow, PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> is particularly remould in our motion stream. Compared to  4 2 , and 7 2 , respectively. Two-stream Feature Aggregation. For motion stream, we utilize PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> pre-trained on Flying Chairs dataset for optical flow esti t that the receptive field in sampling stream for offset prediction is smaller than that in PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> for optical flow generation. As such, the range of estimat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch has been widely used and studied, as well as its connection to cosine similarity and L2 distance <ref type=\"bibr\" target=\"#b29\">(Mussmann and Ermon, 2016;</ref><ref type=\"bibr\" target=\"#b34\">Ram an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /www.tei-c.org/ns/1.0\"><head n=\"2.1\">Backbone Network: Transformer</head><p>Multi-layer Transformer <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017)</ref> has been widely adopted in pretrained mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ariability in behavioural and environmental patterns have stymied predictive modeling of this kind. <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref> have tried to predict stress of student tp://www.tei-c.org/ns/1.0\"><head n=\"3.2.1.\">LOCATION FEATURE BASED MLP</head><p>In the work done by <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>, a Multilayer Perceptron (MLP) with 4 f ><head n=\"4.\">Result</head><p>Due to a heavy imbalance of class labels on a scale of 1-5, we follow <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>, converting the five stress label scale ses of 23 students, totaling to 1183 data points achieving roughly equal amount of training data in <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>. These 1183 data points have the follow. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to a conventional design due to an increase in the access time on a filter cache miss. The L-Cache <ref type=\"bibr\" target=\"#b5\">[6]</ref> similarly reduces switching activity by holding loop-nested . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: at such approaches need large amounts of data for training <ref type=\"bibr\" target=\"#b3\">[4]</ref>- <ref type=\"bibr\" target=\"#b5\">[6]</ref>. This could be a fundamental issue for low-resource language \" target=\"#b13\">[14]</ref> and transfer learning from a highresource language to a low-resource one <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Transfer learning is one of the most straightforward  to deal with the low-resource problem, which has been applied successfully in some previous studies <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>In transfe rsDat corpus <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We use a fully convolutional neural network <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref> as our ASR model,  would be better merely train Softmax layer instead of retraining more layers.</p><p>The authors in <ref type=\"bibr\" target=\"#b5\">[6]</ref> used a fully convolutional end-to-end ASR model. They traine d><p>We used 11 1Dconvolutional layers on top of each other based on the architecture introduced in <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We used zero-  adapt well to Persian speech data. Our results are consistent with the finding of previous work in <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The speed of convergence: As Fig. <ref type=\"figure\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l recommendations <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>Despite the success of sequential models in Natural L proposed embedding smoothing method with a state-of-the-art sequential recommendation model, SASRec <ref type=\"bibr\" target=\"#b3\">[4]</ref>, on three public datasets. Results of extensive experiments  i-c.org/ns/1.0\"><head n=\"2.2\">SASRec</head><p>Here we concisely introduce the backbone model SASRec <ref type=\"bibr\" target=\"#b3\">[4]</ref>. SASRec (short for Self-Attention based Sequential Recommend e performances of the models on sequential recommendation, which has been widely used in literature <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Given a user's i o-layer hierarchical attention network to couple user's long-term and short-term preference. SASRec <ref type=\"bibr\" target=\"#b3\">[4]</ref> and BERT4Rec <ref type=\"bibr\" target=\"#b34\">[35]</ref> adopt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: measure of how evenly data is distributed. The percent imbalance metric equation 1 is commonly used <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Where Lmax is maximum load for any load unit and \u00b5L is th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plit graphs. So DMGI still puts more emphasis on learning the correlation of homogeneous nodes. GMI <ref type=\"bibr\" target=\"#b26\">[26]</ref> proposes a new approach  The yellow dotted lines (Eq.( <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: read. Helper threads <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> also utilize parallel hardware for speedup, without actuall e similar structures in the capture stage of our migration system.</p><p>Speculative Precomputation <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> targets memory ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Our system extracts features from those scales using a similar concept to feature pyramid networks <ref type=\"bibr\" target=\"#b7\">[8]</ref>. From our base feature extractor we add several convolutiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nals learned by the generator and the discriminator in order to obtain a better ranking model. DESA <ref type=\"bibr\" target=\"#b28\">[29]</ref> leveraged both document novelty and subtopic coverage base h R-LTR and PAMM, denoted as R-LTR-NTN and PAMM-NTN, respectively.</p><p>(4) Ensemble methods. DESA <ref type=\"bibr\" target=\"#b28\">[29]</ref> and DVGAN <ref type=\"bibr\" target=\"#b20\">[21]</ref> are tw wing previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, the relevance scor  the previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar 45]</ref>. Consistent with previous diversification models <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar ns on the intent graph, which is the same as previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. The number of GCN  stent with that in <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>.</p><formula xml:id=\"formula_12\">\ud835\udc53 rel (\ud835\udc51 \ud835\udc56 ) is produced ( loss function of the model. We follow the previous studies <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and utilize the list-pairwise loss function for optimizatio e as previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. The ClueWeb09 contains 200 queries of Web Track dataset fr get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Note that our proposed Graph4DIV does not use subtopics an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [9]</ref> and Convolutional Neural Network (CNN) features <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For each feature type, multiple channel features capturin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  SMARTS <ref type=\"bibr\" target=\"#b28\">[29]</ref>) and representative sampling (as done in SimPoint <ref type=\"bibr\" target=\"#b21\">[22]</ref>). Our experimental results using the SPEC CPU2000 benchmar st well known representative sampling approach is the SimPoint approach proposed by Sherwood et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref>. SimPoint picks a small number of sampling units that accu onsiders multiple randomly chosen cluster centers and uses the Bayesian Information Criterion (BIC) <ref type=\"bibr\" target=\"#b21\">[22]</ref> to assess the quality of the clustering: the clustering wi ifferent ways for doing so, such as code working sets <ref type=\"bibr\" target=\"#b5\">[6]</ref>, BBVs <ref type=\"bibr\" target=\"#b21\">[22]</ref>, procedure calls <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e, it has also been shown that many other combinatorial problems on graph cannot be solved by MPNNs <ref type=\"bibr\" target=\"#b31\">(Sato et al., 2019)</ref>.</p><p>In <ref type=\"bibr\" target=\"#b26\">(M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  give a brief introduction to GNNs, and one can refer to <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> for a more detailed information. GNNs deal with learning p able.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Gra here u \u2208 N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>: It uses the MLP and sum pooling as the aggregation and co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b8\">[9]</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which is a standard multimodal annotation scheme for interp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ]</ref>. Consequently, several defense methods were proposed, such as Multi-Party Computation (MPC) <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, Homomorphic Encry. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r, a pointwise non-linearity, and either an invariant or equivariant linear output layer. Recently, <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> showed that by allowing higherorder tensor =\"bibr\" target=\"#b13\">Hornik et al., 1989;</ref><ref type=\"bibr\" target=\"#b20\">Pinkus, 1999)</ref>. <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> recently proved that certain invariant GNN aces of continuous invariant (resp. equivariant) functions.</p><p>2 The case of invariant functions <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> recently proved that invariant GNNs simila n the whole set G inv. , that is, for all numbers of nodes n n max simultaneously. On the contrary, <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> work with a fixed n, and it does not seem   on the order of tensorization k s . Indeed, through Noether's theorem on polynomials, the proof of <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> shows that k s n d (n d \u2212 1)/2 is sufficie  previous invariant case could be easily extended to invariance to subgroups of O n , as is done by <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref>, for the equivariant case our theorem only ions in the rest of the introduction, in Section 2 we provide an alternative proof of the result of <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref> for invariant GNNs (Theorem 1), which wil t. Theorem 1. For any \u03c1 \u2208 F MLP , N inv. (\u03c1) is dense in C(G inv. , d edit ).</p><p>Comparison with <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <re th <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. The two proofs are however different: th See the next subsection for details.</p><p>One improvement of our result with respect to the one of <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref> is that it can handle graphs of varying s bibr\" target=\"#b0\">(Battaglia et al., 2016)</ref>. Another outstanding open question, formulated in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>, is the characterization of the approxima ing a single hidden layer of such equivariant operators followed by an invariant layer is proved in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b</ref>) (see also <ref type=\"bibr\">(Kondor et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[28]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref>, have made sign o existing dimensions of depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>, width 2 , and cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref>. We state in Sec. 4.4 that increasing scale is more effect rformance of state-of-the-art CNNs, e.g., ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref>.</p><p> \">[28]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref>, achieving stat modern backbone CNNs architectures, e.g., ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref>. Instea odules have been proposed in recent years, including cardinality dimension introduced by Xie et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, as well as squeeze and excitation (SE) block presented by nts. As shown in Fig. <ref type=\"figure\">3</ref>, we can easily integrate the cardinality dimension <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the SE block <ref type=\"bibr\" target=\"#b24\">[25]</ref> sion cardinality.</head><p>The dimension cardinality indicates the number of groups within a filter <ref type=\"bibr\" target=\"#b55\">[56]</ref>. This dimension changes filters from single-branch to mult ig. <ref type=\"figure\">3</ref>: The Res2Net module can be integrated with the dimension cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> (replace conv with group conv) and SE <ref type=\"bibr\" tar  into the state-ofthe-art models, such as ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref> and Big-Lit and bLRes2Net-50, respectively.</p><p>The proposed scale dimension is orthogonal to the cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> dimension and width <ref type=\"bibr\" target=\"#b22\">[23]</r 4]</ref> dataset, we mainly use the ResNet-50 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt-50 <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA-60 <ref type=\"bibr\" target=\"#b59\">[60]</ref>, and bLR ments on the CIFAR <ref type=\"bibr\" target=\"#b26\">[27]</ref> dataset, we use the ResNeXt-29, 8c\u00d764w <ref type=\"bibr\" target=\"#b55\">[56]</ref> as our baseline model. Empirical evaluations and discussio ons, we use the Pytorch implementation of ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref> as well as  type=\"bibr\" target=\"#b22\">[23]</ref>. On the CIFAR dataset, we use the implementation of ResNeXt-29 <ref type=\"bibr\" target=\"#b55\">[56]</ref>. For all tasks, we use the original implementations of bas ement of 0.73% in terms of top-1 error over the  <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, SE-Net <ref type=\"bibr\" target=\"#b24\">[25]</ref>, bLResNe ve been shown to have stronger representation capability <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref> for vision tasks. To validate our model with greater depth  which contains 50k training images and 10k testing images from 100 classes. The ResNeXt-29, 8c\u00d764w <ref type=\"bibr\" target=\"#b55\">[56]</ref> is used as the baseline model. We only replace the origina iv xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Scale Variation</head><p>Similar to Xie et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, we evaluate the test performance of the baseline model by ng different CNN dimensions, including scale (Equation ( <ref type=\"formula\">1</ref>)), cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>. Whil fix all other dimensions. A series of networks are trained and evaluated under these changes. Since <ref type=\"bibr\" target=\"#b55\">[56]</ref> has already shown that increasing cardinality is more effe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ance modeling. State-of-the-art techniques represent hardware dataflow using either compute-centric <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref> or data-centric no ataflow is specified using loop transformation directives including reorder, blocking, and parallel <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. The compute-centr pute-centric notation-based models only analyze data reuse opportunities in a coarse-grained manner <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. For example, Inte n is widely adopted as it can directly represents dataflow in high-level languages using directives <ref type=\"bibr\" target=\"#b78\">[39,</ref><ref type=\"bibr\" target=\"#b95\">56]</ref>. In <ref type=\"bib .org/ns/1.0\"><head>Features</head><p>Computation-centric Data-centric STT Relation-Centric Timeloop <ref type=\"bibr\" target=\"#b78\">[39]</ref> Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> MAE ze various hardware metrics, and support for non-systolic array spatial architecture. Both Timeloop <ref type=\"bibr\" target=\"#b78\">[39]</ref> and Interstellar <ref type=\"bibr\" target=\"#b95\">[56]</ref> 3\">[14]</ref>, dataflow is notated using two hyperplanes with polyhedral dependency graph. Timeloop <ref type=\"bibr\" target=\"#b78\">[39]</ref> describes the design space using a concise and unified loo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cting the Markov dependence to be a regular undirected graph. It was recently proved by Garg et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>, based on a new multi-matrix extension of the Golden-Thomps ot necessarily the stationary distribution), which significantly improves the result of Garg et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>. More formally, we prove the following theorem: Theorem 1 ( ry distribution \u03c0. Our strategy is to incorporate the concentration of matrix-valued functions from <ref type=\"bibr\" target=\"#b9\">[10]</ref> into the study of general Markov chains from <ref type=\"bib To bound the expectation term, we invoke the following multi-matrix Golden-Thompson inequality from <ref type=\"bibr\" target=\"#b9\">[10]</ref>, by letting</p><formula xml:id=\"formula_14\">H j = tf (v j )  tf (v j ), j \u2208 [k].</formula><p>Theorem 4 (Multi-matrix Golden-Thompson Inequality, Theorem 1.5 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let H The key point of this theorem is to relate the expo urther bounded via the following lemma by letting e i\u03c6 = \u03b3 + ib. Lemma 1 (Analogous to Lemma 4.3 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let P be a regular Markov chain with state space [N ] wit  of k.</p><p>The analysis relies on incorporating the concentration of matrix-valued functions from <ref type=\"bibr\" target=\"#b9\">[10]</ref> into the study of general Markov chains from <ref type=\"bib ctral expansion from such inner products. In contrast, the undirected regular graph case studied in <ref type=\"bibr\" target=\"#b9\">[10]</ref> can be handled using the standard inner products, as well a uality</head><p>We need the following multi-matrix Golden-Thompson inequality from from Garg et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Theorem 4 (Multi-matrix Golden-Thompson Inequality, Theore =\"bibr\" target=\"#b9\">[10]</ref>. Theorem 4 (Multi-matrix Golden-Thompson Inequality, Theorem 1.5 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let </p><formula xml:id=\"formula_45\">f : [N ] \u2192 R d\u00d7d be  te e i\u03c6 = \u03b3 + ib with \u03b3 2 + b 2 = |\u03b3 + ib| 2 = e i\u03c6 2 = 1:</p><p>Lemma 1 (Analogous to Lemma 4.3 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let P be a regular Markov chain with state space [N ] wit http://www.tei-c.org/ns/1.0\"><head>B.3 Proof of Lemma 1</head><p>Lemma 1 (Analogous to Lemma 4.3 in <ref type=\"bibr\" target=\"#b9\">[10]</ref>). Let P be a regular Markov chain with state space [N ] wit  \u03c0 \u221a d \u03b11 + \u03b12\u03b13 1 \u2212 \u03b14 k , which implies \u03c0 \u2297 vec(I d ), z k \u03c0 \u2264 \u03c6 \u03c0 d \u03b11 + \u03b12\u03b13 1 \u2212 \u03b14k The same as<ref type=\"bibr\" target=\"#b9\">[10]</ref>, we can bound \u03b1 1 , \u03b1 2 \u03b1 3 , \u03b1 4 by:\u03b11 = exp (t ) \u2212 t \u2264 1 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rediction tasks have achieved major advances recently. Encoder-decoder architectures like the U-Net <ref type=\"bibr\" target=\"#b31\">(Ronneberger et al., 2015)</ref> are state-of-the-art methods for the tion. Similar to pixelwise prediction tasks <ref type=\"bibr\" target=\"#b15\">(Gong et al., 2014;</ref><ref type=\"bibr\" target=\"#b31\">Ronneberger et al., 2015)</ref>, node classification tasks aim to mak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al influence. Indeed, extensive work has been done on social influence prediction in the literature <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta ial equations extended from the classic 'Susceptible-Infected' (SI) model; Most recently, Li et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> proposed an end-toend predictor for inferring cascade size  efforts to detect those global patterns automatically using deep learning, e.g., the DeepCas model <ref type=\"bibr\" target=\"#b25\">[26]</ref> which formulate cascade prediction as a sequence problem a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: galy 2010;</ref><ref type=\"bibr\" target=\"#b23\">Remmert et al. 2012)</ref>, or a combination of both <ref type=\"bibr\" target=\"#b2\">(Altschul et al. 1997)</ref> to align the sequence against the given d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce between the predicted target embedding h \ud835\udc3f \ud835\udc62 and the ground-truth embedding h \ud835\udc62 , as proposed by <ref type=\"bibr\" target=\"#b15\">[16]</ref>, due to its popularity as an indicator for the semantic si models and the GNN models are initialized by the NCF embedding results. We use Spearman correlation <ref type=\"bibr\" target=\"#b15\">[16]</ref> to measure the agreement between the ground truth embeddin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b8\">(Klicpera, Bojchevski, and G\u00fcnnemann 2019)</ref>. For disassortative networks, we add Geom-GCN <ref type=\"bibr\" target=\"#b15\">(Pei et al. 2020)</ref> and MLP as new benchmarks. All methods were i are different from those in Geom-GCN. The reason is that in the disassortative networks provided by <ref type=\"bibr\" target=\"#b15\">(Pei et al. 2020)</ref>, i.e., Cham-5 and Squi-5 in Table <ref type=\"  and G\u00fcnnemann 2019)</ref> incorporates personalized PageRank to the aggregation function; Geom-GCN <ref type=\"bibr\" target=\"#b15\">(Pei et al. 2020</ref>) utilizes the structural similarity to capture. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ure is based on recent attention-based end-to-end ASR models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and TTS models such as Tacotron <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and it's detections are publicly available.</p><p>We use the methodology and tools of Hoiem et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> For each category at test time we look at the top N predic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d architecture efficient, as well as maintain higher prediction capacity?</p><p>Vanilla Transformer <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017</ref>) has three significant limitations when so tei-c.org/ns/1.0\"><head>Efficient Self-attention Mechanism</head><p>The canonical self-attention in <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017</ref>) is defined on receiving the tuple input ( ing long sequential outputs through one forward procedure</p><p>We use a standard decoder structure <ref type=\"bibr\" target=\"#b29\">(Vaswani et al. 2017)</ref> in Fig.</p><p>(2), and it is composed of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: int locations based on hand-crafted features. Recent works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targe ]</ref> uses deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> and Zisserman et al. <ref type=\"bibr\" target=\"#b1\">[2]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, which explicitly aims to distinguish positive user-item in ype=\"bibr\" target=\"#b27\">27]</ref> to deep neural networks <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref>, a variety of techniques have been studied to effectively m personalized ranking <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> defines the similarity of a user and an item by the inner p hbor-based encoder <ref type=\"bibr\" target=\"#b10\">[10,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> which additionally takes a given set of users (or items) as et=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> which provide the minimum count of user-item interactions f get=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b33\">33]</ref> train their model s xploit the neighborhood information of users and items to compute the representations.</p><p>\u2022 NGCF <ref type=\"bibr\" target=\"#b32\">[32]</ref>: A neighbor-based method which encodes a user's (and item'. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ward and backward, and advocate setting g l to the identity function.</p><p>The vision transformers <ref type=\"bibr\" target=\"#b18\">[19]</ref> instantiate a particular form of residual architecture: af consider more specifically the vision transformer (ViT) architecture proposed by Dosovitskiy et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> as the reference architecture and adopt the data-efficient [63]</ref> fails to properly converge above 18 layers without adjusting hyper-parameters. Large ViT <ref type=\"bibr\" target=\"#b18\">[19]</ref> models with 24 and 32 layers were trained with large train ween the width and the depth, both contribute to the performance as reported by Dosovitskiy et al.. <ref type=\"bibr\" target=\"#b18\">[19]</ref> with longer training schedules. But if one parameter is to d without external data. We compare CaiT with DeiT <ref type=\"bibr\" target=\"#b62\">[63]</ref>, Vit-B <ref type=\"bibr\" target=\"#b18\">[19]</ref>, TNT <ref type=\"bibr\" target=\"#b25\">[26]</ref>, T2T <ref t  transfer learning results to those of Efficient-Net <ref type=\"bibr\" target=\"#b61\">[62]</ref>, ViT <ref type=\"bibr\" target=\"#b18\">[19]</ref> and DeiT <ref type=\"bibr\" target=\"#b62\">[63]</ref>. These  er architecture working directly on small patches has obtained state of the art results on ImageNet <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Nevertheless, the state of the art has since returned to  : as discussed in the introduction, the architecture (a) of ViT and DeiT is a pre-norm architecture <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, in which the laye ion 224, and optionally fine-tune them at a higher resolution to trade performance against accuracy <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b3\">[4]</ref> and Neural Network structure with Connectionist temporal classification (CTC) loss <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The hybrid hidden   by the acoustic model, and finally get better results. y * = arg max y log p(y|x) + \u03bb log P LM (y) <ref type=\"bibr\" target=\"#b4\">(5)</ref> where P LM (y) is provided by the LM, y * denotes the final . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b0\">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type=\"bibr\" target=\"#b7\">[8]</ref>, CIFAR-100 <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and LFW . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ruments such as hyperspectral and synthetic aperture radar <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, increasingly more computer vision methods have achieved rem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce to show themselves. However,  <ref type=\"bibr\" target=\"#b0\">[1]</ref> 59.5% 60.1% 70.7% SemeiEmb <ref type=\"bibr\" target=\"#b22\">[23]</ref> 59.0% 59.6% 71.7% LP <ref type=\"bibr\" target=\"#b27\">[28]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> allow end-to-end differentable losses over data with arbitr for unsupervised losses, most work follows the semi-supervised setting for node classification from <ref type=\"bibr\" target=\"#b28\">[29]</ref>. For a complete introductions to the vast topic we refer i ider transductive GNNs that output a single embedding per node. Graph convolutional networks (GCNs) <ref type=\"bibr\" target=\"#b28\">[29]</ref> are simple yet effective <ref type=\"bibr\" target=\"#b50\">[5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to handle the variety of models of parallelism that appear in HPC programs. Toward this end, PEBIL <ref type=\"bibr\" target=\"#b0\">[1]</ref> has recently added support for handling multithreaded x86 64 afety</head><p>PEBIL generates and inserts code into the program which has two principle functions: <ref type=\"bibr\" target=\"#b0\">(1)</ref> to add functionality to a program, functionality which is us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> on the mixture of experts <ref type=\"bibr\" target=\"#b27\">[28]</ref> (MoE) show that MoE can improve the overall model's capabi  domains are usually totally different from source domains.</p><p>Mixture of Experts. Jacobs et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> first introduced the mixture of experts (MoE). MoE aims to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ke other deep learning models, GNNs have also been shown to be vulnerable under adversarial attacks <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which has recently attracted increasing research interest sting (evasion); the attacker may aim to mislead the prediction on specific nodes (targeted attack) <ref type=\"bibr\" target=\"#b27\">[28]</ref> or damage the overall task performance (untargeted attack) model parameters, input data, and labels; grey-box attacks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> have partial inform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN <ref type=\"bibr\" target=\"#b1\">(Dong et al., 2014)</ref>. It consists of more than one composition fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b22\">Lin et al. 2018)</ref>, etc., with different language environment <ref type=\"bibr\" target=\"#b2\">(Bahaddad et al. 2018</ref>).</p></div><figure xmlns=\"http://www.tei-c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndependence assumptions between target tokens <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2019;</ref><ref type=\"bibr\" target=\"#b52\">Wang et al., 2019)</ref>. These issues have been addressed via partia. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  search and indexing <ref type=\"bibr\" target=\"#b44\">[43]</ref>, personalized content recommendation <ref type=\"bibr\" target=\"#b47\">[46]</ref>, and question answering <ref type=\"bibr\" target=\"#b43\">[42. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ased document embeddings from GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> (as Doc2vec implementation <ref type=\"bibr\" target=\"#b32\"> they encode the segments with GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> and compute their similarity to determine whether papers a ref type=\"bibr\" target=\"#b34\">35]</ref> but unable to represent entire documents. Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> (also known as Doc2vec), extends word2vec to learn embeddi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar uage to bootstrap the low-resource AM; multi-task training <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and ensemble learning <ref type=\"bibr\" target=\"#b18\">[19,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \"the deeper the better\" might not be the case in SR. Inspired by the success of very deep networks <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">27,</ref><ref type=\"bibr\" targ p networks could suffer from the performance degradation problem, as observed in visual recognition <ref type=\"bibr\" target=\"#b7\">[8]</ref> and image restoration <ref type=\"bibr\" target=\"#b16\">[17]</r nce Sec. 1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <re iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1.\">ResNet</head><p>The main idea of ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref> is to use a residual learning framework to ease the training iple weight layers in the residual unit) Table <ref type=\"table\">1</ref>. Strategies used in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref t bel>(1)</label></formula><p>where x is the output of the residual unit, h(x) is an identity mapping <ref type=\"bibr\" target=\"#b7\">[8]</ref> : h(x) = x, W is a set of weights (the biases are omitted to ng the recursive block structure, in which several residual units are stacked. Noted that in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, different residual units use different inputs for the ident </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1.\">Residual Unit</head><p>In ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the acti fig_1\"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Simplified structures of (a) ResNet<ref type=\"bibr\" target=\"#b7\">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r any combination of input parameters ( \u00a7 3).</p><p>\u2022 Based on the red-blue pebble game abstraction <ref type=\"bibr\" target=\"#b34\">[34]</ref>, we provide a new method of deriving I/O lower bounds (Lem o parents (or no children, respectively). Red-Blue Pebble Game Hong and Kung's red-blue pebble game <ref type=\"bibr\" target=\"#b34\">[34]</ref> models an execution of an algorithm in a two-level memory  achinery for deriving I/O lower bounds for general CDAGs. We extend the main lemma by Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref>, which provides a method to nd an I/O lower bound for a gi rresponding pebbling. Hong and Kung use a speci c variant of this partition, denoted as S-partition <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p><p>We rst introduce our generalization of S-partition, ation that each V i performs at least S I/O operations, we phrase the lemma by Hong and Kung: L 1 ( <ref type=\"bibr\" target=\"#b34\">[34]</ref>). e minimal number Q of I/O operations for any valid execu ulation of the CDAG, where a calculation is a sequence of allowed moves in the red-blue pebble game <ref type=\"bibr\" target=\"#b34\">[34]</ref>. Divide the complete calculation into h consecutive subcom d Lemma. We now use the above de nitions and observations to generalize the result of Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> he remaining properties of a valid X -partition S(X ), we use the same reasoning as originally done <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p><p>erefore, a complete calculation performing q &gt; ( , and therefore may be seen as the last step in the long sequence of improved bounds. Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref> derived an asymptotic bound \u2126 n 3 / \u221a S for the sequential  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"10.1\">General I/O Lower Bounds</head><p>Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref> analyzed the I/O complexity for general CDAGs in their the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng from position s. This neural network combines the roles of both policy network and value network <ref type=\"bibr\" target=\"#b11\">12</ref> into a single architecture. The neural network consists of m perfect information. We follow the formalism of alternating Markov games described in previous work <ref type=\"bibr\" target=\"#b11\">12</ref> , noting that algorithms based on value or policy iteration  ompare three distinct versions of AlphaGo:</p><p>1. AlphaGo Fan is the previously published program <ref type=\"bibr\" target=\"#b11\">12</ref> that played against Fan Hui in October 2015. This program wa described in this paper. However, it uses the same handcrafted features and rollouts as AlphaGo Lee <ref type=\"bibr\" target=\"#b11\">12</ref> and training was initialised by supervised learning from hum ue component, it was possible to avoid overfitting to the values (a problem described in prior work <ref type=\"bibr\" target=\"#b11\">12</ref> ). After 72 hours the move prediction accuracy exceeded the  the KGS test set; the value prediction error was also substantially better than previously reported <ref type=\"bibr\" target=\"#b11\">12</ref> . The validation set was composed of professional games from of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored to the tournament values from prior work <ref type=\"bibr\" target=\"#b11\">12</ref> , and correspond to the players reported in that work. The r lso performed against baseline players with Elo ratings anchored to the previously published values <ref type=\"bibr\" target=\"#b11\">12</ref> .</p><p>We measured the head-to-head performance of AlphaGo  hat maximise an upper confidence bound Q(s, a) + U (s, a), where U (s, a) \u221d P (s, a)/(1 + N (s, a)) <ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b23\">24</ref> , until a leaf node  After 72 hours the move prediction accuracy exceeded the state of the art reported in previous work <ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b2\">(Caruana, 1995;</ref><ref type=\"bibr\" target=\"#b1\">Bengio et al., 2011;</ref><ref type=\"bibr\" target=\"#b0\">Bengio, 2011)</ref>. In transfer learning, we first train a base netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ransformers in the context of image processing, several approximations have been tried in the past: <ref type=\"bibr\" target=\"#b32\">Parmar et al. (2018)</ref> applied the self-attention only in local n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ofed traffic that may enter deployers' ASes. They either filter packets based on the number of hops <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> or the mapping betw ning accurate mappings for all input interfaces from route information is difficult. HCF and NetHCF <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> extracts the number. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o make utmost of the multi-scale features, the full-scale skip connections are designed in U-Net 3+ <ref type=\"bibr\" target=\"#b15\">[16]</ref>. However, the design philosophy of full-scale skip connect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cal works include XLA <ref type=\"bibr\" target=\"#b8\">[9]</ref> (applicable to training as well), TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, Glow <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Tensor Com gebra operations and calls into backend-specific libraries for execution on different backends. TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref> is an end-to-end compiler framework with Halide at the core. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s, such as recommender system <ref type=\"bibr\" target=\"#b33\">[34]</ref>, knowledge graph completion <ref type=\"bibr\" target=\"#b40\">[41]</ref> and drug repurposing <ref type=\"bibr\" target=\"#b26\">[27]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are important contributors to PUMA's performance and energy efficiency.</p><p>The Emu architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref> is a recently proposed architecture for big data analysis, i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref> try to improve the static injection framework. CriticalFau structions using symbolic execution, which enumerates all potential hardware errors.</p><p>The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> presents a selective protection technique that allows user tion technique that allows users to selectively protect these SDC-prone data. The main idea of work <ref type=\"bibr\" target=\"#b15\">[16]</ref> is predicting the SDC proneness of a program's data firstl put program are selected by GA.</p><p>Step 3 strengthens the identified vulnerable blocks. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> introduces a prediction model named SDCAuto to predict the causing errors. Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> illustrates the diagram of the work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> first  _1\">2</ref> illustrates the diagram of the work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> first compiles the source code into LLVM IR, and extracts  sidered, since it always causes illegal opcode exception rather than SDC.</p><p>Finally, as in work <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we assume that at most one fault occurs during a program' e our results with the work <ref type=\"bibr\" target=\"#b14\">[15]</ref> and SDCAuto presented in work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. We use our approach to maximize SDC coverage under the us /1.0\" xml:id=\"fig_1\"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of the work<ref type=\"bibr\" target=\"#b15\">[16]</ref> </figDesc><graphic url=\"image-2.png\" coords=\"4,99.47,451.0. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  for other network analysis tasks, such as link prediction <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and graph classification <ref type=\"bibr\" target=\"#b16\">[17. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n this work, we focus on three of these proposed schemes -the first based on working set signatures <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the second based n lead to unpredictable, non-optimal results. Consequently, algorithms such as the ones proposed in <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref> do not perform tu ets, BBVs, and conditional branch counters. In addition to instruction working set based techniques <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11], we evaluate branch and procedure working set based tec  are defined as the set of branches/procedures touched over the sampling interval. In previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11], we defined a similarity metric called the relative wor  and working sets are too large to be efficiently stored and compared in hardware. In previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>, we proposed a ha e</head><p>A working set signature is a lossy-compressed representation of the complete working set <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The signature is 512 bits) and accumulator tables (1024, 128, 32 entries) are similar to those used in previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11] <ref type=\"bibr\" target=\"#b16\">[19]</ref>. Procedure si  be used in tuning algorithms to reuse previously found optimal configurations for recurring phases <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11] <ref type=\"bibr\">[12] [19]</ref>. This eliminates a sig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecifically, we propose a novel module based on dimension recalibration and self-attention mechanism <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> to learn the relations. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s can better understand their users' preferences using the social network. Following the convention <ref type=\"bibr\" target=\"#b12\">[14]</ref>, we call this variant of SR sessionbased social recommenda are not suitable for SSR because they do not consider the sequential order of user behaviors. DGRec <ref type=\"bibr\" target=\"#b12\">[14]</ref> is currently the only method for SSR but it is not efficie n-based recommendation. Currently, the only method for session-based social recommendation is DGRec <ref type=\"bibr\" target=\"#b12\">[14]</ref>, which models dynamic user behaviors with an RNN and conte online bookmarking system where users can assign a variety of semantic tags to bookmarks. Following <ref type=\"bibr\" target=\"#b12\">[14]</ref>, we consider a sequence of tags with timestamps assigned t session recommender could be used for PSR. ( <ref type=\"formula\" target=\"#formula_9\">8</ref>) DGRec <ref type=\"bibr\" target=\"#b12\">[14]</ref> is the state-of-the-art method for SSR that captures users We did not include the methods for social recommendation because they are uncompetitive as shown in <ref type=\"bibr\" target=\"#b12\">[14]</ref>. Following <ref type=\"bibr\" target=\"#b0\">[2,</ref><ref typ wing previous studies <ref type=\"bibr\" target=\"#b5\">[7,</ref><ref type=\"bibr\" target=\"#b9\">11,</ref><ref type=\"bibr\" target=\"#b12\">14,</ref><ref type=\"bibr\" target=\"#b17\">19]</ref>, we embed the user  monly used in the literature of SR and social recommendation <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b12\">14,</ref><ref type=\"bibr\" target=\"#b17\">19]</ref>: (1) Gowalla <ref t composes the functions defined in Equations ( <ref type=\"formula\" target=\"#formula_11\">10</ref>) to <ref type=\"bibr\" target=\"#b12\">(14)</ref>. It is easy to verify that the complexity of \ud835\udc53 is \ud835\udc42 (|\ud835\udc41 \ud835\udc3f . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: data sources. We use natural language processing methods, such as Latent Dirichlet Allocation (LDA) <ref type=\"bibr\" target=\"#b5\">[8]</ref> and topical phrase mining <ref type=\"bibr\" target=\"#b12\">[16 as their topical similarity using a process they call Phrase LDA.</p><p>Latent Dirichlet Allocation <ref type=\"bibr\" target=\"#b5\">[8]</ref> is the most common topic modeling process and PLDA+ is a sca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  instead of handcrafting them or using other representation is an idea that emerged a few years ago <ref type=\"bibr\" target=\"#b5\">(Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b8\">Gilmer et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, the work of <ref type=\"bibr\" target=\"#b0\">[1]</ref> replaces linear product operation in GRU-RNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> with convolution and proposes ConvGRU for action recognition. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olutional kernels such as a fixed window <ref type=\"bibr\" target=\"#b5\">(Collobert et al. 2011;</ref><ref type=\"bibr\" target=\"#b12\">Kalchbrenner and Blunsom 2013)</ref>. When using such kernels, it is  e=\"bibr\" target=\"#b17\">Mikolov (2012)</ref> uses recurrent neural network to build language models. <ref type=\"bibr\" target=\"#b12\">Kalchbrenner and Blunsom (2013)</ref> proposed a novel recurrent netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Hidasi et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> use d each item a fixed weight based on the relative distance with response to the target item. Li et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> propose an RNN based encoder-decoder model (NARM), which ta transaction data is used in this study.</p><p>Following <ref type=\"bibr\" target=\"#b4\">[5]</ref> and <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we filter out sessions of length 1 and items that appear l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mum additional information utility for the current selected document sequence. However, researchers <ref type=\"bibr\" target=\"#b14\">[15]</ref> have already proved that this greedy document selection me al ranking, the model has to search all the ranking space, which is an NP-hard problem. Feng et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed the M2DIV model with Monte-Caro Tree Search (MCTS ult to train since MCTS is so time consuming that the M2DIV propose another raw policy without MCTS <ref type=\"bibr\" target=\"#b14\">[15]</ref> in adaption to some online ranking tasks.</p><p>In this pa . Based on the reinforced learning approach MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Feng <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed the M2DIV model with the Monte-Caro Tree Search ( imal and global optimal rankings. However, M2DIV is difficult to train since MCTS is time consuming <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and M2DIV only models the document novelty, ignoring the  p reinforced learning based models e.g. MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref> and M2DIV <ref type=\"bibr\" target=\"#b14\">[15]</ref> are taking too much time to train, we do not take those mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: x 1 , ..., x M ) and q [M ] (x) \u2236= \u220f M i=1 p(x i ).</formula><p>According to dual representation in <ref type=\"bibr\" target=\"#b25\">[27]</ref>, we have the following lower bound for KL divergence betwe bound for KL divergence between p and q, and hence TC.</p><p>Lemma 1 (Dual version of f -divergence <ref type=\"bibr\" target=\"#b25\">[27]</ref>).</p><formula xml:id=\"formula_5\">D KL p [M ] q [M ] \u2265 sup . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the issues of fairness and stability in GNNs <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021</ref><ref type=\"bibr\" target=\"#b11\">, Fisher et al., 2020</ref><ref type=\"bibr\" target=\"#b13\">, Geisler e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ower consumption numbers provided by McPAT.</p><p>We create representative 1B-instruction SimPoints <ref type=\"bibr\" target=\"#b16\">[17]</ref> for the SPEC CPU2017 benchmarks. We sort the benchmarks by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the coverage analysis to automatically modify the directives to the test generator. For example, in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a genetic algorithm is used to select and modify test-cases. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b30\">[30]</ref>, many researchers have applied attention mechanisms to computer vision. Mnih et al. <ref type=\"bibr\" target=\"#b31\">[31]</ref> first used the attention mechanism with recurrent neural n  localization.</p><p>Encouraged by the successes of the attention mechanism on various applications <ref type=\"bibr\" target=\"#b31\">[31]</ref>, <ref type=\"bibr\" target=\"#b38\">[38]</ref>- <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: equential recommenders <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. It has been demonstrated that contextual information is im ich is able to achieve the same effect as previous methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Besides, the pre-trained data representations can be also   them by the interaction timestamps ascendingly. Following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we only keep the 5-core datasets, and filter unpopular ite te the performance, which are widely used in related works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Since HR@1 is equal to NDCG@1, we report results on HR@{1, ation Machines to incorporate arbitrary real-valued features to the sequential recommendation. FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> employed a feature-level self-attention block to leverage  ttribute-aware sequential models such as TransFM <ref type=\"bibr\" target=\"#b15\">[16]</ref> and FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> leverage the contextual features to improve the sequential  and attribute as the input to the model. ( <ref type=\"formula\" target=\"#formula_16\">11</ref>) FDSA <ref type=\"bibr\" target=\"#b28\">[29]</ref> constructs a feature sequence and uses a featurelevel self. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  from the source to the output. Recently, encoder-decoder (enc-dec) models with attention mechanism <ref type=\"bibr\" target=\"#b4\">[3,</ref><ref type=\"bibr\" target=\"#b5\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> and relevance-based entity recommendation <ref type=\"bibr\" target=\"#b7\">[Gu et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Zhou et al., 2020]</ref>, the most commonly adopted user interface is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ectures (e.g., NASNet, AmoebaNet) are not efficient for inference. Recent hardwareaware NAS methods <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Tan et al., 201 ers and skip the last N \u2212 D layers, rather than keeping any D layers as done in current NAS methods <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Wu et al., 2019 nd input image size<ref type=\"foot\" target=\"#foot_1\">2</ref> . We also build a latency lookup table <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref> on each target hardware platform to predict t forms (Figure <ref type=\"figure\" target=\"#fig_7\">7</ref>) using the ProxylessNAS architecture space <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref>. OFA consistently improves the trade-off betw ). It is impossible for previous NAS methods <ref type=\"bibr\" target=\"#b29\">(Tan et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Cai et al., 2019)</ref> due to the prohibitive training cost.</p><p>Re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b101\">102,</ref><ref type=\"bibr\" target=\"#b102\">103]</ref>. MetaSys allows communicating an applications' QoS requir. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce on heterophily (or low-homophily) graphs <ref type=\"bibr\" target=\"#b15\">(Pei et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Zhu et al., 2020)</ref>, which-unlike homophily graphs-have many neig g works have proposed some effective designs <ref type=\"bibr\" target=\"#b15\">(Pei et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Zhu et al., 2020)</ref>, it remains an under-explored area. These two , 2002)</ref>. For instance, in protein networks, amino acids of different types tend to form links <ref type=\"bibr\" target=\"#b33\">(Zhu et al., 2020)</ref>, and in transaction networks, fraudsters are ls tackling heterophily: Geom-GCN <ref type=\"bibr\" target=\"#b15\">(Pei et al., 2019)</ref> and H2GCN <ref type=\"bibr\" target=\"#b33\">(Zhu et al., 2020)</ref>;</p><p>(3) the state-of-the-art model for ov e use the code from a well-accepted Github repository 2 . For GraphSage, we report the results from <ref type=\"bibr\" target=\"#b33\">(Zhu et al., 2020)</ref>, which uses the same data and splits. For th ayers. Best model per benchmark highlighted in gray. The \" \u2020 \" results (GraphSAGE) are obtained from<ref type=\"bibr\" target=\"#b33\">(Zhu et al., 2020)</ref>.</figDesc><table><row><cell></cell><cell>Tex s first outlined in the context of GNNs in <ref type=\"bibr\" target=\"#b15\">(Pei et al., 2019)</ref>. <ref type=\"bibr\" target=\"#b33\">Zhu et al. (2020)</ref> identified a set of effective designs that al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b5\">6]</ref>, previous model bias <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and position bias < niform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. In this paper, we  f a recommender system, and that explicitly handling of the biases may help improve the performance <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta f>.</p><p>A recent work has shown that a uniform data can alleviate the previous model bias problem <ref type=\"bibr\" target=\"#b15\">[16]</ref>. But the uniform data is always few and expensive to colle us on how to solve the bias problems in a recommender system with a uniform data. Along the line of <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we conduct empirical studies on a real advertising system /p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">MOTIVATION</head><p>In a recent work <ref type=\"bibr\" target=\"#b15\">[16]</ref>, it is shown that a uniform (i.e., unbiased) data can alle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: CAREER Award CCR -0133777.</p><p>technology, the larger the cache, the slower the cache will become <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and larger caches increase the cost of manufacturing. Anoth he. An inclusive cache system implies that the contents of the L1 cache be a subset of the L2 cache <ref type=\"bibr\" target=\"#b1\">[2]</ref>. This decreases the effective cache capacity available for u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ted head sequences by warping a single or multiple static frames. Both classical warping algorithms <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b26\">28]</ref> and warping fields s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: also argue that training objectives of these algorithms (either reconstructing the adjacency matrix <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or feature matrix  CN as the encoder, then decode by inner product with cross-entropy loss. As variants of GAE (VGAE), <ref type=\"bibr\" target=\"#b22\">[23]</ref> exploits adversarially regularized method to learn more ro ional networks with the (variational) autoencoder for representation learning.</p><p>ARGA and ARVGA <ref type=\"bibr\" target=\"#b22\">[23]</ref> add adversarial constraints to GAE and VGAE respectively, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: alistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27]  and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p></div> \t\t\t</abstract> \t\t</profileDesc> \t</teiHeader> \t e instead of pixel space. Ledig et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Sajjadi et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> further propose adversarial loss to encourage the network  7]</ref> introduce an adversarial loss, generating images with more natural details. Sajjadi et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> develop a similar approach and further explore the local t pe=\"figure\">5</ref>. GAN-based methods (SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref>, EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref> and ours) clearly outperform PSNR-oriented approaches in t ef>, and GAN-based methods, such as SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and En-hanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. More results are provided in the supplementary material.  -GAN and the counterpart generated by SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> or EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. The users were asked to pick the image with more natural  on, our method is ranked higher than SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>, especially in building, animal, and grass categories. Com r studies, comparing our method with SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Second row: our methods produce visual results that are r ser studies, comparing our method with SRGAN<ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet<ref type=\"bibr\" target=\"#b37\">[38]</ref>. Second row: our methods produce visual results that are r ref type=\"bibr\" target=\"#b2\">3]</ref> and adversarial loss <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> are introduced to solve the regression-to-the-mean problem  ur framework is based on adversarial learning, inspired by <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Specifically, it consists of one generator G \u03b8 and one dis d n=\"3.3.\">Loss Function</head><p>We draw inspiration from <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and apply perceptual loss and adversarial loss in our model leasing textures, outperforming previous GAN-based methods <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>.</p><p>Our work currently focuses on SR of outdoor scenes. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Towards video recommendation, Hamilton et al. <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref>, remote sensing <ref type=\"bibr\" target=\"#b4\">[Li et al., 2009]</ref>, surveillance monitoring <ref type=\"bibr\" target=\"#b0\">[Fang et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Park et al., 20 rized into three main types: PSNR-oriented, GANdriven and flow-based methods. PSNR-oriented methods <ref type=\"bibr\" target=\"#b0\">[Dong et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Lim et al., 201 ef><ref type=\"bibr\" target=\"#b12\">Zhang et al., 2018b;</ref><ref type=\"bibr\">Qiu et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">Guo et al., 2020]</ref> are trained with simple distribution assumptio mation loss. One groundbreaking solution to tackle the over-smoothing problem is GAN-driven methods <ref type=\"bibr\" target=\"#b0\">[Cheon et al., 2018;</ref><ref type=\"bibr\">Kim et al., 2019;</ref><ref [Ho et al., 2020]</ref> and speech synthesis <ref type=\"bibr\" target=\"#b2\">[Kong et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Chen et al., 2021]</ref> witness the power of diffusion models in gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: While previous learning simulation approaches <ref type=\"bibr\" target=\"#b17\">(Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Ummenhofer et al., 2020)</ref> have been highly specialized for parti lution 3D water scenario with randomized water position, initial velocity and volume, comparable to <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>'s containers of water. We used SPlisHS d boundary particles, a loss function that weights slow particles with few neighbors more heavily). <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> reported CConv outperformed DPI, so we  is to, during training, provide the model with its own predictions by rolling out short sequences. <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>, for example, train with two-step pred e=\"bibr\" target=\"#b14\">(Kipf &amp; Welling, 2016)</ref> work. The full CConv update as described in <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> is,</p><formula xml:id=\"formula_19\">f  e comparisons.</head><p>We implemented the CConv model, loss and training procedure as described by <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>. For simplicity, we only tested the CC  appended a particle type learned embedding to the input node features.</p><p>To be consistent with <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>, we used their batch size of 16, learn \"><head>D. Supplementary baseline comparisons D.1. Continuous convolution (CConv)</head><p>Recently <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> presented Continuous Convolution (CCon eral tasks.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Interpretation.</head><p>While <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> state that \"Unlike previous approaches. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y one utterance of an unseen speaker, we propose a deep discriminative speaker encoder. Inspired by <ref type=\"bibr\" target=\"#b27\">[28]</ref>, first residual network and squeezeand-excitation network  atial translation invariance, which make convolution layer suitable to extract frame level features <ref type=\"bibr\" target=\"#b27\">[28]</ref>. SE block expands the temporal context of the frame level  nel interdependence in features, which has been verified to be helpful in speaker verification task <ref type=\"bibr\" target=\"#b27\">[28]</ref>. The framework of SE block is shown in Figure <ref type=\"f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: linical notes <ref type=\"bibr\" target=\"#b0\">(Alsentzer et al., 2019)</ref>, human phenotype-gene RE <ref type=\"bibr\" target=\"#b26\">(Sousa et al., 2019)</ref> and clinical temporal RE <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uage understanding tasks for the existing VLP models is VQA. The SoTA result for VQA is from UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large model. Table <ref type=\"table\" target=\"#tab_3\">6</ref> Oscar B is the best among the models with equivalent size, even slightly better (0.04%) than UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large. And the Oscar L improves the SoTA overall accuracy wi other major task for the existing VLP models is NLVR2. Similarly, the SoTA model on NLVR2 is UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large. As reported in Table <ref type=\"table\" target=\"#tab_4 =\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b10\">10]</ref> employ BERT-like objectives <ref type=\"bibr\" target=\"#b6\">[6]</ref> to learn crossmodal representations from a concatenated-sequ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ly very few works have tried to integrate edge features into GNN architecture. Schlichtkrull et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> proposed an extension architecture of GCNs named R-GCNs. G e features in graph neural networks, and all of them have obvious limitations. Schlichtkrull et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> proposed R-GCNs to process modeling relational data. Howev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Ns) <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and U-Net <ref type=\"bibr\" target=\"#b17\">[18]</ref> algorithms have been used.</p><p>Although it is possible f er for upsampling.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. U-Net</head><p>U-Net <ref type=\"bibr\" target=\"#b17\">[18]</ref> is a modified FCN for yielding more precise segmentation.  formation such as boundaries of objects. To overcome this problem, skip connection methods are used <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The features e U-Net. It is important to note that the compared U-Net is not the original architecture proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref> but a highly calibrated model for the enhanced capability . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"figure\" target=\"#fig_1\">3</ref>. Second, when decoding an entity from scratch, we use beam search <ref type=\"bibr\" target=\"#b41\">[42]</ref> to search the token combinations with the highest probabil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b6\">Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020)</ref>  <ref type=\"bibr\" target=\"#b29\">(Wang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ian dropout <ref type=\"bibr\" target=\"#b24\">(Wang &amp; Manning, 2013)</ref> and variational dropout <ref type=\"bibr\" target=\"#b11\">(Kingma et al., 2015)</ref> use other random masks to improve dropout. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep learning models have been proved vulnerable against perturbations. Specifically, Szegedy et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Goodfellow et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref nd security of deep learning models, there has been a surge of interests in the adversarial attacks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t ost commonly used method. Gradients have been successfully used to perform attacks in other domains <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Since most of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: abuncu, 2018;</ref><ref type=\"bibr\" target=\"#b31\">Wang et al., 2019b)</ref>, noise filtering layers <ref type=\"bibr\" target=\"#b25\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Goldberg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tput mappings, potentially stochastic, with learnable parameters using directed acyclic graphs (see <ref type=\"bibr\" target=\"#b38\">Schulman et al. (2015)</ref> for a review). The state of each non-inp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: as designed to be representative of next-generation sharedmemory programs for chip-multiprocessors\" <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Our experiments show that for those programs, no matter how </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Benchmarks</head><p>We use PARSEC <ref type=\"bibr\" target=\"#b2\">[3]</ref> as the benchmark suite. It is a recently released suite desi  Because there is no close-form expression for the equation, the program uses numerical computation <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>The input data file of this benchmark includes an arr 6MB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : see<ref type=\"bibr\" target=\"#b2\">[3]</ref> for detail.</note></figure> <figure xmlns=\"http://www.tei-c. , as well as systems applications that mimic large-scale multithreaded commercial programs. Studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> have shown that the su  0 {0 2 4 6},{1 3 5 7},{0 2 4 6},{1 3 5 7} 1 161.6 0 {0 2 1 3},{4 5 6 7},{0 2 1 3},{4 5 6 7} 4 161. <ref type=\"bibr\" target=\"#b2\">3</ref> No binding 165.7</p><p>In PARSEC, ferret and dedup are two suc urement are relevant to this current work. Bienia and others <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> have shown a detailed exploration of the characterization of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b68\">69,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>.</p><p>We first consider the Cityscapes <ref type=\"bibr\" target=\"#b9\">[10]</ref> dataset, which consists of 5K highquality labeled images. W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mentation <ref type=\"bibr\" target=\"#b32\">[33]</ref>), and deep contextual language models from BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a v  GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref>, to contextual embeddings as the ones used in BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XL-Net <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The  tations based on the Transformer architecture <ref type=\"bibr\" target=\"#b36\">[37]</ref>, named BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The t r Siamese</figDesc><table /><note>XLNet-512 (most complex Transformer architecture). As suggested in<ref type=\"bibr\" target=\"#b14\">[15]</ref>, the Transformer training is performed with batch size b =. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e displacement attention (CADA) module and an RNN. The RNN component, implemented based on Con-vGRU <ref type=\"bibr\" target=\"#b0\">[1]</ref>, aims at long-term representation by learning to aggregate t \"#b38\">[40,</ref><ref type=\"bibr\" target=\"#b58\">60]</ref>.</p><p>Our approach is closely related to <ref type=\"bibr\" target=\"#b0\">[1]</ref> that proposes convolutional gated recurrent unit networks (C  frame to frame. Considering the importance of spatial information in 2D images/videos, the work of <ref type=\"bibr\" target=\"#b0\">[1]</ref> replaces linear product operation in GRU-RNN <ref type=\"bibr aSOT and TC-128 and performs favorably against many trackers on OTB-2015. adding RNN (i.e., ConvGRU <ref type=\"bibr\" target=\"#b0\">[1]</ref>) for temporal representation, the performance is improved to el>3</label><figDesc>Figure 3. Comparison between feature aggregation in existing RNN (e.g., ConvGRU<ref type=\"bibr\" target=\"#b0\">[1]</ref>) and our MA-RNN. Instead of directly aggregating features, w ect segmentation <ref type=\"bibr\" target=\"#b49\">[51]</ref>, video action classification/recognition <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, video object detect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: veral extensions of our own. Surprisingly, we find that a single-parameter variant of Platt scaling <ref type=\"bibr\" target=\"#b40\">(Platt et al., 1999</ref>) -which we refer to as temperature scaling  od P(D | S = s). This allows us to compute P(qte | pte , D) for any test input.</p><p>Platt scaling <ref type=\"bibr\" target=\"#b40\">(Platt et al., 1999)</ref> is a parametric approach to calibration, u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: user privacy leakage <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. At present, the problem of user privacy protection is rece bserved query behaviors. Some studies utilize anonymous user id or group id to mask user identities <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. But some users ma [23]</ref>, privacy protection in personalization receives widespread attentions.</p><p>Shen et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> defined four levels of privacy protection in personalized . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> and speech recognition <ref type=\"bibr\" target=\"#b11\">[12]</ref> to machine translation <ref type=\"bibr\" target=\"#b23\">[24]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by the offload engine, including hardware IP cores, embedded processors, and even embedded FPGAs <ref type=\"bibr\" target=\"#b68\">[74]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b65\">(Zoph et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Luo et al., 2018b)</ref>, SRM <ref type=\"bibr\" target=\"#b1\">(Baker et al., 2018)</ref>, MLP <ref type=\"bibr\" target=\"#b20\">(Liu et b57\">Yan et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Chu et al., 2019)</ref>, neural predictor <ref type=\"bibr\" target=\"#b1\">(Baker et al., 2018;</ref><ref type=\"bibr\" target=\"#b50\">Wen et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns=\"http://www.tei-c.org/ns/1.0\"><head>Class</head><p>Ontological Rule</p><p>Uncertain Extractions  <ref type=\"bibr\" target=\"#b0\">[Blum and Mitchell, 1998]</ref>, to combine the strengths of PSL-KGI a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b12\">(Gururangan et al., 2018)</ref> and passage-only baselines for reading comprehension <ref type=\"bibr\" target=\"#b19\">(Kaushik and Lipton, 2018)</ref>. We evaluate two ROUGE-L lower bound. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b49\">51]</ref>. We point interested readers to <ref type=\"bibr\" target=\"#b  this end, we evaluate our approach on two rotationrelated datasets: the rotated Modelnet40 dataset <ref type=\"bibr\" target=\"#b49\">[51]</ref> and the 3DMatch dataset <ref type=\"bibr\" target=\"#b52\">[54 eate the rotated ModelNet40 dataset based on the train/test split of the aligned ModelNet40 dataset <ref type=\"bibr\" target=\"#b49\">[51]</ref>. We mainly focus on a more challenging \"rotated\" setting w n and Retrieval. The classification and retrieval tasks on Modelnet40 follow evaluation metric from <ref type=\"bibr\" target=\"#b49\">[51]</ref>. In addition, our network is trained with GA pooling and p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> omit batch normalization (BN) <ref type=\"bibr\" target=\"#b36\">[37]</ref> to combat the sources of randomness that could potentially. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f attack scenarios <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, node features <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, or combinations o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref> to automatically designed neural architectures <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t es human experts to frequently try and evaluate numerous different operation and connection options <ref type=\"bibr\" target=\"#b3\">[4]</ref>. In contrast to architectures that are manually designed, th NAS-generated architectures have shown promising results in many domains, such as image recognition <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t rates a total search space of itive training procedure of each selected architecture can be avoided <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> so that researche. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ponding trailers instead of the full-length videos from Youtube 5 . We use the pre-trained ResNet50 <ref type=\"bibr\" target=\"#b15\">[16]</ref> models to extract the visual features from key frames extr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s for self-supervised have been proposed that do not work within the CID paradigm, including RotNet <ref type=\"bibr\" target=\"#b15\">(Gidaris et al., 2018)</ref>, Jigsaw <ref type=\"bibr\" target=\"#b22\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sing a high quality autoencoding DNN <ref type=\"bibr\" target=\"#b2\">(Bourlard and Kamp [1988]</ref>, <ref type=\"bibr\" target=\"#b20\">Wang et al. [2014]</ref>) which first compresses the given image into. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b20\">[21]</ref>. Recently, with the great impact of neural networks on computer vision <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and natural langua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the inference phase by adopting model-dependent techniques <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. For example, the o order-preserving transformations <ref type=\"bibr\" target=\"#b0\">[1]</ref> and the pruning techniques <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> have been adopted . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plementation would be computational prohibitive. Inspired by the core idea of separable convolution <ref type=\"bibr\" target=\"#b3\">[4]</ref>, we observe that the kernel h with a kernel size |P| \u00d7 |G| c r to that of the Inception module <ref type=\"bibr\" target=\"#b40\">[42]</ref> and its follow-up works <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which have shown the promising property of separable convol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) <ref type=\"bibr\" target=\"#b30\">(Park et al., 2018)</ref> is a most similar model to ours, which also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: CNN which is trained on pairs of artificially blurred images and their clear originals. Chen et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> proposed another method that predicts video frames of the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ercent of the branch mispredictions are due to indirect branches. In two programs, Virtutech Simics <ref type=\"bibr\" target=\"#b39\">[39]</ref> and Microsoft Excel 2003, almost half of the branch mispre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xploit the event schema knowledge, we propose to employ a trie-based constrained decoding algorithm <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b3\">Cao et al., 20 o form event records via constrained decoding <ref type=\"bibr\" target=\"#b3\">(Cao et al., 2021;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a)</ref>, which allows TEXT2EVENT to handle various e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Shi et al., 2019)</ref>. A typical GNN architecture <ref type=\"bibr\" target=\"#b27\">(Xu et al., 2018a)</ref> for the node classification task can be deco eural Networks. In general, for the node classification task, GNNs can be decomposed into two steps <ref type=\"bibr\" target=\"#b27\">(Xu et al., 2018a)</ref>: (1) neighborhood propagation and aggregatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b22\">(Xu et al. 2019c)</ref>, <ref type=\"bibr\">GAT (Velickovic et al. 2018)</ref>, MoNet <ref type=\"bibr\" target=\"#b11\">(Monti et al. 2017)</ref>, GraphSAGE (Hamilton, Ying, and Leskovec 20 2018)</ref> employs self-attention to calculate the coefficients of neighbors in aggregation; MoNet <ref type=\"bibr\" target=\"#b11\">(Monti et al. 2017</ref>) provides a unified generalization of graph . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  it has recently demonstrated promising results on certain tasks, specifically image classification <ref type=\"bibr\" target=\"#b18\">[19]</ref> and joint vision-language modeling <ref type=\"bibr\" target -purpose backbone for various vision tasks, in contrast to previous Transformer based architectures <ref type=\"bibr\" target=\"#b18\">[19]</ref> which produce feature maps of a single resolution and have .</p><p>Transformer based vision backbones Most related to our work is the Vision Transformer (ViT) <ref type=\"bibr\" target=\"#b18\">[19]</ref> and its follow-ups <ref type=\"bibr\" target=\"#b59\">[60,</re  architecture <ref type=\"bibr\" target=\"#b60\">[61]</ref> and its adaptation for image classification <ref type=\"bibr\" target=\"#b18\">[19]</ref> both conduct global selfattention, where the relationships n in Table <ref type=\"table\">4</ref>. Further adding absolute position embedding to the input as in <ref type=\"bibr\" target=\"#b18\">[19]</ref> drops performance slightly, thus it is not adopted in our  ar classifier. We find this strategy to be as accurate as using an additional class token as in ViT <ref type=\"bibr\" target=\"#b18\">[19]</ref> and DeiT <ref type=\"bibr\" target=\"#b59\">[60]</ref>. In eva get=\"#b52\">53]</ref>. In existing Transformer-based models <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, tokens are all of a fixed scale, a property unsuitable for  of image classification, object detection and semantic segmentation. It outperforms the ViT / DeiT <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref> and ResNe(X)t mode d to initialize a model for fine-tuning with a different window size through bi-cubic interpolation <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>.</p></div> <div xm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  MI alone, and the choice of encoder and MI estimators have a significant impact on the performance <ref type=\"bibr\" target=\"#b52\">(Tschannen et al., 2020)</ref>.</p><p>Figure <ref type=\"figure\">1</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  has been widely adopted in previous studies <ref type=\"bibr\" target=\"#b45\">(Wang et al. 2017;</ref><ref type=\"bibr\" target=\"#b31\">Poria et al. 2016</ref> Recently, there have been a wide range of res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: these new edges are then utilized as contextual information to compute its embedding. Previous work <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b8\">(Caccia et al., 2020)</ref>. So additionally, we report Self-BLEU-3,4,5 <ref type=\"bibr\" target=\"#b60\">(Zhu et al., 2018)</ref> to measure repetitions at a distributional l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r/item embeddings by mimicking the meta-learning setting via episode based training, as proposed in <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Specifically, we pick the users/items with sufficient int rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, which consists of metric-based recommendation <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  optimizing pipeline parallelism for synchronous training <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. This approach requires necessary gradients synchronizatio llelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m cropping, separating color channels, etc. <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref type=\"bibr\" target=\"#b47\">Tian et al., 2019)</ref>. Such bibr\" target=\"#b47\">(Tian et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b12\">c;</ref><ref type=\"bibr\" target=\"#b19\">He et al., 2020;</ref><ref typ his distribution by adopting a PU-learning viewpoint <ref type=\"bibr\">(Elkan &amp; Noto, 2008;</ref><ref type=\"bibr\" target=\"#b12\">Du Plessis et al., 2014;</ref><ref type=\"bibr\" target=\"#b9\">Chuang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a trainable and personalized relation scoring func  where GCNs can be used directly, while here we investigate GCNs for heterogeneous KGs. Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC o a user-personalized weighted graph that characterizes user's preferences. To this end, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero \u03bb is better than \u03bb = 0 (the case of Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref>), which justifies our claim that LS regularization can ass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, speech <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and body gestures <ref type=\"bibr\" target=\"#b4\">[5]</ref>. S ic emotion recognition, recurrent models, such as Long Short Term Memory networks (LSTM) are common <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. These networks of  <ref type=\"bibr\" target=\"#b55\">[56]</ref> and LSTM with Connectionist Temporal Modeling (LSTM-CTC) <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>Results: Table <ref type=\"table\" target=\"#tab_1\">II</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cation, because the attacks don't necessarily interfere with the program execution itself. Meltdown <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a classic example. Meltdown operates as a cache side-ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for word embedding has been shown to be an implicit factorization of a certain word-context matrix <ref type=\"bibr\" target=\"#b25\">[24]</ref>, and there is recent effort to theoretically explaining th ol(G) log \u0434 \u2212x \u22a4 i y j .</formula><p>Let us define z i, j = x \u22a4 i y j . Following Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref>, where the authors suggested that for a sufficient large e w i\u2212T , \u2022 \u2022 \u2022 , w i\u22121 , w i+1 , \u2022 \u2022 \u2022 , w i+T .</formula><p>Following the work by Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref>, SGNS is implicitly factorizing</p><formula xml:id=\"formul x is not only ill-defined (since log 0 = \u2212\u221e), but also dense. Inspired by the Shifted PPMI approach <ref type=\"bibr\" target=\"#b25\">[24]</ref>, we define M \u2032 such that M \u2032 i, j = max(M i, j , 1) (Line  </ref>. Recently, there has been effort in understanding this model. For example, Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref> prove that SGNS is actually conducting an implicit matrix   target=\"#b19\">[18]</ref> frame word embedding as a metric learning problem. Built upon the work in <ref type=\"bibr\" target=\"#b25\">[24]</ref>, we theoretically analyze popular skip-gram based network . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b6\">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation methods of <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b4\">Jozefowicz et  pled per training example):</p><p>\u2022 For any K 1, a binary classification variant of NCE, as used by <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b8\">Mikolov et al. n square error as the MLE) as K ! 1.</p><p>\u2022 We discuss application of our results to approaches of <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b8\">Mikolov et al. history x. This is the most straightforward extension of NCE to the conditional case; it is used by <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012)</ref>. It has the clear drawback however of intro o motivate the importance of the two algorithms, we now discuss their application in previous work. <ref type=\"bibr\" target=\"#b9\">Mnih and Teh (2012)</ref> consider language modeling, where x = w 1 w  n of the parameters c</p><p>x corresponding to normalization terms for each history. Interestingly, <ref type=\"bibr\" target=\"#b9\">Mnih and Teh (2012)</ref> acknowledge the difficulties in maintaining . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ]</ref>. Human social interactions have also been modelled using other knowledge-based perspectives <ref type=\"bibr\" target=\"#b54\">[57,</ref><ref type=\"bibr\" target=\"#b46\">49,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS <ref type=\"bibr\" target=\"#b8\">[8]</ref>). We take this idea further by proposing a stochastic multi- p>We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis <ref type=\"bibr\" target=\"#b8\">[8]</ref> and Graclus <ref type=\"bibr\" target=\"#b4\">[4]</ref> aim to c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: onally, existing LM+KG methods for reasoning <ref type=\"bibr\" target=\"#b19\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Wang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b12\">Feng et al.,  target=\"#b25\">(Mihaylov and Frank, 2018;</ref><ref type=\"bibr\" target=\"#b19\">Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Wang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b49\">Yang et al.,  s the facts corresponding to each question, prepared by <ref type=\"bibr\">Clark et al. (2019)</ref>  <ref type=\"bibr\" target=\"#b46\">(Wang et al., 2019a)</ref> 72.61( \u00b10.39) 68.59 (\u00b10.96) + KagNet <ref  17)</ref>, (2) RGCN <ref type=\"bibr\" target=\"#b36\">(Schlichtkrull et al., 2018)</ref>, (3) GconAttn <ref type=\"bibr\" target=\"#b46\">(Wang et al., 2019a)</ref>, ( <ref type=\"formula\" target=\"#formula_4\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tandard Enc-Dec model PointerGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> with attention mechanism <ref type=\"bibr\" target=\"#b19\">[18]</ref> and copy mechanism, and a pretrained language model BART <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aw distribution <ref type=\"bibr\" target=\"#b36\">[37]</ref>, i.e., <ref type=\"bibr\">3.</ref> Refer to <ref type=\"bibr\" target=\"#b35\">[36]</ref>, the time complexity of GCN is O(|E|d 2 ), and the computa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  i to see how well the model performs on that task. The goal of Model-Agnostic Meta-Learning (MAML) <ref type=\"bibr\" target=\"#b9\">[9]</ref> is to obtain a parameter initialization \u03b8 * that can adapt t nowledge across meta-training tasks and is the optimal parameter to adapt to unseen tasks quickly.  <ref type=\"bibr\" target=\"#b9\">[9]</ref> switches ProtoNet to MAML as the meta-learner. All experimen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: raining. However, in this case three-fold data augmentation was applied prior to feature extraction <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the acoustic features comprised 40-dimensional MFCCs (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: easure theoretical details, see <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> and <ref type=\"bibr\" target=\"#b1\">Daley and Vere-Jones (2008)</ref>.</p></div> <div xmlns=\"http://www.te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref type=\"bibr\" target=\"#b55\">[56]</ref>, MCPF <ref type=\"bibr\" target=\"#b56\">[57]</ref>, STRCF <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: echniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks <ref type=\"bibr\" target=\"#b24\">(Scarselli et al., 2009)</ref>, which we modify to use gated recurren ucing a sequence of outputs. Here, (1) is mostly achieved by previous work on Graph Neural Networks <ref type=\"bibr\" target=\"#b24\">(Scarselli et al., 2009)</ref>; we make several minor adaptations of   on graphs, including Graph Neural Networks <ref type=\"bibr\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009)</ref>, spectral networks <ref type=\"bibr\" tar ion, we review Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009)</ref> and introduce notation and concepts tha irected edge v \u2192 v , but we note that the framework can easily be adapted to undirected graphs; see <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref>. The node vector (or node representatio v = f * (l v , l CO(v) , l NBR(v) , h (t\u22121) NBR(v)</formula><p>). Several variants are discussed in <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> including positional graph forms, node- l graph forms, node-specific updates, and alternative representations of neighborhoods. Concretely, <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> suggest decomposing f * (\u2022) to be a sum unction g(h v , l v ) that maps to an output. This is generally a linear or neural network mapping. <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> focus on outputs that are independent p r\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b6\">Di Massa et al., 2006;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009;</ref><ref type=\"bibr\">Uwents et al., 2011)</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  by randomly selecting 32 examples for each task using a fixed random seed. Following previous work <ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al. d pronoun p and noun n, and the task is to determine whether p refers to n. We follow previous work <ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al.  boundary between two text segments.</p><p>WSC Unlike other SuperGLUE tasks, the WSC formulation of <ref type=\"bibr\" target=\"#b27\">Raffel et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b1\">Brown et ss multiple evaluations and perform greedy decoding as described in Section 3.</p><p>We then follow <ref type=\"bibr\" target=\"#b27\">Raffel et al. (2019)</ref> to map the output produced by the LM to a  g on FewGLUE. State-of-the-art results when using the regular, full size training sets for all tasks<ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2019)</ref> are shown in italics.</figDesc><table><ro generation with bidirectional context (e.g.,<ref type=\"bibr\" target=\"#b17\">Lewis et al., 2020;</ref><ref type=\"bibr\" target=\"#b27\">Raffel et al., 2019)</ref>, we stick with MLMs as they are more light. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hesizing it from scratch. It is most similar to recent work on sequence-tosequence voice conversion <ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" t #b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref>. <ref type=\"bibr\" target=\"#b15\">[16]</ref> uses a similar end-toend model, conditioned on speaker ide dard speech. In the future, we plan to test it on other speech disorders, and adopt techniques from <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> to preserve the sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the Open Graph Benchmark (OGB) has been introduced to provide a collection of larger graph datasets <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a)</ref>, but they are still small compared to graphs room for further improvement. All the OGB-LSC datasets are available through the OGB Python package <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a)</ref>. All the baseline and package code is availa  molecular feature developed by the chemistry community) can be obtained. By default, we follow OGB <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a)</ref> to convert the SMILES string into a molecula virtual node is shown to be effective across a wide range of graph-level prediction datasets in OGB <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a)</ref>. Edge features are incorporated following <r heir PubChem ID (CID) with ratio 80/10/10. Our original intention was to provide the scaffold split <ref type=\"bibr\" target=\"#b20\">(Hu et al., 2020a;</ref><ref type=\"bibr\" target=\"#b64\">Wu et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. <ref type=\"bibr\" target=\"#b35\">(Yun et al. 2019;</ref><ref type=\"bibr\" target=\"#b33\">Xu, Joshi, and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ord-powered algorithms such as fastText <ref type=\"bibr\" target=\"#b4\">[5]</ref>, Byte-Pair Encoding <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and WordPiece <ref type=\"bibr\" target=\"#b35\">[36]</ref> b i-c.org/ns/1.0\"><head n=\"3.1\">Anchor Selection</head><p>Subword tokenization algorithms such as BPE <ref type=\"bibr\" target=\"#b36\">[37]</ref> employ deterministic strategies to create tokens and const. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . . . , 0.003] (6)</p><p>Feature-space Maximum Likelihood Linear Regression (FMLLR) was explored in <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> for speaker ada. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l language understanding and generation tasks <ref type=\"bibr\" target=\"#b9\">(Dai et al., 2019;</ref><ref type=\"bibr\" target=\"#b38\">Shaw et al., 2018)</ref>. The proposed Disentangled Attention mechani tent, and position-to-position<ref type=\"foot\" target=\"#foot_0\">1</ref> .</p><p>Existing approaches <ref type=\"bibr\" target=\"#b38\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Huang et al., LEMENTATION</head><p>For an input sequence of length N , it requires a space complexity of OpN 2 dq <ref type=\"bibr\" target=\"#b38\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Huang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have distilled know >[20]</ref> proposes \"hint regression\" that matches the intermediate representations. Subsequently, <ref type=\"bibr\" target=\"#b27\">[28]</ref> matches the gram matrices of the representations, <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nipulate the input image. Some recent efforts in developing physical-world attacks are addressed in <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" targ rial examples of small perturbations can be easily mitigated in complex physical-world environments <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ b24\">[26]</ref>, or camouflaging the adversarial patch into specific shape (e.g. eye-glasses frames <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, etc.). Due to vario re various adaptations over a distribution of transformations to adapt to physical-world conditions <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" targ ect to impose them <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b24\">26,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Besides the challeng physical-world attacks <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b19\">21]</ref>. Also, these attacks require attacker physically pasting th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: UNI, we first collect all the triples which belong to the same 41 relations with LAMA from Wikidata <ref type=\"bibr\" target=\"#b42\">(Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014)</ref>, then we randomly sample 50K tri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  vision <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref type=\"bibr\" target=\"#b12\">Cheng et al., 2018)</ref>, and ASR <ref type=\"bibr\" target=\"#b45\">(Sp 6)</ref> presented a general method to stabilize model predictions against small input distortions. <ref type=\"bibr\" target=\"#b12\">Cheng et al. (2018)</ref> continued their work and developed the adve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he language model will be sent into a token-level softmax classifier. We use the BIO tagging scheme <ref type=\"bibr\" target=\"#b18\">(Ramshaw and Marcus, 1995)</ref> and output the tag with the maximum . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> are hampered by typically lower input data resolution than v data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> uses PointNets <ref type=\"bibr\" target=\"#b6\">[7]</ref> in an  The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, PointRCNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and PV-RC is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> publish their training configurations for class pedestrian a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  approach addresses these problems through the use of a number of locality-sensitive hash functions <ref type=\"bibr\" target=\"#b8\">[9]</ref> which hash each incoming tuple into a fixed number of bucket. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eep learning entirely infeasible.</p><p>Applying RMD to hyperparameter optimization was proposed by <ref type=\"bibr\" target=\"#b3\">Bengio (2000)</ref> and <ref type=\"bibr\" target=\"#b2\">Baydin &amp; Pea , Eigenmann &amp; Nossek (1999)</ref>, <ref type=\"bibr\" target=\"#b9\">Chen &amp; Hagan (1999)</ref>, <ref type=\"bibr\" target=\"#b3\">Bengio (2000)</ref>, <ref type=\"bibr\" target=\"#b0\">Abdel-Gawad &amp; R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b31\">(Weinberger &amp; Saul, 2009)</ref>. In this paradigm, selecting the hardest <ref type=\"bibr\" target=\"#b5\">(Bucher et al., 2016)</ref> or harder <ref type=\"bibr\" target=\"#b23\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: position encoding from the perspective with Neural ODE <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and <ref type=\"bibr\" target=\"#b21\">[22]</ref> has proposed to model the position information in complex . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on (DG) literature <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\">31, act domain-invariant features over multiple source domains <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar  so that irrelevant features vary across different domains while relevant features remain invariant <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b14\">Nair95b]</ref> use predictors similar to a \"two-level\" adaptive branch predictor <ref type=\"bibr\" target=\"#b21\">[Yeh91]</ref>. Then, we demonstrate that these \"twolevel like\" predic  level and have one or more tables of 2-bit counters (pattern history tables) in their second level <ref type=\"bibr\" target=\"#b21\">[Yeh91]</ref>. The contents of the first level shift-registers are ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ <ref type=\"bibr\" target=\"#b1\">(Budzianowski et al. 2018)</ref>, M2M <ref type=\"bibr\" target=\"#b15\">(  Asri et al. 2017)</ref>, M2M <ref type=\"bibr\" target=\"#b15\">(Shah et al. 2018</ref>) and Multi-WOZ <ref type=\"bibr\" target=\"#b1\">(Budzianowski et al. 2018</ref>). These datasets have utilized a varie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: parameters using 600\u21e5600 images for training. In this paper, we focus on the ResNet-50 architecture <ref type=\"bibr\" target=\"#b10\">[11]</ref> due to its good accuracy/cost tradeoff (25.6M parameters)  -of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 <ref type=\"bibr\" target=\"#b10\">[11]</ref>. For larger experiments, we use PNASNet-5-Large <ref type=  although this means that sev- eral forward passes are required to classify one image. For example, <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta  Another performanceboosting strategy is to classify an image by feeding it at multiple resolutions <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n while recognizing the emotion of the current state by utilizing the dueling deep-Q-network (DDQN) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The structure of DDQN in this RL module is depicted in Fi to compute the loss function Loss(t) as below. q eval (s(t + 1), a(t + 1)) = Q (s(t + 1), a(t + 1)) <ref type=\"bibr\" target=\"#b11\">(12)</ref> q ex pect (s(t + 1), a(t + 1)) = R + \u03b3 max a(t +1) q eval . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ine recommendations, researchers proposed to introduce privileged distillation into recommendations <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Selective Distillat ibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Selective Distillation Network <ref type=\"bibr\" target=\"#b5\">[6]</ref> was proposed to use a review process framework as the teache. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ]</ref> , Marginalized Graph Kernels <ref type=\"bibr\" target=\"#b5\">[6]</ref> , Graph Hopper Kernels <ref type=\"bibr\" target=\"#b24\">[25]</ref> , Deep Graph Kernels <ref type=\"bibr\" target=\"#b25\">[26]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b13\">[Gilmer et al., 2017</ref><ref type=\"bibr\" target=\"#b5\">, Battaglia et al., 2018</ref><ref type=\"bibr\" target=\"#b53\">, Yang et al., 2019]</ref> computes node embeddings h v \u2208 R d , \u2200v \u2208 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: labeling errors. This problem has even drastically affected widely used benchmarks, such as CoNLL03 <ref type=\"bibr\" target=\"#b27\">(Tjong Kim Sang, 2002)</ref> and TA-CRED <ref type=\"bibr\" target=\"#b4 e conducted based on TA-CRED <ref type=\"bibr\" target=\"#b42\">(Zhang et al., 2017b)</ref> and CoNLL03 <ref type=\"bibr\" target=\"#b27\">(Tjong Kim Sang, 2002)</ref>. TACRED is a crowdsourced dataset for re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b18\">[16]</ref> or efficient design spaces <ref type=\"bibr\" target=\"#b23\">[21,</ref><ref type=\"bibr\" target=\"#b24\">22]</ref> can still pro- compound scaling (dwr), in which the width,  he state-of-the-art but faster. As a concrete example, we apply fast scaling to scale a RegNetY-4GF <ref type=\"bibr\" target=\"#b24\">[22]</ref> model to 16GF (gigaflops), and find it uses less memory an e tuned to a specific setting (e.g., dataset or flop regime). As an alternative, Radosavovic et al. <ref type=\"bibr\" target=\"#b24\">[22]</ref> recently introduced the idea of designing design spaces, a t; w and round w to be divisible by g otherwise (w will change by at most 1/3 under such a strategy <ref type=\"bibr\" target=\"#b24\">[22]</ref>).</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head tegies on three networks families: EfficientNet <ref type=\"bibr\" target=\"#b33\">[31]</ref>, Reg-NetY <ref type=\"bibr\" target=\"#b24\">[22]</ref>, and RegNetZ (described below). We chose these models as t n larger models.</p><p>RegNets. As an alternative to neural architecture search, Radosavovic et al. <ref type=\"bibr\" target=\"#b24\">[22]</ref> introduced the idea of designing design spaces, where a de esign spaces, where a design space is a parameterized population of models. Using this methodology, <ref type=\"bibr\" target=\"#b24\">[22]</ref> designed a design space consisting of simple, regular netw rmined by a quantized linear function which has 4 parameters (d, w 0 , w a , w m ), for details see <ref type=\"bibr\" target=\"#b24\">[22]</ref>. Any other block parameters (like group width or bottlenec al 1\u00d71 conv. The 1\u00d71 convs can change w via the bottleneck ratio b, however, we set b = 1 following <ref type=\"bibr\" target=\"#b24\">[22]</ref>. BatchNorm <ref type=\"bibr\" target=\"#b16\">[14]</ref> and R lly it uses a Squeezeand-Excitation (SE) layer <ref type=\"bibr\" target=\"#b14\">[12]</ref>. Following <ref type=\"bibr\" target=\"#b24\">[22]</ref>, we set the bottleneck ratio b to 1 (effectively no bottle ck). A Reg-NetY model is thus fully specified with 5 parameters: d, w 0 , w a , w m , and g. Unlike <ref type=\"bibr\" target=\"#b24\">[22]</ref>, we additionally vary the image input resolution r (bringi ofthe-art results. This creates a tension between using a simple yet weak optimization setup (e.g., <ref type=\"bibr\" target=\"#b24\">[22]</ref>) versus a strong setup that yields good results but may be </ref> we report results for baseline RegNet models. We obtain these models via random search as in <ref type=\"bibr\" target=\"#b24\">[22]</ref>. 3 Note that there are two versions of the 4GF RegNets (us  random models in a given flop regime is typically sufficient to obtain accurate models as shown in <ref type=\"bibr\" target=\"#b24\">[22]</ref>. (Right) Models obtained with w scaling are much faster th nal 1\u00d71 conv. The 1\u00d71 convs can change w via the bottleneck ratio b, however, we set b = 1 following<ref type=\"bibr\" target=\"#b24\">[22]</ref>. BatchNorm<ref type=\"bibr\" target=\"#b16\">[14]</ref> and Re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the 2.5D technique for sparse matrices, both for square and rectangular grids. Koanantakool et al. <ref type=\"bibr\" target=\"#b37\">[37]</ref> observed that for sparse-dense MMM, 1.5D decomposition per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ole classification <ref type=\"bibr\" target=\"#b16\">[16,</ref><ref type=\"bibr\" target=\"#b35\">35,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> and graph classific. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head n=\"3.2.2.\">Attention</head><p>ESPnet uses a location-aware attention mechanism <ref type=\"bibr\" target=\"#b34\">[35]</ref>, as a default attention. A dot-product attention <ref type conditions (e.g., <ref type=\"bibr\" target=\"#b32\">[33]</ref> does not use any language models, while <ref type=\"bibr\" target=\"#b34\">[35]</ref> and <ref type=\"bibr\" target=\"#b10\">[11]</ref> use a word-b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ction. While the first three strategies have been taken into account in existing performance models <ref type=\"bibr\" target=\"#b10\">[10]</ref>- <ref type=\"bibr\" target=\"#b12\">[12]</ref>, to the best of  of developing the vacation time as the whole succession of switch-over and processing times (as in <ref type=\"bibr\" target=\"#b10\">[10]</ref>), <ref type=\"bibr\" target=\"#b12\">[12]</ref> keeps in the v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t in time. This is especially true for Long Short Term Memory (LSTM) networks-a popular type of RNN <ref type=\"bibr\" target=\"#b14\">[16]</ref>.</p><p>Recurrent neural networks are competitive or state- =\"formula_3\">h 0 h 1 h 2 h 3 h T x 1 x 2 x 3 x T</formula><p>Long Short Term Memory (LSTM) networks <ref type=\"bibr\" target=\"#b14\">[16]</ref> are a more complex variant of RNNs that often prove more p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  of GAS into a distributed training algorithm <ref type=\"bibr\" target=\"#b24\">(Ma et al., 2019;</ref><ref type=\"bibr\" target=\"#b52\">Zhu et al., 2016;</ref><ref type=\"bibr\" target=\"#b37\">Tripathy et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type=\"bibr\" target=\"#b25\">(Peters et al., 2018)</ref>, GPT-2 <ref type=\"bibr\" target=\"#b26\">(Ra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hallow word embedding popularized with word2vec <ref type=\"bibr\" target=\"#b22\">[23]</ref> and GloVe <ref type=\"bibr\" target=\"#b27\">[28]</ref> that learned a vocabulary of 400K-2M most frequent words, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[2]</ref>, phase-based sampling <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Of these techniques, the sampling based approaches typicall 8\">[9]</ref> extended SimPoint to provide statistical confidence measures.</p><p>Wunderlich, et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> developed the SMARTS framework, which applies statistical sa wn techniques for inferring statistics about a population given a sample of that population. SMARTS <ref type=\"bibr\" target=\"#b3\">[4]</ref> demonstrated that systematic sampling can be used to approxi mpared LiveSim with no sampling simulation and with a sampling mode that was very similar to SMARTS <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= f type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: asets</head><p>The effectiveness of MACU-Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and Gaofen Image D. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, contrastive learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and it was used in <ref type=\"bibr\" target=\"#b1\">[2]</ref>  nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=\"bibr\" target=\"#b0\">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=\"bibr\" target=\"#b0\">(1)</ref>, and so now we generate results by performing a second pass . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: fline model lack fine-grained estimation and customized models are not general as desired. Timeloop <ref type=\"bibr\" target=\"#b21\">[21]</ref> and Eyeriss <ref type=\"bibr\" target=\"#b22\">[22]</ref> use . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: network representation learning methods have been proposed to learn node representations in a graph <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bib  NetMF <ref type=\"bibr\" target=\"#b29\">[30]</ref>, (2) shallow embedding approaches such as DeepWalk <ref type=\"bibr\" target=\"#b28\">[29]</ref>, LINE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, HARP <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oding. They have been utilized to perform multi-purpose text generation with a single unified model <ref type=\"bibr\" target=\"#b36\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al bility present in large pretrained models <ref type=\"bibr\" target=\"#b28\">(McCann et al., 2018;</ref><ref type=\"bibr\" target=\"#b36\">Radford et al., 2019;</ref><ref type=\"bibr\">Keskar et al., 2019;</ref ng with the vanilla BART model, we also include the zero-shot performance from GPT2 language models <ref type=\"bibr\" target=\"#b36\">(Radford et al., 2019)</ref> (without fine-tuning) as a reference poi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. The recognition process has become a necessary part of man aved first, then the recognition tasks are conducted on text <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Given the plain text of an academic homepage, we aim to re ding, i.e., S \u2208 R n\u00d7d e . Following state-of-the-art methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, we use GloVe <ref type=\"bibr\" target=\"#b18\">[19]</ref> to  ods have been developed to address these problems. The state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> uses a Bi-LSTM-CRF based model to learn the page-level and l language processing methods. For example, state-of-the-art techniques for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> and for person names recognition <ref type=\"bibr\" target=\" ifferent methods to capture the position patterns. The state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> trains webpage-level and line-level models together to cap nd Preprocessing. We use the same datasets used by the state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> and person name recognition <ref type=\"bibr\" target=\"#b0\"> ref type=\"table\" target=\"#tab_0\">2</ref> summarises the dataset statistics.</p><p>\u2022 HomePub dataset <ref type=\"bibr\" target=\"#b30\">[31]</ref> contains the plain text of 2,087 homepages from different  formation about the position patterns and person names. PAM also outperforms the hierarchical PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> model, which can capture the positional diversity, by 3.64 a manual inspection of recognition results of state-ofthe-art models for the two tasks (i.e., PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> and CogNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>) and our ublications and 5,542 person names.</p><p>For publication string recognition, we observe that PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> misrecognises strings about patents, grants, and research . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  entries and OS changes.</p><p>Cache bypassing technique <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> selectively inserts data blocks in the cache. Because many. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs M which can be clustered among all modalitycomplete samples using K-means (MacQueen 1967) or PCA <ref type=\"bibr\" target=\"#b28\">(Pearson 1901)</ref>. Specifically, instead of generating \u03c9 = f \u03c6c (x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: approaches which select samples based on the query policy design. So far, various uncertainty-based <ref type=\"bibr\" target=\"#b31\">(Scheffer et al., 2001;</ref><ref type=\"bibr\" target=\"#b7\">Culotta an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: earning is motivated by the self-training algorithm <ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al. model parameters, in a way similar to self-training <ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  genera-tive model based on a Laplacian pyramid framework (LAP-GAN) to generate realistic images in <ref type=\"bibr\" target=\"#b8\">[6]</ref>, which is the most related to our work. However, the propose. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: category loss term. To enable distribution q \u03d5 c (z|x q c ) differentiable, we follow previous work <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d in label propagation methods represents structure-based prior, and has been shown to be underused <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> in graph convoluti N by adding regularizations <ref type=\"bibr\" target=\"#b29\">[30]</ref> or manipulating graph filters <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Their experimenta .7% in terms of classification accuracy. It is worth noting that we also apply our framework on GLP <ref type=\"bibr\" target=\"#b13\">[14]</ref> which unified GCN and label propagation by manipulating gr al prediction mechanisms, i.e., label propagation. For example, Generalized Label Propagation (GLP) <ref type=\"bibr\" target=\"#b13\">[14]</ref> modified graph convolutional filters to generate smooth fe ng to avoid oversmoothing of GCN model.</p><p>Here we use 16 layers GCNII as a teacher.</p><p>\u2022 GLP <ref type=\"bibr\" target=\"#b13\">[14]</ref> is a label-efficient model which combines label propagatio e effectiveness of our framework. \u2022 Note that the teacher model Generalized Label Propagation (GLP) <ref type=\"bibr\" target=\"#b13\">[14]</ref> has already incorporated the label propagation mechanism i \"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracies with teacher model as GLP<ref type=\"bibr\" target=\"#b13\">[14]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols , 0.6 as dropout probability, 256 as batch size and 0.1/0.0005 as learning rate decays.</p><p>\u2022 GLP <ref type=\"bibr\" target=\"#b13\">[14]</ref>: we use 16 as hidden-layer size, 0.01 as learning rate, 0.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dominant and thus provide more relevant context. This observation is also confirmed by recent works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> in the context of bina. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tential energy, docking poses <ref type=\"bibr\" target=\"#b34\">[McGann, 2011]</ref>, shape similarity <ref type=\"bibr\" target=\"#b30\">[Kumar and Zhang, 2018]</ref>, pharmacophore searching <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dversarial samples <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>; adversaries subtly alter legitimate inputs (call input per g on previous work <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> describing how adversaries can efficiently select perturbat f previous attacks <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> for knowledge of the target architecture and parameters. We ike neural networks<ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. In addition, we introduce new techniques to craft adversar ep neural network (DNN) using crafted inputs and output labels generated by the target \"victim\" DNN <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Thereafter, the local network was used to generate advers d in <ref type=\"bibr\" target=\"#b11\">[12]</ref> or the Jacobian-based iterative approach proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We only provide here a brief description of the fast grad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  have a longer access time, and this may increase the critical path length and penalize performance <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>In this paper we propose a novel register renaming ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ral architectures, such as DRMM <ref type=\"bibr\" target=\"#b8\">(Guo et al., 2016)</ref> and Co-PACRR <ref type=\"bibr\" target=\"#b13\">(Hui et al., 2018)</ref>, adopt an interaction-based design. They ope. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tes the feature representation in each position by weighted sum the features of all other positions <ref type=\"bibr\" target=\"#b14\">[15]</ref> <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Thus, it can mo , it can model the longrange context information for semantic segmentation task. For example, DANet <ref type=\"bibr\" target=\"#b14\">[15]</ref> uses two self-attention mechanisms to model long-range con. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: iven molecular representation learning and its applications, including chemical property prediction <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar turally encode the structure information, have been introduced to molecular representation learning <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar arning, especially deep learning driven by neural networks <ref type=\"bibr\" target=\"#b80\">[81,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. In conventional che. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odels for each semantic category on exemplar-based methods <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and show that SR results can be improved by semantic priors ately for each semantic category on exemplar-based methods <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. In contrast to these studies, we explore categorical prior et=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and random forest <ref type=\"bibr\" target=\"#b38\">[39]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eature interactions from raw data automatically. A popular approach is factorization machines (FMs) <ref type=\"bibr\" target=\"#b26\">[27]</ref>, which embeds features into a latent space and models the  underlying structure, FM may not be expressive enough. Although higher-order FMs have been proposed <ref type=\"bibr\" target=\"#b26\">[27]</ref>, they still belong to the family of linear models and are  ing the second-order factorized interactions between features. By specifying input features, Rendle <ref type=\"bibr\" target=\"#b26\">[27]</ref> showed that FM can mimic many speci c factorization models  value, rather than simply an embedding table lookup, so as to account for the real valued features <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>Bi-Interaction Layer. We then feed the embedding se nsorFlow implementation<ref type=\"foot\" target=\"#foot_7\">7</ref> of higherorder FM, as described in <ref type=\"bibr\" target=\"#b26\">[27]</ref>. We experimented with order size 3, since the MovieLens da n Machines</head><p>Factorization machines are originally proposed for collaborative recommendation <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Given a real valu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ediction accuracy. Similar attention mechanisms have been proposed for natural image classification <ref type=\"bibr\" target=\"#b10\">[11]</ref> and captioning <ref type=\"bibr\" target=\"#b0\">[1]</ref> to  ntributions of this work can be summarised as follows: \u2022 We take the attention approach proposed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> a step further by proposing grid-based gating that allows   intermediate space. In image captioning <ref type=\"bibr\" target=\"#b0\">[1]</ref> and classification <ref type=\"bibr\" target=\"#b10\">[11]</ref> tasks, the  softmax activation function is used to normali n. This results experimentally in better training convergence for the AG parameters. In contrast to <ref type=\"bibr\" target=\"#b10\">[11]</ref> we propose a grid-attention technique. In this case, gatin  the feature-maps and map them to lower dimensional space for the gating operation. As suggested in <ref type=\"bibr\" target=\"#b10\">[11]</ref>, low-level feature-maps, i.e. the first skip connections,  <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" ta [2,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and more recently applied to image classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. In <ref type=\"bib  was the top-performer in the ILSVRC 2017 image classification challenge. Self-attention techniques <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> have been proposed tention is used in <ref type=\"bibr\" target=\"#b32\">[33]</ref> to capture long range dependencies. In <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> self-attention is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing performance for clustering, SSL, active learning, etc. <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. A simple yet effective way to overcome the limitations is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, they still belong to the family of linear models and are claimed to be di cult to estimate <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Moreover, they are known to have only marginal improvemen ag recommendation. With one hidden layer only, our NFM signi cantly outperforms FM (the ocial LibFM <ref type=\"bibr\" target=\"#b27\">[28]</ref> implementation) with a 7.3% improvement. Compared to the s te) linear models. In other words, the predicted target \u02c6 (x) is linear w.r.t. each model parameter <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Formally, for each model parameter \u03b8 \u2208 {w 0 , {w i }, { i itive embedding-based models that are speci cally designed for sparse data prediction:</p><p>-LibFM <ref type=\"bibr\" target=\"#b27\">[28]</ref>. is is the o cial implementation <ref type=\"foot\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[33]</ref>, Insertion Transformer <ref type=\"bibr\" target=\"#b37\">[37]</ref>, and Tree Transformer <ref type=\"bibr\" target=\"#b40\">[40]</ref>, to be studied for this task in the future, since the mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  itself.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. Benchmarks</head><p>DPDK. DPDK <ref type=\"bibr\" target=\"#b37\">[38]</ref> is a popular networking application development library fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ble to DT-ORS-Net applications. To this end, the two-player Markov stopping game (MSG) developed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> can serve as a good starting point, which extends the clas e theoretic setting so as to handle the potential conflicts between the two players.</p><p>However, <ref type=\"bibr\" target=\"#b16\">[17]</ref> does not provide a systematic method to deal with a genera  the optimal strategy for each player in such situations, the two-player MSG framework developed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> may serve as a basis. Particularly, in the two-player MSG, yer. However, a systematic method for handling a general number of players in a MSG is missing from <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Considering this, a general M-MSG is proposed in the next  needed. Particularly, when two players coexist in the MSG, the randomized stopping time is used in <ref type=\"bibr\" target=\"#b16\">[17]</ref> to deal with the potential competition from the other play ynamism in the number of players as the game evolving, the concept of selection time is proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, constructing the selection time essentially requ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: plemented in various ways: lightweight userlevel threading <ref type=\"bibr\" target=\"#b19\">[23,</ref><ref type=\"bibr\" target=\"#b35\">39]</ref>, closures or coroutines <ref type=\"bibr\" target=\"#b1\">[4,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e paradigm as well <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Prior work on unsu br\" target=\"#b10\">[11]</ref> and (2) deep learning methods including Graph Autoencoders (GAE, VGAE) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b43\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lection can be modeled as a hierarchical graph, in which a document is regarded as a graph-of-words <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and then a set of documents are interconnected via the ci. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b40\">42,</ref><ref type=\"bibr\" target=\"#b53\">55,</ref><ref type=\"bibr\" target=\"#b55\">57,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>. Despite considerable advancement, most existing trackers f 8,</ref><ref type=\"bibr\">54]</ref> and deeper architecture <ref type=\"bibr\" target=\"#b30\">[32,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>.</p><p>Our work is related to but different from <ref type= ype=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\">54,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\" target=\"#b61\">63]</ref>) or IoUNet (e.g., <  <ref type=\"bibr\" target=\"#b30\">[32]</ref>, C-RPN <ref type=\"bibr\" target=\"#b16\">[18]</ref>, SiamDW <ref type=\"bibr\" target=\"#b60\">[62]</ref>, MDNet <ref type=\"bibr\" target=\"#b40\">[42]</ref>, SiamFC <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al model which adopts hierarchical attention to model the multi-view graph for fraud detection. GAS <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a GCN-based large-scale anti-spam method for detecting . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e with maximum-likelihood training, motivating objectives that operate on model-generated sequences <ref type=\"bibr\" target=\"#b3\">(Daum\u00e9 et al., 2009;</ref><ref type=\"bibr\" target=\"#b21\">Ross et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: by all the modalities.</p><p>Inspired by such an assumption and the fact that the Total Correlation <ref type=\"bibr\" target=\"#b27\">[29]</ref> can measure the amount of information shared by M (M \u2265 2)  sifiers of each modality.</p><p>Total Correlation/Mutual information maximization Total Correlation <ref type=\"bibr\" target=\"#b27\">[29]</ref>, as an extension of Mutual Information, measures the amoun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: forcement learning <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> or evolution algorithms <ref type=\"bibr\" target=\"#b30\">[31,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> or external knowledge graphs (KGs) <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to compensate the  ef type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> and knowledge graphs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> to enhance the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[26]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>, <ref type=\"bibr\" target=\"#b60\">[61]</ref>, <ref type=\"bibr\" target=\"#b62\">[63]</ref>, <ref type=\"bibr\" target=\"#b71\">[72]</ref> and cloud appli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: formation, which is not sufficient for modeling deep interactions between the two directions. GPT-2 <ref type=\"bibr\" target=\"#b32\">(Radford et al., 2019)</ref> proposes an autoregressive language mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ethods have been presented, relying on different techniques such as matching networks and bi-LSTM ( <ref type=\"bibr\" target=\"#b13\">[14]</ref>) or memory-networks ( <ref type=\"bibr\" target=\"#b9\">[10]</ use the recent memory-augmented neural network, to integrate and store the new examples. Similarly, <ref type=\"bibr\" target=\"#b13\">[14]</ref> propose to rely on external memories for neural networks, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , and deep contextual language models from BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a vanilla and Siamese architecture <ref type=\"bibr\" tar contextual embeddings as the ones used in BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XL-Net <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The Transformer architecture allowed the efficient unsupe ype=\"bibr\" target=\"#b36\">[37]</ref>, named BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The two Transformer models are originally designed to sol  <ref type=\"bibr\" target=\"#b42\">[43]</ref> alone, XLNet uses additional Web corpora for pretraining <ref type=\"bibr\" target=\"#b40\">[41]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  pected is that BERT generally achieves slightly better results than XLNet. According to Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>, XLNet surpasses BERT on the related GLUE benchmark <ref t  may be attributed to two reasons, pretraining on different corpora, and smaller models compared to <ref type=\"bibr\" target=\"#b40\">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained 0\">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained models used by Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>. Furthermore, the published XLNet BASE model we considered ublished XLNet BASE model we considered is pretrained on different data than the one in Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>  <ref type=\"foot\" target=\"#foot_13\">15</ref> . In contrast. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: prohibitively expensive w.r.t. multiple metrics of cost and doesn't scale.</p><p>Once-For-All (OFA) <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref> proposed to address this challenge by decoupl nfeasible for an ever-growing need for multi-platform, multi-latency deployment. Once-For-All (OFA) <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref> proposed to reduce this cost by using weight- K i are lists denoting the width &amp; kernel sizes of each of these d i layers.</p><p>Once-For-All <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref> builds a family of networks N 1 , N 2 , . . . ence, the resulting number of possible networks is enormous, with O(10 19 ) models for m = 5 blocks <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n  complicates their simultaneous optimization and necessitates techniques like progressive shrinking <ref type=\"bibr\" target=\"#b4\">(Cai et al., 2020)</ref>, increasing training time. Further, extractin nd narrower support for diverse latency targets. We explore a middle ground between these works and <ref type=\"bibr\" target=\"#b4\">Cai et al. (2020)</ref> to build design spaces that are tractable yet  rms. For latency, we use a lookup-table based latency estimator for Samsung Note 10 CPU provided by <ref type=\"bibr\" target=\"#b4\">Cai et al. (2020)</ref>. For other hardware platforms -namely NVIDIA G on of accurate models with fewer sub-optimal models -i.e. a more accurate overall model population. <ref type=\"bibr\" target=\"#b4\">Cai et al. (2020)</ref> showed that OFA networks suffer a top-1 accura mlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_0\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b4\">Cai et al. (2020)</ref> proposed a \"progressive shrinking\" approach to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: h capability of the EA and the local search capability of the Bayesian optimization algorithm (BOA) <ref type=\"bibr\" target=\"#b37\">(Pelikan et al., 1999)</ref>.</p></div> <div xmlns=\"http://www.tei-c. he proposed SceneNet method. Moreover, a Bayesian network is proposed, which is inspired by the BOA <ref type=\"bibr\" target=\"#b37\">(Pelikan et al., 1999)</ref>.</p><p>The BOA is designed to optimize t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: llions, or even billions, of valid schedules with a wide range of performance and energy efficiency <ref type=\"bibr\" target=\"#b48\">[49]</ref>. Considering the vast range of DNN layer dimensions and ha r cost models <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b70\">[71]</ref>. However, navig </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Brute-force Approaches:</head><p>Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> Brute-force &amp; Random dMazeRunner <ref type=\"bibr\" targ  Buffer space <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b64\">[65]</ref>, <ref type=\"bib ial architecture. Listing 1 shows an example of a schedule. Here, we use a loop-nest representation <ref type=\"bibr\" target=\"#b48\">[49]</ref> to explicitly describe how the computation of a convolutio e the one with the best result for the target metric, and 2) the Timeloop Hybrid mapper in Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> that randomly selects a tiling factorization, prunes super. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eural methods, abstractive summarization received less attention than extractive summarization, but <ref type=\"bibr\" target=\"#b9\">Jing (2000)</ref> explored cutting unimportant parts of sentences to c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: finition of black-box access as query access <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al., 2017;</ref><ref type=\"bibr\" target=\"#b11\">Hayes &amp; Dan rediction API with small datasets like MNIST and successfully demonstrated an untargeted attack. As <ref type=\"bibr\" target=\"#b15\">Liu et al. (2017)</ref> demonstrated, it is more difficult to transfe ularly when attacking models trained on large datasets like ImageNet. Using ensemble-based methods, <ref type=\"bibr\" target=\"#b15\">Liu et al. (2017)</ref> overcame these limitations to attack the Clar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: xtractive question answering.</p><p>GLUE General Language Understanding Evaluation (GLUE) benchmark <ref type=\"bibr\" target=\"#b39\">(Wang et al., 2019)</ref> consists of two single-sentence classificat p>The summary of datasets used for the General Language Understanding Evaluation (GLUE) benchmark 4 <ref type=\"bibr\" target=\"#b39\">(Wang et al., 2019)</ref> is presented in Table <ref type=\"table\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b3\">4]</ref>, node clustering <ref type=\"bibr\" target=\"#b4\">[5]</ref>, recommender systems <ref type=\"bibr\" target=\"#b5\">[6]</ref> and drug discovery <ref type=\"bibr\" target=\"#b6\">[7]</ref>.< \"#b51\">[52]</ref>, friend ranking <ref type=\"bibr\" target=\"#b52\">[53]</ref> and item recommendation <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, which impact many e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch as image-to-image translation at a rudimentary level. This previously required custom approaches <ref type=\"bibr\" target=\"#b13\">(Isola et al., 2017)</ref>, rather emerging as a capability of a sing. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ndmarks. They employ a discriminator network to improve image quality. In another work, Egor et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed a style-based landmark-to-image conversion method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe target.</p><p>We demonstrate the GIB principle by applying it to the Graph Attention Networks (GAT) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, where we leverage the attention weights of GAT to sample th ning. In the next subsection, we will introduce two instantiations of GIB, which is inspired by GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=  can be applied to many GNN models. As an example, we apply it to the Graph Attention Network model <ref type=\"bibr\" target=\"#b4\">[5]</ref> and present GIB-Cat and GIB-Bern. Algorithm 1 illustrates th resentations will be sampled. Note that we may also use a mechanism similar to multi-head attention <ref type=\"bibr\" target=\"#b4\">[5]</ref>: We split Z(l\u22121)</p><p>X into different channels w.r.t. its  e GIB-Cat and GIB-Bern with baselines including GCN <ref type=\"bibr\" target=\"#b2\">[3]</ref> and GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the most relevant baseline as GIB-Cat and GIB-Bern are to i he standard transductive node classification setting and standard trainvalidation-test split as GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The summary statistics of the datasets and their splitting   and GIB-Bern follows Alg. 1 (and Alg. 2 and 3 for the respective neighbor-sampling). We follow GAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>'s default architecture, in which we use 8 attention heads, n rporate the attention mechanism to adaptively learn the correlation between a node and its neighbor <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Recent literature s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: G <ref type=\"bibr\" target=\"#b6\">[7]</ref>, ERR-IA <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and NRBP <ref type=\"bibr\" target=\"#b7\">[8]</ref>, which are official diversity evaluation metrics used in Web. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  group fairness measures such as statistical parity and equality of opportunity. On the other hand, <ref type=\"bibr\" target=\"#b38\">Zhu et al. [2019]</ref> aimed to make GNNs stable and robust to adver rget=\"#b2\">, Bose and Hamilton, 2019</ref><ref type=\"bibr\" target=\"#b28\">, Rahman et al., 2019</ref><ref type=\"bibr\" target=\"#b38\">, Zhu et al., 2019</ref><ref type=\"bibr\" target=\"#b37\">, Zhang and Zi e-aggregation <ref type=\"bibr\" target=\"#b13\">[Geisler et al., 2020]</ref>, and attention mechanisms <ref type=\"bibr\" target=\"#b38\">[Zhu et al., 2019]</ref> to defend GNNs against a variety of attacks  wo baseline methods: FairGCN <ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref> and RobustGCN <ref type=\"bibr\" target=\"#b38\">[Zhu et al., 2019]</ref>; all hyperparameters are set following the a te>, FairGCN<ref type=\"bibr\" target=\"#b7\">[Dai and Wang, 2021]</ref>) and stability (i.e., RobustGCN<ref type=\"bibr\" target=\"#b38\">[Zhu et al., 2019]</ref>) of GNNs. Shown is average performance acros. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: is nondifferentiable. Therefore, for S with model parameters \u03a6, we use the policy gradient based RL <ref type=\"bibr\" target=\"#b13\">[Sutton et al., 2000]</ref> to derive its gradient:</p><formula xml:i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: puter vision methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar xperiments, we follow the protocol used in previous papers <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, which uses unseen  #foot_0\">1</ref> , 10 2 ]), where the implementation of traditional classifiers becomes challenging <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>.</p><p>Arguably, t rix of pairwise distances of a subset of the training set (i.e., the mini-batch) allows Song et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> to design of a new loss function that integrates all posit > (with and without FANNG <ref type=\"bibr\" target=\"#b4\">[5]</ref>), (2) lifted structured embedding <ref type=\"bibr\" target=\"#b20\">[21]</ref>, (3) N-pairs metric loss <ref type=\"bibr\" target=\"#b19\">[2 hod, and ( <ref type=\"formula\">5</ref>) ), we use the same training and test set split described in <ref type=\"bibr\" target=\"#b20\">[21]</ref> across all datasets. Specifically, the means CUB200-2011 < target=\"#b22\">[23]</ref> weights and randomly initialize the final fully connected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>. We set the embedding size to 64 <ref type=\"bibr\" target=\" nnected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>. We set the embedding size to 64 <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the learning rate for the randomly initialized fully c omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>For the experiments using triplet combined with glo =\"bibr\" target=\"#b22\">[23]</ref> and randomly initialize the final fully connected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref> . The learning rate for the randomly initialized fully con omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y small-scale desktop applications.</p><p>Previously proposed indirect branch prediction techniques <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr equires only 2,048 bits but a 1,024-entry gsharelike indirect branch predictor (tagged target cache <ref type=\"bibr\" target=\"#b9\">[10]</ref>) needs at least 2,048 bytes along with additional tag stora hat the indirect branch will jump to the same target address it jumped to in its previous execution <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b33\">[33]</ref>. 3 To our knowle rget addresses of many indirect branches alternate rather than stay stable for long periods of time <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b33\">[33]</ref>.</p><p>4. Since  ow path leading to an indirect branch is strongly correlated with the target of the indirect branch <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This is very similar to modern conditional branch predicto predictor has low (about 50 percent) prediction accuracy <ref type=\"bibr\" target=\"#b37\">[37]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b33\">[33]</ref>.</p><p>Chang et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> first proposed to use branch history information to disting 5\">15</ref> compares the performance of VPC prediction with the tagged target cache (TTC) predictor <ref type=\"bibr\" target=\"#b9\">[10]</ref>. On average, VPC prediction provides performance improvemen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are masked auto-encoders, while in vision the recently popular choices are Siamese networks (e.g., <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" tar trastive self-supervised pre-training can outperform their supervised counterparts in certain tasks <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p><p>Contrastive   \"MoCo v3\" framework that facilitates our study. MoCo v3 is an incremental improvement of MoCo v1/2 <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, and we strike for calability. The pseudocode of MoCo v3 is in Alg. 1, described next.</p><p>As common practice (e.g., <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>), we take two crops robing accuracy. In this regime, the larger batch improves accuracy thanks to more negative samples <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. The curve of a 4k  g rate as lr\u00d7BatchSize/256, where lr is a \"base\" learning rate. lr is the hyper-parameter being set <ref type=\"bibr\" target=\"#b18\">[20,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" tar th masked auto-encoding, we study the frameworks that are based on Siamese networks, including MoCo <ref type=\"bibr\" target=\"#b18\">[20]</ref> and *: equal contribution. framework model params acc. (%) d by two encoders, f q and f k , with output vectors q and k. Intuitively, q behaves like a \"query\" <ref type=\"bibr\" target=\"#b18\">[20]</ref>, and the goal of learning is to retrieve the corresponding , in MoCo v3 we use the keys that naturally co-exist in the same batch. We abandon the memory queue <ref type=\"bibr\" target=\"#b18\">[20]</ref>, which we find has diminishing gain if the batch is suffic ckbone and projection head, but not the prediction head. f k is updated by the movingaverage of f q <ref type=\"bibr\" target=\"#b18\">[20]</ref>, excluding the prediction head.</p><p>As a reference, we e arget=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. The methodology is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: serving transformations <ref type=\"bibr\" target=\"#b0\">[1]</ref>, pruning and compression techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b46\">Wang et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Kolesnikov et al., 2020;</ref><ref type=\"bibr\" target=\"#b39\">Tian et al., 2020)</ref>. Alternatively, they may assume that model i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s not restricted to small-scale domain-specific information <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25]</ref>; and (ii) leverages the rich graphical structure of linked  mploys a large scale knowledge base with generic entities, concepts and relationships; some studies <ref type=\"bibr\" target=\"#b35\">[25,</ref><ref type=\"bibr\" target=\"#b80\">70]</ref> utilize small, dom 59\">49]</ref> and more recently, graph transformer networks <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25,</ref><ref type=\"bibr\" target=\"#b79\">69]</ref> have been proposed  ype=\"bibr\" target=\"#b17\">[7]</ref>: encodes the input knowledge graph via a GNN.</p><p>(2) KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref>: their graph transformer encoder and knowledge graph const  FALSE: jacques cl\u00e9ment succeeded the throne catholic and become king of house of bourbon. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: in 1692, jacques cl\u00e9ment, a catholic tribe historian, ve been described to treat in memory caused by discrimination or other forms of phonation. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A:dissociative identity disorder did suffer from mental  economic plan by Labor Chris Bowen is the most comprehensive plan by an opposition leader. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: Economic history of Australia shows that 15 pages bud ields the earth from the sun's ultraviolet radiation, healing positive effect of lockdown. KBLLH'19 <ref type=\"bibr\" target=\"#b35\">[25]</ref> N/A: The Arctic ozone depletion has nothing to do with the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roaches for representation learning of graphs <ref type=\"bibr\" target=\"#b23\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b21\">Kipf &amp arget=\"#b6\">Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b8\">Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b13\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b19\">Kearnes e erage via attention <ref type=\"bibr\" target=\"#b34\">(Velickovic et al., 2018)</ref> and LSTM pooling <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b24\">Murphy e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >43]</ref>. Hypergraph neural networks <ref type=\"bibr\" target=\"#b16\">[17]</ref> and their variants <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> use the clique exp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bination of bidirectional LSTM and CTC has been applied to characterlevel speech recognition before <ref type=\"bibr\" target=\"#b5\">(Eyben et al., 2009)</ref>, however the relatively shallow architectur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p model, we apply Layer Normalization for word embedding and pre-norm residual connection following <ref type=\"bibr\" target=\"#b34\">Wang et al. (2019a)</ref> for both encoder and decoder. Therefore, ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b9\">Du and Cardie, 2020;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 2020;</ref><ref type=\"bibr\" target=\"#b24\">Liu et al., 2020)</ref>.</p><p>Compared with previous methods, we mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Work</head><p>The primary amino acid sequences are confirmed to contain all the protein information <ref type=\"bibr\" target=\"#b0\">[Anfinsen, 1972]</ref> and are extremely easy to obtain. Thus, there i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\"   face landmarks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\"  loss besides the reconstruction loss to improve the temporal dependency across frames. Song et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> proposed another method that generates talking faces by usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng works has paid special attention to embed bipartite networks. While a recent work by Dong et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed metapath2vec++ for embedding heterogeneous networ ght be suboptimal for learning vertex representations for a bipartite network.</p><p>Metapath2vec++ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HNE <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EOE <re  We assign a probability to stop a random walk in each step. In contrast to DeepWalk and other work <ref type=\"bibr\" target=\"#b13\">[14]</ref> that apply a fixed length on the random walk, we allow the  hyper-parameters p and q are set to 0.5 which has empirically shown good results. \u2022 Metapath2vec++ <ref type=\"bibr\" target=\"#b13\">[14]</ref>: This is the state-of-the-art method for embedding heterog. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p>Existing literature on training with noisy labels focuses primarily on loss correction approaches <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015;</ref><ref type=\"bibr\" target=\"#b9\">Hendrycks et a ling using the network predictions to predict hard or soft labels.</p><p>Loss correction approaches <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015;</ref><ref type=\"bibr\" target=\"#b11\">Jiang et al., pe=\"bibr\" target=\"#b11\">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrapping loss <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>, which introduces a perceptual consistency  ilities used to compute it, to compensate for the incorrect guidance provided by the noisy samples. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> extend the loss with a perceptual term that is unsupervised model to implement a loss correction approach that benefits both from bootstrapping <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=\"bibr ibr\" target=\"#b33\">(Zhang et al., 2017)</ref>.</p><p>The static hard bootstrapping loss proposed in <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> provides a mechanism to deal with label noi ) ,<label>(10)</label></formula><p>where w i weights the model prediction z i in the loss function. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> use w i = 0.2, \u2200i. We refer to this approac Reed et al., 2015)</ref> use w i = 0.2, \u2200i. We refer to this approach as static hard bootstrapping. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> also proposed a static soft bootstrapping l beling. We also run our proposed approach under these conditions in Subsection 4.5 for comparison.  <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>. The overall results demonstrate that apply sed dynamic hard bootstrapping exhibits better performance than the state-of-the-art static version <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>. It is, however, not better than the perfor d the 300 epochs training scheme (see Subsection 4.1) . We introduce bootstrapping in epoch 105 for <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> for the proposed methods, estimate the T ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: consists of hardware and software components. Jenga hardware requires small changes over prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target  most of the heavy lifting to build virtual cache hierarchies. Specifically, Jenga builds on Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, which constructs sing systems with non-uniform SRAM banks, single-lookup NUCAs generally outperform directory-based NUCAs <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target ng just enough capacity to fit the working set at minimum latency and energy. In particular, Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> achieves this by letti rdware needs to be flexible and reconfigurable at low cost. We thus base Jenga's hardware on Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, which supports applic  hardware components, emphasizing differences from Jigsaw at the end of the section. See prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> for details of Jigsaw' g page reclassifications: Like Jigsaw and R-NUCA, Jenga uses a simple technique to map pages to VHs <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ vel hierarchy in a single curve, and thus can use the same partitioning algorithms as in prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> to allocate capacity ref type=\"bibr\" target=\"#b2\">[3]</ref>, R-NUCA <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Jigsaw <ref type=\"bibr\" target=\"#b6\">[7]</ref> do away with hierarchy entirely, adopting a single-lookup de type=\"bibr\" target=\"#b27\">28]</ref> and more aggressive than <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>). Table <ref type=\"ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: system through iterative backtranslation <ref type=\"bibr\" target=\"#b15\">(Lample et al., 2018b;</ref><ref type=\"bibr\" target=\"#b2\">Artetxe et al., 2018b)</ref>.</p><p>In this paper, we develop a more p  more suitable for this problem, and <ref type=\"bibr\" target=\"#b15\">Lample et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> adapted the same principles discussed abo back-translation <ref type=\"bibr\" target=\"#b25\">(Sennrich et al., 2016)</ref> which, in the case of <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>, is preceded by an unsupervised tuning st 1.0\"><head n=\"3.1\">Initial phrase-table</head><p>So as to build our initial phrase-table, we follow <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> and learn n-gram embeddings for each lang it, and taking the product of their respective translation probabilities. The reader is referred to <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> for more details.</p></div> <div xmlns=\"h  well with test performance. Unfortunately, neither of the existing unsupervised SMT systems do so: <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> use a heuristic that builds two initial m 4</ref> shows some translation examples from our proposed system in comparison to those reported by <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>. We choose the exact same sentences repor pe=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>. We choose the exact same sentences reported by <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>, which were randomly taken from newstest2 supervised machine translation can be a usable alternative in practical settings.</p><p>Compared to <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>, our translations are generally more flue at they are produced by an NMT system rather than an SMT system. In addition to that, the system of <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> has some adequacy issues when translating l information alone, yielding to translation errors like \"Sunday Telegraph\" \u2192 \"The Times of London\" <ref type=\"bibr\" target=\"#b2\">(Artetxe et al., 2018b)</ref>. So as to overcome this issue, we propos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: egress blendshapes of a 3D face using the combined audio-visual embedding from a deep network. VOCA <ref type=\"bibr\" target=\"#b11\">[12]</ref> pre-registers subject-specific 3D mesh models using FLAME . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and cannot achieve a compromise between them. Yu et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref> utilized semantic branch and detail branch to extract two  t we use GTX 1660Ti during the evaluation phase). For training, we adopt minibatch gradient descent <ref type=\"bibr\" target=\"#b41\">[42]</ref> with a batch size of 4. The Adam optimizer is selected to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: entation <ref type=\"bibr\" target=\"#b37\">(Wang et al., 2018)</ref> and multi-resolution segmentation <ref type=\"bibr\" target=\"#b0\">(Benz et al., 2004)</ref> have been widely used to generate image obje nitial segmentation results for HR images. MSEG is developed from the multi-resolution segmentation <ref type=\"bibr\" target=\"#b0\">(Benz et al., 2004)</ref>. It initially generates an oversegmentation  ough the segmentation algorithm used in this study is based on multi-resolution segmentation (MSEG) <ref type=\"bibr\" target=\"#b0\">(Benz et al. (2004)</ref>; <ref type=\"bibr\" target=\"#b36\">Tzotsos and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b18\">[22,</ref><ref type=\"bibr\" target=\"#b20\">24,</ref><ref type=\"bibr\" target=\"#b21\">25,</ref><ref type=\"bibr\" target=\"#b38\">42]</ref>. These are mainly (1) to ease the customizing and debugging. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: usted so that it can fit different models and datasets.</p><p>Inspired by the success of Focal Loss <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we estimate \ud835\udf14 (\ud835\udc62, \ud835\udc56) with a function of \ud835\udc53 ( \u0177\ud835\udc62\ud835\udc56 ) that ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#fig_0\">1</ref>, these are mining-based methods inspired by previous relation extraction methods <ref type=\"bibr\" target=\"#b34\">(Ravichandran and Hovy, 2002)</ref>, and paraphrasing-based methods t ased Generation</head><p>Our first method is inspired by template-based relation extraction methods <ref type=\"bibr\" target=\"#b34\">(Ravichandran and Hovy, 2002)</ref>, which are based on the observati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nal GNNs can operate on the transformed homogeneous graphs <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. This is a two-stage approach and requires hand-crafted met <head n=\"3.2\">Meta-Path Generation</head><p>Previous works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> require manually defined meta-paths and perform Graph Neura. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: GK), we also compare with four unsupervised graph-level representation learning methods as node2vec <ref type=\"bibr\" target=\"#b65\">[66]</ref>, sub2vec <ref type=\"bibr\" target=\"#b66\">[67]</ref>, graph2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion has been demonstrated to be effective in improving the generalization ability of linear models. <ref type=\"bibr\" target=\"#b28\">[29]</ref> proposes a sample weighting approach with the goal of deco f type=\"bibr\" target=\"#b51\">[53]</ref>.</p><p>Learning sample weights for decorrelation Inspired by <ref type=\"bibr\" target=\"#b28\">[29]</ref>, we propose to eliminate the dependence between features i een correlation and model stability under misspecification <ref type=\"bibr\" target=\"#b49\">[51,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, and propose to address such a problem via a sample reweigh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arded as belief based on indirect experience, while trust is belief derived from direct experiences <ref type=\"bibr\" target=\"#b23\">[24]</ref> . In accordance with real situations of service-oriented c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b16\">[17]</ref>.</p><p>A more direct approach to LTR metric optimization was proposed by Qin et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref>, where the rank variable in the definition of metrics like Recent hardware and software advances in the training of neural networks, however, make the work in <ref type=\"bibr\" target=\"#b13\">[14]</ref> relevant again and potentially allow us to harvest the eff ng state-of-the-art LTR algorithms such as LambdaMART. We give an overview of LTR and in particular <ref type=\"bibr\" target=\"#b13\">[14]</ref> in Section 2. We discuss experimental results in Section 3 \"#b14\">[15]</ref>, boosting <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and approximating the metric <ref type=\"bibr\" target=\"#b13\">[14]</ref>. It is the latter that can tightly bound any ranking metri \" target=\"#b13\">[14]</ref>. It is the latter that can tightly bound any ranking metric such as NDCG <ref type=\"bibr\" target=\"#b13\">[14]</ref> and can be easily optimized with gradient descent.</p><p>S adient descent.</p><p>Surprisingly, despite its attractive theoretical properties, the framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> has received little attention in LTR studies in the decade ts to optimize NDCG-referred to as ApproxNDCG. Our results show that  the theoretical guarantees in <ref type=\"bibr\" target=\"#b13\">[14]</ref> materialize in practice. Before we go any further, we give  the indicator which is 1 if s &lt; t and 0 otherwise. <ref type=\"bibr\">Qin et al.</ref> propose in <ref type=\"bibr\" target=\"#b13\">[14]</ref> a smooth approximation of Equation <ref type=\"formula\" tar ture of ranking utility functions.</p><p>In this work, we set out to revisit the work of Qin et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> which formulates a smooth approximation to any ranking met  any ranking metric such as NDCG. Unlike many other existing surrogate LTR losses, the framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> offers a way to directly optimize ranking metrics. Because g metrics rather than loosely related surrogate losses; and (b) that the approximation framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> could lay out the foundation of deep neural networks in LT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sticated model architectures to better understand the complex relationships between users and items <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. A large recommend re already ranked highly by the student. In this regard, unlike the motivation of the previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, items merely rank  by the teacher are already ranked highly by the student and vice versa. Thus, the existing methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> that simply choose perparameters, we use the values recommended from the public implementation and the original papers <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. For BD, \ud835\udf06 \ud835\udc47 \u2192\ud835\udc46 an e platforms.</p><p>To tackle this problem, a few recent work <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23</ref>] has adopted Knowled odels).</p><p>To tackle this challenge, a few recent methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23</ref>] have adopted knowle ned to give high scores on the top-ranked items of the teacher's recommendation list. Similarly, in <ref type=\"bibr\" target=\"#b15\">[16]</ref>, the student is trained to imitate the teacher's predictio \"bibr\" target=\"#b25\">[26]</ref> and Foursquare <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Following <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we hold out the last interaction of each user as test int tab_1\">1</ref>. We also report the experimental results on ML100K and AMusic, which are used for CD <ref type=\"bibr\" target=\"#b15\">[16]</ref>, in Appendix for the direct comparison.</p><p>5.1.2 Evalua  the student give high scores on top-ranked items by the teacher. \u2022 Collaborative Distillation (CD) <ref type=\"bibr\" target=\"#b15\">[16]</ref>: A state-of-the-art KD method for top-\ud835\udc3e RS. CD makes the s e different sampling schemes as follows: 1) Rank discrepancy-aware sampling, 2) Rank-aware sampling <ref type=\"bibr\" target=\"#b15\">[16]</ref>, 3) Top-\ud835\udc41 selection <ref type=\"bibr\" target=\"#b22\">[23]</r ted due to the restricted capability and thus the loss of recommendation performance is unavoidable <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Second, several methods try to accelerate the inference p tudent give high scores on the top-ranked items of the teacher's recommendation list. Similarly, CD <ref type=\"bibr\" target=\"#b15\">[16]</ref> makes the student imitate the teacher's prediction scores  foot\" n=\"3\" xml:id=\"foot_2\">We sample the items by using the rank-aware sampling scheme suggested in<ref type=\"bibr\" target=\"#b15\">[16]</ref>. Note that in<ref type=\"bibr\" target=\"#b15\">[16]</ref>, th g the rank-aware sampling scheme suggested in<ref type=\"bibr\" target=\"#b15\">[16]</ref>. Note that in<ref type=\"bibr\" target=\"#b15\">[16]</ref>, the sampled items are used for the distillation.</note> \t ><p>In this section, we report the experimental results on ML100K and AMusic, which are used for CD <ref type=\"bibr\" target=\"#b15\">[16]</ref>, for the direct comparison with CD. We do not include this nd the second last interacted item for validation as done in <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. If there is no timestamp in the dataset, we randomly take  re thorough evaluation compared to using random candidates <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>.</p><p>As we focus on top-\ud835\udc3e recommendation for implicit fee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion probabilistic modeling (DDPM) <ref type=\"bibr\" target=\"#b39\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Ho et al., 2020)</ref> trains a sequence of probabilistic models to r  at generation of images <ref type=\"bibr\">(Song &amp; Ermon, 2019;</ref><ref type=\"bibr\">2020;</ref><ref type=\"bibr\" target=\"#b17\">Ho et al., 2020)</ref>, audio <ref type=\"bibr\" target=\"#b6\">(Chen et  s of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> has reported higher sample quality than <ref t  Eq. ( <ref type=\"formula\" target=\"#formula_3\">3</ref>) described here is equivalent to L simple in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, but we re-write it in a slightly different fo rget=\"#b16\">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the same DDPM model in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we obtain better bits/dim compared to the upp e=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref>. With PC samplers and the same model architecture in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, the gaps can be reduced, but score-based mode  data pxq. In our experiments, we let \u03b2min \" 0.1 and \u03b2max \" 20, which correspond to the settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. The perturbation kernel is given by</p><formu arget=\"#fig_4\">4</ref>, we use a DDPM model trained on 256 \u02c6256 CelebA-HQ with the same settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. We use the RK45 ODE solver <ref type=\"bibr\" t on and temperature scaling. The model tested here is a DDPM model trained with the same settings in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns tially a different discretization to the same reverse-time SDE. This unifies the sampling method in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> as a numerical solver to the reverse-time VP S  \u00b4\u03c32 i\u00b41 qIq, i \" 1, 2, \u00a8\u00a8\u00a8, N.</formula><p>Here we assume \u03c3 0 \" 0 to simplify notations. Following <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we can compute</p><formula xml:id=\"formula_55  \" x i `p\u03c3 2 i \u00b4\u03c32 i\u00b41 qs \u03b8 px i , iq,</formula><p>where s \u03b8 px i , iq is to estimate z{\u03c3 i . As in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, we let \u03c4 i \"</p><formula xml:id=\"formula_58\"> DDITIONAL DETAILS ON PREDICTOR-CORRECTOR SAMPLERS</head><p>Training We use the same architecture in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for our score-based models. For the VE SDE, we  noise scales at test time. The specific architecture of the noise-conditional score-based model in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> uses sinusoidal positional embeddings for cond amples with TF-GAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We follow <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for optimization, including the learning rate, mula\" target=\"#formula_0\">1</ref>) and use a batch size of 128. Our architecture is mostly based on <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>. We additionally search over the following com pe=\"bibr\" target=\"#b20\">Karras et al. (2018)</ref>; <ref type=\"bibr\">Song &amp; Ermon (2019)</ref>; <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref>, the FID value here is the lowest over the cou odel upon conditioning on continuous time variables, we change positional embeddings, the layers in <ref type=\"bibr\" target=\"#b17\">Ho et al. (2020)</ref> for conditioning on discrete time steps, to ra al., 2018)</ref> 3.40 -Flow++ <ref type=\"bibr\" target=\"#b16\">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.70 13.51 DDPM (Lsimple) <ref type=\"bibr\"  > 3.29 -DDPM (L) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.70 13.51 DDPM (Lsimple) <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> \u010f 3.75 3.17   <ref type=\"bibr\">(Song &amp; Er )</ref> 25.32 8.87 \u02d8.12 NCSNv2 <ref type=\"bibr\">(Song &amp; Ermon, 2020)</ref> 10.87 8.40 \u02d8.07 DDPM <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> 3.17 9.46 \u02d8.11 Exact likelihood computation L s on CIFAR-10 is 10.23 <ref type=\"bibr\">(Song &amp; Ermon, 2020)</ref>, whereas for DDPM it is 3.17 <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref>. With PC samplers and the same model architec \"#formula_52\">41</ref>)) reverse diffusion samplers.</p><p>Note that the ancestral sampling of DDPM <ref type=\"bibr\" target=\"#b17\">(Ho et al., 2020)</ref> (Eq. ( <ref type=\"formula\" target=\"#formula_4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng process. Some pioneering works based on Lasso framework <ref type=\"bibr\" target=\"#b54\">[56,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref> propose to decorrelate features by adding a regularizer that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ew training samples. Previous methods studied meta-learning from the perspective of metric learning <ref type=\"bibr\" target=\"#b14\">(Koch 2015;</ref><ref type=\"bibr\" target=\"#b44\">Vinyals et al. 2016;< is non-trivial to train a reconstruction network from limited modalitycomplete samples. Inspired by <ref type=\"bibr\" target=\"#b14\">(Kuo et al. 2019)</ref>, we approximate the missing modality using a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  such as Long Short Term Memory networks (LSTM) are common <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>. These networks often have complex architecture with millio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: istic-based hardware prefetchers, e.g., stride <ref type=\"bibr\" target=\"#b54\">[55]</ref> and stream <ref type=\"bibr\" target=\"#b55\">[56]</ref>, or even sophisticated prefetchers that rely on repeated p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  two well-accepted propositions-deep neural networks learn meaningful patterns before fitting noise <ref type=\"bibr\" target=\"#b2\">[3]</ref> and minimum entropy regularisation principle [10]-we propose  meaningful patterns before fitting noise, even when severe label noise exists in human annotations <ref type=\"bibr\" target=\"#b2\">[3]</ref>. (2) As a learner attains confident knowledge as time progre e meaningful patterns before fitting noise, even when severe label noise exists in human annotations<ref type=\"bibr\" target=\"#b2\">[3]</ref>. (2) As a learner attains confident knowledge as time progre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by powerful baseline systems, such as the Fast/Faster R-CNN <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29]</ref> and Fully Convolutional Network (FCN) <ref type=\"bibr\" targ target=\"#b14\">[9]</ref>. N is 64 for the C4 backbone (as in <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29]</ref>) and 512 for FPN (as in <ref type=\"bibr\" target=\"#b27\">[22] state-of-the-art instance segmentation results. Our method, called Mask R-CNN, extends Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref> by adding a branch for predicting segmentation masks on ea ding to RoIs on feature maps using RoIPool, leading to fast speed and better accuracy. Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref> advanced this stream by learning the attention mechanism w e of Fast/Faster R-CNN.</p><p>Faster R-CNN: We begin by briefly reviewing the Faster R-CNN detector <ref type=\"bibr\" target=\"#b34\">[29]</ref>. Faster R-CNN consists of two stages. The first stage, cal are shareable.</p><p>Inference: At test time, the proposal number is 300 for the C4 backbone (as in <ref type=\"bibr\" target=\"#b34\">[29]</ref>) and 1000 for FPN (as in <ref type=\"bibr\" target=\"#b27\">[2 hares features between the RPN and Mask R-CNN stages, following the 4-step training of Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref>. This model runs at 195ms per image on an Nvidia Tesla M40  hyper-parameters following existing Fast/Faster R-CNN work <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29,</ref><ref type=\"bibr\" target=\"#b27\">22]</ref>. Although these dec decisions were made for object detection in original papers <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29,</ref><ref type=\"bibr\" target=\"#b27\">22]</ref>, we found our insta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b34\">36,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" tar get=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b34\">36,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>. Most recently, <ref type=\"bibr\" target=\"#b24\">[25]</ref> achieved impressive performance on several image recogniti re. VATT borrows the exact architecture from BERT <ref type=\"bibr\" target=\"#b22\">[23]</ref> and ViT <ref type=\"bibr\" target=\"#b24\">[25]</ref> except the layer of tokenization and linear projection res <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, image recognition <ref type=\"bibr\" target=\"#b24\">[25]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b107\">[10 #b56\">57]</ref>. However these methods still rely on the feature extracted by CNNs.</p><p>Recently, <ref type=\"bibr\" target=\"#b24\">[25]</ref> proposes a set of convolution-free vision Transformers whi mance with CNNs. <ref type=\"bibr\" target=\"#b85\">[86]</ref> improves the training data efficiency of <ref type=\"bibr\" target=\"#b24\">[25]</ref> by using stronger data augmentations and knowledge distill ]</ref>. Recently, <ref type=\"bibr\" target=\"#b17\">[18]</ref> conduct contrastive learning using ViT <ref type=\"bibr\" target=\"#b24\">[25]</ref> and achieve impressive results. As for the video domain, i  R t\u2022h\u2022w\u20223\u00d7d</formula><p>. This can be seen as a 3D extension of the patching mechanism proposed in <ref type=\"bibr\" target=\"#b24\">[25]</ref>. To encode the position of these patches, we define a dime ecture <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which has been widely used in NLP. Similar to ViT <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we do not tweak the architecture so that our weights can  n, we still achieve competitive results to the supervised pre-training using large-scale image data <ref type=\"bibr\" target=\"#b24\">[25]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  ustrated in Figure <ref type=\"figure\" target=\"#fig_0\">1</ref> middle panel) and refer the reader to <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for more details o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: operties that could be used to craft adversarial samples <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b28\">[30]</ref>, <ref type=\"bibr\" target=\"#b34\">[36]</ref>. Simply put, th ages that are unrecognizable to humans, but are nonetheless labeled as recognizable objects by DNNs <ref type=\"bibr\" target=\"#b28\">[30]</ref>. For instance, they demonstrated how a DNN will classify a e backpropagation procedure used during network training <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b28\">[30]</ref>, <ref type=\"bibr\" target=\"#b34\">[36]</ref>. This approach . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tacks. The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020)</ref> library. Due to the lack of computationally e rnejad, and G\u00fcnnemann 2018)</ref>. Since then, several graph adversarial attacks have been proposed <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2020;</ref><ref type=\"bibr\" target=\"#b17\">Sun et al. 2018 accard similarity between the constituent nodes were removed prior to training a GNN. Similarly, in <ref type=\"bibr\" target=\"#b11\">(Jin et al. 2019)</ref>, explicit graph smoothing was performed by tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type=\"bibr\" target=\"#b12\">[13]</ref>, as the answer sentence fails to be directly retrieved by  re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: osed approach is more practical because the training data is generally inaccessible to the attacker <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Our contributions can be summarized as follows:</p><p>\u2022 W  original training data. However, in practice the attacker often has no access to the training data <ref type=\"bibr\" target=\"#b31\">[32]</ref>. To overcome this limitation, Mopuri et al. propose to gen ome this limitation, Mopuri et al. propose to generate universal perturbation without training data <ref type=\"bibr\" target=\"#b31\">[32]</ref>. However, their approach is specifically designed for non- -agnostic) attacks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uch as bagof-words or n-grams <ref type=\"bibr\" target=\"#b19\">(Wang and Manning, 2012)</ref> or SVMs <ref type=\"bibr\" target=\"#b16\">(Tang et al., 2015)</ref>. The neural network based methods like <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>We use the official BERT model from Google as the starting point. Following the notation from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>, we denote the number of layers (i.e., tran blocks are our pruning target.</p><p>Data: In pre-training, we use the same pre-training corpora as <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref>: BookCorpus (800M words) <ref type=\"bibr\" t </ref> .</p><p>Input/Output representations: We follow the input/output representation setting from <ref type=\"bibr\" target=\"#b5\">Devlin et al. [2019]</ref> for both pre-training and fine-tuning. We u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vals must wait until the next core visit. This policy is usually known as \"gated M -limited\" policy <ref type=\"bibr\" target=\"#b13\">[13]</ref>. To complete the processing of a packet, a core needs at l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  size) is usually a pre-specified parameter. Current works <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> did not efficiently handle the change of discriminative attr y a graph auto-encoder, but this method neglects linkage between paper and author and coauthorship. <ref type=\"bibr\" target=\"#b6\">[7]</ref> addresses the pairwise classification problem by extracting . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2017;</ref><ref type=\"bibr\" target=\"#b39\">Veli\u010dkovi\u0107 et al., 2018;</ref><ref type=\"bibr\">2019;</ref><ref type=\"bibr\" target=\"#b33\">Qu et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Gao &amp; Ji, 20 rs and the edge weights between them correspond to the degree of trust between the users. Following <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>, we treat edges with weights greater than 3 a t state-of-the-art methods GAT <ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref> and Graph U-Net <ref type=\"bibr\" target=\"#b13 the results of GraphMix(GCN) are comparable with the recently proposed state-of-the-art method GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>. Since GraphMix consists of various component  \u00b1 0.3% GraphScan <ref type=\"bibr\" target=\"#b11\">(Ding et al., 2018)</ref> 83.3 \u00b11.3 73.1\u00b11.8 -GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref> 83.7% 73.1% 81.8% DisenGCN <ref type=\"bibr\" t ; Welling, 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>, among others. This architecture has one hidd hence much of the recent attention is dedicated to proposing architectural changes to these methods <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Gao &amp; Ji, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n the same train/validation/test splits of the same three datasets (CORA, CiteSeer and PubMed) from <ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref>. Such experimental setup favors the model th  consider the problem of semi-supervised transductive node classification in a graph, as defined in <ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref>. In this paper we compare the four following raged over all datasets. See text for the definition. (b) Model accuracy on the Planetoid split from<ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref> and another split on the same datasets. Diff ute the following simple experiment. We run the 4 models on the datasets and respective splits from <ref type=\"bibr\" target=\"#b15\">[Yang et al., 2016]</ref>. As shown in Table <ref type=\"table\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ate to handle multi-relation QA due to the lack of reasoning ability.</p><p>Recent reasoning models <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016;</ref><ref type=\"bibr\" target=\"#b26\">Wang et al. manner during reasoning. MemNN <ref type=\"bibr\" target=\"#b27\">(Weston et al., 2015)</ref>, KVMemN2N <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016)</ref> and EviNet <ref type=\"bibr\" target=\"#b21\" re the settings are the same as <ref type=\"bibr\" target=\"#b8\">(Bordes et al., 2015)</ref>. KVMemN2N <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016)</ref> improves the MemN2N for KBQA as it divide. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ary. Shashi et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> performed global optimization of ROUGE <ref type=\"bibr\" target=\"#b17\">[18]</ref> metrics through reinforcement learning, conceptualized ext ion indicator. ROUGE is a text summary automatic evaluation method proposed by Chin-yew Lin in 2004 <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We use the standard F1 scores of ROUGE-1, ROUGE-2 and ROU. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fense strategies into consideration <ref type=\"bibr\">(Athalye et al., 2018b)</ref>.</p><p>Recently, <ref type=\"bibr\" target=\"#b13\">Shafahi et al. (2019)</ref> showed that, for two classes of data dist for general classifiers, and their relationship to some recent works in literature.</p><p>Recently, <ref type=\"bibr\" target=\"#b13\">Shafahi et al. (2019)</ref> shows that no classifier can achieve low . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eacher (ResNet 18 trained on ImageNet).</p><p>We then compute the adjusted mutual information (AMI) <ref type=\"bibr\" target=\"#b42\">(Vinh et al., 2010)</ref> between the resulting grouping and ground t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: M contains a certain piece of knowledge <ref type=\"bibr\" target=\"#b8\">(Hewitt and Liang, 2019;</ref><ref type=\"bibr\" target=\"#b26\">Voita and Titov, 2020)</ref>. Attention visualization, another common. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ransformer networks <ref type=\"bibr\" target=\"#b19\">[9,</ref><ref type=\"bibr\" target=\"#b35\">25,</ref><ref type=\"bibr\" target=\"#b79\">69]</ref> have been proposed to directly encode graph inputs. In this -KEG-GTN enc.: replaces our proposed graph encoder with a different graph transformer network (GTN) <ref type=\"bibr\" target=\"#b79\">[69]</ref>, that learns multiple meta-path graphs from the input grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s within a sliding window <ref type=\"bibr\" target=\"#b45\">[45]</ref>. The reconstruction-based works <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" ta \"bibr\" target=\"#b34\">[34,</ref><ref type=\"bibr\" target=\"#b42\">42]</ref> and collaborative filtering <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b37\">37]</ref> are also connected ed on local subgraphs for the task of inductive matrix completion. \u2022 Collaborative filtering: NeuMF <ref type=\"bibr\" target=\"#b15\">[15]</ref> and NGCF <ref type=\"bibr\" target=\"#b37\">[37]</ref>. NeuMF . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Sequence variation within a protein family conveys information about the structure of the protein <ref type=\"bibr\" target=\"#b52\">(Yanofsky et al., 1964;</ref><ref type=\"bibr\" target=\"#b2\">Altschuh e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t with the Skilling-Hutchinson trace estimator <ref type=\"bibr\" target=\"#b38\">(Skilling, 1989;</ref><ref type=\"bibr\" target=\"#b18\">Hutchinson, 1990)</ref>.</p><p>In particular, we have</p><formula xml. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tly. On one hand, prior non-uniform cache access (NUCA) work <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= NUCA by adaptively placing data close to the requesting core <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= istance. However, these best-effort techniques often result in hotspots and additional interference <ref type=\"bibr\" target=\"#b2\">[3]</ref>. On the other hand, prior work has proposed a variety of par at D-NUCA often causes significant bank contention and uneven distribution of accesses across banks <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We also see this effect in Sec. VI -R-NUCA has the highest . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hypernymy detection tasks, pattern-based approaches outperform those based on distributional models <ref type=\"bibr\" target=\"#b29\">(Roller et al., 2018)</ref>. Subsequent work pointed out the sparsity. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: steful, making models less interpretable and assigning probability mass to many implausible outputs <ref type=\"bibr\" target=\"#b14\">(Peters, Niculae, and Martins 2019)</ref>.</p><p>To overcome the issu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: )</ref> proposed FloWorM system that includes tracker, analyzer and reporter based on NetFlow data. <ref type=\"bibr\" target=\"#b5\">Abdulla et al. (2011)</ref> presented a support vector machine (SVM) m nti and Rossi, 2011)</ref>, or data fusion with other log files such as Snort, DNS related requests <ref type=\"bibr\" target=\"#b5\">(Abdulla et al., 2011)</ref> (number of DNS requests, response, normal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: plexity constraint. The searched architectures generate new stateof-the-art performance on ImageNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>. For instance, as shown in Fig. <ref type=\"figure\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e terminology \"architecture size\" or \"size\" refer to the number of channels in each layer following <ref type=\"bibr\" target=\"#b20\">[21]</ref>. for the DAG and the size of the operation set. We choose  e Search Space S s . The size search space is inspired by transformable architecture search methods <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bib , GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref type=\"bibr\" target=\"#b20\">[21]</ref>, FBNet-V2 <ref type=\"bibr\" target=\"#b43\">[44]</ref>, TuNAS e interpolation to aggregate feature tensors with different shapes with the architecture parameters <ref type=\"bibr\" target=\"#b20\">[21]</ref>. (2) FBNetV2 utilises a masking mechanism to represent dif ple, how to design a FLOPs constrain loss to regularize the discovered architecture to be efficient <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: using an LMs' representations as features <ref type=\"bibr\" target=\"#b3\">(Conneau et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Liu et al., 2019)</ref>. However, probing classifiers require additio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph signal processing literature to measure the smoothness of a signal defined over nodes of a graph <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2015)</ref>. More specifically, given a graph with the a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t harm their representational capacity for object recognition.</p><p>The Single Shot Detector (SSD) <ref type=\"bibr\" target=\"#b21\">[22]</ref> is one of the first attempts at using a ConvNet's pyramida tiple layers before computing predictions, which is equivalent to summing transformed features. SSD <ref type=\"bibr\" target=\"#b21\">[22]</ref> and MS-CNN <ref type=\"bibr\" target=\"#b2\">[3]</ref> predict >[35]</ref>, context modeling <ref type=\"bibr\" target=\"#b15\">[16]</ref>, stronger data augmentation <ref type=\"bibr\" target=\"#b21\">[22]</ref>, etc. These improvements are complementary to FPNs and sho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: its better capability of capturing the local textual information compared to other GNNs such as GCN <ref type=\"bibr\" target=\"#b6\">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: it feedback, implicit feedback is more difficult to utilize because of the lack of negative samples <ref type=\"bibr\" target=\"#b10\">[Pan et al., 2008]</ref>. Secondly, generating top-k preferred items  it operation. Finding approximate top-K items can be even finished in sublinear or logarithmic time <ref type=\"bibr\" target=\"#b10\">[Wang et al., 2012;</ref><ref type=\"bibr\" target=\"#b10\">Muja and Lowe n finished in sublinear or logarithmic time <ref type=\"bibr\" target=\"#b10\">[Wang et al., 2012;</ref><ref type=\"bibr\" target=\"#b10\">Muja and Lowe, 2009]</ref> by making use of index technique.</p><p>Se bibr\">[Zhou and Zha, 2012]</ref>, PPH <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2014]</ref>, CH <ref type=\"bibr\" target=\"#b10\">[Liu et al., 2014]</ref> incur large quantization loss <ref type=\"bib rm for f (x) and \u03b2 is its penalty coefficient. <ref type=\"bibr\">[Giannessi and Tardella, 1998;</ref><ref type=\"bibr\" target=\"#b10\">Lucidi and Rinaldi, 2010]</ref> show that the above two problems are  13)</label></formula><p>In terms of the loss function, we employ the popular and effective BPR loss <ref type=\"bibr\" target=\"#b10\">[Rendle et al., 2009]</ref>. In particular, given a user matrix U and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bels jointly. Prevailing generative models <ref type=\"bibr\" target=\"#b49\">(Zhang et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Bowman et al., 2016)</ref> are inapplicable because they can only gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m for the chest database creation or expansion, performing NER training and modeling using NeuroNER <ref type=\"bibr\" target=\"#b9\">[12]</ref> and then generating the current or the new model, and hence. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rning has inspired some researchers applying data-driven models to weather predictions. Early works <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> usually treat machi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re\">1</ref>). Since the problem roots in the high O(L 2 ) time and space complexity in transformers <ref type=\"bibr\" target=\"#b45\">[46]</ref> (L is the length of the text), another line of research at. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>In many real-world studies, structured objects such as molecules are naturally modeled as graphs <ref type=\"bibr\" target=\"#b21\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b55\">Wu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, contrastive learning <ref type=\"bibr\" target=\"#b3\">[4,</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  well as encoding the temporal context, we borrow an unsupervised context reconstruction block from <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We use the same structure, but extend it by training the  boring clips are related. Recurrent neural networks have been developed in modeling sequential data <ref type=\"bibr\" target=\"#b18\">[19]</ref>. However, they ignored that the neighboring clips are more previously described, video sequences are context relevant. We follow the RNN training described in <ref type=\"bibr\" target=\"#b18\">[19]</ref> to train the context reconstruction. Instead of training r f> to train the context reconstruction. Instead of training reconstruction module in separate stage <ref type=\"bibr\" target=\"#b18\">[19]</ref> (i.e., unsupervised context reconstruction and supervised  construction module into a joint end-to-end learning framework. The original context-reconstruction <ref type=\"bibr\" target=\"#b18\">[19]</ref> considered the reconstruction at the sequence-level, e.g., uage and both of them have been used to model video data <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Following <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we u ata <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Following <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we use the GRU architecture as it has shown similar perfo nabled to predict the previous frame which are combined together to form the bi-directional network <ref type=\"bibr\" target=\"#b18\">[19]</ref>. This bi-direction process forces the encoded representati iginal feature, we can obtain more expressive representation for the classification task. Following <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we choose the huber loss for reconstruction,</p><formula  ess of neighboring reconstruction, we compare to a sequence-level reconstruction method proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We use the sequence length to 30 as <ref type=\"bibr\" targ n method proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We use the sequence length to 30 as <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We use the schema (a) described in Section III-C and the  deling method compared with the mean pooling method, especially on the HMDB-51 dataset. Compared to <ref type=\"bibr\" target=\"#b18\">[19]</ref>, our neighboring reconstruction outperforms it by 0.6% on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#b41\">(Sennrich et al., 2016a;</ref><ref type=\"bibr\" target=\"#b18\">Ficler and Goldberg, 2017;</ref><ref type=\"bibr\" target=\"#b40\">Scarton and Specia, 2018)</ref>, which condition generation on users'. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: les <ref type=\"bibr\" target=\"#b50\">[51]</ref>. The related study is presented in our previous paper <ref type=\"bibr\" target=\"#b49\">[50]</ref>.</p><formula xml:id=\"formula_15\">Hmatch 2 = 1/(norm(|P U \u2212 6\"><head></head><label></label><figDesc>) T U describes learning styles. Referring to other research<ref type=\"bibr\" target=\"#b49\">[50]</ref>, we design the elements of learning styles as: T U = {CL, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the target user and item based on historical interactions <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. For example, given two paths p 1 u 1 \u2192 i 1 \u2192 u 2 \u2192 i 2 and terests; meanwhile, the user groups can also profile items <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Hence, in each modality (e.g., visual), we aggregate signa e select LeakyReLU(\u2022) as the nonlinear activation function <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Such aggregation method assumes that different neighbors w ntegrate multi-modal features as the node features to learn the representation of each node. \u2022 NGCF <ref type=\"bibr\" target=\"#b41\">[41]</ref>. This method represent a novel recommendation framework to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: owledge-aware recommendations, such as RippleNet <ref type=\"bibr\" target=\"#b45\">[46]</ref>, KGCN-LS <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and KGAT <ref type=\"bibr\" target=\"#b47\">[48]</ref>.</p></. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: presentation learning for generic graphs (GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>). In general, GNNs recursively update each node's feature by icult. Although sampling methods, such as GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and FastGCN <ref type=\"bibr\" target=\"#b0\">[1]</ref>), have b  type=\"bibr\" target=\"#b9\">[10]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We use a large-scale bipartite graph dataset from the Tence igh memory cost. Sampling methods like GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref> and AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> have been proposed to deal with this issue by reducing the n ons: GCN and MEAN aggregator. Node-wise sampling is used to address the scalability issue. \u2022 AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>: This method uses adaptive sampling between each layer to de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ER score, significantly outperforming previous BERT and Graph Neural Network (GNN) based approaches <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>. Our experiments demonstrate KGAT's strong   2018)</ref> further incorporates evidence identification to improve claim verification.</p><p>GEAR <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> formulates claim verification as a graph re tei-c.org/ns/1.0\"><head n=\"3.1\">Reasoning with Evidence Graph</head><p>Similar to previous research <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, KGAT constructs the evidence graph G by us narios and produces a probability P (y|c, D) to predict claim label y. Different from previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, we follow the standard graph label predict sentation v p . The aggregation is done by a graph attention mechanism, the same with previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>.</p><p>It first calculate the attention wei n</head><p>The per-node predictions are combined by the \"readout\" function in graph neural networks <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, where KGAT uses node kernels to learn the  ds without pre-training. BERT-pair, BERT-concat and GEAR are three baselines from the previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>. BERT-pair and BERTconcat regard claim-evid eriments are all based on ESIM sentence retrieval, which is the one used by GEAR, our main baseline <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org head n=\"6\">Case Study</head><p>Table <ref type=\"table\">5</ref> shows the example claim used in GEAR <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> and the evidence sentences retrieved by ESI htweight backpacker, inventor, author and global adventurer. Label: SUPPORT Table5: An example claim<ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> whose verification requires multiple pieces \"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Soleimani et a d is kept the same with previous work <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Soleimani et a l keeps the same as the previous work <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019)</ref>. The base version of BERT is used to impleme  KGAT is the best on all testing scenarios. With ESIM sentence retrieval, same as the previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Hanselowski et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ween 2D and 3D bounding boxes to recover 3D information. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> estimate 3D object information by calculating the similari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rage graph structure to enhance the representation of documents and queries. For example, Li et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> learned text representation with graph structure that cont. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s. Although the time cost was reduced, it brought a large number of false positives.</p><p>The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a configurable protection technique for SDC-causi s, machine learning based methods are introduced to identify the SDC-causing instructions. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a machine learning algorithm based model, namely  pproach in detail. We first define some terms used in this paper, some of which are drawn from work <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>Dynamic Dependency Graph: A Dynamic Dependency Grap ased on prior work <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" t  are based on duplicating the backward slices of the instructions to protect, similar to prior work <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>We insert a check immediately after the instruction tion efficiency and SDC impact are imperative parameters for evaluating our approach. In literature <ref type=\"bibr\" target=\"#b15\">[16]</ref>, the SDC detection efficiency (DE) is defined as the ratio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: vely search the tree-structured architecture space. Motivated by these AutoML frameworks, He et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> leveraged the reinforcement learning to automatically prun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: orithms, they usually generate enormous amounts of intermediate data. GPM systems such as Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, and Fr ly proportional to the graph size, but also increases exponentially as the embedding size increases <ref type=\"bibr\" target=\"#b80\">[81]</ref>. Furthermore, GPM problems require compute-intensive opera describe two of these GPM systems briefly and then discuss their major limitations.</p><p>Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref> is a distributed GPM system. It proposes \"think like an em anonical test for each embedding, which has been demonstrated to be very expensive for large graphs <ref type=\"bibr\" target=\"#b80\">[81]</ref>.</p><p>Materialization of Data Structures: The list or arr oking the isomorphism test, embeddings in the worklist are first reduced using their quick patterns <ref type=\"bibr\" target=\"#b80\">[81]</ref>, and then quick patterns are aggregated using their canoni Experimental Setup</head><p>We compare Pangolin with the state-of-the-art GPM frameworks: Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, G-Mine r ease of programming.</p><p>GPM Frameworks: For ease-of-programming, GPM systems such as Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, G-Mine head n=\"2.3\">Existing GPM Frameworks</head><p>Existing GPM systems target either distributed-memory <ref type=\"bibr\" target=\"#b80\">[81,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  in the literature are the techniques called Enhanced Control Flow Checking Using Assertions (ECCA) <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Control Flow Checking by Software Signatures (CFCSS) <  from the signature associated with the current node, it means an error has occurred in the program <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> ) in which the execution always enters at the first instruction and leaves via the last instruction <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eam generates about 1.2TB intermediate data to count 4-motif on the MiCo graph with 1 million edges <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>Recently, specialized systems have been developed f thms in these specialized pattern matching systems can be described with nested loops, and AutoMine <ref type=\"bibr\" target=\"#b17\">[18]</ref> and GraphZero <ref type=\"bibr\" target=\"#b11\">[12]</ref> re ate-of-the-art singlemachine pattern matching systems. GraphZero is an upgraded version of AutoMine <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and it outperforms AutoMine by up to 40\u00d7. Fractal is a JV ef>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Automine <ref type=\"bibr\" target=\"#b17\">[18]</ref> is built upon a set-based representation and uses compilat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ments on five popular benchmarks including GOT-10k <ref type=\"bibr\" target=\"#b23\">[25]</ref>, LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref>, TC-128 <ref type=\"bibr\" target=\"#b36\">[38]</ref>, OTB-15  at a speed of around 31 fps. We train the target localization branch using training splits of LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref>, GOT-10k <ref type=\"bibr\" target=\"#b23\">[25]</ref> and VID k and \u03b1 are set to 5, 1 and 0.8, respectively. For BBR-IoU network, we use training splits of LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref>, GOT-10k <ref type=\"bibr\" target=\"#b23\">[25]</ref> and VID p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.\">Experiment on LaSOT</head><p>LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a large-scale dataset consisting of 1,400 sequences. Fo ead><p>To validate the effect of different components, we conduct ablation experiments on LaSOT tst <ref type=\"bibr\" target=\"#b15\">[16]</ref> regarding the target localization and target estimation.</ re <ref type=\"figure\">5</ref>. Comparisons of our MART and other state-of-the-art trackers on LaSOT <ref type=\"bibr\" target=\"#b15\">[16]</ref>, TC-128 <ref type=\"bibr\" target=\"#b36\">[38]</ref> and OTB-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wer the expected stochastic gradient variance. As shown in <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, the reduction of variance can lead to faster convergence.  ques, such as stratified sampling <ref type=\"bibr\" target=\"#b34\">[35]</ref> and importance sampling <ref type=\"bibr\" target=\"#b35\">[36]</ref> are proposed to achieve the variance reduction. Different . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uring more comprehensive graph features for downstream tasks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>.</p><p>Multiscale f res in parallel and merge them as the final representation <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar #b41\">42]</ref>  (a) Encoder-decoder <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>(b) Graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</ref>. (c) Readout <ref type=\"bibr\" target=\"#b30\">[31]</ref>.</p rchanging across scales forms a crossing shape.</p><p>Remark: In each individual scale, graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</ref> simply uses skip connections while GXN uses multiple graph \"bibr\" target=\"#b34\">[35]</ref> designs various graph filters on the multiscale graphs. Graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</ref> and readout functions <ref type=\"bibr\" target=\"#b18\">[19,< tion. 2) GXN extracts hierarchical multiscale features through a deep network, previous Graph U-net <ref type=\"bibr\" target=\"#b19\">[20]</ref> extracts features only once in each scale and then uses sk cess of vertex selection and graph pooling process.</p><p>To implement graph unpooling, inspired by <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we design an inverse process against graph pooling. We in eatures, explicitly utilizing some structural information. We use the same dataset separation as in <ref type=\"bibr\" target=\"#b19\">[20]</ref>, perform 10-fold cross-validation, and show the average ac ent datasets. GXN (gPool) and GXN (SAGPool) denote that we apply previous pooling operations, gPool <ref type=\"bibr\" target=\"#b19\">[20]</ref> and SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref> in o =\"bibr\" target=\"#b52\">[53]</ref>, AttPool <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Graph U-Net <ref type=\"bibr\" target=\"#b19\">[20]</ref>. In the VIPool, we use a 2-layer MLP and R-layer GCN (R =  pe=\"bibr\" target=\"#b52\">[53]</ref>, DiffPool <ref type=\"bibr\" target=\"#b49\">[50]</ref>, Graph U-Net <ref type=\"bibr\" target=\"#b19\">[20]</ref>, SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref>, AttPoo ditionally, we design several variants of GXN: 1) to test the superiority of VIPool, we apply gPool <ref type=\"bibr\" target=\"#b19\">[20]</ref>, SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Att ype=\"bibr\" target=\"#b6\">[7]</ref>, ASGCN <ref type=\"bibr\" target=\"#b27\">[28]</ref>, and Graph U-Net <ref type=\"bibr\" target=\"#b19\">[20]</ref> for vertex classification. We reproduce these methods for  ifferent pooling methods with the same GXN model framework, where the pooling methods include gPool <ref type=\"bibr\" target=\"#b19\">[20]</ref>, SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Att p><p>We compare the proposed VIPool operation with several baseline methods: random sampling, gPool <ref type=\"bibr\" target=\"#b19\">[20]</ref>, SAGPool <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Att er graph pooling methods adaptively select vertices based on their importance over the entire graph <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>; however, they fai tructure and information fusion easier to achieve. Compared to other vertex-selection-based methods <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, VIPool considers  association to preserve the original vertex information. The vertex-selection-based pooling methods <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> preserve selected . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ort, baseball is false since Matt Flynn is an NFL player), incompatible entity types, and many more <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. It has also been observed that such nois fectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>.</p></div> <div xmlns=\"http://www.tei-c.o exclusive (MUT and RMUT); and inverse relations (INV). We reproduce the list of information used in <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> in tabular form in Table <ref type=\"table mal distributions: N (0.7, 0.2) for facts in the original KG and N (0.3, 0.2) for added noisy facts <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. The SAMEENT facts between entities are g hyper-parameter threshold as the cutoff for classifying a test triple based on the prediction score <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. Our experiments were run on Intel(R) Xeo the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx e the probabilistic sources of information such as the confidence scores obtained during extraction <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b8\">, Jiang et al pe=\"bibr\" target=\"#b8\">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b19\">[Pujara et a ref type=\"bibr\" target=\"#b1\">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b8\">, Jiang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o discover pairwise topic correlations by the covariance matrix. In terms of treestructured topics, <ref type=\"bibr\" target=\"#b23\">[24]</ref> introduces to generate a series of topics from the root to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nd item representations <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Recently, GCN-based models <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta ually, current GCN-based recommendation models achieve their peak performance at most 3 or 4 layers <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Besides the over- CF on recommendation accuracy.</p><p>It is worth mentioning that the LightGCN proposed by He et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> has a similar formulation as LR-GCN. With careful experime pt the simplified network structure of LightGCN, as its effectiveness has been well demonstrated in <ref type=\"bibr\" target=\"#b13\">[14]</ref> and it can alleviate the over-smoothing problem to some ex epresentation of user \ud835\udc62 and item \ud835\udc56 as Eq. 2. Similar to LightGCN, \ud835\udefc \ud835\udc58 is set uniformly as 1/(\ud835\udc3e + 1) <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>With the learned embeddings of users (i.e., \ud835\udc86 \ud835\udc62 ) a the final representations of users and items, this formulation keeps consistent with it in LightGCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>:</p><formula xml:id=\"formula_17\">\ud835\udc6c = \ud835\udefc 0 \ud835\udc6c (0) + \ud835\udefc 1 \ud835\udc6c (1) der connectivities by performing embedding propagation in the user-item bipartite graph. \u2022 LightGCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>: It is an simplified version of NGCF by removing the featu zing high-order information directly in representation learning. Similar to the results reported in <ref type=\"bibr\" target=\"#b13\">[14]</ref>, LightGCN achieves substantially improvement over NGCF by  /ref>, researchers also introspect the complex design in GCN-based recommendation models. He at al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> pointed out that the two common designs feature transforma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e at most 10 Sim-Points for each SPEC benchmark.</p><p>We present multi-core results for CloudSuite <ref type=\"bibr\" target=\"#b42\">[43]</ref> and multi-programmed SPEC benchmarks. For CloudSuite, we u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b19\">18]</ref> have been widely adopted to extract useful structural infor oss various domains, such as neuroscience <ref type=\"bibr\" target=\"#b31\">[30]</ref>, bioinformatics <ref type=\"bibr\" target=\"#b19\">[18]</ref>, and information networks <ref type=\"bibr\" target=\"#b6\">[5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: so related to metric learning works that employ generators <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>. Apart from not requiring labels, our method exploits the m od exploits the memory component, something not present in <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>. It has no extra parameters or loss terms that need to be o </ref><ref type=\"bibr\" target=\"#b36\">35]</ref>. Works like <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref> use generators to synthesize negatives in a supervised scen and exploit its memory component. What is more, and unlike <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87]</ref>, we do not require a generator, i.e. have no extra paramete izing negatives was explored in metric learning literature <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b88\">87,</ref><ref type=\"bibr\" target=\"#b36\">35]</ref>. Works like <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b16\">[Nickel et al., 2012</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016</ref><ref type=\"bibr\" target=\"#b3\">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, wi porate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> and ComplEx <ref type=\"bibr\" target=\"#b2 tion). We work with ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> embeddings which have shown state of the #b23\">[Trouillon et al., 2016]</ref>, <ref type=\"bibr\">SimplE [Kazemi and Poole, 2018]</ref>, ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a dif Then all facts upto length 3 in the hierarchy of taxonomy were included.</p><p>FB15K-237: FB15K-237 <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref>, another popular benchmark does not have  compare them with Com-plEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref>, two state-of-the-art KG embeddings meth g the same class balance, and use them as our validation and test split.</p><p>YAGO3-10: YAGO3-10 [ <ref type=\"bibr\" target=\"#b3\">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rained language models (e.g., ELMo <ref type=\"bibr\" target=\"#b29\">(Peters et al., 2018)</ref>, BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, XLnet <ref type=\"bibr\" target=\"#b45\">(Yan e achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al.,  entations of the fully supervised NER methods attain very close to the state-of-the-art performance <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Limsopatham . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ld <ref type=\"bibr\" target=\"#b28\">(Morcos et al., 2011)</ref>, sparse inverse covariance estimation <ref type=\"bibr\" target=\"#b20\">(Jones et al., 2011)</ref>, and pseudolikelihood maximization <ref ty vised structure learning with Potts models performs poorly when few related sequences are available <ref type=\"bibr\" target=\"#b20\">(Jones et al., 2011;</ref><ref type=\"bibr\" target=\"#b21\">Kamisetty et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Related to our work, for example, <ref type=\"bibr\" target=\"# ibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Related to our work, for example, <ref type=\"bibr\" target=\"#b6\">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model rget=\"#b6\">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS <ref type=\"bibr\" target=\"#b6\">[7]</ref>. These studies consider learning methods within the traditio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">INTRODUCTION</head><p>Score matching <ref type=\"bibr\" target=\"#b12\">(Hyv\u00e4rinen, 2005)</ref> is particularly suitable for learning unnorma MLE) can be difficult due to the intractable partition function Z \u03b8 . To avoid this, score matching <ref type=\"bibr\" target=\"#b12\">(Hyv\u00e4rinen, 2005)</ref> minimizes the Fisher divergence between p d a  not have access to the score function of the data s d (x).</p><p>By applying integration by parts, <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref> shows that L(\u03b8) can be written as L(\u03b8) = J(\u03b8)  </p><p>Other than our requirements on p v , the assumptions are exactly the same as in Theorem 1 of <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>. We advise the interested readers to read Appe he consistency of <ref type=\"bibr\">MLE (van der Vaart, 1998)</ref>. We also adopt the assumption in <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref> that all densities are strictly positive (Assu M . These two facts lead to consistency. For a complete proof, see Appendix B.3.</p><p>Remark 1. In <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>, the authors only showed that J(\u03b8) = 0 \u21d4 \u03b8 = \u03b8 s a constant w.r.t. \u03b8.</p><p>Proof. The basic idea of this proof is similar to that of Theorem 1 in <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>. First, note that L(\u03b8, p v ) can be expanded t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff an ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Most importantly, there are fast approximations for both PPR <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b75\">76]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b33\">[34]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e been the dominating model architecture for computer vision <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= tectures for many computer vision tasks. Traditionally, regular convolutions, such as ResNet blocks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, are popular in large-scale ConvNets; in contrast, depthwise. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ver, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\" ng techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type=\"bibr\" target=\"#b12\">(Hochreiter and Schmidhuber 1997;</ref><ref type=\"bibr\" target=\"#b16\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to overcome the sensitivity to cluster shapes and scales of Euclidean distance in the feature space <ref type=\"bibr\" target=\"#b23\">[24]</ref> . FCM_S algorithm introduces a constraint in image domain,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntiable doubly-linked lists and stacks <ref type=\"bibr\" target=\"#b14\">[15]</ref>, queues and deques <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Different from exploring various forms of dynamic storage. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ehring et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Kalchbrenner et al., 2016)</ref>, attention <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref>, or a combination of recurrence and atte ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Transformer</head><p>The Transformer <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref> employs an encoder-decoder structure, co  for all sequences, heads, and positions in a batch using parallel matrix multiplication operations <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref>. Without relative position representatio d><p>We compared our model using only relative position representations to the baseline Transformer <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref> with sinusoidal position encodings. We g e a deterministic function of position <ref type=\"bibr\" target=\"#b7\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b10\">Vaswani et al., 2017)</ref> or learned representations. Convolutional se in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as <ref type=\"bibr\" target=\"#b10\">Vaswani et al. (2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o  1 = 0.9, \u03b2 2 = 0.98, and = 10 \u22129 . We used the same warmup and decay strategy for learning rate as <ref type=\"bibr\" target=\"#b10\">Vaswani et al. (2017)</ref>, with 4,000 warmup steps. During training. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . They brought great advancements in many applications of neural network, such as visual perception <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hitecture of TransformerCPI</head><p>The model we proposed is based on the transformer architecture <ref type=\"bibr\" target=\"#b41\">(Vaswani et al., 2017)</ref>, which was originally devised for neural. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> adopt a pre-trained VGG-M model on the VGG face dataset <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>Figure <ref type=\"figure\">5</ref> displays the atte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: evidence to be retrieved from Wikipedia.</p><p>We constructed a purpose-built dataset for this task <ref type=\"bibr\" target=\"#b15\">(Thorne et al., 2018)</ref> that contains 185,445 human-generated cla nce when constructing the dataset was the trade-off between annotation velocity and evidence recall <ref type=\"bibr\" target=\"#b15\">(Thorne et al., 2018)</ref>. Evidence selected by annotators was ofte ata was released through the FEVER website. 1 We used the reserved portion of the data presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>   </p></div> <div xmlns=\"http://www.tei-c. www.tei-c.org/ns/1.0\"><head n=\"2.2\">Scoring Metric</head><p>We used the scoring metric described in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref> to evaluate the submissions. The FEVER sha pe=\"table\" target=\"#tab_2\">2</ref>). 19 of these teams scored higher than the baseline presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>. All participating teams were invited to s her individually or as a group, can be used as evidence. We retained the annotation guidelines from <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref> (see Sections A.7.1, A.7.3 and A.8 from th rom 86 submissions from 23 teams. 19 of these teams exceeded the score of the baseline presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>. For the teams which provided a system des. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b1\">[2]</ref> and thus results in the use of more sophisticated features <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> and regularised fil CF2 <ref type=\"bibr\" target=\"#b54\">[55]</ref> CSRDCF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, C-COT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, ECO <ref type=\"bibr\" target=\"#b3\">[4]</ref>, CREST <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>[8] <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta d 100003FA, which is often observed and utilized for prediction in the Markov prefetching algorithm <ref type=\"bibr\" target=\"#b9\">[10]</ref> . The following section discusses data prefetching methodol strides are recognizable. To capture repetitiveness in data reference addresses, Markov prefetching <ref type=\"bibr\" target=\"#b9\">[10]</ref> was proposed. This strategy assumes the history might repea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -item interactions. One kind of the methods is meta-learning <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  over a graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Considering a video frame sequence as a 'structured' grap ef>. GCNs have been used to address skeleton-based action recognition recorded using motion capture <ref type=\"bibr\" target=\"#b12\">[13]</ref>. The application of graph networks has also started emergi nown. A common way to define the elements in A is through constructing a distance function manually <ref type=\"bibr\" target=\"#b12\">[13]</ref>. However, this may result into a suboptimal graph represen s a binary adjacency matrix constructed following the method used in graph-based action recognition <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>In addition to the baselines, we compare with two s adjacency: a natural choice is a binary adjacency matrix as used for graph-based action recognition <ref type=\"bibr\" target=\"#b12\">[13]</ref>. This is defined as (A b ) ij = 1 if |i \u2212 j| = 1 and 0 oth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rrence of tokens (e.g. a single enemy or ground block) in existing game levels to identify patterns <ref type=\"bibr\" target=\"#b3\">(Dahlskog and Togelius 2012</ref>) and combined them using simple stat ref type=\"bibr\" target=\"#b11\">Khalifa et al. (2019)</ref>.</p><p>Super Mario Bros. Level Generation <ref type=\"bibr\" target=\"#b3\">Dahlskog and Togelius (2012)</ref> identified and analyzed patterns wi lined how those patterns could be combined and varied to create new levels. In their continued work <ref type=\"bibr\" target=\"#b3\">(Dahlskog and Togelius 2014)</ref>, they additionally defined micro-(v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eroshot learning using task descriptions <ref type=\"bibr\" target=\"#b30\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Puri and Catanzaro, 2019;</ref><ref type=\"bibr\" target=\"#b34\">Schick . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s (i.e.negative pairs) should be far away.</p><p>Experiments on three public DSRE benchmarks -NYT10 <ref type=\"bibr\" target=\"#b20\">(Riedel et al., 2010;</ref><ref type=\"bibr\" target=\"#b10\">Hoffmann et d the dataset statistics are listed in Table <ref type=\"table\" target=\"#tab_0\">1</ref>.</p><p>NYT10 <ref type=\"bibr\" target=\"#b20\">(Riedel et al., 2010)</ref> aligns Freebase entity relations with New. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: training a network that encodes a given image into a latent representation of the manipulated image <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ositions is challenging as there are symmetries which prevent canonical node positional information <ref type=\"bibr\" target=\"#b19\">(Murphy et al. 2019)</ref>. In fact, most of the GNNs which are train ormance on graph datasets. The issue of positional embeddings has been explored in recent GNN works <ref type=\"bibr\" target=\"#b19\">(Murphy et al. 2019;</ref><ref type=\"bibr\" target=\"#b34\">You, Ying, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref> and the trace cache architecture as proposed by Rotenberg, h prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref>. The branch prediction mechanism is a fully autonomous eng ion cache is then driven by the requests stored in the FTQ.</p><p>Another important contribution of <ref type=\"bibr\" target=\"#b29\">[30]</ref> is the Fetch Target Buffer (FTB). It extends the BTB by al ssibly containing multiple basic blocks.</p><p>The use of an FTQ is not novel, it was introduced in <ref type=\"bibr\" target=\"#b29\">[30]</ref>. It decouples the branch prediction from the memory access .0\"><head n=\"3.3.\">Fetch target queue</head><p>Following the proposal of Reinman, Austin and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref> we have decoupled the branch prediction stage from the ins ream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture <ref type=\"bibr\" target=\"#b29\">[30]</ref> using a perceptron branch predictor <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: been applied in the transductive setting with fixed graphs <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this work we both extend GCNs to the task of inductive  aph convolutional network (GCN), introduced by Kipf et al. <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN algorithm <ref type=\"bibr\" target=\"#b16\"> n O(|V|), so this requirement is not entirely unreasonable <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>.</p><p>Following Theorem 1, we let x v \u2208 U, \u2200v \u2208 V denote t  trained on a single, fixed graph. (That said, Kipf et al <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref> found that GCN-based approach consistently outperformed De. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cements in the computer vision community on very deep CNNs <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that have not been * Work done as Google Brain interns. exp  build such deeper models. NiN has seen great success in computer vision, building very deep models <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We show how to apply NiN principles in hierarchical Recur on that led to the success of very deep networks in vision <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion information to the model. These position encodings can be a deterministic function of position <ref type=\"bibr\" target=\"#b7\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b10\">Vaswani . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b19\">[20]</ref>, TIRAMISU <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Triton <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>In principle, accelerating primitives intends to op. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  advantage of these neighbors as input features of users and items, we use a neighbor-based encoder <ref type=\"bibr\" target=\"#b10\">[10,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" ta  networks (GCN). It can consider multi-hop neighbors as well based on a stack of GCN layers. \u2022 LGCN <ref type=\"bibr\" target=\"#b10\">[10]</ref>: The state-of-the-art method that further tailors the GCN- k based on the LGCN encoder. It computes the user/item representations by using the lightweight GCN <ref type=\"bibr\" target=\"#b10\">[10]</ref> that adopts the proposed neighbor augmentation technique (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hyperlinks, has also been explored recently <ref type=\"bibr\" target=\"#b27\">(Min et al., 2019b;</ref><ref type=\"bibr\" target=\"#b0\">Asai et al., 2020)</ref>. The use of dense vector representations for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mply adopt the early-fusion strategy by concatenating the input sequences from different modalities <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> or the late-fusion s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b40\">Ma et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Kosaraju et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Nathani et al., 2019;</ref><ref type=\"bibr\" target=\"#b61\">Wu et al.,  ibr\" target=\"#b40\">Ma et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Kosaraju et al., 2019;</ref><ref type=\"bibr\" target=\"#b42\">Nathani et al., 2019;</ref><ref type=\"bibr\" target=\"#b61\">Wu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Conventional studies concentrate on the area of multilingual r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> for a long time, these researches are commonly limited to mak uage dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM <ref type=\"bibr\" target=\"#b4\">[5]</ref> further explores long short-term memory (LSTM) <ref type=\"bi nder the condition of language information being known during training. A comparison with SHL-MLSTM <ref type=\"bibr\" target=\"#b4\">[5]</ref> with residual learning is investigated on CALL-HOME datasets .tei-c.org/ns/1.0\"><head n=\"4.4.\">Results</head><p>The baseline systems come from our previous work <ref type=\"bibr\" target=\"#b4\">[5]</ref> and all results are summarized in Table <ref type=\"table\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [2]</ref>). All of the models mentioned above are shallow networks (less than 5 layers). Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> first introduced the residual architecture for training mu ly concatenate together, which leads to the underutilization of local features. In 2016, Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed a residual learning framework (Fig. <ref type=\"fi r module, we design a set of comparative experiments to compare the performance with residual block <ref type=\"bibr\" target=\"#b11\">[12]</ref>, dense block <ref type=\"bibr\" target=\"#b23\">[24]</ref>   < figDesc>Fig. 5. Quantitative comparison of three different feature extraction blocks (residual block<ref type=\"bibr\" target=\"#b11\">[12]</ref>, dense block<ref type=\"bibr\" target=\"#b23\">[24]</ref>, and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aborating the works by adapting attention mechanism to GNNs<ref type=\"bibr\" target=\"#b33\">[33,</ref><ref type=\"bibr\" target=\"#b54\">54,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  presented an intelligent video-based system for automated detection of suicide by hanging attempts <ref type=\"bibr\" target=\"#b1\">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  regions at multiple scales of the original image.</p><p>Lastly, inspired by the recent Transformer <ref type=\"bibr\" target=\"#b22\">[23]</ref> model in dealing with a number of difficult NLP tasks such idal way, following by corresponding pooling strategy. Moreover, inspired by the recent Transformer <ref type=\"bibr\" target=\"#b22\">[23]</ref> model in dealing with a number of difficult NLP tasks such iation between these local regions are still not explored. Inspired by the recent Transformer model <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we implant a self attention module adapted to image featu g sentiment class number of datasets, and then tunes the parameters of all layers. \u2022 Self-Attention <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a variant derived from the Transformer model. Transform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  black blob). <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bib bibr\" target=\"#b55\">[56]</ref>, as well as squeeze and excitation (SE) block presented by Hu et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The proposed Res2Net module introduces the scale dimensio sily integrate the cardinality dimension <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the SE block <ref type=\"bibr\" target=\"#b24\">[25]</ref> with the proposed Res2Net module.</p></div> <div xmlns=\"ht mension cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> (replace conv with group conv) and SE <ref type=\"bibr\" target=\"#b24\">[25]</ref> blocks.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\" calibrates channel-wise feature responses by explicitly modelling inter-dependencies among channels <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Similar to <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we  y modelling inter-dependencies among channels <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Similar to <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we add the SE block right before the residual connections ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, SE-Net <ref type=\"bibr\" target=\"#b24\">[25]</ref>, bLResNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and DLA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b7\">8]</ref> or high-order interactions <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, as shown in Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>. .1361 PNN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=\"b crease at 10 \u22123 level in Criteo dataset is already clear compared with recent works such as xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> and DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Sinc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e mainstream strategy resorts to local shape context encoded by geometry histogram and its variants <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m, let q i , k i , v i stand for the i-th row in Q, K, V respectively. Following the formulation in <ref type=\"bibr\" target=\"#b28\">(Tsai et al. 2019)</ref>, the i-th query's attention is defined as a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et. LCSTS is a public Chinese short text summary generation data set constructed by Hu Chen in 2015 <ref type=\"bibr\" target=\"#b29\">[30]</ref>, which contains 2.4 million real Chinese short text data a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ut <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>When the graph structure of the input is known, <ref type=\"bibr\" target=\"#b1\">[2]</ref> introduced a model to generalize ConvNets using low learning r parameters. Our main contributions can be summarized as follows:</p><p>\u2022 We extend the ideas from <ref type=\"bibr\" target=\"#b1\">[2]</ref> to large-scale classification problems, specifically Imagene vised fashion. However, it does not attempt to exploit any weight-sharing strategy.</p><p>Recently, <ref type=\"bibr\" target=\"#b1\">[2]</ref> proposed a generalization of convolutions to graphs via the  v xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Spectral Networks</head><p>Our work builds upon <ref type=\"bibr\" target=\"#b1\">[2]</ref> which introduced spectral networks. We recall the definition mula_4\">\u2202 k x(\u03be) \u2202\u03be k \u2264 C |u| k |x(u)|du ,</formula><p>where x(\u03be) is the Fourier transform of x. In <ref type=\"bibr\" target=\"#b1\">[2]</ref> it was suggested to use the same principle in a general grap ction</head><p>Whereas some recognition tasks in non-Euclidean domains, such as those considered in <ref type=\"bibr\" target=\"#b1\">[2]</ref> or <ref type=\"bibr\" target=\"#b11\">[12]</ref>, might have a p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b47\">Tang et al. 2013;</ref><ref type=\"bibr\" target=\"#b31\">Lee et al. 2016;</ref><ref type=\"bibr\" target=\"#b14\">Chikka 2016</ref>) such as SVMs, Max-Ent and CRFs, and neural network. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  two major end-to-end ASR implementations based on both connectionist temporal classification (CTC) <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  nodes is at the center of much network analysis and corresponds to homophily or assortative mixing <ref type=\"bibr\" target=\"#b27\">(McPherson et al., 2001;</ref><ref type=\"bibr\" target=\"#b30\">Newman, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ccess, to generate a speculative value that does not necessarily exhibit value locality (e.g., DLVP <ref type=\"bibr\" target=\"#b2\">[3]</ref>). While value predictors can generate speculative results fo rk has shown that load-only predictors are most efficient with a modest hardware budget (e.g., 8KB) <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>In this stud r\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref> Context Address Prediction (CAP) <ref type=\"bibr\" target=\"#b2\">[3]</ref> one another. We found that no individual predictor is strict e focus only on predicting load values since that is most effective with limited hardware resources <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xml sm needed to communicate the predicted values from the value-predicted producers to their consumers <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Consumers of the load can use the prediction by reading the s practical implementations of value prediction, we encourage the readers to visit prior art papers <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t about the baseline ISA, microarchitecture, and storage constraints (Sheikh reports similar findings <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>).</p><p>In all of \"#b7\">[8]</ref>, and subsequent work confirmed the same is true for load instructions in particular <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>Our implemen an directly generating values from We use the state-of-the-art DLVP predictor as a reference design <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The predictor consists of one tagged table indexed by a has. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t al. 2017;</ref><ref type=\"bibr\" target=\"#b6\">B\u00e9rard et al. 2018</ref>) and pretraining techniques <ref type=\"bibr\" target=\"#b3\">(Bansal et al. 2019)</ref> have been applied to end-to-end ST model to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: amples from the approximate posterior through Monte Carlo sampling. Interestingly, it was showed in <ref type=\"bibr\" target=\"#b9\">(Gal and Ghahramani 2016)</ref> that the dropout inference minimizes t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ks has drawn increasing research attention in the community, and various methods have been proposed <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Among them, ConOu ef type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Among them, ConOut <ref type=\"bibr\" target=\"#b21\">[22]</ref> identifies the local context for each node and performs an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s <ref type=\"bibr\" target=\"#b14\">[15]</ref>, dynamic predication based on frequently executed paths <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and predicate prediction <ref type=\"bibr\" target=\"#b25\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  NICs use an embedded CPU core to orchestrate the processing of a packet across offloaded functions <ref type=\"bibr\" target=\"#b30\">[34]</ref>. This is because the on-chip network cannot parse complex   avenue for future work once the DPU is generally available.</p><p>PANIC is also similar to FairNIC <ref type=\"bibr\" target=\"#b30\">[34]</ref>, which improves fairness between competing applications ru. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ncoder learnt by IB tends to be more robust and transferable. Recently, IB has been applied to GNNs <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>. But IB needs the   representation is. Recently, the information bottleneck has applied to learn graph representations <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>. Specifically, the ormation also makes GNNs trained w.r.t. GIB robust to adverserial attack and strongly transferrable <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b48\">48]</ref>.</p><p>Unfortunate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d relation extraction <ref type=\"bibr\">(DSRE)</ref>.</p><p>With awareness of the existing DS noise, <ref type=\"bibr\" target=\"#b27\">Zeng et al. (2015)</ref> introduces the multi-instance learning (MIL). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gain feature maps for the graph kernel. Recently, Bai et al. <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> built a graph kernel based on multilayer representation as   G 2 . The corresponding condition is recorded by exploiting <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> C(i, j) = 1 if R (i, j) is the smallest element both in row rnel is connected with the layered representation defined in <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> . Nevertheless, there exists two distinctions. Firstly, the , there exists two distinctions. Firstly, the graphs in <ref type=\"bibr\" target=\"#b2\">[3]</ref> and <ref type=\"bibr\" target=\"#b30\">[31]</ref> were decomposed into Shannon entropy substructures with gr lop a method of point set matching. However, Bai et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> and <ref type=\"bibr\" target=\"#b30\">[31]</ref> adopted the Euclidean distance between the multilayer Shan <ref type=\"bibr\" target=\"#b6\">[7]</ref> , (3) the graph matching kernel with Shannon entropy (GMKS) <ref type=\"bibr\" target=\"#b30\">[31]</ref> , (4) the graph matching kernel with approximated von Neum rget=\"#b30\">[31]</ref> , (4) the graph matching kernel with approximated von Neumann entropy (GMKV) <ref type=\"bibr\" target=\"#b30\">[31]</ref> , <ref type=\"bibr\" target=\"#b4\">(5)</ref> the Jensen-Shann are our SREGK method with graph kernel from the deep representation based on Shannon entropy (GMKS) <ref type=\"bibr\" target=\"#b30\">[31]</ref> , since they are both graph kernels based on the depth-bas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ww.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>The task of Grounded video description (GVD) <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> aims to generate more grounded and accurate  existing works either encode region proposals independently or using selfattention-based mechanisms <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>. Therefore, it either fails to consider impl a]</ref>, many works model the video in both global video features and regional object features. In <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>, they encode the objects with transformer <r /head><p>We model the video's global level feature by a Bi-directional LSTM network like most works <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ... ose a novel visual representation method from the perspective of regions. First of all, inspired by <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>, we enhance the proposal features by adding  <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Feature Enhancement</head><p>In this part, we follow <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>'s work, which fusing the spatial-temporal an  feature aggregation on the enhanced feature R.</p><p>We adopt the same classification loss just as <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> do denoted as L cls .</p></div> <div xmlns=\" > t , h f rame + h attention ). h t is used to generate descriptions. We adopt the same MLE loss as <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> which denoted by L sent .</p><p>Finally, the ad n=\"4.1\">Dataset</head><p>We conduct our experiments on the Grounded ActivityNet-Entities Dataset <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> for evaluation. It contains 15k video with 1 ph2Seq method. Data processing. For a fair comparison, the data processing procedure is the same to <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we u  2018]</ref>, BiM-STM+TempoAtnn <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> on Grounded ActivityNet Captions Dataset to  emove the hierarchical attention and replace it with the coarsegrain proposal attention proposed by <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>.</p><p>Table <ref type=\"table\" target=\"#tab_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hallenges above, we propose to model the attributed networks with graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b15\">[16]</ref>. GCN, which takes the topological structure and nodal attr se a new type of attributed network encoder inspired by the graph convolutional network (GCN) model <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Specifically, GCN considers the high-order node proximity a particular layer, the convolution operation is D \u2212 1 2 A D \u2212 1 2 XW, and its complexity is O(mdh) <ref type=\"bibr\" target=\"#b15\">[16]</ref> as AX can be efficiently implemented using sparse-dense ma rning performance by considering neighbors of nodes that are multiple hops away. In particular, GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> takes the structure and attribute information as input, an  autoencoder architecture. Meanwhile, recent research advances on graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: variants of the architecture have been applied to parsing and other tasks requiring tree structures <ref type=\"bibr\" target=\"#b2\">(Blunsom et al., 2014)</ref>. However, the effectiveness of character-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Previous work has mostly focused on the diversity of each individual output using Dist-1,2,3 scores <ref type=\"bibr\" target=\"#b27\">(Li et al., 2016a)</ref> to measure repetitions within a single gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tailed statistics is summarized in Table <ref type=\"table\" target=\"#tab_1\">2</ref>.</p><p>\u2022 Wiki-CS <ref type=\"bibr\" target=\"#b24\">[25]</ref> is a reference network constructed from Wikipedia.</p><p>T ures. For the Wiki-CS dataset, we evaluate the models on the public splits shipped with the dataset <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Regarding the other four co-coauthor and co-purchase data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: omes essential. Scaling has proven effective in terms of obtaining larger models with good accuracy <ref type=\"bibr\" target=\"#b33\">[31]</ref>. However, existing work on model scaling focuses on model  arget=\"#b13\">[11]</ref>. More recently scaling multiple dimensions at once, coined compound scaling <ref type=\"bibr\" target=\"#b33\">[31]</ref>, has been shown to achieve excellent accuracy.</p><p>Exist GF (gigaflops), and find it uses less memory and is faster (and more accurate) than EfficientNet-B4 <ref type=\"bibr\" target=\"#b33\">[31]</ref> -a model with 4\u00d7 fewer flops.</p><p>In order to facilitate imension is equal. Scaling uniformly along all dimensions, which closely resembles compound scaling <ref type=\"bibr\" target=\"#b33\">[31]</ref>, results in near linear scaling of activations.</p><p>Comm on, an intuitive approach is to scale along multiple dimensions at once. Coined compound scaling by <ref type=\"bibr\" target=\"#b33\">[31]</ref>, such an approach has been shown to achieve higher accurac , we scale g proportionally to w. For networks that use depthwise conv (g = 1), as in previous work <ref type=\"bibr\" target=\"#b33\">[31]</ref>, we do not scale g. Finally, we note that when scaling g,  line networks. In this work we evaluate scaling strategies on three networks families: EfficientNet <ref type=\"bibr\" target=\"#b33\">[31]</ref>, Reg-NetY <ref type=\"bibr\" target=\"#b24\">[22]</ref>, and R ur scaling experiments. Moreover, Ef-ficientNet was introduced in the context of model scaling work <ref type=\"bibr\" target=\"#b33\">[31]</ref>, making it an excellent candidate for our study.</p><p>Eff [30]</ref> and scaled to larger sizes (B1-B7) via compound scaling. For further details, please see <ref type=\"bibr\" target=\"#b33\">[31]</ref>.</p><p>Note that EfficientNets are specified by \u223c30 parame [22]</ref>) versus a strong setup that yields good results but may be difficult to reproduce (e.g., <ref type=\"bibr\" target=\"#b33\">[31]</ref>). To address this, we use a training setup that effectivel icientNet reproduction. The first set of results includes the originally reported errors (from ICML <ref type=\"bibr\" target=\"#b33\">[31]</ref> and updated numbers later reported on arXiv), the second s e biggest nets they slightly lag the updated arXiv errors). We emphasize that unlike the results in <ref type=\"bibr\" target=\"#b33\">[31]</ref>, we use the same, easy to reproduce optimization setup for e=\"table\">5</ref>, we report Efficient-Net results using our optimization setup versus results from <ref type=\"bibr\" target=\"#b33\">[31]</ref>. We report our results using a '1\u00d7', '2\u00d7', or '4\u00d7' schedul nce, we also show the original EfficientNet models (orig) obtained via non-uniform compound scaling <ref type=\"bibr\" target=\"#b33\">[31]</ref>, the results closely match uniform compound scaling (dwr). 6.2.\">Simple and Compound Scaling</head><p>We now turn to evaluation of simple and compound scaling <ref type=\"bibr\" target=\"#b33\">[31]</ref> described in \u00a73.3 and \u00a73.4, respectively. For these experi <p>We also compare uniform compound scaling (dwr) to the original compound scaling rule (orig) from <ref type=\"bibr\" target=\"#b33\">[31]</ref>, which empirically set the per-dimension scalings factors. ence, we also show the original EfficientNet models (orig) obtained via non-uniform compound scaling<ref type=\"bibr\" target=\"#b33\">[31]</ref>, the results closely match uniform compound scaling (dwr). nsion and by 3 \u221a s 3 = s in total.Interestingly, the compound scaling rule discovered empirically in<ref type=\"bibr\" target=\"#b33\">[31]</ref> scaled by 1.2, 1.1, and 1.15 along d, w, and r, which corr  top results compared to deeper or higher-resolution models <ref type=\"bibr\" target=\"#b11\">[9,</ref><ref type=\"bibr\" target=\"#b33\">31]</ref>.</p><p>To address this, in this work we introduce the conce. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s (iDT). Dense Trajectories <ref type=\"bibr\" target=\"#b0\">[1]</ref> and improved Dense Trajectories <ref type=\"bibr\" target=\"#b1\">[2]</ref> were introduced for action recognition with considerable per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rt the video transcripts into the pre-trained Glove model to obtain 300-dimensional word embeddings <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For the visual modality, we process the video frames by F. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Multimodal Transformer (MulT) approach to fuse crossmodal information from unaligned data sequences <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Their approach introduces the modality reinforcement unit lly determine the passed proportions of the reinforced features. Compared with the prior MulT model <ref type=\"bibr\" target=\"#b17\">[18]</ref>, the advantage of our approach lies in two aspects. First, al. propose the crossmodal attention mechanism to learn the inherent correlations across modalities <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Their approach repeatedly reinforces one modality with in with information from a source modality by learning the directional pairwise attention between them <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Denote by X s \u2208 R Ts\u00d7ds the data sequence from the source 1.0\"><head n=\"3.3.\">Model overview</head><p>Our model is trained in an end-to-end manner. Following <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we use a 1D temporal convolutional layer to process the i  \ud835\udc3f\u2192\ud835\udc36 \ud835\udc4d [\ud835\udc56]   \ud835\udc49\u2192\ud835\udc36 \ud835\udc4d [\ud835\udc56+1]   \ud835\udc36 MUM [\ud835\udc56]   CA mul   where C\u2192 * unit. Compared with the prior MulT model <ref type=\"bibr\" target=\"#b17\">[18]</ref> which reinforces the target modality by repeatedly attendi Translation Network (MCTN) <ref type=\"bibr\" target=\"#b12\">[13]</ref>, Multimodal Transformer (MulT) <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Of these, MulT and LF-LSTM can be applied directly to the ns/1.0\"><head n=\"4.1.\">Experimental setup</head><p>We follow the common protocol of the prior works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tailment to yes, disagreement to no and neutral to maybe.</p><p>Given a premise p, the task in COPA <ref type=\"bibr\" target=\"#b28\">(Roemmele et al., 2011)</ref> is to determine the cause or effect of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rg/ns/1.0\"><head>E. Comparison to other Graph Processors</head><p>The Cray Urika-GD graph processor <ref type=\"bibr\" target=\"#b7\">[8]</ref> was one of the first commercial graph-oriented big data proc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. To generate diversified results, these methods either exp ef type=\"bibr\" target=\"#b7\">[8]</ref>, HxQuAD/HPM2 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>Those existing approaches used greedy document sequ strategy may not lead to global optimal rankings. Based on the reinforced learning approach MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Feng <ref type=\"bibr\" target=\"#b14\">[15]</ref> proposed t rmula><p>Here \ud835\udc98 \ud835\udc5f is a learnable parameter. We use the same relevance features as the previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref> for \ud835\udc99 \ud835\udc5e and \ud835\udc99 \ud835\udc5e \ud835\udc56 , including BM25, TF-IDF, language model bers of incoming links and outgoing links, et al. More details about these features can be found in <ref type=\"bibr\" target=\"#b13\">[14]</ref> and we omit the details due to space limitation. In the fu  used are actually the document embeddings. We use the subtopic embeddings released by Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> based on doc2vec. The subtopic embeddings is produced from t diversification task is limited, we inherit the list-pairwise sampling approach from Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> in order to get enough training samples. We are using pair ance features and embeddings exactly the same as the DSSA, which have been released by Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> in the repository on GitHub<ref type=\"foot\" target=\"#foot_ =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>, all those metrics are computed on top 20 results of a doc \"#b11\">[12]</ref> and PAMM-NTN <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Inspired by previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we use the metric of \ud835\udefc \u2212 \ud835\udc5bDCG@20 to tune the parameters.   100-dimensional vectors generated by the LDA <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>DSSA <ref type=\"bibr\" target=\"#b13\">[14]</ref>. We train the DSSA model with the code and data released b lt is denoted as DSSA (doc2vec).</p><p>Since the deep reinforced learning based models e.g. MDP-DIV <ref type=\"bibr\" target=\"#b13\">[14]</ref> and M2DIV <ref type=\"bibr\" target=\"#b14\">[15]</ref> are ta target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> (i.e., explicit approaches), or directly reduce result redu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <p>Identifying compound-protein interaction (CPI) plays an import role in discovering hit compounds <ref type=\"bibr\" target=\"#b39\">(Vamathevan et al., 2019)</ref>. Conventional methods, such as struct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  loss formulation.</p><p>The first two terms are similar to those in prior self-training literature <ref type=\"bibr\" target=\"#b51\">(Xie et al., 2020)</ref>. However, while in prior self-training work, RTUP and train a linear classifier on the support set and evaluate the classifier on the query set. <ref type=\"bibr\" target=\"#b51\">Xie et al. (2020)</ref> found that training the student from scratch  \" xml:id=\"tab_1\"><head></head><label></label><figDesc>STARTUP ADDS NOISE WHICH INCREASES ROBUSTNESS.<ref type=\"bibr\" target=\"#b51\">Xie et al. (2020)</ref> posit that self-training introduces noise whe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in visual representations, there has been a recent surge in self-supervised representation learning <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta mploys proxy tasks to guide the learned embeddings, such as predicting the angle of a rotated image <ref type=\"bibr\" target=\"#b18\">[19]</ref>, the relative location of patches <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b45\">Ying et al. 2018b;</ref><ref type=\"bibr\" target=\"#b15\">Hasanzadeh et al. 2019;</ref><ref type=\"bibr\" target=\"#b30\">Qu, Bengio, and Tang 2019;</ref><ref type=\"bibr\" target=\"#b29\">Pei et \" target=\"#b41\">(Vilalta and Drissi 2002;</ref><ref type=\"bibr\" target=\"#b38\">Vanschoren 2018;</ref><ref type=\"bibr\" target=\"#b30\">Peng 2020</ref>). Among previous works on meta-learning, metric-based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to guide the pixel denoiser; in contrast, our denoising is applied directly on features. Guo et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> transform the images via non-differentiable image preprocess ts of their non-differentiable computations <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In contrast to <ref type=\"bibr\" target=\"#b7\">[8]</ref>, our feature denoising models are differentiable, but are st y increases as the image is propagated through the network <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, and non-existing activations in the feature maps are halluci. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch cannot be simply integrated like other auxiliary information in multiple feedback recommendation <ref type=\"bibr\" target=\"#b5\">[Ding et al., 2018b;</ref><ref type=\"bibr\" target=\"#b6\">Gao et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have demonstrated their effectiveness in classifying node l  and employ several popular GNN models including GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref>, GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>, SAGE <ref type=\"bibr\" target=\"#b6\">[7]</ref>, APPNP <ref  s a variant of convolutional neural networks that operates on graphs. Graph Attention Network (GAT) <ref type=\"bibr\" target=\"#b28\">[29]</ref> further employed attention mechanism in the aggregation of ramework can be an arbitrary GNN model such as GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> or GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>. We denote the pretrained classifier in a teacher model as ssification accuracies with teacher models as GCN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>. \u2022 Citeseer <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a o the number of layers and we employ the most widely-used 2-layer setting in this work.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref> improves GCN by incorporating attention mechanism which as ze, 0.01 as learning rate, 0.8 as dropout probability and 0.001 as learning rate decay.</p><p>\u2022 GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>: we use 64 as hidden-layer size, 0.01 as learning rate, 0.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as one way to address these inefficiencies <ref type=\"bibr\" target=\"#b1\">(Berman et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Bender et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Brock et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b42\">Toutanova et al., 2015;</ref><ref type=\"bibr\" target=\"#b48\">Xiong et al., 2019;</ref><ref type=\"bibr\" target=\"#b39\">Sun et al., 2019;</ref><ref type=\"bibr\" target=\"#b47\">Wang et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b31\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">Lewis et al., 2020</ref>), yet they are far from perfect. In generati eration framework, built upon the pre-trained sequence-to-sequence (seq2seq) Transformer model BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref>. As shown in Figure <ref type=\"figure\" tar  et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lawrence et al., 2019)</ref>. Our work uses BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref>, a state-of-the-art seq2seq model that off U card with 24 GB memory.</p><p>Model Sizes. Our generation model has the same architecture as BART <ref type=\"bibr\" target=\"#b27\">(Lewis et al., 2020)</ref> with 406M parameters. The content planner . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[37]</ref> uses semantic segmentation for video deblurring. Zhu et al. <ref type=\"bibr\" target=\"#b51\">[52]</ref> propose an approach to generate new clothing on a wearer.  nal bias at the input layer.</p><p>2) Compositional mapping -This method is identical to Zhu et al. <ref type=\"bibr\" target=\"#b51\">[52]</ref>. It decomposes an LR image based on the predicted semantic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -supervised learning <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b77\">76,</ref><ref type=\"bibr\" tar contrastive learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b46\">45,</ref><ref type=\"bibr\" tar /ref><ref type=\"bibr\" target=\"#b77\">76,</ref><ref type=\"bibr\" target=\"#b79\">78]</ref>. Iscen et al. <ref type=\"bibr\" target=\"#b34\">[33]</ref> mine hard negatives from a large set by focusing on the fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, and on using adversarial learning <ref type=\"bibr\" target=\"#b25\">[26]</ref>. A few works have proposed to use attention networks to ac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y predicted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy ibr\" target=\"#b15\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Lample et al., 2016)</ref>, using LSTM-CRF as the main network struct epresentations Both character CNN <ref type=\"bibr\" target=\"#b27\">(Ma and Hovy, 2016)</ref> and LSTM <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016)</ref> have been used for representing the chara. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d by deep learning based speech enhancement and separation <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar  as in <ref type=\"bibr\" target=\"#b20\">[21]</ref>, proposed in the same conference. A follow-up work <ref type=\"bibr\" target=\"#b18\">[19]</ref> of <ref type=\"bibr\" target=\"#b22\">[23]</ref> supplies clea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: stic depth for the Transformer inspired by the Stochastic Residual Network for image classification <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>We discovered that its ability to regularize is the  r\" target=\"#b15\">[16]</ref>, and thus there are redundant layers. Motivated by the previous work of <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we propose to apply stochastic residual layers into our Tr  sub-layers inside). This way we have one hyper-parameter p for each layer.</p><p>\u2022 As suggested by <ref type=\"bibr\" target=\"#b9\">[10]</ref>, the lower layers of the networks handle raw-level acoustic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rious heuristic approaches have been proposed to improve the the robustness to adversarial examples <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, such heuristics are often broken by new attack m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: entation learning algorithms that use a contrastive loss have outperformed even supervised learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ rvation that a larger number of negative/positive examples in the objective leads to better results <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. The last two terms  </ref>, or different views of the same scene <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Chen et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> extensively study verious data augmentation methods. For lan br\" target=\"#b22\">[23]</ref> and STL10 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with ResNet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as  ef type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. Following <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we set the temperature t = 0.5 and the dimension of the lat /ns/1.0\"><head>B Experiment Details</head><p>Cifar10 and STL10 We adopt PyTorch to implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with Resnet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rg/ns/1.0\"><head n=\"1\">Introduction</head><p>Self attention mechanisms, represented by Transformers <ref type=\"bibr\" target=\"#b0\">[1]</ref>, have driven the advancement of various machine learning pro ref type=\"bibr\" target=\"#b29\">[30]</ref>, and follow a standard warmup learning rate schedule as in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We use an initial learning rate of 3 \u00d7 10 \u22123 a weight decay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p><p>Query-based black-box attacks can settle the suscept >32]</ref> or images <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. More related to our work is the regularization-based appro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\">(Zitnik &amp; Leskovec, 2017;</ref><ref type=\"bibr\" target=\"#b18\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b47\">Subramanian et al., 2005)</ref> is a well-known benchmark in the indu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trics</head><p>Following previous literature <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Vashishth et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Alt et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: equences that pass the sequence quality screening. For screening, we utilize a language model GPT-2 <ref type=\"bibr\" target=\"#b28\">(Radford et al., 2019)</ref> to score sequence x by computing its per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  intent graph. Motivated by the powerful aggregating capability of the graph convolutional networks <ref type=\"bibr\" target=\"#b17\">[18]</ref> (GCN), we adapt GCN to this dynamic intent graph for learn  representation of query and items in the product search.</p><p>Graph convolutional networks (GCNs) <ref type=\"bibr\" target=\"#b17\">[18]</ref> can collect the neighbors' information by generalizing tra  fields, such as computer vision <ref type=\"bibr\" target=\"#b10\">[11]</ref>, social network analysis <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and natural langua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For example, DeepMusic <ref type=\"bibr\" target=\"#b27\">[28]  collaborative space <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and leverage deep cross-network structure to capture highe \" target=\"#b29\">30]</ref>. For example, DeepMusic <ref type=\"bibr\" target=\"#b27\">[28]</ref> and CDL <ref type=\"bibr\" target=\"#b29\">[30]</ref> were proposed to incorporate content data into deep neural. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ring posterior and prior distributions for the gating function. There are also concurrent MoE works <ref type=\"bibr\" target=\"#b40\">(Ma et al., 2018;</ref><ref type=\"bibr\" target=\"#b46\">Qin et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: with a conversion model or a pre-trained model for speaker information extraction, such as i-vector <ref type=\"bibr\" target=\"#b21\">[22]</ref>, dvector <ref type=\"bibr\" target=\"#b22\">[23]</ref>, or x-v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b49\">(Rogers et al., 2020)</ref>, which specifically studies the BERT model <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>.</p><p>In this work, we adapt and extend   In NLP, transformers are the backbone of state-of-the-art pre-trained language models such as BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>. BERTology focuses on interpreting what t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  increasing of depth brings benefits to representation power <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and LSTMbased approaches <ref type=\"bibr\" target=\"#b30\">[31]</ref>. These approaches all consider relations lying in a single. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s by aggregating their features. Both DeepGL and GraphSage are designed for homogeneous graphs. LAN <ref type=\"bibr\" target=\"#b14\">[15]</ref> aggregates neighbors with both rule-based and network-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bust training (e.g. <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>), we tackle various additional challenges: Being the first  arget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> providing guarantees that no perturbation w.r.t. a specific ss of methods based on convex relaxations are of relevance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. They construct a convex relaxation for computing a lower b nity-norm or L2-norm <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, often e.g. \u03f5 &lt; 0.1 This is clearly not practical in our  can often been done efficiently, and by exploiting duality it enables to even train a robust model <ref type=\"bibr\" target=\"#b19\">[20]</ref>. As already mentioned, our work differs significantly from  the ReLU activation function. While there are many ways to achieve this, we follow the approach of <ref type=\"bibr\" target=\"#b19\">[20]</ref> in this work. The core idea is (i) to treat the matrices H  makes this approach rather slow. As an alternative, we can consider the dual of the linear program <ref type=\"bibr\" target=\"#b19\">[20]</ref>. There, any dual-feasible solution is a lower bound on the  appendix. Note that parts of the dual problem in Theorem 4.3 have a similar form to the problem in <ref type=\"bibr\" target=\"#b19\">[20]</ref>. For instance, we can interpret this dual problem as a bac akes the computation of robustness certificates extremely fast. For example, adopting the result of <ref type=\"bibr\" target=\"#b19\">[20]</ref>, instead of optimizing over \u2126 we can set it to</p><formula o this). While there exist more computationally involved algorithms to compute more accurate bounds <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we leave adaptation of such bounds to the graph domain fo odes in the graph. y * t denotes the (known) class label of node t.</p><p>To improve robustness, in <ref type=\"bibr\" target=\"#b19\">[20]</ref> (for classical neural networks) it has been proposed to in oblem of the above linear program is max</p><p>for l = 2, . . . L, (n, j) \u2208 I (l )</p><p>As done in <ref type=\"bibr\" target=\"#b19\">[20]</ref> we can exploit complementarity of the ReLU constraints cor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t item embeddings. We adopt the dynamic setting in our model, more details will be described in \u00a73. <ref type=\"bibr\" target=\"#b3\">4</ref>.</p><p>The basic idea of our work is to learn a recommendation egularization is also included to avoid coincidental high similarities between rarely visited items <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. \u2022 BPR-MF: BPR-MF <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st of the paper.</p><p>\u2022 Graph inception. We extend the idea of inception layer in traditional CNNs <ref type=\"bibr\" target=\"#b43\">[44]</ref> to the graph domain, and introduce a graph inception modul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  bounding boxes around targets. In addition, LiDAR methods <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t etworks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the upgrade version of <ref type=\"bibr\" target=\"#b4\">[5]< ted examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are 70400 (200 \u00d7 176 \u00d7 2) predictions in each frame.  scade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Point  official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and Cascade R-CNN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, dation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PointPillars <ref type=\"bibr\" target=\"#b8\">[9]</ref> pub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ile feature maps generated by the decoder contain high-level and coarse-gained semantic information <ref type=\"bibr\" target=\"#b14\">[15]</ref>. And skip connections, which combine the low-level and hig ive method to boost the semantic extraction ability of encoder-decoder frameworks.</p><p>In U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>, plain skip connections are substituted by nested and dens =\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The major C. Duan is with the State Key Laboratory of Inf =\"bibr\" target=\"#b20\">[21]</ref>, FGC <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MSFCN, and U-Net++ <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Excluding SegNet, DeepLab V3 and DeepLab V3+, the remaini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f> to team collaboration networks <ref type=\"bibr\" target=\"#b52\">[53]</ref>, from citation networks <ref type=\"bibr\" target=\"#b31\">[32]</ref> to molecular graphs <ref type=\"bibr\" target=\"#b43\">[44]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 30\">(Sagot, 2008)</ref>, Italian <ref type=\"bibr\" target=\"#b19\">(Magnini et al., 1994)</ref>, Dutch <ref type=\"bibr\" target=\"#b28\">(Postma et al., 2016)</ref>, Polish <ref type=\"bibr\" target=\"#b27\">(P. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  treebanks. Each word is assigned one of 17 universal POS tags. For NER, we use the Wikiann dataset <ref type=\"bibr\" target=\"#b39\">(Pan et al., 2017)</ref>. (iii) Question answering includes three tas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: exponentially increasing dependency of nodes over layers; a phenomenon framed as neighbor explosion <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>. Due to neighbor explosion and since th hen et al., 2018b)</ref>. VR-GCN aims to reduce the variance in estimation during neighbor sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, and avoids the need to sample a large   memory usage of GCN+GAS training with the memory usage of full-batch GCN, and mini-batch GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and CLUSTER-GCN <ref type=\"bibr\" target as they will run out of memory on common GPUs. We compare with 10 scalable GNN baselines: GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, FASTGCN <ref type=\"bibr\" target=\"#b2\"> a &amp; Tang, 2020;</ref><ref type=\"bibr\" target=\"#b32\">Rong et al., 2020)</ref>: Nodewise sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b45\">(Sch\u00fctt et al., 2017;</ref><ref type=\"bibr\" target=\"#b27\">Klicpera et al., 2020;</ref><ref type=\"bibr\" target=\"#b43\">Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Hu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ication citation MiCo <ref type=\"bibr\" target=\"#b13\">[14]</ref> 96638 1080156 Co-authorship Patents <ref type=\"bibr\" target=\"#b25\">[26]</ref> 3.8M 16.5M US Patents LiveJournal-1 <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graphs, and they can be important in molecular learning, such as bond lengths, angles between bonds <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017;</ref><ref type=\"bibr\" target=\"#b29\">Klicpera et ethods is in early stage, and existing studies focus on leveraging different geometries. The SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref> incorporates the distance information dur al., 2017)</ref>. Baseline methods include <ref type=\"bibr\">PPGN (Maron et al., 2019)</ref>, SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref>, PhysNet (Unke &amp; Meuwly, 2019), Cormo e models employ a joint loss of forces and conserved energy during training. In the original SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref> and DimeNet <ref type=\"bibr\" target=\"#b29 ine the expressive power of SphereNet for molecular dynamics simulations. Following the settings in <ref type=\"bibr\" target=\"#b41\">Sch\u00fctt et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b29\">Klicpera e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ei-c.org/ns/1.0\"><head n=\"4.1\">Stressor Event and Subject Dictionaries</head><p>The word embeddings <ref type=\"bibr\" target=\"#b8\">[Mikolov et al., 2013]</ref> have been found effective in estimating t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: While there is research work that has used court trial transcripts to identify deceptive statements <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we are not aware of any previous work that took into cons cusing on real-life high-stake data. The work closest to ours is presented by Fornaciari and Poesio <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which targets the identification of deception in statemen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures across languages which can be mapped to the same space <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Authors in <ref type=\"bibr\" target=\"#b11\">[12]</ref> looke. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, LaSOT   <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DTB70 <ref type=\"bibr\" target=\"#b20\">[21]</ref>, UAV123 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, VOT2018 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, LaSOT . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es, when present, also play a vital role. The presence of visual cues improves speech comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>. First, the extracted multi-channel features maps, which m ST <ref type=\"bibr\" target=\"#b55\">[56]</ref>, MCPF <ref type=\"bibr\" target=\"#b56\">[57]</ref>, STRCF <ref type=\"bibr\" target=\"#b17\">[18]</ref>, LADCF <ref type=\"bibr\" target=\"#b12\">[13]</ref>, SiamFC <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ifferent domains require specialized normalization methods. In computer vision, batch normalization <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a standard component. While in natural language process h set of feature values the normalization is applied to. For example, in computer vision, BatchNorm <ref type=\"bibr\" target=\"#b15\">[16]</ref> is the de facto method that normalizes the feature values  ring testing, the estimated dataset-level statistics are used instead of the batch-level statistics <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>In GNNs, for each feature dimension, the BatchNorm  -invariant\" property <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In comparison, BatchNorm in the lower branch suffers from  e-invariant\" property<ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In comparison, BatchNorm in the lower branch suffers from  ed to improve the training process in different applications <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar h of the parameters; <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> show that the norma rom BatchNorm layers under different settings of batch sizes <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b31\">32</ref>, 64], as in Figure <  matrix in layer k. We apply the normalization after the linear transformation as in previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta hich try to provide accurate estimations for the mean and standard deviation over the whole dataset <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion for logical reasoning and decision-making. Experiments <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> already showed that t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on methods employ the decomposition strategy <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b33\">Nguyen and Nguyen, 2019;</ref><ref type=\"bibr\" target=\"#b43\">Wadden e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sfully used the idea of defining a sequence over a graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Considering a  ost attention <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. GCNs have been successfully applied to various image and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: secondary structure via attention by training a separate logistic regression on the Netsurf dataset <ref type=\"bibr\" target=\"#b23\">(Klausen et al., 2019)</ref>. As with the logistic regression on cont. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gt; 0. This mitigates network overconfidence and overfitting.</p><p>Auto Augmentation. Auto-Augment <ref type=\"bibr\" target=\"#b10\">[11]</ref> is a strategy that augments the training data with transfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  apart from MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> (cf. \u00a7 3.1), Graph Diffusion Convolution <ref type=\"bibr\" target=\"#b17\">[18]</ref> replaces the adjacency matrix with a sparsified version of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the relationship between the preference factors and the disentangled embeddings.</p><p>According to <ref type=\"bibr\" target=\"#b30\">(Yang et al., 2018)</ref>, the mutual information maximization can be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nd Sch\u00fctze, 2020a)</ref>; our proposed self-debiasing algorithm bears some resemblance with that of <ref type=\"bibr\" target=\"#b35\">Schick and Sch\u00fctze (2020b)</ref>. It is also related to other recent . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref> to sample negative nodes to increase the optimization effi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and pre-trained components <ref type=\"bibr\" target=\"#b10\">[11]</ref> in order to utilize weakly supervised data, i.e. speech-to ing so, both of them achieved better performance with the end-to-end model than the cascaded model. <ref type=\"bibr\" target=\"#b10\">[11]</ref> conducts experiments on a larger 236 hour English-to-Frenc n previous literature <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> in order to improve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n task, the quality of generated summaries can be improved <ref type=\"bibr\" target=\"#b35\">[34]</ref><ref type=\"bibr\" target=\"#b36\">[35]</ref><ref type=\"bibr\" target=\"#b37\">[36]</ref>. PEGASUS <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b6\">(Graves et al., 2014;</ref><ref type=\"bibr\" target=\"#b0\">Bahdanau et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015)</ref>. Our approach is data-driven, computat s of memory network in question answering <ref type=\"bibr\" target=\"#b35\">(Weston et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015)</ref>. We describe the background on memory  ple hops is that more abstractive evidences could be found based on previously extracted evidences. <ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al. (2015)</ref> demonstrate that multiple hops could u ion information in the attention model. The details are described below.</p><p>\u2022 Model 1. Following <ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al. (2015)</ref>, we calculate the memory vector m i wi gure\" target=\"#fig_0\">1</ref>, which is inspired by the use of memory network in question answering <ref type=\"bibr\" target=\"#b27\">(Sukhbaatar et al., 2015)</ref>. Our approach consists of multiple co r\" target=\"#b6\">(Graves et al., 2014;</ref><ref type=\"bibr\" target=\"#b35\">Weston et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b0\">Bahdanau . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: res before predicting answers. In particular, inspired by the success of Multi-Head Attention (MHA) <ref type=\"bibr\" target=\"#b28\">(Vaswani et al. 2017)</ref>, we refer to the MHA mechanism and propos  and inter-modality of audio features and textual features, we adopt the Multi-Head Attention (MHA) <ref type=\"bibr\" target=\"#b28\">(Vaswani et al. 2017)</ref>, which compute the association weights be res P to summarize the properties of the audio features A.</p><p>According to the attention theorem <ref type=\"bibr\" target=\"#b28\">(Vaswani et al. 2017</ref>), taking the first situation as example, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b69\">70,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" target=\"#b79\">80]</ref>, a variety of different approaches have been used in the li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: noisy labels are involved.</p><p>Motivated by this reason, Negative Learning for Noisy Labels; NLNL <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which is an indirect learning method for training CNNs, h ibly clean data and trains the other network with this data. Use of complementary labels Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed a noise-robust learning method where instead of m rpreted as a probability vector p \u2208 \u2206 c\u22121 , where \u2206 c\u22121 denotes the c-dimensional simplex.</p><p>NL <ref type=\"bibr\" target=\"#b11\">[12]</ref> is an indirect learning method for training CNNs with nois  <ref type=\"bibr\" target=\"#b29\">[30]</ref>, APL <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and NLNL <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Dataset We conduct the experiments on CIFAR10, CI-FAR100  a. We provide 110 y to each data in order to match the training speed to when training with CIFAR10 <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  <ref type=\"table\" target=\"#tab_7\">6</ref>    <ref type=\"bibr\" target=\"#b14\">(Ju et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Sohrab and Miwa, 2018)</ref>. Short word first is good at identifying. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g Solaris 8. We employ a wait-free implementation of the total store order memory consistency model <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. We perform speculati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: problem of the loss of detailed information due to the use of downsampling operations, Sun and Wang <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a maximum fusion scheme, which aims to improve th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: holar corpus <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Other large academic corpora including AMiner <ref type=\"bibr\" target=\"#b40\">[41]</ref>, OAG <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"b /p><p>In addition to that, we also integrate the OAG-BERT as a fundamental component for the AMiner <ref type=\"bibr\" target=\"#b40\">[41]</ref> system. In AMiner, we utilize OAG-BERT to handle rich info /ref>. Other large academic corpora including AMiner <ref type=\"bibr\" target=\"#b40\">[41]</ref>, OAG <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, and Microsoft Aca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b34\">[35]</ref>, which was then used to develop a multimodal deception detection system <ref type=\"bibr\" target=\"#b1\">[2]</ref>. An extensive review of approaches for evaluating human cred sent useful clues for deception, their performance is often similar to that of the n-grams features <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Since in our current work we are not focusing on the insigh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uch as context representation learning <ref type=\"bibr\" target=\"#b3\">[4]</ref>, machine translation <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and language modeling <ref type=\"bibr\" target=\"#b15\">[16] arget=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> or generated by pre-defined function <ref type=\"bibr\" target=\"#b20\">[21]</ref> were added to context representations. The other line of w f trainable vectors p i \u2208 {p t } L t=1 , where L is the maximum sequence length. On the other hand, <ref type=\"bibr\" target=\"#b20\">[21]</ref> has proposed to generate p i using the sinusoidal function >) under the self-attention setting in eq. ( <ref type=\"formula\">2</ref>), which is originally from <ref type=\"bibr\" target=\"#b20\">[21]</ref>. They share the same nature that the position information  =\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Properties of RoPE</head><p>Long-term decay: Following <ref type=\"bibr\" target=\"#b20\">[21]</ref>, we choose \u03b8 i = 10000 \u22122i/d . One can prove that this set. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on, called receiver class prediction optimization (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib ll with direct method calls in object-oriented languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib morphic inline caches <ref type=\"bibr\" target=\"#b23\">[23]</ref>, and type feedback/devirtualization <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. As we show in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  sequence. Recently, the models fully based on self-attention mechanism (denoted as self-attention  <ref type=\"bibr\" target=\"#b15\">[16]</ref> in the Neural Machine Translation (NMT) task, have achieve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b18\">(Jones et al., 2012)</ref>, and the current state-of-the-art pseudolikelihood maximization <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Ekeber itting and ultimately perform better after more training.</p><p>The CCMpred implementation of Potts <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Ekeber The performance of this model is reported in Table <ref type=\"table\" target=\"#tab_8\">7</ref>. Potts <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011)</ref>, TAPE transformer <ref type=\"bibr\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (FDP) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Address Map Pattern Matching Prefetching (AMPM) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. We now describe both of these techniques in some detail.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref>. The cSBM allows for smoothly controll rning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref> to generate synthetic graphs. We consi d have similar performances for \u03c6 and \u2212\u03c6. Due to space limitation we refer the interested reader to <ref type=\"bibr\" target=\"#b10\">(Deshpande et al., 2018)</ref> for a review of all formal theoretical philic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>. The results show that, asymptotically, ate synthetic data is that the information-theoretic limit of the model is already characterized in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p de et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type=\"bibr\" target=\"#b10\">Deshpande et al. (2018)</ref>). Assume that n, f \u2192 \u221e, n f \u2192 \u03be and d \u2192. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the vertex adjacency information (e.g. GAE <ref type=\"bibr\" target=\"#b21\">[22]</ref> and GraphSAGE <ref type=\"bibr\" target=\"#b22\">[23]</ref> in network embedding). This scheme can be very limited (as tioned limitations of proximity-based pre-training methods <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" tar rying to reconstruct the adjacency information of vertices <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> can be treated as a kind of \"local contrast\", while over-em. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng high scalability and efficiency. We also find recently emerging scalable algorithms, such as SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SIGN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, are spec sage. However, the GMLP-GU still allows a variety of graph aggregators provided in Section 3.2. SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> can be taken as a special case of this version with the no NP <ref type=\"bibr\" target=\"#b14\">[15]</ref>, AP-GCN <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref>, SIGN <ref type=\"bibr\" target=\"#b23\">[24]</ref>, which are t could compromise performance on a range of benchmark tasks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and suggest separating GCN from the aggregation scheme. We. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arse transformations tend to yield zero for the low-scoring in the vector, which is named sparsemax <ref type=\"bibr\" target=\"#b12\">(Martins and Astudillo 2016)</ref>:</p><formula xml:id=\"formula_1\">sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: of d(\u2022) including the 1 loss, Structural Similarity Loss (SSIM), and Gradient Difference Loss (GDL) <ref type=\"bibr\" target=\"#b26\">[27]</ref> and found SSIM to perform the best.</p><p>Blendshapes deco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  cases. In recent years, unsupervised learning has received increasing attention from the community <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Our novel appro  metric in an unsupervised fashion, without any human annotations.</p><p>Exemplar CNN. Exemplar CNN <ref type=\"bibr\" target=\"#b4\">[5]</ref> appears similar to our work. The fundamental difference is t tecture <ref type=\"bibr\" target=\"#b17\">[18]</ref> in their original papers, except for exemplar CNN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, whose results are reported with ResNet-101 <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e majority voting and the weighted majority voting that takes worker reliability into consideration <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>.</p><p>In this pape eighted majority voting (WMV) by putting different weights on workers to measure worker reliability <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>.</p></div> <div xml e it is intractable to directly solve problem ( <ref type=\"formula\" target=\"#formula_9\">9</ref>) or <ref type=\"bibr\" target=\"#b9\">(10)</ref>, we introduce the structured mean-field assumption on the p <p>Infer y: Fixing the distributions of \u03a6 and \u03b7 at their optimum q * , we find y by solving problem <ref type=\"bibr\" target=\"#b9\">(10)</ref>. To make the prediction more efficient, we approximate the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2017;</ref><ref type=\"bibr\" target=\"#b34\">Veli\u010dkovi\u0107 et al., 2018)</ref> or variants of Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref> that apply self-attention on all nodes t ype=\"bibr\" target=\"#b7\">Cai and Lam, 2020)</ref> base their encoder on the Transformer architecture <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref> and thus, in each layer, compute self-at raformer follows the general multi-layer encoderdecoder pattern known from the original Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>. In the following, we first describe our  computations for one head. The output of multiple heads is combined as in the original Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>.</p><p>Text self-attention. <ref type=\"b ead n=\"3.4\">Graformer decoder</head><p>Our decoder follows closely the standard Transformer decoder <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>, except for the modifications suggested  of the ith node's label.</p><p>To compute the node representation H (L) in the Lth layer, we follow <ref type=\"bibr\" target=\"#b33\">Vaswani et al. (2017)</ref>, i.e., we first normalize the input from . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sages based on the angle between atoms.</p><p>Benefiting from the growth of available molecule data <ref type=\"bibr\" target=\"#b91\">[92,</ref><ref type=\"bibr\" target=\"#b92\">93,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, as well as independent component analysis <ref type=\"bibr\" target=\"#b3\">[Cao et al. 2003]</ref>. <ref type=\"bibr\" target=\"#b4\">Cao et al. [2005]</ref> extract emotions using support vector machines lso present a user study rating the level of realism in emotion synthesis, covering several methods <ref type=\"bibr\" target=\"#b4\">[Cao et al. 2005;</ref><ref type=\"bibr\" target=\"#b27\">Liu and Osterman ing samples based on the apparent emotion <ref type=\"bibr\" target=\"#b0\">[Anderson et al. 2013;</ref><ref type=\"bibr\" target=\"#b4\">Cao et al. 2005;</ref><ref type=\"bibr\" target=\"#b11\">Deng et al. 2006;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th different weights. However, these methods <ref type=\"bibr\" target=\"#b27\">(Wu et al., 2019b;</ref><ref type=\"bibr\" target=\"#b31\">Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">An et al., 2019)  set of documents given a query. Some works <ref type=\"bibr\" target=\"#b24\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Zhu et al., 2019)</ref> propose to improve news representations via e eddings. These embeddings can be pre-trained from a large corpus or randomly initialized. Following <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we define the profile embedding</p><formula ed news representations would be taken as initial input embeddings of our model GNUD. Following DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we use two parallel convolutional neural ne sa-10week, which respectively collect news click logs as long as 1 week and 10 weeks. Following DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we just select user id, news id, time-stamp ws title and profile as semantic-level and knowledge-level representations, respectively.</p><p>DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, a deep attention neural network for news re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or SPEC benchmarks we use the reference input set. For all single-core benchmarks, we use SimPoints <ref type=\"bibr\" target=\"#b41\">[42]</ref> to find representative regions. Each Sim-Point is warmed u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad><p>In the first experiment, we used K-LEB to collect the performance counter samples for LINPACK <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The core of the LINPACK benchmark is to solve a dense sys. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b5\">(Chen et al. 2017;</ref><ref type=\"bibr\" target=\"#b36\">Yim et al. 2017;</ref><ref type=\"bibr\" target=\"#b37\">Yu et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Schmitt et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" target=\"#b21\">(Verma et al., 2019)</ref> depending on the location of the mix opera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b62\">[63]</ref>, firewall <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and load balancer <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Hash tables are also used for in-memory key-value stores . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: high memory footprint, both of which grow quadratically with respect to the image height (or width) <ref type=\"bibr\" target=\"#b37\">[38]</ref>. In real-world applications like content-based image searc ployed for higher efficiency. This differentiates our method from early recurrent attention methods <ref type=\"bibr\" target=\"#b37\">[38]</ref> which adopt pure recurrent models. In addition, we focus o \">8]</ref>.</p><p>One similar work to our GFNet is the recurrent visual attention model proposed in <ref type=\"bibr\" target=\"#b37\">[38]</ref>. However, our method differs from it in two important aspe ults with only a few class-discriminative patches, such as the head of a dog or the wings of a bird <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 6\">(Yao et al. 2015)</ref> and extracting the last hidden state of recurrent visual feature encoder <ref type=\"bibr\" target=\"#b20\">(Venugopalan et al. 2015)</ref>.</p><p>Those feature encoding methods Our basic video caption framework is extended from S2VT (sequence to sequence: video to text) model <ref type=\"bibr\" target=\"#b20\">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: time ERC, even less using different modalities as inputs <ref type=\"bibr\" target=\"#b52\">[52]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tempt. The most recent combinations are HyperRec <ref type=\"bibr\" target=\"#b35\">[36]</ref> and DHCF <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which borrow the strengths of hypergraph neural networks   method that models the recursive dynamic social diffusion in both the user and item spaces. \u2022 DHCF <ref type=\"bibr\" target=\"#b14\">[15]</ref> is a recent hypergraph convolutional network-based method  petence in all the cases. We are unable to reproduce its superiority reported in the original paper <ref type=\"bibr\" target=\"#b14\">[15]</ref>. There are two possible causes which might lead to its fai encounter the over-smoothing problem with the increase of depth. This problem is also found in DHCF <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which is based on hypergraph modeling as well. Considerin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> and voxels <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b27\">[1]</ref>) or lack a clear objective function tailored for network embedding (e.g., <ref type=\"bibr\" target=\"#b42\">[16]</ref>). We anticipate that a new model with a carefully designed e for both undirected and directed graphs.</p><p>The most recent work related with ours is DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>, which deploys a truncated random walk for social network  /ref> . The Flickr network is denser than the Youtube network (the same network as used in DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>). (3) Citation Networks. Two types of citation networks ar rk can be represented as an affinity matrix, and is able to represent each vertex with a \u2022 DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>. DeepWalk is an approach recently proposed for social netw r\" target=\"#b39\">[13]</ref>. For other networks, the dimension is set as 128 by default, as used in <ref type=\"bibr\" target=\"#b42\">[16]</ref>. Other default settings include: the number of negative sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bustness of SGNAS more fairly, we evaluate it based on a NAS benchmark dataset called NAS-Bench-201 <ref type=\"bibr\" target=\"#b16\">[17]</ref>. NAS-Bench-201 includes 15,625 architectures in total. It . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thm affects the overall quality of generated texts, we measure perplexity on the Wikitext-2 dataset <ref type=\"bibr\" target=\"#b23\">(Merity et al., 2017)</ref>. 6 We use a maximum sequence length of |x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tences contribute to the classification decision which can be of value in applications and analysis <ref type=\"bibr\" target=\"#b21\">(Shen et al., 2014;</ref><ref type=\"bibr\">Gao et al., 2014)</ref>.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b7\">Garg et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">May et al., 2010;</ref><ref type=\"bibr\" target=\"#b38\">Zhao et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Rudinger et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uts a human player would have. In contrast to previous work <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> , our approach incorporates 'end-to-end' reinforcement learn as inputs to the neural network by some previous approaches <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> . The main drawback of this type of architecture is that a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: xt to speech, TTS) <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and speech recognit et=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and ASR <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"b , we re-sample it to 16kHZ and convert the raw waveform into mel-spectrograms following Shen et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> with 50ms frame size, 12.5ms hop size. For the text, we us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: derstand the complex relationships between users and items <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. A large recommender with many learning parameters usually  In this regard, unlike the motivation of the previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, items merely ranked highly by the teacher may be not infor  by the student and vice versa. Thus, the existing methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> that simply choose the high-ranked items cannot give enough ded from the public implementation and the original papers <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. For BD, \ud835\udf06 \ud835\udc47 \u2192\ud835\udc46 and \ud835\udf06 \ud835\udc46\u2192\ud835\udc47 are set to 0.5, the number of ite m, a few recent work <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23</ref>] has adopted Knowledge Distillation (KD) to RS. KD is a mode a few recent methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23</ref>] have adopted knowledge distillation to RS. KD is a model-ag e recommendation list predicted by the teacher along with the binary training set. Specifically, in <ref type=\"bibr\" target=\"#b22\">[23]</ref>, the student is trained to give high scores on the top-ran 0 N@50 N@100 H@50 H@100 N@50 N@100 H@50 H@100 N@50 N@100 Teacher 0.1566 \u2022 Ranking Distillation (RD) <ref type=\"bibr\" target=\"#b22\">[23]</ref>: A pioneering KD method for top-\ud835\udc3e RS. RD makes the student ware sampling, 2) Rank-aware sampling <ref type=\"bibr\" target=\"#b15\">[16]</ref>, 3) Top-\ud835\udc41 selection <ref type=\"bibr\" target=\"#b22\">[23]</ref>, 4) Uniform sampling, 5) Swapped rank discrepancy-aware sa  RS. KD is a model-agnostic strategy and we can employ any recommender system as the base model. RD <ref type=\"bibr\" target=\"#b22\">[23]</ref> firstly proposes a KD method that makes the student give h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: scal GPU, and a 8GB 128-bit LPDDR4, for various deep learning applications including classification <ref type=\"bibr\" target=\"#b26\">(Li et al., 2020)</ref>, segmentation <ref type=\"bibr\" target=\"#b36\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emporal convolution with a (2+1)D block, which can double the number of non-linearities. Xie et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> used a similar approach which replaces some 3D convolution. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: antify the diversity across operators inspired by fixed-size determinantal point processing (K-DPP) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, a popular sampling model with great ability to measure th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ial recommendation has been widely studied in the literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targe 4]</ref>.</p><p>Typically, sequential recommendation methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" targe s of works follow this line and extend it for high-order MCs <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. With the development ad><p>Existing studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> mainly emphasize the  d is a parameter matrix to learn. Note that existing methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> seldom directly model nal neural networks (CNNs) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and self-attention mechanisms <ref type=\"bibr\" target=\"#b7\">[8]</ref> have been proposed to learn good representations of user pre commendation with MIM, which is called S 3 -Rec. Based on a self-attentive recommender architecture <ref type=\"bibr\" target=\"#b7\">[8]</ref>, we propose to first pre-train the sequential recommender wi introduce the base model of our proposed approach that is developed on the Transformer architecture <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Then, we will describe how we utilize the correlation signa tions.</p><p>Sequential models such as GRU4Rec <ref type=\"bibr\" target=\"#b20\">[21]</ref> and SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> mainly focus on modeling the sequential dependencies between ng horizontal and vertical convolutional operations for sequential recommendation.</p><p>(6) SASRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> is a self-attention based sequential recommendation model, w  heads as 2. The dimension of the embedding is 64, and the maximum sequence length is 50 (following <ref type=\"bibr\" target=\"#b7\">[8]</ref>). Note that our training phase contains two stages (i.e., pr type=\"bibr\" target=\"#b26\">[27]</ref>, etc. There are also studies that leverage other architectures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ tem prediction loss L M I P in Eq. 12 has a similar effect to capture sequential dependencies as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> except that it can a qual to NDCG@1, we report results on HR@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ ems as candidates for testing. Following the common strategy <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, we pair the ground-truth item with 99 randomly sampled negat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_2\"><head>\u2022</head><label></label><figDesc>NeuMF<ref type=\"bibr\" target=\"#b3\">[4]</ref>: A deep model that combines MF and Multi-Layer Perceptron (M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b2\">[3]</ref> (i.e., the latest large-scale language model of OpenAI), pre-training language models <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" ta e.g., UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref>) and two-tower architecture (e.g., OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref>). In this project, similar to OpenAI CLIP, 'WenLan' adopts dea, we introduce comparative learning into our two-tower architecture. However, unlike OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> that adopts a simple contrastive learning method, we propo tly, our CMCL model outperforms both UNITER <ref type=\"bibr\" target=\"#b5\">[6]</ref> and OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> on the RUC-CAS-WenLan test set and AIC-ICC <ref type=\"bibr </p><p>Tasks Image-to-Text Retrieval Text-to-Image Retrieval Metrics R@1 R@5 R@10 R@1 R@5 R@10 CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> 13 </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><he sks</head><p>Image-to-Text Retrieval Text-to-Image Retrieval Metrics R@1 R@5 R@10 R@1 R@5 R@10 CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref> 7 </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><hea  out over the text-image retrieval results obtained by the pre-training models (e.g., CMLC and CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref>). We select a group of image and text queries for testing. t=\"#tab_1\">4</ref>. As expected, the user study does validate that our CMCL outperforms OpenAI CLIP <ref type=\"bibr\" target=\"#b25\">[26]</ref>. When the candidate set (per query) of UNITER is obtained . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. In essence, the goal of meta-learning is to train a model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Data stream strides. The data stream is characterized with respect to local and global data strides <ref type=\"bibr\" target=\"#b9\">[10]</ref>. A global stride is defined as the difference in the data m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: earning has been closing the gap with, and in some cases even surpassing its supervised counterpart <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ air), creating contradicting objectives.</p><p>While recent efforts focus on improved architectures <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ef type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and data augmentation <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, relatively little w get=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>.</p><p>In contrastive learning, the embedding space is govern different images, regardless of their semantic information <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Figure <ref type=\"fi s this problem by maintaining a momentum encoder and a limited queue of previous samples. SimCLR v1 <ref type=\"bibr\" target=\"#b8\">[9]</ref> eschews a momentum encoder in favor of a large batch size, a \"bibr\" target=\"#b33\">[34]</ref> 63.6 -PCL <ref type=\"bibr\" target=\"#b31\">[32]</ref> 65.9 -SimCLR v1 <ref type=\"bibr\" target=\"#b8\">[9]</ref> 69.3 89.0 MoCo v2 <ref type=\"bibr\" target=\"#b10\">[11]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 8\">[9]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Our work extends these works by using predictions to plac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e challenging. A number of GPM frameworks have been proposed to reduce the burden on the programmer <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta lgorithms. Low-level systems such as RStream <ref type=\"bibr\" target=\"#b55\">[56]</ref> and Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref> provide low-level API functions for the user to control th form the state-of-the-art GPM systems, AutoMine <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Peregrine <ref type=\"bibr\" target=\"#b28\">[29]</ref> b e an example in Appendix B.6). This technique is called Memoization of Embedding Connectivity (MEC) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. \u2022 For edge-induced extension, a set of edges instead of v tern Classification (CP): FP and CP are low-level optimizations enabled in a prior system, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, so we describe them in Appendices B.4 and B.5. To support type=\"foot\" target=\"#foot_0\">3</ref> : AutoMine <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Peregrine <ref type=\"bibr\" target=\"#b28\">[29]</ref>.  \"#b10\">[11]</ref> is a distributed GPM system which incorporates task-parallel processing. Pangolin <ref type=\"bibr\" target=\"#b11\">[12]</ref> is a shared-memory GPM system targeting both CPU and GPU.  ferent patterns. For small implicit patterns, Sandslash uses customized pattern classification (CP) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For example, in FSM, the labeled wedge patterns can be di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> has demonstrated superior performance to the previous hand-cr m, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> has drawn considerable attention due to its simple network st Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. Motivated by SRCNN, some problems such as face hallucination ><p>We first briefly describe the network structure of SRCNN <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, and then we detail how we reformulate the network layer by l Different Upscaling Factors</head><p>Unlike existing methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> that need to train a network from scratch for a different sca lgorithms are mostly learning-based (or patch-based) methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target= space, then followed by a complex mapping to another high-dimensional HR feature space. Dong et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> show that the mapping accuracy can be substantially improved a wider mapping layer, but at the cost of the running time. For example, the large SRCNN (SRCNN-Ex) <ref type=\"bibr\" target=\"#b1\">[2]</ref> has 57,184 parameters, which are six times larger than that  with no pre-processing. 2) The proposed model achieves a speed up of at least 40\u00d7 than the SRCNN-Ex <ref type=\"bibr\" target=\"#b1\">[2]</ref> while still keeping its exceptional performance. One of its  ters in a layer) and depth (i.e., the number of layers) of the mapping layer. As indicated in SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a 5 \u00d7 5 layer achieves much better results than a 1 \u00d7 1 lay s an average PSNR of 32.87 dB, which is already higher than that of SRCNN-Ex (32.75 dB) reported in <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The FSRCNN (48,12,2) contains only 8,832 parameters, then t F) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SRCNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>, SRCNN-Ex <ref type=\"bibr\" target=\"#b1\">[2]</ref> and the sparse coding based network (SCN) <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ument, where the pseudo summary can be either sentence-level <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b12\">11,</ref><ref type=\"bibr\" target=\"#b30\">29]</ref> or summary-level <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hods in search result diversification in order to learn an optimized ranking function automatically <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta document-document similarity regardless the use of subtopics <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta e. Inheriting the spirit of MMR, researchers have also proposed supervised methods, such as SVM-DIV <ref type=\"bibr\" target=\"#b9\">[10]</ref>, R-LTR <ref type=\"bibr\" target=\"#b10\">[11]</ref>), PAMM <re hts of the hierarchical subtopic layers. The parameters are tuned with cross validation and ListMLE <ref type=\"bibr\" target=\"#b9\">[10]</ref> is used to learn a prior relevance function with no diversi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction on transformed elements in the point set to approximate a general function defined on the set <ref type=\"bibr\" target=\"#b20\">[21]</ref>:</p><formula xml:id=\"formula_12\">AGGREGATE({x 1 , \u2022 \u2022 \u2022 , . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ph representations <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b97\">99,</ref><ref type=\"bibr\" target=\"#b90\">91,</ref><ref type=\"bibr\" target=\"#b93\">94,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion.</p><p>Accurately predicting (black-box) workload criticality is itself a challenge. Prior work <ref type=\"bibr\" target=\"#b5\">[6]</ref> associated a diurnal utilization pattern with user interacti system</head><p>To integrate predictions into VM scheduling in practice, we target Resource Central <ref type=\"bibr\" target=\"#b5\">[6]</ref>, the existing ML and predictionserving system in Azure. The  e workload of each VM is performance-critical or not, before we can train a model. As in prior work <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we consider a workload critical if it is user-facing, i.e.  he problem reduces to identifying VMs whose time series of CPU utilizations exhibit 24-hour periods <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Obviously, some background VMs may exhibit 24-hour pe identifying periods in time series, such as FFT or the autocorrelation function (ACF). For example, <ref type=\"bibr\" target=\"#b5\">[6]</ref> assumes a workload is user-facing if the FFT indicates a 24- /task length for provisioning or scheduling purposes, e.g. <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y works have proposed data augmentation technologies on different types of features, such as images <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  using cross-entropy <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. This architecture can also be used as a \"bottleneck\" featu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Two alternative sampling implementations are commonly used nts in the program <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Several techniques have been proposed to warmup cache stat target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. The MTR adds multiprocessor and directory support to these itectural state is updated while fast-forwarding and then used to initialize the detailed simulator <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Microarchitectural state is less amenable to checkpointin ed simulation needed to achieve small error rates with the desired confidence, the SMARTS framework <ref type=\"bibr\" target=\"#b26\">[27]</ref> recently proposed functional warming, which simulates larg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nce the L1 hybrid is used to filter easily predicted highly biased branches, a confidence estimator <ref type=\"bibr\" target=\"#b13\">[14]</ref> indicates whether the branch is more difficult to predict  ch prediction process involve correlating the actual branch register values with the branch outcome <ref type=\"bibr\" target=\"#b13\">[14]</ref> using a conventional value predictor. The authors of the s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> and statistical models <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. We describe now som train it using the three principles of unsupervised machine translation identified in Lample et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref>, namely initialization, language modeling, and back-transl training</head><p>Pretraining is a key ingredient of unsupervised machine translation Lample et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref>. It ensures that sequences with a similar meaning are mapp an important component of unsupervised machine translation <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>In the unsuper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in RDB has access to all the subsequent layers and passes on information that needs to be preserved <ref type=\"bibr\" target=\"#b10\">[7]</ref>. Concatenating the states of preceding RDB and all the prec oposed DenseNet, which allows direct connections between any two layers within the same dense block <ref type=\"bibr\" target=\"#b10\">[7]</ref>. With the local dense connections, each layer reads informa e the bias term is omitted for simplicity. We assume F d,c consists of G (also known as growth rate <ref type=\"bibr\" target=\"#b10\">[7]</ref>) feature-maps.</p><formula xml:id=\"formula_6\">[F d\u22121 ,F d,1 k architecture as residual dense block (RDB). More differences between RDB and original dense block <ref type=\"bibr\" target=\"#b10\">[7]</ref> would be summarized in Section 4.</p></div> <div xmlns=\"htt .tei-c.org/ns/1.0\"><head n=\"4.\">Discussions</head><p>Difference to DenseNet. Inspired from DenseNet <ref type=\"bibr\" target=\"#b10\">[7]</ref>, we adopt the local dense connections into our proposed res ne is the design of basic building block. SRDenseNet introduces the basic dense block from DenseNet <ref type=\"bibr\" target=\"#b10\">[7]</ref>. Our residual dense block (RDB) improves it in three ways:  <ref type=\"bibr\" target=\"#b6\">[3]</ref> and also demonstrates that stacking many basic dense blocks <ref type=\"bibr\" target=\"#b10\">[7]</ref> in a very deep network would not result in better performan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tation models (PLMs) such as ELMo <ref type=\"bibr\" target=\"#b29\">(Peters et al., 2018a)</ref>, BERT <ref type=\"bibr\" target=\"#b9\">(Devlin et al., 2019a)</ref> and XLNet <ref type=\"bibr\" target=\"#b46\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a length-limited inquiry text for recommendation and user's identity for personalized customization <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Under these requirements, the text feature used for the r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > with various efficient CNNs (e.g., MobileNet-V3 <ref type=\"bibr\" target=\"#b15\">[16]</ref>, RegNet <ref type=\"bibr\" target=\"#b39\">[40]</ref>, EfficientNet <ref type=\"bibr\" target=\"#b49\">[50]</ref>, e l state-of-the-art CNNs, including MobileNet-V3 <ref type=\"bibr\" target=\"#b15\">[16]</ref>, RegNet-Y <ref type=\"bibr\" target=\"#b39\">[40]</ref>, EfficientNet <ref type=\"bibr\" target=\"#b49\">[50]</ref>, R  in Image Classification</head><p>A Implementation Details A.1 Recurrent Networks</p><p>For RegNets <ref type=\"bibr\" target=\"#b39\">[40]</ref>, MobileNets-V3 <ref type=\"bibr\" target=\"#b15\">[16]</ref> a pe=\"bibr\" target=\"#b13\">[14]</ref>, DenseNets <ref type=\"bibr\" target=\"#b21\">[22]</ref> and RegNets <ref type=\"bibr\" target=\"#b39\">[40]</ref>, we use a GRU with 1024 hidden units. For MobileNets-V3 <r \"bibr\" target=\"#b38\">[39]</ref>, for RegNets, we use the pre-trained models provided by their paper <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and for MobileNets-V3 and EfficientNets, we first train t e training process <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e Cheeger bound. This connection has inspired many spectral solutions to the problem, including ARV <ref type=\"bibr\" target=\"#b5\">[11]</ref> and the many works that followed.</p><p>However, spectral m y balanced 2-partitioning algorithm to approximate a balanced k-partitioning when k is a power of 2 <ref type=\"bibr\" target=\"#b5\">[11]</ref>.</p><p>While we are unaware of any previous work on the exa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: no extra</head><p>queueing is introduced in the server. When combined with real-time I/O subsystems <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the Leader/Followers thread pool implementation can reduc /ref> operation scheduling, event processing <ref type=\"bibr\" target=\"#b6\">[7]</ref>, I/O subsystem <ref type=\"bibr\" target=\"#b11\">[12]</ref> and pluggable protocol <ref type=\"bibr\" target=\"#b13\">[14]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" targ rget=\"#b10\">11]</ref> have been two fast-rising fields, boosted by lottery tickets hypothesis (LTH) <ref type=\"bibr\" target=\"#b9\">[10]</ref> and singleshot network pruning (SNIP) <ref type=\"bibr\" targ performance of the subnetworks discovered by GraNet. The authors of Lottery Ticket Hypothesis (LTH) <ref type=\"bibr\" target=\"#b9\">[10]</ref> introduced a retraining technique, even if they did not eva m accuracy drops. This finding makes a connection to the success of the iterative magnitude pruning <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" tar inference. The pruning criterion includes weight magnitude <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, gradient <ref type=\"bibr\" target=\"#b60\">[61]</ref> Hessian . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arge in order to express the huge number of interest profiles at Tmall. Deep Interest Network (DIN) <ref type=\"bibr\" target=\"#b29\">[30]</ref> makes the user representation vary over different items wi \">[3]</ref>. Besides the industrial applications proposed by <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, various types of deep models have gained significant atten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b44\">45]</ref>. Recent works on sequential recommendation <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and interactive recommendation <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the graph. To address the second challenge, inspired by the previous graph sparsification studies <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, we propose a rela t important edges in the graph. Inspired by previous studies <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, personalized PageR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>, compression artifacts <ref type=\"bibr\" target=\"#b12\">[13]</ref>, etc. When applied to real-world scenarios, it becomes mor type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref> and compression removal <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. To achieve visual. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng marginalized graph autoencoder. Its training objective is reconstructing the feature matrix. AGC <ref type=\"bibr\" target=\"#b37\">[38]</ref> exploits high-order graph convolution to filter node featu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eled through iterative sparse matrix vector multiplication <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. In each iteration, the system traverses all active vertice stems that explicitly maintain states for subgraph patterns <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" tar 6,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>. Arabesque <ref type=\"bibr\" target=\"#b45\">[46]</ref> is the first distributed system that proposes the \"think l .tei-c.org/ns/1.0\"><head n=\"9\">Related Work</head><p>Graph mining systems and algorithms. Arabesque <ref type=\"bibr\" target=\"#b45\">[46]</ref> built on Giraph <ref type=\"bibr\" target=\"#b0\">[1]</ref> is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: formance gains. We can also incorporate our techniques into big GNN models, providing modest gains. <ref type=\"bibr\" target=\"#b11\">Mahoney, 2015)</ref>. The salient point for this paper is that we ass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=\"bibr\" target=\"#b13\">[14]</ref> . It could obtain transformation rules automatically durin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: blems have been extensively studied, and many efficient graph processing systems have been proposed <ref type=\"bibr\" target=\"#b3\">[4]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>. On the other han -to-one correspondences <ref type=\"bibr\">([4,5,6,7,3]</ref>, <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target= \" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, <ref type=\"bibr\" targe target=\"#b3\">4]</ref>, <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target= r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" targ , 2), (2, 3)and(4, 5) into g. After partitioning g, there are three connected components: [1,2,3] , <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> and <ref type=\"bibr\" t ), (2, 3), and (4, 5) into g. After partitioning g, there are three connected components: [1,2,3] , <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, and <ref type=\"bibr\"   numbers in a square bracket denote a one-to-one correspondence (a bijective function). For example,<ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graphs <ref type=\"bibr\" target=\"#b8\">[9]</ref>, prediction of customer types in e-commerce networks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, or the assignment of scientific papers from a citation netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ssage passing, in order to capture richer topological properties. Our method draws inspiration from <ref type=\"bibr\" target=\"#b32\">[33]</ref>, where it was shown that GNNs become universal when the ve  target=\"#b35\">[36]</ref>.</p><p>It is important to note here that contrary to identifierbased GNNs <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bib rovide a unique identification of the vertices, then universality will also hold (Corollary 3.1. in <ref type=\"bibr\" target=\"#b32\">[33]</ref>).</p><p>We conjecture, that in real-world scenarios the nu ><p>Unique identifiers. From a different perspective, <ref type=\"bibr\" target=\"#b67\">[68]</ref> and <ref type=\"bibr\" target=\"#b32\">[33]</ref> showed the connections between GNNs and distributed local . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ee' approach does not need video, and can be driven by either recorded audio, TTS, or voice cloning <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Voice controlled Avatars: Our model's blendshapes output   videos into different languages. In conjunction with appropriate video re-timing and voice-cloning <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the resulting videos look fairly convincing. We have empl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  designs have been utilized separately in some prior works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=  applied to different neighborhoods can be the same or different. This design-employed in GCN-Cheby <ref type=\"bibr\" target=\"#b6\">[7]</ref> and MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>-augments  rhoods by combining Chebyshev polynomials to approximate a higher-order graph convolution operation <ref type=\"bibr\" target=\"#b6\">[7]</ref>, outperforms GCN and GAT, which aggregate over only the imme N <ref type=\"bibr\" target=\"#b16\">[17]</ref> GAT <ref type=\"bibr\" target=\"#b35\">[36]</ref> GCN-Cheby <ref type=\"bibr\" target=\"#b6\">[7]</ref> GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> MixHop <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b21\">(Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">van den Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Ma et al., 2019)</ref>. However, their output quality suffers due to  y suffers due to the large decoding space and strong independence assumptions between target tokens <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2019;</ref><ref type=\"bibr\" target=\"#b52\">Wang et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: upervised fashion. <ref type=\"bibr\" target=\"#b54\">[55,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> exploit the mutual information maximization scheme to const. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tei-c.org/ns/1.0\"><head n=\"4\">Attacking Reading Comprehension</head><p>We create triggers for SQuAD <ref type=\"bibr\" target=\"#b25\">(Rajpurkar et al., 2016)</ref>. We use an intentionally simple baseli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i-c.org/ns/1.0\"><head n=\"2\">Related Work</head><p>CNN Compression and Acceleration. Extensive works <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" ta time. We observe a correlation between the pre-fine-tune accuracy and the post fine-tuning accuracy <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b22\">22]</ref>. As shown in Table  For channel pruning, we use max response selection (pruning the weights according to the magnitude <ref type=\"bibr\" target=\"#b20\">[20]</ref>), and preserve Batch Normalization <ref type=\"bibr\" target /ref>. However, it requires iterative prune &amp; fine-tune procedure to achieve decent performance <ref type=\"bibr\" target=\"#b20\">[20]</ref>, and single-shot pruning without retraining will greatly h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: representations. Other methods apply IB to various domains <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. The difference is that we develop information-theoretic mo  and <ref type=\"bibr\" target=\"#b5\">(6)</ref>. To allow more flexibility (in similar spirit as \u03b2-VAE <ref type=\"bibr\" target=\"#b40\">[41]</ref>), we allow the coefficient before AIB and XIB to be differ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>Our hybrid pointer-generator network facilitates copying words from the source text via pointing <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref>, which improves accuracy and handling of twork</head><p>Our pointer-generator network is a hybrid between our baseline and a pointer network <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref>, as it allows both copying words via poi plore sentence fusion using dependency trees.</p><p>Pointer-generator networks. The pointer network <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref> is a sequence-tosequence model that uses. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: olution operations <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. GC-MC <ref type=\"bibr\" target=\"#b28\">[29]</ref> applies th t is captured on the level of item relations, rather than the collective user behaviors. SpectralCF <ref type=\"bibr\" target=\"#b41\">[42]</ref> proposes a spectral convolution operation to discover all  , is used as suggested in <ref type=\"bibr\" target=\"#b28\">[29]</ref>.</p><p>We also tried SpectralCF <ref type=\"bibr\" target=\"#b41\">[42]</ref> but found that the eigen-decomposition leads to high time . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  state-of-the-art enc-dec models, i.e., PointGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> and BART <ref type=\"bibr\" target=\"#b14\">[13]</ref>, both frequently make incorrect alignments by either missi t large . authorities sealed off the store and summoned six top officials from fabindia.</p><p>BART <ref type=\"bibr\" target=\"#b14\">[13]</ref> federal education minister smriti irani was visiting a fab  <ref type=\"bibr\" target=\"#b19\">[18]</ref> and copy mechanism, and a pretrained language model BART <ref type=\"bibr\" target=\"#b14\">[13]</ref> finetuned on our pseudo summaries and reference summaries.  \ud835\udf06 \ud835\udc50 = 1.0, \ud835\udf06 \ud835\udc58 = 0.5, \ud835\udf06 \ud835\udc60 = 0.5 (Eq. 7). For abstractor, the lr of PG is 1\ud835\udc52 \u2212 03. We follow Lewise <ref type=\"bibr\" target=\"#b14\">[13]</ref> in fine-tuning BART with \ud835\udc59\ud835\udc5f = 3\ud835\udc52 \u2212 05 and warmup = 500. Fo -layer transformer network, including unidirctional, bidirectional and seq2seq language model. BART <ref type=\"bibr\" target=\"#b14\">[13]</ref> takes combines bidirectional transformer encoder and auto- hich has been a popular method for abstractive summarization recently. Unlike the end-to-end models <ref type=\"bibr\" target=\"#b14\">[13,</ref><ref type=\"bibr\" target=\"#b18\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng, most notably the group of NLP models known as word2vec <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. A number of recent research publications have proposed wor learn the distributed representations of words in a corpus <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Inspired by it, DeepWalk <ref type=\"bibr\" target=\"#b21\">[2 lelized by using the same mechanism as word2vec and node2vec <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. All codes are implemented in C and C++ and our experiments e distributed representations of words in natural language <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Building on word2vec, Perozzi et al. suggested that the \"c ghborhoods with network semantics for various types of nodes. Second, we extend the skip-gram model <ref type=\"bibr\" target=\"#b17\">[18]</ref> to facilitate the modeling of geographically and semantica p 2 &amp; p 3 ).</p><p>To achieve e cient optimization, Mikolov et al. introduced negative sampling <ref type=\"bibr\" target=\"#b17\">[18]</ref>, in which a relatively small set of words (nodes) are samp aximize the network probability in terms of local structures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, that is:</p><formu d as a so max function <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, that is:</p><formu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b19\">Li et al., , 2014;;</ref><ref type=\"bibr\" target=\"#b47\">Yang and Mitchell, 2016;</ref><ref type=\"bibr\" target=\"#b32\">Nguyen et al., 2016;</ref><ref type=\"bibr\" target=\"#b25\">Liu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , an optimal relay selection strategy is developed in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. In <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the dynamism of electricity price and the deferrability o onding Nash value V i k N (XN , RN ) using ( <ref type=\"formula\" target=\"#formula_15\">10</ref>) and <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p><formula xml:id=\"formula_21\">End For 1 \u2264 n \u2264 N \u2212 1</fo sponding Nash value V i k n (Xn, Rn) using ( <ref type=\"formula\" target=\"#formula_15\">10</ref>) and <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  that have been trained with very few data. <ref type=\"bibr\" target=\"#b3\">Finn et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b23\">Snell et al. (2017)</ref> train models in a meta-learning fashion so  tric learning. MatchingNet <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2016)</ref> and ProtoNet <ref type=\"bibr\" target=\"#b23\">(Snell et al., 2017)</ref> learned to classify samples by comparing t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct and have high intersubject variability. To learn personalized models for each student, we follow <ref type=\"bibr\" target=\"#b3\">(Jaques et al., 2017)</ref> and use a Multitask approach which compris. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: has been initiated. In two independent studies <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> the distinguishing power of GNNs is link rmer class of MPNNs covers the GNNs studied in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref t thm.</p><p>For anonymous MPNNs related to GNNs <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs < er the graph neural network architectures <ref type=\"bibr\" target=\"#b4\">[Hamilton et al., 2017</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> defined by:</p><formula xml:id=\"formula_ tly see, it follows from two independent works <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> that the distinguishing power of aMPNNs  et=\"#fig_1\">1</ref>. Proposition 5.2 (Based on <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref>). The classes M anon and M WL are equall h between anonymous graph neural networks <ref type=\"bibr\" target=\"#b4\">[Hamilton et al., 2017</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> and degree-aware graph neural networks < n as a slight generalisation of the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019</ref>]. (ii) The distinguishing power of degree ep-by-step, by GNNs that use ReLU or sign as activation function. This result refines the result in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> in that their simulation using the ReLU f of aMPNNs which are of special interest: those arising from the graph neural networks considered in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. In Example 3.1 we established that such  of the proofs of Lemma 2 in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. We show, by induction on the number of r remark that we cannot use the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> as a black box because the class M anon i onsidered in those papers. The proofs in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> relate to graph neural networks which, in nd M WL , and thus also M anon , are equally strong. The following results are known. Theorem 5.5 ( <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>). (i) The classes M sign GNN and M WL are side effect, we obtain a simpler aMPNN M in M GNN , satisfying M WL M , than the one constructed in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. The proof strategy is inspired by that o ef type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. The proof strategy is inspired by that of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. Crucial in the proof is the notion of ro  of two, at the cost of introducing an extra parameter p \u2208 A. Furthermore, the aMPNN constructed in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> uses two distinct weight matrices in A (s By the induction hypothesis, these rows are linearly independent. Following the same argument as in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>, this implies that there exists an (s t\u22121 ure the labelling \"refines\" \u2113 \u2113 \u2113 (t) MWL . To do so, we again follow closely the proof strategy of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. More specifically, we will need an analo  entries having value 1 and whose size will be determined from the context. Lemma 5.9 (Lemma 9 from <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>). Let C \u2208 A m\u00d7w be a matrix in which all  ns of the non-zero entries in \u00b5 \u00b5 \u00b5 Regarding future work, we point out that, following the work of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>, we fix the input graph in our analysis. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cess to ground truth facial information. We show experiments with three talking head datasets: GRID <ref type=\"bibr\" target=\"#b9\">[10]</ref>, TCD-TIMIT <ref type=\"bibr\" target=\"#b15\">[16]</ref> and CR e a LipNet model <ref type=\"bibr\" target=\"#b1\">[2]</ref> pretrained for lip-reading on GRID dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Observations: We report the metrics in Figure <ref t  GRID, CREMA-D and TCD-TIMIT</head><p>For self-reenactment studies we performed experiments on GRID <ref type=\"bibr\" target=\"#b9\">[10]</ref>, TCD TMIT <ref type=\"bibr\" target=\"#b15\">[16]</ref> and CRE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata management support, OS, core, and memory system.</p><p>Compared to the closest prior-work, XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MetaSys uses a similar tagged memory-based metadata manag cate a diverse set of metadata at runtime and (ii) lightweight and low-overhead metadata management <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Even small overheads imposed as a result of the system's  agged architectures. MetaSys is inspired by the metadata management and interfaces proposed in XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref> and the large body of work on tagged memory <ref type=\"bib s table is allocated by the OS for each process and is saved in memory. In MetaSys (similar to XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cheri <ref type=\"bibr\" target=\"#b19\">[20]</ref>), we t >To associate memory address ranges with an ID, we provide the MAP/UNMAP interface (similar to XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>). MAP and UNMAP are implemented as new RISC-V instructions work</head><p>MetaSys implements a tagged-memory-based system with a metadata cache similar to XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>. MetaSys however has three major benefits. First, MetaSys  UCA systems. MetaSys can flexibly implement the range of crosslayer optimizations supported by XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>. MetaSys's dynamic interface for metadata communication en udies were conducted with our baseline 128-entry MMC with a tagging granularity of 512B (as in XMem <ref type=\"bibr\" target=\"#b21\">[22]</ref>). Fig. <ref type=\"figure\">9</ref> plots the corresponding  es with a single change to the hardware-software interface <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and consolidating common metadata management support; thus, nagement, data placement, thread scheduling, memory scheduling, data compression, and approximation <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. For QoS optimizat nagement support across multiple optimizations. Such systems were recently proposed for performance <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, memory protection. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 1\">[42]</ref> fed the text embeddings to both generator and discriminator as extra inputs. StackGAN <ref type=\"bibr\" target=\"#b53\">[54]</ref> decomposed the generation into a sketch-refinement process. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d event representations are also natural language words, we adopt the pre-trained language model T5 <ref type=\"bibr\" target=\"#b35\">(Raffel et al., 2020)</ref> as our transformer-based encoder-decoder . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b21\">(Salimans &amp; Kingma, 2016)</ref> with momentum 0.999 to all of them. We used leaky ReLU <ref type=\"bibr\" target=\"#b13\">(Maas et al., 2013)</ref> with \u03b1 = 0.1 as the non-linearity, and chos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graph).</p><p>Our method also connects to PinSage <ref type=\"bibr\" target=\"#b20\">[21]</ref> and GAT <ref type=\"bibr\" target=\"#b14\">[15]</ref>. But note that both PinSage and GAT are designed for homog ula><p>(5) 2 The knowledge graph G is treated undirected. 3 Technically, S(v) \u2022 Neighbor aggregator <ref type=\"bibr\" target=\"#b14\">[15]</ref> directly takes the neighborhood representation of entity v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ope et al. [2021]</ref>). We can approximate this manifold by using a high quality autoencoding DNN <ref type=\"bibr\" target=\"#b2\">(Bourlard and Kamp [1988]</ref>, <ref type=\"bibr\" target=\"#b20\">Wang e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We adopt larger input data sets varying from 147 to 187 G  both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in the memory system. And for each workload, we collect t  performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type=\"bibr\" target=\"#b16\">[17]</ref>).</p><p>We find that big data analytics applications share chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type=\"bibr\" target=\"#b16\">[17]</ref>) workloads. Meanwhile the service workloads in data center s, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>\u2022 Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and service workloa the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type=\"bibr\" target=\"#b16\">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall b vel cache), respectively. For the service workloads, our observations corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><p>\u2022 For the big data ana rk of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, CloudSuite paper is biased towards online servic . The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, our approach are more pragmatic. We adopt a larger data i  in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in memory. The number of instructions retired of the big  , HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and compared them with big data analytics workloads.</p>< as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the o n fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The front end inefficiency may caused by high-level langu  Figure <ref type=\"figure\" target=\"#fig_3\">5</ref>.</p><p>Implications: Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and the service wor ns, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and large binary size complicated by high-level language, sor and save the die area. For the service workloads, our observation corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id=\"formula_ s in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type=\"bibr\" target=\"#b16\">[17]</ref>. According to our correlation analysis in this section, ar ten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Moreover, for mod s prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and etc. Narayanan et al. <ref type=\"bibr\" target=\"#b32\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ive progress made in many generation tasks, neural systems are known to produce low-quality content <ref type=\"bibr\" target=\"#b51\">(Wiseman et al., 2017;</ref><ref type=\"bibr\" target=\"#b43\">Rohrbach e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>The Transformer <ref type=\"bibr\" target=\"#b45\">[45]</ref> is well acknowledged as the most powerful neural network i f>.</p><p>Transformer. The Transformer architecture consists of a composition of Transformer layers <ref type=\"bibr\" target=\"#b45\">[45]</ref>.</p><p>Each Transformer layer has two parts: a self-attent sequential data, one can either give each position an embedding (i.e., absolute positional encoding <ref type=\"bibr\" target=\"#b45\">[45]</ref>) as the input or encode the relative distance of any two p r. Graphormer is built upon the original implementation of classic Transformer encoder described in <ref type=\"bibr\" target=\"#b45\">[45]</ref>. In addition, we apply the layer normalization (LN) before. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#foot_0\">3</ref>  <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. They analyzed the relationship between internal properties bibr\" target=\"#b4\">[5]</ref>, narcissism <ref type=\"bibr\" target=\"#b6\">[7]</ref>, self-presentation <ref type=\"bibr\" target=\"#b12\">[13]</ref> and the corresponding user profiles on Facebook.</p><p>We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hts into global model behavior.</p><p>Triggers are a new form of universal adversarial perturbation <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017)</ref> adapted to discrete textual inp or anyone to fool machine learning models. Moreover, universal attacks often transfer across models <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017)</ref>, which further decreases attack e adversarial threat is higher if an attack is universal: using the exact same attack for any input <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Br. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f DNNs' performance, researchers and industry practitioners have turned to search-based compilation <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe e performance of programs during the search. We adopt a learned cost model similar to related works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> with newly designed  ion and automatic search. Halide has three versions of auto-scheduler based on different techniques <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" targ >) or aggressive pruning by inaccurately evaluating incomplete programs (e.g. Halide auto-scheduler <ref type=\"bibr\" target=\"#b1\">[2]</ref>), which prevents them from covering a large enough search sp <ref type=\"bibr\" target=\"#b29\">[31]</ref> to search for good decisions (e.g., Halide auto-scheduler <ref type=\"bibr\" target=\"#b1\">[2]</ref>). In this approach, the compiler constructs a tensor program limited search space heuristically.</p><p>(2) Aggressive early pruning (e.g., Halide auto-scheduler <ref type=\"bibr\" target=\"#b1\">[2]</ref>). Aggressive early pruning based on evaluating incomplete pr ntel CPU.</p><p>We include PyTorch <ref type=\"bibr\" target=\"#b34\">[36]</ref>, Halide auto-scheduler <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Flex-Tensor <ref type=\"bibr\" target=\"#b51\">[53]</ref> and A for complete programs but fails to accurately predict the final performance of incomplete programs. <ref type=\"bibr\" target=\"#b1\">(2)</ref> The fixed order of sequential decisions limits the design of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ised learning can be achieved by predicting whether a video has correspondence with an audio stream <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target n=\"3.4\">Multimodal Contrastive Learning</head><p>Inspired by <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, we use Noise Contras. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dels learn contacts in the self-attention maps with state-of-the-art performance. We compare ESM-1b <ref type=\"bibr\" target=\"#b37\">(Rives et al., 2020)</ref>, a large-scale (650M parameters) Transform  LSTM pretrained on protein sequences to fit contacts. <ref type=\"bibr\">Rao et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b37\">Rives et al. (2020)</ref> perform benchmarking of multiple protein la gn a score of 0 for these sequences. We also provide a comparison to the bilinear model proposed by <ref type=\"bibr\" target=\"#b37\">Rives et al. (2020)</ref>. The logistic regression model achieves a l tion, we compare our logistic regression model to the bilinear contact prediction model proposed by <ref type=\"bibr\" target=\"#b37\">Rives et al. (2020)</ref>. This model trains two separate linear proj. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ortant corollary of this formulation is that any GNN with neighborhood sampling, such as Graph-SAGE <ref type=\"bibr\" target=\"#b16\">(Hamilton et al., 2017)</ref>, could be considered as its correspondi nd over-smoothing in GNNs. Sampling-based stochastic reduction by random walk neighborhood sampling <ref type=\"bibr\" target=\"#b16\">(Hamilton et al., 2017)</ref> and node sampling <ref type=\"bibr\" targ entation learning literature to reduce the size of input graphs. In GNNs, specifically in GraphSAGE <ref type=\"bibr\" target=\"#b16\">(Hamilton et al., 2017)</ref>, random walk sampling has been deployed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\">17,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" tar th many extensions <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b56\">58]</ref>. Notably, by integr i-stage BBR (e.g., <ref type=\"bibr\" target=\"#b16\">[18,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\">54,</ref><ref type=\"bibr\" target=\"#b60\">62,  usually works well (as observed in recent studies as well <ref type=\"bibr\" target=\"#b30\">[32,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref>), mBBR works efficiently most of time, with very few iterat tegrating with region proposal network (RPN) <ref type=\"bibr\" target=\"#b43\">[45]</ref>, the work of <ref type=\"bibr\" target=\"#b31\">[33]</ref> greatly improves Siamese tracker in dealing with scale cha  type=\"bibr\" target=\"#b16\">[18]</ref>, DaSiamRPN <ref type=\"bibr\" target=\"#b61\">[63]</ref>, SiamRPN <ref type=\"bibr\" target=\"#b31\">[33]</ref>, Grad-Net <ref type=\"bibr\" target=\"#b33\">[35]</ref>, SA-Si. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed nodes, and this causes GNNs to learn suboptimal representations. Graph attention networks (GATs) <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> adopt self-attention to alleviate thi l., 2016)</ref>. A representative work in a non-spectral way is the graph attention networks (GATs) <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> which model relations in graphs using http://www.tei-c.org/ns/1.0\"><head n=\"3\">MODEL</head><p>In this section, we review the original GAT <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and then describe our selfsupervised   indicate the probability of edge between node i and j. The attention mechanism of the original GAT <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> is in the dashed rectangle.</p><p>Not AGE <ref type=\"bibr\" target=\"#b18\">(Hamilton et al., 2017)</ref>, and graph attention network (GAT) <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref>. Furthermore, for Cora, CiteSeer, and aining process of three runs using a single GPU (GeForce GTX 1080Ti). We compare our model with GAT <ref type=\"bibr\" target=\"#b52\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and GAM <ref type=\"bibr\" target=\"#b46. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> mostly rely on the development of convolutional neural netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: datasets like CIFAR10 or part of ImageNet was proposed in <ref type=\"bibr\" target=\"#b36\">[37]</ref> <ref type=\"bibr\" target=\"#b35\">[36]</ref>[21] <ref type=\"bibr\" target=\"#b25\">[26]</ref>[39] <ref typ respectively, because of the cost of re-executing search. Supernet retraining is needed for FBNetV2 <ref type=\"bibr\" target=\"#b35\">[36]</ref> and AtomNAS <ref type=\"bibr\" target=\"#b27\">[28]</ref>, whi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  segmentation <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In dilated convolutional layers, filter weights are emplo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ilt before prefetching can be useful.</p><p>The closest prior work to ours is that of Nesbit et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> where a Global History Buffer (GHB) is proposed for holding  mlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_3\"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type=\"bibr\" target=\"#b3\">4</ref>. Performance with and without GHL-prefetching. Four cases are . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: node aggregates the representations of its neighbors and combines them with its own representation. <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> pioneered the use of attentionbased ne ms (Definitions 3.1 and 3.2), and derive our claims theoretically (Theorem 1) from the equations of <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref>. Empirically, we use a synthetic probl /head><p>Although the scoring function e can be defined in various ways, the original definition of <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> (Equation ( <ref type=\"formula\" target here different curves denote different queries (h i ).</p><p>Generalization to multi-head attention <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> found it beneficial to employ H separa O (|V|dd + |E|d ). However, by merging its linear layers, GAT can be computed faster than stated by <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref>. For a detailed time-and parametric-co >Setup All models use skip connections <ref type=\"bibr\" target=\"#b23\">(He et al., 2016)</ref> as in <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref>. When previous results exist, we take  ich could not fit this dataset, is provided in Appendix F.1.</p><p>The role of multi-head attention <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> found the role of multi-head attention tional networks <ref type=\"bibr\" target=\"#b48\">(Santoro et al., 2017)</ref>. The GAT formulation of <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref> rose as the most popular framework for \u03b2 \u0177 K (39) = q Q x K + \u03b2 q Q \u0177 K<label>(40)</label></formula><p>C.1 Time Complexity GAT As noted by <ref type=\"bibr\" target=\"#b53\">Veli\u010dkovi\u0107 et al. (2018)</ref>, the time complexity of a single GAT h 0.09 0.06 0.07 0.07 0.11 0.08 0.30 0.07 k0 k1 k2 k3 k4 k5 k6 k7 k8 k9 (a) Attention in standard GAT <ref type=\"bibr\" target=\"#b53\">(Veli\u010dkovi\u0107 et al. (2018)</ref>) k0 k1 k2 k3 k4 k5 k6 k7 k8 k9 q0 q1  N i with equal importance (e.g., mean or max-pooling as AGGREGATE). To address this limitation, GAT <ref type=\"bibr\" target=\"#b53\">(Veli\u010dkovi\u0107 et al., 2018)</ref> instantiates Equation ( <ref type=\"fo foot_0\">1</ref> effectively applying an MLP to compute the score for each query-key pair:</p><p>GAT <ref type=\"bibr\" target=\"#b53\">(Veli\u010dkovi\u0107 et al., 2018)</ref>:</p><formula xml:id=\"formula_5\">e (h  \"><head>C Complexity Analysis</head><p>We repeat the definitions of GAT, GATv2 and DPGAT:</p><p>GAT <ref type=\"bibr\" target=\"#b53\">(Veli\u010dkovi\u0107 et al., 2018)</ref>:</p><p>GATv2 (our fixed version):</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b8\">9]</ref>. The rational is that, by the universal approximation theorem and its variants <ref type=\"bibr\" target=\"#b35\">[36]</ref><ref type=\"bibr\" target=\"#b36\">[37]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ervised (proxy) task harder. At the same time, data mixing techniques operating at either the pixel <ref type=\"bibr\" target=\"#b71\">[70,</ref><ref type=\"bibr\" target=\"#b84\">83,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ech comprehension <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> in noisy environmen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: at some pairs of classes exhibit homophily, while others exhibit heterophily. In belief propagation <ref type=\"bibr\" target=\"#b39\">[40]</ref>, a message-passing algorithm used for inference on graphic =\"#b19\">20]</ref>, loopy belief propagation) are used to solve the problem. Belief propagation (BP) <ref type=\"bibr\" target=\"#b39\">[40]</ref> is a classic messagepassing algorithm for graph-based semi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[5]</ref> 70.01 \u00b1 0.63% 67.26 \u00b1 1.12% Low&amp;SentiBank 70.54 \u00b1 1.00% 68.03 \u00b1 1.36% SentiStrength <ref type=\"bibr\" target=\"#b36\">[29]</ref> 59.30 \u00b1 0.87% 62.78 \u00b1 0.91% USEA <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rable to different noise distributions at test time? Inspired by recent research in computer vision <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref typ  model using a mixture of noisy and clean samples.</p><p>\u2022 We implement a stability training method <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, adapted to the sequence labeling scenario r method to improve robustness is to design a representation that is less sensitive to noisy input. <ref type=\"bibr\" target=\"#b51\">Zheng et al. (2016)</ref> presented a general method to stabilize mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a\">1</ref>)) becomes a good ranking function for retrieval is essentially a metric learning problem <ref type=\"bibr\" target=\"#b20\">(Kulis, 2013)</ref>. The goal is to create a vector space such that r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rameter space and the test-case space.</p><p>We see coverage-guided mutational fuzzing (e.g., RFUZZ <ref type=\"bibr\" target=\"#b2\">[3]</ref>) as complementary to PBT. PBT can be used to quickly find bu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tured data. In particular, the Information Bottleneck (IB) <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> provides a critical principle for representation learning: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rent MegaPipe design naturally fits event-driven servers based on callback or event-loop mechanisms <ref type=\"bibr\" target=\"#b28\">[32,</ref><ref type=\"bibr\" target=\"#b36\">40]</ref>. We mostly focus o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ges during training <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. An alternative to approximate the loss is to approximate t set as its own class <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. Dosovitskiy et al. <ref type=\"bibr\" target=\"#b15\">[16]</re ata. In addition, SwAV works with small and large batch sizes and does not need a large memory bank <ref type=\"bibr\" target=\"#b55\">[56]</ref> or a momentum encoder <ref type=\"bibr\" target=\"#b23\">[24]< h as many classes as images in the dataset. As this approach becomes quickly intractable, Wu et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref> mitigate this issue by replacing the classifier with a mem fferent augmentations of the same image. This solution is inspired by contrastive instance learning <ref type=\"bibr\" target=\"#b55\">[56]</ref> as we do not consider the codes as a target, but only enfo  feature. A similar comparison appears in contrastive learning where features are compared directly <ref type=\"bibr\" target=\"#b55\">[56]</ref>. In Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>, we il bel>2</label></formula><formula xml:id=\"formula_3\">)</formula><p>where \u03c4 is a temperature parameter <ref type=\"bibr\" target=\"#b55\">[56]</ref>. Taking this loss over all the images and pairs of data au cating pass forwards to the assignments. This is similar to the memory bank introduced by Wu et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, without momentum.</p><p>Assignment phase in DeepCluster-v .6 Image classification with KNN classifiers on ImageNet</head><p>Following previous work protocols <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>, we evaluate the q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: multiple graph convolution layers on the item-item graph for Pinterest image recommendation; MEIRec <ref type=\"bibr\" target=\"#b7\">[8]</ref> utilizes metapath-guided neighbors to exploit rich structure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erformance simulators (1) Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Parashar et al., 2019)</ref> and (2) DNN-Chip Predictor <ref type=\"b P-PREDICTOR</head><p>Both Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Parashar et al., 2019)</ref> and <ref type=\"bibr\">DNN-Chip Predictor nces of estimation given by Accelergy<ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop<ref type=\"bibr\" target=\"#b31\">(Parashar et al., 2019)</ref> andDNN-Chip Predictor (Zhao et al., 202  for DNN accelerators (1) Accelergy <ref type=\"bibr\" target=\"#b45\">(Wu et al., 2019)</ref>+Timeloop <ref type=\"bibr\" target=\"#b31\">(Parashar et al., 2019)</ref> and (2) <ref type=\"bibr\">DNN-Chip Predi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e system is still a text-based MC system. <ref type=\"bibr\" target=\"#b2\">Chuang et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b12\">Kuo, Luo, and Chen (2020)</ref> studied the end-to-end spoken questio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  The results are presented in Figure <ref type=\"figure\" target=\"#fig_3\">5</ref>. Muralidhara et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> classified programs with MPKI higher than 10 as high memor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: quence Models for Peptide Binding Prediction</head><p>The approach builds on the UDSMProt-framework <ref type=\"bibr\" target=\"#b11\">[12]</ref> and related work in natural language processing <ref type=  with a concat pooling layer and two fully connected layers. The setup closely follows that used in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, where protein properties were predicted. The smaller data  of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50. Similar to <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the training procedure included 1-cycle learning rate sch e potential of unlabeled peptide data in order to observe similar improvements as seen for proteins <ref type=\"bibr\" target=\"#b11\">[12]</ref> in particular for small datasets.  Turning to MHC Class II uarcy of 0.137, which is is considerably lower than the accuracy of 0.41 reported in the literature <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This effect is a direct consequence of the considerably s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ts decrease along with neighbor hops increasing. Inspired by the architecture design of Transformer <ref type=\"bibr\" target=\"#b30\">[31]</ref>, we leverage attention mechanism to learn the weight of so. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d by the number of stacked layers (depth). Recent evidence <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> reveals that network depth is of crucial importance, and th rk depth is of crucial importance, and the leading results <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" tar to the output <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. In <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, a few intermediat entering layer responses, gradients, and propagated errors, implemented by shortcut connections. In <ref type=\"bibr\" target=\"#b42\">[43]</ref>, an \"inception\" layer is composed of a shortcut branch and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  graph, including auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, and knowledge distill  related tasks simultaneously so that knowledge obtained from each task can be reused by the others <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target ef type=\"figure\" target=\"#fig_1\">1 (c</ref>). This structure is very similar to multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, in which different su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s have been achieved dramatic improvement in SR. Dong et al. <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> first exploit a three-layer convolutional neural network, nam posed method with other SR methods, including bicubic, SRCNN <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRC-N <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: GNNs, e.g., Weisfeiler-Lehman-PE (WL-PE) <ref type=\"bibr\" target=\"#b55\">[55]</ref> and Laplacian PE <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref>. We report the perfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[6,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" target=\"#b32\">34,</ref><ref type=\"bibr\" target=\"#b33\">35]</ref>. Early and late lay br\" target=\"#b33\">[35]</ref>, MobileNetV2 <ref type=\"bibr\" target=\"#b26\">[28]</ref>,</p><p>and UNet <ref type=\"bibr\" target=\"#b32\">[34]</ref>. The final column (f) presents the average results across  type=\"bibr\" target=\"#b33\">[35]</ref>, MobileNetV2<ref type=\"bibr\" target=\"#b26\">[28]</ref>, and UNet<ref type=\"bibr\" target=\"#b32\">[34]</ref>. The final column (f) presents the average results across . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al. used a hybrid method to combine lexico-syntactic patterns, semantic web, and neural networks <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Manzoor et al. proposed a joint-learning framework to simul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d-to-end training <ref type=\"bibr\" target=\"#b10\">[10]</ref><ref type=\"bibr\" target=\"#b11\">[11]</ref><ref type=\"bibr\" target=\"#b12\">[12]</ref><ref type=\"bibr\" target=\"#b13\">[13]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mantic information in an efficient fashion. As reported by <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, this overclustering problem can lead to unnecessary harmful   just memorizes the data instead of learning from the data <ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>To overcome the over-clustering problem, recent works  k just memorizes the data instead of learning from the data<ref type=\"bibr\" target=\"#b47\">[47,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.Ideally, we would like to use just the right amount of negati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resent in natural images, video, and speech. These properties are exploited efficiently by ConvNets <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, which are designed to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b15\">[15]</ref>. There are also a few of studies <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b29\">29]</ref> that adopt the enha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and typically  etup for self-driving cars. Frustum PointNet <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Pointfusion <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Frustum ConvNet <ref type=\"bibr\" target=\"#b24\">[25]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e recommendations <ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref>. With the development of related technologies in the field. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Mix, a data augmentation method for generating sub-sequences along with their labels based on mixup <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref>. Under the active sequence labeling framew ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Sequence Mixup in the Embedding Space</head><p>Mixup <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref> is a data augmentation method that impleme lation-based Regularizations Mixup implements interpolation in the input space to regularize models <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type=\"b r limit rather than a too narrow score range setting.</p><p>For the mixing coefficient \u03bb, we follow <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref> to sample it from Beta(\u03b1, \u03b1) and explore \u03b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e x i t s h a 1 _ b a s e 6 4 = \" t A  We use primitives from an existing code generation framework <ref type=\"bibr\" target=\"#b8\">[9]</ref> to form S e . Our search space includes multi-level tiling o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sic idea is very similar to instruction fetch in Multiscalar <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>: Multiscalar divides the sequential instruction stream into  i o n i s s i m i l a r t o t h a t u s e d b y Multiscalar <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. When a fragment is renamed, the hardware determines which  fragments. This is similar to the idea of traces <ref type=\"bibr\" target=\"#b19\">[20]</ref> or tasks <ref type=\"bibr\" target=\"#b20\">[21]</ref>, except that fragments are completely general, whereas the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: as been a surge of interest in Graph Neural Networks (GNNs) for learning with graph-structured data <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta arn the representations of nodes and graphs. Modern GNNs follow a neighborhood aggregation strategy <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s guaranteed to converge to a local optimum of it.</p><p>Inspired by the previous work on CycleGANs <ref type=\"bibr\" target=\"#b29\">(Zhu et al., 2017)</ref> and dual learning <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> and have connections to the large literature on metric lear otivated by noise contrastive estimation and N-pair losses <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, wherein the ability to discriminate between signal and noi  <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> or N-pair losses <ref type=\"bibr\" target=\"#b42\">[43]</ref>. Typically, the loss is applied at the last layer of a dee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: quency of individual words is naturally much higher. Inspired by recent advances in text generation <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>, we propose a new  roposed concept name generation in taxonomies. Meng et al. <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> applied Seq2Seq to generate keyphrases from scientific arti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ere has been a surge of interest in learning graph representations from data. For example, DeepWalk <ref type=\"bibr\" target=\"#b19\">[20]</ref>, one recent model, transforms a graph structure into a sam thod, which used stochastic gradient descent to optimize matrices from large graphs. Perozzi et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> presented an approach, which transformed graph structure i ep relational information and tuning the threshold of maximum number of vertices.</p><p>2. DeepWalk <ref type=\"bibr\" target=\"#b19\">[20]</ref>. DeepWalk is a method that learns the representation of so uction strategy for vertices with small degrees to achieve the optimal performance. As mentioned in <ref type=\"bibr\" target=\"#b19\">[20]</ref>, for DeepWalk and E-SGNS, we set window size as 10, walk l lti-label classification task by regarding the learned representations as features.</p><p>Following <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, we use the LibLin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p>Many recent sequence labeling frameworks <ref type=\"bibr\" target=\"#b25\">(Ma and Hovy, 2016b;</ref><ref type=\"bibr\" target=\"#b27\">Misawa et al., 2017)</ref> share a very basic structure: a bidirectio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ently flat and do not learn hierarchical representations of graphs. On one hand, it demonstrates in <ref type=\"bibr\" target=\"#b19\">[20]</ref> that hierarchical representations of graphs can be combine ng GNNs with different clustering processes. In particular, the recently proposed approach DIFFPOOL <ref type=\"bibr\" target=\"#b19\">[20]</ref>, a differentiable graph pooling module that can generate h ing to be effective in graph classification tasks, in addition to a user's individual embedding. In <ref type=\"bibr\" target=\"#b19\">[20]</ref>, authors make some efforts in effectively co-training two  world e-commerce tasks of such large scale, including <ref type=\"bibr\" target=\"#b29\">[30]</ref> and <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Our baseline algorithms are as follows:</p><p>\u2022 CGNN: A g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: handle data in unsupervised ways.</p><p>Problems for Importance Identification While previous works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> attempt to quan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d expensive. For instance, the latest SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> trackers respectively utilize 7.1G and 20.3G model Flops a  type=\"bibr\" target=\"#b27\">[30]</ref>, SiamFC++ <ref type=\"bibr\" target=\"#b49\">[52]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref>, while using much fewer Flops and parameters. Best viewed  n existing methods. On Snapdragon 845 Adreno 630 GPU [3], our LightTrack runs 12\u00d7 faster than Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> (38.4 v.s. 3.2 fps), while using 13\u00d7 fewer parameters (1.9 he localization precision. Meanwhile, SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> take the powerful ResNet-50 <ref type=\"bibr\" target=\"#b19\" orted in <ref type=\"bibr\" target=\"#b25\">[28]</ref>. Ocean(off) denotes the offline version of Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref>. Some values are missing because either the tracker is not <ref type=\"bibr\" target=\"#b45\">[48]</ref> DiMP r <ref type=\"bibr\" target=\"#b3\">[6]</ref> Ocean(off) <ref type=\"bibr\" target=\"#b53\">[56]</ref> Ours  <ref type=\"formula\" target=\"#formula_3\">4</ref>). On n, we train the head and the backbone supernets jointly on tracking data. The same as previous work <ref type=\"bibr\" target=\"#b53\">[56]</ref>, the tracking data consists of Youtube-BB <ref type=\"bibr\" th subsequent search images. The hyper-parameters in testing are selected with the tracking toolkit <ref type=\"bibr\" target=\"#b53\">[56]</ref>, which contains an automated parameter tuning algorithm. O f type=\"bibr\" target=\"#b27\">[30]</ref> ATOM <ref type=\"bibr\" target=\"#b11\">[14]</ref> Ocean-offline <ref type=\"bibr\" target=\"#b53\">[56]</ref> SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> Ocea =\"bibr\" target=\"#b53\">[56]</ref> SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> Ocean-online <ref type=\"bibr\" target=\"#b53\">[56]</ref> DiMP-50 <ref type=\"bibr\" target=\"#b3\">[6]</ref> Ours  ECO  is 1.6% and 1.9% superior than SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> and Ocean(off) <ref type=\"bibr\" target=\"#b53\">[56]</ref>, respectively. Besides, if we loosen the computation const e of 0.555, which surpasses SiamFC++(G) <ref type=\"bibr\" target=\"#b49\">[52]</ref> and Ocean-offline <ref type=\"bibr\" target=\"#b53\">[56]</ref> by 1.2% and 2.9%, respectively.  Compared to the online Di va 7 and Xiaomi Mi 8. We observe that SiamRPN++ <ref type=\"bibr\" target=\"#b27\">[30]</ref> and Ocean <ref type=\"bibr\" target=\"#b53\">[56]</ref> cannot run at real-time speed (i.e., &lt; 25 fps) on these ef type=\"bibr\" target=\"#b27\">[30]</ref>, SiamFC++<ref type=\"bibr\" target=\"#b49\">[52]</ref> and Ocean<ref type=\"bibr\" target=\"#b53\">[56]</ref>, while using much fewer Flops and parameters. Best viewed  > (middle). In essence, it is a variant of Siamese tracker <ref type=\"bibr\" target=\"#b27\">[30,</ref><ref type=\"bibr\" target=\"#b53\">56]</ref>. Specifically, it takes a pair of tracking images as the in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b3\">[4]</ref> and Neural Network structure with Connectionist temporal classification (CTC) loss <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The hybrid hidden   by the acoustic model, and finally get better results. y * = arg max y log p(y|x) + \u03bb log P LM (y) <ref type=\"bibr\" target=\"#b4\">(5)</ref> where P LM (y) is provided by the LM, y * denotes the final . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ely studied in recent years. In the following we discuss the most related work and refer readers to <ref type=\"bibr\" target=\"#b34\">[36,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  lead to significant performance loss compared to the homophilous regime due to assumption mismatch <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ t against structural attacks. Inspired by recent works on improving GNNs for heterophilous settings <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ  node classification, several GNN designs for handling heterophilous connections have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b14\">Dosovitskiy et al., 2014;</ref><ref type=\"bibr\" target=\"#b41\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>.</p><p>In this work, we introduce a simple rget=\"#b33\">(Krizhevsky et al., 2012;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>, it has not been considered as a systemati br\" target=\"#b22\">H\u00e9naff et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>. However, it is not clear if the success o tation learning methods:</p><p>\u2022 DIM/AMDIM <ref type=\"bibr\" target=\"#b24\">(Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref> achieve global-to-local/local-to-neighbor  gure\" target=\"#fig_0\">1</ref>), but it is also simpler, requiring neither specialized architectures <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et a ibr\" target=\"#b20\">(Zhang et al., 2016;</ref><ref type=\"bibr\" target=\"#b41\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Kolesnikov e n is useful for self-supervised learning <ref type=\"bibr\" target=\"#b11\">(Doersch et al., 2015;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et al the default nonlinear projection with one additional hidden layer (and ReLU activation), similar to <ref type=\"bibr\" target=\"#b1\">Bachman et al. (2019)</ref>. We observe that a nonlinear projection is tch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2019)</ref>, which achieves 91.2% with a model 25\u00d7 la. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: struction performance. <ref type=\"bibr\">Kim et al.</ref> propose a 20-layer CNN model known as VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which adopts residual learning and adaptive gradient clip eover, the traditional convolutional networks usually adopt cascaded network topologies, e.g., VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref> and DRC-N <ref type=\"bibr\" target=\"#b12\">[13]</ref>. In th cise structure of the proposed IDN, it is much faster than several CNN-based SR methods, e.g., VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRCN <ref type=\"bibr\" target=\"#b12\">[13]</ref>, LapSRN <r o accelerate SRCNN in combination with smaller filter sizes and more convolution layers. Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> propose a very deep CNN model with global residual archite nerate the residual image. The bias term of this transposed convolution can auto-Dataset Scale VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref> DRCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> LapSRN <ref  bicubic, SRCNN <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRC-N <ref type=\"bibr\" target=\"#b12\">[13]</ref>, LapSRN < v> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.1\">Training datasets</head><p>By following <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e nodes into a K-dimensional vector space, which preserves certain properties among nodes. Deepwalk <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref>, LINE <ref type=\"bibr\" target=\"#b6\">[Tang et tain properties among nodes. Deepwalk <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref>, LINE <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref> and Node2vec <ref type=\"bibr\" target=\"#b2\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p://www.tei-c.org/ns/1.0\"><head n=\"2\">Related Work</head><p>Modern CNN Architectures. Since AlexNet <ref type=\"bibr\" target=\"#b33\">[34]</ref>, deep convolutional neural networks <ref type=\"bibr\" targe t convolutional kernels. ResNeXt <ref type=\"bibr\" target=\"#b60\">[61]</ref> adopts group convolution <ref type=\"bibr\" target=\"#b33\">[34]</ref> in the ResNet bottle block, which converts the multi-path . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref> have gone a step   and extracts meta-path features to represent the connectivity between users and items.</p><p>\u2022 CKE <ref type=\"bibr\" target=\"#b22\">[23]</ref> combines CF with structural, textual, and visual knowledge presentation vectors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. However, commonly-used KGE methods focus on modeling rigor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te networks do not always transfer to the target model, especially when conducting targeted attacks <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">Narodytska &am :</p><p>Black-box setting. In this paper, we use the definition of black-box access as query access <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al., 20 orders of magnitude more query-efficient than previous methods based on gradient estimation such as <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>. We show that our approach reliably produces  imated by querying the classifier rather than computed by autodifferentiation. This idea is used in <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>, where the gradient is estimated via pixel-by =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.2.\">BLACK-BOX ATTACKS WITH GRADIENT</head><p>ESTIMATION <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref> explore black-box gradient estimation methods mpability of the 2 and \u221e metric as well as the fixed-budget nature of the optimization algorithm in <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>, our method takes far fewer queries to genera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et is one of the earliest deep CNNs, and has been employed in many image scene classification tasks <ref type=\"bibr\" target=\"#b35\">(Nogueira et al., 2017;</ref><ref type=\"bibr\" target=\"#b21\">Han et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, show substantially better effectiveness at the cost of orders of magnitude longer inference time <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta ><p>Recently, the issue of efficiency gained traction in the neural IR community. Hofst\u00e4tter et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> establish efficiency baselines for common neural IR models. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: produced by the implicit assumption of periodicity of the input signal induced by circulant samples <ref type=\"bibr\" target=\"#b38\">[39]</ref>, a variety of advanced DCF-based trackers have been propos introduced a fixed opposite Gaussian-shaped spatial weighting mask for correlation filters learning <ref type=\"bibr\" target=\"#b38\">[39]</ref>, concentrating the energy of the learned filters on the ce #b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In this work, noting that each feature map (channel) refl SDCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, BACF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SRDCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Staple <ref type=\"bibr\" target=\"#b51\">[52]</ref>, STAPLE_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uately explored <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bib  of channels, include irrelevant and redundant information <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The filters trained with such feature maps often contain  ion redundancy of the feature maps in DCF-based tracking <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Besides, for spatial regularisation, existing DCF tracker ture extraction <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>. However, these CF-based trackers <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In this work,   visual object tracking approaches, such as ASRCF <ref type=\"bibr\" target=\"#b15\">[16]</ref>, GFSDCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, BACF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SRDCF <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: deep metric learning <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, part-based methods < stage, we use the meta-test D u to compute the domain loss and relation alignment loss with Eq. (3) <ref type=\"bibr\" target=\"#b6\">(7)</ref>, which is formulated as follows:</p><formula xml:id=\"formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s root-mean-square deviation of atomic positions (RMSD), computed by the Kabsch alignment algorithm <ref type=\"bibr\" target=\"#b26\">[Kabsch, 1976]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1 matrices of 3D coordinates after being SE(3)-aligned a priori (using the Kabsch alignment algorithm <ref type=\"bibr\" target=\"#b26\">[Kabsch, 1976]</ref>). Next, we introduce four types of metrics to co bling is done using all nodes except one, in turn. We align all S i together using Kabsch algorithm <ref type=\"bibr\" target=\"#b26\">[Kabsch, 1976]</ref> <ref type=\"foot\" target=\"#foot_2\">7</ref> , foll. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6\">[37]</ref> (with 2,122M sequences). The latter merged all protein sequences available in UniProt <ref type=\"bibr\" target=\"#b37\">[38]</ref> and proteins translated from multiple metagenomic sequenci  BFD <ref type=\"bibr\" target=\"#b35\">[36]</ref>, more than an order of magnitude larger than UniProt <ref type=\"bibr\" target=\"#b37\">[38]</ref>, the standard in the field. Although bigger did not equate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ossibility to translate programming languages with machine translation. For instance, Nguyen et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> trained a Phrase-Based Statistical Machine Translation (PB  3. Chen et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> used the Java-C# dataset of Nguyen et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> to translate code with tree-to-tree neural networks.</p><p arget=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, which is not a reliable metric, as a generation can be a v arget=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, or other metrics based on the relative overlap between the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s seek to make the class distribution more balanced, using over-sampling or downsampling techniques <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>; algorithm-level app t reduces this imbalance, but can lead to over-fitting as no extra information is introduced. SMOTE <ref type=\"bibr\" target=\"#b7\">[8]</ref> addresses this problem by generating new samples, performing nority so as to alleviate the issue of majority classes dominating the loss function.</p><p>\u2022 SMOTE <ref type=\"bibr\" target=\"#b7\">[8]</ref>: Synthetic minority oversampling techniques generate synthet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s posed on 12,744 long news articles. <ref type=\"foot\" target=\"#foot_2\">3</ref> Since previous SOTA <ref type=\"bibr\" target=\"#b42\">[43]</ref> is not BERT based (due to long texts) in NewsQA, to keep t , for example BiDAF <ref type=\"bibr\" target=\"#b40\">[41]</ref> (+17.8% F 1 ), previous SOTA DECAPROP <ref type=\"bibr\" target=\"#b42\">[43]</ref>, which incorporates elaborate self-attention and RNN mecha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  embeddings during training.</p><p>We use the Averaged Stochastic Gradient Descent (ASGD) algorithm <ref type=\"bibr\" target=\"#b27\">(Polyak and Juditsky, 1992)</ref> to train the LM, with 0.4 as the dr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>Finally, a recent trend in computer graphics has been the use of rendered images as textures <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a result, it has become desirable to unify the framebuff  accessed in parallel is possible if the texels are stored in a morton order within the cache lines <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Morton order implies that the texels are stored in 2x2 bloc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lenge, we propose a novel nonlinear feature decorrelation approach based on Random Fourier Features <ref type=\"bibr\" target=\"#b43\">[45]</ref> with linear computational complexity. As for the second ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y. Several methods <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> have utilized the binary representations of users and items. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . 2 This is a product-review dataset crawled from Amazon.com, a well-known online shopping platform <ref type=\"bibr\" target=\"#b21\">[22]</ref>. The entire data is split into several subsets according t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral angle is a commonly used distance metric to measure the difference between two spectral vectors <ref type=\"bibr\" target=\"#b22\">(Kruse et al., 1993)</ref>. The reflectance of individual pixel is re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: amounts of supervision. However, even systems that have relied extensively on unsupervised features <ref type=\"bibr\" target=\"#b7\">(Collobert et al., 2011;</ref><ref type=\"bibr\" target=\"#b36\">Turian et s, and present a hybrid tagging architecture. This architecture is similar to the ones presented by <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref> and <ref type=\"bibr\" target=\"#b19\">Huang ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">Pretrained embeddings</head><p>As in <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref>, we use pretrained word embeddings to in g of our    Several other neural architectures have previously been proposed for NER. For instance, <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref> uses a CNN over a sequence of word embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b62\">Son et al., 2013;</ref><ref type=\"bibr\" target=\"#b63\">Soontranon et al., 2015;</ref><ref type=\"bibr\" target=\"#b82\">Zhou et al., 2017)</ref>. In the past decades, crop classification ha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g performs best. We now perform an evaluation in the more realistic setting of the UniRef50 dataset <ref type=\"bibr\" target=\"#b40\">(Suzek et al., 2007)</ref>. First we examine MSA depth across UniRef5 ef type=\"bibr\" target=\"#b40\">(Suzek et al., 2007)</ref>. First we examine MSA depth across UniRef50 <ref type=\"bibr\" target=\"#b40\">(Suzek et al., 2007)</ref>. Appendix C.4 Fig. <ref type=\"figure\" targ EF50 TRAINING DATA AND SETUP</head><p>For the experiments in Section 4.2, we retrieve the UniRef-50 <ref type=\"bibr\" target=\"#b40\">(Suzek et al., 2007)</ref> database dated 2018-03. The UniRef50 clust. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b18\">(Schein et al., 2016;</ref><ref type=\"bibr\" target=\"#b1\">Ben-Younes et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Yang and Hospedales, 2017)</ref>, factorizes a tensor into a core ten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> show that GNNs are vulnerable to poisoning attacks which ad get=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. As already noted, such attacks can be (i) node specific, a ) node specific, as in the case of a target evasion attack <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> that is designed to ensure that the GNNs are fooled into mi graph. As shown by <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, both node specific and non-target attacks can be executed  n adversary can attack the GNNs by poisoning the graph data used for training. For example, Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> shows that by adding the adversarial perturbations on the  des in the graph so as to reduce the accuracy of the resulting graph neural networks.</p><p>Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> is one of the first methods that perturbs the graph data t  Baseline Methods. Though there are several adversarial attack algorithms on graphs such as Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> and RL-S2v <ref type=\"bibr\" target=\"#b11\">[12]</ref>, most //www.tei-c.org/ns/1.0\"><head n=\"5.2.1\">Node Classification on Clean</head><p>Graph. As the Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> points out that \"poisoning attacks are in general harder a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tering transforms or certain CNN architectures have been shown to be stable to spatial deformations <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ ze the stability of GCNs to small deformation of the underlying random graph model. Similar to CNNs <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, studying GCNs in th es of stability are often balanced by discussions on how the representation preserves signal (e.g., <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ ine intuitive notions of deformations and stability in the continuous world like the Euclidean case <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ p><p>Related work on stability. The study of stability to deformations has been pioneered by Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref> in the context of the scattering transform for signals on  ph models and to obtain deformation stability bounds that are similar to those on Euclidean domains <ref type=\"bibr\" target=\"#b31\">[32]</ref>. We note that <ref type=\"bibr\" target=\"#b28\">[29]</ref> al eformations is an essential feature for the generalization properties of deep architectures. Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref> studied the stability to small deformations of the wavelet for small enough \u2207\u03c4 \u221e , we obtain N P (\u03c4 ) d \u2207\u03c4 \u221e , recovering the more standard quantity of Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref>. In this case, we also have the bound</p><formula xml:id=\"  ). Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>Proposition 4 (Signal deformation). Consider a GCN  \u03c4 \u221e , the GCN is invariant to translations and stable to deformations, similar to Euclidean domains <ref type=\"bibr\" target=\"#b31\">[32]</ref>. We note that studies of stability are often balanced by d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t information for making matching decisions. The last technique, data augmentation, is adapted from <ref type=\"bibr\" target=\"#b30\">[31]</ref> for EM to help D learn \"harder\" to understand the data inv  address this issue, D applies MixDA, a recently proposed data augmentation technique for NLP tasks <ref type=\"bibr\" target=\"#b30\">[31]</ref> illustrated in Figure <ref type=\"figure\" target=\"#fig_3\">3 h the entry_swap operator. We compare the different combinations and report the best one. Following <ref type=\"bibr\" target=\"#b30\">[31]</ref>, we apply MixDA with the interpolation parameter \u03bb sampled arget=\"#b58\">59</ref>]. We designed a set of DA operators suitable for EM and apply them with MixDA <ref type=\"bibr\" target=\"#b30\">[31]</ref>, a recently proposed DA strategy based on convex interpola nd span_shuffle. These two operators are used in NLP tasks <ref type=\"bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> and shown to be effective for text classification. For span DA) has been extensively studied in computer vision and has recently received more attention in NLP <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ropy-based method to model a decision-maker's subjective utility for a criterion value. Zhou et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> showed an ave-entropy based weight assignment process cons. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arn both long-term interests and short-term interests of such implicit feedbacks. As Jannach et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref> noted that both the users' short-term and long-term interest. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e failing test case to a minimal failing case once a bug is discovered.</p><p>Compared to BlueCheck <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a prior PBT framework for hardware, the key distinctions ar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >(Zhang et al., 2017;</ref><ref type=\"bibr\">2018)</ref>, integrating attention and auxiliary losses <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, and leveraging additional sources of conditi Results</head><p>We evaluate our model zero-shot by comparing it to three prior approaches: AttnGAN <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2018)</ref>, DM-GAN <ref type=\"bibr\" target=\"#b52\">(Zhu e reas et al., 2017)</ref>, and is also similar to the auxiliary text-image matching loss proposed by <ref type=\"bibr\" target=\"#b49\">Xu et al. (2018)</ref>. Unless otherwise stated, all samples used for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 9b)</ref>) to be factual knowledge bases <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bouraoui et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Jiang et al Ettinger, 2020)</ref> and world knowledge <ref type=\"bibr\" target=\"#b5\">(Davison et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bouraoui et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">Forbes et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  use datasets from Open Graph Benchmark (OGB) <ref type=\"bibr\" target=\"#b52\">[52]</ref>, TU Dataset <ref type=\"bibr\" target=\"#b73\">[73]</ref> and ZINC <ref type=\"bibr\" target=\"#b74\">[74]</ref> for gra e evaluate AD-GCL on semi-supervised learning for graph classification on the benchmark TU datasets <ref type=\"bibr\" target=\"#b73\">[73]</ref>. We follow the setting in <ref type=\"bibr\" target=\"#b24\">[ :</head><label>3</label><figDesc>Semi-supervised learning performance with 10% labels on TU datasets<ref type=\"bibr\" target=\"#b73\">[73]</ref> (10-Fold Accuracy (%)\u00b1 std over 5 runs). Bold/Bold \u22c6 indic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that pulls representations of noisy samples away from clean ones. Finally, mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> has recently demonstrated outstanding robu ushing the state-of-the-art one step forward by combining our approach with mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref>.</p><p>4. Guiding mixup data augmentation   bootstrapping <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> to deal with the closed-set label noise sc i-c.org/ns/1.0\"><head n=\"3.3.\">Joint label correction and mixup data augmentation</head><p>Recently <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> proposed a data augmentation technique nam ref type=\"bibr\" target=\"#b9\">(Hendrycks et al., 2018)</ref>), and use the configuration reported in <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> for mixup. We outperform the related work  tab_8\">6</ref> shows the results of the proposed approaches M-DYR-H and MD-DYR-SH compared to mixup <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> on TinyImageNet to demonstrate that our ap onstrate that our approach is useful far from CIFAR data. The proposed approach clearly outperforms <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> for different levels of label noise, obtai ref type=\"bibr\" target=\"#b11\">Jiang et al., 2018b;</ref><ref type=\"bibr\">Patrini et al., 2017;</ref><ref type=\"bibr\" target=\"#b34\">Zhang et al., 2018)</ref> modify either the loss directly, or the pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /expressing emotions and emotionally interacting with the interlocutors. In literature, Zhou et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> successfully build an emotional chat machine (ECM) that is enerate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Hence, the response generation problem faces a significan tional factors, which are most related to our proposed conversation generation problem. Zhou et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> develop an Emotional Chat Machine (ECM) model using three  \" target=\"#b32\">[33]</ref>, to evaluate our experimental results. In particular, we follow the work <ref type=\"bibr\" target=\"#b45\">[46]</ref> to train an emotion classifier for assigning emotional lab ifferent datasets, i.e., NLPCC 2013 2 and NLPCC 2014 3 emotion classification datasets by following <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which contain 29, 417 manually annotated data in total, a o any emotion information, and rare emotion categories like fear are removed. In particular, unlike <ref type=\"bibr\" target=\"#b45\">[46]</ref> using solely one label for classification, we consider bot rget=\"#b35\">[36]</ref>, the traditional Seq2seq model is adopted as one of our baselines.</p><p>ECM <ref type=\"bibr\" target=\"#b45\">[46]</ref>, as mentioned, ECM model is improper to directly be as the onal matrix (if used). The parameters of imemory and ememory in ECM are the same as the settings in <ref type=\"bibr\" target=\"#b45\">[46]</ref>. We use stochastic gradient descent (SGD) with mini-batch  <note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">Here we follow the work<ref type=\"bibr\" target=\"#b45\">[46]</ref>, where the emotion categories are {Angry, Disgust, Happy,  o the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, Seq2seq with emotion embedding (Seq2seqemb) is also adopte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that workers be in the United States and have a &gt; 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a pay rate of at least $15/hou eted at least 5,000 HITs, and to have greater than a 98% acceptance rate. We use the Fair Work tool <ref type=\"bibr\" target=\"#b35\">(Whiting et al., 2019)</ref> to ensure a minimum of $15 hourly wage.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bib s the substitution of an indirect method call with direct method calls in object-oriented languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bib nce impact due to virtual function calls. These approaches include the method cache in Smalltalk-80 <ref type=\"bibr\" target=\"#b10\">[11]</ref>, polymorphic inline caches <ref type=\"bibr\" target=\"#b23\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  challenged by their vulnerability to adversarial examples <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, which are crafted by adding small, human-imperceptible noise icted labels and probabilities of these images given by the Inception v3. more varied training data <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p><p>With the knowl >, adversarial training is the most extensively investigated way to increase the robustness of DNNs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ef type=\"bibr\" target=\"#b22\">[23]</ref>, one-step gradient-based methods such as fast gradient sign <ref type=\"bibr\" target=\"#b4\">[5]</ref> and iterative variants of gradient-based methods <ref type=\" t-based methods that iteratively perturb the input with the gradients to maximize the loss function <ref type=\"bibr\" target=\"#b4\">[5]</ref>, momentum-based methods accumulate a velocity vector in the  e-box attacks and the transferability, and act as a stronger attack algorithm than one-step methods <ref type=\"bibr\" target=\"#b4\">[5]</ref> and vanilla iterative methods <ref type=\"bibr\" target=\"#b8\"> ply derived.</p><p>One-step gradient-based approaches, such as the fast gradient sign method (FGSM) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, find an adversarial example x * by maximizing the loss func. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e-based Masked Language Model task is inspired by the Masked Language Model (MLM) objective of BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, and semantic mask for ASR task <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tence <ref type=\"bibr\" target=\"#b4\">(Bangalore et al., 2007)</ref>, or the Operation Sequence Model <ref type=\"bibr\" target=\"#b17\">(Durrani et al., 2015;</ref><ref type=\"bibr\" target=\"#b44\">Stahlberg . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 18</ref><ref type=\"bibr\">), CJRC (Duan et al., 2019</ref>) \u2022 Natural Language Inference (NLI): XNLI <ref type=\"bibr\" target=\"#b4\">(Conneau et al., 2018)</ref> \u2022 Sentiment Classification (SC): ChnSenti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arned the fine-grained multimodal representations, inspired by the knowledge distillation technique <ref type=\"bibr\" target=\"#b24\">(Romero et al. 2015)</ref>, we further introduce the Multimodal Knowl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b9\">10]</ref> 2) handcrafted invariant features w/ and w/o deep learning, e.g. <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ated calculations in high-dimensional space for correct prediction. As shown in the computer vision <ref type=\"bibr\" target=\"#b10\">[11]</ref>, a simple image without complex background or patterns can. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t-dependent sentiment classification. The approach is an extension on long short-term memory (LSTM) <ref type=\"bibr\" target=\"#b6\">(Hochreiter and Schmidhuber, 1997)</ref> by incorporating target infor e. These gates adaptively remember input vector, forget previous history and generate output vector <ref type=\"bibr\" target=\"#b6\">(Hochreiter and Schmidhuber, 1997)</ref>. LSTM cell is calculated as f problem of gradient vanishing or exploding <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 1994;</ref><ref type=\"bibr\" target=\"#b6\">Hochreiter and Schmidhuber, 1997)</ref>, where gradients may grow or d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: structions a di cult task, even in the presence of aiding devices like a hardware trace cache (HTC) <ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Indeed, a trace cach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ffline sparsification to retain the most important edges in the graph. Inspired by previous studies <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ on matrix multiplication, which is infeasible for a large-scale graph. Inspired by previous studies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we obtain the personalized PageRank for a relation \ud835\udc45 using   allows us to simply truncate small values of \u03a0 \ud835\udc45 and recover sparsity. Similar to previous studies <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we use the top-k entries with the highest mass per column a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er, recent research showed that an attacker could generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ b0\">(1)</ref> Training the target classifier with adversarial examples, called adversarial training <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>; <ref type=\"bibr\" ta cally, researchers showed that it was possible to generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ  one may use a mixture of normal and adversarial examples in the training set for data augmentation <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, or mix the advers =\"2.3\">Existing attacks</head><p>Since the discovery of adversarial examples for neural networks in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, researchers have found adversarial examples on various ne h leveraged gradient based optimization from normal examples <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Moosavi et al. showe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al., 2020b)</ref> 75.6 Albert <ref type=\"bibr\">(Lan et al., 2020) (ensemble)</ref> 76.5 UnifiedQA * <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref> 79.1</p><p>RoBERTa + QA-GNN (Ours) 76.1 6 Albert + KB 81.0 T5 * <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref> 83.2 UnifiedQA * <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref> 87.2</p><p>AristoRoBERTa + QA-GNN (Ours y, the top two systems, T5 <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref> and UnifiedQA <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref>, are trained with more data and use 8x . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that the model manifold and the true distribution's support have a non-negligible intersection (see <ref type=\"bibr\" target=\"#b0\">[1]</ref>), and this means that the KL distance is not defined (or sim ining GANs is well known for being delicate and unstable, for reasons theoretically investigated in <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>In this paper, we direct our attention on the various  zero. This happens to be the case when two low dimensional manifolds intersect in general position <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>Since the Wasserstein distance is much weaker than th ing gradients, as can be seen in Figure <ref type=\"figure\">1</ref> of this paper and Theorem 2.4 of <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In Figure <ref type=\"figure\" target=\"#fig_1\">2</ref> we sho e <ref type=\"bibr\" target=\"#b3\">[4]</ref>. This last phenomenon has been theoretically explained in <ref type=\"bibr\" target=\"#b0\">[1]</ref> and highlighted in <ref type=\"bibr\" target=\"#b10\">[11]</ref> nerated image, when the pixels were already normalized to be in the range <ref type=\"bibr\">[0,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref>. This is a very high amount of noise, so much that when paper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ort from historical frames, even when difficult challenges occur. A recent representative effort in <ref type=\"bibr\" target=\"#b62\">[64]</ref> develops such a spatial-temporal representation for tracki nt tracking inference. <ref type=\"bibr\" target=\"#b1\">(2)</ref> Long-term representation. In tracker <ref type=\"bibr\" target=\"#b62\">[64]</ref>, spatial-temporal representation is achieved by warping an ype=\"bibr\" target=\"#b47\">49,</ref><ref type=\"bibr\" target=\"#b62\">64]</ref>. Especially, the work of <ref type=\"bibr\" target=\"#b62\">[64]</ref> leverages temporal information for tracking. Despite robus [32,</ref><ref type=\"bibr\" target=\"#b60\">62]</ref>.</p><p>Our work is related to but different from <ref type=\"bibr\" target=\"#b62\">[64]</ref> that uses a large FlowNet <ref type=\"bibr\" target=\"#b13\">[ le for motion dynamics. Besides, we model a long-term representation using RNNs, which differs from <ref type=\"bibr\" target=\"#b62\">[64]</ref> with a short-term representation. Our work is relevant to  get=\"#b42\">44,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" target=\"#b47\">49,</ref><ref type=\"bibr\" target=\"#b62\">64]</ref>. Especially, the work of <ref type=\"bibr\" target=\"#b62\">[64. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on-based models <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b12\">[12]</ref>, <ref type=\"bibr\" target=\"#b52\">[43]</ref> widely used in information retrieval. The interaction-base d by each person to 100.</p><p>The hyper-parameters of the RBF kernel functions are set the same as <ref type=\"bibr\" target=\"#b52\">[43]</ref>. We use 11 RBF kernels, with the hyper-parameters m = f1; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ity and its performance drops dramatically for similarity based tasks, e.g. nearest neighbor search <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" ta s usually built on classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memorized features <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which has limited efficiency and discriminability. We pro  images and predefined noise signals, which constrains the distribution between raw data and noises <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Bolztmann Machines (RBMs) <ref type=\"bibr\" target=\"#b23\"> discriminability. Softmax Embedding with Memory Bank. To improve the inferior efficiency, Wu et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> propose to set up a memory bank to store the instance feat tance feature rather than classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memory bank <ref type=\"bibr\" target=\"#b45\">[46]</ref>. To achieve the goal that features of the same instance un =\"bibr\" target=\"#b2\">[3]</ref> 67.6 Exemplar <ref type=\"bibr\" target=\"#b7\">[8]</ref> 74.5 NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80.8 NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80. </p ype=\"bibr\" target=\"#b7\">[8]</ref> 74.5 NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80.8 NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80. </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><h ead n=\"4.1.\">Experiments on Seen Testing Categories</head><p>We follow the experimental settings in <ref type=\"bibr\" target=\"#b45\">[46]</ref> to conduct the experiments on CIFAR-10 <ref type=\"bibr\" ta  ColorJitter, RandomHorizontalFlip) in PyTorch with default parameters are adopted.</p><p>Following <ref type=\"bibr\" target=\"#b45\">[46]</ref>, we adopt weighted kNN classifier to evaluate the performa  200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label <ref type=\"bibr\" target=\"#b45\">[46]</ref>   plar CNN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoft  type=\"bibr\" target=\"#b45\">[46]</ref>   plar CNN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Triplet N <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Triplet loss with and without hard mining. Triplet (ha and the margin parameter is set to 0.5. DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> represent the state-of-the-art unsupervised feature learni  classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which u hod outperforms it by 9.1%. Compared to NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which use memorized feature for optimizing, the proposed  target=\"#fig_4\">3</ref>. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while <ref type=\"bibr\" target=\"#b45\">[46]</ref> takes 25 epochs and <ref type=\"bibr\" target=\"#b7\">[8]</ref ance features rather than classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memory bank <ref type=\"bibr\" target=\"#b45\">[46]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  d images in ten The classifier is used to predict the label of test samples. We implement NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepClu ct the label of test samples. We implement NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> (c the best accuracy with both classifiers (kNN: 74.1%, Linear: 69.5%), which are much better than NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> un  three state-of-the-art unsupervised methods (Exemplar <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref>) o n Table <ref type=\"table\">3</ref>.</p><p>Generally, the instance-wise feature learning methods (NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, Examplar <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Ours) o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  ActivityNet1.3 dataset as it does on the THUMOS2014 dataset compared with several existing methods <ref type=\"bibr\" target=\"#b58\">[58]</ref>, <ref type=\"bibr\" target=\"#b59\">[59]</ref>, probably due t  that the boundaries of long action instances are captured. Our method performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58] ethod performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> conducts frame-level predictions rather than segment-level 1.3 dataset. Nevertheless, our method yields a higher mAP at a threshold of 0.95 than the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref>, which indicates that our method locates the action bounda e difficult scenarios. Furthermore, although the average mAP of our method is 4% worse than that of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the ActivityNet1.3 dataset, our method achieves a signi ef> on the ActivityNet1.3 dataset, our method achieves a significant improvement over the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the THU-MOS2014 dataset, and the mAP at the threshold o 62\">[62]</ref> and two framelevel proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> for comparison, and the inference speed is directly copied od is slower than the frame-level proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> that perform fully convolutional operations on the frame l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding block for many efficient neural network architectures <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> and we use them in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: u et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Huang et al., 2018)</ref>; In subgraph sampling <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019;</ref><ref type=\"bibr\" target=\"#b49\">Zeng et al., story access, and increases closeness and reduces staleness in return.</p><p>Similar to CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019)</ref>, we make use of graph clustering technique chniques for minibatch selection, as first introduced in the subgraph sampling approach CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019)</ref>. CLUSTER-GCN leverages clustering in order d mini-batch GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019)</ref> training, cf. Table <ref type=\"table\" targ  et al., 2018b)</ref>, MVS-GNN <ref type=\"bibr\" target=\"#b6\">(Cong et al., 2020)</ref>, CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">(Chiang et al., 2019)</ref>, GRAPHSAINT <ref type=\"bibr\" target=\"#b49\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tle as possible. We will elaborate.</p><p>There are previous works that achieve the first condition <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017;</ref><ref type=\"bibr\" target=\"#b17\">Hein &amp; A ef><ref type=\"bibr\" target=\"#b16\">Haber &amp; Ruthotto, 2017)</ref>. For example, Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> bound the Lipschitz constant by requiring   leads to degradation in nominal accuracy, average confidence gap and robustness. Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> can be viewed as models without L c term,   weight matrices and shows its effect in reducing generalization gap. The work on Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> shows that it is possible to control Lipsc ing the first condition, allows greater degrees of freedom in parameter training than the scheme in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>; a new loss function is specially designed   |M i,j | (2)</formula><p>The above is where our linear and convolution layers differ from those in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>: they require W W T to be an identity matri s to be 1; they also propose to restrict aggregation operations. The reported robustness results of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>, however, are much weaker than those by adv  common parameter with (9).</p><p>ResNet-like reconvergence is referred to as aggregation layers in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> and a different formula was used:</p><formu abel></formula><p>) where \u03b1 \u2208 [0, 1] is a trainable parameter. Because splitting is not modified in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>, their scheme may seem approximately equiva  preserve distances. In contrast, because splitting is not modified, at reconvergence the scheme of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> must apply the shrinking factor of 1 \u2212 \u03b1 on ents. We can also have a different t per channel or even per entry.</p><p>To be fair, the scheme of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> has an advantage of being nonexpansive with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng over graph structures using convolution operators that offer promise as an embedding methodology <ref type=\"bibr\" target=\"#b16\">[17]</ref>. So far, graph convolutional networks (GCNs) have only bee nificant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Lastly, we probe the expressive capability of our approac \"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN algorithm <ref type=\"bibr\" target=\"#b16\">[17]</ref> is designed for semi-supervised learning in a transductive r is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework <ref type=\"bibr\" target=\"#b16\">[17]</ref>. In particular, we can derive an inductive variant of the  regator convolutional since it is a rough, linear approximation of a localized spectral convolution <ref type=\"bibr\" target=\"#b16\">[17]</ref>. An important distinction between this convolutional aggre utional\" variant of GraphSAGE is an extended, inductive version of Kipf et al's semi-supervised GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we term this variant GraphSAGE-GCN. We test unsupervised  ctive setting, where it can be extensively trained on a single, fixed graph. (That said, Kipf et al <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref> found that GCN-b d=\"foot_2\">Note that this differs from Kipf et al's exact equation by a minor normalization constant<ref type=\"bibr\" target=\"#b16\">[17]</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place  convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this work we b our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN  less\" GCN approach has parameter dimension O(|V|), so this requirement is not entirely unreasonable <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>.</p><p>Following T \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>). The majority of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cedure. We learn residuals only and use extremely high learning rates (10 4 times higher than SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref>) enabled by adjustable gradient clipping. Our proposed metho ely, random forest <ref type=\"bibr\" target=\"#b20\">[18]</ref> and convolutional neural network (CNN) <ref type=\"bibr\" target=\"#b8\">[6]</ref> have also been used with large improvements in accuracy.</p> 8\">[6]</ref> have also been used with large improvements in accuracy.</p><p>Among them, Dong et al. <ref type=\"bibr\" target=\"#b8\">[6]</ref> has demonstrated that a CNN can be used to learn a mapping f  are highly correlated. Moreover, our initial learning rate is 10 4 times higher than that of SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref>. This is enabled by residual-learning and gradient clipping. d reconstruction. Filters of spatial sizes 9 \u00d7 9, 1 \u00d7 1, and 5 \u00d7 5 were used respectively.</p><p>In <ref type=\"bibr\" target=\"#b8\">[6]</ref>, Dong et al. attempted to prepare deeper models, but failed  ce. We successfully use 20 weight layers (3 \u00d7 3 for each layer). Our network is very deep (20 vs. 3 <ref type=\"bibr\" target=\"#b8\">[6]</ref>) and information used for reconstruction (receptive field) i  for Very Deep Networks Training deep models can fail to converge in realistic limit of time. SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref> fails to show superior performance with more than three weig  a network to converge within a week on a common GPU. Looking at Fig. <ref type=\"figure\">9</ref> of <ref type=\"bibr\" target=\"#b8\">[6]</ref>, it is not easy to say their deeper networks have converged  n of 200 images from Berkeley Segmentation Dataset <ref type=\"bibr\" target=\"#b18\">[16]</ref>. SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref> uses a very large ImageNet dataset.</p><p>We use 291 images   The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al <ref type=\"bibr\" target=\"#b8\">[6]</ref> in their paper based on a GPU implementation.</p><p>In Figur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b22\">(Madotto et al., 2018)</ref>, but also some early works use text as additional information <ref type=\"bibr\" target=\"#b44\">(Xie et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">An et al., 201 target=\"#b37\">(Sun et al., 2019a)</ref> combines the advantages of both of them. Among these works, <ref type=\"bibr\" target=\"#b44\">Xie et al. (2016)</ref> propose to utilize entity descriptions as an . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and natural language processing <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>The novelty of Caser is to represent the previous L where d is the number of latent dimensions and the rows preserve the order of the items. Similar to <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we regard this embedding matrix as the \"image\" of the L i r\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and natural language processing <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref  sing <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, our approach regards the L \u00d7 d matrix E as the \"image\" of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: researchers have found convolutional networks (ConvNets) <ref type=\"bibr\" target=\"#b16\">[17]</ref>  <ref type=\"bibr\" target=\"#b17\">[18]</ref> are useful in extracting information from raw signals, ran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d new items. State-of-art reinforcement learning methods usually apply the simple \u03f5-greedy strategy <ref type=\"bibr\" target=\"#b29\">[31]</ref> or Upper Confidence Bound (UCB) <ref type=\"bibr\" target=\"# dynamic nature of news characteristics and user preference, we propose to use Deep Q-Learning (DQN) <ref type=\"bibr\" target=\"#b29\">[31]</ref> framework. This framework can consider current reward and  avoid the harm to recommendation accuracy induced by classical exploration strategies like \u03f5-greedy <ref type=\"bibr\" target=\"#b29\">[31]</ref> and Upper Confidence Bound <ref type=\"bibr\" target=\"#b21\"> ss stored in the memory to update the network Q.</p><p>Here, we use the experience replay technique <ref type=\"bibr\" target=\"#b29\">[31]</ref> to update the network. Specifically, agent G maintains a m ture of news recommendation and the need to estimate future reward, we apply a Deep Q-Network (DQN) <ref type=\"bibr\" target=\"#b29\">[31]</ref> to model the probability that one user may click on one sp ead><p>The most straightforward strategies to do exploration in reinforcement learning are \u03f5-greedy <ref type=\"bibr\" target=\"#b29\">[31]</ref> and UCB <ref type=\"bibr\" target=\"#b21\">[23]</ref>. \u03f5-greed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f its neighbor pairs <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. For example, SiGMa <ref type=\"bibr\" target=\"#b8\">[9]</ref> enerate candidate user pairs from all the pairs. Following <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, we only keep the user pairs if their names are similar to  gate the matching scores from a confidential seed set of user pairs to their neighbor pairs. COSNET <ref type=\"bibr\" target=\"#b27\">[28]</ref> proposed a supervised method to infer the marginal probabi ls of the unlabeled pairs and update the model based on the inferred labels and the user attributes <ref type=\"bibr\" target=\"#b27\">[28]</ref>. However, error propagations may be introduced in above me fore resorting to the model, we can easily select the most useful neighbor pairs by heuristic rules <ref type=\"bibr\" target=\"#b27\">[28]</ref>. This paper simply selects the neighbor pairs if their nam  by propagating the matching scores (predicted by SVM) through the two input networks.</p><p>COSNET <ref type=\"bibr\" target=\"#b27\">[28]</ref>: is a factor graph model that incorporates the attributes . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 13\">[14]</ref>, obtained from some BGP related protocols <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, or employing the existing control plane routing protocols. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b16\">Phang et al., 2018)</ref>.</p><p>When finetuning a big, pretrained language model, dropout <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014)</ref> has been used as a regularization tec er of that neuron as w during training, then we use (1 \u2212 p)w for that weight parameter at test time <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014)</ref>. This ensures that the expected outpu ght decay of \u03bb is equivalent to wdecay(0, \u03bb).</p><p>Probability for Dropout and Dropconnect Dropout <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014</ref>) is a regularization technique selecti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref> utilizes its proposed model based on audio features into health psychology field. Huang et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> focuses on nonverbal sounds which naturally exists in our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mparison with Peer Methods</head><p>We compare CCA-SSG with classical unsupervised models, Deepwalk <ref type=\"bibr\" target=\"#b31\">[32]</ref> and GAE <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and sel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and Gaofen Image Dataset (GID) <ref type=\"bibr\">[30]</ref>.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>How to better leverage word information for Chinese NER has received continued research attention <ref type=\"bibr\" target=\"#b9\">(Gao et al., 2005)</ref>, where segmentation information has been used. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rks (GANs) <ref type=\"bibr\">[Sabokrou et al., 2018;</ref><ref type=\"bibr\">Perera et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020a]</ref> have been a common choice for novelty detect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ement to corpora, many domain knowledge graphs are available to provide knowledge. For example, OAG <ref type=\"bibr\" target=\"#b46\">[47]</ref> is the largest publicly available heterogeneous academic e ding AMiner <ref type=\"bibr\" target=\"#b40\">[41]</ref>, OAG <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, and Microsoft Academic Graph (MAG) <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: adient-based methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Path-based methods sample paths in each iteration to optim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: out the learning process would be beneficial to enhance the OOD discriminative power of the system. <ref type=\"bibr\" target=\"#b3\">Hendrycks et al. (2019)</ref> demonstrate that utilizing auxiliary dat ing higher likelihood estimates on unseen OOD samples. The ominous observation is presented also by <ref type=\"bibr\" target=\"#b3\">Hendrycks et al. (2019)</ref>, but they concentrate on improving the O r\" target=\"#b8\">Nalisnick et al., 2019a;</ref><ref type=\"bibr\" target=\"#b0\">Choi et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Hendrycks et al., 2019)</ref>. These works report that despite intuiti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ame fold or not. This is also known as one-shot learning and we adopt the siamese architecture from <ref type=\"bibr\" target=\"#b33\">Koch et al. (2015)</ref> using our protein encoder.</p><p>Data. We us uring training. It also shows that our method is flexible enough to take concepts like the one from <ref type=\"bibr\" target=\"#b33\">Koch et al. (2015)</ref>, which uses hierarchical image convolutions,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion to particular regions in images <ref type=\"bibr\" target=\"#b37\">[38]</ref> or words in sentences <ref type=\"bibr\" target=\"#b38\">[39]</ref> in order to extract useful information of interest efficie rforming average-pooling over multiple frames to obtain video-level features, we use self-attention <ref type=\"bibr\" target=\"#b38\">[39]</ref> to assign weights to different frames to obtain a video-le formula><formula xml:id=\"formula_5\">)</formula><p>This is similar in spirit to multi-head attention <ref type=\"bibr\" target=\"#b38\">[39]</ref> that is widely used in transformer models for NLP tasks.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rix, which represents the convolutional kernel in the spectral domain, replacing U f . Spectral CNN <ref type=\"bibr\" target=\"#b0\">(Bruna et al. 2014</ref>) uses a non-parametric convolutional kernel g lution kernel in spectral domain, by leveraging the theory of graph signal processing. Spectral CNN <ref type=\"bibr\" target=\"#b0\">(Bruna et al. 2014</ref>) treats the convolution kernel as a trainable. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2017)</ref>, and have achieved a large success on node classification and link prediction tasks <ref type=\"bibr\" target=\"#b39\">(Velickovic et al., 2018;</ref><ref type=\"bibr\" target=\"#b49\">You et  ction with a soft adjacency matrix, which might be further connected to the Graph Attention Network <ref type=\"bibr\" target=\"#b39\">(Velickovic et al., 2018)</ref>.</p></div> <div xmlns=\"http://www.tei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctly from characters have made rapid progress in recent years, and achieved very high voice quality <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe s the latent variable does in VAE. Therefore, in this paper we intend to introduce VAE to Tacotron2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, a state-of-the-art end-to-end speech synthesis model, to le ate before add operation. The attention module and decoder have the same architecture as Tacotron 2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Then, WaveNet <ref type=\"bibr\" target=\"#b18\">[19]</ref> voc  usually neutral speaking style, is approaching the extreme quality close to human expert recording <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, the interests in expr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \u6ee1\u6c5f\u7ea2), Shuidiaogetou(\u6c34\u8c03\u6b4c\u5934), etc .</p><p>Various methods e.g., <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> have been proposed to generate classical Chinese poetry. Howe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> did not efficiently ised <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, unsupervised <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graph-based one  of author mentions belonging to the same author and are essential in the name disambiguation task. <ref type=\"bibr\" target=\"#b5\">[6]</ref> first learns representation for every name mention in a pair ative attribute to separate papers into small blocks and we use the same trainset and testset as in <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>In Semantic Scholar, the selected meta-paths of our m block as sequence s \u2208 S; 3 Construct meta-path based view {G p1</figDesc><table /><note>\u2022 Aminer-AND<ref type=\"bibr\" target=\"#b5\">[6]</ref>: This dataset contains 70,285 records of 12,798 unique autho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: atures with a linear classifier can achieve good performance on different video analysis benchmarks <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>As for task of video-based emotion recognition, few w  3D ConvNets, the operations are performed spatio-temporally by adding an additional time dimension <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Hence such C3D networks preserve the temporal information o , and 2 fully connected layers, followed by a softmax output layer. Other parameters are similar to <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The specific C3D structure used in our implementation is sh del alone can achieve good performance in action recognition <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. And we found that the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.8\">Comparison Against CESP</head><p>Palacharla et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose the complexity-effective superscalar processor (CE oint out the most closely related work.</p><p>Complexity-Effective Architectures. Palacharla et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose the complexity-effective superscalar processors (C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e most widely used one, <ref type=\"bibr\">BERT [Devlin et al., 2018]</ref> builds on the Transformer <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> architecture and improves the pre-trainin -c.org/ns/1.0\"><head n=\"4.1\">Background of BERT</head><p>Based on a multi-layer Transformer encoder <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> (The transformer architecture has been ub /1.0\"><head>Visit Embedding</head><p>Similar to BERT, we use a multi-layer Transformer architecture <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> as our visit encoder. The model takes the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: fically, to capture the nonlinear relationship between features, we introduce explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> to CCA. Finally, u high computational complexity and memory use. In contrast, recent advances of explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> can convert nonlin l trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref>. Let \u03c6(x) denote a IST features, attribute features, and SentiBank features, we use the random Fourier feature mapping <ref type=\"bibr\" target=\"#b19\">[12]</ref> to approximate the Gaussian kernel. All other histogram-ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recurs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Somogyi et al., build on STMS to combine spatial and temporal streams with their STeMs prefetcher <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>While the GHB improves significantly over table-bas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b5\">[6]</ref>, to advanced methods including support vector machine (SVM) <ref type=\"bibr\" target=\"#b6\">[7]</ref> and random forest (RF) <ref type=\"bibr\" target=\"#b7\">[8]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: models by fine-tuning pre-trained LMs in a simpler architecture.</p><p>Pre-trained LMs such as BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref> and GPT-2 <ref type=\"bibr\" target=\"#b40\">[41]</ref> have d task. See Appendix A for the model architecture. In D , we fine-tune the popular 12layer BERT model <ref type=\"bibr\" target=\"#b12\">[13]</ref>, RoBERTa <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and a  currently support 4 pre-trained models: Distil-BERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, RoBERTa <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and XL Figure <ref type=\"figure\">6</ref> shows the model architecture of D 's language models such as BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DistilBERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, and  best performing pre-trained model among DistilBERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, XLNet <ref type=\"bibr\" target=\"#b60\">[61]</ref>, and RoBE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b34\">[35]</ref>, SEED <ref type=\"bibr\" target=\"#b35\">[36]</ref> and QKCount <ref type=\"bibr\" target=\"#b36\">[37]</ref>) and general-purpose systems (Arabesque <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b23\">Ray 1990;</ref><ref type=\"bibr\" target=\"#b24\">Seeger et al. 2017;</ref><ref type=\"bibr\" target=\"#b25\">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniqu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  due to the over-smoothing and gradient vanishing problems <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. A famous solution to mitigate this problem in computer vis g is the concatenation of  embeddings from all the layers. This adapted version is similar to JKNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s.</p><p>Some work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> proposes different strategy of adding or removing edges to  verages these insights to improve performance in GNN-based node classification via edge prediction. <ref type=\"bibr\" target=\"#b25\">[26]</ref> present the Node-Parallel Augmentation scheme, that create. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ontents <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> and graphs <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, either concate ods, including skip-gram based, such as LINE <ref type=\"bibr\" target=\"#b26\">[27]</ref> and node2vec <ref type=\"bibr\" target=\"#b12\">[13]</ref> and matrix factorization based like NetSMF <ref type=\"bibr b13\">[14]</ref> rely on labeled data for supervised training. Skip-gram based methods like node2vec <ref type=\"bibr\" target=\"#b12\">[13]</ref> or matrix factorization based methods mainly produce struc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hi et al., 2020]</ref>, or as a variant of this problem <ref type=\"bibr\">[Kargar and An, 2011;</ref><ref type=\"bibr\" target=\"#b9\">Le et al., 2014;</ref><ref type=\"bibr\" target=\"#b15\">Yang et al., 2019 e cost function of the vertex-weighted GST problem <ref type=\"bibr\" target=\"#b8\">[Ihler, 1991;</ref><ref type=\"bibr\" target=\"#b9\">Klein and Ravi, 1995]</ref> by introducing a quadratic term sd(v i , v longer paths.</p><p>4.2 Algorithm B 3 F B 3 F is detailed in Algorithm 1. Following common practice <ref type=\"bibr\" target=\"#b9\">[Kacholia et al., 2005;</ref><ref type=\"bibr\" target=\"#b3\">Cheng et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etroni et al., 2020)</ref> and applied to the Reddit-derived \"Explain Like I'm Five\" (ELI5) dataset <ref type=\"bibr\" target=\"#b9\">(Fan et al., 2019)</ref>, which is the only publicly-available large-s lns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">A state-of-the-art LFQA system</head><p>The ELI5 task <ref type=\"bibr\" target=\"#b9\">(Fan et al., 2019)</ref> asks models to generate paragraph-length answ d=\"fig_1\"><head>A. 1</head><label>1</label><figDesc>Dataset StatisticsWe downloaded the ELI5 dataset<ref type=\"bibr\" target=\"#b9\">(Fan et al., 2019)</ref> from the KILT Github repository (https:// git ng set, and almost all validation questions are topically similar to a training set question. While <ref type=\"bibr\" target=\"#b9\">Fan et al. (2019)</ref> attempted to identify and remove question over jor reason for ignoring retrievals is the large amount of train / validation overlap in ELI5. While <ref type=\"bibr\" target=\"#b9\">Fan et al. (2019)</ref> attempted to fix this issue through TF-IDF ove l dataset curation for LFQA tasks is needed to prevent du-plicates. We acknowledge the efforts from <ref type=\"bibr\" target=\"#b9\">Fan et al. (2019)</ref> to fix this issue, and suggest alternative met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tensions such as for example the use of residual connections <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, dense connections <ref type=\"bibr\" target=\"#b4\">[5]</ref> or b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> we favor this formulation over other variants <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. The dice loss is im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b2\">Baydin &amp; Pearlmutter (2014)</ref>, and applied to small problems by <ref type=\"bibr\" target=\"#b12\">Domke (2012)</ref>. However, the na\u00efve approach fails for real-sized  s=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.\">Related work</head><p>The most closely-related work is <ref type=\"bibr\" target=\"#b12\">Domke (2012)</ref>, who derived algorithms to compute reverse-mode de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  future process generations. In this section, we review a technique called pipeline reconfiguration <ref type=\"bibr\" target=\"#b20\">[20]</ref>, that allows a large logical design to be implemented on a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: er could generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targe sible to generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targe ier with adversarial examples, called adversarial training <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>; <ref type=\"bibr\" target=\"#b1\">(2)</ref> Training a classifie from normal examples <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Moosavi et al. showed that it was even possible to find one  \"2.3.1\">Fast gradient sign method(FGSM). Given a normal image</head><p>x, fast gradient sign method <ref type=\"bibr\" target=\"#b4\">[5]</ref> looks for a similar image x \u2032 in the L \u221e neighborhood of x t \"#b21\">22]</ref>, or mix the adversarial objective with the classification objective as regularizer <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Though this idea is promising, it is hard to reason about w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hat backpropagation improves neither accuracy nor detectability of a GCN-based GNN model. Li et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> empirically analyzed GCN models with many layers under the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g is typically conducted on the entire document collection <ref type=\"bibr\" target=\"#b18\">[17,</ref><ref type=\"bibr\" target=\"#b23\">22]</ref>. However, such learning paradigm faces a major drawback in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: satisfactory results as they might still require a relatively large percentage of positive examples <ref type=\"bibr\" target=\"#b22\">[23]</ref>. To handle such incomplete supervision challenge <ref type a reference score for guiding the subsequent anomaly score learning. By leveraging a deviation loss <ref type=\"bibr\" target=\"#b22\">[23]</ref>, GDN is able to enforce statistically significant deviatio ng to the computed anomaly scores with few-shot labels. Here we propose to adopt the deviation loss <ref type=\"bibr\" target=\"#b22\">[23]</ref> to enforce the model to assign large anomaly scores to tho ion as defined in <ref type=\"bibr\" target=\"#b20\">[21]</ref> and is used as the evaluation metric in <ref type=\"bibr\" target=\"#b22\">[23]</ref>. \u2022 Precision@K is defined as the proportion of true anomal those of normal nodes.</p><p>According to previous studies <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, Gaussian distribution is commonly a robust choice to fit t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b18\">[19]</ref> models such as StyleGAN <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. These face GANs are capable of generating faithful faces w >Current face GANs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> are able to generate realistic and vivid human faces with a of pretrained GANs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> is previously exploit  GFP-GAN is comprised of a degradation removal module and a pretrained face GAN (such as Style-GAN2 <ref type=\"bibr\" target=\"#b36\">[37]</ref>) as prior. They are bridged by a latent code mapping and s r the solutions in the natural image manifold and generate realistic textures. Similar to StyleGAN2 <ref type=\"bibr\" target=\"#b36\">[37]</ref>, logistic loss <ref type=\"bibr\" target=\"#b18\">[19]</ref> i ery severe degradation on both details and color. Implementation. We adopt the pretrained StyleGAN2 <ref type=\"bibr\" target=\"#b36\">[37]</ref> with 512 2 outputs as our generative facial prior. The cha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n mAP on MS-COCO <ref type=\"bibr\" target=\"#b41\">[42]</ref> and semantic segmentation mIoU on ADE20K <ref type=\"bibr\" target=\"#b70\">[71]</ref>.</p><p>not even trainable on a GPU with an appropriate per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: slation quality on par or better than both EDITOR and Levenshtein Transformer with hard constraints <ref type=\"bibr\" target=\"#b47\">(Susanto et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c. r\" target=\"#b16\">(Dinu et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Post and Vilar, 2018;</ref><ref type=\"bibr\" target=\"#b47\">Susanto et al., 2020)</ref>. Compared to <ref type=\"bibr\" target=\"#b3 constraints (Table <ref type=\"table\" target=\"#tab_6\">6</ref>). Consistent with previous findings by <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref>, incorporating soft constraints in LevT i s in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on IATE. Enforcing hard constraints as in <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref> increases the term usage by +8-10% and im  they help close the small gap to reach 100% term usage and do not 14 We use our implementations of <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref>'s technique for a more controlled compari b47\">Susanto et al. (2020)</ref>'s technique for a more controlled comparison. The LevT baseline in <ref type=\"bibr\" target=\"#b47\">Susanto et al. (2020)</ref> achieves higher BLEU than ours on the sma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ings for CiteULike, twenty ratings for Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Data statistics are   We follow the widely used leave-one-out evaluation protocol <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For each user, we le rmance of each method with widely used three ranking metrics <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>: hit ratio (H@\ud835\udc41 ), nor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: only provide few partitions and often degrade performance, D-NUCA schemes seldom use them. ASP-NUCA <ref type=\"bibr\" target=\"#b11\">[12]</ref>, ESP-NUCA <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and E tency of private caches. These schemes often size partitions using hill-climbing (e.g., shadow tags <ref type=\"bibr\" target=\"#b11\">[12]</ref> or LRU way hit counters <ref type=\"bibr\" target=\"#b15\">[16. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed learning for computing meaningful and interpretable clusters on input graphs. On the other hand, <ref type=\"bibr\" target=\"#b21\">[22]</ref> proposes an approach that automatically constructs an easy such as link prediction, e-commerce recommendation, etc, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. There are some recent works that learn hierarchical graph raph representation is e-commerce taxonomy for offering a personalized dynamic shopping navigation. <ref type=\"bibr\" target=\"#b21\">[22]</ref> illstrates a topic-driven hierarchical taxonomy based on u </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D. Experiments and Results</head><p>SHOAL <ref type=\"bibr\" target=\"#b21\">[22]</ref> is Alibaba's current topic-driven taxonomy solution deploy iveness, we compare our proposed method with Alibaba's current topic-driven taxonomy solution SHOAL <ref type=\"bibr\" target=\"#b21\">[22]</ref>. In the parameter setting, we set the level number of the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref>). We directly modify the searched architecture by replacing all ReLU activation with H-Swish <ref type=\"bibr\" target=\"#b18\">[19]</ref> activation and equip it with the squeeze-and-excitation mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b1\">[2]</ref>.</p><p>Recently, driven by the advances of graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: into three aspects. (1) Learning domain-invariant features <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>: These methods assu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e their remarkable performance, recent studies show that GCNs are vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, i.e. carefully desi bibr\" target=\"#b37\">38]</ref> try to attack the model by changing training data and evasion attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> try to generate fake after (evasion attacks) the training phase of GCNs. \u2022 Targeted or Non-targeted. In targeted attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, the attacker focus  eted attacks can be further divided into two categories based on attack settings. In direct attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, the attacker can di r example, the attacker tends to connect nodes from different communities to confuse the classifier <ref type=\"bibr\" target=\"#b6\">[7]</ref>. While plain vectors cannot adapt to such changes, Gaussian   into the graph. We regard this method as an illustrating example of non-targeted attacks. \u2022 RL-S2V <ref type=\"bibr\" target=\"#b6\">[7]</ref> <ref type=\"foot\" target=\"#foot_2\">3</ref> : This method gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: users may be given suboptimal recommendations, we follow the literature on conservative exploration <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2016;</ref><ref type=\"bibr\" target=\"#b17\">Garcelon et al. en baseline. Outside pure exploration, in the regret minimization setting, conservative exploration <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2016)</ref> enforces the anytime average performance to b p index) chosen at round s, this requirement is formalized as a conservative exploration constraint <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2016)</ref>:</p><formula xml:id=\"formula_19\">\u2200t, 1 t t s= se 2: 0 was pulled because \u03be t &lt; 0. Here the proof follows similar steps as that of Theorem 5 in <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2016)</ref>.     Recall that \u03a6(t) = min( K k=1 N k (t \u2212 1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  2017)</ref> to graphstructured inputs, building on the recent Graph Attention Network architecture <ref type=\"bibr\" target=\"#b31\">(Veli\u010dkovi\u0107 et al., 2018)</ref>. The result is a powerful, general mo trong baselines. In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of <ref type=\"bibr\" target=\"#b31\">(Veli\u010dkovi\u0107 et al., 2018)</ref>. This encoder consists of PReLU activ ibr\" target=\"#b9\">(Kipf and Welling, 2017)</ref>. Our model extends the graph attention networks of <ref type=\"bibr\" target=\"#b31\">Veli\u010dkovi\u0107 et al. (2018)</ref>, a direct descendant of the convolutio  loss of infor- Graph Transformer Our model is most similar to the Graph Attention Network (GAT) of <ref type=\"bibr\" target=\"#b31\">Veli\u010dkovi\u0107 et al. (2018)</ref>, which computes the hidden representat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and adaptive spatial regularisation <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b15\">[16]</ref> have been explo riminative data fitting, have not been adequately explored <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib few works focusing on reducing the information redundancy of the feature maps in DCF-based tracking <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Besides, for s  CN and CNN, collaboratively for robust feature extraction <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib mples. In contrast to a fixed spatial mask, an adaptive spatial regularisation is proposed in LADCF <ref type=\"bibr\" target=\"#b12\">[13]</ref> to learn an adaptive DCF via spatial feature selection. Th F <ref type=\"bibr\" target=\"#b56\">[57]</ref>, STRCF <ref type=\"bibr\" target=\"#b17\">[18]</ref>, LADCF <ref type=\"bibr\" target=\"#b12\">[13]</ref>, SiamFC <ref type=\"bibr\" target=\"#b26\">[27]</ref>, DSiam < T \u03bb 2 + V j ( X) T V j ( X) \u00d7 (V j ( X) \u0176j +\u03bb 2 V j ( \u0134r )).<label>(13)</label></formula><p>In Eqn. <ref type=\"bibr\" target=\"#b12\">(13)</ref>, J r can be computed directly with predefined P r . Once J. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>Along with wide applications, the major difficulty  \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>. Beyond that, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h each other. Current state-of-the-art models highly depend on Graph Convoluational Networks (GCNs) <ref type=\"bibr\" target=\"#b12\">[13]</ref> originated from the theory of Graph Fourier Transform (GFT tp://www.tei-c.org/ns/1.0\"><head>Spectral Graph Convolution</head><p>The Spectral Graph Convolution <ref type=\"bibr\" target=\"#b12\">[13]</ref> is composed of three steps.</p><p>(1) The multivariate tim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tiple services and deploy them to prevent correlated peaks <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  work is that it can simultaneously also encourage efforts <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b75\">76]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b33\">[34]</ref>, with which GDC achieves a linear runtime O(N ). Furthermo e=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, especially for clustering <ref type=\"bibr\" target=\"#b33\">[34]</ref>, semi-supervised classification <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar  that causes the performance degradation in previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>.</p><p>In this pape  state-of-the-arts for defending against adversarial attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar ing with random normal perturbations) or adversarial noise <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, fail to improve ac pe=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Meanwhile, recent works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" ta etter recognition than the vanilla training baseline. These results contradict previous conclusions <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" ta -1 accuracy on ImageNet, which beats the vanilla training baseline by 0.6%. However, previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> show adversarial t r\" target=\"#b15\">16]</ref> show adversarial training always degrades performance.</p><p>Compared to <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, we make two chang ting noise. However, all previous attempts, by augmenting either with random noise (e.g., Tab. 5 in <ref type=\"bibr\" target=\"#b17\">[18]</ref> shows the result of training with random normal perturbati ojection step in PGD; or (2) we skip the random noise initialization step in PGD, turn it to I-FGSM <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Other attack hyper-parameters are unchanged: the maximum  Vanilla Training 81.7 83.7 84.5 PGD <ref type=\"bibr\" target=\"#b22\">[23]</ref> 81.8 84.3 85.2 I-FGSM <ref type=\"bibr\" target=\"#b17\">[18]</ref> 81.9 84. In Sec. 5.3, we show that adversarial training ca re of adversarial examples and clean images, as suggested in <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>,</p><formula xml:id=\"formula_3\">arg min \u03b8 E (x,y)\u223cD L(\u03b8, x, al and clean domains. However, as observed in former studies <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, directly optimizing Eq. ( <ref type=\"formula\" target=\"#for  stronger performance than the adversarial training baseline <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Besides, compared to the fine-tuning strategy in Sec. 3, A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ce drop.</p><p>To tackle this issue, a socially-aware SSL framework which combines the tri-training <ref type=\"bibr\" target=\"#b46\">[47]</ref> (multi-view co-training) with SSL is proposed in this pape t perspectives and also provide us with a scenario to fuse tri-training and SSL.</p><p>Tri-training <ref type=\"bibr\" target=\"#b46\">[47]</ref> is a popular semi-supervised learning algorithm which expl g, in this paper, matrices appear in bold capital letters and vectors appear in bold lower letters. <ref type=\"bibr\" target=\"#b46\">[47]</ref> is a popular semi-supervised learning algorithm which deve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  conditioned on the categorical priors. Both DBPN <ref type=\"bibr\" target=\"#b9\">[10]</ref> and DSRN <ref type=\"bibr\" target=\"#b8\">[9]</ref> made use of the mutual dependencies of low-and high-resoluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: through which they occur, are important and challenging problems that have attracted much attention <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This paper focuses on predicting protein interfaces. Despi ppears to be saturated. This calls for new methodologies or sources of information to be exploited\" <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Most machine learning methods for interface prediction use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  &amp; Lample, 2019)</ref> have been made for learning cross-lingual representation. More recently, <ref type=\"bibr\" target=\"#b10\">Conneau et al. (2020)</ref> present XLM-R to study the effects of tra =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">DATA AND MODEL</head><p>During pre-training, we follow <ref type=\"bibr\" target=\"#b10\">Conneau et al. (2020)</ref> to build a Common-Crawl Corpus using the  e=\"bibr\" target=\"#b32\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Lan et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Conneau et al., 2020)</ref>. For the latter, <ref type=\"bibr\" target= antic discrepancy among cross-lingual sentences.</p><p>The HICTL is conducted on the basis of XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref> and experiments are performed on several ned a transformer-based model on multilingual Wikipedia which covers various languages, while XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref> studied the effects of training unsuperv e , and 24 layers and 1024 hidden units for HICTL. We initialize the parameters of HICTL with XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref>. Hyperparameters for pre-training and fi s for pre-training HICTL. We use the same vocabulary as well as the sentence-piece model with XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref>. During finetuning on XTREME, we search  n et al., 2019)</ref>, XLM<ref type=\"bibr\" target=\"#b8\">(Conneau &amp; Lample, 2019)</ref> and XLM-R<ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref> are from XTREME<ref type=\"bibr\" target=\" ; Lample, 2019)</ref>, Unicoder <ref type=\"bibr\" target=\"#b22\">(Huang et al., 2019)</ref> and XLM-R <ref type=\"bibr\" target=\"#b10\">(Conneau et al., 2020)</ref>. Results of \u2021 are from our in-house repl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: proaches learn the joint distribution of multimodal data. Multimodal variational autoencoder (MVAE) <ref type=\"bibr\" target=\"#b46\">(Wu and Goodman 2018</ref>) models the joint posterior as a product-o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sting effects <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and the non-local similarity in the images <ref type=\"bibr\" target=\"#b24\">[25]</ref>, have been explored. However, these low-level image priors. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b7\">(Collobert et al., 2011)</ref> and incorporating SP directly into DL models <ref type=\"bibr\" target=\"#b9\">(Dyer et al., 2015)</ref>.</p><p>Current state-of-the-art approaches f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rimary structure and then apply GCNNs to capture the tertiary structure. Also, the recent work from <ref type=\"bibr\" target=\"#b28\">Ingraham et al. (2019)</ref> proposes an amino acid encoder that can . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ifferent camera views. Many works on fully supervised ReID <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> have achieved quite p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: are written in managed languages such as Java, garbage collection is a major consumer of CPU cycles <ref type=\"bibr\" target=\"#b54\">[55]</ref>.</p><p>Trie. We distinguish trie from tree because they ha rators for operations like Malloc <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and garbage collection <ref type=\"bibr\" target=\"#b54\">[55]</ref>, even if the end-to-end performance improvement is not ama ef type=\"bibr\" target=\"#b38\">[39]</ref> and previous works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>, as summarized in Fig. <ref type=\"figure\" target=\"#fig_1\">1 get=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es have been widely used to enhance the performance of many applications such as question answering <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> and personalized r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b34\">(Leshno et al., 1993;</ref><ref type=\"bibr\" target=\"#b44\">Pinkus, 1999;</ref><ref type=\"bibr\" target=\"#b43\">Park et al., 2021)</ref>, and it was chosen only for consistency with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rk based methods have achieved great success in relation extraction, including CNN-based approaches <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and LSTMbased appr ions in a paragraph. To this end, our model consists of a single-sentence module with Piecewise CNN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and a cross-sentence module which leverages self-attentio based methods <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and supervised relation extraction methods <ref type=\"bibr\" target=\"#b39\">[40]</ref>. The pattern-based method <ref type=\"bibr\" target=\"#b13\">[ . In the following we introduce evaluated methods in detail.</p><p>PCNN_single: Piecewise CNN model <ref type=\"bibr\" target=\"#b39\">[40]</ref>, which is one of the state of art single-sentence relation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , however, it also needs to enlarge the TLB entries and OS changes.</p><p>Cache bypassing technique <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> selectively ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  they need large datasets for training and do not adequately capture person-specific idiosyncrasies <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Personalized models like ours and NVP <ref type=\"bibr\" targ ed based on input audio and a reference frame. Among efforts on full-frame synthesis, Video Rewrite <ref type=\"bibr\" target=\"#b4\">[5]</ref> was a pioneering work.</p><p>It represented speech with phon. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cal imaging field <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref>. This task aims a h patient directly <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, while ignoring the relative ranking order among these pati d, we follow the experimental settings of previous methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, i.e., randomly sel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ploy policy gradient <ref type=\"bibr\" target=\"#b34\">[35]</ref> to train the generator. According to <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, the loss function. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings from the captured images, has been widely applied in many applications, such as urban planning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, geographic informatio ref><ref type=\"bibr\" target=\"#b22\">[23]</ref>. There are also a few attempts on building extraction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) <ref type=\"bibr\" target=\"#b24\">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  e.g., paper network <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" target=\"#b20\">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  of NAS methods, especially when the search space is huge. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar  are three basic types: accuracy-based, magnitudebased, and angle-based metrics. For example, PCNAS <ref type=\"bibr\" target=\"#b19\">[20]</ref> drop unpromising operators layer by layer using accuracy a ect for elastic depth. The split point space is set to range <ref type=\"bibr\" target=\"#b8\">(9,</ref><ref type=\"bibr\" target=\"#b19\">20)</ref> to handle different complexity constrains. In total we have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b36\">[37]</ref> to tackle the speaker variability issue. As shown in our recent paper <ref type=\"bibr\" target=\"#b0\">[1]</ref>, VTLP is especially useful when the speaker variability in t  robustness against speaker variability, we apply an onthe-fly VTLP algorithm on the input waveform <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The warping factor is generated randomly for each input utt e frequency, and K is the DFT size. More details about our VTLP algorithm is described in detail in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The acoustic simulator in Fig. <ref type=\"figure\">1</ref> i <ref type=\"bibr\" target=\"#b49\">[50]</ref>. In the example server, we ran the VTLP data augmentation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, acoustic simulator <ref type=\"bibr\" target=\"#b1\">[2]</ref>  c). NBF, VTLP, and AS stand for Neural Beam Former (NBF) [59], Vocal Tract LengthPerturbation (VTLP)<ref type=\"bibr\" target=\"#b0\">[1]</ref> , and Acoustics Simulator (AS)<ref type=\"bibr\" target=\"#b1\"> MFCC features, we use the power mel filterbank energies, since it shows slightly better performance <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Motivated by our pr own scoring and Inverse Text Normalization (ITN) modules, support for power mel filterbank features <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, etc. We have tried . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eful tool for dealing with those aspects of compiler and program development. Models of performance <ref type=\"bibr\" target=\"#b5\">[6]</ref> and energy <ref type=\"bibr\" target=\"#b6\">[7]</ref> can also . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Zeiler and Fergus, 2014)</ref> of the input. The best networks are using more than 150 layers as in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a;</ref><ref type=\"bibr\" target=\"#b7\">He et al., 2016b n computer vision, in particular <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b6\">He et al., 2016a)</ref>.</p><p>This paper is structured as follows. Th ayers <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>. In the remainder of this paper, we describe  serman, 2015)</ref>. We have also investigated the same kind of \"ResNet shortcut\" connections as in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>, namely identity and 1 \u00d7 1 convolutions (see  ng even deeper degrades accuracy. Shortcut connections help reduce the degradation. As described in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>, the gain in accuracy due to the the increase onnections between convolutional blocks that allow the gradients to flow more easily in the network <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>.</p><p>We evaluate the impact of shortcut con works to temporal convolutions as we think this a milestone for going deeper in NLP. Residual units <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>  work and ImageNet is that the latter deals w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns, ranging from social networks <ref type=\"bibr\" target=\"#b12\">(Lin et al., 2020)</ref> to biology <ref type=\"bibr\" target=\"#b26\">(Zitnik et al., 2018)</ref> and chemistry <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref ty N <ref type=\"bibr\" target=\"#b6\">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016</ref>) is unable to distinguish edges with different. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: versarial training <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>; low-rank approximation of graph adjacency <ref type=\"bibr\" t e to certain types of structural and feature perturbations <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. Interested readers a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s on Graph Convolutional Networks (GCNs) <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b8\">Gao et al., 201  large graphs, layer sampling techniques <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b26\">Ying et al., 2 ly a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b14\">Huang et  he above edge sampler to perform layer sampling. Under the independent layer sampling assumption of <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref>, one would sample a connection u ( ) , v ( + b3\">Chen et al. (2018a)</ref>. Point (2) is due to the better interlayer connectivity compared with <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref>, and unbiased minibatch estimator compared w s to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b)</ref> performs sampling from another perspective. , 2016)</ref>, 2. GraphSAGE <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017)</ref>, 3. FastGCN <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b)</ref>, 4. S-GCN <ref type=\"bibr\" target=\"#b3\">(Ch of each layer independently. This is similar to the treatment of layers independently by prior work <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., ip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b52\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b55\">Ye et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">He et al., 2019; y contains the hard negative samples which are beneficial for learning high-quality representations <ref type=\"bibr\" target=\"#b55\">(Ye et al., 2019)</ref>. Specifically, the word-level contrastive los. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ine learning workloads to a GPU or a dedicated accelerator <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b61\">62]</ref>. Communication only happens at the kernel initialization an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ure the representation quality. All of these works sample negative examples from p(x). Arora et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> theoretically analyze the effect of contrastive representati erparameter. Without loss of generality, we set t = 1 for all theoretical results.</p><p>Similar to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we assume an underlying set of discrete latent classes C th f ) = inf W\u2208R K\u00d7d L Softmax (T , W f ).<label>(10)</label></formula><p>In line with the approach of <ref type=\"bibr\" target=\"#b0\">[1]</ref> we analyze the supervised loss of a mean classifier <ref typ commonly the case. The dependence on on N and T in Theorem 5 is roughly equivalent to the result in <ref type=\"bibr\" target=\"#b0\">[1]</ref>, but the two bounds are not directly comparable since the pr .</formula><p>In order to derive our bound we will exploit a concentration of measure result due to <ref type=\"bibr\" target=\"#b0\">[1]</ref>. They consider an objective of the form</p><formula xml:id=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  few data but achieve good performance. A typical example of this approach is prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref>, which averages the vector of few support  http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">Prototypical Networks</head><p>The prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> has achieved excellent performance in few- cia and Bruna, 2018)</ref>, SNAIL <ref type=\"bibr\" target=\"#b10\">(Mishra et al., 2018)</ref>, Proto <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> and PHATT <ref type=\"bibr\" target=\"#b3\">(G  its label, and obviate the need for fine-tuning to adapt to new class types. Prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017</ref>) learns a metric space in which the model c  (y = l i q) = exp(\u2212d(g \u03b8 (q), c i ) \u03a3 L l=1 exp(\u2212d(g \u03b8 (q), c l )<label>(9)</label></formula><p>As <ref type=\"bibr\" target=\"#b13\">Snell et al. (2017)</ref> mentioned, squared Euclidean distance is a  he implementation details are as follows.</p><p>For FewRel dataset, we cite the results reported by <ref type=\"bibr\" target=\"#b13\">Snell et al. (2017)</ref> which includes Finetune, kNN, MetaN, GNN, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b5\">[6]</ref> adopts a random vertex-cut method and two greedy variants for GP.</p><p>GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref> provides some heuristics, such as the grid-based constrained sting vertex-cut methods, such as random method in PowerGraph and grid-based method in GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref>, cannot make effective use of the powerlaw distribution to a \"bibr\" target=\"#b5\">[6]</ref> and the grid-based constrained solution (called Grid) of GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref> are adopted as baselines. Our analysis is based on randomiza. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b15\">16]</ref> further extend the deep models for multimodal learning. <ref type=\"bibr\" target=\"#b16\">[17]</ref> design a cross-media learning method based on DNN, and lev network (CNN) with cross autoencoders to learn the latent high-level attributes on crossmodal units <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Finally, we pro very tweets by utilizing a recently proposed cross-media model, namely the Cross Autoencoders (CAE) <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>An auto encoder is a basic unit in deep neural netw s for comparison with previous work, due to the different goal, our results are not comparable with <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Actually, the most related user-level prediction work is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rent best architectures. This makes DNAS methods inflexible.</p><p>In contrast, single-path methods <ref type=\"bibr\" target=\"#b17\">[18]</ref>[10] <ref type=\"bibr\" target=\"#b8\">[9]</ref>[40] decouple s different search strategies, like the evolution algorithm <ref type=\"bibr\" target=\"#b39\">[40]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref>, can be used to search the architecture under different co huge number of supernet parameters and complex supernet structure. Previous single-path NAS methods <ref type=\"bibr\" target=\"#b17\">[18]</ref>[40] <ref type=\"bibr\" target=\"#b8\">[9]</ref> determine a bl d n=\"2.\">Related Work</head><p>Recently, one-shot NAS <ref type=\"bibr\" target=\"#b25\">[26]</ref>[37] <ref type=\"bibr\" target=\"#b17\">[18]</ref> has received much attention because of reduced search cost uld be retrained for N times. This makes DNAS less flexible.</p><p>In contrast, single-path methods <ref type=\"bibr\" target=\"#b17\">[18]</ref>[10] <ref type=\"bibr\" target=\"#b8\">[9]</ref>[8] decouple su .3.\">Unified Supernet</head><p>Previous single-path NAS <ref type=\"bibr\" target=\"#b9\">[10]</ref>[9] <ref type=\"bibr\" target=\"#b17\">[18]</ref>[40] adopts the MobilenetV2 inverted bottleneck <ref type=\" 2\">[23]</ref>. Besides, we also search the network with FLOPs under 320M by the evolution algorithm <ref type=\"bibr\" target=\"#b17\">[18]</ref> as another baseline. Table <ref type=\"table\" target=\"#tab_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hind scientific publications and some previous works share similar objectives with us. For example, <ref type=\"bibr\" target=\"#b20\">[21]</ref> try to detect topic evolution in scientific literature by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e rank range from 1 to K. As pointed out above, this scheme pre-determines the weight. Rendle et al <ref type=\"bibr\" target=\"#b28\">[29]</ref> proposed an empirical weight for sampling a single positio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  unknown a priori and can vary between one and several thousand conformations for a single molecule <ref type=\"bibr\" target=\"#b6\">[Chan et al., 2021]</ref>. Nevertheless, various facets of the curse o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se approach <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2018)</ref> and its layer-dependent variant <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2018)</ref>. Specifically, GAT <ref type=\"bibr\" target on et al., 2017)</ref>, FastGCN <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2018)</ref>, and AS-GCN <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2018)</ref>. We name this category of approaches as Dr e <ref type=\"table\" target=\"#tab_1\">2</ref>; for the SOTA methods, we reuse the results reported in <ref type=\"bibr\" target=\"#b13\">Huang et al. (2018)</ref>.</p><p>We have these findings: (1) Clearly, ing the testing nodes are unseen for training. We apply the full-supervised training fashion used in<ref type=\"bibr\" target=\"#b13\">Huang et al. (2018)</ref> and<ref type=\"bibr\" target=\"#b2\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ning-to-learn) that can improve the model's generalization <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for the unseen targ with meta-learning <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>: These methods adopted the episodic training paradigm to sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: leased our implementation of ApproxNDCG in Tensorflow in the open-source Tensorflow Ranking library <ref type=\"bibr\" target=\"#b11\">[12]</ref>. <ref type=\"foot\" target=\"#foot_0\">1</ref></p></div> <div . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y similar but nonequivalent samples. To address this issue, we employ smoothed linear interpolation <ref type=\"bibr\" target=\"#b3\">(Bowman et al., 2016;</ref><ref type=\"bibr\" target=\"#b59\">Zheng et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion so far have been linear, based on various methods of factorizing the third-order binary tensor <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011;</ref><ref type=\"bibr\" target=\"#b28\">Yang et al.  expressiveness.</p><p>Finally, we show that several previous stateof-the-art linear models, RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref>, DistMult <ref type=\"bibr\" target=\"#b28\"> lated Work</head><p>Several linear models for link prediction have previously been proposed: RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref>  HypER <ref type=\"bibr\" target=\"#b0\">(Bal d><p>Several previous tensor factorization models can be viewed as a special case of TuckER: RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref> Following the notation introduced in Sect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et, a recent 3D-aware GNN architecture which achieves state-of-the-art on many small-molecule tasks <ref type=\"bibr\" target=\"#b18\">(Klicpera et al., 2019)</ref>, on CASP 11-12. DimeNet differs from GV  xmlns=\"http://www.tei-c.org/ns/1.0\"><head>E ADDITIONAL RESULTS</head><p>E.1 DIMENET ON MQA DimeNet <ref type=\"bibr\" target=\"#b18\">(Klicpera et al., 2019</ref>) is a recent GNN architecture designed t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: x sparse, too. So before the process of data augmentation, we first use the update rule proposed in <ref type=\"bibr\" target=\"#b2\">[3]</ref> through the original adjacency matrix A to build new edges b tures contain more useful information than original graph features and help to node classification. <ref type=\"bibr\" target=\"#b2\">(3)</ref> We noticed that two graph augmentation methods GAug and MCGL. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rogeneous textrich network. Meta-paths <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b19\">18]</ref> have been widely ado b31\">[30]</ref>, bioinformatics <ref type=\"bibr\" target=\"#b19\">[18]</ref>, and information networks <ref type=\"bibr\" target=\"#b6\">[5]</ref>. In the context of heterogeneous information networks, netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ies designed for natural language processing <ref type=\"bibr\" target=\"#b6\">[7]</ref> and graph data <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta e and graph levels, and an example is the self-supervised learning method for graph neural networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Similarly, the generative framework GPT-GNN <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /head><p>Previous survey papers on knowledge graphs mainly focus on statistical relational learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>, knowledge graph refinement <ref type=\"bibr\" target=\"#b5\">[6. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tly on a large-scale heterogeneous graph. For the first challenge, inspired by contrastive learning <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we design a contrastive pre-training strategy to model th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rg/ns/1.0\"><head n=\"1\">Introduction</head><p>The majority of the research efforts on improving VAEs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> is dedicated to the st e posterior up to the (l \u2212 1) th group. The objective is trained using the reparameterization trick <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>The main questi  and bidirectional encoder networks <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>The goal of VAEs <ref type=\"bibr\" target=\"#b0\">[1]</ref> is to train a generative model in the form of p(x x x, z z z. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b55\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b44\">Shervashidze et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Fout et al., 2017;</ref><ref type=\"bibr\" target=\"#b34\">Liu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: models can improve model predictions on all tasks by utilizing regularization and transfer learning <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, in practice, multi-task learning models do not alw <p>The backbone of MMoE is built upon the most commonly used Shared-Bottom multi-task DNN structure <ref type=\"bibr\" target=\"#b7\">[8]</ref>. The Shared-Bottom model structure is shown in Figure <ref t igure <ref type=\"figure\" target=\"#fig_0\">1</ref> (a), which is a framework proposed by Rich Caruana <ref type=\"bibr\" target=\"#b7\">[8]</ref> and widely adopted in many multi-task learning applications  sks.</p><p>Prior works <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> investigated task di erences in multi-task learning by assumi target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>Instead of sharing hidden layers and same model parame lt in both improved e ciency and model quality for each task <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. One of the widely us \" target=\"#b29\">30]</ref>. One of the widely used multi-task learning models is proposed by Caruana <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, which has a shared-bo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inciple to match traditional network-embedding requirement <ref type=\"bibr\" target=\"#b32\">[32]</ref><ref type=\"bibr\" target=\"#b33\">[33]</ref><ref type=\"bibr\" target=\"#b34\">[34]</ref><ref type=\"bibr\" t br\" target=\"#b76\">[76]</ref><ref type=\"bibr\" target=\"#b77\">[77]</ref> and network embedding methods <ref type=\"bibr\" target=\"#b33\">[33,</ref><ref type=\"bibr\" target=\"#b34\">34,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ches to pooling a graph. Node drop methods <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b)</ref> (Figure <ref type=\"figure\" target=\"#fig_0\">1 br\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b15\">Gao &amp; Ji, 2019;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b)</ref>. Moreover, node clustering methods cast the  s have limited scalability to large graphs <ref type=\"bibr\" target=\"#b5\">(Cangea et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b)</ref>. Therefore, we need a sophisticated graph po  which may be problematic for large graphs <ref type=\"bibr\" target=\"#b5\">(Cangea et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b)</ref>. On the other hand, our Graph Multiset Pooli rom exploiting sparsity in the graph topology, leading to excessively high computational complexity <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b)</ref>. Furthermore, since they need a single grap ptions of baselines and our model).</p><p>Implementation Details For a fair comparison of baselines <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b)</ref>, we fix the GCN <ref type=\"bibr\" target=\"#b /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Global</head><p>6) SAGPool. This method <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b)</ref> is the node drop baseline that selects the  er <ref type=\"bibr\" target=\"#b20\">(Kingma &amp; Ba, 2014)</ref>. For a fair comparison of baselines <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b)</ref>, we use the three GCN layers <ref type=\"bib  2019)</ref>, most deep graph pooling works <ref type=\"bibr\" target=\"#b48\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b;</ref><ref type=\"bibr\" target=\"#b15\">Gao &amp; Ji,  SAGPool. 6) TopKPool. 7) ASAP. The methods <ref type=\"bibr\" target=\"#b52\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b;</ref><ref type=\"bibr\" target=\"#b15\">Gao &amp; Ji,   tackle this limitation, Node Drop methods <ref type=\"bibr\" target=\"#b15\">(Gao &amp; Ji, 2019;</ref><ref type=\"bibr\" target=\"#b23\">Lee et al., 2019b</ref>) select the high scored nodes i (l+1) \u2208 R n l x as described in the equation 4. However, this approach is well known for their scalability issues <ref type=\"bibr\" target=\"#b23\">(Lee et al., 2019b;</ref><ref type=\"bibr\" target=\"#b5\">Cangea et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> to motion regression <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. However, DNNs are a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utions.</formula><p>as a means for, e.g., model debugging or architecture selection. A recent paper <ref type=\"bibr\" target=\"#b3\">(Jain and Wallace, 2019)</ref> points to possible pitfalls that may ca ntion Might be Explanation</head><p>In this section, we briefly describe the experimental design of <ref type=\"bibr\" target=\"#b3\">Jain and Wallace (2019)</ref> and look at the results they provide to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: imilarity measure. A is the action space, f is the reward function, and T is the terminal condition <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Given an initial p (l ) r , the neighbor selector choose . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum <ref type=\"bibr\" target=\"#b19\">(Sutskever et al., 2013)</ref> and Adagrad <ref type=\"bibr\" target=\"# </ref>, using 5 concurrent steps on each of 10 model replicas, using asynchronous SGD with momentum <ref type=\"bibr\" target=\"#b19\">(Sutskever et al., 2013)</ref>, with the mini-batch size of 32. All n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the attacker prints adversarial perturbation as a sticker and then pastes it onto the target object <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ target=\"#b1\">[2]</ref> (c) AdvLB (Ours) the most effective area in the target object to impose them <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b24\">26,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g SEC will directly forward the negotiation packets instead of trying to understand what they carry <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Moreover, SEC places tags in the option fields of the IP . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type=\"bibr\" target=\"#b21\">Shi et al. (2016)</ref> replace convolution filters with deep LSTM, w of recurrent units. We find that using GRU as recurrent units outperforms LSTM which is utilized by <ref type=\"bibr\" target=\"#b21\">Shi et al. (2016)</ref>.</p><formula xml:id=\"formula_0\">w 1 w 2 w 3 w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b32\">33]</ref>, graph classification <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Peters et al., 2018)</ref>, GPT-2 <ref type=\"bibr\" target=\"#b26\">(Radford et al., 2019)</ref>, BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, <ref type=\"bibr\">XLNet (Yang et al., 2019 as reestablished the new state-ofthe-art baselines across various tasks, such as question answering <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, coreference resolution <ref type=\"bibr\" t in the pre-training stage, such as updating the model using only short sequences in the early stage <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>.</p><p>Common strategies for reducing memo ction. Following the paradigm of language model pre-training and down-stream task fine-tuning, BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref> consists of multiple layers of bidirection lti-head self-attention layer and a position-wise feed-forward layer. Using the same notation as in <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, we denote the number of Transformer layer ource-intensive process. For instance, the training of BERT-family models is notoriously expensive. <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref> report that it takes four days for pre-trai e compare BlockBERT with the following baselines:</p><p>Google BERT The pre-trained base model from <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref>.</p><p>RoBERTa-2seq and RoBERTa-1seq We com \"figure\">6</ref>.</p><p>For all the pre-trained models, we adopt the same fine-tuning QA setup from <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref>.</p><p>The tokenized paragraph (p 1 , \u2022 \u2022 \u2022 n units H leads to significant performance degradation <ref type=\"bibr\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Devlin et al., 2019)</ref> and does not address the long sequence issu  target=\"#b3\">Dai et al., 2019)</ref> and its successful application on language model pre-training <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Radford et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: en some works related to exploring data-centric approaches <ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" target=\"#b20\">[22]</ref><ref type=\"bibr\" target=\"#b21\">[23]</ref>, where the approa ructure centric analysis, for localityenhancement transformations such as multi-level data blocking <ref type=\"bibr\" target=\"#b20\">[22]</ref> and data shackling <ref type=\"bibr\" target=\"#b19\">[21]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el training process, which may lead to summary length and excessive redundant information. The SUMO <ref type=\"bibr\" target=\"#b18\">[19]</ref> model proposed an end-to-end extractive text summary gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions <ref type=\"bibr\" target=\"#b22\">(Klicpera et al., 2018)</ref>. However, they focus on integrating thi ep, decoupled from the other steps. This type of prediction smoothing is similar in spirit to APPNP <ref type=\"bibr\" target=\"#b22\">(Klicpera et al., 2018)</ref>, which we compare against later. Howeve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: kolov et al., 2013)</ref>, ELMo <ref type=\"bibr\" target=\"#b21\">(Peters et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b3\">(Devlin et al., 2019)</ref> are trained and tested mainly on datasets  )</ref> uses machine translation to embed context information into word representations.</p><p>BERT <ref type=\"bibr\" target=\"#b3\">(Devlin et al., 2019)</ref> is a contextualized word representation mo r\" target=\"#b10\">(Krallinger et al., 2017)</ref>. Due to the space limitations, we refer readers to <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref> for a more detailed description of BERT.</p pora were used for pre-training, we initialized BioBERT with the pre-trained BERT model provided by <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref>. We define BioBERT as a language representa han an hour as the size of the training data is much smaller than that of the training data used by <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref>. On the other hand, it takes more than 20 e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in that represents the DUT. The Markov Chain is then used to generate test-cases for the design. In <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the coverage analysis results trigger a set of generation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b57\">Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Gao &amp; Ji, 2019;</ref><ref type=\"bibr\" target=\"#b15\">Gao et al., 2018;</ref><ref type=\"bibr\">2021)</ref>. Currently, the m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uate mixup methods in sentence classification <ref type=\"bibr\" target=\"#b5\">(Guo et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Thulasidasan et al., 2019)</ref>. We use two different versions of TR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e ensemble approaches <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref> fused different text features and achieved promising result. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: gly, training different models for each desired behavior, which can cause high environmental impact <ref type=\"bibr\" target=\"#b38\">(Strubell et al., 2019)</ref>. We therefore argue that, instead of tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hborhoods via different mechanisms (e.g., averaging, LSTM) <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, in the re ifferent neighbors more precisely as a weighted average of the ego-and neighbor-features. GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> generalizes the aggregation beyond averaging, and models t -and the aggregated neighbor-embeddings without 'mixing' them is with concatenation as in GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>-rather than averaging all of them as in the GCN model by K ef type=\"bibr\" target=\"#b35\">[36]</ref> GCN-Cheby <ref type=\"bibr\" target=\"#b6\">[7]</ref> GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> H2GCN (prop ding transformations per round in H 2 GCN? GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> and other GNN models embed the intermediate representation &amp; GCN-Cheby <ref type=\"bibr\" target=\"#b16\">[17]</ref>: https://github.com/tkipf/gcn \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>: https://github.com/williamleif/graphsage-simple (PyTorch  : 3 * early_stopping: 40 We report the best performance, for Set 1 with a = 64, b = 5e-4.\u2022 GraphSAGE<ref type=\"bibr\" target=\"#b10\">[11]</ref>:-hid_units: a \u2208 {64, 128} lr: b \u2208 {0.1, 0.7} epochs: 500</ led the default feature normalization in the official implementation for this baseline. \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>:</p><p>-hid_units: a \u2208 {64, 128} lr: b \u2208 {0.1, 0.7} epochs led the default feature normalization in the official implementation for this baseline. \u2022 GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>:</p><p>-hid_units: 64 lr: {0.1, 0.7} epochs: 500</p><p>\u2022 M intermediate representations. While these designs have been utilized separately in some prior works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" targ 1)d)</formula><p>Solving the above inequality for \u03b4 1 , we get the amount of perturbation needed as <ref type=\"bibr\" target=\"#b10\">(11)</ref> and the least absolute amount of perturbation needed is |\u03b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the expressive power of chemical fingerprints, some studies <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref> introduce convolutional layers to learn the neural fingerprin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esearch has also shown that people prefer dialog systems that can provide more supportive responses <ref type=\"bibr\" target=\"#b21\">(Rains et al., 2020)</ref>.</p><p>Research has shown that providing e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: by hand-crafted models, in particular discrete choice models <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>, and the high accuracy of the neural network-based predicti ed in DCM literature, we follow the formulation described in <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>, which is well adapted to our problem setting. Functions mo n hand-designed functions are set to the estimated values in <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>. For synthetic data, we embed the goals in a 64-dimensional r selecting the next action, but relative to each individual <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50,</ref><ref type=\"bibr\" target=\"#b44\">47]</ref>. The high interpret corresponding functions. The exact mathematical formulations of the above functions are detailed in <ref type=\"bibr\" target=\"#b47\">[50,</ref><ref type=\"bibr\" target=\"#b4\">7]</ref>. Each person is assu  L-MNL for measuring the anchor probabilities, rather than those of the cross-nested logit model in <ref type=\"bibr\" target=\"#b47\">[50]</ref>.</p><p>Training: All the parameters of our model are learn ble the integration of the DCM framework. According to the DCM framework for pedestrian forecasting <ref type=\"bibr\" target=\"#b47\">[50]</ref>, the anchor set A at each time-step is defined dynamically. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b4\">[5]</ref>, TensorFlow <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Mxnet <ref type=\"bibr\" target=\"#b6\">[7]</ref> and PyTorch <ref type=\"bibr\" target=\"#b7\">[8]</ref> all prov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> have distilled know >[20]</ref> proposes \"hint regression\" that matches the intermediate representations. Subsequently, <ref type=\"bibr\" target=\"#b27\">[28]</ref> matches the gram matrices of the representations, <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roup convolution characterizes many recent works on images <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, spherical signal <ref type=\"bibr\" target=\"#b39\">[41]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ised machine learning methods for implicit diversification <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" tar oves the scoring function by considering difference between positive and negative rankings. NTN-DIV <ref type=\"bibr\" target=\"#b40\">[41]</ref> uses a neural tensor network to learn document similarity  features as input, while methods below are taking the distributed representations as input. NTN-DIV <ref type=\"bibr\" target=\"#b40\">[41]</ref>: a learning approach which learns novelty features based o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"1\">INTRODUCTION</head><p>Self-attention-based architectures, in particular Transformers <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref>, have become the model of choice in natu ww.tei-c.org/ns/1.0\"><head n=\"3\">METHOD</head><p>In model design we follow the original Transformer <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref> as closely as possible. An advantage of  sulting sequence of embedding vectors serves as input to the encoder.</p><p>The Transformer encoder <ref type=\"bibr\" target=\"#b43\">(Vaswani et al., 2017)</ref> consists of alternating layers of multih xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">RELATED WORK</head><p>Transformers were proposed by <ref type=\"bibr\" target=\"#b43\">Vaswani et al. (2017)</ref> for machine translation, and have since b i-c.org/ns/1.0\"><head>APPENDIX A MULTIHEAD SELF-ATTENTION</head><p>Standard qkv self-attention (SA, <ref type=\"bibr\" target=\"#b43\">Vaswani et al. (2017)</ref>) is a popular building block for neural a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nverge to the same optimum since we explicitely consider arbitrary GNNs solving non-convex problems <ref type=\"bibr\" target=\"#b6\">(Cong et al., 2020)</ref>.</p><p>It is well known that the most powerf >(Zou et al., 2019)</ref>, VR-GCN <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>, MVS-GNN <ref type=\"bibr\" target=\"#b6\">(Cong et al., 2020)</ref>, CLUSTER-GCN <ref type=\"bibr\" target=\"#b5\">( (Hamilton et al., 2017)</ref>, and avoids the need to sample a large amount of neighbors in return. <ref type=\"bibr\" target=\"#b6\">Cong et al. (2020)</ref> further simplified this scheme into a one-sho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h node features and graph topological structural information to make predictions. Velickovic et al. <ref type=\"bibr\" target=\"#b28\">[27]</ref> adopt attention mechanism into graph learning, and propose ctions of the contents of a neighborhood or sequence. Therefore, they are adaptive to the contents. <ref type=\"bibr\" target=\"#b28\">[27]</ref> adapts an attention mechanism to graph learning and propos  thus can help stabilize the process, compared with the previously used row normalization as in GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>:</p><formula xml:id=\"formula_4\">E ijp = \u00caijp N j=1 \u00caijp<la ention based EGNN layer</head><p>We describe the attention based EGNN layer. The original GAT model <ref type=\"bibr\" target=\"#b28\">[27]</ref> is only able to handle one dimensional binary edge feature ula xml:id=\"formula_9\">X l\u22121 i\u2022 , X l\u22121 j\u2022</formula><p>and E ijp . In existing attention mechanisms <ref type=\"bibr\" target=\"#b28\">[27]</ref>, the attention coefficient depends on X i\u2022 and X j\u2022 only.  yer. Indeed, the essential difference between GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref> is whether we use the attention coefficients (i.e., matrix The three citation network datasets are also used in <ref type=\"bibr\" target=\"#b33\">[32]</ref> [18] <ref type=\"bibr\" target=\"#b28\">[27]</ref>. However, they all use a pre-processed version which disca class.</p><p>The baseline methods we used are GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>. To investigate the effectivenesses of each components, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b5\">5]</ref>,and trusted measurement is a key problem of this technology <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7]</ref>. Trusted computing tre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b45\">[43,</ref><ref type=\"bibr\" target=\"#b56\">54]</ref>. Based on Cycle-GAN <ref type=\"bibr\" target=\"#b58\">[56]</ref>, Yuan et al. <ref type=\"bibr\" target=\"#b45\">[43]</ref> pro scheme has also been used to perform image translation without paired training data, e.g., CycleGAN <ref type=\"bibr\" target=\"#b58\">[56]</ref> and DualGAN <ref type=\"bibr\" target=\"#b44\">[42]</ref>. Spe avoid the possible mode collapse issue when solving the under-constrained image translation problem <ref type=\"bibr\" target=\"#b58\">[56]</ref>. Unlike these methods, we seek to improve the performance  es collected from YouTube. Thus, there are 3 DRN-adapt models in total. And We also train a CinCGAN <ref type=\"bibr\" target=\"#b58\">[56]</ref> model for each kind of unpaired data for comparison. Based  Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods <ref type=\"bibr\" target=\"#b58\">[56,</ref><ref type=\"bibr\" target=\"#b6\">4,</ref><ref type=\"bibr\" targ  CycleGAN based SR methods. First, Cycle-GAN based methods <ref type=\"bibr\" target=\"#b45\">[43,</ref><ref type=\"bibr\" target=\"#b58\">56]</ref> use a cycle consistency loss to avoid the possible mode col. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nship between features, we introduce explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> to CCA. Finally, using the features that are projected to t use. In contrast, recent advances of explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> can convert nonlinear problems to linear problems, which ca  computation complexity, one can use explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref>. Let \u03c6(x) denote an explicit feature mapping such that K i  rnel. All other histogram-based features were mapped using the exact Bhattacharyya kernel map- ping <ref type=\"bibr\" target=\"#b20\">[13]</ref>. Finally, similar to <ref type=\"bibr\" target=\"#b16\">[9]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b42\">43]</ref> and have connections to the large literature on metric learning <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>.</p><p>As the name s function encourages learning from hard positives and hard negatives. We also show that triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref> is a special case of our loss when only a single positive  ss can thus be seen to be efficient in its training. Other contrastive losses, such as triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref>, often use the computationally expensive technique of hard .\">Connections to Triplet Loss</head><p>Contrastive learning is closely related to the triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref>, which is one of the widely-used alternatives to cross-ent  contrastive learning are metric learning and triplet losses <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. These losses have . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. A noise-transition target=\"#b45\">[46]</ref>. Forward is a loss correction approach that uses a noise-transition matrix <ref type=\"bibr\" target=\"#b31\">[32]</ref>. D2L monitors the subspace dimensionality change at traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y:</p><formula xml:id=\"formula_30\">\u2206 i = max(max k\u2208[G] \u00b5 i k \u2212 \u00b5 i i , 0).</formula><p>In line with <ref type=\"bibr\" target=\"#b10\">(Chevaleyre et al., 2017)</ref>, we consider two ways of measuring th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igh quality for most languages, which can potentially improve the performance of end-to-end systems <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>Sub-word representations have recently seen their suc d proposed a worddependent silence model to improve ASR accuracy; for use in end-to-end ASR models, <ref type=\"bibr\" target=\"#b6\">[7]</ref> investigated the value of a lexicon in end-to-end ASR. Sub-w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >), only learn the partial modes of target distributions, which causes the problem of mode dropping <ref type=\"bibr\" target=\"#b0\">[Arora et al., 2018]</ref>. Second, the imbalance capacity of generato ion is, the more similar two variables have. Given two random variables x and y, mutual information <ref type=\"bibr\" target=\"#b0\">[Belghazi et al., 2018]</ref> can be estimated by the JS divergence be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (Sundaram et al., 2018;</ref><ref type=\"bibr\">Frazer et al., 2020)</ref>, remote homology detection <ref type=\"bibr\" target=\"#b17\">(Hou et al., 2018)</ref>, and protein design <ref type=\"bibr\">(Russ e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b0\">[1]</ref>, Giffin, et al., <ref type=\"bibr\" target=\"#b22\">[23]</ref>, Spivey <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Bond and McKinley <ref type=\"bibr\" target=\"#b24\">[25]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  unknown true labels and some behavior assumptions, with examples of the Dawid-Skene (DS) estimator <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the minimax entropy (Entropy) estimator<ref type=\"foot\" tar ://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Dawid-Skene Estimator</head><p>The method of Dawid and Skene <ref type=\"bibr\" target=\"#b4\">[5]</ref> is a generative approach by considering worker confusability ed majority voting (IWMV) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the Dawid-Skene (DS) estimator <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and the minimax entropy (Entropy) estimator <ref type=\"bibr m c = 2\u02c6[\u22128 : 0] and = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> by the method in Sec. 6.2. As for Gibbs-CrowdSVM, we generate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b29\">[30]</ref> and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>In Section 3 we describe our proposed stream fetch  f> shows a block diagram of the trace cache mechanism as proposed by Rotenberg, Benett and Smith in <ref type=\"bibr\" target=\"#b31\">[32]</ref>. The trace cache captures the dynamic instruction stream,  <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and the trace cache architecture using a trace predictor <ref type=\"bibr\" target=\"#b31\">[32]</ref> and selective trace storage <ref type=\"bibr\" target=\"#b28\" rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> is one such high fetch width mechanism, recently implemente. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, because it effectively couples the attention mechanism an  state-of-the-art methods using deep two-stream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Zhao et al. propose a novel loss function to train GNNs for anomaly-detectable node representations <ref type=\"bibr\" target=\"#b47\">[48]</ref>. Apart from the aforementioned methods, our approach focus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h are either news, web pages or user generated QAs on the web for training and test. CNN/Daily Mail <ref type=\"bibr\" target=\"#b11\">[10]</ref> (CNNDM) is a popular summarization dataset, which contains. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ence model is an encoder-decoder architecture with attention <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Let X = [X 1 , . . . , P (y u | X, y &lt;u ) = h(S u , Q u ).<label>(4)</label></formula><p>The function g(\u2022) is a GRU RNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> which encodes the previous token and query vector Q u\u22121 to p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f> realizes long-range dependencies, which helps to focus on what networks want to model. Fu et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> proposed the DANet based on a self-attention mechanism tha is selected to optimize the network, and the initial learning rate is set as 0.001. Following DANet <ref type=\"bibr\" target=\"#b37\">[38]</ref>, we employ the ''poly'' learning rate policy where the pow. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  assification, these perturbations cause the legitimate sample to be misclassified at inference time <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  , adversarial training which augments the training data of the classifier with adversarial examples <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  xamples designed to fool the substitute often end up being misclassified by the targeted classifier <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b19\">Papernot e ch to defend against adversarial noise is to augment the training dataset with adversarial examples <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: racting information of the image-style feature map. Hence, our work is also related to the study of <ref type=\"bibr\" target=\"#b8\">[Liu et al., 2020]</ref>, who formulated incomplete utterance rewritin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: search of graph embedding ( <ref type=\"bibr\" target=\"#b15\">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Grover and Leskovec, 2016)</ref>, where graph topology and node relati he performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref>, VGAE <ref type=\"bibr\" target=\"#b9\"> ds. DeepWalk <ref type=\"bibr\" target=\"#b15\">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref> are representative random walk-based ns from nodes' raw features, without using any graph structure information incorporated. \u2022 Node2Vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref>: This approach is an extension of Wo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  different task, i.e., person re-identification (re-ID), which is fundamental in video surveillance <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Re-ID refers to the problem of re-identifying individuals. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een intra-aware features based on Graph Neural Network (GNN) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>.</p><p>Inter-RM learns a set of relation messages and estim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rrence matrix in Equation 1 can be simplified as 2 which is known as random-walk matrix polynomials <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. <ref type=\"bibr\">Chen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es not preserve the eigenvectors and its effect therefore cannot be calculated precisely. Wu et al. <ref type=\"bibr\" target=\"#b76\">[77]</ref> empirically found that adding self-loops shrinks the graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Some following literature modifies the framework for the purpose of the adversarial training. IRGAN <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2017)</ref>  </p></div> <div xmlns=\"http://www.tei-c.org ive samples iteratively. And to make the generative module aware of relation information, following <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2018a)</ref>, we design a random walk based generating s name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: re sequential prefetchers that, upon activation, send prefetch requests for a few subsequent blocks <ref type=\"bibr\" target=\"#b44\">[47,</ref><ref type=\"bibr\" target=\"#b53\">56]</ref>. While sequential  et=\"#b22\">[25,</ref><ref type=\"bibr\" target=\"#b39\">42,</ref><ref type=\"bibr\" target=\"#b43\">46,</ref><ref type=\"bibr\" target=\"#b44\">47]</ref>, prior work has shown that such prefetchers leave a signifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ence (seq2seq) models. Sub-word units were used in seq2seq <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref> and RNNT <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  based on the trustworthiness of the authors of the claims <ref type=\"bibr\" target=\"#b29\">[19,</ref><ref type=\"bibr\" target=\"#b58\">48,</ref><ref type=\"bibr\" target=\"#b73\">63]</ref>. Fact checking webs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odels achieve satisfactory performance on KG completion and are robust against the sparsity of data <ref type=\"bibr\" target=\"#b11\">(Hao et al., 2019)</ref>. RotatE <ref type=\"bibr\" target=\"#b30\">(Sun . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w York Times (NYT) <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010)</ref> and WebNLG <ref type=\"bibr\" target=\"#b6\">(Gardent et al. 2017)</ref>. NYT comes from the distant supervised rel ramework <ref type=\"bibr\" target=\"#b15\">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type=\"bibr\" target=\"#b6\">(Gu et al. 2016)</ref>. But it cannot predict the entire entities. In . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tone of many state-of-the-art models in various natural language understanding and generation tasks <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b31\">Liu et al.,  els to generate more rele-vant and coherent text. We first study a planning model trained from BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> to produce the initial content plan, which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f>, these encoders encode either structures <ref type=\"bibr\" target=\"#b26\">(Luo et al., 2018b;</ref><ref type=\"bibr\" target=\"#b60\">Ying et al., 2019;</ref><ref type=\"bibr\" target=\"#b48\">Wang et al., 2 ee major encoding-dependent subroutines as well as eight status quo NAS algorithms on NAS-Bench-101 <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref> (small), NAS-Bench-301 <ref type=\"bibr\" tar ctures. However, adjacency matrix-based encoding grows quadratically as the search space scales up. <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref> propose categorical adjacency matrixbased e ead><p>We restrict our search space to the cell-based architectures. Following the configuration in <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref>, each cell is a labeled directed acyclic gr div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>NAS-Bench-101</head><p>The NAS-Bench-101 search space <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref> consists of approximately 420K architecture -101. These encoding schemes include (1-3) one-hot/categorical/continuous adjacency matrix encoding <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref>, (4-6) one-hot/categorical/continuous path  ithout considering graph isomorphism, which is a much larger search space compared to NAS-Bench-101 <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019)</ref> and NAS-Bench-201 <ref type=\"bibr\" target=\"  structure encodings may not be computationally unique unless some certain graph hashing is applied <ref type=\"bibr\" target=\"#b60\">(Ying et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Ning et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a vanilla and Siamese architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Each system is evaluated under specific configurations rega vych <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed to combine BERT with a Siamese architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref> for semantic representations of sentences and their similari ese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. In Siamese networks, two inputs are fed through identical s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: milar semantics.</p><p>Sharing a similar philosophy, there have been works on contrastive attention <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. MGCAM <ref type=\"  attention <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. MGCAM <ref type=\"bibr\" target=\"#b30\">[31]</ref> uses the contrastive feature between persons and backgroun for context modeling; instead of using extra supervision to localize regions to compare as in MGCAM <ref type=\"bibr\" target=\"#b30\">[31]</ref>, ACM automatically learns to focus on meaningful regions t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b29\">29]</ref> that adopt the enhanced sequential inference model (ESIM) <ref type=\"bibr\" target=\"#b4\">[5]</ref> to infer the relationship between evidence and claims, which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e gradient update per data sample, PPO enables training with mini-batch updates. In addition, RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> is used to implement our search algorithm (see Figure <ref ormula_9\">L V F t the square-error loss V \u03b8 (s t ) \u2212 V target t</formula><p>. Please refer to RLlib <ref type=\"bibr\" target=\"#b27\">[28]</ref> for more implementation details.</p></div> <div xmlns=\"htt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: order to improve lie detection in criminalsuspect interrogations, Sumriddetchkajorn and Somboonkaew <ref type=\"bibr\" target=\"#b36\">[37]</ref> developed an infrared system to detect lies by using therm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2020b)</ref> 75.6 Albert <ref type=\"bibr\">(Lan et al., 2020) (ensemble)</ref> 76.5 UnifiedQA * <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref> 79.1</p><p>RoBERTa + QA-GNN (Ours) 76.1 6 Albert + KB 81.0 T5 * <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref> 83.2 UnifiedQA * <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref> 87.2</p><p>AristoRoBERTa + QA-GNN (Ours y, the top two systems, T5 <ref type=\"bibr\" target=\"#b30\">(Raffel et al., 2020)</ref> and UnifiedQA <ref type=\"bibr\" target=\"#b17\">(Khashabi et al., 2020)</ref>, are trained with more data and use 8x . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b40\">Zoph et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Real et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Cai et al., 2018a;</ref><ref type=\"bibr\" target=\"#b21\">Liu et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ll describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> and <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: chers' noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, which tightens the privacy bound when the t effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, which represent the state-of-the-art in dif e a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>.</p><p>Section 5 further discusses the relat g the need for supervision. \u2022 We present a new application of the moments accountant technique from <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> for improving the differential-privacy analy (8.19, 10 \u22126 ) for SVHN, respectively with accuracy of 98.00% and 90.66%. In comparison, for MNIST, <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> obtain a looser (8, 10 \u22125 ) privacy bound an y cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, building on previous work <ref type=\"bibr\"  rivacy loss random variable.</p><p>The following properties of the moments accountant are proved in <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>.</p><p>Theorem 1. 1. [Composability] Suppose 00% and 90.66%. These results improve the differential privacy state-of-the-art for these datasets. <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> previously obtained 97% accuracy with a (8,  he large number of parameters prevents the technique from providing a meaningful privacy guarantee. <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> provided stricter bounds on the privacy loss  we keep track of the privacy budget throughout the student's training using the moments accountant <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>. When teachers reach a strong quorum, this  e cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b33\">Shokri &amp; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: usion are personalized PageRank (PPR) <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b35\">[36]</ref>. PPR corresponds to choosing T = T rw and \u03b8 PPR k = \u03b1(1 \u2212  cal graph learning <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, especially for clu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: gment every physical register with a Superseded bit and a Pending count. This support is similar to <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The Superseded bit marks whether the instruction that sup  techniques.</p><p>The second category includes work related to register recycling. Moudgill et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> discuss performing early register recycling in out-of-orde er processors that support precise exceptions. However, the implementation of precise exceptions in <ref type=\"bibr\" target=\"#b16\">[17]</ref> relies on either checkpoint/rollback for every replay even. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ay. It is a commonly used approach to make graph convolution network feasible for large-scale graph <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Let \ud835\udc6c (0) be the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b4\">Fan et al., 2017;</ref><ref type=\"bibr\" target=\"#b25\">Scarton and Specia, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Nishihara et al., 2019)</ref> where a Sequence-to-Sequence (Seq2Seq)  =\"#b4\">(Fan et al., 2017)</ref>. <ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref> and <ref type=\"bibr\" target=\"#b17\">Nishihara et al. (2019)</ref> similarly showed that adding control to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b4\">[5]</ref>, Superthreaded <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Trace Processors <ref type=\"bibr\" target=\"#b16\">[17]</ref> [21], Speculative Multithreaded <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the entities due to their typeagnostic nature <ref type=\"bibr\" target=\"#b25\">[Xie et al., 2016</ref><ref type=\"bibr\" target=\"#b7\">, Jain et al., 2018]</ref>. Since PSL-KGI is able to predict entity ty sion.</p><p>(ii) Explicit type supervised models also outperform the implict type supervised models <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>.</p><p>The margin of improvement is large wh  on the use of ontological rules (exemplified by PSL-KGI) and embeddings (we use ComplEx, ConvE and <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>). Rule induction methods are orthogonal to o  target=\"#b3\">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a different approach <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> propose extending standard KG embeddings wit es for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> as follows:</p><p>Instead of just using the  are our explicitly supervised TypeE-X methods with the implicitly supervised embeddings proposed by <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>.</p><p>\u2022 In Section 6.3, we analyse how our  e supervision with the unsupervised type-compatible embeddings-based method proposed by Jain et al. <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>. As these results indicate, while explicitly nificantly improves the relation scores, improving weighted F1 up to 18% (over NELL).</p><p>Dataset <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>. We showed their performance on existing dat d>Table 7 :</head><label>7</label><figDesc>Weighted F1 scores on relation triples in the test set by<ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> and TypeE-ComplEx.Anecdotes. Looking at the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  mixing coefficients <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><formula xml:id=\"formula_0\">M = \u03b1B + \u03b2R,<label>(1)</lab ion results. Moreover, we introduce the gradient constraints <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> to make the model learning more effective, in which the edg  In this scenario, image priors such as different blur levels between the background and reflection <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, ghosting effects  \" target=\"#fig_3\">4</ref> 1 ) than previous linear functions <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar <p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type=\"bibr\" target=\"#b16\">[17]</ref> 0.801 0.829 21.77 WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ >, FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and LB14 <ref type=\"bibr\" target=\"#b16\">[17]</ref>. For a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[12]</ref>.</p><p>On the other hand, many researchers have found convolutional networks (ConvNets) <ref type=\"bibr\" target=\"#b16\">[17]</ref>  <ref type=\"bibr\" target=\"#b17\">[18]</ref> are useful in e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s. Plenty of researches have been conducted to solve the name ambiguity problem. Supervised methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>,</p><p>The research is supported by the National Key Researc representations play a critical role to quantify distinctions and similarities between publications <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The majority of existing solutions utilize biographical fea lustering (HAC) method works well for skewed data and is widely in many name disambiguation methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t ected components to generate the clustering results for each name to be disambiguated. Zhang et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>: This method uses a global metric learning and local linkage For example, <ref type=\"bibr\" target=\"#b4\">[5]</ref> need to specify the number of distinct author, <ref type=\"bibr\" target=\"#b0\">[1]</ref> need labeled data to estimate the number. For a fair compari d co-occurrence information of text and loss a certain amount of semantic information. Zhang et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> also use a graph convolutional network based encoder-decoder based framework to extract multiple types of characteristics and relations in publication database. <ref type=\"bibr\" target=\"#b0\">[1]</ref> use a global metric learning and local linkage graph auto-en. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lution streams. This paper represents a very substantial extension of our previous conference paper <ref type=\"bibr\" target=\"#b105\">[105]</ref> with an additional material added from our unpublished t t object detection and instance segmentation frameworks. The main technical novelties compared with <ref type=\"bibr\" target=\"#b105\">[105]</ref> lie in threefold. <ref type=\"bibr\" target=\"#b0\">(1)</ref efold. <ref type=\"bibr\" target=\"#b0\">(1)</ref> We extend the network (named as HRNetV1) proposed in <ref type=\"bibr\" target=\"#b105\">[105]</ref>, to two versions: HRNetV2 and HRNetV2p, which explore al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ct a synonym prediction sub-task to fine-tune a pre-trained language model which is the ERNIE model <ref type=\"bibr\" target=\"#b23\">[23]</ref> in this paper. As ERNIE is a continual pre-training framew ation discovery rather than relation automatic verification. Pre-trained language models like ERNIE <ref type=\"bibr\" target=\"#b23\">[23]</ref>, BERT <ref type=\"bibr\" target=\"#b7\">[7]</ref>, XLNet <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ies focus primarily on the intrinsic properties of graph <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, but measuring the impact of graph adversarial attacks is  >. Unlike previous works, Newman et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Foster et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> focus on another important network feature, i.e., assortat ure the attack impacts, degree assortativity coefficient <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, it measures the tendency of nodes connected to each other. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: train. Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion <ref type=\"bibr\" target=\"#b21\">[22]</ref>. No parameter tuning was performed during neural network t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b25\">(Sung, 1996;</ref><ref type=\"bibr\">Can\u00e9vet &amp; Fleuret, 2015;</ref><ref type=\"bibr\" target=\"#b24\">Shrivastava et al., 2016)</ref>. However, none of the aforementioned . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> which needs a CTC trained model to conduct pre-partition before the attention decoding.</p><p>In <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Li al. present the important Adaptive Computation Steps (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 1\">[42]</ref>. In this work, we employ the covariance matrix adaptation evolution strategy (CMA-ES) <ref type=\"bibr\" target=\"#b42\">[43]</ref>, a state-of-the-art optimizer for continuous black-box fun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  are designed to track the flow of emotion states of each interlocutor throughout the conversations <ref type=\"bibr\" target=\"#b9\">[10]</ref>. However, rare researches pay the same attention on the rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sign assorted classifiers from diverse perspectives, from orthodox methods such as distance measure <ref type=\"bibr\" target=\"#b5\">[6]</ref>, to advanced methods including support vector machine (SVM) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: viewed as a sequence-to-sequence learning problem. We exploit deep recurrent neural networks (RNNs) <ref type=\"bibr\" target=\"#b16\">[15,</ref><ref type=\"bibr\" target=\"#b17\">16]</ref> as the acoustic mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  attack-agnostic manner, except a few touches on denoising <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and obfuscating gradients <ref type=\"bibr\" target=\"#b10\">[1 le FPD can circumvent the structure-replaced white-box attack. Our proposal is partially related to <ref type=\"bibr\" target=\"#b28\">[29]</ref>, as the denoising layers in our FPD are inspired by their  ng layers in our FPD are inspired by their feature denoising approach. Nevertheless, different from <ref type=\"bibr\" target=\"#b28\">[29]</ref>, the principle behind our FPD is to improve the intrinsic  antic information. We will compare the performance between FPD-enhanced CNN and the CNN enhanced by <ref type=\"bibr\" target=\"#b28\">[29]</ref> in Section 4.1.</p></div> <div xmlns=\"http://www.tei-c.org  the Gaussian filtering operator, the dot product operator helps improve the adversarial robustness <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Meanwhile, as the dot product operator does not involve e  framework structure through the exploration study. Moreover, we compare with the most related work <ref type=\"bibr\" target=\"#b28\">[29]</ref> as well. In the comparison experiments, we focus on compar a><p>Comparison with the Related Work As mentioned in Section 2, the denoising approach proposed in <ref type=\"bibr\" target=\"#b28\">[29]</ref> is similar to our denoising layers in FPD. Therefore, we c /ref> is similar to our denoising layers in FPD. Therefore, we conduct a comparison experiment with <ref type=\"bibr\" target=\"#b28\">[29]</ref> as well. In Table <ref type=\"table\" target=\"#tab_5\">1</ref /ref> as well. In Table <ref type=\"table\" target=\"#tab_5\">1</ref>, X represents the enhanced CNN by <ref type=\"bibr\" target=\"#b28\">[29]</ref>. We observe that our F 2I\u2212Mid outperforms X . Especially, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ).</p><p>To effect our study, we use a collection of abstracts from a corpus of scientific articles <ref type=\"bibr\" target=\"#b0\">(Ammar et al., 2018)</ref>. We extract entity, coreference, and relati  and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences <ref type=\"bibr\" target=\"#b0\">(Ammar et al., 2018)</ref>.</p><p>For each abstract, we create a knowl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s by studying the two-month history made available by Amazon <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b13\">15]</ref>. Though these statistics alone are sufficient for the user  are limited to statistical studies of historical spot prices <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b13\">15]</ref>.</p><p>Game theoretic pricing. Spot pricing is a distribute  tradeoff is the fact that user jobs can have long runtimes spanning many changes in the spot price <ref type=\"bibr\" target=\"#b13\">[15]</ref>. Users then face two key challenges: 1) Users must predict int ensures that the job is sufficiently interruptible. We use p to denote the optimal bid price to <ref type=\"bibr\" target=\"#b13\">(15)</ref>.</p><p>We now observe that the expected running time in <r  of the spot price monotonically decreases, i.e., F \u03c0 (p) is concave, the optimal bid price solving <ref type=\"bibr\" target=\"#b13\">(15)</ref> </p><formula xml:id=\"formula_29\">is p = \u03c8 \u22121 t k t r \u2212 1 , . Comparing <ref type=\"bibr\" target=\"#b17\">(19)</ref> to bidding for a single persistent request in <ref type=\"bibr\" target=\"#b13\">(15)</ref>, we see that <ref type=\"bibr\" target=\"#b17\">(19)</ref> can \"#b13\">(15)</ref>, we see that <ref type=\"bibr\" target=\"#b17\">(19)</ref> can be solved similarly to <ref type=\"bibr\" target=\"#b13\">(15)</ref> in Proposition 5.</p><p>By comparing the costs for multipl t k ts .</p><p>Proof of Proposition 5.</p><p>Proof. By taking the first-order derivative of \u03a6(p) in <ref type=\"bibr\" target=\"#b13\">(15)</ref>  <ref type=\"figure\" target=\"#fig_3\">3</ref>), F \u03c0 (p) is c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es has improved both the rate of learning and final performance. Similar to our findings about MoCo,<ref type=\"bibr\" target=\"#b32\">Wu et al. (2017)</ref> find that mining the very hardest negatives hu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es with few contributions in a prediction. Inspired by the theory of hierarchical abstract machines <ref type=\"bibr\" target=\"#b15\">(Parr and Russell 1998)</ref>, we cast the task of profile reviser as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b16\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">He et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b23\">Wu et al. (2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b19\">Maclaurin et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b0\">Abadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b23\">Paszke et al., ribution, and code generation (see, e.g., <ref type=\"bibr\" target=\"#b2\">Bergstra et al., 2010;</ref><ref type=\"bibr\" target=\"#b0\">Abadi et al., 2016)</ref>. But, because declarative DSLs prevent users bed in \u00a74.6. TensorFlow graphs come with their own set of design principles, which are presented in <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>. The following terminology will be used in  devices and parallelizes operations when possible. Readers interested in the runtime should consult <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>.</p><p>The function decorator supports code. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o be predicted by performing calculations on these vectors <ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>. However, traditi  learnt h\u00c3. Such estimation is sensitive to random variations of data which may lead to overfitting <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Therefore, in this paper, a complete Bayesian treatment i SVD++ model with 30 epochs with learning rate c \u00bc 0:004 and regularization parameter k \u00bc 0:13. RTTF <ref type=\"bibr\" target=\"#b14\">[15]</ref>: RTTF model considers the linguistic similarity between re ive function is equivalent to using SGD in Eq. ( <ref type=\"formula\" target=\"#formula_10\">14</ref>) <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Second, the impact of using different vector lengths to e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: trackers as TrSiam and TrDiMP, respectively. In these two versions, the backbone model is ResNet-50 <ref type=\"bibr\" target=\"#b20\">[18]</ref> for feature extraction. Before the encoder and decoder, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing multiple properties simultaneously, as GAs are likely to get trapped in regions of local optima <ref type=\"bibr\" target=\"#b26\">(Paszkowicz, 2009)</ref>. RationaleRL is a very strong baseline that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms, health-care, etc. <ref type=\"bibr\" target=\"#b28\">[29]</ref>. According to psychological studies <ref type=\"bibr\" target=\"#b8\">[9]</ref>, the FER task is to classify an input facial image into one . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Knowledge bases (KBs), such as Freebase <ref type=\"bibr\" target=\"#b1\">(Bollacker et al., 2008)</ref>, NELL <ref type=\"bibr\" target=\"#b19\">(M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: The negative sampling used in Eq.( <ref type=\"formula\" target=\"#formula_17\">14</ref>) is similar to <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b21\">21]</ref>: \ud835\udc38 \u2032 (\ud835\udc62,\ud835\udc63) is comp n rate \ud835\udefd in Eq.( <ref type=\"formula\" target=\"#formula_10\">9</ref>) and the harmonic factor \ud835\udf06 in Eq. <ref type=\"bibr\" target=\"#b12\">(12)</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_5\">4</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lar to recent work on sequence-tosequence voice conversion <ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref>. <ref type=\"bibr\" r identities, to transform word segments from multiple speakers into multiple target voices. Unlike <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which trained separate models for each source-target spea a pretrained speech recognizer to more explicitly capture phonemic information in the source speech <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, we do find it helpful to multitask train the mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Autoencoder (VAE) <ref type=\"bibr\" target=\"#b6\">[7]</ref> and Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, are powerful architectures which can learn complicated dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onsider it as a complementary and generic add-on to enrich the training process of any neural model <ref type=\"bibr\" target=\"#b11\">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ture model.</p><p>Pyramid pooling layer (PPL) <ref type=\"bibr\" target=\"#b14\">[15]</ref> and DeepLab <ref type=\"bibr\" target=\"#b15\">[16]</ref> have shown remarkable learning capabilities of feature rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on image) can be captured in a \"remove-and-occlude\" manner <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>: 1) Taking a photo of the mixture image through the glass;  or a much larger scale than those used in existing methods <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Finally, we build a dataset containing 4027 images under v era with fully manual control model) like previous methods <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, we also use the cameras on different types of mobile phone. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tural language applications such as language understanding <ref type=\"bibr\" target=\"#b51\">[41,</ref><ref type=\"bibr\" target=\"#b70\">60]</ref>, knowledge graph completion <ref type=\"bibr\" target=\"#b27\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar algorithm design, our work is most closely related to Hamilton et al. (2017a)'s GraphSAGE algorithm <ref type=\"bibr\" target=\"#b17\">[18]</ref> and the closely related follow-up work of <ref type=\"bibr\"  nodes to aggregate from allows us to control the memory footprint of the algorithm during training <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Second, it allows Algorithm 1 to take into account the im ean); \u2022 mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. \u2022 mean-pooling-hard is the same as mean-pooling, except t ing and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref>-other variants (e.g., based on Kipf et al. <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: m reasoning on the working graph G W , our GNN module builds on the graph attention framework (GAT) <ref type=\"bibr\" target=\"#b43\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, which induces node representations v contrast to these works, QA-GNN jointly models the language and KG. Graph Attention Networks (GATs) <ref type=\"bibr\" target=\"#b43\">(Veli\u010dkovi\u0107 et al., 2018)</ref> perform attention-based message passi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. In its early stage f type=\"bibr\" target=\"#b5\">[6]</ref> and sequence modeling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>Recently, a  f type=\"bibr\" target=\"#b23\">[24]</ref>, language modelling <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, etc. Despite their efines operation candidates on the node, whereas we associate operations on the edge as inspired by <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t y Search Space S t . The topology search space is inspired by the popular cell-based NAS algorithms <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t e API. The implementation difference between DARTS <ref type=\"bibr\" target=\"#b7\">[8]</ref> and GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref> is only less than 20 lines of code. Our library reduces the  order DARTS (DARTS-V1) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, second order DARTS (DARTS-V2), GDAS <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SETN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TAS <ref ty cy. Such strategy is more robust than using the arg max over the learned architecture parameters in <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>. <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bility than RNNs. However, in the NER task, Transformer encoder has been reported to perform poorly <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>, our experiments also confirm this result. T =\"#tab_3\">3</ref>. The poor performance of the Transformer in the NER datasets was also reported by <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>. Although performance of the Transformer is  ibr\" target=\"#b13\">(Guo et al., 2019)</ref>. Although performance of the Transformer is higher than <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>, it still lags behind the BiLSTM-based model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erial images and LiDAR data. The developed network is based on a modified residual learning network <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> that extracts robust low/mid/high-level featu res in the input images <ref type=\"bibr\" target=\"#b50\">(Zhang et al., 2016)</ref>. Previous studies <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> have found that increasing the depth of neura e=\"figure\" target=\"#fig_2\">1</ref>).</p><p>A more detailed description of ResNet-50 can be found in <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> and here ResNet-50 is modified as follows to  cy may degrade after a saturation. This phenomenon is often referred to as the degradation problem. <ref type=\"bibr\" target=\"#b13\">He et al. (2016)</ref> recently proposed a Residual Network (ResNet) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , AutoMine).</p><p>To address these challenges, general-purpose graph mining systems like Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref>, Fracta e 16-core machine outperforms state-of-the-art distributed graph mining systems including Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref> and G-M </formula><p>Step 1</p><p>Step 2</p><p>Step 3  <ref type=\"bibr\" target=\"#b70\">[57]</ref>, Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> and Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref>. Num </ref> (a realworld graph dataset), RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref> and Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> generate over a billion partial matches for clique countin neral purpose graph mining systems 3 : Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref>, Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref> and G-M  is because its breadth-first exploration generates large amounts of partial matches which must be  <ref type=\"bibr\" target=\"#b65\">[52]</ref> and RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref>. '\u00d7' 4,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>. Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> is a distributed graph mining system that follows a filter n memory or on disk) so that they can be extended. While systems based on breadth-first exploration <ref type=\"bibr\" target=\"#b65\">[52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref> demand high memory oesn't need to separately define the exploration strategy, as done in other pattern-unaware systems <ref type=\"bibr\" target=\"#b65\">[52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>.</p></div> <div xm get=\"#b21\">[8,</ref><ref type=\"bibr\" target=\"#b25\">12,</ref><ref type=\"bibr\" target=\"#b47\">34,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>, they are not patte ico and labeled Patents have been used by previous systems <ref type=\"bibr\" target=\"#b25\">[12,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref> to evaluate FSM whi get=\"#b25\">12,</ref><ref type=\"bibr\" target=\"#b35\">22,</ref><ref type=\"bibr\" target=\"#b47\">34,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>. Arabesque <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns and proposed Definition 1 which emphasizes the reasoning engine of knowledge graphs. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> proposed a definition as a multi-relational graph in Definit ormation into an ontology and applies a reasoner to derive new knowledge. Definition 2 (Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref>). A knowledge graph is a multirelational graph composed of e \"#b5\">[6]</ref>, Chinese knowledge graph construction <ref type=\"bibr\" target=\"#b9\">[10]</ref>, KGE <ref type=\"bibr\" target=\"#b7\">[8]</ref> or KRL <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The latter </ref> presented KRL in a linear manner, with a concentration on quantitative analysis. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> categorized KRL according to scoring functions, and specific s of auxiliary information for KRL such as attributes, relation path and logical rules. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> gave a detailed review on these information. This paper disc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oc bottom-up fashion, where PMU designers attempted to cover key issues via \"dedicated miss events\" <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Yet, how does one pin-point performance issues that were no rchy's top level. This accurate classification distinguishes our method from previous approaches in <ref type=\"bibr\" target=\"#b0\">[1]</ref>[5][6].</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><h same symptom. Such scenarios of L1 hits and near caches' misses, are not handled by some approaches <ref type=\"bibr\" target=\"#b0\">[1]</ref> <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p><p>Note performa t=\"#b5\">[6]</ref>, nor complex structures with latency counters as in Accurate CPI Stacks proposals <ref type=\"bibr\" target=\"#b0\">[1]</ref>[8] <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div x tempted to accurately classify performance impacts on out-of-order architectures. Eyerman et al. in <ref type=\"bibr\" target=\"#b0\">[1]</ref>[9] use a simulation-based interval analysis model in order t reference model) is that it restricts all stalls to a fixed set of eight predefined miss events. In <ref type=\"bibr\" target=\"#b0\">[1]</ref>[4] <ref type=\"bibr\" target=\"#b4\">[5]</ref> there is no consi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n auxiliary trusted training set to differentiate examples <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. This requires extr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: handle data in unsupervised ways.</p><p>Problems for Importance Identification While previous works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> attempt to quan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: large unlabeled molecule dataset. Through contrastive loss <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, MolCLR learns the representations by contrasting positive  tum encoder, which builds an on-the-fly consistent dictionary. Instead of using memory bank, SimCLR <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> demonstrates contr .1\">MolCLR Framework</head><p>Our MolCLR model is developed upon the contrastive learning framework <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Latent representa rocessing and augmentation, GNN-based feature extractor, non-linear projection head, and an NT-Xent <ref type=\"bibr\" target=\"#b36\">[37]</ref> contrastive loss. In our case, we implement the commonly-u  The choice of the temperature parameter \u03c4 in Eq. 1 impacts the performance of contrastive learning <ref type=\"bibr\" target=\"#b36\">[37]</ref>. An appropriate \u03c4 benefits the model to learn from hard ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: model with fewer parameters. The recently-introduced factorised time-delay neural networks (TDNN-F) <ref type=\"bibr\" target=\"#b12\">[13]</ref> utilise half the number of parameters than the hybrid netw to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Consequently, a TDNN-F acoustic model (10 time-delay laye. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n. To clarify, FLAG is intrinsically different from the previous graph adversarial training methods <ref type=\"bibr\" target=\"#b8\">(Feng et al., 2019;</ref><ref type=\"bibr\" target=\"#b5\">Deng et al., 20 \" target=\"#b5\">Deng et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Jin &amp; Zhang, 2019)</ref>. <ref type=\"bibr\" target=\"#b8\">Feng et al. (2019)</ref> proposed to reinforce local smoothness to mak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent matching tasks, where a query is a short-form text and a document is a long-form text. DeepRank <ref type=\"bibr\" target=\"#b23\">[24]</ref> is the first work to treat query and document differently, atching signals loss.</p><p>Inspired by the previous works <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, who tell us that two texts can help each other for noise d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: representation to implement optimizations, e.g., auto differentiation and dynamic memory management <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target tional Graphs</head><p>Computational graphs are a common way to represent programs in DL frameworks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">Related Work</head><p>Deep learning frameworks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target on graph DSLs are a typical way to represent and perform high-level optimizations. Tensorflow's XLA <ref type=\"bibr\" target=\"#b2\">[3]</ref> and the recently introduced DLVM <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nteraction-focused <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or representation-focused <ref type=\"bibr\" target=\"#b14\">[1 h positions. It is also similar to the indicator matching matrix proposed previously by Pang et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref>. While the interaction matrix X perfectly captures every q <ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>.</p><p>Pang et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref> propose the use of matching matrices to represent the simi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: usal model which could measure this causal relationship i.e. Linear Structual Equation Models (SEM) <ref type=\"bibr\" target=\"#b20\">(Shimizu et al., 2006)</ref>. Existing methods for disentangled repre et=\"#b6\">(Hoyer et al., 2009;</ref><ref type=\"bibr\" target=\"#b24\">Zhang &amp; Hyvarinen, 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shimizu et al., 2006)</ref>. <ref type=\"bibr\" target=\"#b19\">Pearl (20 2009)</ref> introduce a probabilistic graphical model based framework to learn causality from data. <ref type=\"bibr\" target=\"#b20\">Shimizu et al. (2006)</ref> proposed an effective method called LiNGA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ph embedding methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> to learn the embedding of each item, dubbed Base Graph Embe . Experiments are conducted to compare four methods: BGE, LINE, GES, and EGES. LINE was proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which captures the first-order and second-order proximity. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which solve the problem of limited equipment resources and e the problem of limited equipment resources and reduce the running time. For example, Zhang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> constructed an embedding based model to distill user's met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ous success in the field of natural language processing (NLP) since the development of Transformers <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> which are currently the best performing n nal information. Since Laplacian PEs are generalization of the PE used in the original transformers <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> to graphs and these better help encode di r</head><p>The Graph Transformer is closely the same transformer architecture initially proposed in <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref>, see Figure <ref type=\"figure\">1</ref> (L target=\"#tab_2\">2</ref>), which employs multi-headed attention inspired by the original transformer <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017)</ref> and have been often used in the literatur e each word attending to each other word in a sentence, as followed by the Transformer architecture <ref type=\"bibr\" target=\"#b27\">(Vaswani et al. 2017</ref>). -b) Next, the so-called graph considered. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mmerce, instant video platforms and social media platforms <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. For example, VBPR  e conduct experiments on three categories of widely used Amazon dataset introduced by McAuley et al.<ref type=\"bibr\" target=\"#b24\">[25]</ref>:</figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ower consumption and chip area using McPAT <ref type=\"bibr\" target=\"#b10\">[11]</ref> and CACTI v6.5 <ref type=\"bibr\" target=\"#b11\">[12]</ref> assuming a 22 nm technology node. Area and per-access powe er and floating-point registers. The MSHR is extended to support 8 outstanding misses. We use CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref> to estimate chip area. CACTI accounts for the area of circ  power consumption. power consumed by the additional FSC hardware structures is modeled using CACTI <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Table <ref type=\"table\" target=\"#tab_3\">3</ref> reports p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s selected using Google Suggest API, where the answers are entities in Freebase. CuratedTREC (TREC) <ref type=\"bibr\" target=\"#b1\">(Baudi\u0161 and \u0160ediv\u1ef3, 2015)</ref>  </p></div> <div xmlns=\"http://www.tei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: stion and the corresponding evidence document <ref type=\"bibr\" target=\"#b10\">[10]</ref>. Fei et al. <ref type=\"bibr\" target=\"#b8\">[8]</ref> propose a hierarchical multi-task word embedding model to le get=\"#b30\">30]</ref> are mainly focus on the medical multiple choices questionanswering, Fei et al. <ref type=\"bibr\" target=\"#b8\">[8]</ref> mainly focus on synonym prediction, which is different from   synonym pairs, we construct additional medical synonym pairs from the existing corpus. Inspired by <ref type=\"bibr\" target=\"#b8\">[8]</ref>, we use two methods for the synonym pairs construction: a ru. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b32\">33]</ref>, satellite imaging <ref type=\"bibr\" target=\"#b37\">[38]</ref>, face recognition <ref type=\"bibr\" target=\"#b16\">[17]</ref> and surveillance <ref type=\"bibr\" target=\"#b52\">[53]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o address challenging implicit feedback in recommendations <ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>. In this scenario o generate feature and then make recommendation via a RNN network structure with our previous study <ref type=\"bibr\" target=\"#b14\">[15]</ref>. However, However, in addition to application scenario, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b12\">(Maal\u00f8e et al., 2016;</ref><ref type=\"bibr\" target=\"#b25\">Springenberg, 2016;</ref><ref type=\"bibr\" target=\"#b15\">Odena, 2016;</ref><ref type=\"bibr\">Salimans et al., 2016)</ref>. It A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecent embedding for node i. The use of max operator is inspired from learning on general point sets <ref type=\"bibr\" target=\"#b34\">(Qi et al., 2017)</ref>. By applying max-pooling operator element-wis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  . , h T \u2032 ) (T \u2032 \u2264 T ), interleaved with subsampling layers to reduce the computational complexity <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The decoder network generates a probability distribution . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"table\">1</ref> compares amortized optimization and NPM, making a connection to multi-task learning <ref type=\"bibr\" target=\"#b6\">(Caruana, 1998)</ref>. Additionally, we could frame NPM as a hypernetw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with the success of deep neural networks (DNNs), some researches have applied DNNs to precipitation <ref type=\"bibr\" target=\"#b20\">[21]</ref> and radar echo <ref type=\"bibr\" target=\"#b31\">[32]</ref> n ation and intensity of rain and snow. To solve the problem of spatiotemporal dependency, Shi et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> developed the conventional LSTM and propose convolutional  , with a goal to overcome the drawbacks of FC-LSTM in handling spatial-temporal data such as videos <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Specifically, in the ConvLSTM network, the fully-connecte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: exponentially increasing dependency of nodes over layers; a phenomenon framed as neighbor explosion <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>. Due to neighbor explosion and since th hen et al., 2018b)</ref>. VR-GCN aims to reduce the variance in estimation during neighbor sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, and avoids the need to sample a large   memory usage of GCN+GAS training with the memory usage of full-batch GCN, and mini-batch GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref> and CLUSTER-GCN <ref type=\"bibr\" target as they will run out of memory on common GPUs. We compare with 10 scalable GNN baselines: GRAPHSAGE <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017)</ref>, FASTGCN <ref type=\"bibr\" target=\"#b2\"> a &amp; Tang, 2020;</ref><ref type=\"bibr\" target=\"#b32\">Rong et al., 2020)</ref>: Nodewise sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ser intent.</p><p>As a general information modeling method, Heterogeneous Information Network (HIN) <ref type=\"bibr\" target=\"#b17\">[18]</ref>, consisting of multiple types of objects and links, has be y data mining tasks <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this paper, we propose to model the intent recommendati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: treated as the appearance features of segments. For the motion features, we follow the operation in <ref type=\"bibr\" target=\"#b50\">[50]</ref> and extract the 400dimensional feature vectors from the TS  means that the NMS is not used. We use the conventional average recall with 100 proposals (AR@100) <ref type=\"bibr\" target=\"#b50\">[50]</ref> to evaluate the performance of the proposal generator. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ot translation with online back translation. <ref type=\"bibr\" target=\"#b19\">Ji et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b23\">Liu et al. (2020)</ref> shows that large scale monolingual data can i EU score after removing Romanian dialects. (**) BLEU scores for Transformer and mBART are cited from<ref type=\"bibr\" target=\"#b23\">(Liu et al., 2020)</ref> gual data. The total number of sentences in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o this end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 20 \"#b20\">(Munkhdalai and Yu, 2016b)</ref>.</p><p>We ensemble our ESIM model with syntactic tree-LSTMs <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015)</ref> based on syntactic parse trees and achieve s here are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015)</ref> same as in model ( <ref type=\"formula\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n this paper, we introduce TOAD-GAN as a solution to these problems. Our work is inspired by SinGAN <ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019)</ref>, a recent Generative Adversa s of TOAD-GAN on Super Mario Bros. level 1-2. The architecture is adapted from SinGAN (cf. Fig. 4 of<ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019)</ref>). We use a downsampling meth e training process.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>SinGAN</head><p>SinGAN <ref type=\"bibr\" target=\"#b15\">(Shaham, Dekel, and Michaeli 2019</ref>) is a novel GAN architecture  p at the lowest scale. For a more in-depth explanation please refer to the original SinGAN paper by <ref type=\"bibr\" target=\"#b15\">Shaham, Dekel, and Michaeli (2019)</ref>.</p></div> <div xmlns=\"http:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: titions. For Indic languages, we fine-tune on Hi\u2192En translation (1.56M sentence pairs are from IITB <ref type=\"bibr\" target=\"#b27\">(Kunchukuttan et al., 2018b</ref>)), and test on {Ro, It, Cs, Nl}\u2192En . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed in the literature, which can be roughly categorized into two branches: (i) co-training strategy <ref type=\"bibr\" target=\"#b4\">[6]</ref>; and (ii) learning joint representation across modalities in  in each modality are the same, which may not be satisfied in the real settings, as self-claimed in <ref type=\"bibr\" target=\"#b4\">[6]</ref>; while the latter branch of methods fails to capture the hig hared by all modalities. The first branch applies the co-training algorithm proposed by Blum et. al <ref type=\"bibr\" target=\"#b4\">[6]</ref>. <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" t representation. A common belief in multi-modality learning <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" targe only assumed in the literature of semi-supervised learning <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b4\">6,</ref><ref type=\"bibr\" target=\"#b11\">13,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k. Although significant progress has been made in generating videos using temporal-dependency model <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target =\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> and temporal-dependent ones <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ ssing The inputs to audio encoders are mel-frequency cepstral coefficient (MFCC) values. Similar to <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target  can better reflect the input audio (e.g., frames in red boxes). The outputs of existing approaches <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, on the other hand, ar n metrics, we quantitatively compare the performance of our AVWnet with state-of-the-art approaches <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> on two datasets: LRW a orporate valuable temporal information into the recurrent neural network (RNN), whereas Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> divide the training into two successive steps: training an a  in different subregions. Pumarola et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> and Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> adapt the facial attention masks and base color features to  e generator and multiform discriminators arranged in a tree-like structure. Inspired by Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref>, we leverage pixel-wise landmark attentions in different sta ss, given a random face image and a speech signal, we first employ the landmark generation model in <ref type=\"bibr\" target=\"#b3\">[4]</ref> to get the sequence of predicted landmarks instead of real l  of the existing works, single-level-based attention is widely utilized to recalibrate the features <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which usually limits the ability to capture more informativ  pixel loss L pi x imposed on mouth region by the mean absolute error and regression-based loss L R <ref type=\"bibr\" target=\"#b3\">[4]</ref> are also combined together to enforce the high-quality gener example landmark and future landmark sequences predicted by a pre-trained landmark-prediction model <ref type=\"bibr\" target=\"#b3\">[4]</ref>. For audio signal, we transform videos to raw audio format f to four sections.</p><p>Fig. <ref type=\"figure\">8</ref> The detailed comparison between Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> (top) and our approach (bottom). Green boxes highlight the a  region that AVWnet produces sharper details. In contrast, the mouth region produced by Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> are blurry</p><p>The initial section consists of a four-laye esults generated by the presented AVWnet with those of two state-of-the-art approaches: Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Chung et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Al ref type=\"figure\" target=\"#fig_2\">7</ref> compares the results generated by AVWnet with Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Chung et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Bo iceable lip movements. Figure <ref type=\"figure\">8</ref> further compares our work with Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> on two cases where their results contain blurry regions and  ics and for both datasets.</p><p>We also compare the computation time needed by AVWnet and approach <ref type=\"bibr\" target=\"#b3\">[4]</ref> for generating each frame under the same parameter settings  Card). We apply both methods to produce the same talking video with 31 frames, respectively. Method <ref type=\"bibr\" target=\"#b3\">[4]</ref> can generate one frame in 0.018 s, whereas our method takes  el. In terms of synchronization, 56% scored higher for AVWnet and 28% scored higher for Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= e\">9</ref> User study on videos generated using the proposed AVWnet and the state-of-the-art method <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which shows AVWnet outperforms <ref type=\"bibr\" target=\"#b3 the state-of-the-art method <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which shows AVWnet outperforms <ref type=\"bibr\" target=\"#b3\">[4]</ref> in synchronization and image quality The performances of the e whole video. To address this limitation, we conduct a user study between our model and Chen et al.<ref type=\"bibr\" target=\"#b3\">[4]</ref>, which has better performance than Chung et al.<ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Usage-based pricing can affect overall demand levels, but does not even out short-term fluctuations <ref type=\"bibr\" target=\"#b11\">[13]</ref>. To manage these fluctuations in demand for a fixed amount re have a shorter expected running time. Job interruptibility. We can use the expected running time <ref type=\"bibr\" target=\"#b11\">(13)</ref> to observe the effect of the recovery time parameter, t r  s feasible at any price.</p><p>The optimal bid price. We can now multiply the expected running time <ref type=\"bibr\" target=\"#b11\">(13)</ref> with the expected spot price <ref type=\"bibr\" target=\"#b7\" o <ref type=\"bibr\" target=\"#b13\">(15)</ref>.</p><p>We now observe that the expected running time in <ref type=\"bibr\" target=\"#b11\">(13)</ref> decreases with the bid price, while the expected spot pric very, execution, and overhead times. Hence, we can extend the result for a single persistent bid in <ref type=\"bibr\" target=\"#b11\">(13)</ref> as</p><formula xml:id=\"formula_32\">M i=1 T i F \u03c0 (p) = t s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n reconstruction over high-level visual concepts, referred to as visual words. For instance, BoWNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> derived a teacherstudent learning scheme following this pr ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Among them, BoWNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> was the first work to use BoWs as reconstruction targets f arget=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>. In self-supervised learning, BowNet <ref type=\"bibr\" target=\"#b24\">[25]</ref> trains a student to match the BoW representations produced  a key ingredient of several recent deep learning approaches <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, regularizes the spectral norm of the weight matrix at each i et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. <ref type=\"bibr\" target=\"#b7\">[8]</ref> utilizes Lipschitz p et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, keep \u03b2 constant across layers. These harder constraints over get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. <ref type=\"bibr\" target=\"#b7\">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustne  utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks. Unlike <ref type=\"bibr\" target=\"#b7\">[8]</ref>, our approach does not require a predetermined set of hyper- 11]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b7\">[8]</ref>. <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" their papers: \u03b2 = 1.0, 1.6, 2.0. The 2 works given in <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b7\">[8]</ref> may be seen as subsets of the works given in <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tive fields. Several recent works attribute this performance degradation to the oversmoothing issue <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" targ nerated by multiple GCN layers, like 6 layers, are very difficult to be separated.  Several studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> attribute this perfo filter on the spectral domain, thus deriving smoothing features across a graph. Another recent work <ref type=\"bibr\" target=\"#b2\">[3]</ref> verify that smoothing is the nature of most typical graph co shallow architectures. A smoothness regularizer term and adaptive edge optimization are proposed in <ref type=\"bibr\" target=\"#b2\">[3]</ref> to relieve the over-smoothing problem. Jumping Knowledge Net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the span length is 2 out of 10. 2020; <ref type=\"bibr\" target=\"#b22\">Walawalkar et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Uddin et al., 2021)</ref> and hidden-level mixup <ref type=\"bibr\" tar  replacement strategy is also adopted in <ref type=\"bibr\" target=\"#b26\">Yun et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b20\">Uddin et al. (2021)</ref>. In situations where span length is the sam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  give a brief introduction to GNNs, and one can refer to <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> for a more detailed information. GNNs deal with learning p able.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Gra here u \u2208 N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>: It uses the MLP and sum pooling as the aggregation and co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b45\">49,</ref><ref type=\"bibr\" target=\"#b42\">46,</ref><ref type=\"bibr\" target=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b26\">30,</ref><ref type=\"bibr\" tar get=\"#b59\">65,</ref><ref type=\"bibr\" target=\"#b42\">46,</ref><ref type=\"bibr\" target=\"#b56\">62,</ref><ref type=\"bibr\" target=\"#b43\">47,</ref><ref type=\"bibr\" target=\"#b36\">40,</ref><ref type=\"bibr\" tar  it is possible to configure different credit numbers for each unit if needed. For example, ClickNP <ref type=\"bibr\" target=\"#b43\">[47]</ref> uses a SHA1 engine that can process 64 packets in parallel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ar how to properly train deep GCN architectures, where several works have studied their limitations <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" ta ref type=\"bibr\" target=\"#b52\">53]</ref> is an open problem in the graph learning space. Recent work <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" ta causes oversmoothing, eventually leading to features of graph vertices converging to the same value <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Due to these limitations, most state-of-the-art GCNs are  is limited to a small number of layers <ref type=\"bibr\" target=\"#b5\">(6)</ref>. Recently, Li et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> studied the depth limitations of GCNs and showed that deep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  is only 0.6%. Similar performance result on the OoO core is reported on the original paper of NoSQ <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Considering that the performance difference is limited, w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nformation retrieval to capture the exact and soft matches between a query and a candidate document <ref type=\"bibr\" target=\"#b10\">[Xiong et al., 2017]</ref>. Specifically, we apply the basic BERT uni ors are disordered and independent from each other. Thus we adopt a RBF kernel aggregation function <ref type=\"bibr\" target=\"#b10\">[Xiong et al., 2017]</ref> to extract features about the accumulation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  The preliminary experiment starts with a representative graph SSL scheme, Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b45\">[46]</ref>, for encoding the global information into node representat ws of nodes (sampled ego-networks). Motivated by DIM <ref type=\"bibr\" target=\"#b14\">[15]</ref>, DGI <ref type=\"bibr\" target=\"#b45\">[46]</ref> and InfoGraph <ref type=\"bibr\" target=\"#b42\">[43]</ref> ha  + (1 \u2212 y i ) \u2022 log (1 \u2212 p i )) , (5)</formula><p>Basic L S S L . We adopt Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b45\">[46]</ref>, a stateof-the-art graph SSL scheme, to instantiate the SS pture transferable structural patterns across graphs, over-emphasizes the structural homophily. DGI <ref type=\"bibr\" target=\"#b45\">[46]</ref> contrasts the whole graph with the node in it to encode th aph Contrastive Coding (GCC) <ref type=\"bibr\" target=\"#b33\">[34]</ref> and Deep Graph Infomax (DGI) <ref type=\"bibr\" target=\"#b45\">[46]</ref> perform contrastive learning between node-node pairs or gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: information which may be head pose, expression, or landmarks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , such as graphs which encode the pairwise relationships. This includes examples of social networks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, protein interfaces <ref type=\"bibr\" target=\"#b3\">[4]</ref>, assification, including Cora, Citeseer, Pubmed <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Reddit <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Intensive experiments verify the effectiveness of our metho lf-attention strategy.</p><p>More recently, two kinds of sampling-based methods including GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGCN <ref type=\"bibr\" target=\"#b20\">[21]</ref> were d and Extensions</head><p>Relation to other sampling methods. We contrast our approach with GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGC-N <ref type=\"bibr\" target=\"#b20\">[21]</ref> regar and Pubmed <ref type=\"bibr\" target=\"#b10\">[11]</ref>  community different posts belong to in Reddit <ref type=\"bibr\" target=\"#b2\">[3]</ref>. These graphs are varying in sizes from small to large. Part  set to be 16. For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The numbers of the sampling nodes for all layers excluding  \"><head n=\"7.1\">Alation Studies on the Adaptive Sampling</head><p>Baselines. The codes of GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGCNN <ref type=\"bibr\" target=\"#b20\">[21]</ref> provi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type=\"bibr\" target=\"#b16\">Liu et al. (2019)</ref> investigated KD from another perspective. Ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">[5,</ref><ref type=\"bibr\" target=\"#b26\">16,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b46\">36,</ref><ref type=\"bibr\" tar  fact verification <ref type=\"bibr\" target=\"#b23\">[13,</ref><ref type=\"bibr\" target=\"#b32\">22,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b55\">45,</ref><ref type=\"bibr\" tar d promising results <ref type=\"bibr\" target=\"#b15\">[5,</ref><ref type=\"bibr\" target=\"#b27\">17,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b57\">47,</ref><ref type=\"bibr\" tar im is true or false <ref type=\"bibr\" target=\"#b15\">[5,</ref><ref type=\"bibr\" target=\"#b32\">22,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b46\">36,</ref><ref type=\"bibr\" tar get=\"#b32\">22,</ref><ref type=\"bibr\" target=\"#b38\">28,</ref><ref type=\"bibr\" target=\"#b40\">30,</ref><ref type=\"bibr\" target=\"#b43\">33,</ref><ref type=\"bibr\" target=\"#b78\">68,</ref><ref type=\"bibr\" tar studied from several perspectives over the last few years, e.g. as natural language inference (NLI) <ref type=\"bibr\" target=\"#b43\">[33,</ref><ref type=\"bibr\" target=\"#b45\">35,</ref><ref type=\"bibr\" ta ef>;</p><p>(2) FEVER leader board methods: Athene <ref type=\"bibr\" target=\"#b32\">[22]</ref>, UCL MR <ref type=\"bibr\" target=\"#b43\">[33]</ref>, UNC <ref type=\"bibr\" target=\"#b78\">[68]</ref>, Papelo <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ormally, given the reference frame I t and each support frame I t+\u03c4 , Feature Pyramid Network (FPN) <ref type=\"bibr\" target=\"#b55\">[56]</ref> is leveraged to extract multi-scale pyramidal feature maps ad><p>Feature Pyramid Network. FPN is built at the top of ResNet-101 pre-trained on ImageNet. As in <ref type=\"bibr\" target=\"#b55\">[56]</ref>, P3, P4, Algorithm 2 Inference Algorithm of our SSVD </p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b14\">15]</ref>, video object detection <ref type=\"bibr\" target=\"#b38\">[40,</ref><ref type=\"bibr\" target=\"#b58\">60]</ref>.</p><p>Our approach is closely related to <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, patch location prediction <ref type=\"bibr\" target=\"#b23\">[24]</ref>, solving jigsaw puzzles <ref type=\"bibr\" target=\"#b62\">[63]</ref>, and image rotation prediction <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d was \"set deliberately low to account for inaccuracies in bounding boxes in the ground truth data\" <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Does COCO have better labelling than VOC? This is definitel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, taking into account additional images <ref type=\"bibr\" target=\"#b32\">[33]</ref> or 3D scans <ref type=\"bibr\" target=\"#b3\">[4]</ref>, or by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vides tolerance against incorrect labels.</p><p>The recently introduced transform/stability loss of <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> is based on the same principle as our wo rements is \u223c0.5 percentage points better than independent flips.</p><p>A principled comparison with <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> is difficult due to several reasons. The  paths, and comparing the outputs of the network instead of pre-activation data of the final layer. <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> recently introduced a new loss function . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  in multi-layer GCNs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. According to the experimental results, it is suitable for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  diverse semantic information of node type and edge type <ref type=\"bibr\" target=\"#b19\">[20]</ref>- <ref type=\"bibr\" target=\"#b21\">[22]</ref>. GTN <ref type=\"bibr\" target=\"#b22\">[23]</ref> converts he. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Rosas et al., 2014)</ref>, which was then used to develop a multimodal deception detection system <ref type=\"bibr\" target=\"#b1\">(Abouelenien et al., 2014)</ref>. An extensive review of approaches fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ation <ref type=\"bibr\" target=\"#b24\">(Puri and Catanzaro, 2019)</ref>, commonsense knowledge mining <ref type=\"bibr\" target=\"#b5\">(Davison et al., 2019)</ref> and argumentative relation classification. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re-based sequence labeling models with bilingual constraints/inferences for Chinese and English NER <ref type=\"bibr\" target=\"#b1\">[Che et al., 2013;</ref><ref type=\"bibr\">Wang et al., 2013]</ref>. Our ty types (Person, Location, Organization, None), which are commonly adopted in previous NER studies <ref type=\"bibr\" target=\"#b1\">[Che et al., 2013;</ref><ref type=\"bibr\">Wang et al., 2013]</ref>.</p> s drawback of this approach is the requirement of manually annotate bilingual NER data. There-fore, <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2010]</ref> proposed approaches to extract bilingual nam od of labeling bilingual corpora with named entity labels automatically based on Wikipedia.</p><p>[ <ref type=\"bibr\" target=\"#b1\">Che et al., 2013;</ref><ref type=\"bibr\">Wang et al., 2013]</ref> tackl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d><p>Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The network predicts 4 coordinates for each bounding box, ocation of filter application using a sigmoid function. This figure blatantly self-plagiarized from <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>is not the best but does overlap a ground truth obj location of filter application using a sigmoid function. This figure blatantly self-plagiarized from<ref type=\"bibr\" target=\"#b14\">[15]</ref>.</figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: not visible at the node-level.</p><p>Graph kernels based on the k-WL have been proposed in the past <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref>. However, a key advantage o ref type=\"bibr\" target=\"#b35\">(Shervashidze et al. 2011)</ref> as well as its higher-order variants <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref>. Graphlet and Weisfeiler-Le  <ref type=\"bibr\" target=\"#b22\">(Kriege, Giscard, and Wilson 2016)</ref>, and the global-local k-WL <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref> with k in {2, 3} as kernel  \"foot_0\">Note that the definition of the local neighborhood is different from the the one defined in<ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref> which is a superset of our  e that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017;</ref><ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: obabilities. But complex and tricky training methods make it hard to implement. Triggered attention <ref type=\"bibr\" target=\"#b8\">[9]</ref> utilizes the spikes produced by connectionist temporal class. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b43\">(Xiong et al., 2016;</ref><ref type=\"bibr\" target=\"#b31\">Seo et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Bahdanau et al., 2015)</ref>. The resulting representation is encoded  l language.</p><p>In this work, we consider attention-based neural machine translation (NMT) models <ref type=\"bibr\" target=\"#b2\">Bahdanau et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b24\">Luong et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.\">Background and Related Work</head><p>Previous work <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> describes support me ware support for thread activation and deactivation, as found in prior studies of thread scheduling <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. While those works u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b38\">39]</ref>, sorting frames or video clips <ref type=\"bibr\" target=\"#b53\">[54,</ref><ref type=\"bibr\" target=\"#b96\">97,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emic paper analysis.</p><p>The pipeline for creating S2ORC was used to construct the CORD-19 corpus <ref type=\"bibr\" target=\"#b2\">(Wang et al., 2020)</ref>, which saw fervent adoption as the canonical. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  as exponential service times, infinite buffers, and so on <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Likewise, analys et=\"#b6\">[7]</ref> and hypercubes <ref type=\"bibr\" target=\"#b22\">[24]</ref>. The study presented in <ref type=\"bibr\" target=\"#b8\">[9]</ref> is not restricted to a particular topology, but it assumes a where T Bus is the service time of the Bus architecture and C Bus is the contention matrix given in <ref type=\"bibr\" target=\"#b8\">(9)</ref>. Finally, we note that when each buffer shown in Fig. <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: urce speaker to make it sound like that of a target speaker without changing the linguistic content <ref type=\"bibr\" target=\"#b0\">[1]</ref>. This technique has many applications, including expressive . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata</head><p>All experiments are performed on the publicly available Lib-riSpeech audio book corpus <ref type=\"bibr\" target=\"#b10\">[11]</ref>. We use the \"train-clean-100\" set as the paired data set,  blic domain books. The books were selected such that there is no overlap with the dev and test sets <ref type=\"bibr\" target=\"#b10\">[11]</ref>. On the other hand, the training data set transcriptions a for the apostrophe in contractions (we replace hyphens with a space). Unlike the original LM corpus <ref type=\"bibr\" target=\"#b10\">[11]</ref> we take no steps to replace non-standard words with a cano. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[49]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, <ref type=\"bibr\" target=\"#b52\">[53]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  R X X (\u03c4 ) can be calculated by Fast Fourier Transforms (FFT) based on the Wiener-Khinchin theorem <ref type=\"bibr\" target=\"#b41\">[37]</ref>:</p><formula xml:id=\"formula_8\">S X X (f ) = F (X t ) F * . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cy matrix with a sparsified version of a diffusion matrix (e.g., heat kernel or PageRank). Geom-GCN <ref type=\"bibr\" target=\"#b25\">[26]</ref> precomputes unsupervised node embeddings and uses neighbor  0.7), and across the full spectrum (\"Overall\"). The \"*\" denotes ranks based on results reported in <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Real datasets &amp; setup We now evaluate the performance of nodes per class for train/validation/test<ref type=\"foot\" target=\"#foot_2\">2</ref> ) provided by <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>For Cora-Full, we generate 3 random splits, with 25 the best results among the three recentlyproposed GEOM-GCN variants ( \u00a7 4), directly from the paper <ref type=\"bibr\" target=\"#b25\">[26]</ref>: other models (including ours) outperform this method sign ng universities, originally collected by the CMU WebKB project. We used the preprocessed version in <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In these networks, nodes are web pages, which are classif br\" target=\"#b28\">[29]</ref>. For the classification task, we utilize the class labels generated by <ref type=\"bibr\" target=\"#b25\">[26]</ref>, where the nodes are categorized into 5 classes based on t erage traffic. \u2022 Actor is a graph representing actor co-occurrence in Wikipedia pages, processed by <ref type=\"bibr\" target=\"#b25\">[26]</ref> based on the film-director-actor-writer network in <ref ty ter network in <ref type=\"bibr\" target=\"#b34\">[35]</ref>. We also use the class labels generated by <ref type=\"bibr\" target=\"#b25\">[26]</ref>. \u2022 Cora, Pubmed and Citeseer are citation graphs originall fferent data splits. Best model per benchmark highlighted in gray. The \"*\" results are obtained from<ref type=\"bibr\" target=\"#b25\">[26]</ref> and \"N/A\" denotes non-reported results.</figDesc><table><r ix of A + I.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_2\"><ref type=\"bibr\" target=\"#b25\">[26]</ref> claims that the ratios are 60%/20%/20%, which is different atent space to define graph convolution. Some of these works <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> acknowledge the cha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ification dataset -FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> propose a hybrid attention-based prototypical   and useless at the same time.</p><p>So we apply a CNN-based feature attention mechanism similar to <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> proposed as a class feature extractor. It depe 17)</ref> which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> which includes Proto and PHATT. For a fair com ><table /><note>* reported by<ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> and \u25c7 reported by<ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref>.</note></figure> \t\t\t<note xmlns=\"http://www.te assification and few-shot text classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Gao et al., 2019)</ref> tasks respectively, so our model is based on p shra et al., 2018)</ref>, Proto <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> and PHATT <ref type=\"bibr\" target=\"#b3\">(Gao et al., 2019)</ref> respectively.</p></div> <div xmlns=\"http://ww. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rize models <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2018)</ref>. Recently, the Mixup variants <ref type=\"bibr\" target=\"#b40\">(Verma et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Summers and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  has made abstractive summarization viable <ref type=\"bibr\" target=\"#b3\">(Chopra et al., 2016;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016;</ref><ref type=\"bibr\" target=\"#b20\">Rush et a arget=\"#b7\">Gulcehre et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Miao and Blunsom, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016;</ref><ref type=\"bibr\" target=\"#b28\">Zeng et a cently-introduced CNN/ Daily Mail dataset <ref type=\"bibr\" target=\"#b8\">(Hermann et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016)</ref>, which contains news articles (39 sente head><p>We use the CNN/Daily Mail dataset <ref type=\"bibr\" target=\"#b8\">(Hermann et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016)</ref>, which contains online news articles (7 ad n=\"2.1\">Sequence-to-sequence attentional model</head><p>Our baseline model is similar to that of <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>, and is depicted in Figure <ref type=\"f e on those datasets.</p><p>However, large-scale datasets for summarization of longer text are rare. <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> adapted the DeepMind question-answering  considerably different from that of <ref type=\"bibr\" target=\"#b7\">Gulcehre et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>. Those works train their pointer compon with multi-sentence summaries (3.75 sentences or 56 tokens on average). We used scripts supplied by <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> to obtain the same version of the the d and b ptr in equation 8), and coverage adds 512 extra parameters (w c in equation 11).</p><p>Unlike <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>, we do not pretrain the word embeddings is (+1.1 ROUGE-1, +2.0 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> by (+4.07 ROUGE-1, +3.98 ROUGE-2, +3.73  scene. (...) Summary: more questions than answers emerge in controversial s.c. police shooting. of <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> by several ROUGE points. Despite the br g Representations <ref type=\"bibr\" target=\"#b24\">(Takase et al., 2016)</ref>, hierarchical networks <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type=\"b nique that has been applied to <ref type=\"bibr\">NMT (Sankaran et al., 2016)</ref> and summarization <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>. In this approach, each attention dist rget=\"#tab_2\">1</ref>), compared to the smaller boost given by temporal attention for the same task <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>.</p></div> <div xmlns=\"http://www.tei- he first three sentences of the article as a summary), and compare to the only existing abstractive <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref> and extractive <ref type=\"bibr\" target training pairs, 13,368 validation pairs and 11,490 test pairs. Both the dataset's published results <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016</ref><ref type=\"bibr\" target=\"#b16\">(Nallapat le online. <ref type=\"foot\" target=\"#foot_5\">6</ref>Given that we generate plain-text summaries but <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016;</ref><ref type=\"bibr\" target=\"#b16\">2017)</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nitial design space and the output is a refined design space of simpler or better models. Following <ref type=\"bibr\" target=\"#b20\">[21]</ref>, we characterize the quality of a design space by sampling gn, elevated to the population level and guided via distribution estimates of network design spaces <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>As a testbed for this paradigm, our focus is on exp essential to use a reliable comparison metric to guide our design process. Recently, the authors of <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposed a methodology for comparing and analyzing populat c scenario).</p><p>We rely on the concept of network design spaces introduced by Radosavovic et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref>. A design space is a large, possibly infinite, population  esign space is a large, possibly infinite, population of model architectures. The core insight from <ref type=\"bibr\" target=\"#b20\">[21]</ref> is that we can sample models from a design space, giving r ce design. To evaluate and compare design spaces, we use the tools introduced by Radosavovic et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref>, who propose to quantify the quality of a design space by  a single ResNet-50 <ref type=\"bibr\" target=\"#b7\">[8]</ref> model at 4GF for 100 epochs.</p><p>As in <ref type=\"bibr\" target=\"#b20\">[21]</ref>, our primary tool for analyzing design space quality is th i-c.org/ns/1.0\"><head>Appendix C: Optimization Settings</head><p>Our basic training settings follow <ref type=\"bibr\" target=\"#b20\">[21]</ref> as discussed in \u00a73. To tune the learning rate lr and weigh initial design space and the output is a refined design space of simpler or better models. Following<ref type=\"bibr\" target=\"#b20\">[21]</ref>, we characterize the quality of a design space by sampling tp://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We use the term design space following<ref type=\"bibr\" target=\"#b20\">[21]</ref>, rather than search space, to emphasize that we are not se ://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"5\" xml:id=\"foot_4\">Our training setup in \u00a73 exactly follows<ref type=\"bibr\" target=\"#b20\">[21]</ref>. We use SGD with momentum of 0.9, mini-batch size of 128 o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b51\">(R\u00f6der et al., 2014)</ref>, and OKE challenge 2015 and 2016 (OKE15 and OKE16) <ref type=\"bibr\" target=\"#b42\">(Nuzzolese et al., 2015)</ref>. More task details and hyperparameters. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d employ an Adam optimizer <ref type=\"bibr\" target=\"#b9\">[10]</ref> for parameter training. Dropout <ref type=\"bibr\" target=\"#b24\">[25]</ref> is also applied to alleviate overfitting.</p></div> <div x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of high inter-class similarities. Most recently, an additional center loss was introduced into CNNs <ref type=\"bibr\" target=\"#b43\">[44]</ref> to reduce the intra-class variations of the learned featur suffers from drastic data expansion when constructing image pairs from the training set. Wen et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> introduced a center loss for face recognition, which targe eview of Center Loss</head><p>As illustrated in Fig. <ref type=\"figure\">1</ref>(b), the center loss <ref type=\"bibr\" target=\"#b43\">[44]</ref> explicitly reduces the intra-class variations by pushing s  the CNN training.</p><p>1) Forward propagation: The center loss denoted as L C is defined in Eq. 1 <ref type=\"bibr\" target=\"#b43\">[44]</ref> as the summation of squared distances between samples and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Indeed, breakthrough generative learning strategies, such as Generative Adversarial Networks (GANs) <ref type=\"bibr\" target=\"#b2\">3</ref> can learn multidimensional distributions in disparate scientif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ative model, when extended to condition on image captions, could also generate novel visual scenes. <ref type=\"bibr\" target=\"#b35\">Reed et al. (2016b)</ref> later demonstrated that using a generative  low et al., 2014)</ref>, rather than a recurrent variational auto-encoder, improved image fidelity. <ref type=\"bibr\" target=\"#b35\">Reed et al. (2016b)</ref> showed that this system could not only gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -order and 2nd-order proximities between vertices to embed homogeneous networks. Specifically, LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref> learns two separated embeddings for 1st-order and 2nd-orde ignal on constructing the bipartite network. Similar to the modeling of 1st-order proximity in LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we model explicit relations by considering the local prox  of vertex sequences. Then the word2vec is applied on the corpus to learn vertex embeddings. \u2022 LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref>: This approach optimizes both the 1st-order and 2nd-order  vec inspire many works <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> to use inner product to model the interaction between two e ural embedding methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, we parameterize the conditional probability P(u c |u i ) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: urrent work further shows the success of search-based unsupervised text generation for paraphrasing <ref type=\"bibr\" target=\"#b15\">(Liu et al., 2020)</ref> and summa-rization <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et al., 2019)</ref> through providing \"task descriptions\" without using any labeled examples. GPT-3 <ref type=\"bibr\" target=\"#b0\">(Brown et al., 2020)</ref> demonstrated an impressive few-shot learnin fine-tuned language models. Even GPT-3 failed to beat the BERT-base baseline on another NLI dataset <ref type=\"bibr\" target=\"#b0\">(Brown et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org guage modeling problems. This approach does not make use of the available training examples. Later, <ref type=\"bibr\" target=\"#b0\">Brown et al. (2020)</ref> demonstrate an effective fewshot transfer by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  directly incorporate multi-hop neighborhood information of a node without explicit message-passing <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Intuitively, propagation based on personalized PageRank c here the node influence decays exponentially with each layer. However, as proposed, Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>'s approach does not easily scale to large graphs since it  ng that can reduce predictive performance.</p><p>To tackle both of these challenges Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> suggest decoupling the feature transformation from the pro instead. Unfortunately, even a moderate number of power iteration evaluations (e.g. Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> used K = 10 to achieve a good approximation) is prohibitiv ency of the subsequent learning step. Both of these approaches are a special case of the PPNP model <ref type=\"bibr\" target=\"#b32\">[33]</ref> which experimentally shows higher classification performan . In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model <ref type=\"bibr\" target=\"#b32\">[33]</ref> which we build upon. The results are summarized in Table < ich experimentally shows higher classification performance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>.</p><p>Approximating PageRank. Recent approaches combine ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b34\">[31]</ref>, and EIP <ref type=\"bibr\" target=\"#b21\">[18]</ref>, from IPC-1, and perfect <ref type=\"bibr\" target=\"#b35\">[32]</ref> with and without FDP. In this experiment, all mechanisms u  27KB 8-way Entangled Table <ref type=\"table\">)</ref>. \u2022 Perfect prefetching (Perfect): proposed in <ref type=\"bibr\" target=\"#b35\">[32]</ref>, where we assume that a prefetch brings the data into the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  machine learning approaches, a number of differential privacy based methods have been proposed. In <ref type=\"bibr\" target=\"#b21\">[22]</ref>, a differentially-private stochastic gradient descent algo ective deep learning model that can benefit from the data of all users. Our work is most related to <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bib \"bibr\" target=\"#b29\">[30]</ref>, but is quite different in several aspects. The proposed schemes in <ref type=\"bibr\" target=\"#b21\">[22]</ref> and <ref type=\"bibr\" target=\"#b22\">[23]</ref> were not des l privacy which can hide the existence of participants, and uses the moment accountant technique in <ref type=\"bibr\" target=\"#b21\">[22]</ref> to track the privacy loss. However, both methods did not c gression task.</p><p>Since the approaches proposed in <ref type=\"bibr\" target=\"#b22\">[23]</ref> and <ref type=\"bibr\" target=\"#b21\">[22]</ref> are not specially designed for collaborative learning, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: CRF) <ref type=\"bibr\" target=\"#b5\">[6]</ref> , Decision Tree <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> , Support Vector Machine (SVM) <ref type=\"bibr\">[9~12]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng generally requires huge metadata storage to be effective.</p><p>Fetch-Directed Prefetching (FDP) <ref type=\"bibr\" target=\"#b8\">[8]</ref>- <ref type=\"bibr\" target=\"#b14\">[12]</ref>, instead, leverag \" target=\"#b16\">[14]</ref>, although the baseline has a decoupled frontend and basic FDP capability <ref type=\"bibr\" target=\"#b8\">[8]</ref>, its shallow 12-instruction FTQ limited the runahead capabil >.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. Fetch Directed Prefetch</head><p>FDP <ref type=\"bibr\" target=\"#b8\">[8]</ref> was proposed to exploit the existing branch prediction mecha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pogation on just single hidden-layer feedforward neural networks. Recent attempts in this direction <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> propose efficient  r the search space through tunable parameters, in contrast to rigid search procedures in prior work <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Consequently, our of nodes. We contrast the performance of node2vec with state-of-the-art feature learning algorithms <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. We experiment wit odel, recent research established an analogy for networks by representing a network as a \"document\" <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. The same way as a r shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Our algorithm nod roceed by extending the Skip-gram architecture to networks <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. We seek to optimize the following objective function, whic  normalized Laplacian matrix of graph G as the feature vector representations for nodes. \u2022 DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>: This approach learns d-dimensional feature representation lude other matrix factorization approaches which have already been shown to be inferior to DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>. We also exclude a recent approach, GraRep <ref type=\"bibr the sampling procedure computationally efficient. We showed how random walks, also used in DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>, allow the sampled nodes to be reused as neighborhoods for SION</head><p>Both DeepWalk and LINE can be seen as rigid search strategies over networks. DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes search using uniform random walks. The obvious li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: trained word embeddings over randomly initialized ones. Embeddings are pretrained using skip-n-gram <ref type=\"bibr\" target=\"#b24\">(Ling et al., 2015a)</ref>, a variation of word2vec <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ich has been applied successfully in some previous studies <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>In transfer learning for ASR, we first train a neur nsfer learning. The first attempt for transfer leaning in the DNN-based ASR system was conducted in <ref type=\"bibr\" target=\"#b14\">[15]</ref>. They used four European languages to build a multilingual. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: are written in managed languages such as Java, garbage collection is a major consumer of CPU cycles <ref type=\"bibr\" target=\"#b54\">[55]</ref>.</p><p>Trie. We distinguish trie from tree because they ha rators for operations like Malloc <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and garbage collection <ref type=\"bibr\" target=\"#b54\">[55]</ref>, even if the end-to-end performance improvement is not ama ef type=\"bibr\" target=\"#b38\">[39]</ref> and previous works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>, as summarized in Fig. <ref type=\"figure\" target=\"#fig_1\">1 get=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eters, and (3) knowledge distillation (KD).</p><p>First, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref> proposed the binary encoding of model parameters. Under th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sion reduction occurs in the recurrent convolutional neural networks used for semantic segmentation <ref type=\"bibr\" target=\"#b21\">[22]</ref>. As SR methods predict full-sized images, dimension reduct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b36\">36,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. Although considerable progress has been achieved in image  get=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b30\">30,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. Due to space limitation, we here briefly review works rela arget=\"#b13\">13,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. For example, Dong et al. <ref type=\"bibr\" target=\"#b2\">[ 2 ing discriminative ability of the network. Channel attention <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> has been shown to be effective for better discriminative re rget=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref  >[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>, we also adopt self-ensemble method to further improve our  f the network, some other networks, such as NLRN <ref type=\"bibr\" target=\"#b22\">[22]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>, improve the performance by considering feature correlatio classification.</p><p>Recently, SENet was introduced to deep CNNs to further improve SR performance <ref type=\"bibr\" target=\"#b38\">[38]</ref>. However, SENet only explores first-order statistics (e.g. se feature interdependencies. Difference to Residual Channel Attention Network (RCAN). Zhang et al. <ref type=\"bibr\" target=\"#b38\">[38]</ref> proposed a residual in residual structure to form a very d sidual blocks in each LSRAG, thus resulting in deep network with over 400 convolution layers. As in <ref type=\"bibr\" target=\"#b38\">[38]</ref>, we also add long and short skip connections in Base model BPN <ref type=\"bibr\" target=\"#b6\">[6]</ref>, RDN <ref type=\"bibr\" target=\"#b39\">[39]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=  <ref type=\"bibr\" target=\"#b36\">[36]</ref>, RDN <ref type=\"bibr\" target=\"#b39\">[39]</ref>, and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. All the results on 3\u00d7 are shown in Table <ref type=\"table. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b8\">[9]</ref>. Thanks to these advances, voice assistant devices such as Google Home <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> , Amazon Alexa or Sam ype=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35</ref>]. An \"acoustic simulator\" <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> is used to generate  /ref>. The acoustic simulator in Fig. <ref type=\"figure\">1</ref> is similar to what we described in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. One difference comp ract LengthPerturbation (VTLP)<ref type=\"bibr\" target=\"#b0\">[1]</ref> , and Acoustics Simulator (AS)<ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, respectively.</figD \">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. One difference compared to our previous one in <ref type=\"bibr\" target=\"#b1\">[2]</ref> is that we do not pre-calculate room impulse responses, but  rver, we ran the VTLP data augmentation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, acoustic simulator <ref type=\"bibr\" target=\"#b1\">[2]</ref> and feature extraction modules shown in Fig. <ref type=\"figu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: igure\" target=\"#fig_0\">1</ref>). For the training data scarcity problem, we introduce meta learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta used as a data augmentation strategy to deal with the lack of training data. However, meta-learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta number of model parameters. To deal with the data scarcity problem, we propose to use meta learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: age model pretraining has had a tremendous impact on solving many natural language processing tasks <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Radford, 2018  the parameters of the language model are frozen and a task-specific head is trained on top of them <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2018)</ref>. The second approach fine-tunes all model   <ref type=\"bibr\" target=\"#b8\">(Radford, 2018)</ref>. The latter can sometimes yield better results <ref type=\"bibr\" target=\"#b6\">(Peters et al., 2019)</ref>, while the first one usually offers better. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  image as points in it. Pixels representing the same object naturally cluster in the spectral space <ref type=\"bibr\" target=\"#b3\">[4]</ref> . This property provides us an opportunity to segment pixels. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nd of limited interpretability.</p><p>Regarding DCF approaches, thanks to the seminal work of MOSSE <ref type=\"bibr\" target=\"#b35\">[36]</ref>, DCF has received much attention in visual object tracking. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -level features, and have emerged as a dominant paradigm in pattern recognition and computer vision <ref type=\"bibr\" target=\"#b4\">(Chen and Liu, 2018)</ref>. In deep learning technology, convolutional. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: asets</head><p>The effectiveness of MACU-Net is verified using Wuhan Dense Labeling Dataset (WHDLD) <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and Gaofen Image D. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmance, most of them ignore the interactions' timestamp values. While recent works such as TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> successfully incorporated time information, their usage of ransformer <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Cloze-task based training method. TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> enhanced SASRec by merging timestamp information into self st, they don't utilize timestamp values which hold important contextual information. While TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref> successfully addressed this issue, they also used a simple pe=\"bibr\" target=\"#b31\">[32]</ref>,</p><p>SASRec <ref type=\"bibr\" target=\"#b9\">[10]</ref>, TiSASRec <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and BERT4Rec <ref type=\"bibr\" target=\"#b18\">[19]</ref>. T essing procedure from <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. We convert each da g the custom practice <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, we discard users a We use the rest for training. We follow the common practice <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> of letting the mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: zing GCN, S-GCN, ChebNet and related methods. Our architecture is analogous to the inception module <ref type=\"bibr\" target=\"#b43\">Szegedy et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b22\">Kazi et a ion ( <ref type=\"formula\" target=\"#formula_6\">4</ref>) is analogous to the popular Inception module <ref type=\"bibr\" target=\"#b43\">Szegedy et al. (2015)</ref> for classic CNN architectures (Figure <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and it's nontrivial to change the system to train in an end-to-end fashion.</p><p>To our knowledge, <ref type=\"bibr\" target=\"#b22\">Wang et al. (2016)</ref> is the earliest work touching end-to-end TTS. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce and is not structured for human communication (e.g., unlike words).</p><p>Several recent studies <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" ta  be used with various pretext tasks. In this paper, we follow a simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" ta 8\">[29]</ref>. Contrastive learning is at the core of several recent works on unsupervised learning <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" ta </ref>. Overall, all three mechanisms benefit from a larger K. A similar trend has been observed in <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> under the memory b t tasks can be based on some form of contrastive loss functions. The instance discrimination method <ref type=\"bibr\" target=\"#b60\">[61]</ref> is related to the exemplar-based task <ref type=\"bibr\" tar +/\u03c4 ) K i=0 exp(q\u2022ki/\u03c4 )<label>(1)</label></formula><p>where \u03c4 is a temperature hyper-parameter per <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The sum is over one positive and K negative samples. Intu these networks to downstream tasks.</p><p>Another mechanism is the memory bank approach proposed by <ref type=\"bibr\" target=\"#b60\">[61]</ref> (Figure <ref type=\"figure\">2b</ref>). A memory bank consis ver the past epoch and thus are less consistent. A momentum update is adopted on the memory bank in <ref type=\"bibr\" target=\"#b60\">[61]</ref>. Its momentum update is on the representations of the same igning a new pretext task, we use a simple one mainly following the instance discrimination task in <ref type=\"bibr\" target=\"#b60\">[61]</ref>, to which some recent works <ref type=\"bibr\" target=\"#b62\" =\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> are related.</p><p>Following <ref type=\"bibr\" target=\"#b60\">[61]</ref>, we consider a query and a key as a positive pair if they  ose last fully-connected layer (after global average pooling) has a fixed-dimensional output (128-D <ref type=\"bibr\" target=\"#b60\">[61]</ref>). This output vector is normalized by its L2-norm <ref typ  (128-D <ref type=\"bibr\" target=\"#b60\">[61]</ref>). This output vector is normalized by its L2-norm <ref type=\"bibr\" target=\"#b60\">[61]</ref>. This is the representation of the query or key. The tempe  or key. The temperature \u03c4 in Eqn.( <ref type=\"formula\" target=\"#formula_0\">1</ref>) is set as 0.07 <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The data augmentation setting follows <ref type=\"bibr\" ta f>) is set as 0.07 <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The data augmentation setting follows <ref type=\"bibr\" target=\"#b60\">[61]</ref>: a 224\u00d7224-pixel crop is taken from a randomly resized ima ate of 0.03. We train for 200 epochs with the learning rate multiplied by 0.1 at 120 and 160 epochs <ref type=\"bibr\" target=\"#b60\">[61]</ref>, taking \u223c53 hours training ResNet-50. For IG-1B, we use a  ecause the positive key is in the same mini-batch). The network is ResNet-50.</p><p>The memory bank <ref type=\"bibr\" target=\"#b60\">[61]</ref> mechanism can support a larger dictionary size. But it is   We hope an advanced pretext task will improve this. Beyond the simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61]</ref>, it is possible to adopt MoCo for pretext tasks like maske 1\">Here 58.0% is with InfoNCE and K=65536. We reproduce 54.3% when using NCE and K=4096 (the same as<ref type=\"bibr\" target=\"#b60\">[61]</ref>), close to 54.0% in<ref type=\"bibr\" target=\"#b60\">[61]</re  when using NCE and K=4096 (the same as<ref type=\"bibr\" target=\"#b60\">[61]</ref>), close to 54.0% in<ref type=\"bibr\" target=\"#b60\">[61]</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place sed on other forms <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, such as margin-bas specific pretext task. The input x q and x k can be images <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, patches <ref type= backbone, by default used in existing ResNet-based results <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, dependency-based cores <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, dataflow inspired operands of consumer instructions depending on the same register with a linked list of SSR pointers <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> (dashed arrows in  zed wakeup issue of SSR and boosting performance by composing multiple execution units (scaling-up) <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta w></table></figure> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\"><ref type=\"bibr\" target=\"#b11\">[12]</ref> compares Forwardflow against a baseline OoO core with 32-e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: his paper, we follow a simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>: a query matches a ke et=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targe  x k can be images <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, patches <ref type=\"bibr\" target=\"#b45\">[46]</ref>, or cont k can be identical <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, partially shared <ref type=\"bibr\" target=\"#b45\">[46,</ref> stance discrimination task in <ref type=\"bibr\" target=\"#b60\">[61]</ref>, to which some recent works <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> are related.</p><p>F tive pair if they originate from the same image, and otherwise as a negative sample pair. Following <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, we take two random . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: resolution networks. Previous works including EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, BTSRN <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RDN <ref type=\"bibr\" target=\"#b41\">[42]</ref> found that br\" target=\"#b30\">31]</ref>. It is also experimentally proved in single image super-resolution task <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ \"#b11\">[12]</ref> hinders the accuracy of image super-resolution. Thus, in recent image SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ small image patches (e.g. 48 \u00d7 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ ny kinds of regularizers, for examples, weight decaying and dropout, are not adopted in SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ a is augmented with random horizontal flips and rotations following common data augmentation methods<ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. During training, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns for diagnosing chest diseases. In the US, more than 35 million chest X-rays are taken every year <ref type=\"bibr\" target=\"#b19\">[20]</ref>. It is primarily used to screen diseases such as lung canc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead n=\"3.1\">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation <ref type=\"bibr\" target=\"#b49\">(Sennrich et al., 2016)</ref> in NMT, referring to training a backwar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t matching models have been proposed and gain some improvement, such as representation based models <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta  <ref type=\"bibr\" target=\"#b11\">[12]</ref>, C-DSSM <ref type=\"bibr\" target=\"#b28\">[29]</ref>, ARC-I <ref type=\"bibr\" target=\"#b10\">[11]</ref>, RNN-LSTM <ref type=\"bibr\" target=\"#b20\">[21]</ref> and MV o input text, rather than focusing on the text representations. The pioneering work includes ARC-II <ref type=\"bibr\" target=\"#b10\">[11]</ref>, MatchPyramid <ref type=\"bibr\" target=\"#b22\">[23]</ref>, a f type=\"bibr\" target=\"#b11\">[12]</ref>, C-DSSM <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and ARC-I <ref type=\"bibr\" target=\"#b10\">[11]</ref>, four interaction-based approaches, i.e. ARC-II <ref type= and ARC-I <ref type=\"bibr\" target=\"#b10\">[11]</ref>, four interaction-based approaches, i.e. ARC-II <ref type=\"bibr\" target=\"#b10\">[11]</ref>, MatchPyramid <ref type=\"bibr\" target=\"#b22\">[23]</ref>, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are not available <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t #b4\">[5]</ref>. The system first predicts 68 face landmarks from speech using an LSTM-based network <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and then predicts a few talking face images from the condit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ge R-CNN framework <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, where detection is et=\"#b12\">[13]</ref> introduced the idea of region-wise feature extraction. Later, the Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[30]</ref> achieved further speeds-up by introducing a Region Proposa  network. SSD <ref type=\"bibr\" target=\"#b24\">[25]</ref> detects objects in a way similar to the RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, but uses multiple feature maps at different resolutions t We focus on modeling a multistage detection sub-network, and adopt, but are not limited to, the RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref> for proposal detection.</p></div> <div xmlns=\"http://www.t evels. At inference, since the majority of the hypotheses produced by a proposal detector, e.g. RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref> or selective search <ref type=\"bibr\" target=\"#b32\">[33]</r \">Object Detection</head><p>In this paper, we extend the two-stage architecture of the Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, shown in Figure < zed by its mean and variance, i.e. is replaced by \u2032 =( \u2212 )/ . This is widely used in the literature <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ PN+.</p><p>Detection Performance: Again, our implementations are better than the original detectors <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ  was further experimented on PAS-CAL VOC dataset <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, the models were t e noted. The sampling of the first detection stage follows <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. In the following stages, resampling is implemented by simp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: refetcher, offset prefetchers, and the sandbox method for selecting the prefetch offset dynamically <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Offset prefetching is a generalization of next-line prefe </ref> (this list is not exhaustive).</p><p>Recently, Pugsley et al. introduced Sandbox prefetching <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The Sandbox prefetcher prefetches line X + D when line X  dge, the first published full-fledged offset prefetcher is the Sandbox prefetcher by Pugsley et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref>. However, the offset selection mechanism in the Sandbox pr owledge, the SBP prefetcher of Pugsley et al. is the first published full-fledged offset prefetcher <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The SBP prefetcher is cost-effective and was shown to out  with actual prefetches.</p><p>We implemented the SBP prefetcher as described in the original paper <ref type=\"bibr\" target=\"#b25\">[26]</ref>, but with a few modifications to make the comparison with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he summation operation may impede the information flow in deep networks.</p><p>Inspired by Densenet <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>, we propose a densely-connected recurrent n et al. 2016;</ref><ref type=\"bibr\" target=\"#b31\">Wu et al. 2016</ref>). More recently, Huang et al. <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref> enable the features to be connected from lo , the summation operation in the residual connection may impede the information flow in the network <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>. Motivated by Densenet <ref type=\"bibr\" tar flow in the network <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>. Motivated by Densenet <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>, we employ direct connections using the con ch to the uppermost layer and all the previous features work for prediction as collective knowledge <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org  max-valued features of every layer affect the loss function and perform a kind of deep supervision <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017</ref>). Thus, we could cautiously interpret the cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[25]</ref> achieved impressive performance on several image recognition tasks, including ImageNet <ref type=\"bibr\" target=\"#b21\">[22]</ref>, using a pre-trained Transformer with minimal architecture ion, and zero-shot text-to-video retrieval. Fine-tuning the vision-modality Transformer on ImageNet <ref type=\"bibr\" target=\"#b21\">[22]</ref> obtains the top-1 accuracy of 78.7%, which is comparable t we evaluate the transferability of the vision backbone by fine-tuning it on ImageNet classification <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Since HMDB51, UCF101, and ESC50 are very small datasets c nsformer in the image domain. We finetune the last checkpoint of the vision Transformer on ImageNet <ref type=\"bibr\" target=\"#b21\">[22]</ref> with no modification to our architecture or the tokenizati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: experiments, so we use a single template. The inputs and outputs are modeled in the standard format <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>.</p><p>We fine-tune pretrained models to  place=\"foot\" n=\"2\" xml:id=\"foot_2\">Since the original pretraining corpus is not available, we follow<ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> and recreate the dataset by crawling http:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> or designing specific CNN-RNN networks <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Such classifier that takes video sequences as input is beco erm Memory (LSTM) recurrent neural network is widely adopted <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. LSTM has memory abilit ion recognition using CNN or RNN structures in recent papers <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Such deep networks r 18\">19]</ref>. Such deep networks reach top competitive results in the history of EmotiW challenges <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Therefore we take s ork shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: iction by reinterpretation of fully connected layers of the classifier as a fully convolution layer <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The FCN consists of an encoder of input images and a deco  overcome this problem, skip connection methods are used <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The features extracted from PPL are upsampled and concate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  al <ref type=\"bibr\" target=\"#b35\">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref type=\"bibr\" target=\"#b15\">[16]</ref>. As described in Algorithm 1, for the r -th document \u03c0 r ,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e minded users. Later, an analogous item-oriented approach <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> became popular. In those methods, a rating is estimated usi  make the itemoriented approach more favorable in many cases <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. In addition, item-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he field of news recommendation <ref type=\"bibr\" target=\"#b15\">[16]</ref> and attachment suggestion <ref type=\"bibr\" target=\"#b13\">[14]</ref>. This is mainly because long-form text matching is quite d not been fully explored. In recent years, thanks to the pioneer work SMASH proposed by Jiang et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref>, they are the first to point out that long-form text match. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. Graph Convolutional Network (GCN) <ref type=\"bibr\" target=\"#b14\">[15]</ref> adopts a message-passing approach and gathers information  vertices and edges, respectively. For ease of presentation, we assume that G is a self-looped graph <ref type=\"bibr\" target=\"#b14\">[15]</ref>, with a self-loop attached to each node in V . Let n = |V   and d(u) = |N (u)| is the degree of u. We use d = m n to denote the average degree of G. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we define the normalized adjacency matrix of G as \u00c3 = D \u2212 ork Friendster.</p><p>Baselines and detailed setup. We adopt three state-of-the-art GNN methods GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GAT <ref type=\"bibr\" target=\"#b28\">[29]</ref>, GDC <ref t e node classification task on the three small standard graphs Cora, Citeseer, and Pubmed. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we apply the standard fixed training/validation/testing s 0 and 1, the convolution matrix D r\u22121 AD \u2212r represents the symmetric normalization adjacency matrix <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y information for character-based model. To integrate words information into character-based model, <ref type=\"bibr\" target=\"#b40\">Zhang and Yang (2018)</ref> propose a lattice-structured LSTM model t characterbased model. The character baseline denotes the original character-based BiLSTM-CRF model. <ref type=\"bibr\" target=\"#b40\">Zhang and Yang (2018)</ref> propose a lattice LSTM to exploit word in y information into Chinese NER task. Another way to obtain word boundary information is proposed by <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, using a lattice LSTM to integrate word   where b &lt; i and c b,i matches a word in lexicon D. The lexicon D is the same as the one used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, which is built by using automatically s x c i with x \u2212 \u2192 ws i to utilize word information. And this is quite different from the way used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, since they use extra shortcut paths to  j , y j )}| N j=1 , we minimize the sentence-level negative loglikelihood loss to train the model:  <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>.</p><formula xml:id=\"formula_16\">L = \u2212 j rget=\"#tab_0\">1</ref>. Implementation Details. We utilize the character and word embeddings used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, both of which are pre-trained on Chines ng, 2018)</ref>, both of which are pre-trained on Chinese Giga-Word using word2vec model. Following <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, we use the word embedding dictionary as with other parameters.</p><p>For hyper-parameter configurations, we mostly refer to the settings in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>. We set both character embedding size an > 91.28 90.62 90.95 <ref type=\"bibr\" target=\"#b0\">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>   approach to integrating word informati n Chinese Resume dataset. Consistent with the previous results, our models outperform lattice model <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>. The above experimental results strongly  some comparative experiments on training time and convergence speed. The lattice model proposed in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref> is our principal comparison object, sinc ns/1.0\" type=\"table\" xml:id=\"tab_5\"><head></head><label></label><figDesc>are the most common methods<ref type=\"bibr\" target=\"#b40\">Zhang and Yang, 2018)</ref> 94.81 94.11 94.46 Character baseline 93.2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rent manifolds. Previous studies on cross-network learning <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> mostly focus on transferring the knowledge only from a sing s on transferring the knowledge from only a single network <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, which may cause negative transfer due to the divergent cha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> considering edge attributes <ref type=\"bibr\" target=\"#b14\"> target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (amino acids). Specifically, we augment the autoregressive self-attention of recent sequence models <ref type=\"bibr\" target=\"#b6\">[7]</ref> with graph-based descriptions of the 3D structure. By compos dependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention <ref type=\"bibr\" target=\"#b6\">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d  Structured Transformer model that draws inspiration from the selfattention based Transformer model <ref type=\"bibr\" target=\"#b6\">[7]</ref> and is augmented for scalable incorporation of relational in an attend to a separate subspace of the embeddings via learned query, key and value transformations <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>The queries are derived from the current embedding at een these self-attention layers and position-wise feedforward layers as in the original Transformer <ref type=\"bibr\" target=\"#b6\">[7]</ref>. We stack multiple layers atop each other, and thereby obtai rained models using the learning rate schedule and initialization of the original Transformer paper <ref type=\"bibr\" target=\"#b6\">[7]</ref>, a dropout rate of 10% <ref type=\"bibr\" target=\"#b41\">[42]</ f their structure. Our model augments the traditional sequence-level self-attention of Transformers <ref type=\"bibr\" target=\"#b6\">[7]</ref> with relational 3D structural encodings and is able to lever ndependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention<ref type=\"bibr\" target=\"#b6\">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mining algorithms or evaluate clusters using data analytics workloads in different aspects, such as <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta pe=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and etc. Narayanan et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> characterize traditional data analytics workloads on singl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  ( <ref type=\"formula\">2018</ref>) train deep variational autoencoders on MSAs to predict function. <ref type=\"bibr\" target=\"#b33\">Riesselman et al. (2019)</ref> train autoregressive models on MSAs, b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ave been considered for HMMbased recognisers as a way of dealing with out of vocabulary (OOV) words <ref type=\"bibr\" target=\"#b6\">(Galescu, 2003;</ref><ref type=\"bibr\" target=\"#b1\">Bisani &amp; Ney, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b21\">[22]</ref>) and region-based convolutional neural networks (R-CNNs) <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Although region-based CNNs were computationally expensive a b5\">[6]</ref>. Although region-based CNNs were computationally expensive as originally developed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, their cost has been drastically reduced thanks to sharing c rk whose last fc layer simultaneously predicts multiple (e.g., 800) boxes, which are used for R-CNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> object detection. Their proposal network is applied on a sin rget=\"#foot_3\">3</ref>For regression, we adopt the parameterizations of the 4 coordinates following <ref type=\"bibr\" target=\"#b5\">[6]</ref>:</p><formula xml:id=\"formula_2\">t x = (x \u2212 x a )/w a , t y = odel for ImageNet classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, as is standard practice <ref type=\"bibr\" target=\"#b5\">[6]</ref>. We tune all layers of the ZF net, and conv3 1 and up for th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: as the training progresses <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Madry, Makelov, Schmidt, et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> used adversarial training on the cifar dataset, which stil gest known attack for this metric. PGD has been conjectured to be a near-optimal first-order attack <ref type=\"bibr\" target=\"#b24\">[25]</ref>. We use the Fo olBox library for the implementation of the we also wish to address ac o n c e r nr a i s e db yM a d r y ,M a k e l o v ,S c h m i d t ,et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which is the computational cost of a threat model. They a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , and several branches, regardless of them being taken or not taken.</p><p>The next trace predictor <ref type=\"bibr\" target=\"#b16\">[17]</ref> provides trace level sequencing. That is, the fetch engine. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b50\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Misra &amp; Maaten, 2020;</ref><ref type=\"bibr\" target=\"#b15\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a text word by word and stores the semantics of all the previous text in a fixed-sized hidden layer <ref type=\"bibr\" target=\"#b7\">(Elman 1990</ref>). The advantage of RecurrentNN is the ability to bet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: h establish new state-of-the-art on many domain-related benchmarks such as named entity recognition <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, topic classificati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he teacher and that of the student. To this end, RRD adopts the list-wise learning-to-rank approach <ref type=\"bibr\" target=\"#b28\">[29]</ref> and learns to ensure the student to preserve the ranking o hat of the student model. To this end, RRD adopts the classical list-wise learning-to-rank approach <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Its core idea is to define a probability of a permutation d of the ground-truth ranking order. For more details about the list-wise approach, please refer to <ref type=\"bibr\" target=\"#b28\">[29]</ref>.</p><p>However, merely adopting the list-wise loss can hav p>Relaxed permutation probability. Then, RRD defines a relaxed permutation probability motivated by <ref type=\"bibr\" target=\"#b28\">[29]</ref>. For user \ud835\udc62, \ud835\udf45 \ud835\udc96 denotes a ranked list of all the sampled . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mpts have been made to integrate multimodal contents into graph-based recommendation systems. MMGCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> constructs modality-specific user-item interaction graphs  tures as the content information to predict the interactions between users and items.</p><p>\u2022 MMGCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> is one of the state-of-the-art multimodal recommendation m \">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. MMGCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> constructed modal-specific graph and refine modal-specific mmendation systems <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. MMGCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> constructe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: end learning framework of a single encoder. Hence, we instead adopt the studentteacher-like network <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b29\">29]</ref> in which only the st get encoder. At the same time, the target encoder is updated based on momentum-based moving average <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target o bootstrap the representations by providing enhanced but consistent targets to the online encoders <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>. Figure <ref type=\"fig d (i.e., \ud835\udf0f = 1). This observation is consistent with previous work on momentum-based moving average <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target pproximate the online encoder, we investigate \ud835\udf0f in the range of [0.9, 1.0], as done in previous work<ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>.</note> \t\t</body> \t\t<b sed a bootstrapping-based self-supervised learning framework <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b6\">6]</ref>, which is capable of avoiding the collapsed solution without  l recent work on bootstrapping-based representation learning <ref type=\"bibr\" target=\"#b3\">[3,</ref><ref type=\"bibr\" target=\"#b6\">6]</ref> empirically demonstrated that this kind of dynamics (i.e., up paper, the term \"bootstrapping\" is not used in the statistical meaning, but in the idiomatic meaning<ref type=\"bibr\" target=\"#b6\">[6]</ref>. Strictly speaking, it refers to using estimated values (i.e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ]</ref>. Consequently, several defense methods were proposed, such as Multi-Party Computation (MPC) <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, Homomorphic Encry. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: asy yet accurate way of taking into account batches in the packet processing in two existing models <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref>. Note that i ef type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref>. Note that in <ref type=\"bibr\" target=\"#b11\">[11]</ref> we came up with a first modeling framework that does not a ODELS WITHOUT BATCH SERVICE</head><p>We start this section by reviewing two of our previous models, <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref> that work wi \"http://www.tei-c.org/ns/1.0\"><head>B. Model for a negligible switch-over time: Model 1</head><p>In <ref type=\"bibr\" target=\"#b11\">[11]</ref> it is assumed that the switch-over time is negligible, whi te with each queue i of the decomposed model, the continuous-time Markov chain depicted in Figure 5 <ref type=\"bibr\" target=\"#b11\">[11]</ref>. A state (k, P ) of this chain, k = 1..., K, corresponds t lution of all Markov chains, the model relies on a fixed-point iterative technique, as developed in <ref type=\"bibr\" target=\"#b11\">[11]</ref>.</p><p>3) Performance parameters: Once all Markov chains ( d><p>The model presented in <ref type=\"bibr\" target=\"#b12\">[12]</ref> is an extension of the one in <ref type=\"bibr\" target=\"#b11\">[11]</ref> that takes into account the switch-over times, and thus co 1 in the case without batches (M = 1), i.e., T M + T R . Let us consider an example (extracted from <ref type=\"bibr\" target=\"#b11\">[11]</ref>) of a virtual switch comprising N = 4 input ports served b ng to standard operational modes. This is exactly what we propose to do in this paper: using models <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref> (presented i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or each kernel, we computed the normalized Gram matrix. We used the C-SVM im-plementation of LIBSVM <ref type=\"bibr\" target=\"#b5\">(Chang and Lin 2011)</ref> to compute the classification accuracies us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ification of the attention weights of BERT alongside BSR sparsity optimizations in the TVM compiler <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2018]</ref>. We show how algorithms and compiler optimiz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: memory-hierarchy parallelism (MHP). <ref type=\"foot\" target=\"#foot_0\">1</ref> Load Slice Core (LSC) <ref type=\"bibr\" target=\"#b4\">[5]</ref> was the first work to propose an sOoO core; Freeway <ref typ TIVATION</head><p>In this section, we briefly cover the background on the two prior sOoO cores -LSC <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> -and we FSC.</p><p>Restricted Out-of-Order Microarchitectures. We extensively discussed the Load Slice Core <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Freeway <ref type=\"bibr\" target=\"#b9\">[10]</ref> through. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing as a complement to the rule-based method to reduce invalid recommendations due to missing rules <ref type=\"bibr\" target=\"#b43\">[44]</ref>. Additionally, the collaborative filtering method can also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b22\">[23]</ref>, and frequent subgraph mining (FSM) <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. ASAP <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a distr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b19\">Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b7\">Chiu and Nichols, 2016;</ref><ref type=\"bibr\" target=\"#b22\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b46\">Yang et al., model we use to perform NER. We will first describe the basic hierarchical neural CRF tagging model <ref type=\"bibr\" target=\"#b22\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy  labels and performs inference.</p><p>In this paper, we closely follow the architecture proposed by <ref type=\"bibr\" target=\"#b22\">Lample et al. (2016)</ref>, and use bidirectional LSTMs for both the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the semantic difference. Mean pooling and Vector of Locally Aggregated Descriptors (VLAD) encoding <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref> fail to take th nce.</p><p>Second, multi-scale features from convolutional layers are not well exploited. Xu et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> found that fusing the features of different layers can boo  layers.</p><p>Some works have been conducted on encoding deep features to a global representation. <ref type=\"bibr\" target=\"#b11\">[12]</ref> showed that VLAD encoded ConvNet activations can significa d Attention</head><p>The classical VLAD encoding method has been found useful for video aggregation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Given a set of date that 2 normalization on global video representation will lead to better classification results <ref type=\"bibr\" target=\"#b11\">[12]</ref> and apply it to our models directly.</p><p>Compared to ave  classical VLAD encoding. To train the unsupervised VLAD encoding, we mainly follow the settings in <ref type=\"bibr\" target=\"#b11\">[12]</ref>. We first apply PCA with whitening to the convolutional ac on, intra-normalization and 2 normalization to the encoded vector, as they show good performance in <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>We first use 10 centers for the unsupervised VLAD e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: etworks (GNN) <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref> as basic modules to learn graph representation.</p><p>In d eatures. In order to generate coarsened graphs to represent graph substructures, following DIFFPOOL <ref type=\"bibr\" target=\"#b46\">[47]</ref>,</p><p>we learn an assignment matrix B l k+1 via another G %, GNN can automatically learn the node proximity and the appropriate number of meaningful clusters <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>Parameters in the graph filter. We analyze how the  ning, such as <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>. In order to generate graph representations, most works em node embeddings first, and then use some pooling or READOUT functions, such as hierarchical pooling <ref type=\"bibr\" target=\"#b46\">[47]</ref> and sum operations <ref type=\"bibr\" target=\"#b35\">[36]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing as a complement to the rule-based method to reduce invalid recommendations due to missing rules <ref type=\"bibr\" target=\"#b43\">[44]</ref>. Additionally, the collaborative filtering method can also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b29\">[31]</ref><ref type=\"bibr\" target=\"#b30\">[32]</ref><ref type=\"bibr\" target=\"#b31\">[33]</ref><ref type=\"bibr\" target=\"#b32\">[34]</ref>, mostly because 1) it has strong security implications; 2). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are highly related, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=\"bibr\" target=\"#b1\">(Caruana 1997</ref>) by sharing some meta-knowledge.</p><p>We propose   from existing work, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=\"bibr\" target=\"#b1\">(Caruana 1997</ref>) by sharing some metaknowledge. Intuitively, answe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b17\">[18]</ref> proposed an extension architecture of GCNs named R-GCNs. Gong et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> presented a framework that augments GCNs and GATs with edges edges are labeled, which indicates that the edges cannot include continuous attributes. Gong et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> presented a framework enhances GCNs <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dustry practitioners have turned to search-based compilation <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b27\">29,</ref><ref type=\"bibr\" targ nually-written assembly code on large matrix multiplications <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b43\">45]</ref>, as the code has bee tation definition of matrix multiplication. ple compiler techniques have been introduced (e.g., TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, Halide <ref type=\"bibr\" target=\"#b36\">[38]</ref>, Tensor C ically, the compiler partitions the large computational graph of a DNN into several small subgraphs <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This partition has a negligible effect on the performance  arch and learned cost model performs the best among them, which is also used in our evaluation. TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref> utilizes a similar scheduling language and includes a templ  search for GPU code automatically, but it is not yet meant to be used for compute-bounded problems <ref type=\"bibr\" target=\"#b9\">[10]</ref>. It cannot outperform TVM on operators like conv2d and matm f type=\"bibr\" target=\"#b9\">[10]</ref>. It cannot outperform TVM on operators like conv2d and matmul <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>. This is because of graph level include layout optimizations <ref type=\"bibr\" target=\"#b27\">[29]</ref>, operator fusion <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b33\">35]</ref>, constant folding <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ent label prediction <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Adversarial examples have been shown to be ubiquitous beyo \" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>. Among them, adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> is one of the most side in a large, contiguous region and a significant portion of the adversarial subspaces is shared <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta e sense of robustness against adversarial attacks due to gradient masking, and adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta se method against adversarial attacks. It improves model robustness by solving a minimax problem as <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>:</p><formula xml:i he proposed formulation deviates from the conventional minimax formulation for adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. More specifically  \u2261 i L \u03b8 (x i , y i ), the proposed approach reduces to the conventional adversarial training setup <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. The overall proce gn method (FGSM) for adversarial attack generation is developed and used in adversarial training in <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Many variants of attacks have been developed later <ref t  inner maximization can be solved approximately, using for example a one-step approach such as FGSM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, or a multi-step projected gradient descent (PGD) method < y measuring the accuracy of the model under different adversarial attacks, including white-box FGSM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, PGD <ref type=\"bibr\" target=\"#b35\">[36]</ref>, CW <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  balance between effectiveness and efficiency. In this paper, we employ knowledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> which is a network compression technique by transferring th ulate the top-N recommendation problem. Then, we explain the concept of knowledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> and present rank distillation (RD) <ref type=\"bibr\" target= r proposed training strategies.</p><p>Temperature in the KD loss. One key factor of the original KD <ref type=\"bibr\" target=\"#b9\">[10]</ref> is to find a proper balance between the soft targets and ha ]</ref> is to find a proper balance between the soft targets and hard labels. To tackle this issue, <ref type=\"bibr\" target=\"#b9\">[10]</ref> introduces the notion of a temperature T . Although the sof. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sks; we report mean accuracy and standard deviation for three training runs. a 12-layer Transformer <ref type=\"bibr\" target=\"#b40\">(Vaswani et al., 2017)</ref>. Both <ref type=\"bibr\" target=\"#b45\">Xie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ics of the datasets are shown in Table <ref type=\"table\" target=\"#tab_1\">1</ref>. As previous works <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b32\">[33]</ref> bypass signal from one layer to the next via identity connections. Stochastic depth <ref type=\"bibr\" target=\"#b12\">[13]</ref> shortens ResNets by randomly dropping layers during traini ation preservation explicit through additive identity transformations. Recent variations of ResNets <ref type=\"bibr\" target=\"#b12\">[13]</ref> show that many layers contribute very little and can in fa ]</ref>. Recently, stochastic depth was proposed as a way to successfully train a 1202-layer ResNet <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Stochastic depth improves the training of deep residual n tion between dense convolutional net-works and stochastic depth regularization of residual networks <ref type=\"bibr\" target=\"#b12\">[13]</ref>. In stochastic depth, layers in residual networks are rand oring/shifting) that is widely used for these two datasets <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar 31 images for additional training. Following common practice <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar st time. Following <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, we report classification errors on the validation set.</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s reported that these numerical errors brought about huge reputation risk, and even economic losses <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Since the documents disclosed by the firm usually have the  significance testing, presented in the academic papers in major psychology journals. A recent study <ref type=\"bibr\" target=\"#b0\">[1]</ref> published a system called AutoDoc, and introduced the module uch more numerical facts in tables than textual paragraphs. Therefore, as an important extension to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we propose Automatic Numerical Cross-Checking over Tables ( ncial, and politic fields. It has attracted a lot of research interests in recent years. Cao et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> propose a system to cross-check numerical facts by extractin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: micro-video. In terms of acoustic modality, we separate audio tracks with FFmpeg 6 and adopt VGGish <ref type=\"bibr\" target=\"#b19\">[20]</ref> to learn the acoustic deep learning features. For textual . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"1\">Introduction</head><p>Bidirectional Encoder Representations from Transformers (BERT) <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> has become enormously popular and proven t RT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>.</p><p>\u2022 As there are so many possibilitie  settings and data statistics in different task. \u2020 represents the dataset was also evaluated by BERT<ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>. \u2021 represents the dataset was also evaluat , which is beneficial for the researcher to design more powerful models based on them.</p><p>Before <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> releasing BERT with whole word masking, <re <ref type=\"foot\" target=\"#foot_1\">3</ref> , and pre-processed with WikiExtractor.py as suggested by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>  <ref type=\"bibr\" target=\"#b8\">Devlin et al sed with WikiExtractor.py as suggested by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>  <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, for computation efficiency and learning lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: irs is through the use of independently sampled dropout masks. In standard training of Transformers <ref type=\"bibr\" target=\"#b46\">(Vaswani et al., 2017)</ref>, there is a dropout mask placed on fully. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b75\">76,</ref><ref type=\"bibr\" target=\"#b77\">78]</ref>, most mimic squeeze-and-excitate <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he dynamics on how oversmoothing is triggered, or which nodes tend to cause it. To date, works like <ref type=\"bibr\" target=\"#b28\">(Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b26\">Wang et al., 2 residual connections and dilated con-volutions <ref type=\"bibr\">(Li et al., 2019)</ref>; skip links <ref type=\"bibr\" target=\"#b28\">(Xu et al., 2018b)</ref>; new normalization strategies <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  ActivityNet1.3 dataset as it does on the THUMOS2014 dataset compared with several existing methods <ref type=\"bibr\" target=\"#b58\">[58]</ref>, <ref type=\"bibr\" target=\"#b59\">[59]</ref>, probably due t  that the boundaries of long action instances are captured. Our method performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58] ethod performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> conducts frame-level predictions rather than segment-level 1.3 dataset. Nevertheless, our method yields a higher mAP at a threshold of 0.95 than the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref>, which indicates that our method locates the action bounda e difficult scenarios. Furthermore, although the average mAP of our method is 4% worse than that of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the ActivityNet1.3 dataset, our method achieves a signi ef> on the ActivityNet1.3 dataset, our method achieves a significant improvement over the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the THU-MOS2014 dataset, and the mAP at the threshold o 62\">[62]</ref> and two framelevel proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> for comparison, and the inference speed is directly copied od is slower than the frame-level proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> that perform fully convolutional operations on the frame l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cal applications pertaining to fairness, privacy, and safety <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. For example, we can train a GNN model to predict the effec  on interpreting GNNs at the model-level. The existing study <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> only provides example-level explanations for graph models.  tudies focusing on the interpretability of deep graph models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type have no baseline to compare with. Note that existing studies <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> only focus on interpreting GNNs at example-level while igno [4,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. The recent GNN interpretation tool GNN Explainer <ref type=\"bibr\" target=\"#b39\">[40]</ref> proposes to explain deep graph models at the example-level. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cently emerging scalable algorithms, such as SGC <ref type=\"bibr\" target=\"#b29\">[30]</ref> and SIGN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, are special variants of our GMLP framework.</p><p>To valida ale messages are indiscriminately aggregated, without effectively exploring their correlation. SIGN <ref type=\"bibr\" target=\"#b6\">[7]</ref> can be viewed as a special case of using the message aggrega e researches show that such entanglement could compromise performance on a range of benchmark tasks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and suggest separat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  added GRU units after convolutional layers to further boost the representation power of the model. <ref type=\"bibr\" target=\"#b8\">Guo et al. (2020)</ref> set the CNN and LSTM networks in parallel to c would directly affect the subsequent protein folding and tertiary structure prediction. Recent work <ref type=\"bibr\" target=\"#b8\">Guo et al. (2020)</ref> exploited the \"Bagging\" mechanism to obtain th otein sequences with low-quality PSSM into several divisions of MSA count and MSA meff according to <ref type=\"bibr\" target=\"#b8\">Guo et al. (2020)</ref>. Shown in Table <ref type=\"table\">2</ref> and  uality PSSM Enhancement. Since MSA and PSSM are critical for protein property prediction, \"Bagging\" <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020)</ref> is the first attempt to enhance the lowquality s=\"http://www.tei-c.org/ns/1.0\"><head>Loss Function</head><p>Additionally, same as in previous work <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020)</ref>, we use the mean square error (MSE) loss to di SSM in the training set, i.e., calculate the frequency of MSA count when the MSA count less than 60 <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020)</ref>. Once the prior distribution of native low-qua g via contrastive learning.</p><p>Contrastive Learning. Contrastive loss was introduced by Hadsell, <ref type=\"bibr\" target=\"#b8\">Chopra, and LeCun (2006)</ref> to learn representation by contrasting  ampling operation based on prior statistics instead of the fixed downsample ratio used in \"Bagging\" <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020</ref>) which details will be given in the experiment  er the vanilla model denoted as \"Real\" and surpasses the previous state-of-the-art method \"Bagging\" <ref type=\"bibr\" target=\"#b8\">(Guo et al. 2020</ref>) by 6% on average and nearly 8% in the extreme . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f a premise and hypothesis through tree-LSTM <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Le and Zuidema,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al networks has concentrated on node classification task <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib ake a specific node (e.g., a person in social networks) misclassified. In this scenario, Dai et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> study the adversarial attack on graph structure data and p olutional Network (SGC) <ref type=\"bibr\" target=\"#b16\">[17]</ref> and gradient-based attack methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we propose a n cient way to generate destructive adversarial examples. Focusing on the targeted attack, Dai et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> propose GradArgmax, which extracts gradients of the surrog  Network (GCN)</head><p>Since a number of existing works <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bib f-the-art targeted attack methods: Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref> and GradArgmax <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  ly nodes belonging to the same class/different classes will be disconnected/connected. \u2022 GradArgmax <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Since the attack budget \u2206 is defined as the degrees of ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rial attacks, designing defense mechanisms or building robust variants of GNNs have become critical <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019)</ref>.</p><p>In this paper, we propose a new approa nism to attenuate the influence of neighbors with large variance (potentially corrupted). Following <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019)</ref>, we set hidden dimensions at 16 and assume a  poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>);</p><p>\u2022 UM-GNN achieves significantly lower  tions for specifically defending against adversarial attacks, the recent robust GCN (RGCN) approach <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>) has been the most effective, when compared to h a weighted aggregation of features in a closed neighborhood where the weights are trainable. RGCN <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>): This is a recently proposed ap-proach that e  random structural perturbations and its low performance strongly corroborates with the findings in <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>).</p><p>(ii) DICE Attack: In this challenging  <ref type=\"bibr\" target=\"#b26\">(Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018)</ref>. Recently, Zhu et al. <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2019</ref>) introduced a robust variant of GCN based on a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: alizable (DG) ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> has been appealing to researchers recently. Generally, DG R  by existing works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Recently, works <ref type=\"bibr\" target=\"#b43\">[44,< ation (DG) in ReID <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, which learns the generalizable ReID models on multi-source the previous works <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> on DG ReID, we conduct our experiments on the public ReID o ng DG ReID methods <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30</ref>] follow the same pipeline, where they collect all source dom alization <ref type=\"bibr\" target=\"#b48\">[49]</ref> to learn a more generalizable model. Jin et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposed Style Normalization and Restitution modules to di For data augmentation, we perform random cropping, random flipping, and color jittering. Similar to <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we discard random erasing (REA) because REA will degenera e evaluated on the average of 10 repeated random splits of gallery and probe sets. Under Protocol-2 <ref type=\"bibr\" target=\"#b29\">[30]</ref>, all the images in M+D+C3+MT (including the training and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e\" target=\"#fig_2\">2c</ref>). Prompts like the latter require the model to perform variable binding <ref type=\"bibr\" target=\"#b41\">(Smolensky, 1990)</ref> -it is the hedgehog that is in the christmas . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: period under study, which was just over 14% and we were long 653 out of 1077 days.)</p><p>As Sharpe <ref type=\"bibr\" target=\"#b5\">(6)</ref> points out, instead of buying and selling short the DJIA, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . DCMs have often been applied in fields of economy <ref type=\"bibr\" target=\"#b0\">[3]</ref>, health <ref type=\"bibr\" target=\"#b49\">[52]</ref> and transportation <ref type=\"bibr\" target=\"#b8\">[11]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Peng et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a text based model which fuses the word-level and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  proposed, including traditional methods relying on features <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targe  We first mine the user's preferences from the log as her interest profile. For example, SLTB model <ref type=\"bibr\" target=\"#b8\">[9]</ref> extracts topic-based and clicked-based features from the sea >[42]</ref><ref type=\"bibr\" target=\"#b42\">[43]</ref><ref type=\"bibr\" target=\"#b43\">[44]</ref>. SLTB <ref type=\"bibr\" target=\"#b8\">[9]</ref> combined both the click-based and topic-based features extra user profiles are used in existing works. Typical profiles include term, topic, click distributions <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, sequential search b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 8 0.1372 DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b56\">58,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>, yet they remain brittle to adversarial examples. A large b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures in recent papers <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Such deep networks reach top competitive results in the hi riginally used for a face verification task. We also evaluate DCN network architecture described in <ref type=\"bibr\" target=\"#b18\">[19]</ref> without any pre-training. The faces are resized to a fixed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  b \u2208 R d\u00d72d is a shared representation of various relations. For all the experiment, we use PyTorch <ref type=\"bibr\" target=\"#b34\">[35]</ref> and PyTorch geometric <ref type=\"bibr\" target=\"#b10\">[11]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, and pseudolikelihood maximization <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b21\">Kamisetty e  commonly used to fit the parameters <ref type=\"bibr\" target=\"#b3\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013)</ref>. Pseudolikelihood approximates the likeli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b39\">42]</ref>. While many works incorporate data-driven methods into the DCM framework <ref type=\"bibr\" target=\"#b7\">[10,</ref><ref type=\"bibr\" target=\"#b23\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet <ref type=\"bibr\" target=\"#b37\">(Oord et al., 2016a;</ref><ref type=\"bibr\">b)</ref>. Discretization m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b5\">6]</ref>, link prediction <ref type=\"bibr\" target=\"#b6\">[7]</ref> and graph classification <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. Intriguingly, in most. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned model <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> and a dual-encoder architecture <ref type=\"bibr\" target=\"#b3\">(Bromley et al., 1994)</ref>, we focus on developing the right trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b13\">Fey et al., 2018;</ref><ref type=\"bibr\" target=\"#b63\">Zhang &amp; Chen, 2019;</ref><ref type=\"bibr\" target=\"#b29\">Jia et al., 2020)</ref>. For instance, in chemistry, a molecule could. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b15\">Malon (2018)</ref> fine-tunes the generative pretraining transformer (GPT) <ref type=\"bibr\" target=\"#b22\">(Radford et al., 2018)</ref> for FV. Based on the methods mentioned a ation models such as ELMo <ref type=\"bibr\" target=\"#b21\">(Peters et al., 2018)</ref> and OpenAI GPT <ref type=\"bibr\" target=\"#b22\">(Radford et al., 2018)</ref> are proven to be effective on many NLP t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms due to its powerful expressive ability, including PPI prediction and classification. These works <ref type=\"bibr\" target=\"#b4\">[Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Hashemifar et al. algorithms in PPI prediction, PIPR <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2019]</ref>, DNN-PPI <ref type=\"bibr\" target=\"#b4\">[Li et al., 2018]</ref> and DPPI <ref type=\"bibr\" target=\"#b3\">[Hashem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ://www.tei-c.org/ns/1.0\"><p>We present a variational approximation to the information bottleneck of <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>. This variational approach allows us to pa oot_2\">3</ref> This approach is known as the information bottleneck (IB), and was first proposed in <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>. Intuitively, the first term in R IB encou challenging. There are two notable exceptions: the first is when X, Y and Z are all discrete, as in <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>; this can be used to cluster discrete data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed representation learning, has been proven to be successful techniques contributing to the success <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In essence, embedding is a way to represent a sparse vector. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Hence, there has been a massive increase in work on MT systems that involve more than two languages <ref type=\"bibr\" target=\"#b7\">(Chen et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Choi et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing significantly fewer training data comparing to the few previous works which address heterophily <ref type=\"bibr\" target=\"#b22\">(Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b38\">Zhu et al. 2020 nificantly smaller fraction of training samples compared to previous works that address heterophily <ref type=\"bibr\" target=\"#b22\">(Pei et al. 2020;</ref><ref type=\"bibr\" target=\"#b38\">Zhu et al. 2020 pe=\"bibr\" target=\"#b20\">Namata et al. 2012)</ref>. We use the features and class labels provided by <ref type=\"bibr\" target=\"#b22\">Pei et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n ch benchmark with an identity matrix I. We use the training, validation and test splits provided by <ref type=\"bibr\" target=\"#b22\">Pei et al. (2020)</ref>.</p><p>Heterophily. We report results on grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ter vision and natural language processing <ref type=\"bibr\" target=\"#b20\">(Gomez et al., 2017;</ref><ref type=\"bibr\" target=\"#b65\">Xie et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Bai et al., 2018  2019)</ref> and grouped convolutions <ref type=\"bibr\" target=\"#b36\">(Krizhevsky et al., 2012;</ref><ref type=\"bibr\" target=\"#b65\">Xie et al., 2017)</ref>, we generalize reversible residual connection. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  an end-to-end training network to learn the mapping between 3D meshes and raw speeches. Fan et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> generate the lower half region of the face by a bi-direction. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [6]</ref>. We omit recently proposed extensions such as for example the use of residual connections <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, dense connections <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rf <ref type=\"bibr\" target=\"#b5\">[6]</ref>, PAPI <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and LiMiT <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Each of these tools use different mechanisms to interact wi rograms, such as Firefox, could not even run properly when profiled with PAPI-C due to its overhead <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>This overhead issue becomes even more critical when t cy or closedsource programs.</p><p>This was improved upon when LiMiT was introduced by Demme et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref>. They addressed the expensive system call problem. LiMiT is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b9\">8,</ref><ref type=\"bibr\" target=\"#b10\">9,</ref><ref type=\"bibr\" target=\"#b22\">21,</ref><ref type=\"bibr\" target=\"#b27\">26]</ref> have made great progress on abstractive summarization. The  get=\"#b18\">17,</ref><ref type=\"bibr\" target=\"#b22\">21,</ref><ref type=\"bibr\" target=\"#b25\">24,</ref><ref type=\"bibr\" target=\"#b27\">26]</ref> in abstractive summarization, the ext-abs framework trains  ory. Table <ref type=\"table\">1</ref> shows that two state-of-the-art enc-dec models, i.e., PointGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> and BART <ref type=\"bibr\" target=\"#b14\">[13]</ref>, both f oom. the four store workers arrested could spend three years each in prison if convicted . PointGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> four employees of a popular indian ethnic chain have been  e two representative Enc-Dec model options as our abstractor: the standard Enc-Dec model PointerGen <ref type=\"bibr\" target=\"#b27\">[26]</ref> with attention mechanism <ref type=\"bibr\" target=\"#b19\">[1 rget=\"#b22\">[21]</ref> with the data preprocessing and use the non-anonymized version as See et al. <ref type=\"bibr\" target=\"#b27\">[26]</ref>.</p><p>Webis-TLDR-17 Corpus <ref type=\"bibr\" target=\"#b33\" ibr\" target=\"#b3\">[2,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref>. The pointer-generator networks <ref type=\"bibr\" target=\"#b27\">[26]</ref> consisting of copy mechanism and coverage model are the mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ntations of sentences and their similarity <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In prior work <ref type=\"bibr\" target=\"#b31\">[32]</ref>, we also utilized a Siamese BERT model to determine the di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ith kernel machines, one typically observes an increase in performance when using soft-DTW over DTW <ref type=\"bibr\" target=\"#b6\">(Cuturi, 2011)</ref> for classification.</p><p>Our contributions. We e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eep Graph Kernels <ref type=\"bibr\" target=\"#b25\">[26]</ref> , and Multiscale Laplacian Graph Kernel <ref type=\"bibr\" target=\"#b26\">[27]</ref> . There are other convolution kernel functions: Shervashid ant graph kernel is studied in recent years <ref type=\"bibr\" target=\"#b28\">[29]</ref> . Zhao et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref> proposed a labeled graph kernel for behavior analysis, Xu . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ld datasets, WebQuestions <ref type=\"bibr\" target=\"#b3\">(Berant et al., 2013)</ref> and WikiAnswers <ref type=\"bibr\" target=\"#b11\">(Fader et al., 2013)</ref>. In this way, the syntactic structure and  ts including WebQuestions <ref type=\"bibr\" target=\"#b3\">(Berant et al., 2013)</ref> and WikiAnswers <ref type=\"bibr\" target=\"#b11\">(Fader et al., 2013)</ref> as well as on the Internet. In this manner. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iciency of academic resources to a great extent, especially for newcomers and students in this area <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In addition, as one of the important carriers of academics . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y, which has been addressed in several very recent works <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Even so, they still heavily rely on the fine-tuning proce questionable for encoding long texts. Those recent works <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref> targeting on long texts also need labeled data for downstr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  also connected to other few-shot learning paradigms in NLP, including (1) semi-supervised learning <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2017;</ref><ref type=\"bibr\" target=\"#b45\">Xie et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Importance of Relation</head><p>The heterogeneity of KGs is usually reflected by the relation-path <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which demonstrates complex semantic features that involve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: th but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing  pe=\"bibr\" target=\"#b26\">28]</ref> on ImageNet <ref type=\"bibr\" target=\"#b19\">[21]</ref>, Kim et al. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose two very d /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">Datasets</head><p>By following <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b21\">23]</ref>, we use a training mmarizes quantitative results on the four testing sets, by citing the results of prior methods from <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. The two DRRN mode pth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods<ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing  meters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30   respectively. On the one hand, to accelerate the convergence speed of very deep networks, the VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> is trained with a very high learning rate (10 \u22121 , instead on focuses on three most related work to ours: ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Fig. < on. Denoting the input as x and the underlying mapping as H(x), the residual mapping is defined as  <ref type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c)   <ref type=\"table\">1</ref>. Strategies used in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and DRRN.  \">VDSR</head><p>Differing from ResNet that uses residual learning in every few stacked layers, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> introduces GRL, i.e., residual learning between the input  structure of DRRN is illustrated in Fig. <ref type=\"figure\" target=\"#fig_4\">5</ref>. Actually, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> can be viewed as a special case of DRRN, i.e., when U = 0, r that, for each original image, we have 7 additional augmented versions. Besides, inspired by VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, we also use scale augmentation to train our model, and im  epochs. Since a large learning rate is used in our work, we adopt the adjustable gradient clipping <ref type=\"bibr\" target=\"#b12\">[13]</ref> to boost the convergence rate while suppressing exploding   VDSR re-implementation also uses BN and ReLU as the activation functions, unlike the original VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> that does not use BN. These results are faithful since our ese results are faithful since our VDSR re-implementation achieves similar benchmark performance as <ref type=\"bibr\" target=\"#b12\">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref  RCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRRN are illustrated in Fig. <ref type=\"figure\" target /1.0\"><head n=\"4.5.\">Discussions</head><p>Since global residual learning has been well discussed in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, in this section, we mainly focus on local residual learni ucture. Local Residual Learning To demonstrate the effectiveness of LRL, DRRN is compared with VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which has no LRL. For fair comparison, the depth and numb 5]</ref> and FSRCNN <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Very deep models (d \u2265 20) include VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, RED <ref  ameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30<r  ResNet<ref type=\"bibr\" target=\"#b7\">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c)  Ratio (PSNR) performance of several recent CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar me depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and non-DL <ref typ images are of the same size. For fair comparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref> developed a tool to mine parallel datasets from ported open source projects. Aggarwal et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> trained Moses on a Python 2 to Python 3 parallel corpus crea ntially rely on BLEU score <ref type=\"bibr\" target=\"#b37\">[38]</ref> to evaluate their translations <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe tudies in source code translation use the BLEU score to evaluate the quality of generated functions <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /www.tei-c.org/ns/1.0\"><head n=\"4.2\">Implementation Details</head><p>Our code is written in PyTorch <ref type=\"bibr\" target=\"#b38\">(Paszke et al., 2019)</ref>. For fine-tuning, we adopt the standard l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>\u2022 CelebA-Test is the synthetic dataset with 3,000 CelebA-HQ images from its testing partition <ref type=\"bibr\" target=\"#b52\">[53]</ref>. The generation way is the same as that during training.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: passages for each question. The trick of in-batch negatives has been used in the full batch setting <ref type=\"bibr\" target=\"#b47\">(Yih et al., 2011)</ref> and more recently for mini-batch <ref type=\" airs of queries and documents, discriminatively trained dense encoders have become popular recently <ref type=\"bibr\" target=\"#b47\">(Yih et al., 2011;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  GraphSAINT <ref type=\"bibr\" target=\"#b26\">(Zeng et al., 2020)</ref>. On the other hand, ClusterGCN <ref type=\"bibr\" target=\"#b7\">(Chiang et al., 2019)</ref> is a heuristic in the sense that, despite  r the baselines: GraphSAINT <ref type=\"bibr\" target=\"#b26\">(Zeng et al., 2020)</ref> and ClusterGCN <ref type=\"bibr\" target=\"#b7\">(Chiang et al., 2019)</ref> (both are message passing methods); and al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e overconfident predictions for regularising deep neural networks. It includes label smoothing (LS) <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and confidence pen e, our work offers a defence of entropy minimisation against the recent confidence penalty practice <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta egative log-likelihood, and q serves as the probability mass function.</p><p>Label smoothing. In LS <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, we soften one-hot romotes entropy minimisation, which is in marked contrast to recent practices of confidence penalty <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta xml:id=\"formula_0\">LS CP 1/3 1/3 1/3 1 0 0 1/2 1/3 1/6 1 0 0 0 0 1 1</formula><p>(a) OR includes LS <ref type=\"bibr\" target=\"#b41\">[42]</ref> and CP <ref type=\"bibr\" target=\"#b32\">[33]</ref>. LS softe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Gui et al., 2019b)</ref>.</p><p>Recently, Transformer <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, l \"#b34\">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, like machine translation <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>, language modeling <ref type=\"bibr\" targ mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Transformer</head><p>Transformer was introduced by <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>, which was mainly based on self-attentio =\"bibr\" target=\"#b7\">Devlin et al., 2018)</ref>. Instead of using the sinusoidal position embedding <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref> and learned absolute position embedding, 1\">Transformer Encoder Architecture</head><p>We first introduce the Transformer encoder proposed in <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>. The Transformer encoder takes in an mat e Transformer encoder includes layer normalization and Residual connection, we use them the same as <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c. ng it unable to capture the sequential characteristic of languages. In order to solve this problem, <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref> suggested to use position embeddings gen  two tokens. For any fixed offset k, P E t+k can be represented by a linear transformation of P E t <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>. In TENER, Transformer encoder is used n d in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Devlin et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: simply regularize model predictions to be invariant to small noise applied to either input examples <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Sajjadi et  r\" target=\"#b1\">(Bachman et al., 2014)</ref> directly applies Gaussian noise and Dropout noise; VAT <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018;</ref><ref type=\"bibr\">2016)</ref> defines the n type=\"bibr\" target=\"#b34\">(Sajjadi et al., 2016;</ref><ref type=\"bibr\">Laine &amp; Aila, 2016;</ref><ref type=\"bibr\" target=\"#b23\">Miyato et al., 2018)</ref>. But different from existing work, we focu has been shown to be beneficial <ref type=\"bibr\" target=\"#b15\">(Grandvalet &amp; Bengio, 2005;</ref><ref type=\"bibr\" target=\"#b23\">Miyato et al., 2018)</ref>, we sharpen predictions when computing the  current parameters \u03b8 indicating that the gradient is not propagated through \u03b8, as suggested by VAT <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref>. We set \u03bb to 1 for most of our experiment cally, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref>, an algorithm that generates adversarial  =\"#b41\">(Tarvainen &amp; Valpola, 2017)</ref> Conv-Large 3.1M 12.31 \u00b1 0.28 3.95 \u00b1 0.19 VAT + EntMin <ref type=\"bibr\" target=\"#b23\">(Miyato et al., 2018)</ref> Conv-Large 3.1M 10.55 \u00b1 0.05 3.86 \u00b1 0.11 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rg/ns/1.0\"><head>III. RU-NET AND R2U-NET ARCHITECTURES</head><p>Inspired by the deep residual model <ref type=\"bibr\" target=\"#b6\">[7]</ref>, RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and U-Net < type=\"bibr\" target=\"#b4\">[5]</ref>, GoogleNet <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Residual Net <ref type=\"bibr\" target=\"#b6\">[7]</ref>, DenseNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Capsu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f>, and variants of Craig, Landin, and Hagersten (CLH) locks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. Historically, CLH lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and RoBERTa <ref type=\"bibr\" target=\"#b25\">(Liu et al., 2019b)</ref>) to be factual knowledge bases <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bouraoui et  paradigms, as shown in Figure <ref type=\"figure\" target=\"#fig_0\">1:</ref> \u2022 Prompt-based retrieval <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b16\">Jiang et a eratures have shown that such paradigms can achieve decent performance on some benchmarks like LAMA <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019)</ref>.</p><p>Despite some reported success, cu rent answer distribution from the widely-used LAMA<ref type=\"foot\" target=\"#foot_2\">4</ref> dataset <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2019)</ref>. However, we find that the prediction di Models (PLMs) raises the question of whether PLMs can be directly used as reliable knowledge bases. <ref type=\"bibr\" target=\"#b30\">Petroni et al. (2019)</ref> propose the LAMA benchmark, which probes  e regarded as the answer. We consider three kinds of prompts: the manually prompts T man created by <ref type=\"bibr\" target=\"#b30\">Petroni et al. (2019)</ref>, the mining-based prompts T mine by <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b6\">Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020)</ref>  <ref type=\"bibr\" target=\"#b29\">(Wang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of NAS methods, especially when the search space is huge. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar  are three basic types: accuracy-based, magnitudebased, and angle-based metrics. For example, PCNAS <ref type=\"bibr\" target=\"#b19\">[20]</ref> drop unpromising operators layer by layer using accuracy a ect for elastic depth. The split point space is set to range <ref type=\"bibr\" target=\"#b8\">(9,</ref><ref type=\"bibr\" target=\"#b19\">20)</ref> to handle different complexity constrains. In total we have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n pro/anti stereotypical conditions is significant (p &lt; .05) under an approximate randomized test<ref type=\"bibr\" target=\"#b8\">(Graham et al., 2014)</ref>. Our methods eliminate the difference betw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: preference from user-item historical interaction behaviors <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar ese embedding models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Earlier works trea ved preference set <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Therefore, how to learn users' preferences from implicit f zation techniques for the user and item embedding learning <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Bayesian Personalized Ranking (BPR) is a popular pairwise  ser-item behaviors <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Learning accurate user and item embeddings is the default  latent embeddings based on matrix factorization techniques <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Recently, researchers argued that user behaviors can be na  prediction, we employ the pairwise ranking based BPR loss <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, which assumes that the observed items' prediction values s esigns a pairwise ranking goal, assuming that a user prefers an observed item to an unobserved item <ref type=\"bibr\" target=\"#b25\">[26]</ref>. As users' behaviors are naturally represented as a user-i ould be categorized into three groups. The first is classical matrix factorization based method BPR <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The second is neural graph based CF models with fixed gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t results on Vaihingen data set <ref type=\"bibr\" target=\"#b17\">[18]</ref> and WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> e used for training in the testing phase. Additionally, we test the method on WHU Building data set <ref type=\"bibr\" target=\"#b18\">[19]</ref> to further evaluate the performance of AFS-based models. S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"2.6.1\">Implementation</head><p>We implement the neural network using the torch7 library <ref type=\"bibr\" target=\"#b7\">(Collobert et al., 2011a)</ref>. Training and inference are done on a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Neural Machine Translation (NMT; <ref type=\"bibr\" target=\"#b12\">Cheng et al., 2018)</ref>, and ASR <ref type=\"bibr\" target=\"#b45\">(Sperber et al., 2017)</ref>, we propose two Noise-Aware Training (NA mputer vision <ref type=\"bibr\" target=\"#b27\">(Krizhevsky et al., 2012)</ref> and speech recognition <ref type=\"bibr\" target=\"#b45\">(Sperber et al., 2017)</ref>.</p><p>During training, we artificially . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: subjects to create a reliable deception detection system <ref type=\"bibr\" target=\"#b70\">[70]</ref>, <ref type=\"bibr\" target=\"#b71\">[71]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualization <ref type=\"bibr\" target=\"#b28\">[28]</ref \">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al.   direct calls <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, as shown in Fig. <ref type=\"figure\" target=\"#fig_0\">11b</re tion call and its devirtualized form. usually has higher accuracy than an indirect branch predictor <ref type=\"bibr\" target=\"#b5\">[6]</ref>. However, not all indirect calls can be converted to multipl form RCPO, the following conditions need to be fulfilled <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>:</p><p>1. The number of frequent target addresses from a cal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Radford et al. (2018)</ref> demonstrate a pre-trained generative model (GPT) and its effects, while <ref type=\"bibr\" target=\"#b10\">Devlin et al. (2019b)</ref> release a pre-trained deep Bidirectional  entation from Transformers (BERT), achieving state-of-the-arts on dozens of benchmarks.</p><p>After <ref type=\"bibr\" target=\"#b10\">Devlin et al. (2019b)</ref>, similar pre-trained encoders spring up r  many alternatives for pre-trained language representation can be used, e.g., masked language model <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref>. Note that those two tasks only share th  output at [CLS] is often used as the sentence representation.</p><p>PLM Objective Inspired by BERT <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref>, MLM randomly selects 15% of input token LS] output for sentence-level prediction and the outputs of all tokens for sequence labelling tasks <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref> </p></div> <div xmlns=\"http://www.tei-c.  use the transformer architecture <ref type=\"bibr\" target=\"#b41\">(Vaswani et al., 2017)</ref> as in <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b;</ref><ref type=\"bibr\" target=\"#b20\">Liu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: procedures such as generating forced alignments and decision trees. Meanwhile, another line of work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" target ction to infer speech-label alignments automatically. This CTC technique is further investigated in <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" target incorporate lexicons and language models into decoding. When decoding CTC-trained models, past work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target enchmark show that Eesen results in superior performance than the existing end-to-end ASR pipelines <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>. The WERs of Eesen are ained using frame-level labels with respect to the cross-entropy (CE) criterion. Instead, following <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target /1.0\"><head n=\"3.1.\">Decoding with WFSTs</head><p>Previous work has introduced a variety of methods <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target  ARPA format (which we will consistently refer to as standard). To be consistent with previous work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>, we report our results e\">3</ref> lists the results of end-to-end ASR systems that have been reported in the previous work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref> and on the same datase ature differ not only in their model architectures but also in their decoding methods. For example, <ref type=\"bibr\" target=\"#b6\">[6]</ref> and <ref type=\"bibr\" target=\"#b8\">[8]</ref> adopt two distin \">[10]</ref> or achieve the integration under constrained conditions (e.g., nbest list rescoring in <ref type=\"bibr\" target=\"#b6\">[6]</ref>). In this work, we propose a generalized decoding approach b ed in decoding. When only the lexicon is used, our decoding behaves similarly as the beam search in <ref type=\"bibr\" target=\"#b6\">[6]</ref>. In this case, the WER rises quickly to 26.92%. This obvious dard language model, the character-based system gets the WER of 9.07%. CTC experiments in past work <ref type=\"bibr\" target=\"#b6\">[6]</ref> have adopted an expanded vocabulary, and re-trained the lang ref type=\"bibr\" target=\"#b8\">8]</ref> and on the same dataset. Our Eesen framework outperforms both <ref type=\"bibr\" target=\"#b6\">[6]</ref> and <ref type=\"bibr\" target=\"#b8\">[8]</ref> in terms of WERs ]</ref> in terms of WERs on the testing set. It is worth pointing out that the 8.7% WER reported in <ref type=\"bibr\" target=\"#b6\">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the  bibr\" target=\"#b6\">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the authors of <ref type=\"bibr\" target=\"#b6\">[6]</ref> generate a nbest list of hypotheses from a hybrid DNN model, e WERs of Eesen systems via more advanced learning techniques (e.g., expected transcription loss in <ref type=\"bibr\" target=\"#b6\">[6]</ref>) and alternative decoding approach (e.g., dynamic decoders <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rotect the proprietary data. However, recent work by Zhu et al., \"Deep Leakage from Gradient\" (DLG) <ref type=\"bibr\" target=\"#b0\">[1]</ref> showed the possibility to steal the private training data fr nables us to always extract the ground-truth labels and significantly simplify the objective of DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> in order to extract good-quality data. Hence, we name our ap extraction with better fidelity.</p><p>\u2022 We empirically demonstrate the advantages of iDLG over DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> via comparing the accuracy of extracted labels and the fidel <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Methodology</head><p>Recent work by Zhu et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> presents an approach (DLG) to steal the proprietary data pro s</head><p>In this section, we empirically demonstrate the advantages of our (iDLG) method over DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We perform experiments on the classification task over thre \" target=\"#b9\">[10]</ref> with 10, 100, and 5749 categories respectively. Following the settings in <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we use the randomly initialized LeNet for all experiments.   is used as the optimizer. For fast training, we resize all images in LFW to 32 \u00d7 32.</p><p>For DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, as described by the authors, we start the procedure with th % 100.0% LFW 79.1% 100.0% Table <ref type=\"table\">1</ref>: Accuracy of the extracted labels for DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> and iDLG. Note that iDLG always extracts the correct label a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nformation, they suffer from sensitivity to data alignment, often involve complicated architectures <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bib -based proposal generation might fail in some cases that could only be observed from 3D space. MV3D <ref type=\"bibr\" target=\"#b10\">[11]</ref> and AVOD <ref type=\"bibr\" target=\"#b11\">[12]</ref> project. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b7\">8]</ref> or high-order interactions <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, as shown in Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>. .1361 PNN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356 FGCNN <ref type=\"b crease at 10 \u22123 level in Criteo dataset is already clear compared with recent works such as xDeepFM <ref type=\"bibr\" target=\"#b4\">[5]</ref> and DCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Sinc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  larger taxonomies from the SemEval-2016 Task 13 benchmark dataset, which contain hundreds of terms <ref type=\"bibr\" target=\"#b6\">(Bordea et al., 2016b)</ref>. <ref type=\"bibr\" target=\"#b31\">Shang et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic, lowering the workers' confidence in rating the ground-truth videos. A Wilcoxon signed-rank test <ref type=\"bibr\" target=\"#b37\">[38]</ref> shows that the median difference between our ratings and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e user preferences <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar o help construct better user profiles. Besides, some works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> attempted to disamb a used to help ranking, such as the shared word embeddings <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. The association be t=\"#b8\">[9]</ref> extracts topic-based and clicked-based features from the search history, and PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref> trains personal word embeddings with the user's individual k. In the first one, namely FedPSFlat, we adapt the state-of-the-art personalized search model PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref> to make it privacy compatible. We select PEPS because it u vior representation vectors <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and personal word embeddings <ref type=\"bibr\" target=\"#b46\">[47]</ref>. User profiles are usually aggregated vectorized represent ing on every client as a separate task. Focusing on state-of-the-art personalized search model PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref>, it sets up a module which contains personal word embeddin 1\">2</ref>. The main modules are briefly introduced as follows, and more details can be referred to <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>Word embedding layer. There is a global word embedd \"#b40\">[41]</ref> to enhance the representation of the current query for disambiguation.</p><p>PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref>: It trained personal word embeddings for each user to clar initialize the global and personal word embeddings. Other hyper-parameters are set the same as PEPS <ref type=\"bibr\" target=\"#b46\">[47]</ref>. We adopt Adam optimizer to train the personalization mode , MRR and P@1. A more reliable metric for personalized search P-Improve is also employed. Following <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, we only use P-Imp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rks (CNNs) <ref type=\"bibr\" target=\"#b0\">[1]</ref> have achieved great success in acoustic modeling <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target \" target=\"#b6\">6]</ref>, like regular Deep Neural Networks (DNNs), which results in a hybrid system <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target the required non-linear modeling capabilities.</p><p>Unlike the time windows applied in DNN systems <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target ://www.tei-c.org/ns/1.0\"><head n=\"2.\">Convolutional Neural Networks</head><p>Most of the CNN models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target n be very slow due to the iterative multiplications over time when the input sequence is very long; <ref type=\"bibr\" target=\"#b1\">(2)</ref> The training process is sometimes tricky due to the well-kno. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b19\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Wang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b12\">Feng et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Lv et al., 202 enges. We first encode the QA context using an LM, and retrieve a KG subgraph following prior works <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref>. Our QA-GNN has two key insights: (i) Relev  compare with prior works, KagNet <ref type=\"bibr\" target=\"#b19\">(Lin et al., 2019)</ref> and MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref> in Table 1. As we handle edges of different tions and linear with respect to the number of nodes. We achieve the same space complexity as MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org  RN <ref type=\"bibr\" target=\"#b35\">(Santoro et al., 2017)</ref> 74.57 (\u00b10.91) 69.08 (\u00b10.21) + MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref> 74   <ref type=\"bibr\" target=\"#b22\">(Lv et   et al., 2020)</ref> 74   <ref type=\"bibr\" target=\"#b22\">(Lv et al., 2020)</ref> 75.3 RoBERTa+MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref> 75.4 Albert+PG <ref type=\"bibr\" target=\"#b4 bibr\">KagNet (Lin et al., 2019)</ref>, and ( <ref type=\"formula\" target=\"#formula_5\">5</ref>) MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref>. ( <ref type=\"formula\" target=\"#formula_1\"> toRoBERTa + PG <ref type=\"bibr\" target=\"#b45\">(Wang et al., 2020b)</ref> 80.2 AristoRoBERTa + MHGRN <ref type=\"bibr\" target=\"#b12\">(Feng et al., 2020)</ref> 80.6 Albert + KB 81.0 T5 * <ref type=\"bibr\" the QA context (z LM = f enc (text(z))), and each node on the G sub using the entity embedding from <ref type=\"bibr\" target=\"#b12\">Feng et al. (2020)</ref>. In the subsequent sections, we will reason  nswer choice), we retrieve the subgraph G sub from G following the pre-processing step described in <ref type=\"bibr\" target=\"#b12\">Feng et al. (2020)</ref>, with hop size k = 2. Henceforth, in this se alt water\"). While prior KG reasoning models <ref type=\"bibr\" target=\"#b19\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Feng et al., 2020)</ref> enumerate individual paths in the KG for mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to-translations it has already been shown that a suitable structure for \u03c6 is a tensor field network <ref type=\"bibr\" target=\"#b24\">[25]</ref>, explained below. Note that Romero et al. <ref type=\"bibr\" q. <ref type=\"bibr\" target=\"#b4\">(5)</ref>.</p><p>Tensor Field Networks Tensor field networks (TFN) <ref type=\"bibr\" target=\"#b24\">[25]</ref> are neural networks, which map point clouds to point cloud annels, but we omit it here. Weiler et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>, Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Kondor <ref type=\"bibr\" target=\"#b12\">[13]</ref> showe duces the kernel to a scalar w multiplied by the identity, W = w I, referred to as self-interaction <ref type=\"bibr\" target=\"#b24\">[25]</ref>. As such we can rewrite the TFN layer as</p><formula xml:i c c = w c c per representation degree, shared across all points.</p><p>As proposed in Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>, this is followed by a norm-based non-linearity.</p><p>Att tron-proton simulation.</p><p>Linear DeepSet <ref type=\"bibr\" target=\"#b39\">[40]</ref> Tensor Field <ref type=\"bibr\" target=\"#b24\">[25]</ref> Set Transformer <ref type=\"bibr\" target=\"#b13\">[14]</ref>   type=\"bibr\" target=\"#b13\">[14]</ref>, a non-equivariant attention model, and Tensor Field Networks <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which is similar to SE(3)-Transformer but does not levera s to train significantly larger versions of both the SE(3)-Transformer and the Tensor Field network <ref type=\"bibr\" target=\"#b24\">[25]</ref> and to apply these models to real-world datasets.</p><p>Ou be found in many mathematical physics libraries.</p><p>Tensor Field Layers In Tensor Field Networks <ref type=\"bibr\" target=\"#b24\">[25]</ref> and 3D Steerable CNNs <ref type=\"bibr\" target=\"#b32\">[33]<  that all the Tensor Field networks we trained were significantly bigger than in the original paper <ref type=\"bibr\" target=\"#b24\">[25]</ref>, mostly enabled by the faster computation of the spherical k to obtain stable training. We used a norm based non-linearity for the Tensor Field network (as in <ref type=\"bibr\" target=\"#b24\">[25]</ref>) and no extra non-linearity (beyond the softmax in the sel he Tensor Field network and the linear baseline are SE(3) equivariant. For the Tensor Field Network <ref type=\"bibr\" target=\"#b24\">[25]</ref> baseline, we used the same hyper parameters as for the SE( linear self-interaction and an additional norm-based nonlinearity in each layer as in Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>. For the DeepSet <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>. To remedy such a prob e roughly divided into two branches: random walk-based and reconstruction-based methods. The former <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target ures with the assumption that nodes within the sliding window or neighborhoods are closely relevant <ref type=\"bibr\" target=\"#b7\">[7,</ref><ref type=\"bibr\" target=\"#b37\">37,</ref><ref type=\"bibr\" targ  <ref type=\"bibr\" target=\"#b44\">[44]</ref>, PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref>, BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref> and FOBE <ref type=\"bibr\" target=\"#b32\">[32]</ref> are speci iv xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1.1\">Data Preprocessing.</head><p>As used in BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref>, we select 60% edges for training and remaining edges for te [36]</ref>. \u2022 Bipartite graph embedding: PinSage <ref type=\"bibr\" target=\"#b40\">[40]</ref> and BiNE <ref type=\"bibr\" target=\"#b7\">[7]</ref>.</p><p>PinSage integrates random walk into GNN architectures. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion in detail and then extrapolate to the entire execution <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. A major challenge in sampled evaluation however is to quic  is transferable across both hardware and software changes <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Functional warming  warms up the microarchitecture state b es not allow for software changes. Functional warming (FW) <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> does not incur any storage overhead, allows for software ch ect number of representative detailed regions that are evaluated in detail to then extrapolate from <ref type=\"bibr\" target=\"#b33\">[34]</ref>. The key challenge in sampling is to get (i) the correct a unctional fast-forwarding, checkpointing and virtualized fastforwarding. Functional fast-forwarding <ref type=\"bibr\" target=\"#b33\">[34]</ref> leverages functional simulation to get to the next represe a detailed warm-up using a small number of instructions (e.g., 30,000) prior to the detailed region <ref type=\"bibr\" target=\"#b33\">[34]</ref>. With this small amount of warming, only a small part of t instructions. Prior research shows that the highest accuracy is achieved for small detailed regions <ref type=\"bibr\" target=\"#b33\">[34]</ref>; larger detailed regions will likely make DeLorean even mo d to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS <ref type=\"bibr\" target=\"#b33\">[34]</ref>. \u2022 CoolSim: Randomized Statistical Warming (RSW) is employ ture state using all memory references between two consecutive detailed regions, which is very slow <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Various approaches have been proposed to reduce the warm-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: atching heuristics to obtain pseudo summaries based on a set of keywords. We use TextRank algorithm <ref type=\"bibr\" target=\"#b20\">[19]</ref> to extract the keywords from the reference summary and obt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned by CLIP.</p><p>Text-guided image generation and manipulation The pioneering work of Reed et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> approached text-guided image generation by training a cond ntion mechanism between the text and image features. Additional supervision was used in other works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  statistics for 10k steps and we do model selection on the validation set. Following previous works <ref type=\"bibr\" target=\"#b66\">(Yamada et al., 2016;</ref><ref type=\"bibr\" target=\"#b16\">Ganea &amp;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion system to utilize the entire MEDLINE data set. By using state-of-the-art tools, such as ToPMine <ref type=\"bibr\" target=\"#b12\">[16]</ref> and FastText <ref type=\"bibr\" target=\"#b6\">[9]</ref>, we a Latent Dirichlet Allocation (LDA) <ref type=\"bibr\" target=\"#b5\">[8]</ref> and topical phrase mining <ref type=\"bibr\" target=\"#b12\">[16]</ref>, along with other data mining techniques to conceptually l word phrases from that corpus such as \"asthma a ack,\" allowing us to treat phrases as single tokens <ref type=\"bibr\" target=\"#b12\">[16]</ref>. Next, we send the corpus through FastText, the most recen urned to the UMLS SPECIALIST NLP toolset <ref type=\"bibr\" target=\"#b0\">[1]</ref> as well as ToPMine <ref type=\"bibr\" target=\"#b12\">[16]</ref> and FastText <ref type=\"bibr\" target=\"#b6\">[9,</ref><ref t o . It is also important to note that we modify the version of ToP-Mine distributed by El-Kishky in <ref type=\"bibr\" target=\"#b12\">[16]</ref> to allow phrases containing numbers, such as gene names li  over weigh a specialized language. However, phrase mining approaches that recover n-grams, such as <ref type=\"bibr\" target=\"#b12\">[16]</ref>, produce accurate methods without limiting the dictionary.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s) and directly harness the resultant example to fool the remote target model (i.e., victim models) <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>Among these t \" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, and the other one is transfer-based <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta or fair comparisons, we adopt default parameters as recommended in benchmark approaches and Foolbox <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. The random noise   of the source model <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Specifically, althou ork is the regularization-based approach: transferable adversarial perturbation (TAP) introduced by <ref type=\"bibr\" target=\"#b40\">[41]</ref>. TAP injects two regularization terms into the vanilla tra 19\">[20,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. We follow the protocol of the baseline method <ref type=\"bibr\" target=\"#b40\">[41]</ref> to curate experimental datasets and target models for fair cannot strictly meet the l \u221e budget, we employ the modified l \u221e version of C&amp;W as introduced by <ref type=\"bibr\" target=\"#b40\">[41]</ref>, which can explicitly satisfy the l \u221e norm constraint. Sim b40\">[41]</ref>, which can explicitly satisfy the l \u221e norm constraint. Similar to our strategy, TAP <ref type=\"bibr\" target=\"#b40\">[41]</ref> boosts adversarial transferability through two regularizat e random noise is sampled from a clipped normal distribution with mean 0 and variance 1.  Following <ref type=\"bibr\" target=\"#b40\">[41]</ref>, we fix the perturbation budget \u01eb to 16 for all methods. W ext attack models defended by adversarial training. For fair comparisons with the baseline approach <ref type=\"bibr\" target=\"#b40\">[41]</ref>, we stick to employing undefended models as local source m d the other one is the regularization-based transferable adversarial perturbation (TAP) proposed by <ref type=\"bibr\" target=\"#b40\">[41]</ref>. With the integrated attacks, we conduct experiments simil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: plexity constraint. The searched architectures generate new stateof-the-art performance on ImageNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>. For instance, as shown in Fig. <ref type=\"figure\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hes have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar ype=\"bibr\" target=\"#b23\">[24]</ref>. The explicit approaches <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar  to solve this problem <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar tly leverage subtopics to determine the diversity of results <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar  target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and supervised approaches such as DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref  xplicit method calculating the distribution by counting the relevant document of the subtopic. DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduces the machine learning method into explicit appro ent \ud835\udc36 to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> score function for the generator. We introduce the score f <p>In the training process, we first train R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref> and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> respectively using MLE loss in both ways. It is because ou br\" target=\"#b5\">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> as supervised baseline methods. Top 20 results of Lemur ar b7\">[8]</ref> as the RNN cell for comparison. In our experiments, we conduct the list-pairwise loss <ref type=\"bibr\" target=\"#b11\">[12]</ref> to train DSSA method. The feature vector 1 http://playbigd DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  KG by easily submitting a set of keywords expressing an information need. State-of-the-art methods <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2020]</ref> efficiently find and present an optimum subg e=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref>, aka a group Steiner tree (GST) <ref type=\"bi e=\"bibr\" target=\"#b6\">[Ding et al., 2007;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref>, or as a variant of this problem <ref type=\"b  Recent faster algorithms for the GST problem <ref type=\"bibr\" target=\"#b10\">[Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref> were not compared because they were designed  sider more scalable approximation algorithms <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2020]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n ly we have extended it to calculate the total weight of graph elements and their semantic distances <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2021]</ref>. This extended optimization problem is refer  cost. This optimization problem is referred to as the quadratic group Steiner tree problem (QGSTP) <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2021]</ref>. It extends the cost function of the vertex-  solution to QGSTP while the algorithms in <ref type=\"bibr\" target=\"#b0\">[Bryson et al., 2020;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 2021]</ref> cannot guarantee to find an optimum solution. is the main concern. For such applications, one may consider more scalable approximation algorithms <ref type=\"bibr\" target=\"#b13\">[Shi et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Shi et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to query for concepts intersecting two keywords, is a notable contribution to hypothesis generation <ref type=\"bibr\" target=\"#b23\">[27]</ref>. is system, proposed by Liu et al., is similar to both our. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> or regularize the label differences between neighbors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>.</p><p>With the success of deep learning, methods based on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ood. In contrast, NVAE is trained directly with the VAE objective. Moreover, VQ-VAE-2 uses PixelCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref> in its prior for latent variables up to 128\u00d7128 dims that  ll similar to Fig. <ref type=\"figure\">3 (a)</ref> with the masking mechanism introduced in PixelCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>. In the autoregressive cell, BN is replaced with WN, and S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n task, the quality of generated summaries can be improved <ref type=\"bibr\" target=\"#b35\">[34]</ref><ref type=\"bibr\" target=\"#b36\">[35]</ref><ref type=\"bibr\" target=\"#b37\">[36]</ref>. PEGASUS <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nomize facial expressions and gestures for emotion-and deceit-related applications. Bartlett et al. <ref type=\"bibr\" target=\"#b54\">[54]</ref> introduced a real-time system to identify deceptive behavi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b31\">32]</ref> data using GNNs, such as HAN <ref type=\"bibr\" target=\"#b38\">[39]</ref> and MAGNN <ref type=\"bibr\" target=\"#b10\">[11]</ref>. These GNN-based heterogeneous models can be interpreted a  attribute information of actors and directors will also have a great impact on node analysis tasks <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The missing attributes will significantly affect the perf >[30,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> based neighbors in a hierarchical manner. MAGNN <ref type=\"bibr\" target=\"#b10\">[11]</ref> model which contains the node content transformation to en sed neighbors aggregation, and heterogeneous types combination.  <ref type=\"formula\">6</ref>) MAGNN <ref type=\"bibr\" target=\"#b10\">[11]</ref>: a heterogeneous GNN. This model realizes the prediction t  Mechanism</head><p>For the case that some types of nodes have no attributes, some previous studies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  larger model size.</p><p>Balancing effectiveness and efficiency has been a line of recent research <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta us on database-related methods, such as pruning and indexing to speed-up retrieval of related items <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, using fast models. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vie reviews (MR) <ref type=\"bibr\" target=\"#b36\">(Pang &amp; Lee, 2005)</ref>.</p><p>Comparison with <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref>: <ref type=\"bibr\" target=\"#b23\">Kalant , 2005)</ref>.</p><p>Comparison with <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref>: <ref type=\"bibr\" target=\"#b23\">Kalantidis et al. (2020)</ref> also consider ways to sample negatives. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b40\">42]</ref>.</p><p>Single image target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" tar tep. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images <ref type=\"bibr\" target=\"#b22\">[24]</ref>. (d) Iterative up and downsampling approach is proposed by can preserve HR components better.</p><p>(c) Progressive upsampling was recently proposed in LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>. It progressively reconstructs the multiple SR images with N <ref type=\"bibr\" target=\"#b20\">[22]</ref>, DRRN <ref type=\"bibr\" target=\"#b40\">[42]</ref>, LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>) on Set5 dataset for 4\u00d7 enlargement.</p><p>the-art methods N <ref type=\"bibr\" target=\"#b20\">[22]</ref>, DRRN <ref type=\"bibr\" target=\"#b40\">[42]</ref>, LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>, and EDSR <ref type=\"bibr\" target=\"#b28\">[30]</ref>. We ca step. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images<ref type=\"bibr\" target=\"#b22\">[24]</ref>. (d) Iterative up and downsampling approach is proposed by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef>. Where the TMO model creates new ORB services to provide its time-based invocation capabilities <ref type=\"bibr\" target=\"#b26\">[27]</ref>, TAO provides a subset of these capabilities by extending . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2020)</ref>: Nodewise sampling <ref type=\"bibr\" target=\"#b14\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b26\">Markowitz et a ying message passing implementation. GAS revisits and generalizes the idea of historical embeddings <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>, which are defined as node embeddings acqui  approximate their embeddings via historical embeddings acquired in previous iterations of training <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>, denoted by h( ) w . After each step of tra e model weights are kept fixed, h( ) v eventually equals h ( ) v after a fixed amount of iterations <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org  an affordable approximation. The idea of historical embeddings was originally introduced in VR-GCN <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>. VR-GCN aims to reduce the variance in esti >(Chen et al., 2018a)</ref>, LADIES <ref type=\"bibr\" target=\"#b53\">(Zou et al., 2019)</ref>, VR-GCN <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018b)</ref>, MVS-GNN <ref type=\"bibr\" target=\"#b6\">(Con 2\">(Wu et al., 2019)</ref>, SIGN <ref type=\"bibr\" target=\"#b11\">(Frasca et al., 2020)</ref> and GBP <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2020a)</ref>. Since results are hard to compare across d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: onization quality, Action Units (AU) detection <ref type=\"bibr\" target=\"#b2\">[3]</ref> (by OpenFace <ref type=\"bibr\" target=\"#b3\">[4]</ref>) for facial action coding consistency between source and gen dence values are better.</p><p>Besides, we employ an action units (AU) detection module by OpenFace <ref type=\"bibr\" target=\"#b3\">[4]</ref> to compute the facial action units intensities for the sourc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: The field that gathers all these questions under a common umbrella is graph signal processing (GSP) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>While the pr of the prior work that is more directly connected and in the spirit of signal processing on graphs, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We organize the di /ref>. We organize the discussion along two main lines; some parts of the exposition follow closely <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>.</p><p>1) From al te the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>. This choice is s /ref> studies time signals. Graph signal processing (GSP)<ref type=\"foot\" target=\"#foot_0\">1</ref>  <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t erpretation of DSP can be extended to develop a linear time shift invariant Graph Signal Processing <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Consider now a graph signal s \u2208 C N , where the entries of  lized Laplacian L = D \u22121/2 LD \u22121/2 .</formula><p>The adjacency matrix A can be adopted as the shift <ref type=\"bibr\" target=\"#b1\">[2]</ref> for this general graph. Other choices have been proposed, in nt if it commutes with the shift,</p><formula xml:id=\"formula_18\">AH = HA.</formula><p>As proven in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, if the characteristic polynomial p A (z) and the minimum po aph filtering to two graph Fourier transforms and a pointwise multiplication in the spectral domain <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With a notion of frequency we can now consider the GSP equi Section II-C). If these conditions do not hold, the Jordan canonical form is used to obtain the GFT <ref type=\"bibr\" target=\"#b1\">[2]</ref>, but this is well known to be a numerically unstable procedu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esses the destination pointer array and retrieves the successor of I5's destination register, I7.R3 <ref type=\"bibr\" target=\"#b5\">( 6 )</ref>, which aims to speculatively wake up I7.src3 in the next c g, dataflow architectures adopt new instruction format to express dataflow dependencies of programs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b11\">[12]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>, extends the vanilla CF framework by incorporating multimoda m visual representations for learning style features of items and sensing preferences of users. ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref> introduced item-level and component-level attention model fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e system is still a text-based MC system. <ref type=\"bibr\" target=\"#b2\">Chuang et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b12\">Kuo, Luo, and Chen (2020)</ref> studied the end-to-end spoken questio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: epeating the procedure with new z + . The importance of iterative retrieval is highlighted by CogQA <ref type=\"bibr\" target=\"#b12\">[13]</ref>, as the answer sentence fails to be directly retrieved by  re information from new blocks in z + , which is neglected by previous multi-step reasoning methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ s. Previous methods usually leverage the graph structure between key entities across the paragraphs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. However, if we ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vantages in processing sequence information <ref type=\"bibr\" target=\"#b39\">[39]</ref>. Zhang et al. <ref type=\"bibr\" target=\"#b40\">[40]</ref> builds a quantum-like multimodal network (QMN), which uses. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atch from the surrounding patches by solving a large ridge regression problem extremely efficiently <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. It has proved to be r to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>.</p></div> <div xmln p://www.tei-c.org/ns/1.0\"><head n=\"2.\">Related work</head><p>Since the seminal work of Bolme et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the Correlation Filter has enjoyed great popularity within  To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window <ref type=\"bibr\" target=\"#b3\">[4]</ref> and the final template is cropped <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b11\">[8]</ref>, along with its representative updated descendants, e.g. Fast R-CNN <ref type=\"bibr\" target=\"#b10\">[7]</ref> and Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[26]</ref>, l methods, which opens the deep learning era in object detection. Its descendants (e.g., Fast R-CNN <ref type=\"bibr\" target=\"#b10\">[7]</ref>, Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[26]</ref>) up. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \u6ee1\u6c5f\u7ea2), Shuidiaogetou(\u6c34\u8c03\u6b4c\u5934), etc .</p><p>Various methods e.g., <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> have been proposed to generate classical Chinese poetry. Howe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of localized spectral filters on graphs <ref type=\"bibr\" target=\"#b11\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b5\">Defferrard et al., 2016)</ref>.</p></div> <div xmlns=\"http://www.tei-c er neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. <ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> use this K-localized convolution to def pectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by<ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> with fast localized convolutions. In co  introduced to the original frameworks of<ref type=\"bibr\" target=\"#b3\">Bruna et al. (2014)</ref> and<ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> that improve scalability and classifica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eter that controls the effects of RD. The base recommender can be any existing RS model such as BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>, NeuMF <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and L \ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 . The output of the mapping function can be a separate representation of a user, an item (e.g., BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>) or their combined representation (e.g., NeuMF <ref type=\" p learning model that are broadly used for top-\ud835\udc41 recommendation with implicit feedback.</p><p>\u2022 BPR <ref type=\"bibr\" target=\"#b20\">[21]</ref>: A learning-to-rank model for implicit feedback. It assume  Foursquare as done in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Data statistics are summarized in Table <ref type=\"table\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st cases. Concretely, STAN outperforms RNN-based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, because it eff ng approaches <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b51\">[51]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref> on the THUMOS20 \">[14]</ref>, <ref type=\"bibr\" target=\"#b51\">[51]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref> on the THUMOS2014 dataset. It is interesting to notice tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b44\">[46,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b51\">53]</ref> is widely employed to transform the local neighborhood of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  similar to that extracted from another basic feature. To achieve this, inspired by the center loss <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we develop a compactness loss. The compactness loss L C l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  (2) a machine reader can thoroughly examine the retrieved contexts and identify the correct answer <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref>. Although reducing open-domain QA to machine ent that can select a small set of relevant texts, before applying the reader to extract the answer <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref>. <ref type=\"foot\" target=\"#foot_1\">4</ref>Fo e source documents for answering questions. We first apply the pre-processing code released in DrQA <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017)</ref> to extract the clean, text-portion of articl o-end QA results, measured by exact match with the reference answer after minor normalization as in <ref type=\"bibr\" target=\"#b5\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Lee et al., 20 ike TF-IDF or BM25 have been used as the standard method applied broadly to various QA tasks (e.g., <ref type=\"bibr\" target=\"#b5\">Chen et al., 2017;</ref><ref type=\"bibr\">Yang et al., 2019a,b;</ref><r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t-flips in <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, a newly proposed Bit-Flip Attack (BFA) <ref type=\"bibr\" target=\"#b16\">[17]</ref> whose progressive bit searching algorithm can successfully ial Weight Attack</head><p>The bit-flip based adversarial weight attack, aka. Bit-Flip Attack (BFA) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, is an adversarial attack variant which performs weight fa loss increment. Thus, the bit searching in iteration i can be formulated as an optimization process <ref type=\"bibr\" target=\"#b16\">[17]</ref>:</p><formula xml:id=\"formula_0\">max { Bi l } L f x; { Bi l e acceleration of modern AI applications. To clarify, we use the same threat model as in prior work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which is listed in Table <ref type=\"table\">1</ref>.</p></ cted in Fig. <ref type=\"figure\" target=\"#fig_2\">2</ref>, the progressive bit search proposed in BFA <ref type=\"bibr\" target=\"#b16\">[17]</ref> is prone to identify vulnerable bit in the weight whose ab .   BFA Configuration. To evaluate the effectiveness of the proposed defense methods, the code from <ref type=\"bibr\" target=\"#b16\">[17]</ref> is utilized with further modification. The number of bit-f  trials. Note that, all the quantized DNN reported hereafter still uses the uniform quantizer as in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, but with quantization-aware training instead of post-trai ns/1.0\"><head n=\"5.3.\">Comparison of Alternative Defense Methods</head><p>Adversarial weight attack <ref type=\"bibr\" target=\"#b16\">[17]</ref> is a recently developed security threat model for modern D Trained adversarial defense <ref type=\"bibr\" target=\"#b14\">[15]</ref> with strong weight attack BFA <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Again, adversarial input defense fails to defend BFA, req. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b22\">(Sutton et al. 2000)</ref> and the monto-carlo based policy gradient method <ref type=\"bibr\" target=\"#b27\">(Williams 1992</ref>) to sample M action-state trajectories, based on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet <ref type=\"bibr\" target=\"#b17\">[10]</ref>, which forms the sentiment view. Then, using a framework o ment aspect of the associate text. For this, we use an external knowledge base, called SentiWordNet <ref type=\"bibr\" target=\"#b17\">[10]</ref>. It is based on the well-known English lexical dictionary  ures with the mid-level features (denoted as Low&amp;SentiBank), and a textual feature-based method <ref type=\"bibr\" target=\"#b17\">[10]</ref> (denoted as SentiStrength<ref type=\"foot\" target=\"#foot_3\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ine learning tasks involve graph structured datasets, such as classifying posts in a social network <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, predicting interfaces between protein o aggregate a local set of lower-level features. We refer to such an operator as a graph aggregator <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> and the set of local nodes as the rece ref type=\"bibr\" target=\"#b15\">Kipf and Welling, 2017</ref>) can be interpreted as graph aggregators <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>.</p><p>Graph aggregators are the basic t to the inductive node classification problem. We also improve the sampling strategy introduced in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> to reduce the memory cost and increase oral forecasting problem. Extensive experiments on two node classification datasets, PPI and Reddit <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, and one traffic speed forecasting dat rtional to the total number of nodes, which could be hundreds of thousands of nodes in large graphs <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computatio \"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computational cost, <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> proposed the GraphSAGE framework that  ble and the goal is to predict the labels of the unseen testing nodes. Our approach follows that of <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, where a mini-batch of nodes are sampl dels in our framework and a two-layer fully connected neural network on the PPI and Reddit datasets <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. The five baseline aggregators include  the effectiveness of incorporating graph structures, we also evaluate a two-layer fully-connected  <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> (61.2)<ref type=\"foot\" target=\"#foot_0  hyperparameters for training. The training, validation, and testing splits are the same as that in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. The micro-averaged F1 score is used t ith the previous state-of-the-art methods on inductive node classification. This includes GraphSAGE <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, GAT <ref type=\"bibr\" target=\"#b27\">(V  can see steady improvement with larger sampling sizes, which is consistent with the observation in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>.</p><p>Effect of output dimensions in  r\" target=\"#b15\">Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b8\">Fout et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b27\">Veli\u010dkovi d on either pooling over neighborhoods <ref type=\"bibr\" target=\"#b15\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a)</ref> or computing a weighted sum of the neig rget=\"#b7\">(Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a)</ref>, while others integrated edge features . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#b51\">[52]</ref>, in which each network block consists of different convolutional kernels. ResNeXt <ref type=\"bibr\" target=\"#b60\">[61]</ref> adopts group convolution <ref type=\"bibr\" target=\"#b33\">[3 ight) depicts an overview of a Split-Attention Block.</p><p>Feature-map Group. As in ResNeXt blocks <ref type=\"bibr\" target=\"#b60\">[61]</ref>, the feature can be divided into several groups, and the n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe  going through an intermediate text transcript. There are many advantages of end-to-end SLU systems <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the most significant of which is that E2E systems can direc  trained on increasingly relevant data until it is fine-tuned on the actual domain data. Similarly, <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> advocate pre-trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b8\">[10,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref> and analysis tools <ref type=\"bibr\" target=\"#b28\">[30]</ref> for DNNs analyze a broad space of software mappings of a D n its analytical network-on-chip (NoC) model based on a pipe model similar to other analytic models <ref type=\"bibr\" target=\"#b28\">[30]</ref>. The pipe model utilizes two parameters, the pipe width (b et=\"#b10\">[12,</ref><ref type=\"bibr\" target=\"#b23\">25,</ref><ref type=\"bibr\" target=\"#b24\">26,</ref><ref type=\"bibr\" target=\"#b28\">30]</ref>. These data-centric directives can express a wide range of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: can be summarized as the paths connecting the target user and item based on historical interactions <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. For example, give al behaviors of users reflect personal interests; meanwhile, the user groups can also profile items <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Hence, in each mo e d \u2032 m is the transformation size; and we select LeakyReLU(\u2022) as the nonlinear activation function <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Such aggregation  y based on CF models <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b38\">[38]</ref><ref type=\"bibr\" target=\"#b39\">[39]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: kloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We adopt larger input data sets varying from 147 to 187 G  both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in the memory system. And for each workload, we collect t  performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type=\"bibr\" target=\"#b16\">[17]</ref>).</p><p>We find that big data analytics applications share chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type=\"bibr\" target=\"#b16\">[17]</ref>) workloads. Meanwhile the service workloads in data center s, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>\u2022 Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and service workloa the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type=\"bibr\" target=\"#b16\">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall b vel cache), respectively. For the service workloads, our observations corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><p>\u2022 For the big data ana rk of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, CloudSuite paper is biased towards online servic . The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, our approach are more pragmatic. We adopt a larger data i  in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in memory. The number of instructions retired of the big  , HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and compared them with big data analytics workloads.</p>< as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the o n fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The front end inefficiency may caused by high-level langu  Figure <ref type=\"figure\" target=\"#fig_3\">5</ref>.</p><p>Implications: Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and the service wor ns, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and large binary size complicated by high-level language, sor and save the die area. For the service workloads, our observation corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id=\"formula_ s in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type=\"bibr\" target=\"#b16\">[17]</ref>. According to our correlation analysis in this section, ar ten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Moreover, for mod s prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and etc. Narayanan et al. <ref type=\"bibr\" target=\"#b32\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Deep Supervision</head><p>We propose to use deep supervision <ref type=\"bibr\" target=\"#b5\">[6]</ref> in UNet++, enabling the model to operate in two modes: (1) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: (2020)</ref> trained a discriminative model for language representation learning. Recent literature <ref type=\"bibr\" target=\"#b17\">(Peng et al., 2020)</ref> has also attempted to relate the contrastiv elate the contrastive pre-training scheme to classical supervised RE task. Different from our work, <ref type=\"bibr\" target=\"#b17\">Peng et al. (2020)</ref> aims to utilize abundant DS data and help cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: In the context of maintaining multiple versions of the data in a B + -tree, it has been proposed in <ref type=\"bibr\" target=\"#b3\">[3]</ref> to use, where possible, the free space in nodes. Other studi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar st inference times <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Thus far, realistic textures in the context of high-magnif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >2007), most Natural Language Processing (NLP) systems fail when processing corrupted or noisy text <ref type=\"bibr\" target=\"#b6\">(Belinkov and Bisk, 2018)</ref>. Although this problem is not new to N bibr\" target=\"#b44\">(Smith, 2007)</ref>. Moreover, we employed two sets of misspellings released by <ref type=\"bibr\" target=\"#b6\">Belinkov and Bisk (2018)</ref> and <ref type=\"bibr\" target=\"#b38\">Pikt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cations in various fields with semantic data, such as images <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, texts <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . QuO's QDLs are based on the separation of concerns advocated by Aspect-Oriented Programming (AoP) <ref type=\"bibr\" target=\"#b22\">[23]</ref>. The QuO middleware adds significant value to adaptive rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o-rank (LTR) is challenging due to its biased nature. To address this bias problem, Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref> proposed a counterfactual inference approach, providing an f type=\"bibr\" target=\"#b18\">[18]</ref>.</p><p>To handle biases in a principled way, Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref> introduced an unbiased learning-to-rank framework, which i ecting the examination bias in learning to rank from implicit feedback. As shown by Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref>, the parameters of the PBM can serve as propensity estimat ent d for query q.</p><p>While Pr(E = 1|k) can be used as an estimate of the examination propensity <ref type=\"bibr\" target=\"#b19\">[19]</ref>, it is a rather simplistic model since it assumes that exa max ]. In this case, randomly swapping results at positions k and k \u2032 before presenting the ranking <ref type=\"bibr\" target=\"#b19\">[19]</ref> makes the expected relevance of results at the two positio ck-through rates is a consistent estimator of the relative propensities p k and p k \u2032 under the PBM <ref type=\"bibr\" target=\"#b19\">[19]</ref>. Note that knowing the relative propensities with respect   sufficient, since the counterfactual ERM learning objective is invariant to multiplicative scaling <ref type=\"bibr\" target=\"#b19\">[19]</ref>.</p><p>While this ratio estimator is a sensible approach f interventions were then used to get a gold-standard estimate of the propensities via the methods in <ref type=\"bibr\" target=\"#b19\">[19]</ref>. To avoid any confounding due to changes in the query dist  <ref type=\"table\" target=\"#tab_0\">1</ref>. We then use the gold-standard propensity estimator from <ref type=\"bibr\" target=\"#b19\">[19]</ref> to learn two PBM models from the swap intervention data, o be expected, given that AllPairs makes more efficient use of the data than the ratio-estimates from <ref type=\"bibr\" target=\"#b19\">[19]</ref>.</p><p>Can AllPairs learn CPBM models with many context fe nce compared to using the propensities from the PBM.</p><p>We trained a Clipped Propensity SVM-Rank <ref type=\"bibr\" target=\"#b19\">[19]</ref> for each of the following three propensity models: PBM est ed via cross-validation. For rank r &gt; 21, we impute the propensity p r (x) = p 21 (x). Following <ref type=\"bibr\" target=\"#b19\">[19]</ref>, we measure test-set ranking performance via the average s imitations of existing propensity estimation methods for LTR <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref>. First, existing me or LTR algorithms like <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b19\">19]</ref>. We evaluate the fidelity of the CPBM model and the effecti  <ref type=\"bibr\" target=\"#b23\">[23]</ref>. The most effective methods use randomized interventions <ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref>, which unfortunate first review how explicit interventions have been used for estimating p k := Pr(E = 1|k) in the PBM <ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref>. The PBM requires . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: key-blocks assumption is strongly related to latent variable models, which are usually solved by EM <ref type=\"bibr\" target=\"#b10\">[11]</ref> or variational bayes <ref type=\"bibr\" target=\"#b18\">[19]</ e viewed as a generalization of (conditional) latent variable model p(y|x; \u03b8) \u221d p(z|x)p(y|z; \u03b8). EM <ref type=\"bibr\" target=\"#b10\">[11]</ref> infers the distribution of z as posterior p(z|y, x; \u03b8) in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, graph classification <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, prediction of mol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: information retrieval community and search engine industry as the next generation search technology <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>In general, a search engine comprises a recall laye l in Facebook search is not a text embedding problem, as is actively researched in the IR community <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Instead it is a more complex problem that requires unders. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  as input.</p><p>Networks <ref type=\"bibr\" target=\"#b32\">[33]</ref> and Residual Networks (ResNets) <ref type=\"bibr\" target=\"#b10\">[11]</ref> have surpassed the 100-layer barrier.</p><p>As CNNs become d (or beginning) of the network. Many recent publications address this or related problems. ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Highway Networks <ref type=\"bibr\" target=\"#b32\">[33]</ uent layer. It changes the state but also passes on information that needs to be preserved. ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> make this information preservation explicit through additi tor that eases the training of these very deep networks. This point is further supported by ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref>, in which pure identity mappings are used as bypassing pat ng image recognition, localization, and detection tasks, such as ImageNet and COCO object detection <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Recently, stochastic depth was proposed as a way to succe =\"#b15\">[16]</ref>, which gives rise to the following layer transition: x \u2113 = H \u2113 (x \u2113\u22121 ). ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> add a skip-connection that bypasses the non-linear transfo ates that they do not suffer from overfitting or the optimization difficulties of residual networks <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>Parameter Efficiency. The results in Table <ref typ s, it typically has many more inputs. It has been noted in <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> that a 1\u00d71 convolution can be introduced as bottleneck laye a standard data augmentation scheme (mirroring/shifting) that is widely used for these two datasets <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta rget=\"#b11\">12]</ref>, and apply a single-crop or 10-crop with size 224\u00d7224 at test time. Following <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta  the same data augmentation scheme for training images as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, and apply a single. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ustness for classical neural networks/robust training (e.g. <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>), we tackle various ertifiable robustness <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> providing guarantee ginal sample measured by, e.g., the infinity-norm or L2-norm <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, often e.g. \u03f5 &lt;  /p><p>For this work, specifically the class of methods based on convex relaxations are of relevance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. They construct a   the remaining layers, since the input to them is no longer binary, we adapt the bounds proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Generalized to the GNN we therefore obtain:</p><formula x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[14]</ref> proposed a DSM fusion (DSMF) branch structure model.</p><p>Pyramid pooling layer (PPL) <ref type=\"bibr\" target=\"#b14\">[15]</ref> and DeepLab <ref type=\"bibr\" target=\"#b15\">[16]</ref> have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Earlier works treated user-item implicit feedback as a use tite graph structure <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. These neural graph models perform graph convolution by upd raph based CF models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we set the embedding propagation depth \ud835\udc3e in range of {0,1, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Pin-Sage <ref type=\"bibr\" target=\"#b37\">[38]</ref>, NGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Light der collaborative signals for better user/item embedding learning with iterative graph convolutions <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Pin-Sage took item-item correlation graph with item conte \">[26]</ref>. The second is neural graph based CF models with fixed graph structure, including NGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, BiGI <ref type=\"bibr\" target=\"#b2\">[3]</ref>, LR-GCCF <re et=\"#b17\">[18]</ref>. Extensive works show the effectiveness of GCNs on graph based recommendations <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" t static pre-trained models <ref type=\"bibr\" target=\"#b16\">[17]</ref>. In recent work, Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed Boundary Attack, which generates adversarial exam ttacks</head><p>Most related to our work is the Boundary Attack method introduced by Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Boundary Attack is an iterative algorithm based on reject s: We compare HopSkipJumpAttack with three state-of-the-art decision-based attacks: Boundary Attack <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Limited Attack <ref type=\"bibr\" target=\"#b8\">[9]</ref> an ibr\" target=\"#b5\">[6]</ref>. A version normalized by image dimension was employed by Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> for evaluating Boundary Attack. The As an alternative metr <formula xml:id=\"formula_38\">|E[\u03c6 x (x t + \u03b4 t u)]| &gt; 0,</formula><p>as we can see from Equation <ref type=\"bibr\" target=\"#b13\">(14)</ref>. To attempt to control the variance, we introduce a baseli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y or with only very few labeled examples <ref type=\"bibr\" target=\"#b26\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze, 2020a)</ref>.</p><p>Very recently, <ref type=\"bib mited to a few hundred tokens.</p><p>An alternative to priming is pattern-exploiting training (PET) <ref type=\"bibr\" target=\"#b30\">(Schick and Sch\u00fctze, 2020a)</ref>, which combines the idea of reformu  are understood well by LMs is difficult <ref type=\"bibr\" target=\"#b12\">(Jiang et al., 2019)</ref>, <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref> propose PET, a method that uses know =\"#b4\">(Dagan et al., 2006)</ref> are textual entailment tasks like MNLI, so we use PVPs similar to <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>. For a premise p and hypothesis h, w ts. 5 We run PET on the FewGLUE training sets for all SuperGLUE tasks using the exact same setup as <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>. For COPA, WSC and ReCoRD, we use ou ref>). Given 32 examples, PET clearly outperforms both baselines, which is in line with findings by <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2020a)</ref>.</p><p>We next compare PET directly  t segments and mark p in s with asterisks.</p><p>MultiRC Deviating from the hyperparameters used by <ref type=\"bibr\" target=\"#b30\">Schick and Sch\u00fctze (2019)</ref>, we use a maximum sequence length of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ther domain where pre-training is usually adopted. The shallow pre-training methods, e.g., word2vec <ref type=\"bibr\" target=\"#b22\">[23]</ref> and GloVe <ref type=\"bibr\" target=\"#b24\">[25]</ref>, learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gh a localized first-order approximation, named graph convolutional networks (GCN). Hamilton et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> propose the GraphSAGE framework based on node sampling and (e s , r, e o ) = \u03c3 (\u03d5(e s , r, e o ) + b) \u2208 (0, 1)</p><p>where b is a bias term, and \u03c3 (x) = 1/(   <ref type=\"bibr\" target=\"#b19\">(20)</ref> in which</p><formula xml:id=\"formula_34\">y = 1 if (e s , r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mains like speech recognition <ref type=\"bibr\" target=\"#b0\">[1]</ref> and visual object recognition <ref type=\"bibr\" target=\"#b1\">[2]</ref>, have achieved a dramatic improvement out of the state-of-th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b47\">Rong et al., 2020b)</ref>, and other tricks <ref type=\"bibr\" target=\"#b58\">(Wang, 2021;</ref><ref type=\"bibr\" target=\"#b28\">Huang et al., 2021)</ref> are orthogonal to the contribution of the G. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xml:id=\"formula_0\">[McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94]</formula><p>Balanced scheduling <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> is an algorithm that can generate schedules that adapt m iss ratios, assuming a workstation-like memory model in which cache misses are normally distributed <ref type=\"bibr\" target=\"#b12\">[KE93]</ref>.</p><p>Since its success depends on the amount of instru no reuse information), which are balanced scheduled.</p><p>The original work on balanced scheduling <ref type=\"bibr\" target=\"#b12\">[KE93]</ref>, which compared it to the traditional approach and witho  produces schedules that are independent of the memory system implementation.</p><p>Initial results <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> indicated that balanced scheduling produced speedups ave results from the original comparison of balanced and traditional scheduling (without optimizations) <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> illustrates both the limitations of simple architecture . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b22\">[25]</ref>, or with pattern-based models such as discrete choice modelling (DCM) <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" targ tability of the trajectories predicted by hand-crafted models, in particular discrete choice models <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>, and the high accura te choice modelling uti-lizes a grid for selecting the next action, but relative to each individual <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50,</ref><ref type=\"bibr\" targ iors for human motion have been described in DCM literature, we follow the formulation described in <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>, which is well adapt all exponential parameters of the chosen hand-designed functions are set to the estimated values in <ref type=\"bibr\" target=\"#b4\">[7,</ref><ref type=\"bibr\" target=\"#b47\">50]</ref>. For synthetic data, atical formulations of the above functions are detailed in <ref type=\"bibr\" target=\"#b47\">[50,</ref><ref type=\"bibr\" target=\"#b4\">7]</ref>. Each person is assumed to select the anchor a k for which th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s.</p><p>The idea of extracting features for NLP using convolutional DNN was previously explored by <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>, in the context of POS tagging, chunking  Recognition (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. In <ref type=\"bibr\" target=\"#b3\">(Collo tation component, each input word token is transformed into a vector by looking up word embeddings. <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> reported that word embeddings learned fr ural network, the convolution approach is a natural method to merge all of the features. Similar to <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>, we first process the output of Window P , we heuristically choose d e = 5. Finally, the word dimension and learning rate are the same as in <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. Table <ref type=\"table\">2 reports</ref> ons.org/licenses/by/4.0/ 1 A word embedding is a distributed representation for a word. For example,<ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> use a 50-dimensional vector to represent -words model</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"3\" xml:id=\"foot_1\"><ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> proposed a pairwise ranking approach to  ares similar intuition with that of <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. In <ref type=\"bibr\" target=\"#b3\">(Collobert et al., 2011)</ref>, all of the tasks are considered as the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>.</p><p>This fetch  div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Fetch Models</head><p>The stream fetch engine <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> model is shown in  for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. However, having h ww.tei-c.org/ns/1.0\"><head n=\"5.1\">The Multiple Stream Predictor</head><p>The next stream predictor <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, which is shown in zed code layout. In addition, data are shown for the original single-stream predictor, described in <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, and a 2-stream mu p>To avoid this increase in the fetch engine complexity, we propose to use long instruction streams <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> as basic predictio /p><p>Our instruction cache setup uses wide cache lines, that is, 4 times the processor fetch width <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and 64KB total hardware budget. The trace fetch architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: academic communities <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" tar /head><p>GCNs are showing great potential in various tasks <ref type=\"bibr\" target=\"#b16\">[16,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" tar ted by recursively aggregating and transforming the representation vectors of its neighbor vertices <ref type=\"bibr\" target=\"#b18\">[18,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" ta to sample a subset from the neighbor vertices of each vertex <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b18\">18]</ref> as the new neighbors, specifically,</p><formula xml:id=\"for ling to alleviate receptive field expansion that effectively trades off accuracy and execution time <ref type=\"bibr\" target=\"#b18\">[18]</ref>. It is formulated as</p><formula xml:id=\"formula_4\">a k v  ing preprocessing <ref type=\"bibr\" target=\"#b20\">[20]</ref> or with random selection during runtime <ref type=\"bibr\" target=\"#b18\">[18]</ref>. Aggregation aggregates the features from its 1-hop neighb he execution time breakdown of GCN (GCN) <ref type=\"bibr\" target=\"#b25\">[25]</ref>, GraphSage (GSC) <ref type=\"bibr\" target=\"#b18\">[18]</ref>, and GINConv (GIN) <ref type=\"bibr\" target=\"#b39\">[39]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and work through the challenges that they face <ref type=\"bibr\" target=\"#b1\">(Burleson, 2003;</ref><ref type=\"bibr\" target=\"#b10\">Langford et al., 1997;</ref><ref type=\"bibr\" target=\"#b3\">Heaney and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. To provide personalized information accurately, the recomm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
