{"content": "The context is:  pre-trained LMs were partially equipped with a specific type of relational knowledge. Furthermore, <ref type=\"bibr\" target=\"#b5\">[Liu et al., 2020]</ref> observed that the incorporation of excessive  ma of negative knowledge infusion is still not well understood. Our work is motivated by approaches <ref type=\"bibr\" target=\"#b5\">[Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b6\">Petroni et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ]</ref> observes that excessive knowledge incorporation could divert the context representation and <ref type=\"bibr\" target=\"#b0\">Bian et al. [2021]</ref> finds that context-sensitive knowledge select. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s this issue, several works have attempted to integrate knowledge graphs (KGs) into pre-trained LMs <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Levine et al. directions for knowledge-driven tasks. Such methods generally retrieve pre-trained graph embeddings <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> or a KG subgraph via entity linking during f external knowledge into a pre-trained LM followed by finetuning to target tasks is more efficient <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>.</p><p>To a certain extent, knowledge infu ired knowledge-enhanced models including ERNIE (Tsinghua)<ref type=\"foot\" target=\"#foot_1\">1</ref>  <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>, ERNIE (Baidu) <ref type=\"bibr\">[Sun et al  target=\"#b9\">[Wang et al., 2020], and</ref><ref type=\"bibr\">CoLAKE [Sun et al., 2020]</ref>. ERNIE <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> injects relational knowledge into the pret  and whether it has a negative impact on task performance. We design two experiments based on ERNIE <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> and KnowBERT <ref type=\"bibr\" target=\"#b6\" el's parameters.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>K-Adaptor</head><p>ERNIE* <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>. Here the model ERNIE* refers to the resul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sks. However, open issues remain as these approaches lack domain-specific knowledge. Recent methods <ref type=\"bibr\" target=\"#b6\">[Peters et al., 2019]</ref> have revealed that the performance of the  #b12\">[Zhang et al., 2019]</ref>, ERNIE (Baidu) <ref type=\"bibr\">[Sun et al., 2019]</ref>, KnowBERT <ref type=\"bibr\" target=\"#b6\">[Peters et al., 2019]</ref>, WKLM <ref type=\"bibr\" target=\"#b10\">[Xion into the pretrained model BERT, which aligns entities from Wikipedia to facts in WikiData. KnowBERT <ref type=\"bibr\" target=\"#b6\">[Peters et al., 2019]</ref> incorporates external KGs into BERT with a o experiments based on ERNIE <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> and KnowBERT <ref type=\"bibr\" target=\"#b6\">[Peters et al., 2019]</ref> for evaluation 2 .</p></div> <div xmlns=\"h o our implementation results, which has the same amount of tuning with ERNIE+SI+SR.</p><p>KnowBERT* <ref type=\"bibr\" target=\"#b6\">[Peters et al., 2019]</ref>. Here the model Know-BERT* refers to the r br\" target=\"#b12\">[Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Levine et al., 2020;</ref><ref type=\"bibr\" target=\"#b6\">Peters et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Xiong et al., amount of external knowledge for effective infusion remains to be well understood. In recent years, <ref type=\"bibr\" target=\"#b6\">[Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Broscheit, 2 #b0\">Bian et al. [2021]</ref> finds that context-sensitive knowledge selection is critical, whereas <ref type=\"bibr\" target=\"#b6\">[Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Broscheit, 2 irrelevant knowledge may hinder the performance. Firstly, it should be noted that recent approaches <ref type=\"bibr\" target=\"#b6\">[Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Broscheit, 2 derstood. Our work is motivated by approaches <ref type=\"bibr\" target=\"#b5\">[Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b6\">Petroni et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Broscheit, 20 vant knowledge for downstream tasks, we analyze both the weights and features with principal angles <ref type=\"bibr\" target=\"#b6\">[Rebuffi et al., 2017]</ref>, which have been introduced to measure th at the lower layers (4th layer) have small relative angles, which is consistent with the finding in <ref type=\"bibr\" target=\"#b6\">[Rogers et al., 2020]</ref>. It is natural that lower layers are more . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref> observes that excessive knowledge incorporation could divert the context representation and <ref type=\"bibr\" target=\"#b0\">Bian et al. [2021]</ref> finds that context-sensitive knowledge select. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Datasets and Setup</head><p>TACRED <ref type=\"bibr\" target=\"#b11\">[Zhang et al., 2017]</ref>  In this case, \u03b7 was set to 0.001, k was s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as rarely been considered. Moreover, our work is inspired by negative transfer in transfer learning <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2019a]</ref> as they both follow a pretrain-finetune par n much performance gains compared with RoBERTa-base. Note that those tasks are not knowledge-driven <ref type=\"bibr\" target=\"#b2\">[Devlin et al., 2018]</ref> which requires linguistic representations . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s this issue, several works have attempted to integrate knowledge graphs (KGs) into pre-trained LMs <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Levine et al. directions for knowledge-driven tasks. Such methods generally retrieve pre-trained graph embeddings <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> or a KG subgraph via entity linking during f external knowledge into a pre-trained LM followed by finetuning to target tasks is more efficient <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>.</p><p>To a certain extent, knowledge infu ired knowledge-enhanced models including ERNIE (Tsinghua)<ref type=\"foot\" target=\"#foot_1\">1</ref>  <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>, ERNIE (Baidu) <ref type=\"bibr\">[Sun et al  target=\"#b9\">[Wang et al., 2020], and</ref><ref type=\"bibr\">CoLAKE [Sun et al., 2020]</ref>. ERNIE <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> injects relational knowledge into the pret  and whether it has a negative impact on task performance. We design two experiments based on ERNIE <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> and KnowBERT <ref type=\"bibr\" target=\"#b6\" el's parameters.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>K-Adaptor</head><p>ERNIE* <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>. Here the model ERNIE* refers to the resul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nowledge graphs (KGs) into pre-trained LMs <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Levine et al., 2020;</ref><ref type=\"bibr\" target=\"#b6\">Peters et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s this issue, several works have attempted to integrate knowledge graphs (KGs) into pre-trained LMs <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Levine et al. directions for knowledge-driven tasks. Such methods generally retrieve pre-trained graph embeddings <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> or a KG subgraph via entity linking during f external knowledge into a pre-trained LM followed by finetuning to target tasks is more efficient <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>.</p><p>To a certain extent, knowledge infu ired knowledge-enhanced models including ERNIE (Tsinghua)<ref type=\"foot\" target=\"#foot_1\">1</ref>  <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>, ERNIE (Baidu) <ref type=\"bibr\">[Sun et al  target=\"#b9\">[Wang et al., 2020], and</ref><ref type=\"bibr\">CoLAKE [Sun et al., 2020]</ref>. ERNIE <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> injects relational knowledge into the pret  and whether it has a negative impact on task performance. We design two experiments based on ERNIE <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref> and KnowBERT <ref type=\"bibr\" target=\"#b6\" el's parameters.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>K-Adaptor</head><p>ERNIE* <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2019]</ref>. Here the model ERNIE* refers to the resul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b6\">Peters et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Xiong et al., 2020;</ref><ref type=\"bibr\" target=\"#b13\">Zhang et al., 2021a]</ref>, which has shed light on promising directi head><p>Background knowledge has been considered as an indispensable part of language understanding <ref type=\"bibr\" target=\"#b13\">[Zhang et al., 2021b]</ref>, which has inspired knowledge-enhanced mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: representations. In particular, the encoder has a 3-block convolutional architecture as proposed in <ref type=\"bibr\" target=\"#b11\">[Wang et al., 2017]</ref>. For an input x, the encoder maps x into a  tion and dropout in between. Pre-norm residual connections, which can produce more stable gradients <ref type=\"bibr\" target=\"#b11\">[Wang et al., 2019]</ref>, are adopted in our Transformer. We stack L. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ls), self-supervised pretrained models can achieve comparable performance with limited labeled data <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020]</ref>. Various selfsupervised approaches relied on d a momentum encoder to learn representations of negative pairs obtained from a memory bank. SimCLR <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020]</ref> replaced the momentum encoder by using a lar  with applying a non-linear transformation to the contexts using a non-linear projection head as in <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020]</ref>. The projection head maps the contexts into  data <ref type=\"bibr\" target=\"#b6\">[Hjelm et al., 2019;</ref><ref type=\"bibr\">He et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020]</ref>. It explores different views of the input ima andard linear benchmarking evaluation scheme <ref type=\"bibr\" target=\"#b7\">[Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020]</ref>. Particularly, we train a linear classifier ( sed for time-series data. For example, <ref type=\"bibr\" target=\"#b7\">[Mohsenvand et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Cheng et al., 2020]</ref> developed contrastive learning methods for b ntation</head><p>Data augmentation is a key part in the success of the contrastive learning methods <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Grill et al., 2 th other samples. It is thus important to design proper data augmentations for contrastive learning <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Mohsenvand et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2020]</ref> replaced the momentum encoder by using a larger batch of negative pairs. Also, BYOL <ref type=\"bibr\" target=\"#b5\">[Grill et al., 2020]</ref> learned representations by bootstrapping re  success of the contrastive learning methods <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Grill et al., 2020]</ref>. Contrastive methods try to maximize the sim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of an image may deviate the model from learning features about the color or orientation of objects <ref type=\"bibr\" target=\"#b7\">[Oord et al., 2018]</ref>.</p><p>Contrastive learning has recently sho ta. So far, few works on contrastive learning have been proposed for time-series data. For example, <ref type=\"bibr\" target=\"#b7\">[Mohsenvand et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Cheng et   data augmentations for contrastive learning <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Mohsenvand et al., 2020]</ref>. Usually, contrastive learning methods  s (FD)</head><p>We conducted the transferability experiment on a real-world fault diagnosis dataset <ref type=\"bibr\" target=\"#b7\">[Lessmeier et al., 2016]</ref>. This dataset was collected under four  e the performance of our TS-TCC model, we follow the standard linear benchmarking evaluation scheme <ref type=\"bibr\" target=\"#b7\">[Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: computer vision domain because of its ability to learn invariant representation from augmented data <ref type=\"bibr\" target=\"#b6\">[Hjelm et al., 2019;</ref><ref type=\"bibr\">He et al., 2020;</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: computer vision domain because of its ability to learn invariant representation from augmented data <ref type=\"bibr\" target=\"#b6\">[Hjelm et al., 2019;</ref><ref type=\"bibr\">He et al., 2020;</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head>Epilepsy Seizure Prediction</head><p>The Epileptic Seizure Recognition dataset <ref type=\"bibr\" target=\"#b1\">[Andrzejak et al., 2001]</ref> consists of EEG recordings from 500 sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nsidered as a separate domain as it has different characteristics from the other working conditions <ref type=\"bibr\" target=\"#b8\">[Ragab et al., 2020]</ref>. Each domain has three classes, namely, two. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2020]</ref> replaced the momentum encoder by using a larger batch of negative pairs. Also, BYOL <ref type=\"bibr\" target=\"#b5\">[Grill et al., 2020]</ref> learned representations by bootstrapping re  success of the contrastive learning methods <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Grill et al., 2020]</ref>. Contrastive methods try to maximize the sim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head>Epilepsy Seizure Prediction</head><p>The Epileptic Seizure Recognition dataset <ref type=\"bibr\" target=\"#b1\">[Andrzejak et al., 2001]</ref> consists of EEG recordings from 500 sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: computer vision domain because of its ability to learn invariant representation from augmented data <ref type=\"bibr\" target=\"#b6\">[Hjelm et al., 2019;</ref><ref type=\"bibr\">He et al., 2020;</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as solving puzzles <ref type=\"bibr\">[Noroozi and Favaro, 2016]</ref> and predicting image rotation <ref type=\"bibr\" target=\"#b4\">[Gidaris et al., 2018]</ref>. However, the pretext tasks can limit the olorization <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2016]</ref> and predicting image rotation <ref type=\"bibr\" target=\"#b4\">[Gidaris et al., 2018]</ref>. Despite the good results achieved by the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e-series data than images, and little time-series data have been labeled in real-world applications <ref type=\"bibr\" target=\"#b3\">[Ching et al., 2018]</ref>. Given that deep learning methods usually r  be able to address the temporal dependencies of data, which are key characteristics of time-series <ref type=\"bibr\" target=\"#b3\">[Franceschi et al., 2019]</ref>. Second, some augmentation techniques . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head>Epilepsy Seizure Prediction</head><p>The Epileptic Seizure Recognition dataset <ref type=\"bibr\" target=\"#b1\">[Andrzejak et al., 2001]</ref> consists of EEG recordings from 500 sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as solving jigsaw puzzles <ref type=\"bibr\">[Noroozi and Favaro, 2016]</ref>, image colorization <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2016]</ref> and predicting image rotation <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as solving jigsaw puzzles <ref type=\"bibr\">[Noroozi and Favaro, 2016]</ref>, image colorization <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2016]</ref> and predicting image rotation <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head>Epilepsy Seizure Prediction</head><p>The Epileptic Seizure Recognition dataset <ref type=\"bibr\" target=\"#b1\">[Andrzejak et al., 2001]</ref> consists of EEG recordings from 500 sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e-series data than images, and little time-series data have been labeled in real-world applications <ref type=\"bibr\" target=\"#b3\">[Ching et al., 2018]</ref>. Given that deep learning methods usually r  be able to address the temporal dependencies of data, which are key characteristics of time-series <ref type=\"bibr\" target=\"#b3\">[Franceschi et al., 2019]</ref>. Second, some augmentation techniques . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ently been shown to synthesize high quality images and audio <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ \">15,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, here we restrict ourselves to diffusion models <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar  on random terms of Eq. ( <ref type=\"formula\" target=\"#formula_2\">3</ref>). As previously suggested <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, we use the reverse t is a weighted form of the ELBO that resembles denoising score matching over multiple noise scales <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. For the case of le tarted with work connecting diffusion models to denoising score matching over multiple noise scales <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. There have been a  hannel multipliers per resolution, refer to hyperparameters of the U-Net in DDPM and related models <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar \"2\">Background</head><p>Diffusion models A diffusion model <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> is defined by a forward process that gradually destroys data or a diffusion timestep t, is provided by adding embeddings into intermediate layers of the network <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Lower resolution image conditioning is provided by channel odels at 32\u00d732 and 64\u00d764 resolutions use 4000 diffusion timesteps and architectures similar to DDPM <ref type=\"bibr\" target=\"#b9\">[10]</ref> and Improved DDPM <ref type=\"bibr\" target=\"#b15\">[16]</ref> ef>, and others <ref type=\"bibr\" target=\"#b5\">[6]</ref>. For simplicity, we base our models on DDPM <ref type=\"bibr\" target=\"#b9\">[10]</ref> with modifications from Improved DDPM <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Diffusion models <ref type=\"bibr\" target=\"#b23\">[24]</ref> have recently been shown to synthesize high quality images \"http://www.tei-c.org/ns/1.0\"><head n=\"5\">Related work</head><p>Recent interest in diffusion models <ref type=\"bibr\" target=\"#b23\">[24]</ref> started with work connecting diffusion models to denoising to diffusion models <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p><p>The simplest and most effective technique we found t ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Background</head><p>Diffusion models A diffusion model <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> is defined by a for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t best architectures for image diffusion models are U-Nets <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, which are a natural choice to map corrupted data x t to re get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. The cosine noise schedule and the hybrid loss method of le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  GANs, VAEs, and flows <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er 10 splits <ref type=\"bibr\" target=\"#b21\">[22]</ref>. We cropped and resized the ImageNet dataset <ref type=\"bibr\" target=\"#b19\">[20]</ref> in the same manner as BigGAN <ref type=\"bibr\" target=\"#b1\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. Most previous work. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> and other types of generative models <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sembles denoising score matching over multiple noise scales <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. For the case of learned \u03a3 \u03b8 , we employ a hybrid loss <ref dels to denoising score matching over multiple noise scales <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. There have been a number of improvements and alternatives . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar target=\"#b3\">4]</ref> and other types of generative models <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>; our work here focu sing extra image classifiers to boost sample quality metrics <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>.</p><p>Our key contribution concerns the use of cascades to nerative model could be used in a cascading pipeline [e.g., <ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, here we restrict ourselves to diffusion models <ref type=\" en shown to be useful with other generative model families <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. A major benefit to training a cascading pipeline over trai =\"#fig_3\">4</ref> for a qualitative assessment of sample quality and diversity compared to VQ-VAE-2 <ref type=\"bibr\" target=\"#b17\">[18]</ref> and BigGAN-deep <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er 10 splits <ref type=\"bibr\" target=\"#b21\">[22]</ref>. We cropped and resized the ImageNet dataset <ref type=\"bibr\" target=\"#b19\">[20]</ref> in the same manner as BigGAN <ref type=\"bibr\" target=\"#b1\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: samples, but all reported FID scores are calculated over 50k samples for comparison with other work <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We additionally report FID scores calculated against valida. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: been led by other classes of generative models such as autoregressive models, GANs, VAEs, and flows <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target oach we attain FID scores on class-conditional ImageNet generation that are better than BigGAN-Deep <ref type=\"bibr\" target=\"#b1\">[2]</ref> at any truncation value. We empirically find that conditioni better in terms of Inception score when their truncation parameter is optimized for Inception score <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We also outperform concurrently released diffusion models t uality and diversity compared to VQ-VAE-2 <ref type=\"bibr\" target=\"#b17\">[18]</ref> and BigGAN-deep <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Our cascading pipelines are structured as a 32\u00d732 bas resized the ImageNet dataset <ref type=\"bibr\" target=\"#b19\">[20]</ref> in the same manner as BigGAN <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d practice of generating 50k samples and calculating the mean and standard deviation over 10 splits <ref type=\"bibr\" target=\"#b21\">[22]</ref>. We cropped and resized the ImageNet dataset <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  GANs, VAEs, and flows <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sembles denoising score matching over multiple noise scales <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. For the case of learned \u03a3 \u03b8 , we employ a hybrid loss <ref dels to denoising score matching over multiple noise scales <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. There have been a number of improvements and alternatives . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d practice of generating 50k samples and calculating the mean and standard deviation over 10 splits <ref type=\"bibr\" target=\"#b21\">[22]</ref>. We cropped and resized the ImageNet dataset <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: samples, but all reported FID scores are calculated over 50k samples for comparison with other work <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We additionally report FID scores calculated against valida. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t best architectures for image diffusion models are U-Nets <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, which are a natural choice to map corrupted data x t to re get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. The cosine noise schedule and the hybrid loss method of le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stract> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>In this paper, we introduce ELECTRA-style tasks <ref type=\"bibr\" target=\"#b10\">(Clark et al., 2020b)</ref> to cross-lingual language model pre-train f>. The compared models are all in base size.</p><p>In this paper, we introduce ELECTRA-style tasks <ref type=\"bibr\" target=\"#b10\">(Clark et al., 2020b)</ref> to cross-lingual language model pre-train aining tends to encourage better cross-lingual transferability.</p><p>2 Background: ELECTRA ELECTRA <ref type=\"bibr\" target=\"#b10\">(Clark et al., 2020b)</ref> introduces the replaced token detection t rpus P = {(e, f )}. We jointly pretrain the generator and the discriminator from scratch. Following <ref type=\"bibr\" target=\"#b10\">Clark et al. (2020b)</ref>, we make the generator smaller to improve . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p><p>Compared with relative position bias <ref type=\"bibr\" target=\"#b34\">(Parikh et al., 2016;</ref><ref type=\"bibr\" target=\"#b35\">Raffel et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Bao et al., 2 ref type=\"bibr\" target=\"#b38\">Song et al. 2019)</ref>, denoising auto-encoding, and span corruption <ref type=\"bibr\" target=\"#b35\">(Raffel et al., 2020)</ref> are designed for learning cross-lingual s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p><p>Compared with relative position bias <ref type=\"bibr\" target=\"#b34\">(Parikh et al., 2016;</ref><ref type=\"bibr\" target=\"#b35\">Raffel et al., 2020;</ref><ref type=\"bibr\" target=\"#b2\">Bao et al., 2 ref type=\"bibr\" target=\"#b38\">Song et al. 2019)</ref>, denoising auto-encoding, and span corruption <ref type=\"bibr\" target=\"#b35\">(Raffel et al., 2020)</ref> are designed for learning cross-lingual s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b46\">(Zeman et al., 2019)</ref>, NER named entity recognition on the Wikiann <ref type=\"bibr\" target=\"#b33\">(Pan et al., 2017;</ref><ref type=\"bibr\" target=\"#b36\">Rahimi et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: modeling <ref type=\"bibr\" target=\"#b12\">(Conneau and Lample, 2019)</ref>, and crosslingual contrast <ref type=\"bibr\" target=\"#b6\">(Chi et al., 2021b)</ref>. However, previous cross-lingual pre-trainin rackets. We also present XLM-R <ref type=\"bibr\" target=\"#b11\">(Conneau et al., 2020)</ref>, InfoXLM <ref type=\"bibr\" target=\"#b6\">(Chi et al., 2021b)</ref>, and XLM-Align <ref type=\"bibr\" target=\"#b7\"  with the cross-lingual language models pretrained with multilingual text, i.e., Multilingual BERT  <ref type=\"bibr\" target=\"#b6\">(Chi et al., 2021b)</ref>, and XLM-ALIGN <ref type=\"bibr\" target=\"#b7\" LM-R base . XLM-E performs slightly worse than INFOXLM base . We believe the cross-lingual contrast <ref type=\"bibr\" target=\"#b6\">(Chi et al., 2021b)</ref> task explicitly learns the sentence represen  al., 2020)</ref> utilizes translation pairs to construct code-switched sequences as input. InfoXLM <ref type=\"bibr\" target=\"#b6\">(Chi et al., 2021b)</ref> considers an input translation pair as cross arning cross-lingual language models <ref type=\"bibr\" target=\"#b12\">(Conneau and Lample, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Chi et al., 2021b)</ref>, while it is under-studied how to improve dis t languages. Tatoeba consists of English-centric parallel corpora covering 122 languages. Following <ref type=\"bibr\" target=\"#b6\">Chi et al. (2021b)</ref> and <ref type=\"bibr\" target=\"#b19\">Hu et al. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <head n=\"1\">Introduction</head><p>It has become a de facto trend to use a pretrained language model <ref type=\"bibr\" target=\"#b14\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b15\">Dong et al. using multilingual corpora, such pretrained models perform well on zero-shot cross-lingual transfer <ref type=\"bibr\" target=\"#b14\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Conneau et  ther than generators, which is different from the previous pretrained language models, such as BERT <ref type=\"bibr\" target=\"#b14\">(Devlin et al., 2019)</ref>, that learn to predict the masked tokens. ly. The generator G is typically a small BERT model trained with the masked language modeling (MLM; <ref type=\"bibr\" target=\"#b14\">Devlin et al. 2019)</ref> task. Consider an input sentence x = {x i } al. 2019</ref>) is typically used to learn cross-lingual encoders such as Multilingual BERT (mBERT; <ref type=\"bibr\" target=\"#b14\">Devlin et al. 2019)</ref> and XLM-R <ref type=\"bibr\" target=\"#b11\">(C 0.2 68.3 / 49.8 62.4 / 45.7 76.6</cell><cell>88.3</cell><cell>69.3</cell></row></table><note>(MBERT;<ref type=\"bibr\" target=\"#b14\">Devlin et al. 2019)</ref>, MT5(Xue et al.,  2020), and XLM-R<ref type roven to be effective for pretraining cross-lingual language models. Masked language modeling (MLM; <ref type=\"bibr\" target=\"#b14\">Devlin et al. 2019</ref>) is typically used to learn cross-lingual en. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  trend to use a pretrained language model <ref type=\"bibr\" target=\"#b14\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b15\">Dong et al., 2019;</ref><ref type=\"bibr\" target=\"#b45\">Yang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  tasks utilize the token-level alignments in parallel data to improve cross-lingual language models <ref type=\"bibr\" target=\"#b3\">(Cao et al., 2020;</ref><ref type=\"bibr\">Zhao et al., 2020;</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b11\">(Conneau et al., 2020</ref>) that consists of 250K subwords tokenized by Sentence-Piece <ref type=\"bibr\" target=\"#b23\">(Kudo and Richardson, 2018)</ref>.</p><p>Training We jointly pretrain. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  drops. This trend is also found in other cross-lingual language models such as XLM-R and XLM-Align <ref type=\"bibr\" target=\"#b20\">(Jalili Sabet et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Chi et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n language modeling (TLM) task that predicts masked tokens from concatenated translation pairs. ALM <ref type=\"bibr\" target=\"#b43\">(Yang et al., 2020)</ref> utilizes translation pairs to construct cod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rch architectures over convolutional neural networks space <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= ype=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>.</p><p>Recent works, OFA <ref type=\"bibr\" target=\"#b5\">[6]</ref>, BigNAS <ref type=\"bibr\" target=\"#b64\">[65]</ref> and slimma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the searching cost <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. The key idea is to t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, evolution algorithms <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> or reinforcement l ype=\"bibr\" target=\"#b71\">72]</ref> or evolution algorithms <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Most recent works resort to the one-shot weight sharing st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  retraining step after the best architecture is identified <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>.</p><p>Recent works. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: regularization training strategy for transformer, to some extent, similar to the effects of dropout <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>. Our weight entanglement training strategy helps to optimiz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, evolution algorithms <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> or reinforcement l ype=\"bibr\" target=\"#b71\">72]</ref> or evolution algorithms <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Most recent works resort to the one-shot weight sharing st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion, which hence reduces the reliance of units. 2) Optimization of deep thin subnets. Recent works <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref> show that deep trans. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  constraint over the well-trained supernets to find promising transformers. Experiments on ImageNet <ref type=\"bibr\" target=\"#b10\">[11]</ref> demonstrate that our method achieves superior performance . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b15\">16]</ref> or reinforcement learning <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> to find the most promising one.</p></div> <div xmlns=\"http:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b64\">[65]</ref> and slimmable networks <ref type=\"bibr\" target=\"#b64\">[65,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>, we propose a supernet training strategy called weight enta ully optimized. Inspried by BigNAS <ref type=\"bibr\" target=\"#b64\">[65]</ref> and slimmable networks <ref type=\"bibr\" target=\"#b65\">[66,</ref><ref type=\"bibr\" target=\"#b63\">64]</ref>, we propose the we br\" target=\"#b5\">[6]</ref>, BigNAS <ref type=\"bibr\" target=\"#b64\">[65]</ref> and slimmable networks <ref type=\"bibr\" target=\"#b65\">[66,</ref><ref type=\"bibr\" target=\"#b63\">64]</ref> alleviate this iss. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b17\">18]</ref>. ML systems in the E2E framework have also been and being investigated <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta  many languages. Researchers have, therefore, investigated the use of different units such as bytes <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>In this paper, we present ML E2E ASR experiments we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Since the performance of E2E systems could not outperform the hybrid HMM-DNN approach, researchers <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> try to improve the per erformance of a pure E2E ASR model, i.e. without an external LM component, is far from satisfactory <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Consequently, ESPRESS. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  acoustic modeling and the use of speech data from closely related languages as well as GlobalPhone <ref type=\"bibr\" target=\"#b22\">[23]</ref> languages, considering the four Ethiopian languages as tar  four target languages. In addition, GlobalPhone, a ML database of speech and text for 22 languages <ref type=\"bibr\" target=\"#b22\">[23]</ref>, has been used as source data. The following is a brief de , Swedish, Tamil, Thai, Turkish, Ukrainian, and Vietnamese. Detailed description of GP can be found <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: MH2020 together with the corpora of other three Ethiopian languages have been collected in Ethiopia <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The AMH2020, Tigrigna and Oromo speech corpora consist of ances when sentences are selected. For details on Ethiopian languages corpora, we direct readers to <ref type=\"bibr\" target=\"#b24\">[25]</ref>.</p><p>We have used the dev and test sets of these speech . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. The problem of using the E2E approach for the development . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ould not outperform the hybrid HMM-DNN approach, researchers <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> try to improve the performance by integrating a language mode . without an external LM component, is far from satisfactory <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Consequently, ESPRESSO provides the possibility of external . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. The problem of using the E2E approach for the development . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  acoustic modeling and the use of speech data from closely related languages as well as GlobalPhone <ref type=\"bibr\" target=\"#b22\">[23]</ref> languages, considering the four Ethiopian languages as tar  four target languages. In addition, GlobalPhone, a ML database of speech and text for 22 languages <ref type=\"bibr\" target=\"#b22\">[23]</ref>, has been used as source data. The following is a brief de , Swedish, Tamil, Thai, Turkish, Ukrainian, and Vietnamese. Detailed description of GP can be found <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in the E2E framework have also been and being investigated <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b5\">[6]</ref>. Consequently, various studies in MLASR <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ) or missing values (e.g., positive pairs in the test set) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Although another line of work <ref type=\"bibr\" target . <ref type=\"figure\">1</ref>) in the related work section.</p><p>To the best of our knowledge, BUIR <ref type=\"bibr\" target=\"#b5\">[6]</ref> is the only framework for CF to learn user and item latent r b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> to slowly approximate the online network <ref type=\"bibr\" target=\"#b5\">[6]</ref>. As BUIR is built upon BYOL, which stems from vision domain, ction (u, i). Intuitively, we predict the future interaction score based on a cross-prediction task <ref type=\"bibr\" target=\"#b5\">[6]</ref>. That is, we both predict the interaction probability of ite e evaluate the framework on three publicly available datasets and compare its performance with BUIR <ref type=\"bibr\" target=\"#b5\">[6]</ref> by encapsulating two popular CF-based baselines. The CF base ting whether the user rated the movie. As shown in Table Our framework is mainly compared with BUIR <ref type=\"bibr\" target=\"#b5\">[6]</ref>, a selfsupervised framework that is derived from BYOL <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  as negative.</p><p>Self-supervised learning (SSL) models <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, that are propo ve sampling in model training is that, the Siamese networks collapse to a trivial constant solution <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Thus, in recent work, BYOL <ref type=\"bibr\" target=\"#b9\"> =\"#b10\">[11]</ref>. Thus, in recent work, BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> introduce asymmetry to the network architecture by adding  viors from previous SSL frameworks (e.g., BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>). \u2022 Finally, we conduct experiments on three public datase re general models for comparing entities. BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> are two specializations of the Siamese network that achiev rks can model invariance with regard to more complicated transformations (e.g., data augmentations) <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The online and target networks in SELFCF use a same copy  i ) = g(e u , e i ) in other view like in BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>We define a symmetrized loss function as the negati ork only. We follow the stop gradient (sg) operator as in <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and implement the operator by updating Equation 4 as:</p> marizes the variants and their recommendation performance.  Different from the predictor in SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>, our framework still works by removing the predictor h, bu ><head n=\"5.3\">Stop-gradient</head><p>Existing researches <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref> on SSL highligh ch the \"stop gradient\" operator is identified as a crucial component to prevent solution collapsing <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The underlying reason is that our loss function is design. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> learn latent representations of users and items, by first fa information from neighbors via the nonlinear activation and linear transformation layers. He et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> simplified the GCNs architecture by removing the feature tra  input. The (u, i) pair is initially processed by an encoder network f in a backbone (e.g. LightGCN <ref type=\"bibr\" target=\"#b3\">[4]</ref>). The output of the encoder f is then copied to the target n =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Hyper-parameters Settings.</head><p>Same as other work <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we fix the embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ther line of work <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> have get rid of negative sampling and take the full unobserv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Historical embedding. First, we introduce embedding perturbation by utilizing historical embeddings <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref> from prior trai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ating dataset <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref>. In MovieLens, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Although another line of work <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uctured data, GCN-based CF is widely researched recently <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. The user-item . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rning based models are further proposed for recommendation <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>With the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ating dataset <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref>. In MovieLens, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mbedding perturbation by utilizing historical embeddings <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref> from prior training iterations. Specifically, we use a mom. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[41]</ref>. In MovieLens, each entry records the rating value of a user and film pair. Similar to <ref type=\"bibr\" target=\"#b41\">[42]</ref>, we transform the rating scores into binary values, so tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r graph-based CF models, the edge pruning method used in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref> provides an alternative way to augment the output embeddin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y may still treat a future positive sample as negative.</p><p>Self-supervised learning (SSL) models <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr s shown that SSL is possible to achieve competitive or even better results than supervised learning <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr o a trivial constant solution <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Thus, in recent work, BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> intro F to learn user and item latent representations without negative samples. BUIR is derived from BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Similar to BYOL, BUIR employs two distinct encoder network  the same time, parameters of the target network are updated based on momentum-based moving average <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr  \"stop gradient\" operator, which shows different behaviors from previous SSL frameworks (e.g., BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>). \u2022 F  networks <ref type=\"bibr\" target=\"#b28\">[29]</ref> are general models for comparing entities. BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> are t ) and matches it to the perturbed embeddings (\u1ebd u , \u1ebdi ) = g(e u , e i ) in other view like in BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p>< ckpropagation of loss over the online network only. We follow the stop gradient (sg) operator as in <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and implement t  BUIR <ref type=\"bibr\" target=\"#b5\">[6]</ref>, a selfsupervised framework that is derived from BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Its architecture follows the Siamese network in Fig. <ref  </head><p>In contrastive learning, it is a common practice for losses measuring a cosine similarity <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr > <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Stop-gradient</head><p>Existing researches <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: embedding parameters with the Xavier method <ref type=\"bibr\" target=\"#b43\">[44]</ref>, and use Adam <ref type=\"bibr\" target=\"#b44\">[45]</ref> as the optimizer. For a fair comparison, we carefully tune. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  latent embeddings of users and items are learnt from the online network. Analogous to convolutions <ref type=\"bibr\" target=\"#b29\">[30]</ref>, which is a successful inductive bias via weight-sharing f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: duce as much as diversity.</p><p>Sparsity: Data sparsity <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref> is measured as the number of interactions divided by the p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Although another line of work <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing researches <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref> on SSL highlight the crucial role of stop-gradient in prev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b5\">[6]</ref>.</p><p>Although another line of work <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> have get rid of neg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: duce as much as diversity.</p><p>Sparsity: Data sparsity <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref> is measured as the number of interactions divided by the p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntations obtained from different distorted versions of a sample using a variant of Siamese networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Siamese networks usually include two symmetric networks ( arn representations by attracting the positive sample pairs and repulsing the negative sample pairs <ref type=\"bibr\" target=\"#b13\">[14]</ref>. A line of work <ref type=\"bibr\" target=\"#b12\">[13]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: embedding parameters with the Xavier method <ref type=\"bibr\" target=\"#b43\">[44]</ref>, and use Adam <ref type=\"bibr\" target=\"#b44\">[45]</ref> as the optimizer. For a fair comparison, we carefully tune. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r graph-based CF models, the edge pruning method used in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref> provides an alternative way to augment the output embeddin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: air comparison, we follow the same evaluation setting of <ref type=\"bibr\" target=\"#b45\">[46]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref> with a chronological data splitting 8:1:1 for training, va. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y may still treat a future positive sample as negative.</p><p>Self-supervised learning (SSL) models <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr s shown that SSL is possible to achieve competitive or even better results than supervised learning <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr o a trivial constant solution <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Thus, in recent work, BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> intro F to learn user and item latent representations without negative samples. BUIR is derived from BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Similar to BYOL, BUIR employs two distinct encoder network  the same time, parameters of the target network are updated based on momentum-based moving average <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr  \"stop gradient\" operator, which shows different behaviors from previous SSL frameworks (e.g., BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>). \u2022 F  networks <ref type=\"bibr\" target=\"#b28\">[29]</ref> are general models for comparing entities. BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> are t ) and matches it to the perturbed embeddings (\u1ebd u , \u1ebdi ) = g(e u , e i ) in other view like in BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p>< ckpropagation of loss over the online network only. We follow the stop gradient (sg) operator as in <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and implement t  BUIR <ref type=\"bibr\" target=\"#b5\">[6]</ref>, a selfsupervised framework that is derived from BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Its architecture follows the Siamese network in Fig. <ref  </head><p>In contrastive learning, it is a common practice for losses measuring a cosine similarity <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr > <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Stop-gradient</head><p>Existing researches <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y may still treat a future positive sample as negative.</p><p>Self-supervised learning (SSL) models <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr s shown that SSL is possible to achieve competitive or even better results than supervised learning <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr o a trivial constant solution <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Thus, in recent work, BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> intro F to learn user and item latent representations without negative samples. BUIR is derived from BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Similar to BYOL, BUIR employs two distinct encoder network  the same time, parameters of the target network are updated based on momentum-based moving average <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr  \"stop gradient\" operator, which shows different behaviors from previous SSL frameworks (e.g., BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>). \u2022 F  networks <ref type=\"bibr\" target=\"#b28\">[29]</ref> are general models for comparing entities. BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> are t ) and matches it to the perturbed embeddings (\u1ebd u , \u1ebdi ) = g(e u , e i ) in other view like in BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p>< ckpropagation of loss over the online network only. We follow the stop gradient (sg) operator as in <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and implement t  BUIR <ref type=\"bibr\" target=\"#b5\">[6]</ref>, a selfsupervised framework that is derived from BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Its architecture follows the Siamese network in Fig. <ref  </head><p>In contrastive learning, it is a common practice for losses measuring a cosine similarity <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr > <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Stop-gradient</head><p>Existing researches <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  CF-based methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> learn latent repres  and compressed latent features, deep learning based models are further proposed for recommendation <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y may still treat a future positive sample as negative.</p><p>Self-supervised learning (SSL) models <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr s shown that SSL is possible to achieve competitive or even better results than supervised learning <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr o a trivial constant solution <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Thus, in recent work, BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> intro F to learn user and item latent representations without negative samples. BUIR is derived from BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Similar to BYOL, BUIR employs two distinct encoder network  the same time, parameters of the target network are updated based on momentum-based moving average <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr  \"stop gradient\" operator, which shows different behaviors from previous SSL frameworks (e.g., BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>). \u2022 F  networks <ref type=\"bibr\" target=\"#b28\">[29]</ref> are general models for comparing entities. BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref> are t ) and matches it to the perturbed embeddings (\u1ebd u , \u1ebdi ) = g(e u , e i ) in other view like in BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref> and SIMSIAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p>< ckpropagation of loss over the online network only. We follow the stop gradient (sg) operator as in <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and implement t  BUIR <ref type=\"bibr\" target=\"#b5\">[6]</ref>, a selfsupervised framework that is derived from BYOL <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Its architecture follows the Siamese network in Fig. <ref  </head><p>In contrastive learning, it is a common practice for losses measuring a cosine similarity <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr > <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Stop-gradient</head><p>Existing researches <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uctured data, GCN-based CF is widely researched recently <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. The user-item . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s well. The input is an image, and techniques for data augmentation on images are relatively mature <ref type=\"bibr\" target=\"#b16\">[17]</ref>, such as random cropping, resizing, horizontal flipping, c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  CF-based methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> learn latent repres  and compressed latent features, deep learning based models are further proposed for recommendation <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"table\" target=\"#tab_3\">1</ref>.</p><p>\u2022 MovieLens-1M: This is a widely used movie rating dataset <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oblem <ref type=\"bibr\" target=\"#b30\">(Zheng et al., 2017)</ref>, a transition-based parsing problem <ref type=\"bibr\" target=\"#b23\">(Wang et al., 2018)</ref>, and a generation problem with  Seq2Seq fra eng et al., 2017)</ref>. Also, our model is more versatile as not relying on complex expertise like <ref type=\"bibr\" target=\"#b23\">(Wang et al., 2018)</ref>, which requires external expert knowledge t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e, 2017)</ref> or decoding interactions <ref type=\"bibr\" target=\"#b25\">(Yang and Cardie, 2013;</ref><ref type=\"bibr\" target=\"#b17\">Sun et al., 2019)</ref> are imposed to explore the common structure o uld take the relation information into account, and vice versa.</p><p>Inspired by the procedures of <ref type=\"bibr\" target=\"#b17\">(Sun et al., 2019)</ref>, We propose a three-steps decoding algorithm /ref>, joint MRT <ref type=\"bibr\" target=\"#b18\">(Sun et al., 2018)</ref>, GCN-based joint inference <ref type=\"bibr\" target=\"#b17\">(Sun et al., 2019)</ref>. Actually, table filling method <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tions). It is also different from the current table filling settings for entity relation extraction <ref type=\"bibr\" target=\"#b15\">(Miwa and Sasaki, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Gupta et a t inference <ref type=\"bibr\" target=\"#b17\">(Sun et al., 2019)</ref>. Actually, table filling method <ref type=\"bibr\" target=\"#b15\">(Miwa and Sasaki, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Gupta et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rovide detailed dataset specifications in the Appendix B.</p><p>Evaluation Following suggestions in <ref type=\"bibr\" target=\"#b19\">(Taill\u00e9 et al., 2020)</ref>, we evaluate Precision (P), Recall (R), a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  squares and rectangles. <ref type=\"bibr\" target=\"#b5\">Cardie, 2017)</ref> or decoding interactions <ref type=\"bibr\" target=\"#b25\">(Yang and Cardie, 2013;</ref><ref type=\"bibr\" target=\"#b17\">Sun et al ction of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder <ref type=\"bibr\" target=\"#b25\">(Yang and Cardie, 2013)</ref>, joint MRT <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or fair comparison with previous works, we use three pre-trained language models: bert-base-uncased <ref type=\"bibr\" target=\"#b1\">(Devlin et al., 2019)</ref>, albert-xxlarge-v1 <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as ILP-based joint decoder <ref type=\"bibr\" target=\"#b25\">(Yang and Cardie, 2013)</ref>, joint MRT <ref type=\"bibr\" target=\"#b18\">(Sun et al., 2018)</ref>, GCN-based joint inference <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ne early paradigm is the pipeline method <ref type=\"bibr\" target=\"#b26\">(Zelenko et al., 2003;</ref><ref type=\"bibr\" target=\"#b14\">Miwa et al., 2009)</ref> that uses two independent models for two sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or fair comparison with previous works, we use three pre-trained language models: bert-base-uncased <ref type=\"bibr\" target=\"#b1\">(Devlin et al., 2019)</ref>, albert-xxlarge-v1 <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b15\">(Miwa and Sasaki, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Gupta et al., 2016;</ref><ref type=\"bibr\" target=\"#b29\">Zhang et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Wang and Lu,  \" target=\"#b15\">(Miwa and Sasaki, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Gupta et al., 2016;</ref><ref type=\"bibr\" target=\"#b29\">Zhang et al., 2017;</ref><ref type=\"bibr\">Wang et al., 2020</ref>) is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pora annotate 7 entity types and 6 relation types. we use the same data splits and preprocessing as <ref type=\"bibr\" target=\"#b7\">(Li and Ji, 2014;</ref><ref type=\"bibr\" target=\"#b13\">Miwa and Bansal,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b23\">(Wang et al., 2018)</ref>, and a generation problem with  Seq2Seq framework <ref type=\"bibr\" target=\"#b27\">(Zeng et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Nayak and Ng,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: put label spaces. In the joint setting, on the other hand, some parameter sharing of feature spaces <ref type=\"bibr\" target=\"#b13\">(Miwa and Bansal, 2016;</ref><ref type=\"bibr\" target=\"#b5\">Katiyar an e more interaction between entities and relations. The most basic joint paradigm, parameter sharing <ref type=\"bibr\" target=\"#b13\">(Miwa and Bansal, 2016;</ref><ref type=\"bibr\" target=\"#b5\">Katiyar an  use the same data splits and preprocessing as <ref type=\"bibr\" target=\"#b7\">(Li and Ji, 2014;</ref><ref type=\"bibr\" target=\"#b13\">Miwa and Bansal, 2016)</ref>, i.e., 5-fold cross-validation for ACE04. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ne early paradigm is the pipeline method <ref type=\"bibr\" target=\"#b26\">(Zelenko et al., 2003;</ref><ref type=\"bibr\" target=\"#b14\">Miwa et al., 2009)</ref> that uses two independent models for two sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b15\">(Miwa and Sasaki, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Gupta et al., 2016;</ref><ref type=\"bibr\" target=\"#b29\">Zhang et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Wang and Lu,  \" target=\"#b15\">(Miwa and Sasaki, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Gupta et al., 2016;</ref><ref type=\"bibr\" target=\"#b29\">Zhang et al., 2017;</ref><ref type=\"bibr\">Wang et al., 2020</ref>) is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fication, which are defined in two separate label spaces. One early paradigm is the pipeline method <ref type=\"bibr\" target=\"#b26\">(Zelenko et al., 2003;</ref><ref type=\"bibr\" target=\"#b14\">Miwa et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ne early paradigm is the pipeline method <ref type=\"bibr\" target=\"#b26\">(Zelenko et al., 2003;</ref><ref type=\"bibr\" target=\"#b14\">Miwa et al., 2009)</ref> that uses two independent models for two sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as ILP-based joint decoder <ref type=\"bibr\" target=\"#b25\">(Yang and Cardie, 2013)</ref>, joint MRT <ref type=\"bibr\" target=\"#b18\">(Sun et al., 2018)</ref>, GCN-based joint inference <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ne early paradigm is the pipeline method <ref type=\"bibr\" target=\"#b26\">(Zelenko et al., 2003;</ref><ref type=\"bibr\" target=\"#b14\">Miwa et al., 2009)</ref> that uses two independent models for two sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dues within the MSA and have a Bidirectional Encoder Representations from Transformers (BERT)-style <ref type=\"bibr\" target=\"#b36\">37</ref> objective to predict the masked elements of the MSA sequence network to reconstruct the masked regions from the output MSA representation using a BERT-like loss <ref type=\"bibr\" target=\"#b36\">37</ref> . Third, the output single representations of the structure . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: for language processing <ref type=\"bibr\" target=\"#b51\">52</ref> and, more recently, computer vision <ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b52\">53</ref> has inspired the exp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  using the whole network (which we term 'recycling' and is related to approaches in computer vision <ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29</ref> ) contributes marked. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 'recycling' and is related to approaches in computer vision <ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29</ref> ) contributes markedly to accuracy with minor extra training. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: W6W <ref type=\"bibr\" target=\"#b82\">82</ref> , 6T1Z <ref type=\"bibr\" target=\"#b83\">83</ref> and 7JTL <ref type=\"bibr\" target=\"#b84\">84</ref> .</p><p>For MSA lookup at both the training and prediction t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: version 14/07/2017). For MSA search on Uniref90 and clustered MGnify, we used jackhmmer from HMMER3 <ref type=\"bibr\" target=\"#b68\">68</ref> . For constrained relaxation of structures, we used OpenMM v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref> . This bioinforma diction as converting an 'image' of evolutionary couplings <ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref> to an 'image' of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b39\">[40]</ref><ref type=\"bibr\" target=\"#b40\">[41]</ref><ref type=\"bibr\" target=\"#b41\">[42]</ref><ref type=\"bibr\" target=\"#b42\">[43]</ref> . Despite the long history of applying neural networks to  tructure prediction <ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b42\">43</ref> , they have only recently come to improve structure predicti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b7\">8</ref> -has been an important open research problem for more than 50 years <ref type=\"bibr\" target=\"#b8\">9</ref> . Despite recent progress <ref type=\"bibr\" target=\"#b9\">[10]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref> , existing methods fall far short of atomic accuracy, espe ructures has had a long and varied development, which is extensively covered in a number of reviews <ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b39\">[40]</ref><ref type=\"bibr\" ta et=\"#b42\">[43]</ref> . Despite the long history of applying neural networks to structure prediction <ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  6SK0 <ref type=\"bibr\" target=\"#b80\">80</ref> , 6FES <ref type=\"bibr\" target=\"#b81\">81</ref> , 6W6W <ref type=\"bibr\" target=\"#b82\">82</ref> , 6T1Z <ref type=\"bibr\" target=\"#b83\">83</ref> and 7JTL <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  resulting in the final clustering. Third, for each of the clusters, we computed an MSA using FAMSA <ref type=\"bibr\" target=\"#b65\">65</ref> and computed the HMMs following the Uniclust HH-suite databa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  6YJ1 <ref type=\"bibr\" target=\"#b78\">78</ref> , 6VR4 <ref type=\"bibr\" target=\"#b79\">79</ref> , 6SK0 <ref type=\"bibr\" target=\"#b80\">80</ref> , 6FES <ref type=\"bibr\" target=\"#b81\">81</ref> , 6W6W <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  6FES <ref type=\"bibr\" target=\"#b81\">81</ref> , 6W6W <ref type=\"bibr\" target=\"#b82\">82</ref> , 6T1Z <ref type=\"bibr\" target=\"#b83\">83</ref> and 7JTL <ref type=\"bibr\" target=\"#b84\">84</ref> .</p><p>For. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: First, 2,423,213,294 protein sequences were collected from UniProt (Swiss-Prot&amp;TrEMBL, 2017-11) <ref type=\"bibr\" target=\"#b62\">62</ref> , a soil reference protein catalogue and the marine eukaryot. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rdinate prediction. A few recent studies have been developed to predict the 3D coordinates directly <ref type=\"bibr\" target=\"#b46\">[47]</ref><ref type=\"bibr\" target=\"#b47\">[48]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s only achieved in the post-prediction relaxation of the structure by gradient descent in the Amber <ref type=\"bibr\" target=\"#b31\">32</ref>  global distance test (GDT) <ref type=\"bibr\" target=\"#b32\">3 ures, we used OpenMM v.7.3.1 <ref type=\"bibr\" target=\"#b69\">69</ref> with the Amber99sb force field <ref type=\"bibr\" target=\"#b31\">32</ref> . For neural network construction, running and other analyse. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on of protein physics <ref type=\"bibr\" target=\"#b15\">16</ref> or statistical approximations thereof <ref type=\"bibr\" target=\"#b16\">17</ref> . Although theoretically very appealing, this approach has p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: W6W <ref type=\"bibr\" target=\"#b82\">82</ref> , 6T1Z <ref type=\"bibr\" target=\"#b83\">83</ref> and 7JTL <ref type=\"bibr\" target=\"#b84\">84</ref> .</p><p>For MSA lookup at both the training and prediction t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rdinates directly <ref type=\"bibr\" target=\"#b46\">[47]</ref><ref type=\"bibr\" target=\"#b47\">[48]</ref><ref type=\"bibr\" target=\"#b48\">[49]</ref><ref type=\"bibr\" target=\"#b49\">[50]</ref> , but the accurac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: from bioinformatics analysis of the evolutionary history of proteins, homology to solved structures <ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19</ref> and pairwise evoluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"figure\" target=\"#fig_3\">4a</ref>) using an approach similar to noisy student self-distillation <ref type=\"bibr\" target=\"#b34\">35</ref> . In this procedure, we use a trained network to predict the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d><p>The predicted structure is compared to the true structure from the PDB in terms of lDDT metric <ref type=\"bibr\" target=\"#b33\">34</ref> , as this metric reports the domain accuracy without requiri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  6YJ1 <ref type=\"bibr\" target=\"#b78\">78</ref> , 6VR4 <ref type=\"bibr\" target=\"#b79\">79</ref> , 6SK0 <ref type=\"bibr\" target=\"#b80\">80</ref> , 6FES <ref type=\"bibr\" target=\"#b81\">81</ref> , 6W6W <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arch problem for more than 50 years <ref type=\"bibr\" target=\"#b8\">9</ref> . Despite recent progress <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta  under the team name 'AlphaFold2' and a completely different model from our CASP13 AlphaFold system <ref type=\"bibr\" target=\"#b9\">10</ref> ). The CASP assessment is carried out biennially using recent f type=\"bibr\" target=\"#b42\">43</ref> , they have only recently come to improve structure prediction <ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rdinates directly <ref type=\"bibr\" target=\"#b46\">[47]</ref><ref type=\"bibr\" target=\"#b47\">[48]</ref><ref type=\"bibr\" target=\"#b48\">[49]</ref><ref type=\"bibr\" target=\"#b49\">[50]</ref> , but the accurac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nary correlations <ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" t ting the problem of protein structure prediction as converting an 'image' of evolutionary couplings <ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s compared to the number of heterotypic contacts (further details are provided in a companion paper <ref type=\"bibr\" target=\"#b38\">39</ref> ). This typically occurs for bridging domains within larger  exciting possibility of predicting structures at the proteome-scale and beyond-in a companion paper <ref type=\"bibr\" target=\"#b38\">39</ref> , we demonstrate the application of AlphaFold to the entire  \" target=\"#b38\">39</ref> , we demonstrate the application of AlphaFold to the entire human proteome <ref type=\"bibr\" target=\"#b38\">39</ref> .</p><p>The explosion in available genomic sequencing techni represents a 95% confidence interval estimated from 10,000 bootstrap samples. In the companion paper<ref type=\"bibr\" target=\"#b38\">39</ref> , additional quantification of the reliability of pLDDT as a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to construct pairwise features in place of the learned 3D points has been applied to protein design <ref type=\"bibr\" target=\"#b59\">59</ref> .</p><p>In addition to the IPA, standard dot product attenti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"figure\" target=\"#fig_3\">4a</ref>) using an approach similar to noisy student self-distillation <ref type=\"bibr\" target=\"#b34\">35</ref> . In this procedure, we use a trained network to predict the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding of molecular driving forces into either thermodynamic or kinetic simulation of protein physics <ref type=\"bibr\" target=\"#b15\">16</ref> or statistical approximations thereof <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">45</ref> . These approaches effectively leverage the rapid improvement in computer vision systems <ref type=\"bibr\" target=\"#b45\">46</ref> by treating the problem of protein structure prediction as c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: First, 2,423,213,294 protein sequences were collected from UniProt (Swiss-Prot&amp;TrEMBL, 2017-11) <ref type=\"bibr\" target=\"#b62\">62</ref> , a soil reference protein catalogue and the marine eukaryot. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: have been developed to predict the 3D coordinates directly <ref type=\"bibr\" target=\"#b46\">[47]</ref><ref type=\"bibr\" target=\"#b47\">[48]</ref><ref type=\"bibr\" target=\"#b48\">[49]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b44\">45</ref> . These approaches effectively leverage the rapid improvemen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b70\">70</ref> , Sonnet 71 , NumPy <ref type=\"bibr\" target=\"#b72\">72</ref> , Python <ref type=\"bibr\" target=\"#b73\">73</ref> and Colab <ref type=\"bibr\" target=\"#b74\">74</ref> .</p><p>To. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19</ref> and pairwise evolutionary correlations <ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to construct pairwise features in place of the learned 3D points has been applied to protein design <ref type=\"bibr\" target=\"#b59\">59</ref> .</p><p>In addition to the IPA, standard dot product attenti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: version 14/07/2017). For MSA search on Uniref90 and clustered MGnify, we used jackhmmer from HMMER3 <ref type=\"bibr\" target=\"#b68\">68</ref> . For constrained relaxation of structures, we used OpenMM v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e loss on the same training examples and is not pre-trained, in contrast to recent independent work <ref type=\"bibr\" target=\"#b37\">38</ref> .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref> , existing method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e loss on the same training examples and is not pre-trained, in contrast to recent independent work <ref type=\"bibr\" target=\"#b37\">38</ref> .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: opment, which is extensively covered in a number of reviews <ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b39\">[40]</ref><ref type=\"bibr\" target=\"#b40\">[41]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly been significant interest in Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar k</head><p>The number of different GNN architectures is vast <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar  of the input graphs for all graph-related computations (i.e.to compute attention layers as in GATs <ref type=\"bibr\" target=\"#b34\">[35]</ref>).</p><p>Given these contrasting issues, the key question w ote by Adj(G) the (weighted) adjacency matrix of G.</p><p>A regular graph attention mechanism (GAT) <ref type=\"bibr\" target=\"#b34\">[35]</ref> processes such a data with standard attention layers <ref  tion networks (SGCs)( <ref type=\"bibr\" target=\"#b7\">[8]</ref>) and graph attention networks (GATs)( <ref type=\"bibr\" target=\"#b34\">[35]</ref>). A feature vector in each vertex was of length l = 5 and  a, Citeseer and Pubmed ( <ref type=\"bibr\" target=\"#b28\">[29]</ref>) with the same data splits as in <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Datasets descriptions are given in Table <ref type=\"table NNs with the corresponding fully-connected graph topologies. Graph attention neural networks (GATs) <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= omain via inverse FT <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. In the latter, then convolution is applied directly on the g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b39\">[40]</ref>, ECC <ref type=\"bibr\" target=\"#b31\">[32]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[13]</ref> and RWNN <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: line of work on GNNs incorporates attention mechanisms that were widely popularized by Transformers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> which became SOTA  GAT) <ref type=\"bibr\" target=\"#b34\">[35]</ref> processes such a data with standard attention layers <ref type=\"bibr\" target=\"#b33\">[34]</ref>, where the attention matrix A i of the ith layer is given  ull GKAT's architecture consists of several blocks, where each block (as in the regular Transformer <ref type=\"bibr\" target=\"#b33\">[34]</ref>), is built from the attention layer and standard MLP layer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: les in several high impact settings, while also making it possible to model relative inductive bias <ref type=\"bibr\" target=\"#b36\">[37]</ref> and define non-Euclidean metrics that are relevant in navi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p;D <ref type=\"bibr\" target=\"#b8\">[9]</ref>, PROTEINS <ref type=\"bibr\" target=\"#b1\">[2]</ref>, NCI1 <ref type=\"bibr\" target=\"#b35\">[36]</ref> and ENZYMES <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b1\">[2]</ref>, NCI1 <ref type=\"bibr\" target=\"#b35\">[36]</ref> and ENZYMES <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and the latter: IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b29\">30]</ref>, yet most of them admit a modular structure of distinct blocks responsible for: <ref type=\"bibr\" target=\"#b0\">(1)</ref> signal propagation between nodes (defining an information ag networks (LGCNs) <ref type=\"bibr\" target=\"#b11\">[12]</ref>, diffusion convolutional neural networks <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Neural FPs <ref type=\"bibr\" target=\"#b9\">[10]</ref> and mor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b42\">[43]</ref>, DiffPool <ref type=\"bibr\" target=\"#b39\">[40]</ref>, ECC <ref type=\"bibr\" target=\"#b31\">[32]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[13]</ref> and R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p;D <ref type=\"bibr\" target=\"#b8\">[9]</ref>, PROTEINS <ref type=\"bibr\" target=\"#b1\">[2]</ref>, NCI1 <ref type=\"bibr\" target=\"#b35\">[36]</ref> and ENZYMES <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b39\">[40]</ref>, ECC <ref type=\"bibr\" target=\"#b31\">[32]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[13]</ref> and RWNN <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> assume an ideal bicubic downsampling kernel, which is diffe </ref> for a more comprehensive taxonomy.</p><p>In this work, we aim to extend the powerful ESR-GAN <ref type=\"bibr\" target=\"#b43\">[44]</ref> to restore general real-world LR images by synthesizing tr Networks and Training</head><p>ESRGAN generator. We adopt the same generator (SR network) as ESRGAN <ref type=\"bibr\" target=\"#b43\">[44]</ref>, i.e., a deep network with several residual-in-residual de or Works</head><p>We compare our Real-ESRGAN with several stateof-the-art methods, including ESRGAN <ref type=\"bibr\" target=\"#b43\">[44]</ref>, DAN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, CDC <ref t e natural manifold <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Most methods assum. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: herefore, we improve the VGG-style discriminator in ESRGAN to an U-Net design with skip connections <ref type=\"bibr\" target=\"#b34\">[35]</ref> (Fig. <ref type=\"figure\" target=\"#fig_6\">6</ref>). The U-N. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2.\">Related Work</head><p>The image super-resolution field <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Sec. 3.1), is widely adopted in explicit modeling meth-ods <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. However, the real- get=\"#b26\">27]</ref> is widely adopted in blind SR methods <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b30\">31</ref>]. Yet, real-world de type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> or jointly (iteratively) <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: R <ref type=\"bibr\" target=\"#b44\">[45]</ref>, OST300 <ref type=\"bibr\" target=\"#b42\">[43]</ref>, DPED <ref type=\"bibr\" target=\"#b15\">[16]</ref>, ADE20K validation <ref type=\"bibr\" target=\"#b51\">[52]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r, most approaches <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar lization of the generator, and train the Real-ESRGAN with a combination of L1 loss, perceptual loss <ref type=\"bibr\" target=\"#b17\">[18]</ref> and GAN loss <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref  feature maps (with weights {0.1, 0.1, 1, 1, 1}) before activation in the pre-trained VGG19 network <ref type=\"bibr\" target=\"#b17\">[18]</ref> as the perceptual loss. Our implementation is based on the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b7\">8]</ref>. To achieve visually-pleasing results, generative adversarial network <ref type=\"bibr\" target=\"#b12\">[13]</ref> is usually employed as loss supervisions to push the solut. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r-resolution field <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" tar  details.</p><p>Similar to ESRGAN, we adopt DIV2K <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Flickr2K <ref type=\"bibr\" target=\"#b38\">[39]</ref> and OutdoorSceneTraining <ref type=\"bibr\" target=\"#b42\">[4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to image restoration <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>.</p><p>There have been several excellent explorations in bl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hresolution (HR) image from its low-resolution (LR) counterpart. Since the pioneering work of SRCNN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, deep convolution neural network (CNN) approaches have broug ,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> has witnessed a variety of developments since SRCNN <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. To achieve visually-p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> has witnessed a varie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b42\">[43]</ref>, DPED <ref type=\"bibr\" target=\"#b15\">[16]</ref>, ADE20K validation <ref type=\"bibr\" target=\"#b51\">[52]</ref> and images from Internet. Since existing metrics for perce. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> has witnessed a varie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to image restoration <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>.</p><p>There have been several excellent explorations in bl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sions to push the solutions closer to the natural manifold <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b39\">40]</ref> utilize data distribution learning with Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b11\">[12]</ref> to obtain the degradation model. Yet, they are limited to  th a combination of L1 loss, perceptual loss <ref type=\"bibr\" target=\"#b17\">[18]</ref> and GAN loss <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p></div> <div xm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b29\">30]</ref>; 3) or synthesized with estimated blur kernels and extracted noise patches <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. However, 1) the c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  by a quantization of DCT coefficients. More details of JPEG compression algorithms can be found in <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Unpleasing block artifacts are usually introduced by the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b5\">[6]</ref>. <ref type=\"bibr\" target=\"#b6\">[7]</ref>) or graph computations (e.g. <ref type=\"bibr\" target=\"#b7\">[8]</ref>); they are not effective for GNN training as they exhibit ch own to significantly outperform CPU-or GPU-based systems both in terms of execution time and energy <ref type=\"bibr\" target=\"#b7\">[8]</ref> [14] <ref type=\"bibr\" target=\"#b14\">[15]</ref>. However, the =\"#tab_1\">I</ref>) of both the V-and E-PEs are based on <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b7\">[8]</ref> respectively. We consider 64 V-PEs (1 planar tier) and 128 E. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  is memory intensive for large-scale graphs, which necessitates use of efficient graph partitioning <ref type=\"bibr\" target=\"#b15\">[16]</ref>. This divide-and-conquer approach enables scalable GNN tra e-and-conquer approach enables scalable GNN training over large graphs with high accuracy and speed <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The design of hardware architectures using commodity proc y concerns. To address this, clustering-based graph partitioning schemes are used. For instance, in <ref type=\"bibr\" target=\"#b15\">[16]</ref>, the authors use a graph partitioning tool (METIS <ref typ >[6]</ref>.</p><p>In this work, we use the popular graph convolutional network (GCN) algorithm from <ref type=\"bibr\" target=\"#b15\">[16]</ref> (implemented in TensorFlow), as the representative GNN for e notion of B in GNNs is not the same as in traditional DNNs. In a GNN, partitioning a graph (as in <ref type=\"bibr\" target=\"#b15\">[16]</ref>) yields several smaller sub-graphs (referred as NumPart he Parts by B. In Fig. <ref type=\"figure\" target=\"#fig_5\">5</ref>, we chose NumPart as 1500 (following <ref type=\"bibr\" target=\"#b15\">[16]</ref>) while varying \u00a7 from 1 to 20. From Fig. <ref type=\"figure hX a conventional GPU-based platform. For the GPU execution, we implement the Cluster-GCN algorithm <ref type=\"bibr\" target=\"#b15\">[16]</ref> on NVIDIA Tesla V100 GPU. Fig. <ref type=\"figure\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed systems both in terms of execution time and energy <ref type=\"bibr\" target=\"#b7\">[8]</ref> [14] <ref type=\"bibr\" target=\"#b14\">[15]</ref>. However, these solutions focus mainly on accelerating the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nge, multicast, and many-to-one-to-many traffic patterns <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>Hence, we conjecture that 3D NoC is a suitable comm eck. In addition, the multi-hop nature of a planar-mesh NoC leads to higher communication latencies <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which is not desirable for training GNNs. A 3D architectu g multiple layers above each other, the physical distance between PE tiles is reduced significantly <ref type=\"bibr\" target=\"#b12\">[13]</ref>. In addition, 3D NoCs enable high-performance multicast su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: itectures are optimized specifically for either DNNs (e.g. <ref type=\"bibr\" target=\"#b5\">[6]</ref>. <ref type=\"bibr\" target=\"#b6\">[7]</ref>) or graph computations (e.g. <ref type=\"bibr\" target=\"#b7\">[ h DNN training and inference have been extensively studied <ref type=\"bibr\" target=\"#b5\">[6]</ref>  <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Moreover, ReRAM based graph accelerators have been shown to r; often the backward-phase computations are implemented on separate set of RERAMs, as described in <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Overall, this results in the output of layer L; being sent  oes not exploit the benefits of RERAM-based architectures, which rely on a pipelined implementation <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Pipelining the different layers of a DNN reduces the number  Hence, deterministic models have been used to evaluate RERAM execution time, on-chip traffic, etc. <ref type=\"bibr\" target=\"#b6\">[7]</ref>. The mapping of DNN layer weights and matrices on the tiles . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: architectures using commodity processors, FPGAs and custom ASICs has been considered in recent work <ref type=\"bibr\" target=\"#b1\">[2]</ref> [9] <ref type=\"bibr\" target=\"#b9\">[10]</ref>. However, all t ation can be envisioned as a sparse matrixvector multiplication (SpMV) involving Adj (sparse matrix <ref type=\"bibr\" target=\"#b1\">[2]</ref>) and the updated vertex feature vectors (Y) as shown in show. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GPUs). However, existing ReRAM-based architectures are optimized specifically for either DNNs (e.g. <ref type=\"bibr\" target=\"#b5\">[6]</ref>. <ref type=\"bibr\" target=\"#b6\">[7]</ref>) or graph computati . Hence, ReRAM-based accelerators for both DNN training and inference have been extensively studied <ref type=\"bibr\" target=\"#b5\">[6]</ref>  <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Moreover, ReRAM ba -layer (which resembles a conventional DNN), we utilize the ReRAM-based tiled architecture for DNNs <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The ReRAM tiles consist of 128x128 ReRAM crossbars (V-PEs)  gy dissipation.</p><p>We show in Fig. <ref type=\"figure\">3</ref> that larger RERAM crossbars (as in <ref type=\"bibr\" target=\"#b5\">[6]</ref>) are not suited for this task. Fig. <ref type=\"figure\">3</re and backward phases) are active all the time, leading to higher throughput and hardware utilization <ref type=\"bibr\" target=\"#b5\">[6]</ref>, as shown in Fig. <ref type=\"figure\">4</ref>.</p><p>However, ons (shown in Table <ref type=\"table\" target=\"#tab_1\">I</ref>) of both the V-and E-PEs are based on <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b7\">[8]</ref> respectively. We <p>To evaluate the characteristics of the ReGraphx architecture, we use the performance models from <ref type=\"bibr\" target=\"#b5\">[6]</ref>.  ReRAM arrays always execute instructions in-order and the    ReRAM arrays always execute instructions in-order and the instruction latencies are deterministic <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Hence, deterministic models have been used to evaluate RERA cuss the RERRAM execution models in detail for the sake of brevity and as it has been elaborated in <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>In this work, we use the popular graph convolutional . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nge, multicast, and many-to-one-to-many traffic patterns <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>Hence, we conjecture that 3D NoC is a suitable comm eck. In addition, the multi-hop nature of a planar-mesh NoC leads to higher communication latencies <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which is not desirable for training GNNs. A 3D architectu g multiple layers above each other, the physical distance between PE tiles is reduced significantly <ref type=\"bibr\" target=\"#b12\">[13]</ref>. In addition, 3D NoCs enable high-performance multicast su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: itectures are optimized specifically for either DNNs (e.g. <ref type=\"bibr\" target=\"#b5\">[6]</ref>. <ref type=\"bibr\" target=\"#b6\">[7]</ref>) or graph computations (e.g. <ref type=\"bibr\" target=\"#b7\">[ h DNN training and inference have been extensively studied <ref type=\"bibr\" target=\"#b5\">[6]</ref>  <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Moreover, ReRAM based graph accelerators have been shown to r; often the backward-phase computations are implemented on separate set of RERAMs, as described in <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Overall, this results in the output of layer L; being sent  oes not exploit the benefits of RERAM-based architectures, which rely on a pipelined implementation <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Pipelining the different layers of a DNN reduces the number  Hence, deterministic models have been used to evaluate RERAM execution time, on-chip traffic, etc. <ref type=\"bibr\" target=\"#b6\">[7]</ref>. The mapping of DNN layer weights and matrices on the tiles . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: architectures using commodity processors, FPGAs and custom ASICs has been considered in recent work <ref type=\"bibr\" target=\"#b1\">[2]</ref> [9] <ref type=\"bibr\" target=\"#b9\">[10]</ref>. However, all t ation can be envisioned as a sparse matrixvector multiplication (SpMV) involving Adj (sparse matrix <ref type=\"bibr\" target=\"#b1\">[2]</ref>) and the updated vertex feature vectors (Y) as shown in show. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based architectures</head><p>ReRAMs can be used as memory and also to perform insitu MAC operations <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Both DNN and graph applications involve significant amount  , both the V-and E-layers can be decomposed as MAC operations which can be implemented using ReRAMs <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p><p>Next, Fig. <ref type=\"figure\">1</ref>(d) shows an exa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b40\">Li et al., 2019 ;</ref><ref type=\"bibr\" target=\"#b45\">Lomber et al., 2010 ;</ref><ref type=\"bibr\" target=\"#b53\">Qiao et al., 2019 ;</ref><ref type=\"bibr\" target=\"#b55\">Rosemann and   helpful in analyzing the connections in the whole brain <ref type=\"bibr\">( Liu et al., 2015 ;</ref><ref type=\"bibr\" target=\"#b53\">Qiao et al., 2019 ;</ref><ref type=\"bibr\" target=\"#b68\">Wang et al.,  icant changes, which is probably due to hearing asymmetry <ref type=\"bibr\">( Li et al., 2019 ;</ref><ref type=\"bibr\" target=\"#b53\">Qiao et al., 2019 ;</ref><ref type=\"bibr\" target=\"#b60\">Schmithorst e k <ref type=\"bibr\" target=\"#b51\">( Propst et al., 2010 )</ref>. Using a visual working memory task, <ref type=\"bibr\" target=\"#b53\">Qiao et al. (2019)</ref> observed decreased activation in the frontop rtex and the frontoparietal lobe using the seeded method <ref type=\"bibr\">( Liu et al., 2015 ;</ref><ref type=\"bibr\" target=\"#b53\">Qiao et al., 2019 )</ref>. <ref type=\"bibr\" target=\"#b78\">Zhang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ne expression level <ref type=\"bibr\" target=\"#b49\">( Pernia et al., 2020 )</ref> and cellular level <ref type=\"bibr\" target=\"#b4\">( Barone et al., 2013 )</ref> to the social level <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\">( Liu et al., 2015 ;</ref><ref type=\"bibr\" target=\"#b53\">Qiao et al., 2019 ;</ref><ref type=\"bibr\" target=\"#b68\">Wang et al., 2019 )</ref>. Thus, data-driven statistical learning met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and cellular level <ref type=\"bibr\" target=\"#b4\">( Barone et al., 2013 )</ref> to the social level <ref type=\"bibr\" target=\"#b10\">( Castellanos et al., 2015 ;</ref><ref type=\"bibr\" target=\"#b19\">Figu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing the frontoparietal and occipital lobes <ref type=\"bibr\" target=\"#b43\">( Lin et al., 2008 ;</ref><ref type=\"bibr\" target=\"#b54\">Rachakonda et al., 2014 ;</ref><ref type=\"bibr\" target=\"#b62\">Shang e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: but also nonauditoryrelated regions <ref type=\"bibr\" target=\"#b14\">( Dewey and Hartley, 2015 ;</ref><ref type=\"bibr\" target=\"#b24\">Han et al., 2018 ;</ref><ref type=\"bibr\" target=\"#b32\">Kim et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b61\">Shang et al., 2020 ;</ref><ref type=\"bibr\" target=\"#b62\">Shang et al., 2018 ;</ref><ref type=\"bibr\" target=\"#b69\">Wang et al., 2014 ;</ref><ref type=\"bibr\" target=\"#b70\">Wang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">( Abel and Lam, 2008 ;</ref><ref type=\"bibr\" target=\"#b6\">Bess and Tharpe, 1986 ;</ref><ref type=\"bibr\" target=\"#b58\">Ruscetta et al., 2005 )</ref>. Since daily communication ability is m te these peripheral auditory system tasks <ref type=\"bibr\" target=\"#b0\">( Abel and Lam, 2008 ;</ref><ref type=\"bibr\" target=\"#b58\">Ruscetta et al., 2005 )</ref>. Additionally, cognition-related region. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  collaborations between brain regions, which requires a more flexible dynamic window control method <ref type=\"bibr\" target=\"#b3\">( Bansal et al., 2019 )</ref>. Currently, there are studies that attem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2017 )</ref> and speech recognition in noise <ref type=\"bibr\" target=\"#b27\">( Hornsby, 2013 ;</ref><ref type=\"bibr\" target=\"#b39\">Lewis et al., 2016 )</ref>, because there are fewer cues (e.g., an ab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b5\">( Berding et al., 2015 ;</ref><ref type=\"bibr\" target=\"#b9\">Campbell and Sharma, 2013 ;</ref><ref type=\"bibr\" target=\"#b16\">Erb et al., 2013 ;</ref><ref type=\"bibr\" target=\"#b48\">Peelle et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2020)</ref>. More recent work has explored the use of contrastive learning in this context <ref type=\"bibr\" target=\"#b16\">(Kipf et al., 2020;</ref><ref type=\"bibr\" target=\"#b23\">Racah &amp; C ould also be close together in latent space <ref type=\"bibr\" target=\"#b22\">(Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Kipf et al., 2020;</ref><ref type=\"bibr\" target=\"#b26\">van der Pol et , 2016)</ref>, and dynamic environments <ref type=\"bibr\" target=\"#b24\">(Sermanet et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Kipf et al., 2020;</ref><ref type=\"bibr\" target=\"#b26\">van der Pol et so, we apply a variety of sampling heuristics to train contrastive structured world models (C-SWMs, <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref>), a recently proposed contrastive method for e step of an episode when sampling negative examples can double the prediction accuracy relative to <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref> in Atari games.</p><p>#2: We identify (and p to a contrastive model failing to represent objects.</p><p>#3: We extend the Atari datasets used in <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref> to cover a diverse set of states within the  ld environments (2D Shapes and 3D Cubes; Figure <ref type=\"figure\" target=\"#fig_1\">1</ref>) used in <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref> can cause C-SWM to fail completely. In both  f type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref>), which uses different hyper-parameters than <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref>. We report means and 95% confidence interval d><p>Setup. A small change in negative sampling can also make a large difference in Atari datasets. <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref> evaluated C-SWM on Pong and Space Invaders d rom a different episode but at the same time step, doubles the 10 step Hits @1 score for C-SWM from <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref> (Figure <ref type=\"figure\" target=\"#fig_3\">3 large impact on its performance. We performed experiments on the grid-world and Atari datasets from <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref>. We also created a new dataset with better c ts). Table <ref type=\"table\">1</ref> reports results on 2D Shapes, 3D Cubes and Atari datasets from <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref>. Table <ref type=\"table\" target=\"#tab_0\">2</ riment Details</head><p>In this section, we summarize the details of experiments that were based on <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref> and describe the details of the full Atari d d>C.4. Model Architectures</head><p>We use the three types of object extractor networks included in <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref>:</p><p>\u2022 Small encoder (2D Shapes): Conv2D ( lts for multi-step prediction in latent space. We use model architectures and hyper-parameters from <ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref>.</p><p>We report means and standard deviatio ef type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref>), which uses different hyper-parameters than<ref type=\"bibr\" target=\"#b16\">Kipf et al. (2020)</ref>. We report means and 95% confidence interval res the positions of immovable objects as well as the transitions of movable objects. datasets from <ref type=\"bibr\" target=\"#b16\">(Kipf et al., 2020)</ref>. Second row: comparison with the Set Refine  In our work, we make a similar observation and find that representations of the recent C-SWM model <ref type=\"bibr\" target=\"#b16\">(Kipf et al., 2020)</ref> can be substantially improved by prioritizi ally improved by prioritizing harder negative samples.</p><p>Object-centric Models. The C-SWM model <ref type=\"bibr\" target=\"#b16\">(Kipf et al., 2020)</ref>, which we study in our work, uses a factori sampling strategies in C-SWM in Pong (first column) and Space Invaders (second column) datasets from<ref type=\"bibr\" target=\"#b16\">(Kipf et al., 2020)</ref>. Second row: comparison with the Set Refine. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b1\">Burgess et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Janner et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Veerapaneni et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Kossen  p>Unlike comparable factored world models <ref type=\"bibr\" target=\"#b14\">(Janner et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Veerapaneni et al., 2019;</ref><ref type=\"bibr\" target=\"#b17\">Kossen . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are commonly used as positive samples <ref type=\"bibr\" target=\"#b5\">(Dosovitskiy et al., 2015;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b30\">Wu et al., 2018 target=\"#b4\">Dosovitskiy et al., 2014;</ref><ref type=\"bibr\" target=\"#b12\">Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020)</ref>, natural language <ref type=\"bibr\" target=\"#b arget=\"#b4\">(Dosovitskiy et al., 2014;</ref><ref type=\"bibr\" target=\"#b12\">Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020)</ref>. For dynamic environments and graphs the (tem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b16\">(Kipf et al., 2020)</ref>. Second row: comparison with the Set Refiner Network (SRN, <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref>), which uses different hyper-parameters th d row of Figure <ref type=\"figure\" target=\"#fig_3\">3</ref>; we also include the Set Refiner Network <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref>. C-SWM-ER performs better in Pong, whereas le <ref type=\"table\" target=\"#tab_0\">2</ref> compares our sampling strategy to Set Refiner Networks <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref> with their hyper-parameter settings (Secti \" target=\"#b16\">(Kipf et al., 2020)</ref>. Second row: comparison with the Set Refiner Network (SRN,<ref type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref>), which uses different hyper-parameters th des and 100 testing episodes with length 10. Actions are selected at random. In the comparison with <ref type=\"bibr\" target=\"#b13\">Huang et al. (2020)</ref>, we collect a validation set of size 1k (pe bel><figDesc>Ranking results for multi-step prediction in latent space. We use hyper-parameters from<ref type=\"bibr\" target=\"#b13\">Huang et al. (2020)</ref>. C-SWM-ER results are averaged over 20 rand. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the Pong and Space Invaders datasets. First, we train A3C, a model-free RL agent, until convergence <ref type=\"bibr\" target=\"#b21\">(Mnih et al., 2016)</ref>. Then, we roll out the trained agent for a  i-c.org/ns/1.0\"><head>C.3. Full Atari Datasets</head><p>We use an open-source implementation of A3C <ref type=\"bibr\" target=\"#b21\">(Mnih et al., 2016)</ref> <ref type=\"foot\" target=\"#foot_3\">2</ref> t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b16\">(Kipf et al., 2020)</ref>. Second row: comparison with the Set Refiner Network (SRN, <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref>), which uses different hyper-parameters th d row of Figure <ref type=\"figure\" target=\"#fig_3\">3</ref>; we also include the Set Refiner Network <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref>. C-SWM-ER performs better in Pong, whereas le <ref type=\"table\" target=\"#tab_0\">2</ref> compares our sampling strategy to Set Refiner Networks <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref> with their hyper-parameter settings (Secti \" target=\"#b16\">(Kipf et al., 2020)</ref>. Second row: comparison with the Set Refiner Network (SRN,<ref type=\"bibr\" target=\"#b13\">(Huang et al., 2020)</ref>), which uses different hyper-parameters th des and 100 testing episodes with length 10. Actions are selected at random. In the comparison with <ref type=\"bibr\" target=\"#b13\">Huang et al. (2020)</ref>, we collect a validation set of size 1k (pe bel><figDesc>Ranking results for multi-step prediction in latent space. We use hyper-parameters from<ref type=\"bibr\" target=\"#b13\">Huang et al. (2020)</ref>. C-SWM-ER results are averaged over 20 rand. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntations of images, data-augmented versions of the same image are commonly used as positive samples <ref type=\"bibr\" target=\"#b5\">(Dosovitskiy et al., 2015;</ref><ref type=\"bibr\" target=\"#b2\">Chen et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are commonly used as positive samples <ref type=\"bibr\" target=\"#b5\">(Dosovitskiy et al., 2015;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b30\">Wu et al., 2018 target=\"#b4\">Dosovitskiy et al., 2014;</ref><ref type=\"bibr\" target=\"#b12\">Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020)</ref>, natural language <ref type=\"bibr\" target=\"#b arget=\"#b4\">(Dosovitskiy et al., 2014;</ref><ref type=\"bibr\" target=\"#b12\">Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2020)</ref>. For dynamic environments and graphs the (tem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b20\">(Mnih &amp; Teh, 2012;</ref><ref type=\"bibr\" target=\"#b19\">Mikolov et al., 2013)</ref>, graphs <ref type=\"bibr\" target=\"#b0\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b11\">Grover &amp;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the Pong and Space Invaders datasets. First, we train A3C, a model-free RL agent, until convergence <ref type=\"bibr\" target=\"#b21\">(Mnih et al., 2016)</ref>. Then, we roll out the trained agent for a  i-c.org/ns/1.0\"><head>C.3. Full Atari Datasets</head><p>We use an open-source implementation of A3C <ref type=\"bibr\" target=\"#b21\">(Mnih et al., 2016)</ref> <ref type=\"foot\" target=\"#foot_3\">2</ref> t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utperformed traditional term-based techniques <ref type=\"bibr\" target=\"#b6\">(Guu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Karpukhin et al., 2020)</ref>. As the key step of finding the relevan ries and passages into lowdimensional vectors <ref type=\"bibr\" target=\"#b6\">(Guu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Karpukhin et al., 2020)</ref>, typically in a dual-encoder architectu e been devoted to improving this architecture <ref type=\"bibr\" target=\"#b6\">(Guu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b29\">Xiong et  the second category, the existing approaches fine-tuned pre-trained language models on labeled data <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b18\">Luan et  om a large collection of M passages. For this task, the dual-encoder architecture is widely adopted <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Qu et al  and irrelevant passages to query q, and they are defined the same as s(q, p) in Eq. (1). Following <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Qu et al e overall better than the sparse retrievers. Such a finding has also been reported in prior studies <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b29\">Xiong et 12\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b18\">Luan et al., 2021)</ref>. Both DPR <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020)</ref> and ME-BERT <ref type=\"bibr\" target=\"# s=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Overview</head><p>The task of dense passage retrieval <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020)</ref> is described as follows. Given a query eve the top-k candidate passages of unlabeled queries from the corpus by an efficient retriever DPR <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020)</ref>, and score them by the well-trained cr introduced as a dataset for open-domain QA. The queries were collected from Google search logs. DPR <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020)</ref> selected the queries that had short an  \u00d7 1 in fine-tuning stage on NQ and 512 \u00d7 8 in other settings. We use the in-batch negative setting <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020)</ref> on NQ and crossbatch negative setting   from them, DeepCT utilizes BERT to learn the term weight. The dense passage retrievers include DPR <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020)</ref>, DPR-E, ANCE <ref type=\"bibr\" target=\" .0\" place=\"foot\" n=\"2\" xml:id=\"foot_0\"><ref type=\"bibr\" target=\"#b29\">Xiong et al. (2020a)</ref> and<ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref> demonstrate the importance of hard nega. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: se <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020)</ref>. ERNIE-2.0 has the same networks as BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, and it introduces a continual pre-trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kar et al.)</ref>, TriviaQA <ref type=\"bibr\" target=\"#b11\">(Joshi et al., 2017)</ref> and Hot-potQA <ref type=\"bibr\" target=\"#b33\">(Yang et al., 2018)</ref>. In the pre-training stage, we reuse the pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2020b)</ref>, information retrieval <ref type=\"bibr\" target=\"#b18\">(Luan et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Khattab and Zaharia, 2020)</ref>, dialogue <ref type=\"bibr\" target=\"# ound large batch size can significantly improve the retrieval performance of dual-encoders. ColBERT <ref type=\"bibr\" target=\"#b13\">(Khattab and Zaharia, 2020)</ref> incorporated light-weight attention >(Luan et al., 2021)</ref>, NPRINC <ref type=\"bibr\" target=\"#b17\">(Lu et al., 2020)</ref>, Col-BERT <ref type=\"bibr\" target=\"#b13\">(Khattab and Zaharia, 2020)</ref> and Rock-etQA <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ense passage retrievers for baselines. The sparse retrievers include the traditional retriever BM25 <ref type=\"bibr\" target=\"#b31\">(Yang et al., 2017)</ref>, and four traditional retrievers enhanced b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b2\">(Craswell et al., 2020)</ref>, SQuAD <ref type=\"bibr\">(Rajpurkar et al.)</ref>, TriviaQA <ref type=\"bibr\" target=\"#b11\">(Joshi et al., 2017)</ref> and Hot-potQA <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: networks, including doc2query <ref type=\"bibr\" target=\"#b24\">(Nogueira et al., 2019b)</ref>, DeepCT <ref type=\"bibr\" target=\"#b3\">(Dai and Callan, 2019)</ref>, docTTTTTquery <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rity) for visualization: the longer the distance is, the less similar it is. ing question answering <ref type=\"bibr\" target=\"#b16\">(Lee et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Xiong et al.,  abeled data.</p><p>In the first category, different pre-training tasks for retrieval were proposed. <ref type=\"bibr\" target=\"#b16\">Lee et al. (2019)</ref> proposed a specific approach to pre-training . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rity) for visualization: the longer the distance is, the less similar it is. ing question answering <ref type=\"bibr\" target=\"#b16\">(Lee et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Xiong et al.,  abeled data.</p><p>In the first category, different pre-training tasks for retrieval were proposed. <ref type=\"bibr\" target=\"#b16\">Lee et al. (2019)</ref> proposed a specific approach to pre-training . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: networks, including doc2query <ref type=\"bibr\" target=\"#b24\">(Nogueira et al., 2019b)</ref>, DeepCT <ref type=\"bibr\" target=\"#b3\">(Dai and Callan, 2019)</ref>, docTTTTTquery <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ally create intuitive templates based on human introspection. For example, the seminal LAMA dataset <ref type=\"bibr\" target=\"#b143\">(Petroni et al., 2019)</ref> provides manually created cloze templat on; Yin et al. ( <ref type=\"formula\">2019</ref>)), but also other tasks such as relation extraction <ref type=\"bibr\" target=\"#b143\">(Petroni et al., 2019)</ref> or named entity recognition <ref type=\" his design.</p><p>Unconstrained Spaces In many cases, the answer space Z is the space of all tokens <ref type=\"bibr\" target=\"#b143\">(Petroni et al., 2019)</ref>, fixed-length spans <ref type=\"bibr\" ta  type=\"bibr\" target=\"#b26\">[16]</ref>, AutoPrompt <ref type=\"bibr\" target=\"#b169\">[159]</ref>, LAMA <ref type=\"bibr\" target=\"#b143\">[133]</ref> Fixed-LM Prompt Tuning Frozen Tuned Prefix-Tuning <ref t <ref type=\"bibr\">(Brown et al., 2020)</ref>. Typical examples of tuning-free prompting include LAMA <ref type=\"bibr\" target=\"#b143\">[133]</ref> and GPT-3 <ref type=\"bibr\" target=\"#b26\">[16]</ref>.</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hods make BART applicable, and (ii) breaks the difficulty of unified modelling among different tasks<ref type=\"bibr\" target=\"#b86\">(Khashabi et al., 2020)</ref>.</figDesc><table /></figure> <figure xm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  extractive QA (which identifies content from the context document containing the answer; e.g. SQuAD<ref type=\"bibr\" target=\"#b153\">(Rajpurkar et al., 2016)</ref>), multiple-choice QA (where the model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that initializing with manual templates can provide a better starting point for the search process. <ref type=\"bibr\" target=\"#b147\">Qin and Eisner (2021)</ref> propose to learn a mixture of soft templ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation aims to incorporate various types of guidance beyond the input text into the generation model <ref type=\"bibr\" target=\"#b178\">(Yu et al., 2020)</ref>. Specifically, the guidance signals could be  Bulling, 2016;</ref><ref type=\"bibr\" target=\"#b206\">Zhang et al., 2020b)</ref>, or knowledge bases <ref type=\"bibr\" target=\"#b178\">(Yu et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Dou et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  score the generation probability of each candidate and choose the one with the highest probability <ref type=\"bibr\" target=\"#b49\">(Ettinger, 2020)</ref>.</p><p>Mathematical Reasoning Mathematical rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b14\">(Bahdanau et al., 2014)</ref>, such as the popular Transformer architecture <ref type=\"bibr\" target=\"#b186\">(Vaswani et al., 2017)</ref>. Some examples of such attention masks . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -output examples for the target task, has long played a central role in many machine learning tasks <ref type=\"bibr\" target=\"#b91\">(Kotsiantis et al., 2007)</ref>, and natural language processing (NLP. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  score the generation probability of each candidate and choose the one with the highest probability <ref type=\"bibr\" target=\"#b49\">(Ettinger, 2020)</ref>.</p><p>Mathematical Reasoning Mathematical rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  which consists of two parallelly connected CA blocks.</p><p>Co-attention block. Co-attention block <ref type=\"bibr\" target=\"#b13\">(Lu et al., 2019)</ref> is a variant of the standard multi-head self-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b13\">(Lu et al., 2019)</ref> is a variant of the standard multi-head self-attention (MSA) block <ref type=\"bibr\" target=\"#b25\">(Vaswani et al., 2017)</ref>, which can capture global dependencies o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rn in the social connection between users and tweets, such as user profile and the number of posts. <ref type=\"bibr\" target=\"#b12\">Liu et al. (2018)</ref> use user profiles on the news propagation pat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ;</ref><ref type=\"bibr\" target=\"#b2\">Bond et al., 2017)</ref>, the number of punctuation and emojis <ref type=\"bibr\" target=\"#b3\">(Castillo et al., 2011)</ref>, and semantic features, such as writing . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hat deep learning methods perform better.</p><p>Visual features are important for news verification <ref type=\"bibr\" target=\"#b9\">(Jin et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">Shu et al., 201 in et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">Shu et al., 2017)</ref>, such as clarity score <ref type=\"bibr\" target=\"#b9\">(Jin et al., 2016)</ref>, the number of images <ref type=\"bibr\" target Jin et al., 2016)</ref>, the number of images <ref type=\"bibr\" target=\"#b30\">(Wu et al., 2015;</ref><ref type=\"bibr\" target=\"#b9\">Jin et al., 2016)</ref>. However, these features are manually crafted . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b3\">(Castillo et al., 2011)</ref>, and semantic features, such as writing styles <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2015)</ref> and language styles <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is widely used in NLP and VQA tasks <ref type=\"bibr\" target=\"#b18\">(Nguyen and Okatani, 2018;</ref><ref type=\"bibr\" target=\"#b7\">Gao et al., 2019)</ref>. The MSA block showed in Figure <ref type=\"fig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ks and a max-pooling layer. The multi-branch networks are the same as architectures in Inception V3 <ref type=\"bibr\" target=\"#b24\">(Szegedy et al., 2016)</ref>. The last parts of the major network are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ted from text content, including statistical features, such as the number of paragraphs in the text <ref type=\"bibr\" target=\"#b27\">(Volkova et al., 2017)</ref>, the percentage of negative words <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rn in the social connection between users and tweets, such as user profile and the number of posts. <ref type=\"bibr\" target=\"#b12\">Liu et al. (2018)</ref> use user profiles on the news propagation pat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>Since its birth <ref type=\"bibr\" target=\"#b29\">[30]</ref>, tremendous effort has been made by cryptography researche require huge computation and communication costs. Since its first introduction by Goldwasser et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref>, there have been significant improvements in the computati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[18]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, anonymous credentials <ref type=\"bibr\" target=\"#b22\">[23]</ref>, verifiable database outsourcing <ref type=\"bibr\" target=\"#b51\">[52]</ref>, verifiable machine learning <ref type=\"bibr\" target=\"#b50 \">[37]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, <ref type=\"bibr\" target=\"#b53\">[54]</ref>, and drawn a lo >, anonymous credentials <ref type=\"bibr\" target=\"#b22\">[23]</ref>, verifiable database outsourcing <ref type=\"bibr\" target=\"#b51\">[52]</ref>, verifiable machine learning <ref type=\"bibr\" target=\"#b50 ive data to generate a result that is returned to the client. Examples include database SQL queries <ref type=\"bibr\" target=\"#b51\">[52]</ref> and machine learning jobs <ref type=\"bibr\" target=\"#b50\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ider range of applications. The NTT module is the key building block in homomorphic encryption (HE) <ref type=\"bibr\" target=\"#b28\">[29]</ref> and modern public-key encryption schemes <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a much more lightweight proof on the critical path [1], <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>Since its birth <ref type=\"bibr\" target=\"#b29\">[30] mes up to hundreds of times, and could be up to a few minutes just for a single payment transaction <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>In this paper, we present PipeZK, an efficient pipe t=\"#b50\">[51]</ref>, privacy-preserving cryptocurrencies <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and various smart contracts on blockchains <ref type=\"bib tandard cryptographic benchmarks on average, and 5x for a real-world large-scale application, Zcash <ref type=\"bibr\" target=\"#b46\">[47]</ref>. When individually executed, the two subsystems of PipeZK  \">[23]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bib t=\"#b50\">[51]</ref>, privacy-preserving cryptocurrencies <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and various smart contracts on blockchains <ref type=\"bib e confidential transactions while still being able to prove the validity of each transaction. Zcash <ref type=\"bibr\" target=\"#b46\">[47]</ref> and Pinocchio Coin <ref type=\"bibr\" target=\"#b21\">[22]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: veral previous proposals have accelerated a single PMULT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bib so been well studied in the literature of circuit design <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[18]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a much more lightweight proof on the critical path [1], <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>Since its birth <ref type=\"bibr\" target=\"#b29\">[30] mes up to hundreds of times, and could be up to a few minutes just for a single payment transaction <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>In this paper, we present PipeZK, an efficient pipe t=\"#b50\">[51]</ref>, privacy-preserving cryptocurrencies <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and various smart contracts on blockchains <ref type=\"bib tandard cryptographic benchmarks on average, and 5x for a real-world large-scale application, Zcash <ref type=\"bibr\" target=\"#b46\">[47]</ref>. When individually executed, the two subsystems of PipeZK  \">[23]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bib t=\"#b50\">[51]</ref>, privacy-preserving cryptocurrencies <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and various smart contracts on blockchains <ref type=\"bib e confidential transactions while still being able to prove the validity of each transaction. Zcash <ref type=\"bibr\" target=\"#b46\">[47]</ref> and Pinocchio Coin <ref type=\"bibr\" target=\"#b21\">[22]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a processing, including electronic voting <ref type=\"bibr\" target=\"#b53\">[54]</ref>, online auction <ref type=\"bibr\" target=\"#b25\">[26]</ref>, anonymous credentials <ref type=\"bibr\" target=\"#b22\">[23]  applications <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bib e properties, including electronic voting <ref type=\"bibr\" target=\"#b53\">[54]</ref>, online auction <ref type=\"bibr\" target=\"#b25\">[26]</ref>, anonymous credentials <ref type=\"bibr\" target=\"#b22\">[23]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a much more lightweight proof on the critical path [1], <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>Since its birth <ref type=\"bibr\" target=\"#b29\">[30] mes up to hundreds of times, and could be up to a few minutes just for a single payment transaction <ref type=\"bibr\" target=\"#b46\">[47]</ref>.</p><p>In this paper, we present PipeZK, an efficient pipe t=\"#b50\">[51]</ref>, privacy-preserving cryptocurrencies <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and various smart contracts on blockchains <ref type=\"bib tandard cryptographic benchmarks on average, and 5x for a real-world large-scale application, Zcash <ref type=\"bibr\" target=\"#b46\">[47]</ref>. When individually executed, the two subsystems of PipeZK  \">[23]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bib t=\"#b50\">[51]</ref>, privacy-preserving cryptocurrencies <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, and various smart contracts on blockchains <ref type=\"bib e confidential transactions while still being able to prove the validity of each transaction. Zcash <ref type=\"bibr\" target=\"#b46\">[47]</ref> and Pinocchio Coin <ref type=\"bibr\" target=\"#b21\">[22]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b18\">[19]</ref> and many pairing-based proof systems <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>. We expect our architecture insights would inspire more op \">[20]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b47\">[48]</ref> and industry <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , and l is the cross-entropy loss. We can then compute the optimal \u03b2 via kernel mean matching (KMM) <ref type=\"bibr\" target=\"#b8\">[9]</ref> by solving a quadratic problem,</p><formula xml:id=\"formula_  over the DGI representations we can use only the instance reweighting regularization from Equation <ref type=\"bibr\" target=\"#b8\">(9)</ref>. Table <ref type=\"table\" target=\"#tab_3\">3</ref> confirms th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  between means of distributions in some rich Hilbert kernel space. Central moment discrepancy (CMD) <ref type=\"bibr\" target=\"#b37\">[38]</ref> extends this idea and matches means and higher order momen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 5\">[16]</ref>. We also include methods based on unsupervised node representation learning (DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref> and DGI <ref type=\"bibr\" target=\"#b31\">[32]</ref>) as a th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utional shift for standard GNN models such as GCNs <ref type=\"bibr\" target=\"#b14\">[15]</ref>, MPNNs <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and many more <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Thes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al networks. Surprisingly, most work on semi-supervised learning using GNNs for node classification <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" ta ions Z which are used for unsupervised <ref type=\"bibr\" target=\"#b29\">[30]</ref> or semi-supervised <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> learning. We denot >First we consider the case of addressing distributional shift for standard GNN models such as GCNs <ref type=\"bibr\" target=\"#b14\">[15]</ref>, MPNNs <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and many m e. adjacency matrix A, A \u2208 R |V |\u00d7|V | ) which form connections between them. Graph neural networks <ref type=\"bibr\" target=\"#b14\">[15]</ref> are neural networks that operate on both node features and d corresponding model accuracy (y-axis) for three common GNN benchmarks using the classic GCN model <ref type=\"bibr\" target=\"#b14\">[15]</ref> in Figure <ref type=\"figure\" target=\"#fig_1\">1</ref>. The  on F with parameters \u0398, over some adjacency matrix A: \u03a6 = F(\u0398, Z, A).</p><p>(4) In the original GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref>, the graph inductive bias is multiplicative at each layer  . In our experiments, we show the application of SR-GNN on two other representative GNN models: GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref> and DGI <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p></di \" target=\"#b10\">[11]</ref>. We use the same validation and test splits as in the original GCN paper <ref type=\"bibr\" target=\"#b14\">[15]</ref> and OGB benchmark. We use the remaining nodes for training thods to investigate their performance under distributional shifts: (1) Traditional GNN Models: GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, (2) Linear e also study how our Shift-Robust framework can be applied to two other representative models -GCNs <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and DGI <ref type=\"bibr\" target=\"#b31\">[32]</ref>. For th et). We compare the CMD regularizer and DANN regularizer using the same GNN architectures (i.e. GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref>, APPNP <ref type=\"bibr\" target=\"#b15\">[16]</ref>). The hyp cted to be larger. We present the accuracy (Micro-F1) and distribution shifts (CMD) of a deeper GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref> model and our Shift-Robust GCN using regularization propos oot_2\">The exact use of the adjacency matrix varies with specific GNN methods. For instance, the GCN<ref type=\"bibr\" target=\"#b14\">[15]</ref> performs mean-pooling using matrix multiplication:\u00c3 = D \u2212 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tribution discrepancy minimization (MMD) has been adopted to train network embedding across domains <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Other regularizations for the latent state of GNNs have b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ar area where this can apply is in the spam and abuse domain, a common area of application for GNNs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplication for GNNs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. However, the labels in these problems usually come from ex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplication for GNNs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. However, the labels in these problems usually come from ex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: R-GNN on two other representative GNN models: GCN <ref type=\"bibr\" target=\"#b14\">[15]</ref> and DGI <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  supervised node representation learning (DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref> and DGI <ref type=\"bibr\" target=\"#b31\">[32]</ref>) as a third category (3). For these methods, we use a line bmed and 256 for ogb-arxiv, with a dropout of 0.5. In order to learn effective representations, DGI <ref type=\"bibr\" target=\"#b31\">[32]</ref> usually needs a higher dimensional hidden space and so, fo applied to two other representative models -GCNs <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and DGI <ref type=\"bibr\" target=\"#b31\">[32]</ref>. For the GCN model, we apply the regularized loss from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imize a domain classifier between source and target graphs <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. In addition, distribution discrepancy minimization (MMD) h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e consider a class of linearized models (APPNP <ref type=\"bibr\" target=\"#b15\">[16]</ref>, SimpleGCN <ref type=\"bibr\" target=\"#b33\">[34]</ref>, etc) which decouple GNNs into non-linear feature encoding r function F 1 , which is decoupled from multi-layer neural network feature encoder F 2 . SimpleGCN <ref type=\"bibr\" target=\"#b33\">[34]</ref> is an example of linearized model when F 1 (A) = A k X. An \" target=\"#b14\">[15]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, (2) Linearized GNNs: SGC <ref type=\"bibr\" target=\"#b33\">[34]</ref> and APPNP <ref type=\"bibr\" target=\"#b15\">[16]</ref>. We al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e consider a class of linearized models (APPNP <ref type=\"bibr\" target=\"#b15\">[16]</ref>, SimpleGCN <ref type=\"bibr\" target=\"#b33\">[34]</ref>, etc) which decouple GNNs into non-linear feature encoding r function F 1 , which is decoupled from multi-layer neural network feature encoder F 2 . SimpleGCN <ref type=\"bibr\" target=\"#b33\">[34]</ref> is an example of linearized model when F 1 (A) = A k X. An \" target=\"#b14\">[15]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, (2) Linearized GNNs: SGC <ref type=\"bibr\" target=\"#b33\">[34]</ref> and APPNP <ref type=\"bibr\" target=\"#b15\">[16]</ref>. We al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5\">[16]</ref>. We also include methods based on unsupervised node representation learning (DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref> and DGI <ref type=\"bibr\" target=\"#b31\">[32]</ref>) as a th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: than adversarial heads, faster loss convergence and easier training. Maximum mean discrepancy (MMD) <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> is a metric that m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imize a domain classifier between source and target graphs <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. In addition, distribution discrepancy minimization (MMD) h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emi-supervised learning using GNNs for node classification <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> have ignored this cri ef type=\"bibr\" target=\"#b29\">[30]</ref> or semi-supervised <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> learning. We denote the labels for SSL as {y i }.</p><p>The k + \u03b1 k\u22121 i=0 (1 \u2212 \u03b1) i \u00c3i approximated personalized page rank F(\u0398, X).</formula><p>feature encoder <ref type=\"bibr\" target=\"#b10\">(11)</ref> It first applies a feature encoder F on node features X an ype=\"bibr\" target=\"#b26\">[27]</ref>, ogb-arxiv <ref type=\"bibr\" target=\"#b25\">[26]</ref> and Reddit <ref type=\"bibr\" target=\"#b10\">[11]</ref>. We use the same validation and test splits as in the orig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplication for GNNs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. However, the labels in these problems usually come from ex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utional shift for standard GNN models such as GCNs <ref type=\"bibr\" target=\"#b14\">[15]</ref>, MPNNs <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and many more <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Thes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tribution discrepancy minimization (MMD) has been adopted to train network embedding across domains <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Other regularizations for the latent state of GNNs have b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ar area where this can apply is in the spam and abuse domain, a common area of application for GNNs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: though no existing work studies the distributional shift problem in GNNs, transfer learning of GNNs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> has explored diffe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cation they resort to complicated spatial attention models <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, which are difficul type=\"bibr\" target=\"#b21\">22]</ref>. In multi-label recognition, one representative paradigm is SRN <ref type=\"bibr\" target=\"#b38\">[39]</ref>, which used a spatial regularization network to rectify or We want to emphasize that our CSRA is significantly different from the previous attention model SRN <ref type=\"bibr\" target=\"#b38\">[39]</ref>. CSRA reuses the classifier's weights and has no additiona raining stage and ignore these unspecified labels in the test stage, following previous settings in <ref type=\"bibr\" target=\"#b38\">[39]</ref>. We adopt VIT <ref type=\"bibr\" target=\"#b6\">[7]</ref> as o assic ResNet backbones, but also for emerging  <ref type=\"bibr\" target=\"#b11\">[12]</ref> 82.9 --SRN <ref type=\"bibr\" target=\"#b38\">[39]</ref> 86.2 75.9 81.3 VAA <ref type=\"bibr\" target=\"#b25\">[26]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ode or pretrained models, we also trained our own models to obtain baseline results using MobileNet <ref type=\"bibr\" target=\"#b24\">[25]</ref>, ResNet <ref type=\"bibr\" target=\"#b14\">[15]</ref>, Efficie et\" and \"MobileNet\" were EfficientNet-B3 <ref type=\"bibr\" target=\"#b28\">[29]</ref> and MobileNet-V2 <ref type=\"bibr\" target=\"#b24\">[25]</ref>, respectively. All the VIT <ref type=\"bibr\" target=\"#b6\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a variety of vision tasks, such as detection <ref type=\"bibr\" target=\"#b22\">[23]</ref> and tracking <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. In multi-label re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are generated by methods like EdgeBoxes <ref type=\"bibr\" target=\"#b1\">[2]</ref> or Selective Search <ref type=\"bibr\" target=\"#b15\">[16]</ref>, then sent to a shared CNN. Finally, a category-wise max-p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: suffer from high computational cost or manually defined adjacency matrices.  Proposal based methods <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" ta hyperparameter tuning, making it unsuitable in real applications.</p><p>Generating object proposals <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta CN <ref type=\"bibr\" target=\"#b3\">[4]</ref> 94.0 -ASL <ref type=\"bibr\" target=\"#b0\">[1]</ref> 94 HCP <ref type=\"bibr\" target=\"#b32\">[33]</ref> 90.5 -RCP <ref type=\"bibr\" target=\"#b30\">[31]</ref> 92.2 -. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h as knowledge distillation <ref type=\"bibr\" target=\"#b20\">[21]</ref>, visual attention consistency <ref type=\"bibr\" target=\"#b12\">[13]</ref>, and suppressing negative class activation map <ref type=\" r\" target=\"#b38\">[39]</ref> 86.2 75.9 81.3 VAA <ref type=\"bibr\" target=\"#b25\">[26]</ref> 86.4 --VAC <ref type=\"bibr\" target=\"#b12\">[13]</ref> 87. </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><h target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n on the train set and evaluate them on the val set. Following <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, we report the precision, recall and F1-measure with and wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b17\">18]</ref>, Recurrent Neural Network (RNN) <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> and Graph Convolutional Network (GCN) <ref type=\"bibr\" targ ond what GCN can represent. Recurrent Neural Network (RNN) has also been applied in various studies <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in various studies <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> to explore high order label dependencies <ref type=\"bibr\" t t=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> to explore high order label dependencies <ref type=\"bibr\" target=\"#b37\">[38]</ref>. But, the effectiveness of RNN in multi-label tasks is yet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , although many network architectures have been proposed for single-label classification, e.g., VGG <ref type=\"bibr\" target=\"#b26\">[27]</ref>, ResNet <ref type=\"bibr\" target=\"#b14\">[15]</ref>, Efficie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b17\">18]</ref>, Recurrent Neural Network (RNN) <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> and Graph Convolutional Network (GCN) <ref type=\"bibr\" targ ond what GCN can represent. Recurrent Neural Network (RNN) has also been applied in various studies <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: labels, object proposals, and attention mechanisms. To explore semantic relations, Bayesian network <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, Recurrent Neural   focus on the semantic relationship between objects or object classes, by using dependency networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>, pairwise co-occurrence adjacency matrices <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: llions, or even billions, of valid schedules with a wide range of performance and energy efficiency <ref type=\"bibr\" target=\"#b48\">[49]</ref>. Considering the vast range of DNN layer dimensions and ha r cost models <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b70\">[71]</ref>. However, navig </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Brute-force Approaches:</head><p>Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> Brute-force &amp; Random dMazeRunner <ref type=\"bibr\" targ  Buffer space <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b64\">[65]</ref>, <ref type=\"bib ial architecture. Listing 1 shows an example of a schedule. Here, we use a loop-nest representation <ref type=\"bibr\" target=\"#b48\">[49]</ref> to explicitly describe how the computation of a convolutio e the one with the best result for the target metric, and 2) the Timeloop Hybrid mapper in Timeloop <ref type=\"bibr\" target=\"#b48\">[49]</ref> that randomly selects a tiling factorization, prunes super. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b55\">[56]</ref> Beamsearch <ref type=\"bibr\" target=\"#b2\">[3]</ref>, OpenTuner <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref> FlexFlow <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[27]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b68\">[69]</ref>.</p><p>State-of-the-art DNN accelerators typically incorpo \">[54]</ref>, <ref type=\"bibr\" target=\"#b54\">[55]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref>, <ref type=\"bibr\" target=\"#b68\">[69]</ref>, <ref type=\"bibr\" target=\"#b70\">[71]</ref>.</p><p>3) Targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Tensor Comprehension <ref type=\"bibr\" target=\"#b66\">[67]</ref> Polyhedral Transformations Tiramisu <ref type=\"bibr\" target=\"#b7\">[8]</ref> CoSA Mixed-Integer Programming (MIP) on operator-level sched ware-managed spatial accelerators that we target. In addition, existing polyhedral-based approaches <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b72\">[73]</ref>, and program synthesis <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, <ref type=\"bibr equals the total valid levels for permutation. The traffic iteration term can thus be expressed as: <ref type=\"bibr\" target=\"#b9\">(10)</ref> This turns the linear objective into quadratic as we multip. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b55\">[56]</ref> Beamsearch <ref type=\"bibr\" target=\"#b2\">[3]</ref>, OpenTuner <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref> FlexFlow <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\">[1]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ls of memory hierarchy, a commonly adopted architecture template in today's DNN accelerator designs <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Polly+Pluto <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref> Tensor Comprehension <ref type=\"bibr\" target=\"#b66\">[67]</ #b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref> [21], algorithm selection <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b72\">[73]</ref>, and program sy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[28]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b43\">[44]</ref>, <ref type=\"bibr\" target=\"#b53\">[54]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e hierar-chies <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> and in-cache address translation <ref type=\"bibr\" target=\" he hierarchies <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> aim to delay this translation to the memory side, thus rem he hierarchies <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> and in-cache address translation <ref type=\"bibr\" target=\" ause of space constraints.</p><p>Virtual caches: Proposals for virtual caches date back to the '80s <ref type=\"bibr\" target=\"#b15\">[16]</ref>, with recent papers proposing virtual caches in the contex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: onfiguration (as described above) at various hierarchy levels, and measure memory-level parallelism <ref type=\"bibr\" target=\"#b11\">[12]</ref> in benchmarks to account for latency overlap.</p><p>To eva. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , DRAM caches <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b60\">[61]</ref>, and non-volatile byte-addressable memories <ref type=\"bib n with high locality (e.g., 3% of the data captures most of the memory accesses in server workloads <ref type=\"bibr\" target=\"#b60\">[61]</ref>), the system requires frequent lookups to 8M mappings. Whi d DRAM caches <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b56\">[57]</ref>, <ref type=\"bibr\" target=\"#b60\">[61]</ref> means that, unlike traditional TLB-based translation, Midg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b15\">[16]</ref>, with recent papers proposing virtual caches in the context of GPUs <ref type=\"bibr\" target=\"#b67\">[68]</ref>. Ceckleov et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion of modern VIPT (and our VIMT) cache hierarchies and allowing the L1 cache to scale in capacity <ref type=\"bibr\" target=\"#b44\">[45]</ref>. Implementations of guard pages <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he complex hardware and OS support pose verification burdens despite which design bugs still abound <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Individual CPU cores (and recent accelerators) integrate . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: translation hardware? This question is inspired in part by prior work on virtual cache hierar-chies <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr only required for efficient capacity management on the memory side.</p><p>Virtual cache hierarchies <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr igates the homonym and synonym problems and access control limitations of virtual cache hierarchies <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr ng virtual caches in the context of GPUs <ref type=\"bibr\" target=\"#b67\">[68]</ref>. Ceckleov et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref> summarize a vari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> and in-cache address translation <ref type=\"bibr\" target=\"#b64\">[65]</ref>, which reduce address translation pressure by deferring th arget=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> and in-cache address translation <ref type=\"bibr\" target=\"#b64\">[65]</ref>. Midgard avoids modifying existing programming abstraction ons but requires significant software modifications.</p><p>Intermediate address spaces: Wood et al. <ref type=\"bibr\" target=\"#b64\">[65]</ref> propose a global virtual address space used to address the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by recent work on die-stacking, chiplets, DRAM caches <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b60\">[61]</ref>, and non-volati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> and in-cache address translation <ref type=\"bibr\" target=\"#b64\">[65]</ref>, which reduce address translation pressure by deferring th arget=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> and in-cache address translation <ref type=\"bibr\" target=\"#b64\">[65]</ref>. Midgard avoids modifying existing programming abstraction ons but requires significant software modifications.</p><p>Intermediate address spaces: Wood et al. <ref type=\"bibr\" target=\"#b64\">[65]</ref> propose a global virtual address space used to address the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he complex hardware and OS support pose verification burdens despite which design bugs still abound <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Individual CPU cores (and recent accelerators) integrate . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ks into cache-friendly and cache-averse by learning from the behavior of the Belady's MIN algorithm <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b33\">[34]</ref> at run-time. The . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  TPC-E simulation is done by replaying an instruction trace of the MySQL server collected using Pin <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  OS-assisted copy-onaccess <ref type=\"bibr\" target=\"#b66\">[67]</ref>, prefetching inclusion victims <ref type=\"bibr\" target=\"#b40\">[41]</ref>, back-filling crosscore LLC victims into inner-level cache. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  OS-assisted copy-onaccess <ref type=\"bibr\" target=\"#b66\">[67]</ref>, prefetching inclusion victims <ref type=\"bibr\" target=\"#b40\">[41]</ref>, back-filling crosscore LLC victims into inner-level cache. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esident in private caches. The Hawkeye policy uses a three-bit re-reference prediction value (RRPV) <ref type=\"bibr\" target=\"#b18\">[19]</ref> to distinguish between the cacheaverse (RRPV=7) and cache- can also be used with other LLC replacement policies that employ RRPVs to grade the blocks in a set <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref>. The most gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  by coherent writes, a private cache eviction is always notified to the home sparse directory slice <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. The eviction n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hannel attacks <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[55]</ref>- <ref type=\"bibr\" target=\"#b56\">[57]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref>, <ref type=\"bibr\" target=\"#b66\">[67]</ref>. These proposals improve shared cache security through lin ibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref>, OS-assisted copy-onaccess <ref type=\"bibr\" target=\"#b66\">[67]</ref>, prefetching inclusion victims <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[22]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>, <ref type=\"bibr\" target=\"#b57\">[58]</ref>, <ref type=\"bibr\" target=\"#b65\">[66]</ref>. In this paper,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b55\">[56]</ref>, <ref type=\"bibr\" target=\"#b56\">[57]</ref>, dynamic encryption of cache addresses <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref>, OS-assisted co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplication is run for a representative segment of 500M dynamic instructions selected using SimPoint <ref type=\"bibr\" target=\"#b48\">[49]</ref>. The early-finishing applications continue to run until ea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b94\">[95]</ref>, AutoMine <ref type=\"bibr\" target=\"#b57\">[58]</ref>, Pangolin <ref type=\"bibr\" target=\"#b15\">[16]</ref>, Peregrine <ref type=\"bibr\" target=\"#b43\">[44]</ref>  have  type=\"bibr\" target=\"#b94\">[95]</ref>, AutoMine <ref type=\"bibr\" target=\"#b57\">[58]</ref>, Pangolin <ref type=\"bibr\" target=\"#b15\">[16]</ref>, Peregrine <ref type=\"bibr\" target=\"#b43\">[44]</ref>, Grap ism in Arabesque is leveraged by Gramer to remove irrelevant subgraphs, it is not sufficient enough <ref type=\"bibr\" target=\"#b15\">[16]</ref> to prune the search space for an arbitrary pattern, and be ntation technique, i.e., converting the undirected data graph G into a directed acyclic graph (DAG) <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The idea is to establish an order between the endpoints o f type=\"bibr\" target=\"#b83\">[84]</ref>, Kaleido <ref type=\"bibr\" target=\"#b94\">[95]</ref>, Pangolin <ref type=\"bibr\" target=\"#b15\">[16]</ref>, AutoMine <ref type=\"bibr\" target=\"#b57\">[58]</ref>, Graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b68\">69]</ref>, SL <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Orange entries are updated. Grey entries are ones became useless.</p><p>Previous work uses a vector <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> to implement a c-m =\"bibr\" target=\"#b57\">[58]</ref>, GraphZero <ref type=\"bibr\" target=\"#b56\">[57]</ref> and Sandslash <ref type=\"bibr\" target=\"#b14\">[15]</ref> are GPM systems targeting single-machine. There are also g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 21]</ref>, k-MC <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b68\">69]</ref>, SL <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b65\">66,</ref><ref type=\"bibr\" target=\"#b75\">76,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ytics accelerators <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b57\">[58]</ref>, Pangolin <ref type=\"bibr\" target=\"#b15\">[16]</ref>, Peregrine <ref type=\"bibr\" target=\"#b43\">[44]</ref>  have been proposed to improve productivity. In general, t e the search tree and avoid isomorphism tests. The pattern is analyzed to generate a matching order <ref type=\"bibr\" target=\"#b43\">[44]</ref> and a symmetry order <ref type=\"bibr\" target=\"#b56\">[57]</ lutions use pattern analysis <ref type=\"bibr\" target=\"#b57\">[58]</ref> to generate a matching order <ref type=\"bibr\" target=\"#b43\">[44]</ref> and a symmetry order <ref type=\"bibr\" target=\"#b56\">[57]</ ation</head><p>A major computation in AutoMine <ref type=\"bibr\" target=\"#b57\">[58]</ref>, Peregrine <ref type=\"bibr\" target=\"#b43\">[44]</ref> and GraphZero <ref type=\"bibr\" target=\"#b56\">[57]</ref> is type=\"bibr\" target=\"#b57\">[58]</ref>, Pangolin <ref type=\"bibr\" target=\"#b15\">[16]</ref>, Peregrine <ref type=\"bibr\" target=\"#b43\">[44]</ref>, GraphZero <ref type=\"bibr\" target=\"#b56\">[57]</ref> simpl //www.tei-c.org/ns/1.0\"><head>B. FlexMiner Execution Flow</head><p>Pattern-aware software solutions <ref type=\"bibr\" target=\"#b43\">[44]</ref> use recursion, which is not suitable for direct implementa er generates matching order and symmetry order using the same approach in prior software frameworks <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b78\">79,</ref><ref type=\"bibr\" target=\"#b84\">85,</ref><ref type=\"bibr\" target=\"#b90\">91]</ref>, k-CL <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b67\">68,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b78\">79,</ref><ref type=\"bibr\" target=\"#b84\">85,</ref><ref type=\"bibr\" target=\"#b90\">91]</ref>, k-CL <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b69\">70,</ref><ref type=\"bibr\" target=\"#b73\">74]</ref>, web spam detection. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" target=\"#b67\">68,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b29\">[30]</ref>, and the concept of multichip modules (MCMs) dates back even further <ref type=\"bibr\" target=\"#b6\">[7]</ref> <ref type=\"bibr\" target=\"#b21\">[22]</ref>[26] <ref type=\"bib  SoC partitioning is not new, and multi-chip module (MCM) technologies have been around for decades <ref type=\"bibr\" target=\"#b6\">[7]</ref>[9] <ref type=\"bibr\" target=\"#b21\">[22]</ref>[26] <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ctional testing, individual chiplets can also be tested for maximum performance (e.g., clock speed) <ref type=\"bibr\" target=\"#b5\">[6]</ref> <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Figure <ref type= f IP and silicon reuse, and more.</p><p>Chiplets also require new inter-chiplet communication paths <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Compared to on-chip metal, these interconnects involve long die area was also required to implement our Infinity Fabric\u2122 interconnect between the four chiplets <ref type=\"bibr\" target=\"#b5\">[6]</ref> as well as other per-chip circuitry as discussed in Section  terconnect, which are implemented as point-to-point links directly on the organic package substrate <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The IFOP links utilize custom high-speed SerDes circuits. C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ities and research in partitioning systems of chips (SoCs) into multiple silicon die over the years <ref type=\"bibr\" target=\"#b3\">[4]</ref>[5] <ref type=\"bibr\" target=\"#b7\">[8]</ref>[19] <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a higher-bandwidth solution such as silicon interposers <ref type=\"bibr\" target=\"#b1\">[2]</ref>[16] <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>The second factor against silicon interposers for o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a higher-bandwidth solution such as silicon interposers <ref type=\"bibr\" target=\"#b1\">[2]</ref>[16] <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>The second factor against silicon interposers for o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: concept of multichip modules (MCMs) dates back even further <ref type=\"bibr\" target=\"#b6\">[7]</ref> <ref type=\"bibr\" target=\"#b21\">[22]</ref>[26] <ref type=\"bibr\" target=\"#b30\">[31]</ref>, AMD is taki p module (MCM) technologies have been around for decades <ref type=\"bibr\" target=\"#b6\">[7]</ref>[9] <ref type=\"bibr\" target=\"#b21\">[22]</ref>[26] <ref type=\"bibr\" target=\"#b30\">[31]</ref>. However, ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  even further <ref type=\"bibr\" target=\"#b6\">[7]</ref> <ref type=\"bibr\" target=\"#b21\">[22]</ref>[26] <ref type=\"bibr\" target=\"#b30\">[31]</ref>, AMD is taking the theory of chiplet-based architectures a or decades <ref type=\"bibr\" target=\"#b6\">[7]</ref>[9] <ref type=\"bibr\" target=\"#b21\">[22]</ref>[26] <ref type=\"bibr\" target=\"#b30\">[31]</ref>. However, many past MCM applications have been sequestered. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: concept of multichip modules (MCMs) dates back even further <ref type=\"bibr\" target=\"#b6\">[7]</ref> <ref type=\"bibr\" target=\"#b21\">[22]</ref>[26] <ref type=\"bibr\" target=\"#b30\">[31]</ref>, AMD is taki p module (MCM) technologies have been around for decades <ref type=\"bibr\" target=\"#b6\">[7]</ref>[9] <ref type=\"bibr\" target=\"#b21\">[22]</ref>[26] <ref type=\"bibr\" target=\"#b30\">[31]</ref>. However, ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of chips (SoCs) into multiple silicon die over the years <ref type=\"bibr\" target=\"#b3\">[4]</ref>[5] <ref type=\"bibr\" target=\"#b7\">[8]</ref>[19] <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and the conce. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  technologies starting with the AMD Radeon\u2122 R9 \"Fury\" GPUs with high-bandwidth memory (HBM) in 2015 <ref type=\"bibr\" target=\"#b15\">[16]</ref>. A   Even accounting for some load imbalance across the CC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>, instruction cache <ref type=\"bibr\" target=\"#b38\">[39]</ref>, TLB <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ocessor architecture.</p><p>Partitioning-based solutions <ref type=\"bibr\" target=\"#b53\">[54]</ref>- <ref type=\"bibr\" target=\"#b58\">[59]</ref> that defend against conflict-based side-channel attacks ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cess to Secret Data. The literature contains a vast amount of prior work on memory safety solutions <ref type=\"bibr\" target=\"#b64\">[65]</ref>- <ref type=\"bibr\" target=\"#b67\">[68]</ref> that prevent un. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s a vast amount of prior work on memory safety solutions <ref type=\"bibr\" target=\"#b64\">[65]</ref>- <ref type=\"bibr\" target=\"#b67\">[68]</ref> that prevent unauthorized access to secret data -these ran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s a vast amount of prior work on memory safety solutions <ref type=\"bibr\" target=\"#b64\">[65]</ref>- <ref type=\"bibr\" target=\"#b67\">[68]</ref> that prevent unauthorized access to secret data -these ran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly 16 such regions in SMT mode. Thus, we hypothesize that the micro-op cache is not way-partitioned <ref type=\"bibr\" target=\"#b47\">[48]</ref> into 32 4-way sets for each thread, but is rather divided . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: up several decode cycles. The decode pipeline can provide a peak bandwidth of 5 micro-ops per cycle <ref type=\"bibr\" target=\"#b32\">[33]</ref>. In contrast, AMD Zen features four 1:2 decoders and releg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b40\">[41]</ref>, functional units <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and memory buses <ref type=\"bibr\" target=\"#b39\">[40]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erincreasing instruction footprint of server applications. According to a study conducted by Google <ref type=\"bibr\" target=\"#b24\">[25]</ref> over three years on one of their Warehouse Scale Computing. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bib . Other studies also report little benefit when adding context information (e.g., Markov Predcitors <ref type=\"bibr\" target=\"#b22\">[23]</ref>, end of Section.2.1).</p><p>2) Merging spatio-temporal bas \">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bib bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Markov-based prefetchers <ref type=\"bibr\" target=\"#b22\">[23]</ref> use probabilities to predict and prefetch the next cache m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a \"good-enough\" distance is identified through careful tuning.</p><p>Inspired by previous proposals <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, this work buil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion Prefetching Championship (IPC-1). The ChampSim version used for IPC-1 modeled a simple frontend <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Our modified version extends the current model of the dev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctor for performance, a plethora of prefetching techniques have been proposed over the last decades <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr [43]</ref>, but more advanced ones have been proposed, from prefetchers guided by branch prediction <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr reasing accuracy, prefetchers that interact with the branch prediction mechanism have been proposed <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rring impractical storage costs.</p><p>Other types of prefetchers interact with hardware structures <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" rior work in prefetching has adopted look-ahead mechanisms <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" at always prefetches the next cache line given the current access. It adds no area overhead. \u2022 SN4L <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a memory-efficient proposal that implements a 16K-bit vec KB of storage.</p><p>\u2022 MANA <ref type=\"bibr\" target=\"#b4\">[5]</ref> is a refinement of SN4L-Dir-BTB <ref type=\"bibr\" target=\"#b5\">[6]</ref> that uses an 8-bit vector for consecutive prefetchers (previ  miss rates, due to its mechanism to reactively pre-fill such branches. More recently, Ansari et al <ref type=\"bibr\" target=\"#b5\">[6]</ref> propose SN4L-Dis-BTB, a lightweight prefetcher that reduces . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: virtual L1 caches and as consequence can efficiently train the L1 prefetcher with virtual addresses <ref type=\"bibr\" target=\"#b17\">[18]</ref>. However, for x86 cores employing smaller virtually-indexe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s the L1I is usually plenty of dead cache lines, that could be evicted without increasing miss rate <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Since the Entangling prefetcher commonly entangles the de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctor for performance, a plethora of prefetching techniques have been proposed over the last decades <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr [43]</ref>, but more advanced ones have been proposed, from prefetchers guided by branch prediction <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr reasing accuracy, prefetchers that interact with the branch prediction mechanism have been proposed <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Techniques to adjust the look-ahead distance dynamically using heuristics have also been proposed <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, however, the l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o processing power <ref type=\"bibr\" target=\"#b25\">[27,</ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" target=\"#b63\">65]</ref>. The fundamental is get=\"#b23\">25,</ref><ref type=\"bibr\" target=\"#b29\">31,</ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" target=\"#b47\">49,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b24\">26,</ref><ref type=\"bibr\" target=\"#b44\">46]</ref> and congestion control algorithms <ref type=\"bibr\" target=\"#b34\">[36,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" ta -to-end RTT delay as the congestion signal, similar to recent sendermanaged, delay-based mechanisms <ref type=\"bibr\" target=\"#b34\">[36,</ref><ref type=\"bibr\" target=\"#b42\">44,</ref><ref type=\"bibr\" ta ased transport implementation. Solutions like 1RMA <ref type=\"bibr\" target=\"#b57\">[59]</ref>, Swift <ref type=\"bibr\" target=\"#b34\">[36]</ref>, HPCC <ref type=\"bibr\" target=\"#b36\">[38]</ref>, IRN <ref   would otherwise maintain states at MNs. To handle MN incast at CNs, we draw inspiration from Swift <ref type=\"bibr\" target=\"#b34\">[36]</ref> by allowing cwnd to fall below one packet when long delay . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">56]</ref> or raw memory devices with no processing power <ref type=\"bibr\" target=\"#b25\">[27,</ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" tar get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b23\">25,</ref><ref type=\"bibr\" target=\"#b29\">31,</ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ggregate resources <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b61\">63,</ref><ref type=\"bibr\" target=\"#b66\">68]</ref>-separating hardware resources into different network-attach get=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b60\">62,</ref><ref type=\"bibr\" target=\"#b66\">68]</ref>. With the success of disaggregated storage, researchers in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re seeing the needs from management and resource utilization perspectives to disaggregate resources <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b61\">63,</ref><ref type=\"bibr\" ta \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b54\">56,</ref><ref type=\"bibr\" target=\"#b55\">57,</ref><ref type=\"bibr\" target=\"#b63\">65,</ref><ref type=\"bibr\" target=\"#b67\">69]</ref>. Different from storage disaggregation, MemDisagg needs to  get=\"#b23\">25,</ref><ref type=\"bibr\" target=\"#b53\">55,</ref><ref type=\"bibr\" target=\"#b54\">56,</ref><ref type=\"bibr\" target=\"#b67\">69]</ref>, usually on top of RDMA. The common limitation of these sys. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  VA allocation tree that records allocated VA ranges and permissions, similar to the Linux vma tree <ref type=\"bibr\" target=\"#b33\">[35]</ref>. To allocate size k of VAs, it first finds an available ad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that could overflowing a bucket. Existing hash-based page table designs rely on collision chaining <ref type=\"bibr\" target=\"#b11\">[12]</ref> or open addressing <ref type=\"bibr\" target=\"#b71\">[73]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -value store. We compared Clio with native RDMA, two RDMA-based disaggregated/remote memory systems <ref type=\"bibr\" target=\"#b32\">[34,</ref><ref type=\"bibr\" target=\"#b63\">65]</ref>, a software emulat  We compare Clio with native one-sided RDMA, Clover <ref type=\"bibr\" target=\"#b63\">[65]</ref>, HERD <ref type=\"bibr\" target=\"#b32\">[34]</ref>, and Le-goOS <ref type=\"bibr\" target=\"#b54\">[56]</ref>. We. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead of the first two tasks, allowing foreground request to proceed immediately.</p><p>A recent work <ref type=\"bibr\" target=\"#b35\">[37]</ref> also handles page fault in hardware. Its focus is on the c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion should be flexible and extendable, for example, to support high-level APIs like pointer chasing <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b53\">55]</ref>, to offload some app. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d images along with the distribution of cosine similarity between query and negative samples in CUT <ref type=\"bibr\" target=\"#b39\">[40]</ref> and our method. The blue histogram refers to the distribut e orange histogram refers to the distribution in our method.</p><p>The recently proposed method CUT <ref type=\"bibr\" target=\"#b39\">[40]</ref> introduces contrastive learning in unpaired image-to-image . As shown in Figure <ref type=\"figure\" target=\"#fig_0\">1</ref>, the negative samples in the method <ref type=\"bibr\" target=\"#b39\">[40]</ref> are randomly sampled from the patches of different positio hat the negative samples produced by negative generator are harder than those sampled in the method <ref type=\"bibr\" target=\"#b39\">[40]</ref>, which push the encoder network to learn distinguishing re ituation, where the images between the two domains are not one-to-one mapping. In view of this, CUT <ref type=\"bibr\" target=\"#b39\">[40]</ref> involves contrastive learning in unpaired image-to-image t  the method leveraging contrastive learning in unpaired image-to-image translation developed in CUT <ref type=\"bibr\" target=\"#b39\">[40]</ref>. To generate images of target domain with the content info NIT <ref type=\"bibr\" target=\"#b32\">[33]</ref> , DRIT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, CUT <ref type=\"bibr\" target=\"#b39\">[40]</ref>, on three benchmark datasets. Compared with previous metho ion Details. To make a fair comparison, we set the hyperparameters consistent with previous methods <ref type=\"bibr\" target=\"#b39\">[40]</ref>. We conduct our adversarial contrastive learning on the 1- e our method with several state-of-theart methods of unpaired image-to-image translation, i.e., CUT <ref type=\"bibr\" target=\"#b39\">[40]</ref>, CycleGAN <ref type=\"bibr\" target=\"#b58\">[59]</ref> and DR ed images along with the distribution of cosine similarity between query and negative samples in CUT<ref type=\"bibr\" target=\"#b39\">[40]</ref> and our method. The blue histogram refers to the distribut , UNIT<ref type=\"bibr\" target=\"#b32\">[33]</ref> , DRIT<ref type=\"bibr\" target=\"#b30\">[31]</ref>, CUT<ref type=\"bibr\" target=\"#b39\">[40]</ref>, on three benchmark datasets. Compared with previous metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b35\">36]</ref>, domain adaption <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> and image colorization <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> and image colorization <ref type=\"bibr\" target=\"#b55\">[56,<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, domain adaption <ref type=\"bibr\" target=\"#b3\">[4,</ref><re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ated images. For the image generator G(\u2022) and the discriminator D(\u2022), we unitize the LSGAN 100 loss <ref type=\"bibr\" target=\"#b37\">[38]</ref>, which is formulated as follows,</p><formula xml:id=\"formu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  importance on various applications such as style transfer <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on by comparing similar and dissimilar pairs. Recent methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mage Translation</head><p>Image-to-image translation (I2I) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> and image colorization <ref type=\"bibr\" target=\"#b55\">[56,<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, domain adaption <ref type=\"bibr\" target=\"#b3\">[4,</ref><re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n in interdisciplinary research, including computer vision, computer graphics and multimedia (e.g., <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t -playing game/move generation, etc. Recently, many works have been proposed for this purpose (e.g., <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t  image as input and achieves comparable lip synchronization and video quality with previous methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t e=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref> and the other is audio <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t br\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref> and for arbitrary target face <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t  both audio and video can serve as input by learning joint audio-visual representation. Chen et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> first transferred the audio to facial landmarks and then gen That <ref type=\"bibr\" target=\"#b3\">[4]</ref>, DAVS <ref type=\"bibr\" target=\"#b8\">[9]</ref> and ATVG <ref type=\"bibr\" target=\"#b1\">[2]</ref>. These three methods are all 2Dbased and operate on the imag ref type=\"bibr\" target=\"#b8\">[9]</ref>, You said that <ref type=\"bibr\" target=\"#b3\">[4]</ref>, ATVG <ref type=\"bibr\" target=\"#b1\">[2]</ref> and Ours-P). Our method achieves good image quality, lip syn ef> 2.17% 2.33% 2.67% You said that <ref type=\"bibr\" target=\"#b3\">[4]</ref> 3.50% 20.50% 4.17% ATVG <ref type=\"bibr\" target=\"#b1\">[2]</ref> 5.67% 32.33% 9.50% Ours-P 88.67% 44.83% 83.67% TABLE 2 Quant that <ref type=\"bibr\" target=\"#b3\">[4]</ref>, DAVS <ref type=\"bibr\" target=\"#b8\">[9]</ref> and ATVG <ref type=\"bibr\" target=\"#b1\">[2]</ref>), evaluated on LRW dataset which contains 25,000 videos. All  (i.e., without fine tuning personalized talking behavior) with representative audio-driven methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t tly compare the generated results by different methods with the ground-truth videos. We follow ATVG <ref type=\"bibr\" target=\"#b1\">[2]</ref> to apply three widely used metrics for audio-driven talking  thod has comparable results (well preserving facial texture and good lip synchronization) with ATVG <ref type=\"bibr\" target=\"#b1\">[2]</ref>. and the landmark distance (LMD) for accuracy evaluation of  ng that our method has the best PSNR values and has comparable SSIM and LMD metric values with ATVG <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Some qualitative comparisons are shown in Figure <ref type= ethod has comparable results (well preserving facial texture and good lip synchronization) with ATVG<ref type=\"bibr\" target=\"#b1\">[2]</ref>.</figDesc><graphic url=\"image-84.png\" coords=\"9,103.53,278.6. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[35]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref> used CNN to lea \">[29]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref> while others use unsupervised or weakly-supervised learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: os, and the other is an audio chosen from VoxCeleb <ref type=\"bibr\" target=\"#b57\">[58]</ref> or TCD <ref type=\"bibr\" target=\"#b58\">[59]</ref>. We choose these two datasets because they have a long aud. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g computer vision, computer graphics and multimedia (e.g., <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t t target person).</p><p>\u2022 By first training a general mapping based on a publicly available dataset <ref type=\"bibr\" target=\"#b2\">[3]</ref> and fine-tuning the mapping using the input short video of t .</p><p>Stage 1: from audio-visual information to 3D facial animation. We use the LRW video dataset <ref type=\"bibr\" target=\"#b2\">[3]</ref> to train a general mapping from the audio speech to the faci -augmented GAN) involves two training steps: (1) a general mapping trained by the LRW video dataset <ref type=\"bibr\" target=\"#b2\">[3]</ref> and (2) fine tuning the general mapping to learn the persona  face generation evaluation, i.e., the classic PSNR and SSIM metrics for image quality evaluation,  <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which says the word \"absolutely\" (the first four columns) a  state-of-the-art methods. The first row is two examples of the ground truth, taken from LRW dataset<ref type=\"bibr\" target=\"#b2\">[3]</ref>, which says the word \"absolutely\" (the first four columns) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>).</p><p>In this pap >[12]</ref>). Although some measurable correlations have been observed between speech and head pose <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, predicting head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng the Lambertian surface assumption and approximated with spherical harmonics (SH) basis functions <ref type=\"bibr\" target=\"#b51\">[52]</ref>. The irradiance of vertex v i with normal vector n i and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>).</p><p>In this paper, we focus on talking face video genera video of an arbitrary target person with personalized head pose. As a comparison, the previous work <ref type=\"bibr\" target=\"#b6\">[7]</ref> can only generate a high-quality talking face video with per he other is audio <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t video generation for specific 1. https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose face <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref> and for arbitrary. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[36]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref> used CNN to learn a mapping from face images to 3DMM param \">[34]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>, <ref type=\"bibr\" target=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In this paper, we adopt the method <ref type=\"bibr\" targe et=\"#b35\">[36]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In this paper, we adopt the method <ref type=\"bibr\" target=\"#b38\">[39]</ref> for 3D face reconstruction.</p></div> <div xmlns=\"http://w head n=\"3.1\">3D face reconstruction</head><p>We adopt a state-of-the-art deep learning based method <ref type=\"bibr\" target=\"#b38\">[39]</ref> for 3D face reconstruction. It uses a CNN to fit a paramet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get person, with personalized head pose and lip synchronization.</p><p>\u2022 Different from the network <ref type=\"bibr\" target=\"#b13\">[14]</ref> that fine tunes the rendering of a specified parametric fa adly categorised into two classes according to the driven signal. One driven signal is video frames <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bib #b23\">[24]</ref> or image warping <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Learning-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref> were trained by </ref>. It has also been applied to the field of facial animation and texture synthesis. Kim et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> use a GAN to transform rendered face image to realistic vi ed GAN. The differences between our method and the previous GAN-based face reenactment (FR) methods <ref type=\"bibr\" target=\"#b13\">[14]</ref> are:</p><p>\u2022 FR only refines the frames for a single, spec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p><p>Learning-based methods <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib DMM parameters. To deal with the lack of sufficient training data, some methods used synthetic data <ref type=\"bibr\" target=\"#b28\">[29]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emory. It has been applied to question-answering systems <ref type=\"bibr\" target=\"#b44\">[45]</ref>, <ref type=\"bibr\" target=\"#b45\">[46]</ref>, summarization <ref type=\"bibr\" target=\"#b46\">[47]</ref>, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n an individual sequence is limited, MSA Transformer <ref type=\"bibr\" target=\"#b19\">[20]</ref>, ESM <ref type=\"bibr\" target=\"#b20\">[21]</ref> leverage sequence alignment information to model protein e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Dataset</head><p>The dataset from ProteinNet <ref type=\"bibr\" target=\"#b1\">[2]</ref>. And evaluation metric is \ud835\udc3f/5, \ud835\udc3f/2, \ud835\udc3f most likely contact pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aptured by deep learning models. Researchers have explored various strategies. Inspired by Word2Vec <ref type=\"bibr\" target=\"#b16\">[17]</ref>, BioVec <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed P. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Carlo (MC) simulations can be applied to protein analysis <ref type=\"bibr\" target=\"#b10\">[11]</ref> <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and can be quite accurate (simulation at atom-scale). How. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b12\">[13]</ref>, Transformer <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and ResNet <ref type=\"bibr\" target=\"#b27\">[28]</ref> are implemented for these tasks, serving as benchmarks for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecular dynamics simulation (MD) and Monte Carlo (MC) simulations can be applied to protein analysis <ref type=\"bibr\" target=\"#b10\">[11]</ref> <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and can be quit -c.org/ns/1.0\"><head n=\"5.2\">TAPE Contact Map</head><p>Through the visualized predictions from TAPE <ref type=\"bibr\" target=\"#b10\">(11)</ref>, we can see that the TAPE model (small-scale transformer) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing, sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1]</ref> and transfer learning <ref type=\"bibr\" target=\"#b11\">[12]</ref> are also introduced to protein analysis. Recent years, the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b8\">[9]</ref>, fluorescence <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and stability <ref type=\"bibr\" target=\"#b21\">[22]</ref>). Commonly used models, like LSTM <ref type=\"bibr\" target= : the upper limit of concentration at which the protein can maintain its original folding structure <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Therefore, for this task, The input is the amino acid seq. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 18\">[19]</ref> put forward a set of five biologically related tasks (secondary structure prediction <ref type=\"bibr\" target=\"#b14\">[15]</ref>[4] <ref type=\"bibr\" target=\"#b17\">[18]</ref>, contact pred. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n-grams representation). With the rise of representation learning, sequence representation learning <ref type=\"bibr\" target=\"#b0\">[1]</ref> and transfer learning <ref type=\"bibr\" target=\"#b11\">[12]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nguage models on massive protein sequences. We have trained multiple large-scale models on the PFAM <ref type=\"bibr\" target=\"#b6\">[7]</ref> dataset, the largest with 3 billion parameters, outperformin rd by TAPE <ref type=\"bibr\" target=\"#b18\">[19]</ref>. So some date descriptions are inherited. PFAM <ref type=\"bibr\" target=\"#b6\">[7]</ref> is a widely-used database consisting of more than 32 million. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  distributions of classification scores ('Score') and localization scores ('IoU') predicted by ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref> (top row) and the proposed TOOD (bottom row). Ground-truth ation and localization. For example, recent FCOS <ref type=\"bibr\" target=\"#b25\">[26]</ref> and ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref> both use a centerness branch to enhance classification sco \">[16]</ref>, where our TOOD achieved a 51.1 AP, surpassing recent one-stage detectors such as ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref>, GFL <ref type=\"bibr\" target=\"#b12\">[13]</ref> and PAA <re er, by considering effi-ciency and simplicity, TOOD uses a single anchor per location (same as ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref>), where the 'anchor' means an anchor point for an anchor-f sNet-101 and ResNeXt-101-64\u00d74d pre-trained on Ima-geNet <ref type=\"bibr\">[2]</ref>. Similar to ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref>, TOOD tiles one anchor per location. Unless specified, we   TOOD achieves 46.7 AP and 48.3 AP, outperforming the most current one-stage detectors such as ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref> (by \u223c3 AP) and GFL <ref type=\"bibr\" target=\"#b12\">[13]</re n object <ref type=\"bibr\">[3,</ref><ref type=\"bibr\">9,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. They assume that an anchor (i.e., an anchor-point for an a  object for both classification and localization <ref type=\"bibr\">[3,</ref><ref type=\"bibr\">9,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, while anchor-based detectors often assign anchor-boxes by  s and ground truth <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. However, the optimal anchors for classification and locali ing sample assignment. Most anchor-based detectors such as <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, collect training samples by computing IoUs between proposa <p>Overview. Similar to recent one-stage detectors such as <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, the proposed TOOD has an overall pipeline of 'backbone-FPN  \u2208 R H\u00d7W \u00d74 with distance-to-bbox conversion as applied in <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>.</p><p>Prediction alignment. At the prediction step, we fur chors from the perspective of joint optimization.</p><p>Training sample assignment. As discussed in <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, training sample a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lization <ref type=\"bibr\">[5,</ref><ref type=\"bibr\">6,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar sed detectors often assign anchor-boxes by computing IoUs between the anchor boxes and ground truth <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta ntoriented learning approach.</p><p>Training sample assignment. Most anchor-based detectors such as <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, collect training  ibr\" target=\"#b23\">[24]</ref> is one of the earliest CNN-based one-stage detectors. Afterward, YOLO <ref type=\"bibr\" target=\"#b20\">[21]</ref> was developed to directly predict bounding boxes and class type=\"bibr\">9,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, and achieve results in a of 42.0\u223c42.5 AP, which suggests t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: effectively by collecting more informative training samples using output results. For example, FSAF <ref type=\"bibr\" target=\"#b33\">[34]</ref> selects meaningful samples from feature pyramids based on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: effectively by collecting more informative training samples using output results. For example, FSAF <ref type=\"bibr\" target=\"#b33\">[34]</ref> selects meaningful samples from feature pyramids based on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: effectively by collecting more informative training samples using output results. For example, FSAF <ref type=\"bibr\" target=\"#b33\">[34]</ref> selects meaningful samples from feature pyramids based on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the loss of bounding box regression computed for each anchor based on t, and a GIoU loss (L GIoU ) <ref type=\"bibr\" target=\"#b22\">[23]</ref> can be reformulated as follows:</p><formula xml:id=\"formul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ptimizing object classification and localization <ref type=\"bibr\">[5,</ref><ref type=\"bibr\">6,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar 16]</ref>.</p><p>Implementation details. As with most one-stage detectors <ref type=\"bibr\">[9,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, we use the detecti roduces anchors with multiscale predictions from multi-layer convolutional features, and Focal loss <ref type=\"bibr\" target=\"#b14\">[15]</ref> was proposed to address the problem of class imbalance for  i denotes the i-th anchor from the N pos positive anchors corresponding to one instance. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we employ a focal loss for classification to mitigate the f> where j denotes the j-th anchor from the N neg negative anchors, and \u03b3 is the focusing parameter <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Localization objective. A bounding box predicted by  target=\"#b15\">[16]</ref>. Following the standard practice <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, we use the trainval135k set (115K images) for training and  head, and the focusing parameter \u03b3 is set to 2 as used in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. More implementation and training details are presented in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , surpassing recent one-stage detectors such as ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref>, GFL <ref type=\"bibr\" target=\"#b12\">[13]</ref> and PAA <ref type=\"bibr\">[8]</ref>, by a large margin. Qua hts the anchor-boxes using a cleanliness score to mitigate the noise incurred by binary labels. GFL <ref type=\"bibr\" target=\"#b12\">[13]</ref> replaces the binary classification label with an IoU score ned with scale jitter (480-800) and for 2\u00d7 learning schedule (24 epochs) as the most current method <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For a fair comparison, we report results of single model  rrent one-stage detectors such as ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref> (by \u223c3 AP) and GFL <ref type=\"bibr\" target=\"#b12\">[13]</ref> (by \u223c2 AP). Furthermore, with ResNet-101-DCN and ResNeXt-1 aligned One-stage Object Detection</head><p>Overview. Similar to recent one-stage detectors such as <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, the proposed TOOD f parameters as the conventional parallel head, and the focusing parameter \u03b3 is set to 2 as used in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. More implementati >[1,</ref><ref type=\"bibr\">7,</ref><ref type=\"bibr\">8,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> in two aspects. Fir ositive anchors. We use different values of m in <ref type=\"bibr\">[5,</ref><ref type=\"bibr\">9,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uality bounding boxes by weighting the loss more carefully to improve the training. As discussed in <ref type=\"bibr\" target=\"#b19\">[20]</ref>, learning from high-quality bounding boxes is beneficial t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n benchmark MS-COCO 2017 <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Following the standard practice <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, we use the trainv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.\">Related Work</head><p>One-stage detectors. OverFeat <ref type=\"bibr\" target=\"#b23\">[24]</ref> is one of the earliest CNN-based one-stage detectors. Afte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , surpassing recent one-stage detectors such as ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref>, GFL <ref type=\"bibr\" target=\"#b12\">[13]</ref> and PAA <ref type=\"bibr\">[8]</ref>, by a large margin. Qua hts the anchor-boxes using a cleanliness score to mitigate the noise incurred by binary labels. GFL <ref type=\"bibr\" target=\"#b12\">[13]</ref> replaces the binary classification label with an IoU score ned with scale jitter (480-800) and for 2\u00d7 learning schedule (24 epochs) as the most current method <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For a fair comparison, we report results of single model  rrent one-stage detectors such as ATSS <ref type=\"bibr\" target=\"#b28\">[29]</ref> (by \u223c3 AP) and GFL <ref type=\"bibr\" target=\"#b12\">[13]</ref> (by \u223c2 AP). Furthermore, with ResNet-101-DCN and ResNeXt-1 aligned One-stage Object Detection</head><p>Overview. Similar to recent one-stage detectors such as <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, the proposed TOOD f parameters as the conventional parallel head, and the focusing parameter \u03b3 is set to 2 as used in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. More implementati >[1,</ref><ref type=\"bibr\">7,</ref><ref type=\"bibr\">8,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> in two aspects. Fir ositive anchors. We use different values of m in <ref type=\"bibr\">[5,</ref><ref type=\"bibr\">9,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in the process of the development of graph computing system <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>.</p><p>At present, most of the graph calculation adopts SFA a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nd the data is extremely complex, so it is difficult to analyze and process the data by other means <ref type=\"bibr\" target=\"#b4\">[5]</ref>. However, graph calculation provides a good way for this kin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ey also found that this algorithm has great limitations in the intelligent application of list data <ref type=\"bibr\" target=\"#b12\">[13]</ref>. However, few studies have proposed the optimization algor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n method restricts the further expansion of the graph scale <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. These are not conducive to the high-speed operation of the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ey also found that this algorithm has great limitations in the intelligent application of list data <ref type=\"bibr\" target=\"#b12\">[13]</ref>. However, few studies have proposed the optimization algor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sis in various industries, which now occupies a very critical position in large-scale data analysis <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. For example, with the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of data every day, and accurate analysis of data is related to the sound development of the society <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The traditional data analysis method has a small amount of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on has promoted the efficiency of information transmission <ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion transmission <ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing recommendation <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>. The typical idea o =\"bibr\" target=\"#b47\">[48]</ref> and social recommendation <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. Although these self-supervised methods have achieved decen ction since it has been proved redundant in recommendation <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. After passing X (0) through \ud835\udc3f graph convolution layers, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: essively developed <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>.</p><p>Early effort  in this field brought Markov Chain into sessionbased scenarios to capture the temporal information <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Afterwards, deep  based recommendation focused on exploiting temporal information from session data with Markov chain <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta p>4.1.2 Baseline Methods. We compare COTREC with the following representative methods:</p><p>\u2022 FPMC <ref type=\"bibr\" target=\"#b27\">[28]</ref> is a sequential method based on Markov Chain. In order to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on-based recommendation models to capture sequential order between items and achieved great success <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>. Hidasi et al. <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on layers to replace all RNN encoders in the previous work and employs the self-attention mechanism <ref type=\"bibr\" target=\"#b33\">[34]</ref> to enhance the session-based recommendation performance. \u2022. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  semi-supervised learning paradigm to exploit unlabeled data <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. Under this regime, two hould be mentioned that there have been several attempts that combine cotraining and recommendation <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. However, these meth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on layers to replace all RNN encoders in the previous work and employs the self-attention mechanism <ref type=\"bibr\" target=\"#b33\">[34]</ref> to enhance the session-based recommendation performance. \u2022. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ommendation models <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" tar get=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Unlike RNN-based approaches, graph structure is an essenti ence of comparing, we follow the experiment environment in <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Specifically, we filter out all sessions whose length is 1  recommendation.</p><p>4.1.3 Evaluation Metrics. Following <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, we use P@K (Precision) and MRR@K (Mean Reciprocal Rank) to r for production environments. To evaluate this, we follow <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> to split the sessions of Tmall and Diginetica into two grou r in graph-based methods, aiming to learn item transitions over session graphs. For example, SR-GNN <ref type=\"bibr\" target=\"#b40\">[41]</ref> is the first to model session sequences in session graphs  e=\"bibr\" target=\"#b33\">[34]</ref> to enhance the session-based recommendation performance. \u2022 SR-GNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>: applies a gated graph convolutional layer to obtain item . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on layers to replace all RNN encoders in the previous work and employs the self-attention mechanism <ref type=\"bibr\" target=\"#b33\">[34]</ref> to enhance the session-based recommendation performance. \u2022. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng iterations. Therefore, it is necessary to make the two encoders differ to some degree. Following <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we impose the divergence constraint on the self-supervise. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ches share a common assumption that user behaviors are constantly recorded and available for access <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. However, in some . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  advances seek to harness SSL for improving recommendation <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" tar nd then utilizes mutual information maximization to refine item representations. Similarly, CL4SRec <ref type=\"bibr\" target=\"#b43\">[44]</ref> adopts item cropping, masking and reordering to construct  generate self-supervision signals and enhance the data representations via pre-training. Xie et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> proposed three data augmentation strategies to construct s ining set in view of the above considerations. Besides, compared with the dropout based SSL methods <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref> which leverage the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctures; simultaneously creating a perfect case to reap the benefits of custom-designed accelerators <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Unsurprisingly ML accelerators are now used across all form ures, over the years, optimization objectives shifted from supporting GEMMs, to native convolutions <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b21\">[21]</ref>, and more recentl  the systolic array named Output Stationary (OS), Weight Stationary (WS), and Input Stationary (IS) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. The \"stationarity\" of a given dataflow is determined by the  the input operands (Input Feature map (IFMAP) and Filter in Convolution neural network terminology <ref type=\"bibr\" target=\"#b7\">[8]</ref>), which are fed into the array from the top or left edges as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b19\">[23]</ref> use RL for task placement on a heterogeneous system. Wang et al <ref type=\"bibr\" target=\"#b32\">[33]</ref> use GCN and RL for automatic transistor sizing. NVCell <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"figure\" target=\"#fig_0\">14</ref> show these models. To summarize we used the scikit-learn <ref type=\"bibr\" target=\"#b25\">[26]</ref> libraries implementation of support vector classifiers <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b19\">[23]</ref> use RL for task placement on a heterogeneous system. Wang et al <ref type=\"bibr\" target=\"#b32\">[33]</ref> use GCN and RL for automatic transistor sizing. NVCell <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion of time throughout the computation. Although many different dataflows exist for spatial arrays <ref type=\"bibr\" target=\"#b20\">[20]</ref>, we only consider true systolic dataflows that only use lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"figure\" target=\"#fig_0\">14</ref> show these models. To summarize we used the scikit-learn <ref type=\"bibr\" target=\"#b25\">[26]</ref> libraries implementation of support vector classifiers <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmed for evaluating the costs of possible architecture choices via simulations or other cost models <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Finally, based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ize it. Most prior works leveraging ML for accelerator DSE <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" on search methods which use genetic algorithm and reinforcement learning (RL) respectively. AutoTVM <ref type=\"bibr\" target=\"#b6\">[7]</ref> use ML model for cost prediction to improve fast mapping sea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ural networks <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref> . This distribu  type=\"bibr\" target=\"#b28\">[29]</ref>, GoogleNet <ref type=\"bibr\" target=\"#b31\">[32]</ref>, Alexnet <ref type=\"bibr\" target=\"#b17\">[18]</ref>, MobileNet <ref type=\"bibr\" target=\"#b11\">[12]</ref> and R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b19\">[23]</ref> use RL for task placement on a heterogeneous system. Wang et al <ref type=\"bibr\" target=\"#b32\">[33]</ref> use GCN and RL for automatic transistor sizing. NVCell <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6\">[7]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref> focus on performing the search faster. Learning the design. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: AV technologies <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>- <ref type=\"bibr\" target=\"#b26\">[27]</ref> assume that legitimate packets with authentic source IP ad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref> proposes a blockchain-based public and efficient certificate audit scheme for TLS connections, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref> utilizes blockc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  efficient certificate audit scheme for TLS connections, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref> utilizes blockchain to audit released IoT systems. In this. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b7\">[8]</ref>. Instead, inter-AS SAV <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b12\">[13]</ref> is what protects deployer ASes from IP spoofing. By identi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  efficient certificate audit scheme for TLS connections, <ref type=\"bibr\" target=\"#b37\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref> utilizes blockchain to audit released IoT systems. In this. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ership of network infrastructure makes the actual deployment of inter-AS SAV particularly difficult <ref type=\"bibr\" target=\"#b13\">[14]</ref>. In general, inter-AS SAV deployment faces two main challe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ivated much research work to enhance network functions and applications with programmable switches. <ref type=\"bibr\" target=\"#b27\">[28]</ref>- <ref type=\"bibr\" target=\"#b29\">[30]</ref> offload network. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref> proposes a blockchain-based incentivizing and auditing framework for decentralized storage. <ref type=\"bibr\" target=\"#b35\">[36]</ref> proposes a deduplicatable data auditing mechanism using bl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  weight matrices in the AE with a radial-basis-function(RBF) kernel, which is known as Kernel Trick <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n space, and then reconstructs the entries in the output space to predict missing ratings. SparseFC <ref type=\"bibr\" target=\"#b5\">[6]</ref> employs an AE whose weight matrices were sparsified using fi egularised alternating least squares based on additional side information (SI) graphs. (9) SparseFC <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a neural network in which weight matrices are reparameter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: support kernel functions. This is technically equivalent to the local kernel of GLocal-K. (10) IGMC <ref type=\"bibr\" target=\"#b11\">[12]</ref> is similar to GCMC but applies a graphlevel GNN to the enc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ith structural and external side information. Recent studies <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> focused on utilising s consider GraphRec with side information as (7) GraphRec+Extra. ( <ref type=\"formula\">8</ref>) GRAEM <ref type=\"bibr\" target=\"#b8\">[9]</ref> formulates a probabilistic generative model and uses expecta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n=\"3.2\">Baselines</head><p>We compare the RMSE with the eleven recommendation baselines: (1) LLORMA <ref type=\"bibr\" target=\"#b3\">[4]</ref> is a matrix factorization model using local low rank sub-mat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e interests of a user by collecting preferences from large number of other users. Matrix completion <ref type=\"bibr\" target=\"#b1\">[2]</ref> is one of the most common formulation, where rows and column  auto encoder model, which mainly pays attention to extract the latent features of users and items. <ref type=\"bibr\" target=\"#b1\">(2)</ref> We propose a new way to integrate pre-training and fine-tuni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s an AE whose weight matrices were sparsified using finite support kernels. Inspired by this, GC-MC <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposed a graph-based AE framework for matrix completion, w  with the neural auto-regressive distribution estimator (NADE) for rating reconstruction. (4) GC-MC <ref type=\"bibr\" target=\"#b0\">[1]</ref> is a graph-based AE framework that applies GNN on the bipart s available, like most real-world cases. The main research contributions are summarised as follows: <ref type=\"bibr\" target=\"#b0\">(1)</ref> We introduce a global and local kernel-based auto encoder mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing sparsely observed matrix entries to a low dimensional feature space by using an autoencoder(AE) <ref type=\"bibr\" target=\"#b10\">[11]</ref>. AEs would help the system better understand users and ite. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s an AE whose weight matrices were sparsified using finite support kernels. Inspired by this, GC-MC <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposed a graph-based AE framework for matrix completion, w  with the neural auto-regressive distribution estimator (NADE) for rating reconstruction. (4) GC-MC <ref type=\"bibr\" target=\"#b0\">[1]</ref> is a graph-based AE framework that applies GNN on the bipart s available, like most real-world cases. The main research contributions are summarised as follows: <ref type=\"bibr\" target=\"#b0\">(1)</ref> We introduce a global and local kernel-based auto encoder mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e interests of a user by collecting preferences from large number of other users. Matrix completion <ref type=\"bibr\" target=\"#b1\">[2]</ref> is one of the most common formulation, where rows and column  auto encoder model, which mainly pays attention to extract the latent features of users and items. <ref type=\"bibr\" target=\"#b1\">(2)</ref> We propose a new way to integrate pre-training and fine-tuni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: support kernel functions. This is technically equivalent to the local kernel of GLocal-K. (10) IGMC <ref type=\"bibr\" target=\"#b11\">[12]</ref> is similar to GCMC but applies a graphlevel GNN to the enc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: els build on this scheme do not need to worry about the implication of encoding order. For example, <ref type=\"bibr\" target=\"#b9\">Fu et al. (2019)</ref> encodes entity and relation information separat f><ref type=\"bibr\">Zeng et al., 2018</ref><ref type=\"bibr\" target=\"#b38\">Zeng et al., , 2019;;</ref><ref type=\"bibr\" target=\"#b9\">Fu et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Wei et al., 2020)  relation separately with sequences <ref type=\"bibr\" target=\"#b33\">(Wei et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b9\">(Fu et al., 2019)</ref> or tables <ref type=\"bibr\" target=\"#b29\">(Wang s and Ulges, 2019;</ref><ref type=\"bibr\" target=\"#b33\">Wei et al., 2020)</ref> or parallel encoding <ref type=\"bibr\" target=\"#b9\">(Fu et al., 2019)</ref> lack proper two-way interaction in feature ext. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t are not affected by those that are extracted later. <ref type=\"bibr\">Zeng et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b33\">Wei et al. (2020)</ref> are typical examples of this category. Their  \"bibr\" target=\"#b38\">Zeng et al., , 2019;;</ref><ref type=\"bibr\" target=\"#b9\">Fu et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Wei et al., 2020)</ref>. The triple overlapping problem refers to tri rget=\"#b2\">(Bekoulis et al., 2018b;</ref><ref type=\"bibr\" target=\"#b8\">Eberts and Ulges, 2019;</ref><ref type=\"bibr\" target=\"#b33\">Wei et al., 2020)</ref> or parallel encoding <ref type=\"bibr\" target= d multiple times in output sequence, another is by modeling each relation separately with sequences <ref type=\"bibr\" target=\"#b33\">(Wei et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b9\">(Fu et R, our method shows a distinct advantage over baselines that report the figures. Compared to Casrel <ref type=\"bibr\" target=\"#b33\">(Wei et al., 2020)</ref>, a competitive method, our F1 scores are 2.3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion.</p><p>We leverage entity gate \u1ebd and relation gate r, which are referred to as master gates in <ref type=\"bibr\" target=\"#b24\">(Shen et al., 2019)</ref>, for neuron partition. As illustrated in fi tp://www.tei-c.org/ns/1.0\"><head>Number of Encoder Layers</head><p>Partition Granularity Similar to <ref type=\"bibr\" target=\"#b24\">(Shen et al., 2019)</ref>, we split neurons into several chunks and p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b10\">Gupta et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">Katiyar and Cardie, 2017;</ref><ref type=\"bibr\" target=\"#b25\">Shen et al., 2021)</ref> and has since become the mainstream of joint. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt methods mainly rely on elaborate feature engineering to establish interaction between NER and RE <ref type=\"bibr\" target=\"#b35\">(Yu and Lam, 2010;</ref><ref type=\"bibr\" target=\"#b17\">Li and Ji, 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g et al., 2018)</ref>, ADE <ref type=\"bibr\" target=\"#b12\">(Gurulingappa et al., 2012)</ref>, SciERC <ref type=\"bibr\" target=\"#b19\">(Luan et al., 2018)</ref>, ACE04 and ACE05 <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Dataset, Evaluation and Implementation Details</head><p>We evaluate our model on six datasets. NYT <ref type=\"bibr\" target=\"#b22\">(Riedel et al., 2010)</ref>, WebNLG <ref type=\"bibr\">(Zeng et al., 20 s=\"http://www.tei-c.org/ns/1.0\"><head>A Dataset</head><p>We evaluate our model on six datasets. NYT <ref type=\"bibr\" target=\"#b22\">(Riedel et al., 2010)</ref> is sampled from New York Times news artic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  prediction sharing the same features <ref type=\"bibr\" target=\"#b27\">(Tran and Kavuluru, 2019;</ref><ref type=\"bibr\" target=\"#b32\">Wang et al., 2020b)</ref>. This could be problematic as information a 7% improvement in WebNLG but performance in NYT is only slightly better than previous SOTA TpLinker <ref type=\"bibr\" target=\"#b32\">(Wang et al., 2020b</ref>) by 0.5% margin. We argue that this is beca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b23\">(Riedel et al., 2013)</ref>, question * * Corresponding author.</p><p>answering <ref type=\"bibr\" target=\"#b7\">(Diefenbach et al., 2018</ref>) and text summarization <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2019)</ref>, TENER <ref type=\"bibr\" target=\"#b34\">(Yan et al., 2019)</ref> and Flair-Embeddings <ref type=\"bibr\" target=\"#b0\">(Akbik et al., 2019)</ref>. Descriptions of the transformation methods. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  entity and relation could sometimes be contradictory. Also, as models that use sequential encoding <ref type=\"bibr\" target=\"#b2\">(Bekoulis et al., 2018b;</ref><ref type=\"bibr\" target=\"#b8\">Eberts and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3\">(Wei et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b9\">(Fu et al., 2019)</ref> or tables <ref type=\"bibr\" target=\"#b29\">(Wang and Lu, 2020)</ref>. Our method uses relation-specific tables <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner <ref type=\"bibr\" target=\"#b36\">(Zelenko et al., 2002;</ref><ref type=\"bibr\" target=\"#b4\">Chan and Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b23\">(Riedel et al., 2013)</ref>, question * * Corresponding author.</p><p>answering <ref type=\"bibr\" target=\"#b7\">(Diefenbach et al., 2018</ref>) and text summarization <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  entity and relation could sometimes be contradictory. Also, as models that use sequential encoding <ref type=\"bibr\" target=\"#b2\">(Bekoulis et al., 2018b;</ref><ref type=\"bibr\" target=\"#b8\">Eberts and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  prediction sharing the same features <ref type=\"bibr\" target=\"#b27\">(Tran and Kavuluru, 2019;</ref><ref type=\"bibr\" target=\"#b32\">Wang et al., 2020b)</ref>. This could be problematic as information a 7% improvement in WebNLG but performance in NYT is only slightly better than previous SOTA TpLinker <ref type=\"bibr\" target=\"#b32\">(Wang et al., 2020b</ref>) by 0.5% margin. We argue that this is beca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g et al., 2018)</ref>, ADE <ref type=\"bibr\" target=\"#b12\">(Gurulingappa et al., 2012)</ref>, SciERC <ref type=\"bibr\" target=\"#b19\">(Luan et al., 2018)</ref>, ACE04 and ACE05 <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2019)</ref>, TENER <ref type=\"bibr\" target=\"#b34\">(Yan et al., 2019)</ref> and Flair-Embeddings <ref type=\"bibr\" target=\"#b0\">(Akbik et al., 2019)</ref>. Descriptions of the transformation methods. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n-free models, including BiLSTM-CRF <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2015)</ref>, BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, TENER <ref type=\"bibr\" target=\"#b34\">(Yan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3\">(Wei et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b9\">(Fu et al., 2019)</ref> or tables <ref type=\"bibr\" target=\"#b29\">(Wang and Lu, 2020)</ref>. Our method uses relation-specific tables <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b22\">(Riedel et al., 2010)</ref>, WebNLG <ref type=\"bibr\">(Zeng et al., 2018)</ref>, ADE <ref type=\"bibr\" target=\"#b12\">(Gurulingappa et al., 2012)</ref>, SciERC <ref type=\"bibr\" target=\"#b al., 2006)</ref> are collected from various sources, including news articles and online forums. ADE <ref type=\"bibr\" target=\"#b12\">(Gurulingappa et al., 2012)</ref>  </p></div> <div xmlns=\"http://www.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (RE) are performed in a pipelined manner <ref type=\"bibr\" target=\"#b36\">(Zelenko et al., 2002;</ref><ref type=\"bibr\" target=\"#b4\">Chan and Roth, 2011)</ref>. These approaches are flawed in that they d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p><p>answering <ref type=\"bibr\" target=\"#b7\">(Diefenbach et al., 2018</ref>) and text summarization <ref type=\"bibr\" target=\"#b11\">(Gupta and Lehal, 2010)</ref>. Conventionally, Named Entity Recogniti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e extracted information provides a supplement to many studies, such as knowledge graph construction <ref type=\"bibr\" target=\"#b23\">(Riedel et al., 2013)</ref>, question * * Corresponding author.</p><p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b10\">Gupta et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">Katiyar and Cardie, 2017;</ref><ref type=\"bibr\" target=\"#b25\">Shen et al., 2021)</ref> and has since become the mainstream of joint. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g et al., 2018)</ref>, ADE <ref type=\"bibr\" target=\"#b12\">(Gurulingappa et al., 2012)</ref>, SciERC <ref type=\"bibr\" target=\"#b19\">(Luan et al., 2018)</ref>, ACE04 and ACE05 <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3\">(Wei et al., 2020)</ref>, graphs <ref type=\"bibr\" target=\"#b9\">(Fu et al., 2019)</ref> or tables <ref type=\"bibr\" target=\"#b29\">(Wang and Lu, 2020)</ref>. Our method uses relation-specific tables <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g et al., 2018)</ref>, ADE <ref type=\"bibr\" target=\"#b12\">(Gurulingappa et al., 2012)</ref>, SciERC <ref type=\"bibr\" target=\"#b19\">(Luan et al., 2018)</ref>, ACE04 and ACE05 <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b22\">(Riedel et al., 2010)</ref>, WebNLG <ref type=\"bibr\">(Zeng et al., 2018)</ref>, ADE <ref type=\"bibr\" target=\"#b12\">(Gurulingappa et al., 2012)</ref>, SciERC <ref type=\"bibr\" target=\"#b al., 2006)</ref> are collected from various sources, including news articles and online forums. ADE <ref type=\"bibr\" target=\"#b12\">(Gurulingappa et al., 2012)</ref>  </p></div> <div xmlns=\"http://www.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner <ref type=\"bibr\" target=\"#b36\">(Zelenko et al., 2002;</ref><ref type=\"bibr\" target=\"#b4\">Chan and Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: act> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Since the introduction of the transformer model by <ref type=\"bibr\" target=\"#b38\">Vaswani et al. (2017)</ref>, a fundamental question has yet to be ans =\"bibr\" target=\"#b19\">Mikolov &amp; Zweig, 2012;</ref><ref type=\"bibr\">Zaremba et al., 2014)</ref>. <ref type=\"bibr\" target=\"#b38\">Vaswani et al. (2017)</ref>, introducing the transformer, speculated  herently sequential, like text, positional information is injected into the inputs in various ways. <ref type=\"bibr\" target=\"#b38\">Vaswani et al. (2017)</ref> discussed two options for embedding posit c.org/ns/1.0\"><head n=\"3\">ATTENTION WITH LINEAR BIASES (ALIBI)</head><p>In the transformer model of <ref type=\"bibr\" target=\"#b38\">Vaswani et al. (2017)</ref>, position embeddings are added to the wor he hyperparameters (the start and end of the geometric progression of wavelengths) were set once by <ref type=\"bibr\" target=\"#b38\">Vaswani et al. (2017)</ref> and then reused in different models of di  trained on shorter inputs).</p><p>While multiple alternatives to the position methods presented in <ref type=\"bibr\" target=\"#b38\">Vaswani et al. (2017)</ref> have been proposed, few have been adopted \"10\" xml:id=\"foot_9\">The ALiBi bias is not multiplied by the \u221a d k scaling factor from Equation 1 of<ref type=\"bibr\" target=\"#b38\">Vaswani et al. (2017)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c. 2.2\">MEASURING EXTRAPOLATION</head><p>Sinusoidal Position Embeddings Sinusoidal position embeddings <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\">\u00a73.5)</ref> are constant ; Auli, 2018;</ref><ref type=\"bibr\" target=\"#b14\">Lewis et al., 2021)</ref> and machine translation <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b26\">Ott et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al., 2021)</ref> and machine translation <ref type=\"bibr\" target=\"#b38\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b26\">Ott et al., 2018)</ref> models.</p><p>We first consider the unmodifie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 33\">(Rosendahl et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Neishi &amp; Yoshinaga, 2019;</ref><ref type=\"bibr\" target=\"#b24\">Newman et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Kiyono et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to train the RoBERTa <ref type=\"bibr\" target=\"#b17\">(Liu et al., 2019)</ref> implementation of BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> and the English part of the CC-100 corpus  he Toronto BooksCorpus <ref type=\"bibr\">(Zhu et al., 2015)</ref>, which has been used to train BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> (in conjuction with the English Wikipedia). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gth 10,000. In recently explored settings where NLP training examples are given as context to an LM <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020)</ref>, our approach will allow exposure to more e br\" target=\"#b36\">Su et al. (2021)</ref> and has recently been popularized by the open source GPT-3 <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020)</ref> implementation GPT-J <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2020</ref>) uses a relative position method <ref type=\"bibr\" target=\"#b35\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b7\">Huang et al., 2019</ref>) that adds no position information to word em. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xml:id=\"foot_6\">).7  Our T5 bias implementation is based on the one used in HuggingFace Transformers<ref type=\"bibr\" target=\"#b41\">(Wolf et al., 2020)</ref>, which in turn is based on the official Mes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to train the RoBERTa <ref type=\"bibr\" target=\"#b17\">(Liu et al., 2019)</ref> implementation of BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> and the English part of the CC-100 corpus  he Toronto BooksCorpus <ref type=\"bibr\">(Zhu et al., 2015)</ref>, which has been used to train BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> (in conjuction with the English Wikipedia). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to train the RoBERTa <ref type=\"bibr\" target=\"#b17\">(Liu et al., 2019)</ref> implementation of BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> and the English part of the CC-100 corpus  he Toronto BooksCorpus <ref type=\"bibr\">(Zhu et al., 2015)</ref>, which has been used to train BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> (in conjuction with the English Wikipedia). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 18)</ref>. We use this model because of its prominent role in recent language modeling developments <ref type=\"bibr\" target=\"#b11\">(Khandelwal et al., 2020;</ref><ref type=\"bibr\">Press et al., 2021)</  obtained by Routing Transformer <ref type=\"bibr\" target=\"#b34\">(Roy et al., 2020)</ref> and kNN-LM <ref type=\"bibr\" target=\"#b11\">(Khandelwal et al., 2020)</ref>. The methods used in those models are ith a sliding window ( \u00a7B). Following <ref type=\"bibr\" target=\"#b0\">(Baevski &amp; Auli, 2018;</ref><ref type=\"bibr\" target=\"#b11\">Khandelwal et al., 2020;</ref><ref type=\"bibr\">Press et al., 2020;</r ia). The corpus is about 700M tokens (2.9 GB).</p><p>We use the same train/validation/test split as <ref type=\"bibr\" target=\"#b11\">Khandelwal et al. (2020)</ref> and their tokenization, which uses BER. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ence training. We propose to use one of its variant known as Self-Critical Sequence Training (SCST) <ref type=\"bibr\" target=\"#b15\">(Rennie et al., 2017)</ref> for both T2G and G2T training.</p><p>To s  type=\"bibr\" target=\"#b17\">Sutton and Barto (2018)</ref>. In Self-Critical Sequence Training (SCST) <ref type=\"bibr\" target=\"#b15\">(Rennie et al., 2017)</ref>, b is chosen to be the reward of x * T ,   TEKGEN) by finetuning using L CE loss.  <ref type=\"bibr\" target=\"#b14\">(Ranzato et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Rennie et al., 2017)</ref>. Therefore, we followed the subsequent sim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g et al., 2020)</ref>, METEOR <ref type=\"bibr\" target=\"#b8\">(Lavie and Agarwal, 2007)</ref>, chrF++ <ref type=\"bibr\" target=\"#b12\">(Popovi\u0107, 2017)</ref> for G2T, or simply F1, Precision, and Recall sc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: volutional network (R-GCN) and a T5 PLM with some canonicalization rules. 2 nd place OSU Neural NLG <ref type=\"bibr\" target=\"#b10\">(Li et al., 2020)</ref>, the closest to our approach in spirit, used  anslated outputs G/T.</p><p>Note that in contrast to many WebNLG+ 2020 Challenge participants, e.g. <ref type=\"bibr\" target=\"#b10\">(Li et al., 2020)</ref>, no preprocessing of the data is performed fo hai) <ref type=\"bibr\" target=\"#b6\">(Guo et al., 2020a)</ref> 0.540 0.535 0.417 0.690 OSU Neural NLG <ref type=\"bibr\" target=\"#b10\">(Li et al., 2020)</ref> 0.535 0.532 0.414 0.688 FBConvAI <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0.688 FBConvAI <ref type=\"bibr\" target=\"#b20\">(Yang et al., 2020)</ref> 0.527 0.523 0.413 0.686 bt5 <ref type=\"bibr\" target=\"#b1\">(Agarwal et al., 2020)</ref> 0 did not lead to any noticeable improvem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al approach on English and Russian, even using WMT English/Russian parallel corpus.</p><p>Recently, <ref type=\"bibr\" target=\"#b5\">Dognin et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b7\">Guo et al. (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0.688 FBConvAI <ref type=\"bibr\" target=\"#b20\">(Yang et al., 2020)</ref> 0.527 0.523 0.413 0.686 bt5 <ref type=\"bibr\" target=\"#b1\">(Agarwal et al., 2020)</ref> 0 did not lead to any noticeable improvem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Indeed, large PLMs like T5 <ref type=\"bibr\" target=\"#b13\">(Raffel et al., 2020)</ref> and BART <ref type=\"bibr\" target=\"#b9\">(Lewis et al., 2020)</ref> that have been pretrained on vast amount of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al approach on English and Russian, even using WMT English/Russian parallel corpus.</p><p>Recently, <ref type=\"bibr\" target=\"#b5\">Dognin et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b7\">Guo et al. (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  accuracy <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Note that in the real-world assortative graphs <ref type=\"bibr\" target=\"#b38\">[39]</ref>, the closely connected nodes are potential to share the sa assumed that class labels of connected nodes are positively related in many real-world applications <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. In other words, f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ir local neighbors <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" tar ding to graph homophily, label propagation passes labels iteratively to learn the label predictions <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" ta ation. Most of previous methods involve trainable weights and cannot scale to the large-scale graph <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" ta ef>. Some recent work connects GNNs with label propagation <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> by studying how labels/features spread over a graph and how. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction \u0177i to attend on the unrelated classes. Motivated from the batch pacing in curriculum learning <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, we propose a smooth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uitous in real-world applications, such as social networks <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> and knowledge graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> and knowledge graphs <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Although graph ne f type=\"bibr\" target=\"#b32\">33]</ref> and knowledge graphs <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Although graph neural networks (GNNs) have shown effective. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ized by massive nodes and edges, are ubiquitous in real-world applications, such as social networks <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction \u0177i to attend on the unrelated classes. Motivated from the batch pacing in curriculum learning <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, we propose a smooth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>, and biochemical module analysis <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>.</p><p>Scalable gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sses. Motivated from the batch pacing in curriculum learning <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, we propose a smooth pacing function to gradually schedule . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve. GNNs are playing increasingly crucial roles in various applications such as recommender systems <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" targ bility issue of GNNs, including sub-graph sampling methods <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ir local neighbors <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" tar ding to graph homophily, label propagation passes labels iteratively to learn the label predictions <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" ta ation. Most of previous methods involve trainable weights and cannot scale to the large-scale graph <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" ta ef>. Some recent work connects GNNs with label propagation <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> by studying how labels/features spread over a graph and how. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Devlin et al., 2019)</ref>, GPT <ref type=\"bibr\" target=\"#b22\">(Radford et al., 2019)</ref>, and T5 <ref type=\"bibr\" target=\"#b23\">(Raffel et al., 2020)</ref> have greatly boosted performance in a wid r model that considers the token type information in code. Our CodeT5 builds on the T5 architecture <ref type=\"bibr\" target=\"#b23\">(Raffel et al., 2020)</ref> that employs denoising sequence-to-sequen \" target=\"#b28\">(Song et al., 2019)</ref>, BART <ref type=\"bibr\">(Lewis et al., 2020)</ref>, and T5 <ref type=\"bibr\" target=\"#b23\">(Raffel et al., 2020)</ref>. Compared to encoder-only and decoder-onl >CodeT5</head><p>Our CodeT5 builds on an encoder-decoder framework with the same architecture as T5 <ref type=\"bibr\" target=\"#b23\">(Raffel et al., 2020)</ref>. It aims to derive generic representation oder to recover the original texts. In this work, we utilize a span masking objective similar to T5 <ref type=\"bibr\" target=\"#b23\">(Raffel et al., 2020)</ref> that randomly masks spans with arbitrary  standing tasks, we investigate two ways of either generating the label as a unigram target sequence <ref type=\"bibr\" target=\"#b23\">(Raffel et al., 2020)</ref>, or predicting it from the vocabulary of  c.org/ns/1.0\"><head n=\"4.5\">Model Configurations</head><p>We build CodeT5 based on Huggingface's T5 <ref type=\"bibr\" target=\"#b23\">(Raffel et al., 2020)</ref> PyTorch implementation 3 and employ two s quite effective in a broad set of NLP tasks <ref type=\"bibr\" target=\"#b28\">(Song et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Raffel et al., 2020;</ref><ref type=\"bibr\">Lewis et al., 2020)</ref>. n capability in NL pre-training <ref type=\"bibr\" target=\"#b18\">(Liu et al., 2019a)</ref>. We follow <ref type=\"bibr\" target=\"#b23\">Raffel et al. (2020)</ref> to employ the same unified model for all t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Besides the BERT-style models, Svyatkovskiy et al. <ref type=\"bibr\" target=\"#b15\">(2020)</ref> and <ref type=\"bibr\" target=\"#b17\">Liu et al. (2020)</ref> respectively employ GPT and UniLM <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eration is the task to generate a code snippet based on NL descriptions. We employ the Concode data <ref type=\"bibr\" target=\"#b12\">(Iyer et al., 2018)</ref> in Java where the input contains both NL te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge the structural aspect of PL. These models only focus on training a better code-specific encoder. <ref type=\"bibr\" target=\"#b34\">Z\u00fcgner et al. (2021)</ref> proposes to capture the relative distances. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2019)</ref>, RoBERTa <ref type=\"bibr\" target=\"#b19\">(Liu et al., 2019b)</ref>, and ELECTRA <ref type=\"bibr\" target=\"#b2\">(Clark et al., 2020)</ref>, decoder-only models like GPT <ref type=\"bi o derive generic code-specific representation, and CodeBERT further adds a replaced token detection <ref type=\"bibr\" target=\"#b2\">(Clark et al., 2020)</ref> task to learn NL-PL cross-modal representat pe=\"bibr\" target=\"#b9\">(Feng et al., 2020)</ref> trained with both MLM and replaced token detection <ref type=\"bibr\" target=\"#b2\">(Clark et al., 2020)</ref>, GraphCode-BERT <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o predict whether a code is vulnerable to software systems or not. We use the C dataset provided by <ref type=\"bibr\" target=\"#b33\">Zhou et al. (2019)</ref> for experiment. The second task is clone det. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Besides the BERT-style models, Svyatkovskiy et al. <ref type=\"bibr\" target=\"#b15\">(2020)</ref> and <ref type=\"bibr\" target=\"#b17\">Liu et al. (2020)</ref> respectively employ GPT and UniLM <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fore subword tokenization, which aims to avoid masking partial subtokens and is shown to be helpful <ref type=\"bibr\" target=\"#b29\">(Sun et al., 2019)</ref>. Notably, we pre-train a shared model for va. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: subset of generation tasks and do not support understanding tasks like us. Apart from these, PLBART <ref type=\"bibr\" target=\"#b0\">(Ahmad et al., 2021)</ref> based on another encoder-decoder model BART om scratch. As encoder-decoder models, the current SOTA model for the CodeXGLUE benchmark is PLBART <ref type=\"bibr\" target=\"#b0\">(Ahmad et al., 2021)</ref> based on BART <ref type=\"bibr\">(Lewis et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> and <ref type=\"bibr\" target=\"#b17\">Liu et al. (2020)</ref> respectively employ GPT and UniLM <ref type=\"bibr\" target=\"#b7\">(Dong et al., 2019)</ref> for the code completion task. <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and predict whether they have the same functionality. We experiment with the Java data provided by <ref type=\"bibr\" target=\"#b32\">Wang et al. (2020)</ref>. We employ F1 score and accuracy for evaluat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e, there exist several well-established GML benchmarks. For example, the Open Graph Benchmark (OGB) <ref type=\"bibr\" target=\"#b25\">[26]</ref> offers abundant datasets and a unified evaluation pipeline ocessing including splitting and feature normalization; it also supports external datasets like OGB <ref type=\"bibr\" target=\"#b25\">[26]</ref> or user-defined datasets.</p><p>(  </p></div> <div xmlns=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Attackers can modify the original graph by adding or remov ng malicious nodes <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> to conduct adversarial attacks. Despite the relatively mino rom previous works <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and are reprocessed for GRB. The basic statistics of these   <ref type=\"bibr\" target=\"#b15\">[16]</ref>, SPEIT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TD-GIA <ref type=\"bibr\" target=\"#b17\">[18]</ref>). Facing the problem of scalability, some attacks are not  ef type=\"bibr\" target=\"#b33\">[34]</ref>, SPEIT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and TDGIA <ref type=\"bibr\" target=\"#b17\">[18]</ref>. More details can be found in Appendix A.4.2.</p><p>Five D a is based on the assumption that nodes with lower degrees are easier to attack, as demonstrated in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. If a target node has few neighbors, it is more likely to  D <ref type=\"bibr\" target=\"#b33\">[34]</ref>, SPEIT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TDGIA <ref type=\"bibr\" target=\"#b17\">[18]</ref>. They are all scalable and FGSM, PGD, SPEIT, TDGIA need to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref  FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>), or furt DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref  FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and PGD  DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref  FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and PGD  r\" target=\"#b29\">30]</ref> or expensive memory consumption <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>.</p><p>Defenses can mainly be categorized into two types: p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s have been made to have robust GML models. For example, recent GNN architectures such as RobustGCN <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GRAND <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and ProG 33\">[34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>) or new model architectures (e.g., RobustGCN <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GNNGuard <ref type=\"bibr\" target=\"#b24\">[25]</ref>). Some ef>. More details can be found in Appendix A.4.2.</p><p>Five Defenses: GRB adopts RobustGCN (R-GCN) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and GN GIA need to train a surrogate model to conduct transfer attacks. (4) For defenses, we include R-GCN <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GNNGua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, have shown promising p f>, GAT <ref type=\"bibr\" target=\"#b5\">[6]</ref>, GIN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, APPNP <ref type=\"bibr\" target=\"#b7\">[8]</ref>, TAGCN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSAGE  f>, GAT <ref type=\"bibr\" target=\"#b5\">[6]</ref>, GIN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, APPNP <ref type=\"bibr\" target=\"#b7\">[8]</ref>, TAGCN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSAGE . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref  FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>), or furt DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref  FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and PGD  DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref  FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref type=\"bibr\" target=\"#b28\">[29]</ref>, NEA <ref type=\"bibr\" target=\"#b28\">[29]</ref>, STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and PGD  r\" target=\"#b29\">30]</ref> or expensive memory consumption <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>.</p><p>Defenses can mainly be categorized into two types: p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: L models are known to be vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar =\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, perturbing node attributes <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta m of scalability, some attacks are not applicable to large graphs due to their high time complexity <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>), or further modifying node features (e.g., Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>, FGSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <r , or further modifying node features (e.g., Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>, FGSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Metatta ot originally designed to increase robustness.</p><p>Twelve Attacks: Seven modification attacks-RND <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Graph machine learning (GML) models, from network embedding <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> to graph neural network. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: L models are known to be vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar =\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, perturbing node attributes <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta m of scalability, some attacks are not applicable to large graphs due to their high time complexity <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>), or further modifying node features (e.g., Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>, FGSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <r , or further modifying node features (e.g., Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>, FGSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Metatta ot originally designed to increase robustness.</p><p>Twelve Attacks: Seven modification attacks-RND <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rove the adversarial robustness of GNNs. In addition, pre-processing based methods, such as GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref> and GNNGuard <ref type=\"bibr\" target=\"#b24\">[25]</ref>, al  the attacked graphs as noisy ones and defenders can preprocess the adjacency matrix (e.g., GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GNN-Jaccard <ref type=\"bibr\" target=\"#b32\">[33]</ref>) or p><p>Five Defenses: GRB adopts RobustGCN (R-GCN) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and GNNGuard <ref type=\"bibr\" target=\"#b24\">[25]</ref>. A sfer attacks. (4) For defenses, we include R-GCN <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GNNGuard <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Among also suffer from the problem of scalability, due to the need of calculation on large dense matrices <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b20\">[21]</ref>, GRAND <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and ProGNN <ref type=\"bibr\" target=\"#b22\">[23]</ref> are designed to improve the adversarial robustness of GNNs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rove the adversarial robustness of GNNs. In addition, pre-processing based methods, such as GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref> and GNNGuard <ref type=\"bibr\" target=\"#b24\">[25]</ref>, al  the attacked graphs as noisy ones and defenders can preprocess the adjacency matrix (e.g., GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GNN-Jaccard <ref type=\"bibr\" target=\"#b32\">[33]</ref>) or p><p>Five Defenses: GRB adopts RobustGCN (R-GCN) <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and GNNGuard <ref type=\"bibr\" target=\"#b24\">[25]</ref>. A sfer attacks. (4) For defenses, we include R-GCN <ref type=\"bibr\" target=\"#b20\">[21]</ref>, GNN-SVD <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GNNGuard <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Among also suffer from the problem of scalability, due to the need of calculation on large dense matrices <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b2\">3]</ref> to graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=  domains, such as social network analysis <ref type=\"bibr\" target=\"#b0\">[1]</ref>, molecular graphs <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and recommender systems <ref type=\"bibr\" target=\"#b9\">[10]< <ref type=\"bibr\" target=\"#b7\">[8]</ref>, TAGCN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Note that the <ref type=\"bibr\" target=\"#b7\">[8]</ref>, TAGCN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. All models ar ense: GRB engages defense mechanisms to GML models, including preprocess-based and model-based ones.<ref type=\"bibr\" target=\"#b4\">(5)</ref> Evaluator: The attack or defense methods are evaluated under. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[31]</ref>, and PGD <ref type=\"bibr\" target=\"#b33\">[34]</ref>-and five injection attacks-RND, FGSM <ref type=\"bibr\" target=\"#b39\">[40]</ref>, PGD <ref type=\"bibr\" target=\"#b33\">[34]</ref>, SPEIT <ref ate model to conduct transfer attacks. (3) For injection attacks, we include 5 baselines: RND, FGSM <ref type=\"bibr\" target=\"#b39\">[40]</ref>, PGD <ref type=\"bibr\" target=\"#b33\">[34]</ref>, SPEIT <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dversarial attacks <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar \"bibr\" target=\"#b19\">20]</ref>, perturbing node attributes <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar plicable to large graphs due to their high time complexity <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> or expensive memory f type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Metattack <ref type=\"bibr\" target=\"#b12\">[13]</ref>). Differently, graph injection attacks add new malicious n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pted to other settings by using GRB's modualr coding framework.</p><p>In the work of Szegedy et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref>, the existence of adversarial examples was revealed for ML. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: L models are known to be vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar =\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, perturbing node attributes <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta m of scalability, some attacks are not applicable to large graphs due to their high time complexity <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref>), or further modifying node features (e.g., Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>, FGSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <r , or further modifying node features (e.g., Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref>, FGSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Metatta ot originally designed to increase robustness.</p><p>Twelve Attacks: Seven modification attacks-RND <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  can modify the original graph by adding or removing edges <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, perturbing node at ph modification attacks directly modify the existing graph, by adding or removing edges (e.g., DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref  p><p>Twelve Attacks: Seven modification attacks-RND <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref  models are salable to large graphs. (2) For modification attacks, we include 7 baselines: RND, DICE <ref type=\"bibr\" target=\"#b18\">[19]</ref>, FGA <ref type=\"bibr\" target=\"#b10\">[11]</ref>, FLIP <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0</ref>) and now, it also shows its powerfulness in natural language processing area. For instance, <ref type=\"bibr\" target=\"#b14\">Iter et al. (2020)</ref> employ contrastive learning to improve the q e works trying to apply contrastive learning into natural language processing domain. For instance, <ref type=\"bibr\" target=\"#b14\">Iter et al. (2020)</ref> propose a pretraining method for sentence re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion tasks whose constraints are taken from a finite set, e.g., binary sentiment or political slant <ref type=\"bibr\" target=\"#b34\">(Yang et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Prabhumoye et ch topic. EGPH is similar to other controlled text generation tasks whose constraints are sentiment <ref type=\"bibr\" target=\"#b34\">(Yang et al., 2018;</ref><ref type=\"bibr\" target=\"#b32\">Xu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Gao et al., 2019b,a)</ref>, generalised language models <ref type=\"bibr\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Devlin et al., 2019)</ref>. All the above methods obtain the feature o retrained GloVe <ref type=\"bibr\" target=\"#b25\">(Pennington et al., 2014)</ref>. We use a BERT-based <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> architecture for the style encoder E s and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Prabhumoye et al., 2018)</ref>, multiple personas <ref type=\"bibr\" target=\"#b16\">(Kang et al., 2019)</ref>, over source <ref type=\"bibr\">(X)</ref> wha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: approach based on variational inference (Kingma and Welling, 2014). The evaluation metrics are BLEU <ref type=\"bibr\" target=\"#b24\">(Papineni et al., 2002)</ref>, METEOR <ref type=\"bibr\" target=\"#b20\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a of deep learning, approaches based on the encoder-decoder framework have emerged in large numbers <ref type=\"bibr\" target=\"#b27\">(Prakash et al., 2016;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ., 2002)</ref>, METEOR <ref type=\"bibr\" target=\"#b20\">(Lavie and Agarwal, 2007)</ref> and ROUGE (R) <ref type=\"bibr\" target=\"#b23\">(Lin, 2004)</ref>. We also conduct human evaluation to investigate th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020b)</ref>. In addition to basic seq2seq model, <ref type=\"bibr\" target=\"#b21\">Li et al. (2018)</ref> add a pair-wise discriminator to judge whether. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntroduction</head><p>Paraphrase generation <ref type=\"bibr\" target=\"#b13\">(Gupta et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019)</ref>, aiming to generate a sentence with the same s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Learning (DRL) which maps different aspects of the input data to independent low-dimensional spaces <ref type=\"bibr\" target=\"#b5\">(Cheng et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b15\">Iyyer et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: intly trained with generation loss and ranking loss. In this work, we build our model based on BART <ref type=\"bibr\" target=\"#b12\">(Lewis et al., 2020)</ref>, a widely used pre-trained language model  generate solution expressions given a math word problem. Following the fine-tuning strategy of BART <ref type=\"bibr\" target=\"#b12\">(Lewis et al., 2020)</ref>, we take problem text, a sequence of token and XLNet <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2019)</ref>. Encoderdecoder models like BART <ref type=\"bibr\" target=\"#b12\">(Lewis et al., 2020)</ref> and T5 <ref type=\"bibr\" target=\"#b21\">(Raf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b20\">(Radford et al., 2019)</ref>, GPT3 <ref type=\"bibr\">(Brown et al., 2020)</ref> and XLNet <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2019)</ref>. Encoderdecoder models like BART <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  different elements of problems. ARIS <ref type=\"bibr\" target=\"#b7\">(Hosseini et al., 2014)</ref>   <ref type=\"bibr\" target=\"#b23\">(Roy and Roth, 2017)</ref> proposes Unit Dependency Graph to enhance . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 19b)</ref> applies recursive neural networks to model the tree structures of expressions. S-Aligned <ref type=\"bibr\" target=\"#b4\">(Chiang and Chen, 2019</ref>) tracks the semantic meanings of operands d graph information to improve performance <ref type=\"bibr\" target=\"#b32\">(Wang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b4\">Chiang and Chen, 2019;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: teof-the-art results in many NLP benchmarks <ref type=\"bibr\" target=\"#b30\">(Wang et al., 2018a</ref><ref type=\"bibr\" target=\"#b29\">(Wang et al., , 2019a))</ref>. These models are usually based on Tran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: M-based sequence-tosequence (Seq2Seq) models <ref type=\"bibr\" target=\"#b33\">(Wang et al., 2017</ref><ref type=\"bibr\" target=\"#b31\">(Wang et al., , 2018b;;</ref><ref type=\"bibr\" target=\"#b34\">Xie and S rget=\"#b33\">(Wang et al., 2017)</ref> uses a vanilla Seq2Seq model to generate expressions. Math-EN <ref type=\"bibr\" target=\"#b31\">(Wang et al., 2018b)</ref> uses the equation normalization to avoid e get=\"#b33\">(Wang et al., 2017)</ref> is the first to apply vanilla RNN-based models to MWP. Math-EN <ref type=\"bibr\" target=\"#b31\">(Wang et al., 2018b)</ref> introduces equation normalization and comp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  different elements of problems. ARIS <ref type=\"bibr\" target=\"#b7\">(Hosseini et al., 2014)</ref>   <ref type=\"bibr\" target=\"#b23\">(Roy and Roth, 2017)</ref> proposes Unit Dependency Graph to enhance . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ges including Chinese. For the MAWPS dataset, we also use mBART25. We optimize our model with AdamW <ref type=\"bibr\" target=\"#b18\">(Loshchilov and Hutter, 2019)</ref>. The training hyperparameters are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b20\">(Radford et al., 2019)</ref>, GPT3 <ref type=\"bibr\">(Brown et al., 2020)</ref> and XLNet <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2019)</ref>. Encoderdecoder models like BART <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmalize MWP as a generation task and commonly adopt LSTM-based sequence-tosequence (Seq2Seq) models <ref type=\"bibr\" target=\"#b33\">(Wang et al., 2017</ref><ref type=\"bibr\" target=\"#b31\">(Wang et al.,  he effectiveness of the proposed model, we conduct extensive experiments on the datasets of Math23K <ref type=\"bibr\" target=\"#b33\">(Wang et al., 2017)</ref> and MAWPS <ref type=\"bibr\" target=\"#b10\">(K rimental Setup</head><p>Datasets. We conduct the experiments on two commonly-used datasets: Math23K <ref type=\"bibr\" target=\"#b33\">(Wang et al., 2017)</ref> and MAWPS <ref type=\"bibr\" target=\"#b10\">(K lines. We compare our model with the following baselines including the state-of-the-art models: DNS <ref type=\"bibr\" target=\"#b33\">(Wang et al., 2017)</ref> uses a vanilla Seq2Seq model to generate ex  methods. Recently, deep learning models have become prevailing methods for math word problems. DNS <ref type=\"bibr\" target=\"#b33\">(Wang et al., 2017)</ref> is the first to apply vanilla RNN-based mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l</head><p>Pre-trained language models have obtained stateof-the-art results in many NLP benchmarks <ref type=\"bibr\" target=\"#b30\">(Wang et al., 2018a</ref><ref type=\"bibr\" target=\"#b29\">(Wang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed large teacher model <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. During the distillat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecommendation model has become very large to capture the complexity of personalized recommendations <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta  has become one of the major obstacles for real-time service <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>.</p><p>To reduce th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ll as high latency for inference, which has become one of the major obstacles for real-time service <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ hey have problems such as easily falling into a local optimum or applicable only to specific models <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. To address the prob 7\">28]</ref>. To address the problems, Knowledge Distillation (KD) has been actively studied for RS <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe tively lower-ranked compared to the former. As consistently shown in the existing distillation work <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ nly on the original feedback.</p><p>The state-of-the-art method, Relaxed Ranking Distillation (RRD) <ref type=\"bibr\" target=\"#b8\">[9]</ref>, formulates the distillation process as a ranking matching p </ref><ref type=\"bibr\" target=\"#b25\">26]</ref> that do not directly utilize the ranking information <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>Still, there are limitations in RRD. First, it transf rg/ns/1.0\"><head n=\"2.2\">Distilling the Ranking Information</head><p>The ranking distillation (RRD) <ref type=\"bibr\" target=\"#b8\">[9]</ref> formulates the distillation as a ranking matching problem be To make the student better focus on top-ranked items, we also adopt relaxed permutation probability <ref type=\"bibr\" target=\"#b8\">[9]</ref> that ignores the lowranked items' detailed orders. Formally, MENTS 3.1 Experiment Setup</head><p>We closely follow the setup of the state-of-the-art method, RRD <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Specifically, datasets, base models, evaluation protocol, a 9]</ref>. Specifically, datasets, base models, evaluation protocol, and the metrics are the same as <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Due to the limited space, we omit the detailed explanations 9]</ref>. Due to the limited space, we omit the detailed explanations of the setup. Please refer to <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Datasets. We use CiteULike <ref type=\"bibr\" target=\"#b26\">[  type=\"bibr\" target=\"#b21\">[22]</ref> which are public real-world datasets. After the preprocessing <ref type=\"bibr\" target=\"#b8\">[9]</ref>, CiteULike has 5,220 users, 25,182 items, and 115,142 intera /item representations are set to 200 for the teacher model, and 20 for the student model. Following <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we denote the student model trained without distillation as interacted items for each user are held out for test/validation, and the rest are used for training <ref type=\"bibr\" target=\"#b8\">[9]</ref>. We adopt two top-\ud835\udc41 ranking evaluation metrics, namely Hit R ependent runs. Baselines. We compare DCD with the state-of-the-art ranking distillation method, RRD <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Note that we do not include the previous methods distilling D <ref type=\"bibr\" target=\"#b18\">[19]</ref>), because RRD already outperforms them by a huge margin <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Implementation details. We use PyTorch <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. KD is a model comp gnificantly improves performance over the previous methods <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> that do not directly utilize the ranking information <ref t ng distillation work <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, the student takes a huge benefit by learning the teacher's /ref>. Note that we do not include the previous methods distilling point-wise information (e.g., RD <ref type=\"bibr\" target=\"#b25\">[26]</ref>, CD <ref type=\"bibr\" target=\"#b18\">[19]</ref>), because RR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ms, and 609,655 interactions. Base models. We use two base models for the top-\ud835\udc41 recommendation: BPR <ref type=\"bibr\" target=\"#b23\">[24]</ref> and NeuMF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blem formulation and Notations</head><p>We focus on top-\ud835\udc41 recommendation task for implicit feedback <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Given implicit user. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  method trains the student to preserve the orders of ranking in \ud835\udc45 \ud835\udc62 \ud835\udc47 by using a variant of ListMLE <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The core idea is to define a permutation probability base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n various industries to provide personalized user experience <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b8\">[9]</ref>. Datasets. We use CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Foursquare <ref type=\"bibr\" target=\"#b21\">[22]</ref> which are public real-world datasets. After the preprocess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o base models for the top-\ud835\udc41 recommendation: BPR <ref type=\"bibr\" target=\"#b23\">[24]</ref> and NeuMF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which have different architectures and optimization strateg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o base models for the top-\ud835\udc41 recommendation: BPR <ref type=\"bibr\" target=\"#b23\">[24]</ref> and NeuMF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which have different architectures and optimization strateg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms, and 609,655 interactions. Base models. We use two base models for the top-\ud835\udc41 recommendation: BPR <ref type=\"bibr\" target=\"#b23\">[24]</ref> and NeuMF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o base models for the top-\ud835\udc41 recommendation: BPR <ref type=\"bibr\" target=\"#b23\">[24]</ref> and NeuMF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which have different architectures and optimization strateg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b8\">[9]</ref>. Datasets. We use CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Foursquare <ref type=\"bibr\" target=\"#b21\">[22]</ref> which are public real-world datasets. After the preprocess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b16\">17]</ref>.</p><p>To reduce the inference latency, early methods adopt hash techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  method trains the student to preserve the orders of ranking in \ud835\udc45 \ud835\udc62 \ud835\udc47 by using a variant of ListMLE <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The core idea is to define a permutation probability base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ized user experience <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. For achieving high. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: at learning only with the user-side ranking information degrades the quality of user representation <ref type=\"bibr\" target=\"#b6\">[7]</ref> and provides a view insufficient to fully understand the spa t=\"#b13\">14]</ref>, learning only the user-side ranking degrades the quality of user representation <ref type=\"bibr\" target=\"#b6\">[7]</ref> and also provides a restricted view insufficient to understa er-side and the item-side, providing a comprehensive view to better understand both users and items <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. We validate the sup  corrections for discrepancy in terms of the item-side ranking. As pointed out in the previous work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, learning only the u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. However, large models incur correspondingly large computat et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> or tree-based data structure <ref type=\"bibr\" target=\"#b0\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b8\">[9]</ref>. Datasets. We use CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Foursquare <ref type=\"bibr\" target=\"#b21\">[22]</ref> which are public real-world datasets. After the preprocess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blem formulation and Notations</head><p>We focus on top-\ud835\udc41 recommendation task for implicit feedback <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Given implicit user. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Knowledge Distillation (KD) has been actively studied for RS <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ insufficient to fully understand the sparse implicit arXiv:2109.03459v1 [cs.IR] 8 Sep 2021 feedback <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. Particularly in KD. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. However, large models incur correspondingly large computat et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> or tree-based data structure <ref type=\"bibr\" target=\"#b0\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ommender Systems (RS) are widely used in various industries to provide personalized user experience <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ized user experience <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. For achieving high. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n various industries to provide personalized user experience <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b8\">[9]</ref>. Datasets. We use CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Foursquare <ref type=\"bibr\" target=\"#b21\">[22]</ref> which are public real-world datasets. After the preprocess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the inference latency, early methods adopt hash techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o base models for the top-\ud835\udc41 recommendation: BPR <ref type=\"bibr\" target=\"#b23\">[24]</ref> and NeuMF <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which have different architectures and optimization strateg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ized user experience <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. For achieving high. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  method trains the student to preserve the orders of ranking in \ud835\udc45 \ud835\udc62 \ud835\udc47 by using a variant of ListMLE <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The core idea is to define a permutation probability base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b8\">[9]</ref>. Datasets. We use CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Foursquare <ref type=\"bibr\" target=\"#b21\">[22]</ref> which are public real-world datasets. After the preprocess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Knowledge Distillation (KD) has been actively studied for RS <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ insufficient to fully understand the sparse implicit arXiv:2109.03459v1 [cs.IR] 8 Sep 2021 feedback <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. Particularly in KD. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  method trains the student to preserve the orders of ranking in \ud835\udc45 \ud835\udc62 \ud835\udc47 by using a variant of ListMLE <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The core idea is to define a permutation probability base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b8\">[9]</ref>. Datasets. We use CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Foursquare <ref type=\"bibr\" target=\"#b21\">[22]</ref> which are public real-world datasets. After the preprocess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  method trains the student to preserve the orders of ranking in \ud835\udc45 \ud835\udc62 \ud835\udc47 by using a variant of ListMLE <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The core idea is to define a permutation probability base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b16\">17]</ref>.</p><p>To reduce the inference latency, early methods adopt hash techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b16\">17]</ref>.</p><p>To reduce the inference latency, early methods adopt hash techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. However, large mod g into a local optimum or applicable only to specific models <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. To address the problems, Knowledge Distillation (KD) has b get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. KD is a model compression technique that improves the perf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the inference latency, early methods adopt hash techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the inference latency, early methods adopt hash techniques <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. However, large mod g into a local optimum or applicable only to specific models <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. To address the problems, Knowledge Distillation (KD) has b get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. KD is a model compression technique that improves the perf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s of the setup. Please refer to <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Datasets. We use CiteULike <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Foursquare <ref type=\"bibr\" target=\"#b21\">[22]</ref> w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blem formulation and Notations</head><p>We focus on top-\ud835\udc41 recommendation task for implicit feedback <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Given implicit user. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odels have been more and more powerful due to recent introduction of large pre-trained Transformers <ref type=\"bibr\" target=\"#b14\">(Liu and Lapata 2019;</ref><ref type=\"bibr\" target=\"#b18\">Raffel et a 14\">(Liu and Lapata 2019)</ref>. As the recent introduction of large pre-trained transformer models <ref type=\"bibr\" target=\"#b14\">(Liu and Lapata 2019;</ref><ref type=\"bibr\" target=\"#b5\">Dong et al.  erful enough to model documents. Quality of summaries produced by these mdoels are not satisfactory <ref type=\"bibr\" target=\"#b14\">(Liu and Lapata 2019)</ref>. As the recent introduction of large pre-  is a baseline which simply takes the leading three sentences in a document as its summary. BERTEXT <ref type=\"bibr\" target=\"#b14\">(Liu and Lapata 2019)</ref> employs BERT as encoder and predicts whet xtractive and abstractive summarization models <ref type=\"bibr\" target=\"#b9\">(He et al. 2020a;</ref><ref type=\"bibr\" target=\"#b14\">Dou et al. 2021)</ref> or multiple summarization models <ref type=\"bi  2020a;</ref><ref type=\"bibr\" target=\"#b14\">Dou et al. 2021)</ref> or multiple summarization models <ref type=\"bibr\" target=\"#b14\">(Liu, Dou, and Liu 2021)</ref>. Unfortunately, pre-training transform rds (or sentences) as additional input. SimCLS <ref type=\"bibr\">(Chen et al. 2020)</ref> and Refsum <ref type=\"bibr\" target=\"#b14\">(Liu, Dou, and Liu 2021)</ref> train re-ranking models to rank multip hout reference summaries by contrasting the document with the summaries using a ranking model. GSum <ref type=\"bibr\" target=\"#b14\">(Dou et al. 2021)</ref> takes different kinds of external guidance as ent the results of recent combination models in the third block. CTRLsum (He et al. 2020a) and GSum <ref type=\"bibr\" target=\"#b14\">(Dou et al. 2021)</ref> combine a keywords extraction model (or an ex ce as additional input to the document and advances summarization performance significantly. SimCLS <ref type=\"bibr\" target=\"#b14\">(Liu and Liu 2021)</ref> proposes a contrastive based framework for a </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">Evaluations</head><p>We use ROUGE <ref type=\"bibr\" target=\"#b14\">(Lin 2004)</ref> to measure the quality of generated summaries. We re and coverage models. Large pre-trained language models mostly dominate summarization. BERTSUMEXTABS <ref type=\"bibr\" target=\"#b14\">(Liu and Lapata 2019</ref>) is an abstractive model with encoder init cessing procedures in<ref type=\"bibr\" target=\"#b7\">(Durrett, Berg-Kirkpatrick, and Klein 2016;</ref><ref type=\"bibr\" target=\"#b14\">Liu and Lapata 2019)</ref>, we first obtain 110,540 articles with abs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"bibr\" target=\"#b18\">Raffel et al. 2020;</ref><ref type=\"bibr\" target=\"#b5\">Dong et al. 2019</ref>; <ref type=\"bibr\" target=\"#b13\">Lewis et al. 2020)</ref>, the training paradigm for abstractive model ased model SeqCo consistently improves upon a strong ab-stractive summarization model based on BART <ref type=\"bibr\" target=\"#b13\">(Lewis et al. 2020</ref>) across three different summarization datase (Raffel et al. 2020)</ref>, PE-GASUS <ref type=\"bibr\" target=\"#b26\">(Zhang et al. 2020)</ref>, BART <ref type=\"bibr\" target=\"#b13\">(Lewis et al. 2020</ref>) and STEP <ref type=\"bibr\" target=\"#b28\">(Zo ype=\"bibr\" target=\"#b5\">Dong et al. 2019;</ref><ref type=\"bibr\" target=\"#b28\">Zou et al. 2020;</ref><ref type=\"bibr\" target=\"#b13\">Lewis et al. 2020;</ref><ref type=\"bibr\" target=\"#b26\">Zhang et al. 2 26\">Zhang et al. (2020)</ref> predict gapped sentences from a document removing these sentences and <ref type=\"bibr\" target=\"#b13\">Lewis et al. (2020)</ref> propose sentence permutation and text infil .org/ns/1.0\"><head n=\"4.2\">Implementation Details</head><p>Our model is initialized from BART Large <ref type=\"bibr\" target=\"#b13\">(Lewis et al. 2020)</ref>. Therefore, the size is identical with BART pe=\"bibr\" target=\"#b13\">(Lewis et al. 2020)</ref>. Therefore, the size is identical with BART Large <ref type=\"bibr\" target=\"#b13\">(Lewis et al. 2020)</ref>. Specifically, the encoder and decoder are  trainable parameters.</p><p>We optimize the model using Adam with \u03b2 1 = 0.9, \u03b2 2 = 0.999. Following <ref type=\"bibr\" target=\"#b13\">(Lewis et al. 2020)</ref>, we employ a linear schedule for the learni g (Paulus, Xiong, and Socher 2018), we also blocked repeated trigrams during beam search. Following <ref type=\"bibr\" target=\"#b13\">(Lewis et al. 2020)</ref>, the articles are truncated to 1024 tokens . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b28\">Zou et al. 2020;</ref><ref type=\"bibr\" target=\"#b13\">Lewis et al. 2020;</ref><ref type=\"bibr\" target=\"#b26\">Zhang et al. 2020;</ref><ref type=\"bibr\" target=\"#b18\">Raffel et al.   designed Transformer encoder and decoder with language model and masked language model objectives. <ref type=\"bibr\" target=\"#b26\">Zhang et al. (2020)</ref> predict gapped sentences from a document re anguage modeling objectives. T5 <ref type=\"bibr\" target=\"#b18\">(Raffel et al. 2020)</ref>, PE-GASUS <ref type=\"bibr\" target=\"#b26\">(Zhang et al. 2020)</ref>, BART <ref type=\"bibr\" target=\"#b13\">(Lewis 2020</ref>) pre-train Seq2Seq transformers using different unsupervised text-to-text tasks. PEGASUS <ref type=\"bibr\" target=\"#b26\">(Zhang et al. 2020</ref>) is trained by predicting gapped sentences (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Abstractive summarization is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem <ref type=\"bibr\" target=\"#b21\">(Sutskever, Vinyals, and Le 2014)</ref>, where a document is viewed a bly because small and shallow LSTM (Hochreiter and Schmidhuber 1997) based attentive seq2seq models <ref type=\"bibr\" target=\"#b21\">(Sutskever, Vinyals, and Le 2014;</ref><ref type=\"bibr\" target=\"#b1\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b13\">(Lewis et al. 2020</ref>) across three different summarization datasets (i.e., CN-N/DailyMail <ref type=\"bibr\" target=\"#b10\">(Hermann et al. 2015)</ref>, New York Times <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion. Note that we can also define a simpler similarity function using the [CLS] pooling as in BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al. 2019)</ref>:</p><formula xml:id=\"formula_10\">sim(S i , . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b14\">(Liu and Lapata 2019;</ref><ref type=\"bibr\" target=\"#b18\">Raffel et al. 2020;</ref><ref type=\"bibr\" target=\"#b5\">Dong et al. 2019</ref>; <ref type=\"bibr\" target=\"#b13\">Lewis et al. 20 n of large pre-trained transformer models <ref type=\"bibr\" target=\"#b14\">(Liu and Lapata 2019;</ref><ref type=\"bibr\" target=\"#b5\">Dong et al. 2019;</ref><ref type=\"bibr\" target=\"#b28\">Zou et al. 2020; ed with generation (or summarization) tailored objectives on huge amount of unlabeled text (\u2265160G). <ref type=\"bibr\" target=\"#b5\">Dong et al. (2019)</ref> pre-train jointly designed Transformer encode  is an abstractive model with encoder initialized with BERT and decoder randomly initialized. UniLM <ref type=\"bibr\" target=\"#b5\">(Dong et al. 2019</ref>) is trained using language modeling and masked. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: duction of large pre-trained Transformers <ref type=\"bibr\" target=\"#b14\">(Liu and Lapata 2019;</ref><ref type=\"bibr\" target=\"#b18\">Raffel et al. 2020;</ref><ref type=\"bibr\" target=\"#b5\">Dong et al. 20 \"bibr\" target=\"#b13\">Lewis et al. 2020;</ref><ref type=\"bibr\" target=\"#b26\">Zhang et al. 2020;</ref><ref type=\"bibr\" target=\"#b18\">Raffel et al. 2020)</ref>, abstractive models are greatly improved. B g et al. 2019</ref>) is trained using language modeling and masked language modeling objectives. T5 <ref type=\"bibr\" target=\"#b18\">(Raffel et al. 2020)</ref>, PE-GASUS <ref type=\"bibr\" target=\"#b26\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  learning methods are proposed in the context of self-supervised learning for image representations <ref type=\"bibr\" target=\"#b25\">(Wu et al. 2018;</ref><ref type=\"bibr\" target=\"#b9\">He et al. 2020b;<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd end of sequence tokens. We predict Y one token at a time given X. We adopt the Transformer model <ref type=\"bibr\" target=\"#b23\">(Vaswani et al. 2017)</ref>, which is composed of an encoder Transfor n(H j , H i , H i ) (9)</formula><p>where MultiHeadAttn(\u2022, \u2022, \u2022) is the multi-head attention module <ref type=\"bibr\" target=\"#b23\">(Vaswani et al. 2017</ref>) and H j , H i and H i are the query, key   = 0.99. We employ label smoothing of 0.1 <ref type=\"bibr\" target=\"#b22\">(Szegedy et al. 2016;</ref><ref type=\"bibr\" target=\"#b23\">Vaswani et al. 2017)</ref>. The models for CNNDM are trained on 8 Tes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ased attentive seq2seq models <ref type=\"bibr\" target=\"#b21\">(Sutskever, Vinyals, and Le 2014;</ref><ref type=\"bibr\" target=\"#b1\">Bahdanau, Cho, and Bengio 2015)</ref> without pre-training are not pow xity for implementation. There is also an interesting line of work without using negative examples. <ref type=\"bibr\" target=\"#b1\">Caron et al. (2020)</ref> employ online clustering to assign codes for type=\"bibr\" target=\"#b25\">(Wu et al. 2018;</ref><ref type=\"bibr\" target=\"#b9\">He et al. 2020b;</ref><ref type=\"bibr\" target=\"#b1\">Caron et al. 2020;</ref><ref type=\"bibr\" target=\"#b9\">Grill et al. 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lly viewed as a sequence to sequence learning problem <ref type=\"bibr\">(Nallapati et al. 2016;</ref><ref type=\"bibr\" target=\"#b20\">See, Liu, and Manning 2017;</ref><ref type=\"bibr\" target=\"#b16\">Paulu nce summaries) from the CNN and Daily Mail websites. We follow the standard pre-processing steps in <ref type=\"bibr\" target=\"#b20\">(See, Liu, and Manning 2017)</ref>  XSum The articles in the XSum dat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce between sentences using randomly sampled sentences as negative examples. More recently, MatchSum <ref type=\"bibr\" target=\"#b27\">(Zhong et al. 2020)</ref> formulates extractive summarization as a se d Lapata 2019)</ref> employs BERT as encoder and predicts whether a sentence is a summary. MatchSum <ref type=\"bibr\" target=\"#b27\">(Zhong et al. 2020)</ref> is the best performing extractive models, w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b14\">(Liu and Lapata 2019;</ref><ref type=\"bibr\" target=\"#b18\">Raffel et al. 2020;</ref><ref type=\"bibr\" target=\"#b5\">Dong et al. 2019</ref>; <ref type=\"bibr\" target=\"#b13\">Lewis et al. 20 n of large pre-trained transformer models <ref type=\"bibr\" target=\"#b14\">(Liu and Lapata 2019;</ref><ref type=\"bibr\" target=\"#b5\">Dong et al. 2019;</ref><ref type=\"bibr\" target=\"#b28\">Zou et al. 2020; ed with generation (or summarization) tailored objectives on huge amount of unlabeled text (\u2265160G). <ref type=\"bibr\" target=\"#b5\">Dong et al. (2019)</ref> pre-train jointly designed Transformer encode  is an abstractive model with encoder initialized with BERT and decoder randomly initialized. UniLM <ref type=\"bibr\" target=\"#b5\">(Dong et al. 2019</ref>) is trained using language modeling and masked. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ee, Liu, and Manning 2017;</ref><ref type=\"bibr\" target=\"#b16\">Paulus, Xiong, and Socher 2018;</ref><ref type=\"bibr\" target=\"#b8\">Gehrmann, Deng, and Rush 2018)</ref>. Probably because small and shall. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion. Note that we can also define a simpler similarity function using the [CLS] pooling as in BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al. 2019)</ref>:</p><formula xml:id=\"formula_10\">sim(S i , . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hods are mostly used in pre-training or natural language understanding tasks. For example, word2vec <ref type=\"bibr\" target=\"#b15\">(Mikolov et al. 2013</ref>) learns the word embeddings by distinguish h these sentences masked. Similar to BERTSUMEXTABS, the encoder of STEP is initialized from RoBERTa <ref type=\"bibr\" target=\"#b15\">(Liu et al. 2019)</ref>. BART + R3F <ref type=\"bibr\" target=\"#b0\">(Ag  is similar. ROBERTA-S2S is a transformer based Seq2Seq model with encoder initialized from RoBERTa <ref type=\"bibr\" target=\"#b15\">(Liu et al. 2019</ref>) and its results are reported in <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and 11,334 articles for test).</p><p>All datasets are tokenized with the byte-pair encoding of GPT2 <ref type=\"bibr\" target=\"#b17\">(Radford et al. 2019)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: els on downstream tasks such as classification and clustering. To address this problem, inspired by <ref type=\"bibr\" target=\"#b26\">[27]</ref>, we introduce a graph prototypical contrastive learning (G many sub-clusters in the embedding space, which is consistent with the prior findings on image data <ref type=\"bibr\" target=\"#b26\">[27]</ref>.   Multiplex Heterogeneous Graph Level. The visualizations clustering objective. Inspired by these works, SwAV <ref type=\"bibr\" target=\"#b2\">[3]</ref> and PCL <ref type=\"bibr\" target=\"#b26\">[27]</ref> combine deep clustering with CL. SwAV compares the cluster ers of E to pull node embeddings closer to their assigned clusters (or prototypes).</p><p>Following <ref type=\"bibr\" target=\"#b26\">[27]</ref>, we maximize the following log likelihood: </p><p>where \ud835\udc44 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >[60]</ref>. Early methods, such as DeepWalk <ref type=\"bibr\" target=\"#b41\">[42]</ref> and node2vec <ref type=\"bibr\" target=\"#b11\">[12]</ref>, sample positive node pairs based on their local proximity hods disregarding node attributes: Deep-Walk <ref type=\"bibr\" target=\"#b41\">[42]</ref> and node2vec <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and methods considering attributes: GCN <ref type=\"bibr\"  >[35]</ref>, early methods, such as DeepWalk <ref type=\"bibr\" target=\"#b41\">[42]</ref> and node2vec <ref type=\"bibr\" target=\"#b11\">[12]</ref> use random walks to sample positive pairs of nodes. LINE <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , including methods disregarding node attributes: CMNA <ref type=\"bibr\" target=\"#b4\">[5]</ref>, MNE <ref type=\"bibr\" target=\"#b67\">[68]</ref>, and methods considering attributes: mGCN <ref type=\"bibr\" to InfoNCE <ref type=\"bibr\" target=\"#b35\">[36]</ref>.</p><p>For multiplex heterogeneous graphs, MNE <ref type=\"bibr\" target=\"#b67\">[68]</ref>, MVN2VEC <ref type=\"bibr\" target=\"#b46\">[47]</ref> and GAT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  designed for single view image data; it heavily relies on data augmentations and momentum contrast <ref type=\"bibr\" target=\"#b14\">[15]</ref>; it has some complex assumptions over cluster distribution. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: leverage graph transformation to generate pairs. DGI <ref type=\"bibr\" target=\"#b52\">[53]</ref>, GMI <ref type=\"bibr\" target=\"#b40\">[41]</ref>, HDI <ref type=\"bibr\" target=\"#b17\">[18]</ref> and CommDGI. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng various objects <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b64\">65,</ref><ref type=\"bibr\" tar  <ref type=\"bibr\" target=\"#b11\">[12]</ref> use random walks to sample positive pairs of nodes. LINE <ref type=\"bibr\" target=\"#b49\">[50]</ref> and SDNE <ref type=\"bibr\" target=\"#b55\">[56]</ref> determi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e, minimizing the KL-divergence will minimize \ud835\udc3b (p \ud835\udc63 \ud835\udc5b , q \ud835\udc63 \u2032 \ud835\udc5b ). On the other hand, according to <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, we have the follo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b38\">39]</ref>, multi-view graph <ref type=\"bibr\" target=\"#b45\">[46]</ref>, multi-layer graph <ref type=\"bibr\" target=\"#b25\">[26]</ref> and multi-dimension graph <ref type=\"bibr\" target=\"#b29\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t in the embedding space <ref type=\"bibr\" target=\"#b59\">[60]</ref>. Early methods, such as DeepWalk <ref type=\"bibr\" target=\"#b41\">[42]</ref> and node2vec <ref type=\"bibr\" target=\"#b11\">[12]</ref>, sa e with methods for (1) attributed graphs, including methods disregarding node attributes: Deep-Walk <ref type=\"bibr\" target=\"#b41\">[42]</ref> and node2vec <ref type=\"bibr\" target=\"#b11\">[12]</ref>, an ce. Inspired by word2vec <ref type=\"bibr\" target=\"#b34\">[35]</ref>, early methods, such as DeepWalk <ref type=\"bibr\" target=\"#b41\">[42]</ref> and node2vec <ref type=\"bibr\" target=\"#b11\">[12]</ref> use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> as well as adaptive augmentation <ref type=\"bibr\" target=\"#b75\">[76]</ref>.</p><p>Albeit the success of these methods, they define po  DGCN <ref type=\"bibr\" target=\"#b76\">[77]</ref>, HDI <ref type=\"bibr\" target=\"#b17\">[18]</ref>, GCA <ref type=\"bibr\" target=\"#b75\">[76]</ref> and GraphCL <ref type=\"bibr\" target=\"#b66\">[67]</ref>;</p> type=\"bibr\" target=\"#b66\">[67]</ref> uses various graph augmentations to obtain positive nodes. GCA <ref type=\"bibr\" target=\"#b75\">[76]</ref> generates positive and negative pairs based on their impor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ty information. Recent methods combine CL with clustering to further improve the performance. SCAGC <ref type=\"bibr\" target=\"#b60\">[61]</ref> treats nodes within the same cluster as positive pairs. MC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nal variational autoencoder (CVAE) <ref type=\"bibr\" target=\"#b28\">(Kingma &amp; Welling, 2013;</ref><ref type=\"bibr\" target=\"#b51\">Sohn et al., 2015)</ref> to learn the conditional distribution of the g, we use X v as a condition since the distribution of X u (u \u2208 N v ) is related to X v . Following <ref type=\"bibr\" target=\"#b51\">Sohn et al. (2015)</ref>, the latent variable z is generated from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#b12\">(Fadaee et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">S \u00b8ahin &amp; Steedman, 2019;</ref><ref type=\"bibr\" target=\"#b64\">Xia et al., 2019)</ref>, but remains under-explored on graph-structur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b14\">Feng et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Kong et al., 2020;</ref><ref type=\"bibr\" target=\"#b13\">Fang et al., 2021)</ref>. Topology-level augmentation perturbs the ad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rt, 2017;</ref><ref type=\"bibr\" target=\"#b23\">Hamilton et al., 2017)</ref>, and random walk methods <ref type=\"bibr\" target=\"#b39\">(Perozzi et al., 2014;</ref><ref type=\"bibr\" target=\"#b54\">Tang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2018)</ref>, GCNII <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020b)</ref>, and RevGNN-Deep <ref type=\"bibr\" target=\"#b33\">(Li et al., 2021)</ref> adds the output of shallow layers to the deep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2018)</ref>, GCNII <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020b)</ref>, and RevGNN-Deep <ref type=\"bibr\" target=\"#b33\">(Li et al., 2021)</ref> adds the output of shallow layers to the deep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b12\">(Fadaee et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">S \u00b8ahin &amp; Steedman, 2019;</ref><ref type=\"bibr\" target=\"#b64\">Xia et al., 2019)</ref>, but remains under-explored on graph-structur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 19a)</ref> propose a generative GCN model to learn node representations for growing graphs. ConDgen <ref type=\"bibr\" target=\"#b68\">(Yang et al., 2019)</ref> exploits the GCN encoder to handle the inva. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: turing asymmetrical relations of objects, we propose to learn event embeddings in hyperbolic spaces <ref type=\"bibr\" target=\"#b7\">(Ganea et al., 2018b)</ref>.</p><p>Hyperbolic spaces can be viewed as  rov et al., 2016)</ref> to the Poincar\u00e9 ball, levering the projected areas to infer data relations. <ref type=\"bibr\" target=\"#b7\">Ganea et al. (2018b)</ref> introduced a framework of hyperbolic neural tates of two events encodes relevant information to predict the event temporal relations. Although, <ref type=\"bibr\" target=\"#b7\">Ganea et al. (2018b)</ref> reported that mixing hyperbolic neural netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: h this has prompted the recent development of neural architectures for automatic feature extraction <ref type=\"bibr\" target=\"#b21\">(Ning et al., 2019;</ref><ref type=\"bibr\" target=\"#b37\">Wang et al.,  g automatically reliable event features for TempRel extraction when provided with high-quality data <ref type=\"bibr\" target=\"#b21\">(Ning et al., 2019)</ref>, alleviating significantly the required hum  We can consider the first three relations as temporal connections, and regard VAGUE as no relation <ref type=\"bibr\" target=\"#b21\">(Ning et al., 2019)</ref>. Therefore, the set D contains event pairs   2018)</ref>, which has been reported achieving the best overall results for the models proposed in <ref type=\"bibr\" target=\"#b21\">(Ning et al., 2019)</ref>. We observe that the proposed Poincar\u00e9 even ngineer effort and yielding results outperforming the above mentioned methodologies. In particular, <ref type=\"bibr\" target=\"#b21\">Ning et al. (2019)</ref> employed an LSTM network <ref type=\"bibr\" ta rk architecture. Additionally, commonsense knowledge can be incorporated within the HGRU. We follow <ref type=\"bibr\" target=\"#b21\">Ning et al. (2019)</ref> and use a Siamese network trained on TEMPROB stigated several ways to utilize them, and include two of them in this paper. The first one follows <ref type=\"bibr\" target=\"#b21\">Ning et al. (2019)</ref>, which only uses the static output of the pr ut of the pre-trained models. This approach is fast and allows a fair comparison with the models in <ref type=\"bibr\" target=\"#b21\">Ning et al. (2019)</ref>.</p><p>The second approach fine-tunes the pr ed methods are directly taken from the cited papers.</p><p>For consistency and fair comparison with <ref type=\"bibr\" target=\"#b21\">Ning et al. (2019)</ref>, we test our method with inputs from the ELM s. smaller dataset, with just 25 documents and 2.6K TempRels. Due to the TCR limited size, we follow<ref type=\"bibr\" target=\"#b21\">Ning et al. (2019)</ref> by using the temporal relations in TCR to te sed in Table <ref type=\"table\">2</ref>, we follow the widely adopted evaluation metrics proposed in <ref type=\"bibr\" target=\"#b21\">Ning et al. (2019)</ref>, which is also adopted by <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al methods leveraging hand-crafted features <ref type=\"bibr\" target=\"#b17\">(Mani et al., 2006;</ref><ref type=\"bibr\" target=\"#b4\">Chambers et al., 2007;</ref><ref type=\"bibr\" target=\"#b35\">Verhagen an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es. In particular, <ref type=\"bibr\" target=\"#b21\">Ning et al. (2019)</ref> employed an LSTM network <ref type=\"bibr\" target=\"#b11\">(Hochreiter and Schmidhuber, 1997)</ref> to encode the textual events. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on <ref type=\"bibr\" target=\"#b8\">(Gulcehre et al., 2018)</ref>, to detect hierarchical entity types <ref type=\"bibr\" target=\"#b16\">(L\u00f3pez and Strube, 2020)</ref>, and for document classification <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ugh time. This has recently led to increasing interest in research for temporal relation extraction <ref type=\"bibr\" target=\"#b3\">(Chambers et al., 2014;</ref><ref type=\"bibr\" target=\"#b37\">Wang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ugh time. This has recently led to increasing interest in research for temporal relation extraction <ref type=\"bibr\" target=\"#b3\">(Chambers et al., 2014;</ref><ref type=\"bibr\" target=\"#b37\">Wang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: efficient to encode tree-like networks <ref type=\"bibr\" target=\"#b19\">(Nickel and Kiela, 2017;</ref><ref type=\"bibr\" target=\"#b33\">Tran et al., 2020)</ref>.</p><p>Previous works have explored their us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral orders. Temporal reasoning has been proven beneficial, for example, in understanding narratives <ref type=\"bibr\" target=\"#b5\">(Cheng et al., 2013)</ref>, answering questions <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eves much higher inter-annotator agreements (IAA) than previous temporal datasets, such as TB-Dense <ref type=\"bibr\" target=\"#b2\">(Cassidy et al., 2014)</ref>, RED <ref type=\"bibr\" target=\"#b26\">(O'Go. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ter generalization and avoids costly design of statistical methods leveraging hand-crafted features <ref type=\"bibr\" target=\"#b17\">(Mani et al., 2006;</ref><ref type=\"bibr\" target=\"#b4\">Chambers et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of trees, thus naturally oriented to encode hierarchical and asymmetrical structures. For instance, <ref type=\"bibr\" target=\"#b28\">Sala et al. (2018)</ref> showed that hyperbolic spaces with just two . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: answering questions <ref type=\"bibr\" target=\"#b22\">(Ning et al., 2020)</ref>, or summarizing events <ref type=\"bibr\" target=\"#b36\">(Wang et al., 2018)</ref>. West German and French authorities have (e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ugh time. This has recently led to increasing interest in research for temporal relation extraction <ref type=\"bibr\" target=\"#b3\">(Chambers et al., 2014;</ref><ref type=\"bibr\" target=\"#b37\">Wang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Euclidean space. However, several studies <ref type=\"bibr\" target=\"#b18\">(Nickel et al., 2014;</ref><ref type=\"bibr\" target=\"#b1\">Bouchard et al., 2015)</ref> have shown the inherent limitations of th asymmetric relations and tree-like graphs <ref type=\"bibr\" target=\"#b18\">(Nickel et al., 2014;</ref><ref type=\"bibr\" target=\"#b1\">Bouchard et al., 2015)</ref>. Hyperbolic spaces, instead, are promisin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k, where event pairs are encoded via a BiLSTM, enhanced with common-sense knowledge from ConceptNet <ref type=\"bibr\" target=\"#b30\">(Speer et al., 2017)</ref> and TEMPROB <ref type=\"bibr\" target=\"#b23\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s are all designed to process data representations in the Euclidean space. However, several studies <ref type=\"bibr\" target=\"#b18\">(Nickel et al., 2014;</ref><ref type=\"bibr\" target=\"#b1\">Bouchard et  mitations of the Euclidean space in terms of representing asymmetric relations and tree-like graphs <ref type=\"bibr\" target=\"#b18\">(Nickel et al., 2014;</ref><ref type=\"bibr\" target=\"#b1\">Bouchard et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: clidean spaces cannot achieve any comparable distortion even with an unbounded number of dimensions <ref type=\"bibr\" target=\"#b14\">(Linial et al., 1994)</ref>. Despite the hierarchical properties aris. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b21\">(Ning et al., 2019;</ref><ref type=\"bibr\" target=\"#b37\">Wang et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Han et al., 2019b)</ref>, which achieves better generalization and av. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ugh time. This has recently led to increasing interest in research for temporal relation extraction <ref type=\"bibr\" target=\"#b3\">(Chambers et al., 2014;</ref><ref type=\"bibr\" target=\"#b37\">Wang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k, where event pairs are encoded via a BiLSTM, enhanced with common-sense knowledge from ConceptNet <ref type=\"bibr\" target=\"#b30\">(Speer et al., 2017)</ref> and TEMPROB <ref type=\"bibr\" target=\"#b23\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hts; 2) Top-K Tuning, which only fine-tune the top-K layers of the model with other layers freezed. <ref type=\"bibr\" target=\"#b12\">Houlsby et al. (2019)</ref> uses it as a strong baseline; 3) Mixout < re also studies focusing on parameter-efficient fine-tuning, for example, the adapter-based methods <ref type=\"bibr\" target=\"#b12\">(Houlsby et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Pfeiffer e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e capacity and limited labeled data, conventional transfer learning tends to aggressive fine-tuning <ref type=\"bibr\" target=\"#b13\">(Jiang et al., 2020)</ref>, resulting in: 1) degenerated results on t e output information <ref type=\"bibr\">(Mahabadi et al., 2021)</ref> or injects noise into the input <ref type=\"bibr\" target=\"#b13\">(Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Aghajanyan et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: els (PLMs) have had a remarkable effect on the natural language processing (NLP) landscape recently <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Liu et al.,  Jiang et al., 2020)</ref>, resulting in: 1) degenerated results on the test data due to overfitting <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Phang et al. d performance due to overfitting and have poor generalization ability, especially on small datasets <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Phang et al. me issues with the construction of the WNLI dataset 5 . Therefore most studies exclude this dataset <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Dodge et al., B provides detailed experimental setups (e.g., batch size, training steps, and etc.) for BERT LARGE <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref>, XLNet LARGE <ref type=\"bibr\" target=\"#b36 ><p>Explosion of PLMs. There has been an explosion of studies on Pretrained Language Models (PLMs). <ref type=\"bibr\" target=\"#b6\">Devlin et al. (2019)</ref> propose BERT that is pretrained on large qu ch et al. (2021)</ref> point out that the omission of bias correction in the Adam optimizer used in <ref type=\"bibr\" target=\"#b6\">Devlin et al. (2019)</ref> is also responsible for the degenerated res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref> . The training epochs/steps, batch size, and warmup steps are listed in Table 7. We use AdamW <ref type=\"bibr\" target=\"#b24\">(Loshchilov and Hutter, 2019)</ref> optimizer, and set \u03b2 1 = 0.9, \u03b2 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ajanyan et al., 2021)</ref>. Moreover, <ref type=\"bibr\" target=\"#b37\">Zhang et al. (2021)</ref> and <ref type=\"bibr\" target=\"#b27\">Mosbach et al. (2021)</ref> point out that the omission of bias corre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: els (PLMs) have had a remarkable effect on the natural language processing (NLP) landscape recently <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Liu et al.,  Jiang et al., 2020)</ref>, resulting in: 1) degenerated results on the test data due to overfitting <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Phang et al. d performance due to overfitting and have poor generalization ability, especially on small datasets <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Phang et al. me issues with the construction of the WNLI dataset 5 . Therefore most studies exclude this dataset <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Dodge et al., B provides detailed experimental setups (e.g., batch size, training steps, and etc.) for BERT LARGE <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref>, XLNet LARGE <ref type=\"bibr\" target=\"#b36 ><p>Explosion of PLMs. There has been an explosion of studies on Pretrained Language Models (PLMs). <ref type=\"bibr\" target=\"#b6\">Devlin et al. (2019)</ref> propose BERT that is pretrained on large qu ch et al. (2021)</ref> point out that the omission of bias correction in the Adam optimizer used in <ref type=\"bibr\" target=\"#b6\">Devlin et al. (2019)</ref> is also responsible for the degenerated res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: In this section, we review and compare prior studies towards effective fine-tuning: 1) Weight Decay <ref type=\"bibr\" target=\"#b5\">(Daum\u00e9 III, 2007)</ref>, which adds the \u03bb w\u2212w 0 2 penalty to the loss  larize the deviation of the fine-tuned model <ref type=\"bibr\" target=\"#b19\">(Lee et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Daum\u00e9 III, 2007;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2020) ection, we simply introduce these approaches and their hyperparameters settings.</p><p>Weight Decay <ref type=\"bibr\" target=\"#b5\">Daum\u00e9 III (2007)</ref> proposes to adds a penalty item to the loss fun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for active communication. There are only very few attempts on building technical tools for Sanskrit <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>. This motivates. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: wo most popular approaches to end-to-end speech recognition are: i) attention-based encoder-decoder <ref type=\"bibr\" target=\"#b19\">[20]</ref> and ii) connectionist temporal classification (CTC) <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </p><p>In recent years there has been a stronger focus on developing ASR for low resource languages <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Different approach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d that only 1% of the languages of the world have the minimum amount of data needed to train an ASR <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Due to this, speech recognition researchers have been focus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>In recent years there has been a stronger focus on developing ASR for low resource languages <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Different approach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ess the word level performance of the ASR, WFST decoding <ref type=\"bibr\" target=\"#b31\">[32]</ref>- <ref type=\"bibr\" target=\"#b33\">[34]</ref> is used.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0 a sequence of words from the lexicon. Such a WFST is constructed by fusing 3 individual WFSTs as in <ref type=\"bibr\" target=\"#b33\">[34]</ref>.  . An example of a token FST. Input labels appear before   computation of prior. This technique has been shown to improve the performance of WFST decoding in <ref type=\"bibr\" target=\"#b33\">[34]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d that only 1% of the languages of the world have the minimum amount of data needed to train an ASR <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Due to this, speech recognition researchers have been focus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ess the word level performance of the ASR, WFST decoding <ref type=\"bibr\" target=\"#b31\">[32]</ref>- <ref type=\"bibr\" target=\"#b33\">[34]</ref> is used.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0 a sequence of words from the lexicon. Such a WFST is constructed by fusing 3 individual WFSTs as in <ref type=\"bibr\" target=\"#b33\">[34]</ref>.  . An example of a token FST. Input labels appear before   computation of prior. This technique has been shown to improve the performance of WFST decoding in <ref type=\"bibr\" target=\"#b33\">[34]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: NN system trained with feature space maximum likelihood linear regression (fMLLR) features in Kaldi <ref type=\"bibr\" target=\"#b32\">[33]</ref>, using the same training-validation split. This is a multi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stem for Sanskrit. We propose an architecture based on residual convolutional neural networks (CNN) <ref type=\"bibr\" target=\"#b21\">[22]</ref> and bidirectional gated recurrent units (GRU). As the mult. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tronger focus on developing ASR for low resource languages <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Different approaches like multilingual pre-training <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  acoustic model training we use an architecture employing CNNs and bidirectional GRUs (BiGRU) as in <ref type=\"bibr\" target=\"#b26\">[27]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>. The details of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: coder <ref type=\"bibr\" target=\"#b19\">[20]</ref> and ii) connectionist temporal classification (CTC) <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Attention-based methods have the advantage that they do n ype=\"bibr\" target=\"#b28\">[29]</ref> is used for training. The network is trained with CTC objective <ref type=\"bibr\" target=\"#b20\">[21]</ref> for a maximum of 200 epochs. Early stopping, with a patien p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>V. Decoding</head><p>We use greedy decoding <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref> to assess the c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b5\">[6]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref> and data augmentation <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b14\">[15]</ref> have been appli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Different approaches like multilingual pre-training <ref type=\"bibr\" target=\"#b5\">[6]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref> and data augmenta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>. Different approaches like multilingual pre-training <ref type=\"bibr\" target=\"#b5\">[6]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref> and data augmentation <ref type=\"bibr\" target=\"#b11\">[12]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b5\">[6]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref> and data augmentation <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b14\">[15]</ref> have been appli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b29\">Wan and Xiao, 2008a;</ref><ref type=\"bibr\" target=\"#b2\">Bougouin et al., 2013;</ref><ref type=\"bibr\" target=\"#b1\">Boudin, 2018;</ref><ref type=\"bibr\" target=\"#b0\">Bennani-Smires et al.  Caragea, 2017b) employs position information to weight the importance of phrases. MultipartiteRank <ref type=\"bibr\" target=\"#b1\">(Boudin, 2018)</ref> splits the whole graph into sub-graph and ranks t hich assigns a significance score to each topic by candidate keyphrase clustering. MultipartiteRank <ref type=\"bibr\" target=\"#b1\">(Boudin, 2018)</ref> encodes topical information within a multipartite. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r, we focus on the unsupervised keyphrase extraction (UKE) model.</p><p>UKE has been widely studied <ref type=\"bibr\" target=\"#b18\">(Mihalcea, 2004;</ref><ref type=\"bibr\" target=\"#b29\">Wan and Xiao, 20 whole document in the vector space. Though, these methods performed better than traditional methods <ref type=\"bibr\" target=\"#b18\">(Mihalcea, 2004;</ref><ref type=\"bibr\" target=\"#b29\">Wan and Xiao, 20 rt the document into a graph. Inspired by <ref type=\"bibr\" target=\"#b21\">(Page et al., 1999)</ref>, <ref type=\"bibr\" target=\"#b18\">(Mihalcea, 2004)</ref> proposed TextRank to convert keyphrase extract. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: (UKE) model.</p><p>UKE has been widely studied <ref type=\"bibr\" target=\"#b18\">(Mihalcea, 2004;</ref><ref type=\"bibr\" target=\"#b29\">Wan and Xiao, 2008a;</ref><ref type=\"bibr\" target=\"#b2\">Bougouin et a hods performed better than traditional methods <ref type=\"bibr\" target=\"#b18\">(Mihalcea, 2004;</ref><ref type=\"bibr\" target=\"#b29\">Wan and Xiao, 2008a;</ref><ref type=\"bibr\" target=\"#b2\">Bougouin et a nvert text to graph with the co-occurrence of words and employ PageRank to rank phrases. SingleRank <ref type=\"bibr\" target=\"#b29\">(Wan and Xiao, 2008a)</ref> improves the graph construction with a sl sk into the rank of nodes in graph. After this, various works focused on the expansion of TextRank. <ref type=\"bibr\" target=\"#b29\">(Wan and Xiao, 2008a</ref>) proposed SingleRank, which employs co-occ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rt or end of documents (document boundary) <ref type=\"bibr\" target=\"#b15\">(Lin and Hovy, 1997;</ref><ref type=\"bibr\" target=\"#b28\">Teufel, 1997;</ref><ref type=\"bibr\" target=\"#b5\">Dong et al., 2021)</ oundaries (the start and end of documents) <ref type=\"bibr\" target=\"#b15\">(Lin and Hovy, 1997;</ref><ref type=\"bibr\" target=\"#b28\">Teufel, 1997)</ref>.</p><p>We reflect this assumption by employing a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2018)</ref> or Doc2Vec <ref type=\"bibr\" target=\"#b13\">(Le and Mikolov, 2014)</ref>.</p><p>ELMo <ref type=\"bibr\" target=\"#b25\">(Peters et al., 2018)</ref> employs Bi-LSTM structure and concatenate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b1\">Boudin, 2018;</ref><ref type=\"bibr\" target=\"#b0\">Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., 2020)</ref> in the keyphrase extraction field. Recently,  esentation, embedding-based models <ref type=\"bibr\" target=\"#b0\">(Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., 2020)</ref> have achieved promising results and become th Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Papagiannopoulou and Tsoumakas, 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., 2020)</ref> has achieved good performance . <ref type=\"bi n with boundary function. To prevent double counting, we follow a simpler position bias weight from <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020)</ref>, which only considers where the candidate ph  Doc2Vec/Sent2Vec and measures the relevance of phrases and documents to select keyphrases. SIFRank <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020)</ref> improves EmbedRank with contextualized embed k, which ranks phrases by measuring the similarity between phrase embedding and document embedding. <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020)</ref> proposed SIFRank, which improves the static . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and candidate phrases and ignored the local information. To jointly model global and local context <ref type=\"bibr\" target=\"#b32\">(Zheng and Lapata, 2019;</ref><ref type=\"bibr\" target=\"#b14\">Liang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> proposed WordAt-tractionRank, which added distance between word embeddings into SingleRank, and <ref type=\"bibr\" target=\"#b7\">(Florescu and Caragea, 2017b</ref>) use node position weights, favorin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  set of words or phrases from a document that can represent the salient information of the document <ref type=\"bibr\" target=\"#b8\">(Hasan and Ng, 2014)</ref>. KE models can be divided into supervised a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing of natural texts for unsupervised tasks. Different from static word embedding, such as Word2Vec <ref type=\"bibr\" target=\"#b20\">(Mikolov et al., 2013)</ref>, GloVe <ref type=\"bibr\" target=\"#b24\">(P. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b2\">Bougouin et al., 2013;</ref><ref type=\"bibr\" target=\"#b1\">Boudin, 2018;</ref><ref type=\"bibr\" target=\"#b0\">Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun e ore, in recent years, embeddingbased keyphrase extraction <ref type=\"bibr\">(Wang et al., 2016;</ref><ref type=\"bibr\" target=\"#b0\">Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Papag ase extraction field. Recently, with the development of text representation, embedding-based models <ref type=\"bibr\" target=\"#b0\">(Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun  te phrases {KP 0 , KP 1 , ..., KP n } were extracted from document D. Different from previous works <ref type=\"bibr\" target=\"#b0\">(Bennani-Smires et al., 2018)</ref> which use static vector to represe , 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., 2020)</ref> has achieved good performance . <ref type=\"bibr\" target=\"#b0\">(Bennani-Smires et al., 2018)</ref> proposed EmbedRank, which ranks ph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures such as word frequency feature, position feature, linguistic features, etc. Topicbased models <ref type=\"bibr\" target=\"#b10\">(Jardine and Teufel, 2014;</ref><ref type=\"bibr\" target=\"#b17\">Liu et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  set of words or phrases from a document that can represent the salient information of the document <ref type=\"bibr\" target=\"#b8\">(Hasan and Ng, 2014)</ref>. KE models can be divided into supervised a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2018)</ref> or Doc2Vec <ref type=\"bibr\" target=\"#b13\">(Le and Mikolov, 2014)</ref>.</p><p>ELMo <ref type=\"bibr\" target=\"#b25\">(Peters et al., 2018)</ref> employs Bi-LSTM structure and concatenate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: jointly model global and local context <ref type=\"bibr\" target=\"#b32\">(Zheng and Lapata, 2019;</ref><ref type=\"bibr\" target=\"#b14\">Liang et al., 2021)</ref>, in this paper, we revisit degree centralit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fectiveness of our models. Firstly, we compare with traditional statistical methods TF-IDF and YAKE <ref type=\"bibr\" target=\"#b3\">(Campos et al., 2018)</ref>. Secondly, We compare five strong graph-ba models, graph-based models, topic-based models, and embedding-based models. Statistics-based models <ref type=\"bibr\" target=\"#b3\">(Campos et al., 2018)</ref> mainly analyze an article's probability fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly keep noun phrases (NP) that consist of zero or more adjectives followed by one or multiple nouns <ref type=\"bibr\" target=\"#b30\">(Wan and Xiao, 2008b)</ref>. (3) We use a pre-trained language model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and candidate phrases and ignored the local information. To jointly model global and local context <ref type=\"bibr\" target=\"#b32\">(Zheng and Lapata, 2019;</ref><ref type=\"bibr\" target=\"#b14\">Liang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se methods compute phrase embeddings and document embedding with static word2vec models (e.g. GloVe <ref type=\"bibr\" target=\"#b24\">(Pennington et al., 2014;</ref><ref type=\"bibr\" target=\"#b13\">Le and  word embedding, such as Word2Vec <ref type=\"bibr\" target=\"#b20\">(Mikolov et al., 2013)</ref>, GloVe <ref type=\"bibr\" target=\"#b24\">(Pennington et al., 2014), and</ref><ref type=\"bibr\">FastText (Joulin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b1\">Boudin, 2018;</ref><ref type=\"bibr\" target=\"#b0\">Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., 2020)</ref> in the keyphrase extraction field. Recently,  esentation, embedding-based models <ref type=\"bibr\" target=\"#b0\">(Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., 2020)</ref> have achieved promising results and become th Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Papagiannopoulou and Tsoumakas, 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., 2020)</ref> has achieved good performance . <ref type=\"bi n with boundary function. To prevent double counting, we follow a simpler position bias weight from <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020)</ref>, which only considers where the candidate ph  Doc2Vec/Sent2Vec and measures the relevance of phrases and documents to select keyphrases. SIFRank <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020)</ref> improves EmbedRank with contextualized embed k, which ranks phrases by measuring the similarity between phrase embedding and document embedding. <ref type=\"bibr\" target=\"#b27\">(Sun et al., 2020)</ref> proposed SIFRank, which improves the static . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he most important information typically occurs at the start or end of documents (document boundary) <ref type=\"bibr\" target=\"#b15\">(Lin and Hovy, 1997;</ref><ref type=\"bibr\" target=\"#b28\">Teufel, 1997 mption that important information typically occurs near boundaries (the start and end of documents) <ref type=\"bibr\" target=\"#b15\">(Lin and Hovy, 1997;</ref><ref type=\"bibr\" target=\"#b28\">Teufel, 1997. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b2\">Bougouin et al., 2013;</ref><ref type=\"bibr\" target=\"#b1\">Boudin, 2018;</ref><ref type=\"bibr\" target=\"#b0\">Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun e ore, in recent years, embeddingbased keyphrase extraction <ref type=\"bibr\">(Wang et al., 2016;</ref><ref type=\"bibr\" target=\"#b0\">Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Papag ase extraction field. Recently, with the development of text representation, embedding-based models <ref type=\"bibr\" target=\"#b0\">(Bennani-Smires et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun  te phrases {KP 0 , KP 1 , ..., KP n } were extracted from document D. Different from previous works <ref type=\"bibr\" target=\"#b0\">(Bennani-Smires et al., 2018)</ref> which use static vector to represe , 2018;</ref><ref type=\"bibr\" target=\"#b27\">Sun et al., 2020)</ref> has achieved good performance . <ref type=\"bibr\" target=\"#b0\">(Bennani-Smires et al., 2018)</ref> proposed EmbedRank, which ranks ph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> improves EmbedRank with contextualized embedding from a pre-trained language model. KeyGames <ref type=\"bibr\" target=\"#b26\">(Saxena et al., 2020)</ref> creatively introduces game theoretic appr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and candidate phrases and ignored the local information. To jointly model global and local context <ref type=\"bibr\" target=\"#b32\">(Zheng and Lapata, 2019;</ref><ref type=\"bibr\" target=\"#b14\">Liang et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> proposed WordAt-tractionRank, which added distance between word embeddings into SingleRank, and <ref type=\"bibr\" target=\"#b7\">(Florescu and Caragea, 2017b</ref>) use node position weights, favorin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: We evaluate our model on three public datasets: Inspec, DUC2001 and SemEval2010. The Inspec dataset <ref type=\"bibr\" target=\"#b9\">(Hulth, 2003)</ref>  We follow the common practice and evaluate the pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se methods compute phrase embeddings and document embedding with static word2vec models (e.g. GloVe <ref type=\"bibr\" target=\"#b24\">(Pennington et al., 2014;</ref><ref type=\"bibr\" target=\"#b13\">Le and  word embedding, such as Word2Vec <ref type=\"bibr\" target=\"#b20\">(Mikolov et al., 2013)</ref>, GloVe <ref type=\"bibr\" target=\"#b24\">(Pennington et al., 2014), and</ref><ref type=\"bibr\">FastText (Joulin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2018)</ref> or Doc2Vec <ref type=\"bibr\" target=\"#b13\">(Le and Mikolov, 2014)</ref>.</p><p>ELMo <ref type=\"bibr\" target=\"#b25\">(Peters et al., 2018)</ref> employs Bi-LSTM structure and concatenate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: We evaluate our model on three public datasets: Inspec, DUC2001 and SemEval2010. The Inspec dataset <ref type=\"bibr\" target=\"#b9\">(Hulth, 2003)</ref>  We follow the common practice and evaluate the pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: timate the efficiency of the model which has been shown to be ineffective and oftentimes misleading <ref type=\"bibr\" target=\"#b33\">[34]</ref> as it loses all nuances related to the dataflow of the acc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: timate the efficiency of the model which has been shown to be ineffective and oftentimes misleading <ref type=\"bibr\" target=\"#b33\">[34]</ref> as it loses all nuances related to the dataflow of the acc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lism and locality in the Machine Learning (ML) applications. The most popular examples, such as TPU <ref type=\"bibr\" target=\"#b0\">[1]</ref>, xDNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, RAPID <ref ty gy efficiency relative to existing popular architectures such as multi-core CPUs and many-core GPUs <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The main architectural features that distinguish these \"spa scribes the convolution operation using the loop nest representation. Some accelerators such as TPU <ref type=\"bibr\" target=\"#b0\">[1]</ref> use algorithmic transformations such as the im2col <ref type rators can be categorized into three groups based on their structure: rigid accelerators (e.g., TPU <ref type=\"bibr\" target=\"#b0\">[1]</ref>, NVDLA <ref type=\"bibr\" target=\"#b3\">[4]</ref>, Eyeriss), fl as two-operand MAC with certain data type. To evaluate the performance of // C4: L2 to L1 for tm4 = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> for tn4 = <ref type=\"b 1 for tm4 = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> for tn4 = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> for tk4 = the given pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te with different high-level languages or frameworks, such as Tensorflow, PyTorch for ML, and COMET <ref type=\"bibr\" target=\"#b21\">[22]</ref> for HPC. To the best of our knowledge, Union is the first   in a form that is amenable for execution of heterogeneous devices. For example, the COMET compiler <ref type=\"bibr\" target=\"#b21\">[22]</ref>, a DSL compiler for dense and sparse tensor algebra for ch ppers and cost models are shared across the various domains.</p><p>2) COMET DSL: The COMET compiler <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b37\">[38]</ref> supports the CO. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, Interstellar <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses heuristic-based search, Mind Mapping <ref type=\"bibr\" target=\"#b32\">[33]</ref> develops a surrogate model to perform gradientbased search. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r example, MAESTRO <ref type=\"bibr\" target=\"#b9\">[10]</ref> uses data-centric mapping, Interstellar <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses Halide scheduling, and Timeloop <ref type=\"bibr\" targ oop <ref type=\"bibr\" target=\"#b10\">[11]</ref> leverages sampling-based search methods, Interstellar <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses heuristic-based search, Mind Mapping <ref type=\"bibr\" rget=\"#b9\">[10]</ref> Loop-centric Timeloop <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Interstellar <ref type=\"bibr\" target=\"#b11\">[12]</ref> Union (This work)</p><p>high-level language and the target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ty of the underlying cost models.</p><p>We choose tensor contractions from the TCCG benchmark suite <ref type=\"bibr\" target=\"#b39\">[40]</ref>, using the reference problem sizes. The input sets are tak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r example, MAESTRO <ref type=\"bibr\" target=\"#b9\">[10]</ref> uses data-centric mapping, Interstellar <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses Halide scheduling, and Timeloop <ref type=\"bibr\" targ oop <ref type=\"bibr\" target=\"#b10\">[11]</ref> leverages sampling-based search methods, Interstellar <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses heuristic-based search, Mind Mapping <ref type=\"bibr\" rget=\"#b9\">[10]</ref> Loop-centric Timeloop <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Interstellar <ref type=\"bibr\" target=\"#b11\">[12]</ref> Union (This work)</p><p>high-level language and the target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lism and locality in the Machine Learning (ML) applications. The most popular examples, such as TPU <ref type=\"bibr\" target=\"#b0\">[1]</ref>, xDNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, RAPID <ref ty gy efficiency relative to existing popular architectures such as multi-core CPUs and many-core GPUs <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The main architectural features that distinguish these \"spa scribes the convolution operation using the loop nest representation. Some accelerators such as TPU <ref type=\"bibr\" target=\"#b0\">[1]</ref> use algorithmic transformations such as the im2col <ref type rators can be categorized into three groups based on their structure: rigid accelerators (e.g., TPU <ref type=\"bibr\" target=\"#b0\">[1]</ref>, NVDLA <ref type=\"bibr\" target=\"#b3\">[4]</ref>, Eyeriss), fl as two-operand MAC with certain data type. To evaluate the performance of // C4: L2 to L1 for tm4 = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> for tn4 = <ref type=\"b 1 for tm4 = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> for tn4 = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> for tk4 = the given pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ergy in vitro.</p><p>Recently, deep learning approaches have demonstrated success in drug discovery <ref type=\"bibr\" target=\"#b37\">(38)</ref>. A common approach is to train a deep neural network to pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: roperty prediction <ref type=\"bibr\" target=\"#b16\">(17,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b38\">39)</ref>. Most of these models learn molecular representations based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: herapies have been shown to be more effec- tive than single drugs for multiple diseases such as HIV <ref type=\"bibr\" target=\"#b0\">(1)</ref> and infections caused by Mycobacterium tuberculosis <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b24\">(25)</ref> and inhibition against Spike\u2212ACE2 protein\u2212protein interaction <ref type=\"bibr\" target=\"#b25\">(26)</ref>. Among the 332 human proteins, we selected 31 targets base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: otensinconverting enzyme 2 (ACE2), and 31 host targets that physically interact with viral proteins <ref type=\"bibr\" target=\"#b17\">(18)</ref>. The GCN learns to predict the most likely targets, using   included in ComboNet are only a subset of the 332 targets that physically interact with SARS-CoV-2 <ref type=\"bibr\" target=\"#b17\">(18)</ref>. Other targets were excluded because they lack available D s depends on ACE2 and TMPRSS2 <ref type=\"bibr\" target=\"#b22\">(23)</ref>. Furthermore, Gordon et al. <ref type=\"bibr\" target=\"#b17\">(18)</ref> identified 332 human proteins that physically interact wit can utilize existing HIV drug combination data to improve the model performance. Indeed, prior work <ref type=\"bibr\" target=\"#b17\">(18)</ref> has shown significant interactome similarity between HIV a nd CD4 (CHEMBL2754).</p><p>Data Curation. The SARS-CoV-2 DTI data are downloaded from Gordon et al. <ref type=\"bibr\" target=\"#b17\">(18)</ref>. The original DTI data are turned into a binary classifica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . <ref type=\"bibr\" target=\"#b9\">(10)</ref> trained a deep neural network on a large oncology screen <ref type=\"bibr\" target=\"#b12\">(13)</ref> and demonstrated the advantage of deep learning over stand ith measured synergy scores. While such data are readily available for some diseases such as cancer <ref type=\"bibr\" target=\"#b12\">(13)</ref> (more than 20,000 combinations), the amount of SARS-CoV-2 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct (CPE) assay data <ref type=\"bibr\" target=\"#b20\">(21)</ref> and available drug combination assays <ref type=\"bibr\" target=\"#b21\">(22)</ref>. In short, ComboNet predicts drug combination synergy by m \"bibr\" target=\"#b4\">(5)</ref>. The training set contains 88 SARS-CoV-2 drug combinations from NCATS <ref type=\"bibr\" target=\"#b21\">(22)</ref> as well as the DTI and single-agent antiviral activity dat m three data sources. NCATS performed two combination assays <ref type=\"bibr\" target=\"#b4\">(5,</ref><ref type=\"bibr\" target=\"#b21\">22)</ref> in VeroE6 cells, which contained 160 two-drug combinations . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: otensinconverting enzyme 2 (ACE2), and 31 host targets that physically interact with viral proteins <ref type=\"bibr\" target=\"#b17\">(18)</ref>. The GCN learns to predict the most likely targets, using   included in ComboNet are only a subset of the 332 targets that physically interact with SARS-CoV-2 <ref type=\"bibr\" target=\"#b17\">(18)</ref>. Other targets were excluded because they lack available D s depends on ACE2 and TMPRSS2 <ref type=\"bibr\" target=\"#b22\">(23)</ref>. Furthermore, Gordon et al. <ref type=\"bibr\" target=\"#b17\">(18)</ref> identified 332 human proteins that physically interact wit can utilize existing HIV drug combination data to improve the model performance. Indeed, prior work <ref type=\"bibr\" target=\"#b17\">(18)</ref> has shown significant interactome similarity between HIV a nd CD4 (CHEMBL2754).</p><p>Data Curation. The SARS-CoV-2 DTI data are downloaded from Gordon et al. <ref type=\"bibr\" target=\"#b17\">(18)</ref>. The original DTI data are turned into a binary classifica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: On the other hand, while traditional cheminformatics tools have modeled DTI for property prediction <ref type=\"bibr\" target=\"#b5\">(6,</ref><ref type=\"bibr\" target=\"#b39\">40)</ref>, most of these metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as HIV <ref type=\"bibr\" target=\"#b0\">(1)</ref> and infections caused by Mycobacterium tuberculosis <ref type=\"bibr\" target=\"#b1\">(2)</ref>. Synergistic combinations can improve both therapeutic poten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: performance on abstractive summarization <ref type=\"bibr\" target=\"#b28\">(Liu and Lapata, 2019;</ref><ref type=\"bibr\" target=\"#b24\">Lewis et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Zhang et al., our new annotation study on errors made by state-of-the-art summarizers-models fine-tuned from BART <ref type=\"bibr\" target=\"#b24\">(Lewis et al., 2020)</ref> and PEGA-SUS <ref type=\"bibr\" target=\"#b46 ence of faithful summaries over summaries with factual errors. It is then used for fine-tuning BART <ref type=\"bibr\" target=\"#b24\">(Lewis et al., 2020)</ref> and PEGA-SUS <ref type=\"bibr\" target=\"#b46 B memory and the A100 GPU with 40GB memory.</p><p>Training Settings. For hyperparameters, we follow <ref type=\"bibr\" target=\"#b24\">Lewis et al. (2020)</ref>  </p></div> <div xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b28\">(Liu and Lapata, 2019;</ref><ref type=\"bibr\" target=\"#b24\">Lewis et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Zhang et al., 2020a)</ref> with impeccable fluency, yet their summari -models fine-tuned from BART <ref type=\"bibr\" target=\"#b24\">(Lewis et al., 2020)</ref> and PEGA-SUS <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2020a)</ref>-on two benchmarks: XSum <ref type=\"bibr\"  en used for fine-tuning BART <ref type=\"bibr\" target=\"#b24\">(Lewis et al., 2020)</ref> and PEGA-SUS <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2020a)</ref> for training summarization models. Formal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  (CL) has obtained impressive results on many visual processing tasks, such as image classification <ref type=\"bibr\" target=\"#b20\">(Khosla et al., 2020;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al.,  and another set of erroneous summaries N (negative samples). The contrastive learning objective is <ref type=\"bibr\" target=\"#b20\">(Khosla et al., 2020;</ref><ref type=\"bibr\">Gunel et al., 2021)</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Three types of remedies have been proposed: running a separately learned error correction component <ref type=\"bibr\" target=\"#b5\">(Dong et al., 2020)</ref>, removing noisy training samples <ref type=\" e=\"bibr\" target=\"#b3\">Chen et al., 2021)</ref>, including replacing entities absent from the source <ref type=\"bibr\" target=\"#b5\">(Dong et al., 2020)</ref> or revising all possible errors <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  generates the full name. Comparisons using cross-entropy loss, beam reranking by entailment scores <ref type=\"bibr\" target=\"#b22\">(Kryscinski et al., 2020)</ref>, and unlikelihood objective <ref type br\" target=\"#b6\">Fabbri et al., 2021)</ref>. Entailment-based scorers are designed at summary level <ref type=\"bibr\" target=\"#b22\">(Kryscinski et al., 2020)</ref> and finer-grained dependency relation has shown high correlation with human judged consistency and relevance.</p><p>We further use FactCC <ref type=\"bibr\" target=\"#b22\">(Kryscinski et al., 2020)</ref>, trained based on their negative samp he RSPCA. ple is constructed for each entity in the reference. Though this idea has been studied by <ref type=\"bibr\" target=\"#b22\">Kryscinski et al. (2020)</ref>, they allow entities of different type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b13\">Goyal and Durrett, 2021)</ref>, and modifying the Transformer architecture <ref type=\"bibr\" target=\"#b19\">(Huang et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Zhu et al.,  and the last token of the decoded summary. Entities and other parsing results are obtained by spaCy <ref type=\"bibr\" target=\"#b19\">(Honnibal et al., 2020)</ref>. We further consider adding a multi-lay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b13\">Goyal and Durrett, 2021)</ref>, and modifying the Transformer architecture <ref type=\"bibr\" target=\"#b19\">(Huang et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Zhu et al.,  and the last token of the decoded summary. Entities and other parsing results are obtained by spaCy <ref type=\"bibr\" target=\"#b19\">(Honnibal et al., 2020)</ref>. We further consider adding a multi-lay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b31\">(Maynez et al., 2020;</ref><ref type=\"bibr\" target=\"#b47\">Zhang et al., 2020b;</ref><ref type=\"bibr\" target=\"#b12\">Goyal and Durrett, 2020)</ref>, even for stateof-the-art models. Thre pe=\"bibr\" target=\"#b22\">(Kryscinski et al., 2020)</ref> and finer-grained dependency relation level <ref type=\"bibr\" target=\"#b12\">(Goyal and Durrett, 2020)</ref>. QA models are employed to measure co tegies both focus on incorrect named entities. To cover more diverse extrinsic and intrinsic errors <ref type=\"bibr\" target=\"#b12\">(Goyal and Durrett, 2020)</ref>, we extend MASKENT to contain relatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: masked language models <ref type=\"bibr\" target=\"#b21\">(Kobayashi, 2018)</ref>, and back-translation <ref type=\"bibr\" target=\"#b30\">(Mallinson et al., 2017)</ref>. We find back-translation to be best a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b13\">Goyal and Durrett, 2021)</ref>, and modifying the Transformer architecture <ref type=\"bibr\" target=\"#b19\">(Huang et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Zhu et al.,  and the last token of the decoded summary. Entities and other parsing results are obtained by spaCy <ref type=\"bibr\" target=\"#b19\">(Honnibal et al., 2020)</ref>. We further consider adding a multi-lay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Large pre-trained Transformers have yielded remarkable performance on abstractive summarization <ref type=\"bibr\" target=\"#b28\">(Liu and Lapata, 2019;</ref><ref type=\"bibr\" target=\"#b24\">Lewis et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Large pre-trained Transformers have yielded remarkable performance on abstractive summarization <ref type=\"bibr\" target=\"#b28\">(Liu and Lapata, 2019;</ref><ref type=\"bibr\" target=\"#b24\">Lewis et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b31\">(Maynez et al., 2020;</ref><ref type=\"bibr\" target=\"#b47\">Zhang et al., 2020b;</ref><ref type=\"bibr\" target=\"#b12\">Goyal and Durrett, 2020)</ref>, even for stateof-the-art models. Thre pe=\"bibr\" target=\"#b22\">(Kryscinski et al., 2020)</ref> and finer-grained dependency relation level <ref type=\"bibr\" target=\"#b12\">(Goyal and Durrett, 2020)</ref>. QA models are employed to measure co tegies both focus on incorrect named entities. To cover more diverse extrinsic and intrinsic errors <ref type=\"bibr\" target=\"#b12\">(Goyal and Durrett, 2020)</ref>, we extend MASKENT to contain relatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  NLP. CL has been a popular method for representation learning, especially for vision understanding <ref type=\"bibr\" target=\"#b17\">(Hjelm et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eferences, there is no guarantee for the model to distinguish references from incorrect generations <ref type=\"bibr\" target=\"#b18\">(Holtzman et al., 2020;</ref><ref type=\"bibr\" target=\"#b42\">Welleck e t part of the decoder output, and finally decodes the rest of the content based on nucleus sampling <ref type=\"bibr\" target=\"#b18\">(Holtzman et al., 2020)</ref> with a cumulative probability threshold. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t scores <ref type=\"bibr\" target=\"#b22\">(Kryscinski et al., 2020)</ref>, and unlikelihood objective <ref type=\"bibr\" target=\"#b42\">(Welleck et al., 2020)</ref> over negative samples all produce unfait urce. Moreover, compared with unlikelihood training method that penalizes the same negative samples <ref type=\"bibr\" target=\"#b42\">(Welleck et al., 2020)</ref>, our summaries also obtain consistently  h references from incorrect generations <ref type=\"bibr\" target=\"#b18\">(Holtzman et al., 2020;</ref><ref type=\"bibr\" target=\"#b42\">Welleck et al., 2020)</ref>. Therefore, potential solutions reside in ve tokens (e.g., repeated words) and sentences (e.g., contradictory responses in a dialogue system) <ref type=\"bibr\" target=\"#b42\">(Welleck et al., 2020;</ref><ref type=\"bibr\" target=\"#b26\">Li et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ple positive samples, in our pilot study, we experiment with paraphrasing with synonym substitution <ref type=\"bibr\" target=\"#b38\">(Ren et al., 2019)</ref>, randomly replacing words based on the predi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ple positive samples, in our pilot study, we experiment with paraphrasing with synonym substitution <ref type=\"bibr\" target=\"#b38\">(Ren et al., 2019)</ref>, randomly replacing words based on the predi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hmarks: XSum <ref type=\"bibr\" target=\"#b33\">(Narayan et al., 2018)</ref> and CNN/DailyMail (CNN/DM) <ref type=\"bibr\" target=\"#b16\">(Hermann et al., 2015)</ref>.</p><p>We fine-tune pre-trained large mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b13\">Goyal and Durrett, 2021)</ref>, and modifying the Transformer architecture <ref type=\"bibr\" target=\"#b19\">(Huang et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Zhu et al.,  and the last token of the decoded summary. Entities and other parsing results are obtained by spaCy <ref type=\"bibr\" target=\"#b19\">(Honnibal et al., 2020)</ref>. We further consider adding a multi-lay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e language tasks. First, a suitable training objective is critical to avoid performance degradation <ref type=\"bibr\" target=\"#b39\">(Saunshi et al., 2019)</ref>. Second, it is nontrivial to construct \". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2020)</ref>, removing noisy training samples <ref type=\"bibr\" target=\"#b32\">(Nan et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Goyal and Durrett, 2021)</ref>, and modifying the Transformer archite om model training has also been investigated <ref type=\"bibr\" target=\"#b32\">(Nan et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Goyal and Durrett, 2021)</ref>, however, it often leads to degraded s amples that mimic the diverse errors made by state-of-the-art systems that vary in words and syntax <ref type=\"bibr\" target=\"#b13\">(Goyal and Durrett, 2021)</ref>.</p><p>To address both challenges, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> and PEGA-SUS <ref type=\"bibr\" target=\"#b46\">(Zhang et al., 2020a)</ref>-on two benchmarks: XSum <ref type=\"bibr\" target=\"#b33\">(Narayan et al., 2018)</ref> and CNN/DailyMail (CNN/DM) <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b31\">(Maynez et al., 2020;</ref><ref type=\"bibr\" target=\"#b47\">Zhang et al., 2020b;</ref><ref type=\"bibr\" target=\"#b12\">Goyal and Durrett, 2020)</ref>, even for stateof-the-art models. Thre pe=\"bibr\" target=\"#b22\">(Kryscinski et al., 2020)</ref> and finer-grained dependency relation level <ref type=\"bibr\" target=\"#b12\">(Goyal and Durrett, 2020)</ref>. QA models are employed to measure co tegies both focus on incorrect named entities. To cover more diverse extrinsic and intrinsic errors <ref type=\"bibr\" target=\"#b12\">(Goyal and Durrett, 2020)</ref>, we extend MASKENT to contain relatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0a)</ref> with impeccable fluency, yet their summaries often contain factually inconsistent content <ref type=\"bibr\" target=\"#b31\">(Maynez et al., 2020;</ref><ref type=\"bibr\" target=\"#b47\">Zhang et al ount of world knowledge which should be evaluated differently instead of as extrinsic hallucination <ref type=\"bibr\" target=\"#b31\">(Maynez et al., 2020)</ref>. We also show that world knowledge can po f data, thus contains less errors than BART and other older models studied for error annotations by <ref type=\"bibr\" target=\"#b31\">Maynez et al. (2020)</ref>. This observation also highlights the usag. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h human-rated factual consistency levels <ref type=\"bibr\" target=\"#b10\">(Gabriel et al., 2021;</ref><ref type=\"bibr\" target=\"#b6\">Fabbri et al., 2021)</ref>. Entailment-based scorers are designed at s swer questions generated from the summaries <ref type=\"bibr\" target=\"#b41\">(Wang et al., 2020;</ref><ref type=\"bibr\" target=\"#b6\">Durmus et al., 2020)</ref>, or considering the summaries for addressin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hmarks: XSum <ref type=\"bibr\" target=\"#b33\">(Narayan et al., 2018)</ref> and CNN/DailyMail (CNN/DM) <ref type=\"bibr\" target=\"#b16\">(Hermann et al., 2015)</ref>.</p><p>We fine-tune pre-trained large mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on model is learned to fix errors made by the summarizers <ref type=\"bibr\">(Zhao et al., 2020;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2021)</ref>, including replacing entities absent from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: The key idea of using RL in prefetching has been previously explored by the context prefetcher (CP) <ref type=\"bibr\" target=\"#b103\">[104]</ref>. Pythia significantly differs from it both in terms of ( Context Prefetcher</head><p>As we discuss in Section 4.5, unlike Pythia, the context prefetcher (CP <ref type=\"bibr\" target=\"#b103\">[104]</ref>) relies on both hardware and software contexts. A tailor e=\"bibr\" target=\"#b88\">[89]</ref> and hardware prefetching <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b103\">104,</ref><ref type=\"bibr\" target=\"#b104\">105,</ref><ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etworks <ref type=\"bibr\" target=\"#b104\">[105]</ref>, LSTMs <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b113\">114]</ref>, and Graph Neural Networks (GNNs) <ref type=\"bibr\" target ional processors <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b104\">105,</ref><ref type=\"bibr\" target=\"#b113\">114,</ref><ref type=\"bibr\" target=\"#b115\">116]</ref>, making them im #b60\">[61,</ref><ref type=\"bibr\" target=\"#b103\">104,</ref><ref type=\"bibr\" target=\"#b104\">105,</ref><ref type=\"bibr\" target=\"#b113\">[114]</ref><ref type=\"bibr\" target=\"#b114\">[115]</ref><ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 103\">104,</ref><ref type=\"bibr\" target=\"#b104\">105,</ref><ref type=\"bibr\" target=\"#b113\">[114]</ref><ref type=\"bibr\" target=\"#b114\">[115]</ref><ref type=\"bibr\" target=\"#b115\">[116]</ref><ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n Atari <ref type=\"bibr\" target=\"#b91\">[92]</ref> and Go <ref type=\"bibr\" target=\"#b117\">[118,</ref><ref type=\"bibr\" target=\"#b118\">119]</ref>. We argue that the RL framework is an inherent fit to mod plications of RL <ref type=\"bibr\" target=\"#b91\">[92,</ref><ref type=\"bibr\" target=\"#b117\">118,</ref><ref type=\"bibr\" target=\"#b118\">119]</ref>, which use deep neural networks to approximately store Q-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b86\">87,</ref><ref type=\"bibr\" target=\"#b109\">110,</ref><ref type=\"bibr\" target=\"#b112\">113,</ref><ref type=\"bibr\" target=\"#b126\">127]</ref>, branch prediction <ref type=\"bibr\">[57, 67-70, 125, 126,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  stride prefetcher at L1-cache and streamer at L2 <ref type=\"bibr\" target=\"#b8\">[9]</ref>) and IPCP <ref type=\"bibr\" target=\"#b102\">[103]</ref> in \u00a76.2.4. For fair comparison, we add a simple PC-based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or various microarchitectural tasks like memory scheduling <ref type=\"bibr\" target=\"#b63\">[64,</ref><ref type=\"bibr\" target=\"#b93\">94]</ref>, cache management <ref type=\"bibr\" target=\"#b27\">[28,</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b95\">[96]</ref><ref type=\"bibr\" target=\"#b96\">[97]</ref><ref type=\"bibr\" target=\"#b97\">[98]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b86\">87,</ref><ref type=\"bibr\" target=\"#b109\">110,</ref><ref type=\"bibr\" target=\"#b112\">113,</ref><ref type=\"bibr\" target=\"#b126\">127]</ref>, branch prediction <ref type=\"bibr\">[57, 67-70, 125, 126,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion. Moreover, inspired by the widely used contrastive loss <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> in computer vision, we propose a cosine contrastive loss (C ple and largely inspired by the widely used contrastive loss <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> in the computer vision tasks, such as face recognition and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b10\">11]</ref>, auto-encoders <ref type=\"bibr\" target=\"#b15\">[16]</ref>, attention networks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, gra dies (e.g., average pooling in YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, attention in ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>). We build Simplex as a unified model that integrates matrix deas from several successful models such as YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and PinSage <ref type=\"bibr\" target=\"#b38\">[39]</ref>.</p>< ective in many existing studies, such as YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref> and ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The key part of Sim-pleX lies in its aggregation layer for  are learnable parameters. Note that similar attention mechanisms can be found in some existing work <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. However, after beha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lized recommendation is ubiquitous in various applications, such as video recommendation in YouTube <ref type=\"bibr\" target=\"#b4\">[5]</ref>, product recommendation in Amazon <ref type=\"bibr\" target=\"# f>, binary cross-entropy loss <ref type=\"bibr\" target=\"#b10\">[11]</ref>, softmax cross-entropy loss <ref type=\"bibr\" target=\"#b4\">[5]</ref>, pairwise hinge loss <ref type=\"bibr\" target=\"#b11\">[12]</re plicity in mind and borrow ideas from several existing studies (e.g., average pooling in YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, attention in ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>).   model-based CF approaches, MLPs-based NeuMF <ref type=\"bibr\" target=\"#b10\">[11]</ref> and YouTubet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, memory network-based CMN <ref type=\"bibr\" target=\"#b6\">[7]< \">[22]</ref>, binary cross-entropy <ref type=\"bibr\" target=\"#b10\">[11]</ref>, softmax cross-entropy <ref type=\"bibr\" target=\"#b4\">[5]</ref>, pairwise hinge loss <ref type=\"bibr\" target=\"#b11\">[12]</re pleX, we keep simplicity in mind and borrow ideas from several successful models such as YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and PinSage <r del user behaviors. This also has been shown effective in many existing studies, such as YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref> and ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The key par ghtforward way to aggregate the interacted items, which has been successfully applied in YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>. But it treats each item equally and fails to account for th pe=\"bibr\" target=\"#b20\">[21]</ref>,</p><p>CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>, YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, CMN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and NBPO <ref  </ref>. \u2022 Softmax cross-entropy (SCE) loss is widely used for multiclass classification. YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref> cast item prediction as a multi-class classification task th h line of work that applies various neural networks to CF, including multi-layer perceptrons (MLPs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, auto-encoders <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: gative sampling ratio on each loss function. Moreover, inspired by the widely used contrastive loss <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> in computer vision,   Choices. The formulation of CCL is simple and largely inspired by the widely used contrastive loss <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> in the computer visi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: an be effectively captured through multi-layers message passing. Ying et al.</p><p>proposed PinSage <ref type=\"bibr\" target=\"#b38\">[39]</ref> that improved GraphSage <ref type=\"bibr\" target=\"#b8\">[9]< t <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ACF <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and PinSage <ref type=\"bibr\" target=\"#b38\">[39]</ref>.</p><p>Figure <ref type=\"figure\" target=\"#fig_0\">1</ref> i ef>; \u2022 Fourteen GNN-based methods, including GC-MC <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Pinsage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b15\">[16]</ref>, attention networks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b9. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and so on.</p><p>(4) Others. We put methods that do not f <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, respectively. Specifically, we compare SimpleX with the c <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>; \u2022 Six methods of other types, including ItemPop, SLIM <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and RecVAE <ref type=\"bibr\" target=\"#b22\">[23]</ref>; \u2022 Fourteen GNN-based methods, including GC-MC <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Pinsage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  autoencoder-based CF models, such as Mult-VAE <ref type=\"bibr\" target=\"#b15\">[16]</ref> and RecVAE <ref type=\"bibr\" target=\"#b22\">[23]</ref>. We follow the strong generalization setting, which split  b15\">[16]</ref>,</p><p>MacridVAE <ref type=\"bibr\" target=\"#b17\">[18]</ref>, EASE R [26], and RecVAE <ref type=\"bibr\" target=\"#b22\">[23]</ref>; \u2022 Fourteen GNN-based methods, including GC-MC <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -MC <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Pinsage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r to introduce graph information. Yang et al. devised a unified and efficient method called HOP-Rec <ref type=\"bibr\" target=\"#b36\">[37]</ref> that incorporated both MF and graph-based models for impli  <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GRMF <ref type=\"bibr\" target=\"#b18\">[19]</ref>, HOP-Rec <ref type=\"bibr\" target=\"#b36\">[37]</ref>, NeuMF <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and ENMF. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and RecVAE <ref type=\"bibr\" target=\"#b22\">[23]</ref>; \u2022 Fourteen GNN-based methods, including GC-MC <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Pinsage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [11]</ref>, softmax cross-entropy loss <ref type=\"bibr\" target=\"#b4\">[5]</ref>, pairwise hinge loss <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and mean square error loss <ref type=\"bibr\" target=\"#b1\"> </ref>, memory network-based CMN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, metric learning-based CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and NBPO <ref type=\"bibr\" target=\"#b40\">[41]</ref> that l b10\">[11]</ref>, softmax cross-entropy <ref type=\"bibr\" target=\"#b4\">[5]</ref>, pairwise hinge loss <ref type=\"bibr\" target=\"#b11\">[12]</ref>, etc. However, there is still a lack of a systematic compa ct (e.g., in LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>) or Euclidean distance (e.g., in CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>) to measure the similarity (or distance) between a user-it ethods of other types, including ItemPop, SLIM <ref type=\"bibr\" target=\"#b20\">[21]</ref>,</p><p>CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>, YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, CMN < f>]. \u2022 Pairwise hinge loss (PHL), is also known as max-margin objective, which has been used in CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>. PHL forces the distance of a negative user-item pair to b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b15\">[16]</ref>, attention networks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b9. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and so on.</p><p>(4) Others. We put methods that do not f <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, respectively. Specifically, we compare SimpleX with the c <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>; \u2022 Six methods of other types, including ItemPop, SLIM <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -MC <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Pinsage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: niform random sampling for recommendation, including mining informative negative samples (e.g., RNS <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and NBPO <ref type=\"bibr\" target=\"#b40\">[41]</ref>), tackli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  loss functions. Furthermore, many recent GNN-based studies <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar  successful applications of GNN in recommendation further inspire many good studies, including BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref> which models the uncertainty in the user-item graph with b , Amazon-Beauty, are adopted by the work NIA-GCN <ref type=\"bibr\" target=\"#b29\">[30]</ref> and BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The other three, Amazon-Electronics, CiteUlike-A, and Mov c <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and LCFN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GAT4Rec <ref type=\"bibr\" target=\"#b24\">[25]</ref> that learn neighborhood relationships, and SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, LCFN <ref ef type=\"bibr\" target=\"#b32\">[33]</ref>, NGAT4Rec <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DHCF <ref rating the high effectiveness of SimpleX. Besides, note that we do not report the results of SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref> and NGAT4Rec <ref type=\"bibr\" target=\"#b24\">[25]</ref> on  get=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. We perform most of our experiments on them and further mak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -MC <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Pinsage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> experiment with the arget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar ion for CF research <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33</ref>]. \u2022 Pairwise hinge l target=\"#b8\">[9]</ref> to model the item-item relationships for Pinterest. Wang et al. devised NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref> that explicitly encoded the collaborative signals as high- sage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LR-GC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  its popularity in CF tasks. This is also similar to the calculation of word similarity in Word2Vec <ref type=\"bibr\" target=\"#b19\">[20]</ref>, where cosine similarity is usually used.</p><p>Second, wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g CF models of different types:</p><p>\u2022 Five methods based on MF and its variants, including MF-BPR <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GRMF <ref type=\"bibr\" target=\"#b18\">[19]</ref>, HOP-Rec <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion. Due to its effectiveness, MF has been wildly studied in CF. Manotumruksa et al. proposed GRMF <ref type=\"bibr\" target=\"#b18\">[19]</ref> that smoothed MF through adding the graph Laplacian regula hods based on MF and its variants, including MF-BPR <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GRMF <ref type=\"bibr\" target=\"#b18\">[19]</ref>, HOP-Rec <ref type=\"bibr\" target=\"#b36\">[37]</ref>, NeuMF . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> experiment with the BPR loss <ref type=\"bibr\" target=\"#b21\" /ref> which models the uncertainty in the user-item graph with bayesian graph neural networks, DGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref> which models a distribution over intents for each user-ite  <ref type=\"bibr\" target=\"#b29\">[30]</ref>, LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>, DGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, NGAT4Rec <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGL-E rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. We perform most of get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33</ref>]. \u2022 Pairwise hinge loss (PHL), is also known as max-margin o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ith an item and thus help users discover potential items of interests. Collaborative filtering (CF) <ref type=\"bibr\" target=\"#b26\">[27]</ref> is a fundamental task in recommendation that leverages the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5]</ref>, pairwise hinge loss <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and mean square error loss <ref type=\"bibr\" target=\"#b1\">[2]</ref>, there is still a lack of systematic evaluation and comparis ]</ref> that incorporated both MF and graph-based models for implicit CF. Chen et al. designed ENMF <ref type=\"bibr\" target=\"#b1\">[2]</ref>, which is an efficient MF-based CF model with modified MSE l ref type=\"bibr\" target=\"#b36\">[37]</ref>, NeuMF <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and ENMF <ref type=\"bibr\" target=\"#b1\">[2]</ref>; \u2022 Four autoencoder-based methods, including Mult-VAE <ref t r (MSE) has been widely used for CF, such as WMF <ref type=\"bibr\" target=\"#b12\">[13]</ref> and ENMF <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Table <ref type=\"table\" target=\"#tab_0\">1</ref> shows the r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b2\">[3]</ref>, transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and so on. Nevertheless, these models tend to become more  icated state-of-the-art methods by a large margin (up to 48.5% improvement in NDCG@20 over LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> on Amazon-Books). We also empirically compare the performan ency of SimpleX, which shows more than 10x speedup over the simplified GNN-based CF model, LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>. We hope that our work could not only serve as a simple and gnals as high-order connectivities by performing embedding propagation. He et al. proposed LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which removed the feature transformation and non-linear ac CF and greatly facilitate model training. First, instead of applying dot product (e.g., in LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>) or Euclidean distance (e.g., in CML <ref type=\"bibr\" targe ref type=\"bibr\" target=\"#b3\">[4]</ref>, NIA-GCN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>, DGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, NGAT4Rec < aluation and comparisons among different loss functions. Furthermore, many recent GNN-based studies <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar  of each negative user-item pair. It is one of the most commonly used loss function for CF research <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar walla, which are commonly used in recent GNN-based CF models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> experiment with the BPR loss <ref type=\"bibr\" target=\"#b21\" /ref> which models the uncertainty in the user-item graph with bayesian graph neural networks, DGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref> which models a distribution over intents for each user-ite  <ref type=\"bibr\" target=\"#b29\">[30]</ref>, LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>, DGCF <ref type=\"bibr\" target=\"#b32\">[33]</ref>, NGAT4Rec <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGL-E rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. We perform most of get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33</ref>]. \u2022 Pairwise hinge loss (PHL), is also known as max-margin o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  its popularity in CF tasks. This is also similar to the calculation of word similarity in Word2Vec <ref type=\"bibr\" target=\"#b19\">[20]</ref>, where cosine similarity is usually used.</p><p>Second, wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: recommendation in YouTube <ref type=\"bibr\" target=\"#b4\">[5]</ref>, product recommendation in Amazon <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and news recommendation in Bing <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  three categories into this \"Others\" category. Here we list some representative models such as SLIM <ref type=\"bibr\" target=\"#b20\">[21]</ref> which is a simple linear model that combines the advantage FN <ref type=\"bibr\" target=\"#b39\">[40]</ref>; \u2022 Six methods of other types, including ItemPop, SLIM <ref type=\"bibr\" target=\"#b20\">[21]</ref>,</p><p>CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>, YouT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  loss functions. Furthermore, many recent GNN-based studies <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar  successful applications of GNN in recommendation further inspire many good studies, including BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref> which models the uncertainty in the user-item graph with b , Amazon-Beauty, are adopted by the work NIA-GCN <ref type=\"bibr\" target=\"#b29\">[30]</ref> and BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The other three, Amazon-Electronics, CiteUlike-A, and Mov c <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and LCFN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b40\">[41]</ref>), tackling the selection bias of implicit user feedback (e.g., MSN <ref type=\"bibr\" target=\"#b35\">[36]</ref>) and so on. In this work, we mainly investigate the influe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g CF models of different types:</p><p>\u2022 Five methods based on MF and its variants, including MF-BPR <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GRMF <ref type=\"bibr\" target=\"#b18\">[19]</ref>, HOP-Rec <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref>, NIA-GCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> experiment with the arget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar ion for CF research <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33</ref>]. \u2022 Pairwise hinge l target=\"#b8\">[9]</ref> to model the item-item relationships for Pinterest. Wang et al. devised NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref> that explicitly encoded the collaborative signals as high- sage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LR-GC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion. Due to its effectiveness, MF has been wildly studied in CF. Manotumruksa et al. proposed GRMF <ref type=\"bibr\" target=\"#b18\">[19]</ref> that smoothed MF through adding the graph Laplacian regula hods based on MF and its variants, including MF-BPR <ref type=\"bibr\" target=\"#b14\">[15]</ref>, GRMF <ref type=\"bibr\" target=\"#b18\">[19]</ref>, HOP-Rec <ref type=\"bibr\" target=\"#b36\">[37]</ref>, NeuMF . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref>, NIA-GCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref>, NIA-GCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GAT4Rec <ref type=\"bibr\" target=\"#b24\">[25]</ref> that learn neighborhood relationships, and SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, LCFN <ref ef type=\"bibr\" target=\"#b32\">[33]</ref>, NGAT4Rec <ref type=\"bibr\" target=\"#b24\">[25]</ref>, SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DHCF <ref rating the high effectiveness of SimpleX. Besides, note that we do not report the results of SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref> and NGAT4Rec <ref type=\"bibr\" target=\"#b24\">[25]</ref> on  get=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. We perform most of our experiments on them and further mak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: datasets Amazon-Books, Yelp2018, and Gowalla, which are commonly used in recent GNN-based CF models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe f type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref>, NIA-GCN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, LightGCN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b10\">[11]</ref> and YouTubet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, memory network-based CMN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, metric learning-based CML <ref type=\"bibr\" target=\"#b11\">[1  <ref type=\"bibr\" target=\"#b11\">[12]</ref>, YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, CMN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and NBPO <ref type=\"bibr\" target=\"#b40\">[41]</ref>.</p></di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing et al.</p><p>proposed PinSage <ref type=\"bibr\" target=\"#b38\">[39]</ref> that improved GraphSage <ref type=\"bibr\" target=\"#b8\">[9]</ref> to model the item-item relationships for Pinterest. Wang et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [11]</ref>, softmax cross-entropy loss <ref type=\"bibr\" target=\"#b4\">[5]</ref>, pairwise hinge loss <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and mean square error loss <ref type=\"bibr\" target=\"#b1\"> </ref>, memory network-based CMN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, metric learning-based CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and NBPO <ref type=\"bibr\" target=\"#b40\">[41]</ref> that l b10\">[11]</ref>, softmax cross-entropy <ref type=\"bibr\" target=\"#b4\">[5]</ref>, pairwise hinge loss <ref type=\"bibr\" target=\"#b11\">[12]</ref>, etc. However, there is still a lack of a systematic compa ct (e.g., in LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>) or Euclidean distance (e.g., in CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>) to measure the similarity (or distance) between a user-it ethods of other types, including ItemPop, SLIM <ref type=\"bibr\" target=\"#b20\">[21]</ref>,</p><p>CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>, YouTubeNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, CMN < f>]. \u2022 Pairwise hinge loss (PHL), is also known as max-margin objective, which has been used in CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>. PHL forces the distance of a negative user-item pair to b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -MC <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Pinsage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5]</ref>, pairwise hinge loss <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and mean square error loss <ref type=\"bibr\" target=\"#b1\">[2]</ref>, there is still a lack of systematic evaluation and comparis ]</ref> that incorporated both MF and graph-based models for implicit CF. Chen et al. designed ENMF <ref type=\"bibr\" target=\"#b1\">[2]</ref>, which is an efficient MF-based CF model with modified MSE l ref type=\"bibr\" target=\"#b36\">[37]</ref>, NeuMF <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and ENMF <ref type=\"bibr\" target=\"#b1\">[2]</ref>; \u2022 Four autoencoder-based methods, including Mult-VAE <ref t r (MSE) has been widely used for CF, such as WMF <ref type=\"bibr\" target=\"#b12\">[13]</ref> and ENMF <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Table <ref type=\"table\" target=\"#tab_0\">1</ref> shows the r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imilar effect to the confidence weight imposed on negative samples in weighted matrix factorization <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  ation task through the SCE loss. \u2022 Mean square error (MSE) has been widely used for CF, such as WMF <ref type=\"bibr\" target=\"#b12\">[13]</ref> and ENMF <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Table <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b15\">[16]</ref>, attention networks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, transformers <ref type=\"bibr\" target=\"#b27\">[28]</ref>, graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b9. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: niform random sampling for recommendation, including mining informative negative samples (e.g., RNS <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and NBPO <ref type=\"bibr\" target=\"#b40\">[41]</ref>), tackli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: datasets Amazon-Books, Yelp2018, and Gowalla, which are commonly used in recent GNN-based CF models <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe f type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref>, NIA-GCN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, LightGCN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > that learn neighborhood relationships, and SGL-ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and so on CiteUlike-A, and Movielens-1M, are provided by NBPO <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, respe orresponding datasets that adopted in their original papers. For example, we will compare with DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref> on CiteUlike-A dataset because DHCF adopts this dataset in -ED <ref type=\"bibr\" target=\"#b34\">[35]</ref>, BGCF <ref type=\"bibr\" target=\"#b28\">[29]</ref>, DHCF <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and LCFN <ref type=\"bibr\" target=\"#b39\">[40]</ref>; \u2022 Six. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> experiment with the arget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar ion for CF research <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33</ref>]. \u2022 Pairwise hinge l target=\"#b8\">[9]</ref> to model the item-item relationships for Pinterest. Wang et al. devised NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref> that explicitly encoded the collaborative signals as high- sage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref type=\"bibr\" target=\"#b30\">[31]</ref>, NGCF <ref type=\"bibr\" target=\"#b31\">[32]</ref>, DisenGCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, LR-GC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and RecVAE <ref type=\"bibr\" target=\"#b22\">[23]</ref>; \u2022 Fourteen GNN-based methods, including GC-MC <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Pinsage <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GAT <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hile multiple loss functions have been used in CF, such as Bayesian personalized ranking (BPR) loss <ref type=\"bibr\" target=\"#b21\">[22]</ref>, binary cross-entropy loss <ref type=\"bibr\" target=\"#b10\"> \"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> experiment with the BPR loss <ref type=\"bibr\" target=\"#b21\">[22]</ref> and simply set the negative sampling ratio to a small valu /head><p>In the CF literature, many different loss functions have been employed, including BPR loss <ref type=\"bibr\" target=\"#b21\">[22]</ref>, binary cross-entropy <ref type=\"bibr\" target=\"#b10\">[11]< here usually exist many redundant yet uninformative samples. But existing loss functions (e.g., BPR <ref type=\"bibr\" target=\"#b21\">[22]</ref>) treat every negative sample equivalently. As such, model  one of the most commonly used loss function for CF research <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: om graph motifs. Graph motifs can be defined as significant subgraph patterns that frequently occur <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Motifs usually contain semantic meanings and are indicati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eled data is scarce. Specifically, we use 250k unlabeled molecules sampled from the ZINC15 database <ref type=\"bibr\" target=\"#b37\">[38]</ref> for self-supervised pre-training tasks. As for the downstr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>. Let G = (V, E) den. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y meaningful motifs and construct motif trees for molecular graphs, we leverage the BRICS algorithm <ref type=\"bibr\" target=\"#b3\">[4]</ref> which is based on retrosynthesis from the chemistry domain.  firstly use the Breaking of Retrosynthetically Interesting Chemical Substructures (BRICS) algorithm <ref type=\"bibr\" target=\"#b3\">[4]</ref> that leverages the domain knowledge from chemistry. BRICS de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d then transferred to downstream tasks with limited labels <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. For example, the pre-t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l and distance information of atoms in molecules, many works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nsuming to do a grid search to determine the optimal weights. Here, we adapts the MGDA-UB algorithm <ref type=\"bibr\" target=\"#b36\">[37]</ref> from multi-task learning to efficiently solve the optimiza. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  f \u03b8 is transferred to downstream tasks.</p><p>We note that most existing works on graph generation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> follow the auto-re a size over 100k while more than 90% motifs have frequencies less than 5. On the other hand, JT-VAE <ref type=\"bibr\" target=\"#b16\">[17]</ref> fragments molecules into rings and bonds and has a motif v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nsuming to do a grid search to determine the optimal weights. Here, we adapts the MGDA-UB algorithm <ref type=\"bibr\" target=\"#b36\">[37]</ref> from multi-task learning to efficiently solve the optimiza. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tions among atoms, <ref type=\"bibr\" target=\"#b7\">[8]</ref> proposes a message passing framework and <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> extend this framew. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: culebased tasks such as retrosynthesis <ref type=\"bibr\" target=\"#b46\">[47]</ref> and drug discovery <ref type=\"bibr\" target=\"#b7\">[8]</ref>. To preserve the internal structural information, molecules  raph convolutional network for property prediction. To better capture the interactions among atoms, <ref type=\"bibr\" target=\"#b7\">[8]</ref> proposes a message passing framework and <ref type=\"bibr\" ta ork (GNN) and some of its variants for molecular property prediction and obtained promising results <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>.</p><p>Though GNNs h  a large amount of labeled data (i.e., molecules with known property data) is required for training <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. However, labeled mo . To fully consider the internal spatial and distance information of atoms in molecules, many works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models <ref type=\"bibr\" target=\"#b46\">(Uria et al., 2014)</ref> and absorbing discrete diffusion <ref type= s x D \u223c p(x D |x 1 , x 2 , . . . , x D\u22121 ).</p><p>Order Agnostic ARMs Order Agnostic ARMs (OA-ARMs) <ref type=\"bibr\" target=\"#b46\">(Uria et al., 2014)</ref> generate variables with a random ordering \u03c3 uire a large number of steps to converge.</p><p>Order agnostic sequence modelling was introduced in <ref type=\"bibr\" target=\"#b46\">(Uria et al., 2014)</ref> and utilizes the same objective as AO-ARDMs optimized where t is sampled from a uniform distribution. This objective was originally proposed by <ref type=\"bibr\" target=\"#b46\">Uria et al. (2014)</ref> to train order-agnostic ARMs. We will develo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al complexity during training. Depth upscaling is reminiscent of the upscaling networks proposed in <ref type=\"bibr\" target=\"#b24\">(Kalchbrenner et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Menic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (van den Oord et al., 2016a;</ref><ref type=\"bibr\">Kalchbrenner et al., 2018, i.a.)</ref>, and text <ref type=\"bibr\" target=\"#b4\">(Bengio et al., 2003;</ref><ref type=\"bibr\" target=\"#b14\">Graves, 2013. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es in deep learning have allowed tremendous progress on various modalities, such as images (van den <ref type=\"bibr\" target=\"#b50\">Oord et al., 2016b;</ref><ref type=\"bibr\" target=\"#b8\">Child et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b4\">(Bengio et al., 2003;</ref><ref type=\"bibr\" target=\"#b14\">Graves, 2013;</ref><ref type=\"bibr\" target=\"#b32\">Melis et al., 2018;</ref><ref type=\"bibr\" target=\"#b35\">Merity et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  rely on bits-back coding <ref type=\"bibr\" target=\"#b44\">(Townsend et al., 2019)</ref>, such as LBB <ref type=\"bibr\" target=\"#b15\">(Ho et al., 2019)</ref>, HiLLoC <ref type=\"bibr\" target=\"#b45\">(Towns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: man and mouse genomes <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref> . However, to make pr f>). This performance increase is twice as large as the performance increase between Basenji1 (ref. <ref type=\"bibr\" target=\"#b2\">3</ref> ) and Basenji2 (ref. <ref type=\"bibr\" target=\"#b1\">2</ref> ) a y from DNA sequences can process distinct alleles and compare predictions to score genetic variants <ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d could make use of the Enformer architecture in the future <ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35</ref> . The sensitivity of the model to genetic variants could be . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ritize rare or de novo variants observed for rare disorders <ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38</ref> , and impute regulatory activity across species to study cis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ritize rare or de novo variants observed for rare disorders <ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38</ref> , and impute regulatory activity across species to study cis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref> . However, to make predictions, these models are only able t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d eQTLs discovered by the GTEx project across dozens of human tissues to validate model predictions <ref type=\"bibr\" target=\"#b19\">20</ref> . The primary challenge of such validation is the influence  ence shifts to the left and right. We computed scores for all 1000 Genomes SNPs.</p><p>We used SLDP <ref type=\"bibr\" target=\"#b19\">20</ref> to estimate the functional correlation between these scores  thout needing to consider LD, we studied statistical fine-mapping of GTEx v8 using the SuSiE method <ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b22\">23</ref> . We focused on vari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing set, such that the training features had a mean of 0 and s.d. of 1. Following our previous work <ref type=\"bibr\" target=\"#b44\">45</ref> , we then trained a lasso regression model for each locus us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref> . A successful model would be able to produce the results . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ritize rare or de novo variants observed for rare disorders <ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38</ref> , and impute regulatory activity across species to study cis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >1</ref>) -for several genes with CRISPRi-validated enhancers <ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b12\">13</ref> . Contribution scores highlight the input sequences that are o large-scale CRISPRi studies performed on the K562 cell line <ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b12\">13</ref> . In these experiments, CRISPRi was used to suppress the act ). The performance of Enformer was comparable to, and in some cases even better than, the ABC score <ref type=\"bibr\" target=\"#b12\">13</ref> , a state-of-the-art method recently proposed specifically f redict the influence of genetic variants on cell-type-specific gene expression, in order to inform  <ref type=\"bibr\" target=\"#b12\">13</ref> ; enformer attention weight averaged across all layers and h lative distance, as measured by auPrC on two CrISPri datasets <ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b12\">13</ref> for different methods, models, and contribution scores (Meth on scores (Methods). AbC score* (H3K27ac/distance) denotes the approximate version of the AbC score <ref type=\"bibr\" target=\"#b12\">13</ref> lacking Hi-C data, which exhibits similar performance (exten  to class imbalance, which differs between the two datasets (1:10 for Gasperini 9 and 1:4 for Fulco <ref type=\"bibr\" target=\"#b12\">13</ref> ). c, Average attention matrix difference of enformer betwee ef type=\"bibr\" target=\"#b8\">9</ref> using scRNA-seq to measure expression changes, and Fulco et al. <ref type=\"bibr\" target=\"#b12\">13</ref> using Flow-FISH. We transformed the enhancer and gene coordi airs that exhibited a significant expression change using area under precision-recall curve (auPRC) <ref type=\"bibr\" target=\"#b12\">13</ref> .</p><p>To prioritize enhancer-gene pairs with sequence-base  sequence: |f(modified) -f(reference)|.</p><p>To reproduce the ABC score introduced in Fulco et al. <ref type=\"bibr\" target=\"#b12\">13</ref> , we obtained the BigWig of H3K27ac ChIP-seq data in K562 fr -validated enhancers (dark gray) exhibiting significant HNRNPA1 expression changes from Fulco et al.<ref type=\"bibr\" target=\"#b12\">13</ref> ; enformer attention weight averaged across all layers and h elative distance, as measured by auPrC on two CrISPri datasets<ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b12\">13</ref> for different methods, models, and contribution scores (Meth ion scores (Methods). AbC score* (H3K27ac/distance) denotes the approximate version of the AbC score<ref type=\"bibr\" target=\"#b12\">13</ref> lacking Hi-C data, which exhibits similar performance (exten e to class imbalance, which differs between the two datasets (1:10 for Gasperini 9 and 1:4 for Fulco<ref type=\"bibr\" target=\"#b12\">13</ref> ). c, Average attention matrix difference of enformer betwee loud.google. com/storage/browser/basenji_barnyard/data. Processed CRISPRi data for Fulco et al 2019 <ref type=\"bibr\" target=\"#b12\">13</ref> was obtained from supplementary material and for Gasperini e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ritize rare or de novo variants observed for rare disorders <ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b37\">38</ref> , and impute regulatory activity across species to study cis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: problem by exploiting recent advances in dense text retrieval.</p><p>A notable exception is CorefQA <ref type=\"bibr\" target=\"#b40\">(Wu et al., 2020b)</ref>, from which we take direct inspiration. In t evel binary classifier) or reduction to tagging (i.e., spans are expressed as a BIO-label sequence) <ref type=\"bibr\" target=\"#b40\">(Wu et al., 2020b;</ref><ref type=\"bibr\" target=\"#b23\">Li et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng with its first argument in KB completion <ref type=\"bibr\" target=\"#b21\">(Levy et al., 2017;</ref><ref type=\"bibr\" target=\"#b23\">Li et al., 2019)</ref>, an entity category in (nested) NER <ref type= spans are expressed as a BIO-label sequence) <ref type=\"bibr\" target=\"#b40\">(Wu et al., 2020b;</ref><ref type=\"bibr\" target=\"#b23\">Li et al., 2019;</ref><ref type=\"bibr\">2020)</ref>. We found these me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iever <ref type=\"bibr\" target=\"#b39\">(Wu et al., 2020a)</ref> and ELECTRA finetuned on a QA dataset <ref type=\"bibr\" target=\"#b5\">(Clark et al., 2019)</ref> to obtain an easy improvement. Second, EntQ rd negative mining. We initialize the joint encoder enc \u03b8 H in the reader module with ELECTRA-large <ref type=\"bibr\" target=\"#b5\">(Clark et al., 2019)</ref> finetuned on SQuAD 2.0 <ref type=\"bibr\" tar  We report F 1 on the validation set of AIDA. The baseline reader is initialized with ELECTRA-large <ref type=\"bibr\" target=\"#b5\">(Clark et al., 2019)</ref> finetuned on SQuAD 2.0, uses the joint pass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ) = P \u22a4 1 E e 1</formula><p>At inference time, we precompute E e \u2208 R d for each e \u2208 E and use Faiss <ref type=\"bibr\" target=\"#b17\">(Johnson et al., 2019)</ref> for fast top-K retrieval.</p><p>Training. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b13\">Hasibi et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Balog et al., 2013;</ref><ref type=\"bibr\" target=\"#b32\">Reinanda et al., 2015)</ref>, and commercial recommendation systems <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e find that we can fit the reader just relation type along with its first argument in KB completion <ref type=\"bibr\" target=\"#b21\">(Levy et al., 2017;</ref><ref type=\"bibr\" target=\"#b23\">Li et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ational building block in automatic text understanding with applications to question answering (QA) <ref type=\"bibr\" target=\"#b9\">(Ferrucci, 2012)</ref>, information retrieval <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e find that we can fit the reader just relation type along with its first argument in KB completion <ref type=\"bibr\" target=\"#b21\">(Levy et al., 2017;</ref><ref type=\"bibr\" target=\"#b23\">Li et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b13\">Hasibi et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Balog et al., 2013;</ref><ref type=\"bibr\" target=\"#b32\">Reinanda et al., 2015)</ref>, and commercial recommendation systems <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  then the mentions are resolved to KB entries. Previous works either assume that mentions are given <ref type=\"bibr\" target=\"#b12\">(Gupta et al., 2017)</ref>, run an off-the-shelf named-entity recogni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s industry-scale pretraining by weak supervision. Specifically, GENRE is trained by finetuning BART <ref type=\"bibr\" target=\"#b22\">(Lewis et al., 2020)</ref> on autoregressive EL training examples con. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ) = P \u22a4 1 E e 1</formula><p>At inference time, we precompute E e \u2208 R d for each e \u2208 E and use Faiss <ref type=\"bibr\" target=\"#b17\">(Johnson et al., 2019)</ref> for fast top-K retrieval.</p><p>Training. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: question answering (QA) <ref type=\"bibr\" target=\"#b9\">(Ferrucci, 2012)</ref>, information retrieval <ref type=\"bibr\" target=\"#b41\">(Xiong et al., 2017;</ref><ref type=\"bibr\" target=\"#b13\">Hasibi et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b24\">(Li et al., 2020)</ref>, an auxiliary verb or a wh-expression in ellipsis resolution <ref type=\"bibr\" target=\"#b2\">(Aralikatte et al., 2021)</ref>, and other task-specific questions <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ational building block in automatic text understanding with applications to question answering (QA) <ref type=\"bibr\" target=\"#b9\">(Ferrucci, 2012)</ref>, information retrieval <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the datasets' statistics. For the KB, we use the 2019 Wikipedia dump provided in the KILT benchmark <ref type=\"bibr\" target=\"#b30\">(Petroni et al., 2021)</ref>, which contains 5.9 million entities.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: distinguish different encoders. We assume the usual special tokens in the input popularized by BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>: [CLS] to represent the whole input and [S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ach of their residue pairs as being independent of one another. Recent work on interface prediction <ref type=\"bibr\" target=\"#b20\">(Liu et al. (2020)</ref>), however, considers the biological insight  nce features still carry important higher-order information concerning residue-residue interactions <ref type=\"bibr\" target=\"#b20\">(Liu et al. (2020)</ref>). In particular, the residue-residue coevolu e of the best result sets obtained by any model for protein interface contact prediction comes from <ref type=\"bibr\" target=\"#b20\">Liu et al. (2020)</ref> where high-order (i.e. sequential and coevolu //www.tei-c.org/ns/1.0\"><head>A.4 Alternative Networks for the Interaction Module</head><p>We, like <ref type=\"bibr\" target=\"#b20\">Liu et al. (2020)</ref>, note that the task of interface prediction b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecific geometric properties, as well as a dilated convolution-based interaction module adapted from <ref type=\"bibr\" target=\"#b2\">Chen et al. (2021)</ref> to predict which inter-chain residue pairs co ion module, consisting of a dilated ResNet module adapted from the intra-chain contact predictor of <ref type=\"bibr\" target=\"#b2\">Chen et al. (2021)</ref>. The core residual network component in this  ethods are scored using the top-k precision metric commonly used for intra-chain contact prediction <ref type=\"bibr\" target=\"#b2\">(Chen et al. (2021))</ref>, where k \u2208 {10, L/10, L/5} with L being the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , the conformation module can operate more stably in the presence of multiple neural network layers <ref type=\"bibr\" target=\"#b9\">(He et al. (2016)</ref>).</p><p>In locating edges with which to update. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: works (CNNs) and graph neural networks (GNNs) have been used to predict protein interface contacts. <ref type=\"bibr\" target=\"#b6\">Fout et al. (2017)</ref> designed a siamese GNN architecture to learn . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecific geometric properties, as well as a dilated convolution-based interaction module adapted from <ref type=\"bibr\" target=\"#b2\">Chen et al. (2021)</ref> to predict which inter-chain residue pairs co ion module, consisting of a dilated ResNet module adapted from the intra-chain contact predictor of <ref type=\"bibr\" target=\"#b2\">Chen et al. (2021)</ref>. The core residual network component in this  ethods are scored using the top-k precision metric commonly used for intra-chain contact prediction <ref type=\"bibr\" target=\"#b2\">(Chen et al. (2021))</ref>, where k \u2208 {10, L/10, L/5} with L being the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  P j , the position of node i and j in the chain's underlying amino acid sequence. In the spirit of <ref type=\"bibr\" target=\"#b12\">Ingraham et al. (2019)</ref>, we then concatenate an edge-wise sinuso boring nodes. For this reason, we adopt similar distance, direction, and orientation descriptors as <ref type=\"bibr\" target=\"#b12\">Ingraham et al. (2019)</ref>. Finally, we complement the protein back s=\"http://www.tei-c.org/ns/1.0\"><head>A.2 Definition of Edge Geometric Features</head><p>Similar to <ref type=\"bibr\" target=\"#b12\">Ingraham et al. (2019)</ref>, we construct a local reference frame (i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecrease the time required to discover new drugs and to advance the study of newly designed proteins <ref type=\"bibr\" target=\"#b23\">(Murakami et al. (2017)</ref>).</p><p>Existing approaches to interfac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o equivariant machine learning architectures such as the Equivariant Graph Neural Network (EGNN) by <ref type=\"bibr\" target=\"#b26\">Satorras et al. (2021)</ref>. Equivariant updates to rich geometric f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , which is a convolutional layer with 64 kernels of size 1 \u00d7 1. A squeeze-and-excitation (SE) block <ref type=\"bibr\" target=\"#b10\">(Hu et al. (2018)</ref>) is added at the end of each residual block.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wledge-aware reasoning.</p><p>In the analysis part, we employ Sparse Variational Dropout (SparseVD; <ref type=\"bibr\" target=\"#b35\">Molchanov et al. (2017)</ref>) as a tool to dissect existing graph ne trains the weights to our pruning prior. We implement the SparseVD with the default threshold as in <ref type=\"bibr\" target=\"#b35\">Molchanov et al. (2017)</ref>. Eventually, we get the pruned model wi eased by the author. To keep the dissection in strict accordance with the theoretical derivation of <ref type=\"bibr\" target=\"#b35\">Molchanov et al. (2017)</ref>, we apply regulation to all the linear  information, we introduce a neural model pruning method named Sparse Variational Dropout (SparseVD) <ref type=\"bibr\" target=\"#b35\">(Molchanov et al., 2017)</ref> as a diagnostic tool to automatically  dge-aware systems, we introduce a neural model pruning method Sparse Variational Dropout (SparseVD, <ref type=\"bibr\" target=\"#b35\">(Molchanov et al., 2017)</ref>) into this scenario as a dissection to ler Divergence approximation following <ref type=\"bibr\" target=\"#b0\">(Achterhold et al., 2018;</ref><ref type=\"bibr\" target=\"#b35\">Molchanov et al., 2017)</ref> to constrain the model parameters to co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Methods Test</head><p>Careful Selection <ref type=\"bibr\" target=\"#b2\">(Banerjee et al., 2019)</ref> 72.0 AristoRoBERTa 77.8 KF + SIR <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y proposed in the field of model compression <ref type=\"bibr\" target=\"#b12\">(Han et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">He et al., 2018;</ref><ref type=\"bibr\" target=\"#b27\">Lin et al., 2017. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph modeling is carried out via an elaborately designed graph-based neural module. For instance, <ref type=\"bibr\" target=\"#b26\">Lin et al. (2019)</ref> uses GCN-LSTM-HPA which combines graph convol NN-based QA employ external embeddings to initialize the node embeddings in the graph. For example, <ref type=\"bibr\" target=\"#b26\">Lin et al. (2019)</ref> employs TransE <ref type=\"bibr\" target=\"#b58\" beddings.</p><p>Relevance score. In order to measure the quality of a path and prune the sub-graph, <ref type=\"bibr\" target=\"#b26\">Lin et al. (2019)</ref> decomposes the KG into a set of triples, the   be evaluated once every two weeks via the official leaderboard. Hence, following the data split of <ref type=\"bibr\" target=\"#b26\">Lin et al. (2019)</ref>, we experiment and report the accuracy on the den, here we report the in-house dev (IHdev) and test (IHtest) accuracy, following the data split of<ref type=\"bibr\" target=\"#b26\">Lin et al. (2019)</ref>.</figDesc><table><row><cell>Implementation an is the set of triplet edges that connect nodes in V with relation types in R. Following prior works <ref type=\"bibr\" target=\"#b26\">(Lin et al., 2019;</ref><ref type=\"bibr\">Yasunaga et al., 2021)</ref> o SparseVD w/ SparseVD Methods IHdev-Acc. (%) IHtest-Acc. (%) IHdev-Acc. (%) IHtest-Acc. (%) KagNet <ref type=\"bibr\" target=\"#b26\">(Lin et al., 2019)</ref> 73.47 (\u00b10.22) 69.01 (\u00b10.76) 75.18 (\u00b11.05) 70 ef>.  <ref type=\"bibr\" target=\"#b57\">(Wang et al., 2019)</ref> 72.61( \u00b10.39) 68.59 (\u00b10.96) + KagNet <ref type=\"bibr\" target=\"#b26\">(Lin et al., 2019)</ref> 73.47 (\u00b10.22) 69.01 (\u00b10.76) + RN <ref type=\" et al., 2019)</ref>, RGCN <ref type=\"bibr\" target=\"#b46\">(Schlichtkrull et al., 2018)</ref>, KagNet <ref type=\"bibr\" target=\"#b26\">(Lin et al., 2019)</ref>, MHGRN <ref type=\"bibr\" target=\"#b10\">(Feng  ered QA, where GNNs naturally fit the graph-structured knowledge and show prominent results. KagNet <ref type=\"bibr\" target=\"#b26\">(Lin et al., 2019)</ref> proposes GCN-LSTM-HPA for path-based relatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gs in the graph. For example, <ref type=\"bibr\" target=\"#b26\">Lin et al. (2019)</ref> employs TransE <ref type=\"bibr\" target=\"#b58\">(Wang et al., 2014)</ref> with GloVe embeddings <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eriments on CommonsenseQA <ref type=\"bibr\" target=\"#b51\">(Talmor et al., 2018)</ref> and OpenBookQA <ref type=\"bibr\" target=\"#b33\">(Mihaylov et al., 2018)</ref>, two popular QA benchmark datasets that. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Methods Test</head><p>Careful Selection <ref type=\"bibr\" target=\"#b2\">(Banerjee et al., 2019)</ref> 72.0 AristoRoBERTa 77.8 KF + SIR <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t knowledge such as knowledge graphs (KGs) <ref type=\"bibr\" target=\"#b47\">(Speer et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Bollacker et al., 2008)</ref> works better for structured reasoning as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uning method agnostic; other pruning schemes <ref type=\"bibr\" target=\"#b11\">(Han et al., 2015;</ref><ref type=\"bibr\" target=\"#b13\">He et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Liu et al., 2017. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as a variational inference problem and use the Kullback-Leibler Divergence approximation following <ref type=\"bibr\" target=\"#b0\">(Achterhold et al., 2018;</ref><ref type=\"bibr\" target=\"#b35\">Molchano. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2020)  74.45 (\u00b10.10) 71.11 (\u00b10.81) + QAGNN <ref type=\"bibr\">(Yasunaga et al., 2021)</ref> 76   <ref type=\"bibr\" target=\"#b31\">(Lv et al., 2020)</ref> 75.3 RoBERTa + MHGRN <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and other custom network policies and business strategies <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>Community values are simply byte strings, and there i ties and events in the Internet.</p><p>Benoit et al. provided the first taxonomy of BGP communities <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Since then, community usage has continued to increase. Spec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ty <ref type=\"bibr\" target=\"#b25\">[26]</ref>, community-driven routing load and unnecessary updates <ref type=\"bibr\" target=\"#b14\">[15]</ref>, community-based outage detection <ref type=\"bibr\" target= g our investigation, Krenc et al. explore the BGP update message load and impact due to communities <ref type=\"bibr\" target=\"#b14\">[15]</ref> and permissive propagation. In particular, Krenc's longitu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n order to accommodate 32-bit ASes as well, the BGP Large Communities Attribute has been introduced <ref type=\"bibr\" target=\"#b12\">[13]</ref>. A large community is a 3\ud835\udc6532-bit integer denoted in the fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ively analyzed, e.g. <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> to name a few studi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ely. In order to facilitate multi-lateral peering, IXPs offer router servers as value-added service <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Also, IXPs offer remote peerings where no physical presen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etwork research community, akin to AS relationship inference <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. Our contributions include:</p><p>\u2022 A novel passive algorithm siness relationships at different locations which further increases the complexity of relationships <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>In order for an AS to become globally reachable, it s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[8]</ref> and blackholing <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and hijack detection schemes <ref type=\"bibr\" target=\"#b23\">[24]</ref> can all benefit from a grounded per-AS understanding of BG. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n order to accommodate 32-bit ASes as well, the BGP Large Communities Attribute has been introduced <ref type=\"bibr\" target=\"#b12\">[13]</ref>. A large community is a 3\ud835\udc6532-bit integer denoted in the fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and unnecessary updates <ref type=\"bibr\" target=\"#b14\">[15]</ref>, community-based outage detection <ref type=\"bibr\" target=\"#b7\">[8]</ref> and blackholing <ref type=\"bibr\" target=\"#b10\">[11]</ref>, a 0\">[11]</ref>. Subsequent work used communities as a proxy to detect peering infrastructure outages <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>Further motivating our work, Streibelt et al. explore. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ely. In order to facilitate multi-lateral peering, IXPs offer router servers as value-added service <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Also, IXPs offer remote peerings where no physical presen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: perational networks in the wild. For instance, work on measuring community propagation and security <ref type=\"bibr\" target=\"#b25\">[26]</ref>, community-driven routing load and unnecessary updates <re k vectors using BGP communities, and their feasibility due to a general lack of community filtering <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Streibelt's work showed that the lack of common understan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xical variants with a richer flexibility.</p><p>The recently proposed Fusion-in-Decoder (FiD) model <ref type=\"bibr\" target=\"#b8\">(Izacard and Grave, 2021)</ref> is representative of those methods wit t to the reader will not be a good solution as it will significantly decrease the model performance <ref type=\"bibr\" target=\"#b8\">(Izacard and Grave, 2021)</ref>. How to overcome such inefficient comp </ref> which contains questions from trivia and quiz-league websites. We follow the same setting as <ref type=\"bibr\" target=\"#b8\">(Izacard and Grave, 2021)</ref> to preprocess these datasets, which is AT) <ref type=\"bibr\" target=\"#b26\">(Velickovic et al., 2018)</ref>. For the reading module, same as <ref type=\"bibr\" target=\"#b8\">(Izacard and Grave, 2021)</ref>, we initialize it with the pretrained  s/1.0\"><head n=\"4.2\">Baseline Methods</head><p>We mainly compare KG-FiD with the baseline model FiD <ref type=\"bibr\" target=\"#b8\">(Izacard and Grave, 2021)</ref>. For other baselines, we compare with  \" target=\"#b3\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b11\">Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Izacard and Grave, 2021)</ref> mainly use Wikipedia as knowledge sourc s</head><p>Knowledge Source: Following <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Izacard and Grave, 2021)</ref>, we use the English Wikipedia as the te  2021;</ref><ref type=\"bibr\" target=\"#b5\">Guu et al., 2020)</ref> and generative reader BART and T5 <ref type=\"bibr\" target=\"#b8\">(Izacard and Grave, 2021;</ref><ref type=\"bibr\">Lewis et al., 2020)</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rks <ref type=\"bibr\" target=\"#b5\">(Guu et al., 2020;</ref><ref type=\"bibr\">Lewis et al., 2020;</ref><ref type=\"bibr\" target=\"#b23\">Sachan et al., 2021)</ref> have shown that additional unsupervised pr get=\"#b5\">(Guu et al., 2020</ref><ref type=\"bibr\">), RAG (Lewis et al., 2020)</ref> and Joint Top-K <ref type=\"bibr\" target=\"#b23\">(Sachan et al., 2021)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: N layers is 3. Note that other GNN models such as <ref type=\"bibr\">GIN (Xu et al., 2019)</ref>, DGI <ref type=\"bibr\" target=\"#b27\">(Velickovic et al., 2019)</ref> can also be applied here and we leave. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g based passage retrieval (DPR) models <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b19\">Qu et al., 2021)</ref> have shown superior performance over BoW/TF-ID odels (PLMs). Moreover, some further improvement are also proposed such as better training strategy <ref type=\"bibr\" target=\"#b19\">(Qu et al., 2021)</ref>, reranking based on retrieved passages <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\">Xu et al., 2021a)</ref>. Some works <ref type=\"bibr\" target=\"#b1\">(Berant et al., 2013;</ref><ref type=\"bibr\" target=\"#b25\">Sun et al., 2018</ref><ref type=\"bibr\" target=\"#b24\">Sun et al., , 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is the set of retrieved passage indices.</formula><p>Then we employ a graph attention network (GAT) <ref type=\"bibr\" target=\"#b26\">(Velickovic et al., 2018)</ref> with L g layers as GNN model to updat ula_9\">(0) i = P (L) i (0) \u2208 R D .</formula><p>Then same as stage-1 reranking, we also employ a GAT <ref type=\"bibr\" target=\"#b26\">(Velickovic et al., 2018)</ref> with L g layers as the graph neural n d passage separately. For the GNN reranking models, we adopt 3-layer Graph Attention Networks (GAT) <ref type=\"bibr\" target=\"#b26\">(Velickovic et al., 2018)</ref>. For the reading module, same as <ref  cost of retrieving part is the same as vanilla FiD. Since we set N 0 = 1000 and N 1 = 100, the GAT <ref type=\"bibr\" target=\"#b26\">(Velickovic et al., 2018)</ref> computation in stage-1 reranking take. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e documents for a given question, and the reader produces an answer based on the retrieved passages <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Guu et al uage Models (PLMs) in neural network research, dense embedding based passage retrieval (DPR) models <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b19\">Qu et al work <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2017)</ref> to PLMs such as extractive reader BERT <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b6\">Iyer et a p://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Implementation Details</head><p>Knowledge Source: Following <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Izacard a swer prediction. For example, previous works <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b11\">Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Izacard an  target=\"#b3\">(Chen et al., 2017;</ref><ref type=\"bibr\">Yang et al., 2019)</ref> to dense retrieval <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020)</ref> based on contextualized embeddings gen =\"3.2\">Passage Retrieving &amp; Stage-1 Reranking</head><p>DPR Retriever: Our framework applies DPR <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020)</ref> as the retriever, which uses a BERT ba  2.7M, 974 and 14M respectively. Model Details: For the retrieving module, we use the DPR retriever <ref type=\"bibr\" target=\"#b11\">(Karpukhin et al., 2020)</ref> which contains two BERT (base) models  tion of a passage. Following previous works <ref type=\"bibr\" target=\"#b30\">(Wang et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Karpukhin et al., 2020)</ref>, each article in the text corpus is spl nt.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\">We follow<ref type=\"bibr\" target=\"#b11\">Karpukhin et al. (2020)</ref> on the definition of gold passages.</no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iplet respectively.</p><p>First, we formalize the definition of a passage. Following previous works <ref type=\"bibr\" target=\"#b30\">(Wang et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Karpukhin et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\">Xu et al., 2021a)</ref>. Some works <ref type=\"bibr\" target=\"#b1\">(Berant et al., 2013;</ref><ref type=\"bibr\" target=\"#b25\">Sun et al., 2018</ref><ref type=\"bibr\" target=\"#b24\">Sun et al., , 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b;</ref><ref type=\"bibr\">Yu et al., 2020;</ref><ref type=\"bibr\">Xu et al., 2021a)</ref>. Some works <ref type=\"bibr\" target=\"#b1\">(Berant et al., 2013;</ref><ref type=\"bibr\" target=\"#b25\">Sun et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b;</ref><ref type=\"bibr\">Yu et al., 2020;</ref><ref type=\"bibr\">Xu et al., 2021a)</ref>. Some works <ref type=\"bibr\" target=\"#b1\">(Berant et al., 2013;</ref><ref type=\"bibr\" target=\"#b25\">Sun et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ;</ref><ref type=\"bibr\" target=\"#b14\">Kiritchenko and Matwin, 2011)</ref>. Many previous frameworks <ref type=\"bibr\" target=\"#b9\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019 t will be transformed into a useful training instance.</p><p>Inspired by some co-training paradigms <ref type=\"bibr\" target=\"#b9\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b9\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Li et al., 2020 pe=\"bibr\" target=\"#b9\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020)</ref>, we propose the collaborative denoising learn set unexplored fully. Co-teaching+ <ref type=\"bibr\" target=\"#b38\">(Yu et al., 2019)</ref> and JoCoR <ref type=\"bibr\" target=\"#b34\">(Wei et al., 2020)</ref> are two classical label denoising methods, d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Matwin, 2011)</ref>. Many previous frameworks <ref type=\"bibr\" target=\"#b9\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020 </p><p>Inspired by some co-training paradigms <ref type=\"bibr\" target=\"#b9\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020 reduce the negative effect of noisy labels, leaving training dataset unexplored fully. Co-teaching+ <ref type=\"bibr\" target=\"#b38\">(Yu et al., 2019)</ref> and JoCoR <ref type=\"bibr\" target=\"#b34\">(Wei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: abel predicted by the teacher \u03b8 t .</p><p>High Confidence Predictions. As studied in previous works <ref type=\"bibr\" target=\"#b3\">(Bengio et al., 2009;</ref><ref type=\"bibr\" target=\"#b0\">Arpit et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as a new teacher to gen-erate pseudo labels <ref type=\"bibr\" target=\"#b36\">(Xie et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Wang et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b19\">Liang et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , 2002)</ref>, OntoNotes5.0 <ref type=\"bibr\" target=\"#b35\">(Weischedel et al., 2013)</ref>, Webpage <ref type=\"bibr\" target=\"#b28\">(Ratinov and Roth, 2009)</ref>, Wikigold <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2013)</ref>, Webpage <ref type=\"bibr\" target=\"#b28\">(Ratinov and Roth, 2009)</ref>, Wikigold <ref type=\"bibr\" target=\"#b1\">(Balasuriya et al., 2009)</ref> and Twitter <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: abel predicted by the teacher \u03b8 t .</p><p>High Confidence Predictions. As studied in previous works <ref type=\"bibr\" target=\"#b3\">(Bengio et al., 2009;</ref><ref type=\"bibr\" target=\"#b0\">Arpit et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in NER. For example, BiLSTM-CRF <ref type=\"bibr\" target=\"#b15\">(Lample et al., 2016)</ref> and BERT <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref> based methods become the paradigm in NER d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plored in several studies, e.g., <ref type=\"bibr\">Adam (Kingma and Ba, 2015)</ref>, semi-supervised <ref type=\"bibr\" target=\"#b31\">(Tarvainen and Valpola, 2017)</ref> and selfsupervised <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2013)</ref>, Webpage <ref type=\"bibr\" target=\"#b28\">(Ratinov and Roth, 2009)</ref>, Wikigold <ref type=\"bibr\" target=\"#b1\">(Balasuriya et al., 2009)</ref> and Twitter <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t network. For example, selftraining copies the student as a new teacher to gen-erate pseudo labels <ref type=\"bibr\" target=\"#b36\">(Xie et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Wang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , 2002)</ref>, OntoNotes5.0 <ref type=\"bibr\" target=\"#b35\">(Weischedel et al., 2013)</ref>, Webpage <ref type=\"bibr\" target=\"#b28\">(Ratinov and Roth, 2009)</ref>, Wikigold <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eled with organization type owing to the labeling ambiguity.</p><p>Recently, many denoising methods <ref type=\"bibr\" target=\"#b30\">(Shang et al., 2018b;</ref><ref type=\"bibr\" target=\"#b37\">Yang et al. y labels (i.e., incomplete and inaccurate annotations in NER).</p><p>DS-NER Denoising. Many studies <ref type=\"bibr\" target=\"#b30\">(Shang et al., 2018b;</ref><ref type=\"bibr\" target=\"#b5\">Cao et al.,  et=\"#b18\">Li et al., 2021)</ref> have been developed to handle noisy labels in DS-NER. For example, <ref type=\"bibr\" target=\"#b30\">Shang et al. (2018b)</ref> obtained high-quality phrases through Auto els. And many new training paradigms were proposed to resist label noise in DS-NER, such as AutoNER <ref type=\"bibr\" target=\"#b30\">(Shang et al., 2018b)</ref>, Reinforcement Learning <ref type=\"bibr\"  .</p><p>We compare several DS-NER denoising baselines which propose to solve noisy labels. Au-toNER <ref type=\"bibr\" target=\"#b30\">(Shang et al., 2018b)</ref> and LRNT <ref type=\"bibr\" target=\"#b5\">(C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Li et al., 2020)</ref> have adopted co-training to denoise, but they . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plored in several studies, e.g., <ref type=\"bibr\">Adam (Kingma and Ba, 2015)</ref>, semi-supervised <ref type=\"bibr\" target=\"#b31\">(Tarvainen and Valpola, 2017)</ref> and selfsupervised <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion from dataset by deploying two teacherstudent networks with different architecture. As stated in <ref type=\"bibr\" target=\"#b2\">(Bengio, 2014)</ref>, a human brain can learn more effectively if guid. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: abel predicted by the teacher \u03b8 t .</p><p>High Confidence Predictions. As studied in previous works <ref type=\"bibr\" target=\"#b3\">(Bengio et al., 2009;</ref><ref type=\"bibr\" target=\"#b0\">Arpit et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plored in several studies, e.g., <ref type=\"bibr\">Adam (Kingma and Ba, 2015)</ref>, semi-supervised <ref type=\"bibr\" target=\"#b31\">(Tarvainen and Valpola, 2017)</ref> and selfsupervised <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b38\">Yu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wei et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Li et al., 2020)</ref> have adopted co-training to denoise, but they . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in NER. For example, BiLSTM-CRF <ref type=\"bibr\" target=\"#b15\">(Lample et al., 2016)</ref> and BERT <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref> based methods become the paradigm in NER d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plored in several studies, e.g., <ref type=\"bibr\">Adam (Kingma and Ba, 2015)</ref>, semi-supervised <ref type=\"bibr\" target=\"#b31\">(Tarvainen and Valpola, 2017)</ref> and selfsupervised <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he co-training paradigm which jointly trains two models is used to improve the robustness of models <ref type=\"bibr\" target=\"#b4\">(Blum and Mitchell, 1998;</ref><ref type=\"bibr\" target=\"#b25\">Nigam an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Settings</head><p>Datasets. We conduct experiments on five publicly available NER datasets: CoNLL03 <ref type=\"bibr\" target=\"#b32\">(Tjong Kim Sang, 2002)</ref>, OntoNotes5.0 <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ensive. To address this issue, several studies attempted to annotate tokens via distant supervision <ref type=\"bibr\" target=\"#b19\">(Liang et al., 2020)</ref>. They matched unlabeled sentences with ext ociated with an entity type, e.g., person, location. In this paper, we use the BIO scheme following <ref type=\"bibr\" target=\"#b19\">(Liang et al., 2020)</ref>. In detail, the begin token of an entity m arget=\"#b18\">(Li et al., 2021)</ref> only handles incomplete annotations by negative sampling. BOND <ref type=\"bibr\" target=\"#b19\">(Liang et al., 2020)</ref> adapts self-training directly to DS-NER, s ., 2021)</ref>. In addition, some studies <ref type=\"bibr\" target=\"#b23\">(Mayhew et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Liang et al., 2020)</ref> performed iterative training procedures to  ibr\" target=\"#b36\">(Xie et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Wang et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b19\">Liang et al. (2020)</ref> applied self-training with teacher-student  alasuriya et al., 2009)</ref> and Twitter <ref type=\"bibr\" target=\"#b7\">(Godin et al., 2015)</ref>. <ref type=\"bibr\" target=\"#b19\">Liang et al. (2020)</ref>  Baselines and Evaluation Metrics. We compa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b30\">(Shang et al., 2018b;</ref><ref type=\"bibr\" target=\"#b5\">Cao et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Jie et al., 2019)</ref> tried to modify the standard CRF for adapting. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ty of extracting entity information and benefiting many NLP applications (e.g., relation extraction <ref type=\"bibr\" target=\"#b20\">(Lin et al., 2017)</ref>, question answering <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b4\">(Blum and Mitchell, 1998;</ref><ref type=\"bibr\" target=\"#b25\">Nigam and Ghani, 2000;</ref><ref type=\"bibr\" target=\"#b14\">Kiritchenko and Matwin, 2011)</ref>. Many previous frameworks <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Learning</head><p>It is widely known that deep neural networks have high capacity for memorization <ref type=\"bibr\" target=\"#b0\">(Arpit et al., 2017)</ref>. When noisy labels become prominent, deep n  Predictions. As studied in previous works <ref type=\"bibr\" target=\"#b3\">(Bengio et al., 2009;</ref><ref type=\"bibr\" target=\"#b0\">Arpit et al., 2017)</ref>, hard samples can not be learnt effectively . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ble NER datasets: CoNLL03 <ref type=\"bibr\" target=\"#b32\">(Tjong Kim Sang, 2002)</ref>, OntoNotes5.0 <ref type=\"bibr\" target=\"#b35\">(Weischedel et al., 2013)</ref>, Webpage <ref type=\"bibr\" target=\"#b2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ensive. To address this issue, several studies attempted to annotate tokens via distant supervision <ref type=\"bibr\" target=\"#b19\">(Liang et al., 2020)</ref>. They matched unlabeled sentences with ext ociated with an entity type, e.g., person, location. In this paper, we use the BIO scheme following <ref type=\"bibr\" target=\"#b19\">(Liang et al., 2020)</ref>. In detail, the begin token of an entity m arget=\"#b18\">(Li et al., 2021)</ref> only handles incomplete annotations by negative sampling. BOND <ref type=\"bibr\" target=\"#b19\">(Liang et al., 2020)</ref> adapts self-training directly to DS-NER, s ., 2021)</ref>. In addition, some studies <ref type=\"bibr\" target=\"#b23\">(Mayhew et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Liang et al., 2020)</ref> performed iterative training procedures to  ibr\" target=\"#b36\">(Xie et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Wang et al., 2020)</ref>. <ref type=\"bibr\" target=\"#b19\">Liang et al. (2020)</ref> applied self-training with teacher-student  alasuriya et al., 2009)</ref> and Twitter <ref type=\"bibr\" target=\"#b7\">(Godin et al., 2015)</ref>. <ref type=\"bibr\" target=\"#b19\">Liang et al. (2020)</ref>  Baselines and Evaluation Metrics. We compa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in NER. For example, BiLSTM-CRF <ref type=\"bibr\" target=\"#b15\">(Lample et al., 2016)</ref> and BERT <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref> based methods become the paradigm in NER d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y.</p><p>Recently, many denoising methods <ref type=\"bibr\" target=\"#b30\">(Shang et al., 2018b;</ref><ref type=\"bibr\" target=\"#b37\">Yang et al., 2018;</ref><ref type=\"bibr\" target=\"#b5\">Cao et al., 201  such as AutoNER <ref type=\"bibr\" target=\"#b30\">(Shang et al., 2018b)</ref>, Reinforcement Learning <ref type=\"bibr\" target=\"#b37\">(Yang et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Nooralahzadeh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b26\">(Liao et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">Luan et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">Abu-El-Haija et al., 2019)</ref> have better performance than localize r\" target=\"#b26\">Liao et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b27\">Luan et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b0\">Abu-El-Haija et al. (2019)</ref> observed that multi-hop aggregation r ilters using higher-order GC requires coefficient sparsity, which brings about learning difficulty. <ref type=\"bibr\" target=\"#b0\">Abu-El-Haija et al. (2019)</ref> overcomes this problem by adding lass he aggregation length of multi-hop GCs. We compute the time complexity with respect to the method of<ref type=\"bibr\" target=\"#b0\">Abu-El-Haija et al. (2019)</ref>.</figDesc><table><row><cell>Kernel</c ing a more complex graph kernel, such as multi-hop connections, seems to work in practical settings <ref type=\"bibr\" target=\"#b0\">(Abu-El-Haija et al., 2019)</ref>. The question is: \"what is the simpl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ref>, or those with expressive edge features <ref type=\"bibr\" target=\"#b25\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>, and hyper-edges <ref type=\"bibr\" target=\" ng block in message-passing GNN architectures <ref type=\"bibr\" target=\"#b25\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>. We make an explorative attempt to first i in lost features on the spectrum. Similar to <ref type=\"bibr\" target=\"#b25\">Li et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b12\">Gilmer et al. (2017)</ref>, we append a shared GRU module after each  #b4\">Cai &amp; Wang, 2020)</ref>. Similar to <ref type=\"bibr\" target=\"#b25\">Li et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b12\">Gilmer et al. (2017)</ref>, we appends a shared GRU module after each. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>, and hyper-edges <ref type=\"bibr\" target=\"#b32\">(Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Maron et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>, and hyper-edges <ref type=\"bibr\" target=\"#b32\">(Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Maron et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>, or those with expressive edge features <ref type=\"bibr\" target=\"#b25\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>, and hyper-edges <ref type=\"bibr\" target=\" ng block in message-passing GNN architectures <ref type=\"bibr\" target=\"#b25\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>. We make an explorative attempt to first i in lost features on the spectrum. Similar to <ref type=\"bibr\" target=\"#b25\">Li et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b12\">Gilmer et al. (2017)</ref>, we append a shared GRU module after each  #b4\">Cai &amp; Wang, 2020)</ref>. Similar to <ref type=\"bibr\" target=\"#b25\">Li et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b12\">Gilmer et al. (2017)</ref>, we appends a shared GRU module after each. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>, or those with expressive edge features <ref type=\"bibr\" target=\"#b25\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>, and hyper-edges <ref type=\"bibr\" target=\" ng block in message-passing GNN architectures <ref type=\"bibr\" target=\"#b25\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>. We make an explorative attempt to first i in lost features on the spectrum. Similar to <ref type=\"bibr\" target=\"#b25\">Li et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b12\">Gilmer et al. (2017)</ref>, we append a shared GRU module after each  #b4\">Cai &amp; Wang, 2020)</ref>. Similar to <ref type=\"bibr\" target=\"#b25\">Li et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b12\">Gilmer et al. (2017)</ref>, we appends a shared GRU module after each. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>, and hyper-edges <ref type=\"bibr\" target=\"#b32\">(Morris et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Maron et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: volutional functions can be recovered by stacking multiple one-hop layers. However, extensive works <ref type=\"bibr\" target=\"#b23\">(Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Oono &amp; Suzu target=\"#b16\">(Hoang &amp; Maehara, 2019;</ref><ref type=\"bibr\" target=\"#b41\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Oono &amp; Suzuk U does not lift accuracy on CLUSTER and PATTERN datasets for node classification task. According to <ref type=\"bibr\" target=\"#b23\">Li et al. (2018)</ref>, that GRU suppresses low-frequency band result. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Deng et al., 2020)</ref>, and, most relevant to this work, answer generation for retrieval-based QA <ref type=\"bibr\" target=\"#b25\">(Hsu et al., 2021</ref>) -that we refer to as GENQA.</p><p>Compared t r, we study and propose a simple technique for open-domain QA in a cross-lingual setting. Following <ref type=\"bibr\" target=\"#b25\">Hsu et al. (2021)</ref> (and as illustrated in Figure <ref type=\"figu  amount of web-based data from 101 languages (we use the base version). We fine-tuned MT5 following <ref type=\"bibr\" target=\"#b25\">(Hsu et al., 2021)</ref>: for each sample, we give the model the ques. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b29\">(Kondratyuk and Straka, 2019)</ref> and perform crosslingual transfer <ref type=\"bibr\" target=\"#b45\">(Pires et al., 2019)</ref> using annotated data in one language to pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vers. We could have chosen to use the recently released multilingual Dense passage Retrieval (mDPR) <ref type=\"bibr\" target=\"#b3\">(Asai et al., 2021b)</ref>. We decided not to for the two following re that outperforms other approaches in many settings (including dense retrievers). Second, as seen in <ref type=\"bibr\" target=\"#b3\">(Asai et al., 2021b)</ref>, multilingual dense retrievers usually retr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b29\">(Kondratyuk and Straka, 2019)</ref> and perform crosslingual transfer <ref type=\"bibr\" target=\"#b45\">(Pires et al., 2019)</ref> using annotated data in one language to pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: map a question in one language to a question in English for which an answer has already been found. <ref type=\"bibr\" target=\"#b2\">Asai et al. (2021a)</ref> showed that extracting relevant passages fro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cy in table 14.</p><p>As seen in previous work discussing the automatic evaluation of QA systems by <ref type=\"bibr\" target=\"#b10\">Chaganty et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b11\">Chen . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b57\">Thakur et al. (2021)</ref>, it is competitive with DPR-based models <ref type=\"bibr\" target=\"#b28\">(Karpukhin et al., 2020)</ref> and it outperforms DPR across a great . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g can be used to answer questions in one language using information retrieved from other languages. <ref type=\"bibr\" target=\"#b15\">Da San Martino et al. (2017)</ref> showed how cross-language tree ker. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cy in table 14.</p><p>As seen in previous work discussing the automatic evaluation of QA systems by <ref type=\"bibr\" target=\"#b10\">Chaganty et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b11\">Chen . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >. In general, large language models have been shown to have a potential to amplify societal biases <ref type=\"bibr\" target=\"#b7\">(Bender et al., 2021)</ref>, and might leak information about the data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, RNNs <ref type=\"bibr\" target=\"#b23\">(Hochreiter &amp; Schmidhuber, 1997)</ref>, and Transformers <ref type=\"bibr\" target=\"#b54\">(Vaswani et al., 2017)</ref>. For GNNs, the position of nodes is more ref type=\"bibr\" target=\"#b63\">Zhou et al. (2020)</ref> for a review. As Transformer neural networks <ref type=\"bibr\" target=\"#b54\">(Vaswani et al., 2017)</ref> are a special case of MP-GNNs <ref type= ation usually propose to concatenate the PE with the input node features, similarly to Transformers <ref type=\"bibr\" target=\"#b54\">(Vaswani et al., 2017)</ref>:</p><formula xml:id=\"formula_3\">MP-GNNs- >, RNNs <ref type=\"bibr\" target=\"#b23\">(Hochreiter &amp; Schmidhuber, 1997)</ref>, and Transformers <ref type=\"bibr\" target=\"#b54\">(Vaswani et al., 2017)</ref>. These architectures integrate structura ; Bresson, 2021)</ref>. They also naturally generalize the positional encoding used in Transformers <ref type=\"bibr\" target=\"#b54\">(Vaswani et al., 2017)</ref> to arbitrary graphs. The main limitation ns are necessary.</p><p>To overcome these limitations, it seems natural to use Transformer networks <ref type=\"bibr\" target=\"#b54\">(Vaswani et al., 2017)</ref> which alleviates the long-range issue as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b62\">(You et al., 2019;</ref><ref type=\"bibr\" target=\"#b18\">Dwivedi et al., 2020;</ref><ref type=\"bibr\" target=\"#b36\">Li et al., 2020b;</ref><ref type=\"bibr\" target=\"#b17\">Dwivedi &amp; B earn this invariance <ref type=\"bibr\" target=\"#b18\">(Dwivedi et al., 2020)</ref>.</p><p>Inspired by <ref type=\"bibr\" target=\"#b36\">Li et al. (2020b)</ref>, we propose RWPE, a PE based on the random wa \u2208 R k ,<label>(10)</label></formula><p>where RW = AD \u22121 is the random walk operator. In contrast of <ref type=\"bibr\" target=\"#b36\">Li et al. (2020b)</ref> which uses the full matrix RW ij for all pair walk based PE initialization (RWPE) is close to one of the Distance Encoding instantiations used in <ref type=\"bibr\" target=\"#b36\">Li et al. (2020b)</ref>. However, we do not require to consider pairw  for the network to learn to be invariant w.r.t the sign of the eigenvectors.</p><p>Other graph PE. <ref type=\"bibr\" target=\"#b36\">Li et al. (2020b)</ref> proposed the use of distance encoding (DE) as >Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Example 3-regular graph with 8 nodes from<ref type=\"bibr\" target=\"#b36\">Li et al. (2020b)</ref> where the nodes are structurally different an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: DGL <ref type=\"bibr\" target=\"#b56\">(Wang et al., 2019)</ref> on standard molecular benchmarks -ZINC <ref type=\"bibr\" target=\"#b26\">(Irwin et al., 2012)</ref>, OGBG-MOLTOX21 and OGBG-MOLPCBA <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e GNNs, such as GatedGCNs <ref type=\"bibr\" target=\"#b8\">(Bresson &amp; Laurent, 2017)</ref> and PNA <ref type=\"bibr\" target=\"#b11\">(Corso et al., 2020)</ref> and fully-connected Transformers-based GNN  al., 2019)</ref>, and additionally some message-passing functions may not be discriminative enough <ref type=\"bibr\" target=\"#b11\">(Corso et al., 2020)</ref>. To this end, k-order Equivariant-GNNs wer N <ref type=\"bibr\" target=\"#b35\">(Li et al., 2020a)</ref>, Principle Neighborhood Aggregation (PNA) <ref type=\"bibr\" target=\"#b11\">(Corso et al., 2020)</ref>, Directional Graph Networks (DGN) <ref typ entation, namely GatedGCN <ref type=\"bibr\" target=\"#b8\">(Bresson &amp; Laurent, 2017)</ref> and PNA <ref type=\"bibr\" target=\"#b11\">(Corso et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.or \uf8ef \uf8f0 \u00b5 \u03c3 max min \uf8f9 \uf8fa \uf8fb,<label>(26)</label></formula><p>where is the principal aggregator designed in <ref type=\"bibr\" target=\"#b11\">(Corso et al., 2020)</ref>, LReLU stands for LeakyReLU activation, am ng functions may not be discriminative enough <ref type=\"bibr\" target=\"#b60\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b11\">Corso et al., 2020)</ref>.</p><p>Equivariant GNNs. Graph Isomorphism . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2021;</ref><ref type=\"bibr\" target=\"#b9\">Cappart et al., 2021)</ref> and medical diagnosis <ref type=\"bibr\" target=\"#b37\">(Li et al., 2020c)</ref>.</p><p>Most GNNs (such as <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed for a graph is its constrained solubility which is a vital chemical property in molecular design <ref type=\"bibr\" target=\"#b27\">(Jin et al., 2018)</ref>. We use the 12,000 subset of the dataset wit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tics. This is in contrast with most existing architectures s.a.</p><p>Dwivedi &amp; Bresson (2021); <ref type=\"bibr\" target=\"#b3\">Beani et al. (2021)</ref>; <ref type=\"bibr\" target=\"#b30\">Kreuzer et a n (PNA) <ref type=\"bibr\" target=\"#b11\">(Corso et al., 2020)</ref>, Directional Graph Networks (DGN) <ref type=\"bibr\" target=\"#b3\">(Beani et al., 2021)</ref> and Parameterized Hypercomplex GNNs (PHC-GN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 20c)</ref>.</p><p>Most GNNs (such as <ref type=\"bibr\" target=\"#b14\">Defferrard et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b52\">Sukhbaatar et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b29\">Kipf &. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed for a graph is its constrained solubility which is a vital chemical property in molecular design <ref type=\"bibr\" target=\"#b27\">(Jin et al., 2018)</ref>. We use the 12,000 subset of the dataset wit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of the node i.</p><p>The design of functions f h and f e depends on the GNN architecture used, see <ref type=\"bibr\" target=\"#b63\">Zhou et al. (2020)</ref> for a review. As Transformer neural networks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: medical diagnosis <ref type=\"bibr\" target=\"#b37\">(Li et al., 2020c)</ref>.</p><p>Most GNNs (such as <ref type=\"bibr\" target=\"#b14\">Defferrard et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b52\">Sukhba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: DGL <ref type=\"bibr\" target=\"#b56\">(Wang et al., 2019)</ref> on standard molecular benchmarks -ZINC <ref type=\"bibr\" target=\"#b26\">(Irwin et al., 2012)</ref>, OGBG-MOLTOX21 and OGBG-MOLPCBA <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and distributed local algorithms <ref type=\"bibr\" target=\"#b45\">(Naor &amp; Stockmeyer, 1995;</ref><ref type=\"bibr\" target=\"#b49\">Sato et al., 2019)</ref>. In order to address the issue of anonymous . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pletely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model <ref type=\"bibr\" target=\"#b66\">(Raffel et al., 2020;</ref><ref type=\"bibr\" target=\"#b43\">Lester et a  mixture with multiple prompts per dataset. We then train a variant of the T5 encoder-decoder model <ref type=\"bibr\" target=\"#b66\">(Raffel et al., 2020;</ref><ref type=\"bibr\" target=\"#b43\">Lester et a nguage modeling has repeatedly been shown to be a dramatically more effective pre-training strategy <ref type=\"bibr\" target=\"#b66\">(Raffel et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Baevski et a ge response to natural language input. The development of text-to-text pretrained models such as T5 <ref type=\"bibr\" target=\"#b66\">(Raffel et al., 2020)</ref> makes prompts a particularly useful metho oder language model pretrained with a masked language modeling-style objective on 1T tokens from C4 <ref type=\"bibr\" target=\"#b66\">(Raffel et al., 2020)</ref>. Since T5's pretraining objective involve f our training datasets varies by two orders of magnitude. We therefore follow the strategy used in <ref type=\"bibr\" target=\"#b66\">Raffel et al. (2020)</ref> and treat any dataset with over 500'000 ex taset. We feed the model input and target sequences of 1024 and 256 tokens, respectively. Following <ref type=\"bibr\" target=\"#b66\">Raffel et al. (2020)</ref>, we use packing to combine multiple traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n hypothesized that this is a consequence of implicit multitask learning in language model training <ref type=\"bibr\" target=\"#b65\">(Radford et al., 2019)</ref>. Can zero-shot generalization instead be ge language models generalize to new tasks as a result of an implicit process of multitask learning <ref type=\"bibr\" target=\"#b65\">(Radford et al., 2019)</ref>. As a byproduct of learning to predict t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bility of our model to recognize gender biases, we evaluate our models using the WinoGender Schemas <ref type=\"bibr\" target=\"#b72\">(Rudinger et al., 2018)</ref> (also called AX-g under SuperGLUE) and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o explore few-shot and zero-shot generalization to new datasets with large pretrained models (e.g., <ref type=\"bibr\" target=\"#b86\">Vu et al., 2020;</ref><ref type=\"bibr\" target=\"#b95\">Ye et al., 2021) based on conventions in the literature <ref type=\"bibr\" target=\"#b35\">(Khashabi et al., 2020b;</ref><ref type=\"bibr\" target=\"#b86\">Vu et al., 2020;</ref><ref type=\"bibr\" target=\"#b95\">Ye et al., 2021). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b72\">(Rudinger et al., 2018)</ref> (also called AX-g under SuperGLUE) and CrowS-Pairs <ref type=\"bibr\" target=\"#b56\">(Nangia et al., 2020)</ref>. WinoGender Schemas are minimal pairs of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o explore few-shot and zero-shot generalization to new datasets with large pretrained models (e.g., <ref type=\"bibr\" target=\"#b86\">Vu et al., 2020;</ref><ref type=\"bibr\" target=\"#b95\">Ye et al., 2021) based on conventions in the literature <ref type=\"bibr\" target=\"#b35\">(Khashabi et al., 2020b;</ref><ref type=\"bibr\" target=\"#b86\">Vu et al., 2020;</ref><ref type=\"bibr\" target=\"#b95\">Ye et al., 2021). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> this data is precisely supervised training data for the task of closed-book question answering <ref type=\"bibr\" target=\"#b69\">(Roberts et al., 2020)</ref>. Given the scale of large language model losed-book QA as a distinct task, which largely evaluates a model's memorization of world knowledge <ref type=\"bibr\" target=\"#b69\">(Roberts et al., 2020)</ref>. The distinction between commonsense and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ists' deductive inference and natural pragmatic inference is not clearly drawn in most NLI datasets <ref type=\"bibr\" target=\"#b61\">(Pavlick and Kwiatkowski, 2019)</ref>. Additionally, coreference and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ists' deductive inference and natural pragmatic inference is not clearly drawn in most NLI datasets <ref type=\"bibr\" target=\"#b61\">(Pavlick and Kwiatkowski, 2019)</ref>. Additionally, coreference and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b10\">Brown et al. (2020)</ref>. One additional exception is the LAMBADA dataset <ref type=\"bibr\" target=\"#b59\">(Paperno et al., 2016)</ref>, which <ref type=\"bibr\" target=\"#b10\">Br. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: u et al. (2020)</ref>.</p><p>Attention Similar to the multi-head attention mechanism of Transformer <ref type=\"bibr\" target=\"#b58\">(Vaswani et al., 2017)</ref>, the Attention(\u2022, \u2022, \u2022) operator in our  get=\"#b48\">(Schuster &amp; Paliwal, 1997)</ref>   number of model parameters. On top of Transformer <ref type=\"bibr\" target=\"#b58\">(Vaswani et al., 2017)</ref>, considerable efforts have been devoted . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al., 2020)</ref> introduced span-level masks rather than just relying on token-level masks. ELECTRA <ref type=\"bibr\" target=\"#b7\">(Clark et al., 2020)</ref> proposed to detect token replacement as opp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b28\">(Lewis et al., 2019;</ref><ref type=\"bibr\" target=\"#b52\">Song et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">Joshi et al., 2020)</ref> introduced span-level masks rather than jus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: u et al., 2020)</ref>, machine translation <ref type=\"bibr\" target=\"#b66\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b57\">Tu et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Jitao et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b47\">(Scarselli et al., 2008;</ref><ref type=\"bibr\" target=\"#b25\">Kipf &amp; Welling, 2016;</ref><ref type=\"bibr\" target=\"#b15\">Hamilton et al., 2017)</ref>. GNNs have demonstrated effectiveness in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ON</head><p>Language modeling (LM) is a basic and long-standing task in natural language processing <ref type=\"bibr\" target=\"#b49\">(Shannon, 2001;</ref><ref type=\"bibr\" target=\"#b2\">Bahl et al., 1983;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e different sets of parameters for different node types \u03c4 (\u2022) and different edge types \u03c6(\u2022) akin to <ref type=\"bibr\" target=\"#b16\">Hu et al. (2020)</ref>.</p><p>Attention Similar to the multi-head att. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: NN Retrieval</head><p>In order to reduce memory usage and time complexity, in practice we use FAISS <ref type=\"bibr\" target=\"#b19\">(Johnson et al., 2019)</ref> for fast approximate kNN search. Concret ote xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\">In practice, we use FAISS<ref type=\"bibr\" target=\"#b19\">(Johnson et al., 2019)</ref> for fast approximate kNN search.</note> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onal co-efficient for each atom pair, where each atom belongs to a different drug.</p><p>Meanwhile, <ref type=\"bibr\" target=\"#b12\">[13]</ref> addressed a similar problem, drug-target prediction, using get=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Regarding the prediction of DTIs, <ref type=\"bibr\" target=\"#b12\">[13]</ref> constructed a network comprising of drugs, different types. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es. It is used in a wide range of applications including the study of research publication networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>, recommendation <ref type=\"bibr\" target=\"#b14\">[15]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orks exist in multiple disciplines such as social networks <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, citation networks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nvolutional network consisting of encoding, decoding, and model training phases for DDI prediction. <ref type=\"bibr\" target=\"#b10\">[11]</ref> utilized GNN to learn drugs and their neighborhood embeddi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ossible DDIs, we need to incorporate multiple data sources <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>\u2022 Imbalanced dataset: Drug-drug interaction data is i rug-centric interactions. Afterward, they employ a neural network to extract relations among drugs. <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructed knowledge graph based on protein-protein interac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d and skewed, which we need to consider in the experiments <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>\u2022 Lack of predicting for new drugs: Most previous wor nt drugs may not give the same effectiveness for new drugs <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>In this work, we propose a novel DDI prediction model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nvolutional network consisting of encoding, decoding, and model training phases for DDI prediction. <ref type=\"bibr\" target=\"#b10\">[11]</ref> utilized GNN to learn drugs and their neighborhood embeddi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ossible DDIs, we need to incorporate multiple data sources <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>\u2022 Imbalanced dataset: Drug-drug interaction data is i rug-centric interactions. Afterward, they employ a neural network to extract relations among drugs. <ref type=\"bibr\" target=\"#b1\">[2]</ref> constructed knowledge graph based on protein-protein interac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and biological networks <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>Next, to model drugs and their interaction with other. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orks exist in multiple disciplines such as social networks <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, citation networks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b14\">[15]</ref>, multi-network link prediction <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b17\">[18]</ref>, co-authorship prediction <ref type=\"bibr\" target=\"#b18\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed to partition the graph with the Dominant Sets algorithm <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. Because DANTE computes context in tandem with a dyad, it r using a Graph Neural Network (GNN), followed by the application of the Dominant Sets (DS) algorithm <ref type=\"bibr\" target=\"#b22\">[23]</ref>. To this end, we first create a fullyconnected interaction matrix \ud835\udc34 \ud835\udc56 for the graph corresponding to the set \ud835\udc43 \ud835\udc56 . \ud835\udc34 \ud835\udc56 is then passed through the DS algorithm <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which iteratively groups graph nodes into clusters by max. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s a wide range of applications, including video surveillance <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, displays and exhib roup detection has traditionally been approached by hand-crafted heuristics and mathematical models <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta \"#b39\">40]</ref>. These affinities are used to partition the graph with the Dominant Sets algorithm <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. Because DANTE com d><p>We consider three methods in our evaluation:</p><p>(1) Dist. Hand-crafted baseline inspired by <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. The method comput s to group together these individuals. To combat this problem, we use the DS stopping criteria from <ref type=\"bibr\" target=\"#b14\">[15]</ref> to consider the global context of the complete graph when  xp \u2212 \ud835\udc51 \ud835\udc56 \ud835\udc57 /2\ud835\udf0e 2 , where \ud835\udc51 \ud835\udc56 \ud835\udc57 is the distance between two participants and \ud835\udf0e = 2 meters, following <ref type=\"bibr\" target=\"#b14\">[15]</ref>. DS is then applied to obtain groupings, as in <ref type=\"  following <ref type=\"bibr\" target=\"#b14\">[15]</ref>. DS is then applied to obtain groupings, as in <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>(2) DANTE. We implement the DANTE neural network <r eural network <ref type=\"bibr\" target=\"#b29\">[30]</ref> in PyTorch and use DS for clustering, as in <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The dyad and context MLPs of DANTE had two layers with 32. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b1\">[2]</ref> for conversational group detection. Inspired by Swofford and colleagues <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we use the GNN to predict pairwise affinities for the gra conversational groups, as shown in Figure <ref type=\"figure\">1</ref>. While Swofford and colleagues <ref type=\"bibr\" target=\"#b29\">[30]</ref> used a Deep Set <ref type=\"bibr\" target=\"#b24\">[25,</ref>< \">14,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. In particular, the approach by Swofford et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref>, called DANTE, outperformed several traditional approaches -crafted feature transformations to preserve rotation and translation invariance.</p><p>While DANTE <ref type=\"bibr\" target=\"#b29\">[30]</ref> mainly reasons about information encoded in the nodes of a from still images <ref type=\"bibr\" target=\"#b27\">[28]</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref> commonly consider datasets with a limited number of people ding to pairwise affinities. We train the GNN using binary cross-entropy on these affinities, as in <ref type=\"bibr\" target=\"#b29\">[30]</ref>.</p><p>Finally, we aggregate the pairwise affinities outpu n <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>(2) DANTE. We implement the DANTE neural network <ref type=\"bibr\" target=\"#b29\">[30]</ref> in PyTorch and use DS for clustering, as in <ref type=\"bib  node features, transforming each feature into a coordinate frame centered between each dyad, as in <ref type=\"bibr\" target=\"#b29\">[30]</ref>. For MatchNMingle, it uses the position, transformed by th nes. In particular, the proposed GNN-based model outperformed the previous stateof-the-art approach <ref type=\"bibr\" target=\"#b29\">[30]</ref> on the complex MatchNMingle dataset with all types of data ter generalization <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. In particular, the approach by Swofford et al. <ref type=\" relative simplicity, high quality features, and popularity <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>.</p><p>MatchNMingle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es. In this respect, future work could explore using attention mechanisms to fuse data, e.g., as in <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onal groups in a scene in advance. Lastly, many models (e.g. <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>) for social interaction analysis are designed and evaluated. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, co-located collaboration <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and interactive playgrounds <ref type=\"bibr\" target=\"#b16. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>For Cocktail Party, the GNN uses distance and both angles, transformed into point pair features <ref type=\"bibr\" target=\"#b8\">[9]</ref>, as edge features. For MatchN-Mingle, it uses distance for e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in comparison to DANTE.</p><p>Prior benchmarks in conversational group detection from still images <ref type=\"bibr\" target=\"#b27\">[28]</ref><ref type=\"bibr\" target=\"#b28\">[29]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>For Cocktail Party, the GNN uses distance and both angles, transformed into point pair features <ref type=\"bibr\" target=\"#b8\">[9]</ref>, as edge features. For MatchN-Mingle, it uses distance for e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, displays and exhibits <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, co-located collabo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onal groups in a scene in advance. Lastly, many models (e.g. <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>) for social interaction analysis are designed and evaluated. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >22]</ref>. Group detection can also enable better spoken language interaction with situated agents <ref type=\"bibr\" target=\"#b3\">[4]</ref>, non-verbal robot behavior generation <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b33\">[34]</ref>, and socially aware robot navigation in human environments <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>Similar to prior work, we approach the problem of c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b20\">[21]</ref>, and interactive playgrounds <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. Group detection can also enable better spoken language int. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: advancements in machine learning have enabled improved social awareness with greater generalization <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b20\">[21]</ref>, and interactive playgrounds <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. Group detection can also enable better spoken language int. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>For Cocktail Party, the GNN uses distance and both angles, transformed into point pair features <ref type=\"bibr\" target=\"#b8\">[9]</ref>, as edge features. For MatchN-Mingle, it uses distance for e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  head orientation information for each individual, and we also consider their body orientation from <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Conversational groups are labeled for 320 frames. We use  mples, but this information could improve model prediction <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Thus, future work could explore detecting groups across mu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he embedding by passing a 32 \u00d7 32 cropped section of the recorded image around the person to ResNet <ref type=\"bibr\" target=\"#b19\">[20]</ref> and extracting the 512 features in the penultimate layer o re are several possible explanations for this phenomenon. First, the image features from the ResNet <ref type=\"bibr\" target=\"#b19\">[20]</ref> model could have been too deep in the network. Low-level f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, co-located collaboration <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and interactive playgrounds <ref type=\"bibr\" target=\"#b16. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, displays and exhibits <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, co-located collabo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b33\">[34]</ref>, and socially aware robot navigation in human environments <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>Similar to prior work, we approach the problem of c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, our problem differs in several key ways. Methods such as < ep learning require information about the number of clusters <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>; however, we do not know the number of conversational group. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b28\">29]</ref>, displays and exhibits <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, co-located collaboration <ref type=\"bibr\" target=\"#b20\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b20\">[21]</ref>, and interactive playgrounds <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. Group detection can also enable better spoken language int. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> commonly consider datasets with a limited number of people, e.g., the Cocktail Party dataset <ref type=\"bibr\" target=\"#b40\">[41]</ref> considers six people. Given the relative simplicity of the iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Datasets</head><p>Cocktail Party Dataset <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The dataset contains 30 minutes of interactions among six. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e\">1</ref>. While Swofford and colleagues <ref type=\"bibr\" target=\"#b29\">[30]</ref> used a Deep Set <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> architecture to ag ffinities by combining the dyad node features with context aggregated using a Deep Set architecture <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. These affinities . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plore using attention mechanisms to fuse data, e.g., as in <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>.</p><p>We processed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b33\">[34]</ref>, and socially aware robot navigation in human environments <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>Similar to prior work, we approach the problem of c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>For Cocktail Party, the GNN uses distance and both angles, transformed into point pair features <ref type=\"bibr\" target=\"#b8\">[9]</ref>, as edge features. For MatchN-Mingle, it uses distance for e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad><p>Conversational group detection has a wide range of applications, including video surveillance <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: athematical models <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. However, advancements in machine learning have enabled imp es, and popularity <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>.</p><p>MatchNMingle Dataset <ref type=\"bibr\" target=\"#b4\">[ .tei-c.org/ns/1.0\"><head n=\"4.3\">Results</head><p>Our main evaluation metric is the Group F1 metric <ref type=\"bibr\" target=\"#b31\">[32]</ref>. For a threshold \ud835\udc47 , the Group F1 metric considers a groun ral correlation of data across dataset samples, but this information could improve model prediction <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Thus, future work. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  with situated agents <ref type=\"bibr\" target=\"#b3\">[4]</ref>, non-verbal robot behavior generation <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and socially aware robot navigation in human environments. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hains for critical loads in order to increase MLP. The work most similar to CDF is Precise Runahead <ref type=\"bibr\" target=\"#b18\">[20]</ref>. Precise Runahead runs dependence chains corresponding to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ance of this design significantly.</p><p>Slice OoO execution <ref type=\"bibr\" target=\"#b3\">[5,</ref><ref type=\"bibr\" target=\"#b12\">14]</ref> tries to extract MLP from InO cores by executing load slice. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nstructions. This includes purely compiler-based solutions <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b29\">31]</ref>, work on Speculativ instructions. This includes purely compiler-based solutions<ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b29\">31]</ref>, work on Speculativ does not improve performance. Moreover, load hoisting is limited by architectural register pressure <ref type=\"bibr\" target=\"#b28\">[30]</ref>; CDF does not have this problem since critical instruction. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g/ns/1.0\"><head n=\"2.4\">Comparison Against Runahead and Compiler Based Techniques</head><p>Runahead <ref type=\"bibr\" target=\"#b6\">[8,</ref><ref type=\"bibr\" target=\"#b7\">9,</ref><ref type=\"bibr\" target cuses on extracting more parallelism from the available window resources.</p><p>Continuous Runahead <ref type=\"bibr\" target=\"#b6\">[8]</ref> runs simple dependence chains on a separate Runahead engine . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ques</head><p>Runahead <ref type=\"bibr\" target=\"#b6\">[8,</ref><ref type=\"bibr\" target=\"#b7\">9,</ref><ref type=\"bibr\" target=\"#b17\">19]</ref> executes dependence chains corresponding to multiple cache . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se Scarab [1], an execution-driven cycle-accurate x86-64 simulator, to implement CDF, and Ramulator <ref type=\"bibr\" target=\"#b9\">[11]</ref> to model main memory. CACTI <ref type=\"bibr\" target=\"#b16\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance of this design significantly.</p><p>Slice OoO execution <ref type=\"bibr\" target=\"#b3\">[5,</ref><ref type=\"bibr\" target=\"#b12\">14]</ref> tries to extract MLP from InO cores by executing load slice. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se Scarab [1], an execution-driven cycle-accurate x86-64 simulator, to implement CDF, and Ramulator <ref type=\"bibr\" target=\"#b9\">[11]</ref> to model main memory. CACTI <ref type=\"bibr\" target=\"#b16\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b29\">31]</ref>, work on Speculative Multi-threading <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" ta threaded parallelism using separate threads or through pre-computation. Speculative multi-threading <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" ta target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b29\">31]</ref>, work on Speculative Multi-threading<ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  slices out-of-order. CDF instead maximizes the parallelism that can be extracted from an OoO core. <ref type=\"bibr\" target=\"#b0\">[2]</ref> uses instruction criticality and control independence to red. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ultiple critical loads and diverse control flow paths. Unlike CDF, CFP uses a ROB-less architecture <ref type=\"bibr\" target=\"#b1\">[3]</ref> to enable a larger instruction window for critical loads.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: econvergent instruction still need to be re-fetched and dispatched. Selective Branch Recovery (SBR) <ref type=\"bibr\" target=\"#b17\">[18]</ref> can be applied when the recovergence point is the start of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: back mechanisms by relying on compiler hints to denote control and data independent regions. NOREBA <ref type=\"bibr\" target=\"#b18\">[19]</ref> uses a similar strategy: compiler inserted instructions in pendence compiler analysis have been discussed in prior work <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, which means an automatic compiler implementation is realis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b38\">[39]</ref> was launched.</p><p>As input graphs, we use synthetically generated RMAT graphs <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Synthetic graphs have the advantage that we can control the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed to spawn threads for control and data independent regions <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, which avoids flushing instructions of other regions on a mis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he correct path is already fetched, while our mechanism can handle all branch misses. Naresh et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref> propose to only reuse convergent instructions in the front. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ogy improvements, providing more transistors and frequency boosts without increasing energy density <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Architecture desi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graph accelerators <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref> and graph analysis . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lgorithms, such as graph analysis <ref type=\"bibr\" target=\"#b16\">[17]</ref>, sparse neural networks <ref type=\"bibr\" target=\"#b44\">[44]</ref> and graph neural networks <ref type=\"bibr\" target=\"#b42\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplications evaluated in our study (see Section 5.1) using a state-of-the-art TAGE branch predictor <ref type=\"bibr\" target=\"#b37\">[38]</ref>, wrong path instructions account for on average 53% more d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to avoid performance inversion of predicating instructions <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Additionally, predication options are limited in current a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: We configure the simulator to resemble an Intel \u00ae Xeon \u00ae Platinum 8180 processor, codenamed Skylake <ref type=\"bibr\" target=\"#b13\">[14]</ref>, see Table <ref type=\"table\" target=\"#tab_0\">1</ref>. We e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the pipeline design, but only reuse a small fraction of the convergent instructions.</p><p>Skipper <ref type=\"bibr\" target=\"#b10\">[11]</ref> belongs to the first category: it skips instructions that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  example of the second category (small additions, limited benefit) is the proposal by Roth and Sohi <ref type=\"bibr\" target=\"#b35\">[36]</ref>. They keep outcomes of squashed instructions and reuse the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b38\">[39]</ref> was launched.</p><p>As input graphs, we use synthetically generated RMAT graphs <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Synthetic graphs have the advantage that we can control the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to avoid performance inversion of predicating instructions <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Additionally, predication options are limited in current a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: branch reconvergence and instruction dependence compiler analysis have been discussed in prior work <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, which means an auto. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: umption in embedded processors, which could potentially be saved by not flushing them. Malik et al. <ref type=\"bibr\" target=\"#b28\">[29]</ref> discuss the performance benefit of parallel branch resolut. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s along the mispredicted path. Conceptually, there are two main methods to restore the rename table <ref type=\"bibr\" target=\"#b2\">[3]</ref>:</p><p>(a) Using checkpoints: At each branch instruction, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thin a thread, multithreading can be used to spawn threads for control and data independent regions <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, which avoids flushing. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: umption in embedded processors, which could potentially be saved by not flushing them. Malik et al. <ref type=\"bibr\" target=\"#b28\">[29]</ref> discuss the performance benefit of parallel branch resolut. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, sparse neural networks <ref type=\"bibr\" target=\"#b44\">[44]</ref> and graph neural networks <ref type=\"bibr\" target=\"#b42\">[42]</ref>, operate on sparse and irregular data, resulting in high b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ry accesses, speeding up branch resolution and thus indirectly reducing the branch penalty. Pipette <ref type=\"bibr\" target=\"#b31\">[32]</ref> exploits pipeline parallelism in irregular applications an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b36\">[38]</ref>, and numerous kernel modules <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b44\">46]</ref>. Such deep stacks r ne such mechanism that is pervasively explored in academia <ref type=\"bibr\" target=\"#b40\">[42,</ref><ref type=\"bibr\" target=\"#b43\">45,</ref><ref type=\"bibr\" target=\"#b44\">46]</ref> and industry <ref t ed BTBs. Furthermore, we show that the state-of-the-art BTB prefetching techniques, such as Shotgun <ref type=\"bibr\" target=\"#b43\">[45]</ref> and Confluence <ref type=\"bibr\" target=\"#b38\">[40]</ref>,  plications, while reducing 65.4% of all BTB misses. Compared to the state-of-the-art BTB prefetcher <ref type=\"bibr\" target=\"#b43\">[45]</ref>, Twig achieves an average 19.82% (up to 139.8%) greater sp tial of FDIP and find that its performance is mainly limited by BTB misses. We then analyze Shotgun <ref type=\"bibr\" target=\"#b43\">[45]</ref> and Confluence <ref type=\"bibr\" target=\"#b38\">[40]</ref>,  anches, but incur 37.5% of all BTB misses. This result justifies the design decisions of prior work <ref type=\"bibr\" target=\"#b43\">[45]</ref> that partitions the BTB structure to prefetch conditional   state-of-the-art BTB prefetchers, Confluence <ref type=\"bibr\" target=\"#b38\">[40]</ref> and Shotgun <ref type=\"bibr\" target=\"#b43\">[45]</ref>.</p><p>Confluence observes that although the I-cache and t to decode (potentially variablelength) instructions. Hardware-based BTB prefetchers such as Shotgun <ref type=\"bibr\" target=\"#b43\">[45]</ref> hence need to prefetch the instructions and decode them be g's speedup to the speedup offered by an ideal BTB and the state-of-the-art BTB prefetcher, Shotgun <ref type=\"bibr\" target=\"#b43\">[45]</ref>. Then, we evaluate the individual speedup contributions of upon the frontend to run far enough ahead, and miss coverage suffers when there are many BTB misses <ref type=\"bibr\" target=\"#b43\">[45]</ref>. Shotgun <ref type=\"bibr\" target=\"#b43\">[45]</ref> partiti  coverage suffers when there are many BTB misses <ref type=\"bibr\" target=\"#b43\">[45]</ref>. Shotgun <ref type=\"bibr\" target=\"#b43\">[45]</ref> partitions the BTB into the Unconditional BTB (U-BTB) and  nism for data center applications. Unlike prior techniques <ref type=\"bibr\" target=\"#b38\">[40,</ref><ref type=\"bibr\" target=\"#b43\">45]</ref>, Twig does not require any modifications to the typical BTB  techniques that rely on limited past run-time information <ref type=\"bibr\" target=\"#b38\">[40,</ref><ref type=\"bibr\" target=\"#b43\">45]</ref>, Twig determines which branch instructions cause frequent B ignificantly more benefit than state-of-the-art mechanisms <ref type=\"bibr\" target=\"#b38\">[40,</ref><ref type=\"bibr\" target=\"#b43\">45]</ref> even while using profiles from a different application inpu above proposals, FDIP has a desirable trade-off between metadata cost and prefetching effectiveness <ref type=\"bibr\" target=\"#b43\">[45,</ref><ref type=\"bibr\" target=\"#b44\">46]</ref>. Even with signifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b49\">51,</ref><ref type=\"bibr\" target=\"#b56\">58,</ref><ref type=\"bibr\" target=\"#b61\">63,</ref><ref type=\"bibr\" target=\"#b65\">67,</ref><ref type=\"bibr\" target=\"#b88\">90]</ref>. Improved layout te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  composed of complex application logic <ref type=\"bibr\" target=\"#b54\">[56]</ref>, diverse libraries <ref type=\"bibr\" target=\"#b36\">[38]</ref>, and numerous kernel modules <ref type=\"bibr\" target=\"#b14 ssociativity since data center applications' instruction footprints grow in an unprecedented manner <ref type=\"bibr\" target=\"#b36\">[38]</ref>. Therefore, we conclude that BTB prefetching is a more fut eep stacks result in multi-megabyte instruction footprints <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" target=\"#b57\">59</ref>] that easily exhaust enter environments <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" target=\"#b56\">58]</ref>.</p><p>Twig introdu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b60\">62]</ref>. The basic-block style BTB also contains the address of the fall-through basic block <ref type=\"bibr\" target=\"#b87\">[89]</ref>. Compressing BTB entry size is common to enable the BTB to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: record and replay\" <ref type=\"bibr\" target=\"#b23\">[25,</ref><ref type=\"bibr\" target=\"#b24\">26,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref>) technique, to perform both I-cache and BTB prefetching. Wh [25,</ref><ref type=\"bibr\" target=\"#b24\">26]</ref>, or they end up being significantly more complex <ref type=\"bibr\" target=\"#b37\">[39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" ta anch predictor unit. Moreover, Confluence relied on a metadata-expensive temporal prefetcher, SHIFT <ref type=\"bibr\" target=\"#b37\">[39,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e and a longer latency access time for important branch prediction metadata. Two-level bulk preload <ref type=\"bibr\" target=\"#b16\">[18]</ref> maintains two BTB levels per-core, with a mechanism to fet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g instruction locality via basic block/function reordering <ref type=\"bibr\" target=\"#b55\">[57,</ref><ref type=\"bibr\" target=\"#b63\">65]</ref>, hot/cold splitting <ref type=\"bibr\" target=\"#b21\">[23]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[15,</ref><ref type=\"bibr\" target=\"#b17\">19,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b29\">31,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b25\">27,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b50\">52,</ref><ref type=\"bibr\" target=\"#b52\">54,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r not. This configuration lowers the overall IPC gains from 14.4% to 13.4%. In a decoupled frontend <ref type=\"bibr\" target=\"#b24\">[26,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>, delaying the BTB  ertain offset from the unconditional branch are prefetched. Prefetching into ICache, on top of FDIP <ref type=\"bibr\" target=\"#b24\">[26]</ref>, also pollutes the ICache entries due to the high speculat  the OOO core we study. Modern OOO cores incorporate a fetch-directed instruction prefetching (FDIP)<ref type=\"bibr\" target=\"#b24\">[26]</ref> pipeline. On BTB misses/mispredictions, resteering happen . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: re <ref type=\"figure\" target=\"#fig_12\">12a</ref>, we show that the state of art BTB design, Shotgun <ref type=\"bibr\" target=\"#b33\">[35]</ref>, brings only about 2.7% IPC speedup over the baseline BTB  aseline. Prefetching the conditionals following the return was modelled similar to what was done in <ref type=\"bibr\" target=\"#b33\">[35]</ref>. Several factors contribute to Shotgun's lower gains. Thes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he high speculation in the front-end. Confluence <ref type=\"bibr\" target=\"#b27\">[29]</ref> and SN4L <ref type=\"bibr\" target=\"#b3\">[5]</ref> operate at the cache line granularity and store the branch i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Figure <ref type=\"figure\" target=\"#fig_11\">11c</ref>, we study PDede in a 2-level BTB configuration <ref type=\"bibr\" target=\"#b20\">[22]</ref>.</p><p>We study a baseline configuration with Level0 BTB a lementary to these works. Recent papers from Samsung and IBM <ref type=\"bibr\" target=\"#b0\">[2,</ref><ref type=\"bibr\" target=\"#b20\">22]</ref> have shown the significant investments made in BTB storage . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: own performance problem across several usage scenarios including web-scale data center applications <ref type=\"bibr\" target=\"#b4\">[6,</ref><ref type=\"bibr\" target=\"#b14\">16,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Figure <ref type=\"figure\" target=\"#fig_11\">11c</ref>, we study PDede in a 2-level BTB configuration <ref type=\"bibr\" target=\"#b20\">[22]</ref>.</p><p>We study a baseline configuration with Level0 BTB a lementary to these works. Recent papers from Samsung and IBM <ref type=\"bibr\" target=\"#b0\">[2,</ref><ref type=\"bibr\" target=\"#b20\">22]</ref> have shown the significant investments made in BTB storage . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Several proposals <ref type=\"bibr\" target=\"#b13\">[15,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plicating <ref type=\"bibr\">[12-14, 24, 47, 52]</ref> the data or by proposing efficient compression <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b19\">21]</ref>. Such compression te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5.6\">Impact of Indirect Branches</head><p>Modern cores incorporate an Indirect Target TAGE (ITTAGE) <ref type=\"bibr\" target=\"#b43\">[45]</ref> predictor to predict the target of indirect branches. The . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he high speculation in the front-end. Confluence <ref type=\"bibr\" target=\"#b27\">[29]</ref> and SN4L <ref type=\"bibr\" target=\"#b3\">[5]</ref> operate at the cache line granularity and store the branch i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: own performance problem across several usage scenarios including web-scale data center applications <ref type=\"bibr\" target=\"#b4\">[6,</ref><ref type=\"bibr\" target=\"#b14\">16,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntroduced Table <ref type=\"table\">3</ref>: Simulator Parameters in Section 3. We leverage Simpoints <ref type=\"bibr\" target=\"#b21\">[23]</ref> to identify the regions of interest and run detailed simul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tadata for implementing a replacement policy (e.g., Static Re-Reference Interval Prediction (SRRIP) <ref type=\"bibr\" target=\"#b25\">[27]</ref>) and maintaining counters that indicate the confidence in  l, avoiding invalid entries in the BTBM. Allocations in the Region-BTB and Page-BTB are SRRIPguided <ref type=\"bibr\" target=\"#b25\">[27]</ref>. SRRIP is sufficient as an entry shared across multiple of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6]</ref>, also pollutes the ICache entries due to the high speculation in the front-end. Confluence <ref type=\"bibr\" target=\"#b27\">[29]</ref> and SN4L <ref type=\"bibr\" target=\"#b3\">[5]</ref> operate a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.4\">PDede access latency</head><p>Using Cacti7 <ref type=\"bibr\" target=\"#b54\">[56]</ref>, we studied the access latency of the baseline BTB and PDe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[15,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with data accesses <ref type=\"bibr\" target=\"#b30\">[36,</ref><ref type=\"bibr\" target=\"#b32\">38,</ref><ref type=\"bibr\" target=\"#b47\">53,</ref><ref type=\"bibr\" target=\"#b50\">56,</ref><ref type=\"bibr\" tar ched PTEs and the prefetch logic is engaged on STLB misses <ref type=\"bibr\" target=\"#b20\">[26,</ref><ref type=\"bibr\" target=\"#b47\">53,</ref><ref type=\"bibr\" target=\"#b73\">79]</ref>. When an instructio f><ref type=\"bibr\" target=\"#b59\">65]</ref>.</p><p>Furthermore, we show that prior dSTLB prefetchers <ref type=\"bibr\" target=\"#b47\">[53]</ref> are ineffective at capturing the iSTLB misses because (i)  dexed by the PC of the instruction that triggered the STLB miss.</p><p>Distance Prefetcher (DP). DP <ref type=\"bibr\" target=\"#b47\">[53]</ref> correlates patterns with the distance between pages. To do ages.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Markov Prefetcher (MP)</head><p>. MP <ref type=\"bibr\" target=\"#b47\">[53]</ref> targets irregular STLB patterns by building Markov chains   would store the full virtual page number (VPN) in each prediction slot (as the state-of-the-art MP <ref type=\"bibr\" target=\"#b47\">[53]</ref>  . . However, such a design choice is expensive, storage-w e replacement policy of the prediction tables. While previous tablebased dSTLB prefetchers, like MP <ref type=\"bibr\" target=\"#b47\">[53]</ref>, use the LRU policy, we find that LRU does not keep the mo ><p>. SDP prefetches the PTE of the virtual page adjacent to the missed virtual page, similar to SP <ref type=\"bibr\" target=\"#b47\">[53]</ref>. SDP further exploits page table locality to prefetch all  es a single prediction table with a fixed number of successors per entry, as the state-of-theart MP <ref type=\"bibr\" target=\"#b47\">[53]</ref> does. We opt to provide an ISO-storage comparison between  .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Sequential Prefetcher (SP)</head><p>. SP <ref type=\"bibr\" target=\"#b47\">[53,</ref><ref type=\"bibr\" target=\"#b72\">78]</ref> prefetches the PTE .0\"><head>Arbitrary Stride Prefetcher (ASP)</head><p>. ASP <ref type=\"bibr\" target=\"#b25\">[31,</ref><ref type=\"bibr\" target=\"#b47\">53]</ref> targets varying stride patterns. To do so, it uses a predic y 4.1%.</p><p>Prefetching into TLB. Prior STLB prefetchers <ref type=\"bibr\" target=\"#b32\">[38,</ref><ref type=\"bibr\" target=\"#b47\">53]</ref> and patents <ref type=\"bibr\" target=\"#b20\">[26,</ref><ref t nt with prior work <ref type=\"bibr\" target=\"#b21\">[27,</ref><ref type=\"bibr\" target=\"#b32\">38,</ref><ref type=\"bibr\" target=\"#b47\">53]</ref> stating that prefetching directly into the STLB causes poll. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rhead of address translation associated with data accesses <ref type=\"bibr\" target=\"#b30\">[36,</ref><ref type=\"bibr\" target=\"#b32\">38,</ref><ref type=\"bibr\" target=\"#b47\">53,</ref><ref type=\"bibr\" tar are inaccurate. Our results are consistent with prior work <ref type=\"bibr\" target=\"#b21\">[27,</ref><ref type=\"bibr\" target=\"#b32\">38,</ref><ref type=\"bibr\" target=\"#b47\">53]</ref> stating that prefet  Morrigan outperforms the this scenario by 4.1%.</p><p>Prefetching into TLB. Prior STLB prefetchers <ref type=\"bibr\" target=\"#b32\">[38,</ref><ref type=\"bibr\" target=\"#b47\">53]</ref> and patents <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: osed many techniques for reducing the overhead of address translation associated with data accesses <ref type=\"bibr\" target=\"#b30\">[36,</ref><ref type=\"bibr\" target=\"#b32\">38,</ref><ref type=\"bibr\" ta  miss latency. Improving the performance of the MMU-Caches <ref type=\"bibr\" target=\"#b26\">[32,</ref><ref type=\"bibr\" target=\"#b30\">36]</ref> is an effective way to reduce the latency penalty of freque. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is not adequate anymore and there is need for creating transparent support for 1GB pages. Finally, <ref type=\"bibr\" target=\"#b42\">[48]</ref> reveals that huge pages can harm the performance of NUMA m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b30\">[36,</ref><ref type=\"bibr\" target=\"#b32\">38,</ref><ref type=\"bibr\" target=\"#b47\">53,</ref><ref type=\"bibr\" target=\"#b50\">56,</ref><ref type=\"bibr\" target=\"#b54\">60,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: zation <ref type=\"bibr\" target=\"#b58\">[64]</ref> or operating system schemes leveraging large pages <ref type=\"bibr\" target=\"#b37\">[43,</ref><ref type=\"bibr\" target=\"#b53\">59,</ref><ref type=\"bibr\" ta > modify hugetlbfs to place only hot functions in huge pages. Moreover, OS schemes using superpages <ref type=\"bibr\" target=\"#b37\">[43,</ref><ref type=\"bibr\" target=\"#b53\">59,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ould also be effective for instruction TLB misses. Incremental approaches try to increase TLB reach <ref type=\"bibr\" target=\"#b35\">[41,</ref><ref type=\"bibr\" target=\"#b62\">68,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y 8 bytes. As a result, a single 64-byte cache line can accommodate up to 8 contiguouslystored PTEs <ref type=\"bibr\" target=\"#b31\">[37,</ref><ref type=\"bibr\" target=\"#b63\">69,</ref><ref type=\"bibr\" ta  an adversary could exploit this contiguity to attack the system. In addition, Bhattacharjee et al. <ref type=\"bibr\" target=\"#b31\">[37]</ref> propose a shared among cores last-level TLB that exploits . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b24\">[30,</ref><ref type=\"bibr\" target=\"#b26\">32,</ref><ref type=\"bibr\" target=\"#b34\">40,</ref><ref type=\"bibr\" target=\"#b41\">47,</ref><ref type=\"bibr\" target=\"#b48\">54,</ref><ref type=\"bibr\" tar n prefetchers target the L1 I-cache and typically find the needed cache blocks in the L2 or the LLC <ref type=\"bibr\" target=\"#b41\">[47,</ref><ref type=\"bibr\" target=\"#b66\">72]</ref>, which means that   levels of the software stack, making the front-end of the processor a major performance pain point <ref type=\"bibr\" target=\"#b41\">[47]</ref>. Indeed, recent work from Google <ref type=\"bibr\" target=\" prefetch distances and low latencies, as the prefetched blocks are often found in the L2 or the LLC <ref type=\"bibr\" target=\"#b41\">[47]</ref>. In contrast, iSTLB misses require larger prefetch distanc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b5\">7,</ref><ref type=\"bibr\" target=\"#b36\">42]</ref> considers instruction address translation as a bottleneck w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cations on SMT cores for better CPU and memory utilization <ref type=\"bibr\" target=\"#b48\">[54,</ref><ref type=\"bibr\" target=\"#b74\">80]</ref>. To consider colocation, we simulate a dual-threaded SMT co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vations, we define the style codes as several interpretable statistics of 3D morphable model (3DMM) <ref type=\"bibr\" target=\"#b1\">[2]</ref> parameters. Having obtained the style codes of each talking  e second stage as the photorealistic render stage. Between two stages, we apply the 3DMM face model <ref type=\"bibr\" target=\"#b1\">[2]</ref> as a crucial bridge. Therefore, before formally defining the . Based on \ud835\udefd \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 (\ud835\udc61) and \ud835\udc5d \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 (\ud835\udc61), we reconstruct the 3D talking meshes with the 3DMM face model <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>The LSF model leverages a latent fusion mechanism to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  incorporate talking style into the audio-driven talking face synthesis framework. Previous efforts <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> have shown the rati =\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> and the multimodal talking face synthesis <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar l lack in personality because of the overlook of talking style. To address this issue, some methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> proposed to incorpo Augmented GAN model to synthesize stylized talking face with the wild training data. Cudeiro et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> proposed Voice Operated Character Animation (VOCA) model to ed with multiple styles, while the facial movements still lack personality. Further, Cudeiro et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> proposed the Voice Operated Character Animation (VOCA) mode \"6.3\">Comparison with VOCA on Style Synthesis</head><p>To the best of our knowledge, the VOCA model <ref type=\"bibr\" target=\"#b9\">[10]</ref> is the only available method that captures diversified talk ial movements. Therefore, in this section, we systematically compare our method with the VOCA model <ref type=\"bibr\" target=\"#b9\">[10]</ref> to demonstrate the effectiveness of the LSF model. Differen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  incorporate talking style into the audio-driven talking face synthesis framework. Previous efforts <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> have shown the rati =\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> and the multimodal talking face synthesis <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar l lack in personality because of the overlook of talking style. To address this issue, some methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> proposed to incorpo Augmented GAN model to synthesize stylized talking face with the wild training data. Cudeiro et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> proposed Voice Operated Character Animation (VOCA) model to ed with multiple styles, while the facial movements still lack personality. Further, Cudeiro et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> proposed the Voice Operated Character Animation (VOCA) mode \"6.3\">Comparison with VOCA on Style Synthesis</head><p>To the best of our knowledge, the VOCA model <ref type=\"bibr\" target=\"#b9\">[10]</ref> is the only available method that captures diversified talk ial movements. Therefore, in this section, we systematically compare our method with the VOCA model <ref type=\"bibr\" target=\"#b9\">[10]</ref> to demonstrate the effectiveness of the LSF model. Differen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng faces by imitating talking styles from the style codes. Detailedly, the LSF model first dropouts <ref type=\"bibr\" target=\"#b26\">[27]</ref> information from the audio stream to prevent the audio fro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: king face synthesis <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ped into two categories: the unimodal talking face synthesis <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: der and the few-shot neural texture generation model, we leverage the Lip Reading in the Wild (LRW) <ref type=\"bibr\" target=\"#b7\">[8]</ref> dataset. \ud835\udf15\ud835\udc61 )). The predicted facial movements \ud835\udefd \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 (\ud835\udc61) an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: we denote the rendered image as Y \u2032 and the ground truth image as Y. We combine the perceptual loss <ref type=\"bibr\" target=\"#b19\">[20]</ref> and L 1 loss together as L to optimize the neural texture . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, the LSF model firstly takes audio X \ud835\udc51 as input and encodes X \ud835\udc51 with the bottom part of ResNet-50 <ref type=\"bibr\" target=\"#b15\">[16]</ref>, yielding latent audio representation X \ud835\udc59 . Afterwards, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thods: (1) the ATVG framework <ref type=\"bibr\" target=\"#b5\">[6]</ref>, (2) the MakeItTalk framework <ref type=\"bibr\" target=\"#b40\">[41]</ref>, (3) the Wav2Lip framework <ref type=\"bibr\" target=\"#b23\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ssifier to automatically categorize objects <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Jiang et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> was the first to apply their ideas in the emotion recognit  <ref type=\"bibr\" target=\"#b17\">[18]</ref> to get this information. Distinguished from Jiang et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref>, using eye movement features as control conditions, a comp thus enabling automated visual classification in a brain-based visual object manifold. Jiang et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> pioneeringly tested those methods in the emotion recogniti  a few crossmodal models used in the emotion recognition task. Compared with the regressor of Jiang <ref type=\"bibr\" target=\"#b10\">[11]</ref>, it is evident that our model fits the characteristics of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng facial expression <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, eye movements <ref type=\"bibr\" target=\"#b21\">[22]</ref>, E. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: up of researchers focusing on EEG signals were surprised by their potentials in emotion recognition <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. Alarcao et al. <r in the training set.9Optimize \ud835\udc6e and \ud835\udc6b by minimizing Equation<ref type=\"bibr\" target=\"#b4\">(5)</ref>.<ref type=\"bibr\" target=\"#b12\">13</ref> Generate the multimodal feature using trained \ud835\udc6e. 14 Use the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng facial expression <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, eye movements <ref type=\"bibr\" target=\"#b21\">[22]</ref>, E. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ures, i.e., multimodal emotion features. It is notable that the existing vanishing gradient problem <ref type=\"bibr\" target=\"#b8\">[9]</ref> deteriorates the training performance of the generator. This. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive modalities to measure emotions, taking facial expression <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, eye movements <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ls to perform this task because it not only can observe the users' states naturally and efficiently <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> but also is easy to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trained EEG features and therefore use the EEG-based classifier to automatically categorize objects <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Jiang et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> wa mbining an LSTM recurrent neural network with conditional GANs. At the same time, Spampinato et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> conducted an RNN-based method to learn visual stimuli-evok. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onship between eye movement features and multimodal features, we adopt the bimodal deep autoencoder <ref type=\"bibr\" target=\"#b17\">[18]</ref> to get this information. Distinguished from Jiang et al. < motion representations from both EEG and eye movement features, namely the bimodal deep autoencoder <ref type=\"bibr\" target=\"#b17\">[18]</ref> shown in training stage I in Figure <ref type=\"figure\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: racting high-level fusion features with a deep neural network model called bimodal deep autoencoder <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Although multimodal fusion can achieve better resul nition. Besides, Liu et al. attempted to use multimodal deep learning techniques to model this task <ref type=\"bibr\" target=\"#b14\">[15]</ref>. These studies have suggested that modality fusion seems t  compare with our model. As shown in the third <ref type=\"bibr\" target=\"#b15\">[16]</ref> and fourth <ref type=\"bibr\" target=\"#b14\">[15]</ref> rows of Table <ref type=\"table\" target=\"#tab_2\">2</ref>, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eutral). The impressive experimental results show that the feature-level fusion strategy works well <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Liu et al. dramatically advanced the state-of-the-art per /ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Zheng et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> innovatively combined them and examined on both feature-le siological changes <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. Zheng et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> inn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Further, while new GNNs that work better in these non-homophilous settings have been developed <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" ta act, many non-homophilous techniques frequently require more parameters and computational resources <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ ting GNNs implicitly leverage homophily, so they often fail to generalize on non-homophilous graphs <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Indeed, a wide rang us methods. Various GNNs have been proposed to achieve higher performance in low-homophily settings <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" ta her works in non-homophilous graph learning evaluation, we take a high proportion of training nodes <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" ta pe=\"bibr\" target=\"#b81\">[82]</ref> results). Secondly, as suggested by prior theory and experiments <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ the Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> data also studies this setting exclusively <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Using other Faceb om more than one-hop neighbors are important for classification in certain non-homophilous settings <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. One reason why grap mall size, narrow range of application areas, and high variance between different train/test splits <ref type=\"bibr\" target=\"#b81\">[82]</ref>. Consequently, method scalability has not been thoroughly   relieved oversmoothing, which empirically performs better in non-homophilous settings, and H 2 GCN <ref type=\"bibr\" target=\"#b81\">[82]</ref> shows that separation of ego and neighbor embeddings, aggr ling neighbor information from ego information, and combining graph information at different scales <ref type=\"bibr\" target=\"#b81\">[82]</ref>. Many of these design choices require additional overhead  t=\"#b61\">[62,</ref><ref type=\"bibr\" target=\"#b73\">74]</ref> are highly homophilous (see Appendix A) <ref type=\"bibr\" target=\"#b81\">[82]</ref>. Recently, the Open Graph Benchmark <ref type=\"bibr\" targe ng; however, most of the node classification datasets tend to be homophilous, as noted in past work <ref type=\"bibr\" target=\"#b81\">[82]</ref> and expanded upon in Appendix A.2. A comparable set of hig et=\"#b57\">[58]</ref> is plagued by high variance across different train/test splits (see results in <ref type=\"bibr\" target=\"#b81\">[82]</ref>). The small size of these datasets may tend to create mode  1} for some number of classes C, and denote by C k the set of nodes in class k. The edge homophily <ref type=\"bibr\" target=\"#b81\">[82]</ref> is the proportion of edges that connect two nodes of the s well on non-homophilous graphs -achieving higher or approximately equal performance to various GNNs <ref type=\"bibr\" target=\"#b81\">[82]</ref>.</p><p>LINK regression on graph topology. On the other ext  computational cost, such as using higher-order neighborhoods or using additional hidden embeddings <ref type=\"bibr\" target=\"#b81\">[82]</ref>.</p><p>For instance, the complexity of MixHop <ref type=\"b  combinations, and it also often achieves best performance with a large number of layers L. H 2 GCN <ref type=\"bibr\" target=\"#b81\">[82]</ref> is significantly more expensive due to its usage of strict \">[72]</ref>, and APPNP <ref type=\"bibr\" target=\"#b38\">[39]</ref>. Non-homophilous methods: H 2 GCN <ref type=\"bibr\" target=\"#b81\">[82]</ref>, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>, GPR-GNN <  is better for our datasets than those of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> (see <ref type=\"bibr\" target=\"#b81\">[82]</ref> results). Secondly, as suggested by prior theory and exper lity matrix, which consists of C 2 values instead of a single scalar value. Following previous work <ref type=\"bibr\" target=\"#b81\">[82]</ref>, for a graph G with C node classes we define the C \u00d7 C com </ref> often used in evaluation of graph representation learning methods in non-homophilous regimes <ref type=\"bibr\" target=\"#b81\">[82]</ref>, basic statistics are listed in Table <ref type=\"table\" ta  has found that the degree distribution can affect the performance of models on node classification <ref type=\"bibr\" target=\"#b81\">[82]</ref>. As a result, we provide degree distributions of all of ou  16, 32, 64}, number of layers \u2208 {1, 2}, dropout \u2208 {0, .5}. The architecture follows Section 3.2 of <ref type=\"bibr\" target=\"#b81\">[82]</ref>.</p><p>\u2022 MixHop: hidden dimension \u2208 {8, 16, 32}, number of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ure <ref type=\"figure\" target=\"#fig_0\">1</ref>. It generalizes node feature MLP and LINK regression <ref type=\"bibr\" target=\"#b78\">[79]</ref>, two baselines that often work well on non-homophilous gra get=\"#b81\">[82]</ref>.</p><p>LINK regression on graph topology. On the other extreme, there is LINK <ref type=\"bibr\" target=\"#b78\">[79]</ref> -a simple baseline that only utilizes graph topology. In p on <ref type=\"bibr\" target=\"#b56\">[57]</ref> and LINK (logistic regression on the adjacency matrix) <ref type=\"bibr\" target=\"#b78\">[79]</ref> have been found to perform well in various non-homophilous nd two-hop) <ref type=\"bibr\" target=\"#b79\">[80,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>, LINK <ref type=\"bibr\" target=\"#b78\">[79]</ref>. Simple methods: SGC <ref type=\"bibr\" target=\"#b70\">[71]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [21]</ref>, which prevents the scaling up of GNNs designed for non-homophilous settings.</p><p>Peel <ref type=\"bibr\" target=\"#b56\">[57]</ref> also studies node classification on network datasets with  feature-agnostic as simple baselines. The node-feature-agnostic models of two-hop label propagation <ref type=\"bibr\" target=\"#b56\">[57]</ref> and LINK (logistic regression on the adjacency matrix) <re ef><ref type=\"bibr\" target=\"#b54\">55]</ref> (genius).</p><p>\u2022 Publication time in citation networks <ref type=\"bibr\" target=\"#b56\">[57]</ref> (arXiv-year, snap-patents).</p><p>\u2022 Biological structures  y graph topology: label propagation (standard and two-hop) <ref type=\"bibr\" target=\"#b79\">[80,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>, LINK <ref type=\"bibr\" target=\"#b78\">[79]</ref>. Simple met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: :</p><p>\u2022 Gender relations in social or interaction networks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> (Penn94, Pokec).</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b80\">81,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" tar et=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b80\">81,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" tar putational resources <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, which is neither evident nor detrimental when they are eva eory and experiments <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, the non-homophilous GNNs usually do wellthough not necessa b57\">[58]</ref> data also studies this setting exclusively <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Using other Facebook 100 datasets besides Penn94 <ref type \">[1]</ref> proposes a graph convolutional layer that mixes powers of the adjacency matrix, GPR-GNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> features learnable weights that can be positive and negati  <ref type=\"bibr\" target=\"#b81\">[82]</ref>, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>, GPR-GNN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GCNII <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and LINK. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  learning, is known to be non-homophilous in many settings <ref type=\"bibr\" target=\"#b54\">[55,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar  target=\"#b57\">58]</ref> (wiki).</p><p>\u2022 Malicious or fraudulent nodes, such as in auction networks <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref> (genius).</p><p>\u2022 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e <ref type=\"figure\">4</ref>. The nodes are entries in the Online Encyclopedia of Integer Sequences <ref type=\"bibr\" target=\"#b63\">[64]</ref>, and directed edges link an entry to any other entry that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: interaction networks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> (Penn94, Pokec).</p><p>\u2022 Technological and internet relatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b53\">54]</ref>, their evaluation is limited to a few graph datasets used by Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> (collected by <ref type=\"bibr\" target=\"#b60\">[61,</ref><re 73,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Geom-GCN <ref type=\"bibr\" target=\"#b57\">[58]</ref> introduces a geometric aggregation scheme, MixHop <ref typ  datasets to evaluate non-homophilous graph representation learning methods were used by Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> (and collected by <ref type=\"bibr\" target=\"#b60\">[61,</ref ts discussed in <ref type=\"bibr\" target=\"#b62\">[63]</ref>, evaluation on the datasets of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> is plagued by high variance across different train/test sp v }| |E| .<label>(1)</label></formula><p>Another related measure is what we call the node homophily <ref type=\"bibr\" target=\"#b57\">[58]</ref>, defined as is the number of neighbors of u that have the  \">2</ref>. Note the substantial difference between the size of our datasets and those of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> in Table <ref type=\"table\" target=\"#tab_1\">1</ref>; our da rstly, the stability of performance across runs is better for our datasets than those of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> (see <ref type=\"bibr\" target=\"#b81\">[82]</ref> results). S e homophily in terms of the node labels, and previous non-homophilous GNN work using the Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> data also studies this setting exclusively <ref type=\"bibr i-c.org/ns/1.0\"><head>A.3 Previous Non-Homophilous Data</head><p>For the six datasets in Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> often used in evaluation of graph representation learning  ophilous GNN on the datasets we present in this paper, it does poorly on the datasets of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref>; in contrast, H 2 GCN achieves excellent performance here, Figure 5 :</head><label>5</label><figDesc>Figure 5: Compatibility matrices of datasets in Pei et al.<ref type=\"bibr\" target=\"#b57\">[58]</ref> (collected by<ref type=\"bibr\" target=\"#b60\">[61,</ref><ref ead>Table 1 :</head><label>1</label><figDesc>Statistics for previously used datasets from Pei et al.<ref type=\"bibr\" target=\"#b57\">[58]</ref> (collected by<ref type=\"bibr\" target=\"#b60\">[61,</ref><ref ng evaluation, we take a high proportion of training nodes <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>; we run each method nd internet relationships, such as in web page connections <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref> (wiki).</p><p>\u2022 Malicious or fraudulent nodes, such as in a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b66\">[67]</ref>, Pokec <ref type=\"bibr\" target=\"#b40\">[41]</ref>, genius <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and twitch-gamers <ref type=\"bibr\" target=\"#b59\">[60]</re sourced by the authors with no attached license. It was originally introduced in a conference paper <ref type=\"bibr\" target=\"#b42\">[43]</ref>. While this is a social network and thus may face privacy  the task to predict the time at which a patent was granted, resulting in five classes.</p><p>genius <ref type=\"bibr\" target=\"#b42\">[43]</ref> is a subset of the social network on genius.com -a site fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head n=\"2\">Prior Work</head><p>Graph Representation Learning. Graph neural networks <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta  large graphs, one line of work samples nodes that are used in each layer of a graph neural network <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" ta et=\"#b76\">[77,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ts. For the citation networks arXiv-year <ref type=\"bibr\" target=\"#b30\">[31]</ref> and snap-patents <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> the goal is to pre  partitioning the posting dates so that class ratios are approximately balanced.</p><p>snap-patents <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> is a dataset of ut. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s by randomly choosing node labels and generating graph edges by the Erd\u0151s-R\u00e9nyi random graph model <ref type=\"bibr\" target=\"#b21\">[22]</ref>. In particular, we fix the number of classes to two, the n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b25\">[26]</ref>. Only graph topology: label propagation (standard and two-hop) <ref type=\"bibr\" target=\"#b79\">[80,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>, LINK <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>Common notice-and-consent policies are often ineffective in actually protecting user privacy <ref type=\"bibr\" target=\"#b51\">[52]</ref>. Indeed, users may not actually have much choice in using . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>. Moreover, we give empirical evidence that existing minibatching techniques in graph learning <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b76\">77]</ref> significantly degr t graph, then passes each subgraph through a GNN to make a prediction for each node of the subgraph <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b75\">76,</ref><ref type=\"bibr\" ta chs.</p><p>These hyperparameter settings are in the same range as those used in the original papers <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b76\">77]</ref> for datasets that  een developed for efficient computation in larger datasets <ref type=\"bibr\" target=\"#b76\">[77,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" tar . As other minibatching methods are trickier to make work with these models, we use the Cluster-GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> and GraphSAINT <ref type=\"bibr\" target=\"#b76\">[77]</ref> m ll GNNs, we fix the hidden dimension to 128, which is a common hidden dimension used in Cluster-GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> and GraphSAINT <ref type=\"bibr\" target=\"#b76\">[77]</ref>. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b53\">54]</ref>, their evaluation is limited to a few graph datasets used by Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> (collected by <ref type=\"bibr\" target=\"#b60\">[61,</ref><re 73,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Geom-GCN <ref type=\"bibr\" target=\"#b57\">[58]</ref> introduces a geometric aggregation scheme, MixHop <ref typ  datasets to evaluate non-homophilous graph representation learning methods were used by Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> (and collected by <ref type=\"bibr\" target=\"#b60\">[61,</ref ts discussed in <ref type=\"bibr\" target=\"#b62\">[63]</ref>, evaluation on the datasets of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> is plagued by high variance across different train/test sp v }| |E| .<label>(1)</label></formula><p>Another related measure is what we call the node homophily <ref type=\"bibr\" target=\"#b57\">[58]</ref>, defined as is the number of neighbors of u that have the  \">2</ref>. Note the substantial difference between the size of our datasets and those of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> in Table <ref type=\"table\" target=\"#tab_1\">1</ref>; our da rstly, the stability of performance across runs is better for our datasets than those of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> (see <ref type=\"bibr\" target=\"#b81\">[82]</ref> results). S e homophily in terms of the node labels, and previous non-homophilous GNN work using the Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> data also studies this setting exclusively <ref type=\"bibr i-c.org/ns/1.0\"><head>A.3 Previous Non-Homophilous Data</head><p>For the six datasets in Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref> often used in evaluation of graph representation learning  ophilous GNN on the datasets we present in this paper, it does poorly on the datasets of Pei et al. <ref type=\"bibr\" target=\"#b57\">[58]</ref>; in contrast, H 2 GCN achieves excellent performance here, Figure 5 :</head><label>5</label><figDesc>Figure 5: Compatibility matrices of datasets in Pei et al.<ref type=\"bibr\" target=\"#b57\">[58]</ref> (collected by<ref type=\"bibr\" target=\"#b60\">[61,</ref><ref ead>Table 1 :</head><label>1</label><figDesc>Statistics for previously used datasets from Pei et al.<ref type=\"bibr\" target=\"#b57\">[58]</ref> (collected by<ref type=\"bibr\" target=\"#b60\">[61,</ref><ref ng evaluation, we take a high proportion of training nodes <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>; we run each method nd internet relationships, such as in web page connections <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref> (wiki).</p><p>\u2022 Malicious or fraudulent nodes, such as in a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r in Appendix D.3. Node features are constructed using averaged title and abstract GloVe embeddings <ref type=\"bibr\" target=\"#b58\">[59]</ref>. Labels represent total page views over 60 days, which are labels of the graph.</p><p>For the node features, we formed 300 dimensional Wikipedia Glove vectors <ref type=\"bibr\" target=\"#b58\">[59]</ref> for each word in the title and abstract, then averaged the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rge social network data such as social media companies or government actors with auxiliary networks <ref type=\"bibr\" target=\"#b49\">[50]</ref>. Smaller actors can perform certain attacks, but this may  y be made more difficult by resource requirements such as the need for certain external information <ref type=\"bibr\" target=\"#b49\">[50]</ref> or the ability to add nodes and edges before an anonymized nymization attacks <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> to reveal user identities in supposedly anonymized datasets. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > \u2022 snap-patents: The data was originally publically released by NBER in 2001 in a working paper by <ref type=\"bibr\" target=\"#b26\">[27]</ref>. To the best of our knowledge, the dataset was not release. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head n=\"2\">Prior Work</head><p>Graph Representation Learning. Graph neural networks <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta  large graphs, one line of work samples nodes that are used in each layer of a graph neural network <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" ta et=\"#b76\">[77,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h a GNN to make a prediction for each node of the subgraph <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b75\">76,</ref><ref type=\"bibr\" target=\"#b76\">77]</ref>. While these method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uently require more parameters and computational resources <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, which is neither evi s). Secondly, as suggested by prior theory and experiments <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, the non-homophilous  eom-GCN <ref type=\"bibr\" target=\"#b57\">[58]</ref> introduces a geometric aggregation scheme, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposes a graph convolutional layer that mixes powers of th  embeddings <ref type=\"bibr\" target=\"#b81\">[82]</ref>.</p><p>For instance, the complexity of MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> is O(K(dL|E| + nd 2 L)), which has an extra factor K that is b38\">[39]</ref>. Non-homophilous methods: H 2 GCN <ref type=\"bibr\" target=\"#b81\">[82]</ref>, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>, GPR-GNN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GCNII <r h various minibatching methods. We take GC-NJK <ref type=\"bibr\" target=\"#b71\">[72]</ref> and MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> as our base models for evaluation, as they are representativ ivations. The last layer is a linear projection layer, instead of the attention output mechanism in <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>\u2022 GPR-GNN: The basic setup and grid is the same as th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y applied to the non-homophilous setting, as they oftentimes assume homophily in their construction <ref type=\"bibr\" target=\"#b70\">[71,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta been used for scalable graph representation learning models, but these methods all assume homophily <ref type=\"bibr\" target=\"#b70\">[71,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta xperiments. Also, we show empirically that scalable methods for graph learning like SGC and C&amp;S <ref type=\"bibr\" target=\"#b70\">[71,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> do not perform wel #b9\">10]</ref>. Many of these methods explicitly make use of an assumption of homophily in the data <ref type=\"bibr\" target=\"#b70\">[71,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta  simple, inexpensive models are able to achieve state-of-the-art performance on homophilic datasets <ref type=\"bibr\" target=\"#b70\">[71,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. However, these me e fact that many scalable graph learning methods rely on implicit or explicit homophily assumptions <ref type=\"bibr\" target=\"#b70\">[71,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta ed, a wide range of GNNs operate as low-pass graph filters <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> that smooth features  get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b70\">71,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar  they have often been overlooked by recent graph representation learning work. Also, we include SGC <ref type=\"bibr\" target=\"#b70\">[71]</ref> and C&amp;S <ref type=\"bibr\" target=\"#b31\">[32]</ref> as s \"bibr\" target=\"#b56\">57]</ref>, LINK <ref type=\"bibr\" target=\"#b78\">[79]</ref>. Simple methods: SGC <ref type=\"bibr\" target=\"#b70\">[71]</ref>, C&amp;S <ref type=\"bibr\" target=\"#b31\">[32]</ref> and the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  modalities are processed and combined in a neural network <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b77\">78]</ref>. In our setting, we can view adjacency information and node. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph neural networks <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b68\">69]</ref> have demonstrated their utility on a variety of graph machi </ref> and their two-hop variants. General GNNs: GCN <ref type=\"bibr\" target=\"#b37\">[38]</ref>, GAT <ref type=\"bibr\" target=\"#b68\">[69]</ref>, jumping knowledge networks (GCNJK, GATJK) <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  entities with powerful inference algorithms. Furthermore, people may rely on obscurity for privacy <ref type=\"bibr\" target=\"#b28\">[29]</ref>, but this assumption may be ignored in courts of law, and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b4\">[5]</ref>. Furthermore, additional actors could make use of deanonymization attacks <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b80\">81,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" tar et=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b80\">81,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" tar putational resources <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, which is neither evident nor detrimental when they are eva eory and experiments <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, the non-homophilous GNNs usually do wellthough not necessa b57\">[58]</ref> data also studies this setting exclusively <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Using other Facebook 100 datasets besides Penn94 <ref type \">[1]</ref> proposes a graph convolutional layer that mixes powers of the adjacency matrix, GPR-GNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> features learnable weights that can be positive and negati  <ref type=\"bibr\" target=\"#b81\">[82]</ref>, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>, GPR-GNN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GCNII <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and LINK. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uently require more parameters and computational resources <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, which is neither evi s). Secondly, as suggested by prior theory and experiments <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, the non-homophilous  eom-GCN <ref type=\"bibr\" target=\"#b57\">[58]</ref> introduces a geometric aggregation scheme, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposes a graph convolutional layer that mixes powers of th  embeddings <ref type=\"bibr\" target=\"#b81\">[82]</ref>.</p><p>For instance, the complexity of MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> is O(K(dL|E| + nd 2 L)), which has an extra factor K that is b38\">[39]</ref>. Non-homophilous methods: H 2 GCN <ref type=\"bibr\" target=\"#b81\">[82]</ref>, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>, GPR-GNN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GCNII <r h various minibatching methods. We take GC-NJK <ref type=\"bibr\" target=\"#b71\">[72]</ref> and MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> as our base models for evaluation, as they are representativ ivations. The last layer is a linear projection layer, instead of the attention output mechanism in <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>\u2022 GPR-GNN: The basic setup and grid is the same as th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>. Moreover, we give empirical evidence that existing minibatching techniques in graph learning <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b76\">77]</ref> significantly degr t graph, then passes each subgraph through a GNN to make a prediction for each node of the subgraph <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b75\">76,</ref><ref type=\"bibr\" ta chs.</p><p>These hyperparameter settings are in the same range as those used in the original papers <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b76\">77]</ref> for datasets that  een developed for efficient computation in larger datasets <ref type=\"bibr\" target=\"#b76\">[77,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" tar . As other minibatching methods are trickier to make work with these models, we use the Cluster-GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> and GraphSAINT <ref type=\"bibr\" target=\"#b76\">[77]</ref> m ll GNNs, we fix the hidden dimension to 128, which is a common hidden dimension used in Cluster-GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> and GraphSAINT <ref type=\"bibr\" target=\"#b76\">[77]</ref>. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b66\">[67]</ref>, Pokec <ref type=\"bibr\" target=\"#b40\">[41]</ref>, genius <ref type=\"bibr\" target=\"#b42\">[43]</ref>, and twitch-gamers <ref type=\"bibr\" target=\"#b59\">[60]</re sourced by the authors with no attached license. It was originally introduced in a conference paper <ref type=\"bibr\" target=\"#b42\">[43]</ref>. While this is a social network and thus may face privacy  the task to predict the time at which a patent was granted, resulting in five classes.</p><p>genius <ref type=\"bibr\" target=\"#b42\">[43]</ref> is a subset of the social network on genius.com -a site fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uently require more parameters and computational resources <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, which is neither evi s). Secondly, as suggested by prior theory and experiments <ref type=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, the non-homophilous  eom-GCN <ref type=\"bibr\" target=\"#b57\">[58]</ref> introduces a geometric aggregation scheme, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposes a graph convolutional layer that mixes powers of th  embeddings <ref type=\"bibr\" target=\"#b81\">[82]</ref>.</p><p>For instance, the complexity of MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> is O(K(dL|E| + nd 2 L)), which has an extra factor K that is b38\">[39]</ref>. Non-homophilous methods: H 2 GCN <ref type=\"bibr\" target=\"#b81\">[82]</ref>, MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>, GPR-GNN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, GCNII <r h various minibatching methods. We take GC-NJK <ref type=\"bibr\" target=\"#b71\">[72]</ref> and MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref> as our base models for evaluation, as they are representativ ivations. The last layer is a linear projection layer, instead of the attention output mechanism in <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>\u2022 GPR-GNN: The basic setup and grid is the same as th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b11\">[14,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" target=\"#b20\">23,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b27\">30,</ref><ref type=\"bibr\" tar ers to pipeline stages is harder; we defer to related work <ref type=\"bibr\" target=\"#b19\">[22,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b38\">41]</ref> to solve this probl  approaches such as PipeMare, PipeDream, and PipeDream-2BW <ref type=\"bibr\" target=\"#b20\">[23,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b27\">30,</ref><ref type=\"bibr\" tar s. We believe this is the fastest training throughput achieved for this size of model: past systems <ref type=\"bibr\" target=\"#b26\">[29,</ref><ref type=\"bibr\" target=\"#b37\">40]</ref> cannot train such  ce of parallelism strategies (such as FlexFlow <ref type=\"bibr\" target=\"#b19\">[22]</ref>, PipeDream <ref type=\"bibr\" target=\"#b26\">[29]</ref>, Tarnawski et al. <ref type=\"bibr\" target=\"#b38\">[41]</ref e number of layers in the model, which is limiting for certain model architectures.</p><p>PipeDream <ref type=\"bibr\" target=\"#b26\">[29]</ref> combined pipeline parallelism and data parallelism in a pr to converge). Automatic Partitioning. FlexFlow <ref type=\"bibr\" target=\"#b19\">[22]</ref>, PipeDream <ref type=\"bibr\" target=\"#b26\">[29]</ref>, DAP-PLE <ref type=\"bibr\" target=\"#b11\">[14]</ref>, and Ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b17\">20,</ref><ref type=\"bibr\" target=\"#b20\">23,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b27\">30,</ref><ref type=\"bibr\" target=\"#b42\">45]</ref> is another techniqu  and PipeDream-2BW <ref type=\"bibr\" target=\"#b20\">[23,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b27\">30,</ref><ref type=\"bibr\" target=\"#b42\">45]</ref>  </p></div> <div xm y as much as 10% compared to previously-proposed schedules <ref type=\"bibr\" target=\"#b17\">[20,</ref><ref type=\"bibr\" target=\"#b27\">30]</ref> with comparable memory footprint. \u2022 Values of hyperparamete es through the lifetime of a training iteration.</p><p>Instead, we use the PipeDream-Flush schedule <ref type=\"bibr\" target=\"#b27\">[30]</ref>. In this schedule, we first enter a warm-up phase where wo with pipeline parallelism to keep memory footprint acceptably low. Previous work like PipeDream-2BW <ref type=\"bibr\" target=\"#b27\">[30]</ref> has looked at the performance ramifications of activation  gatron <ref type=\"bibr\" target=\"#b37\">[40]</ref>) nor pipeline model parallelism (used by PipeDream <ref type=\"bibr\" target=\"#b27\">[30]</ref> and others) in isolation can match the performance of usin us accelerators. Pipeline parallelism can also be implemented with relaxed semantics: PipeDream-2BW <ref type=\"bibr\" target=\"#b27\">[30]</ref> maintains two weight versions and guarantees 1-stale weigh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: x multiplications (GEMMs), potentially decreasing GPU utilization.</p><p>Pipeline model parallelism <ref type=\"bibr\" target=\"#b11\">[14,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" ta r\" target=\"#b26\">[29]</ref>, Tarnawski et al. <ref type=\"bibr\" target=\"#b38\">[41]</ref>, and DAPPLE <ref type=\"bibr\" target=\"#b11\">[14]</ref>), but instead suggest heuristics (in \u00a73) that we found wor  type=\"bibr\" target=\"#b19\">[22]</ref>, PipeDream <ref type=\"bibr\" target=\"#b26\">[29]</ref>, DAP-PLE <ref type=\"bibr\" target=\"#b11\">[14]</ref>, and Tarnawski et al. <ref type=\"bibr\" target=\"#b38\">[41]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h \"stable\" weights, and instead dedicates resources to train the remaining \"active\" layers. HetPipe <ref type=\"bibr\" target=\"#b28\">[31]</ref> uses a combination of pipeline and data parallelism on a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b9\">[12,</ref><ref type=\"bibr\" target=\"#b15\">18,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" target=\"#b18\">21]</ref> is an optional technique that trades off an increase in the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h \"stable\" weights, and instead dedicates resources to train the remaining \"active\" layers. HetPipe <ref type=\"bibr\" target=\"#b28\">[31]</ref> uses a combination of pipeline and data parallelism on a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e that we do not automatically explore the search space of parallelism strategies (such as FlexFlow <ref type=\"bibr\" target=\"#b19\">[22]</ref>, PipeDream <ref type=\"bibr\" target=\"#b26\">[29]</ref>, Tarn unrealistic training times (e.g., thousands of years to converge). Automatic Partitioning. FlexFlow <ref type=\"bibr\" target=\"#b19\">[22]</ref>, PipeDream <ref type=\"bibr\" target=\"#b26\">[29]</ref>, DAP- el architectures, where assignment of layers to pipeline stages is harder; we defer to related work <ref type=\"bibr\" target=\"#b19\">[22,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Activation Recomputation</head><p>Activation recomputation <ref type=\"bibr\" target=\"#b9\">[12,</ref><ref type=\"bibr\" target=\"#b15\">18,</ref><ref type=\"bibr\" target=\"#b17\">20,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>HPC for Model Training. Goyal et al. <ref type=\"bibr\" target=\"#b14\">[17]</ref> and You et al. <ref type=\"bibr\" target=\"#b44\">[47]</ref> both demonstrate the use of High Performance Computing tec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: parallelism techniques have been proposed to address these two challenges. For example, recent work <ref type=\"bibr\" target=\"#b36\">[39,</ref><ref type=\"bibr\" target=\"#b37\">40]</ref> has shown how tens to larger models as well, but would need more GPUs to keep training time practical. Mesh-TensorFlow <ref type=\"bibr\" target=\"#b36\">[39]</ref> proposes a language for easily specifying parallelization . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e that we do not automatically explore the search space of parallelism strategies (such as FlexFlow <ref type=\"bibr\" target=\"#b19\">[22]</ref>, PipeDream <ref type=\"bibr\" target=\"#b26\">[29]</ref>, Tarn unrealistic training times (e.g., thousands of years to converge). Automatic Partitioning. FlexFlow <ref type=\"bibr\" target=\"#b19\">[22]</ref>, PipeDream <ref type=\"bibr\" target=\"#b26\">[29]</ref>, DAP- el architectures, where assignment of layers to pipeline stages is harder; we defer to related work <ref type=\"bibr\" target=\"#b19\">[22,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lled as graphs, such as user-item bipartite graph and item-item co-occurrence graph, recent studies <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar , Yelp2018, Gowalla, and MovieLens-1M to conduct our experiments, as many recent GCNbased CF models <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" tar \ud835\udc3e to 10. In particular, we fix the embedding size to 64 which is identical to recent GCN-based work <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" tar s for model designs. Towards this end, some research efforts <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> have been made to si ide success of GCN in graph learning, several recent studies <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ f \ud835\udc3e. We test the performance of UltraGCN with different \ud835\udc3e in <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\">50]< ow convergence of GCN-based models on CF tasks. Although the aforementioned models such as LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> have already been simplified for training, the message pass f> found that simplifying GCN appropriately can further boost the performance on CF tasks. LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref> is one such simplified GCN model that removes feature trans e weight assignments may mislead the model training and finally result in sub-optimal performance.  <ref type=\"bibr\" target=\"#b9\">[10]</ref>. We partially attribute it to the over-smoothing problem of ref type=\"bibr\" target=\"#b24\">[24]</ref>, LR-GCCF <ref type=\"bibr\" target=\"#b3\">[4]</ref>, LightGCN <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and DGCF <ref type=\"bibr\" target=\"#b28\">[28]</ref>).</p><p CN, proposing a simplified GCN (SGCN) model by removing these two parts. Inspired by SGC, He et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> devise LightGCN for recommendation by removing nonlinear ac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> tend to seek for more and more sophisticated network encode get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> are evaluated on these four datasets. We closely follow the BGCF <ref type=\"bibr\" target=\"#b23\">[23]</ref>, SCF <ref type=\"bibr\" target=\"#b34\">[34]</ref>, LCFN <ref type=\"bibr\" target=\"#b32\">[32]</ref>, and SGL-ED <ref type=\"bibr\" target=\"#b30\">[30]</ref>. For. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref> opt for powerful graph convolutional/neural networks (GCNs,  GNN-based CF models <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> tend to seek for mo  conduct our experiments, as many recent GCNbased CF models <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" tar cent GCN-based work <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b28\">28]</ref> to keep the same le target=\"#b25\">[25]</ref>, and Node2Vec <ref type=\"bibr\" target=\"#b6\">[7]</ref>), and GCNbased (NGCF <ref type=\"bibr\" target=\"#b27\">[27]</ref>, NIA-GCN <ref type=\"bibr\" target=\"#b24\">[24]</ref>, LR-GCC  items that incorporate both graph structure as well as item feature information. Then, Wang et al. <ref type=\"bibr\" target=\"#b27\">[27]</ref> design NGCF which is a new graph-based framework for colla. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re recent stateof-the-art CF models, including NBPO <ref type=\"bibr\" target=\"#b33\">[33]</ref>, BGCF <ref type=\"bibr\" target=\"#b23\">[23]</ref>, SCF <ref type=\"bibr\" target=\"#b34\">[34]</ref>, LCFN <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ite graph and item-item co-occurrence graph, recent studies <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> tend to seek for more and more sophisticated network encode get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b27\">27,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> are evaluated on these four datasets. We closely follow the BGCF <ref type=\"bibr\" target=\"#b23\">[23]</ref>, SCF <ref type=\"bibr\" target=\"#b34\">[34]</ref>, LCFN <ref type=\"bibr\" target=\"#b32\">[32]</ref>, and SGL-ED <ref type=\"bibr\" target=\"#b30\">[30]</ref>. For. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive sampling during training. This is inspired from the negative sampling strategy used in Word2Vec <ref type=\"bibr\" target=\"#b18\">[19]</ref>, which provides a more simple and effective way to counter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  state-of-the-art models, covering MF-based (MF-BPR <ref type=\"bibr\" target=\"#b14\">[15]</ref>, ENMF <ref type=\"bibr\" target=\"#b2\">[3]</ref>), metric learing-based (CML <ref type=\"bibr\" target=\"#b11\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Revisiting GCN and LightGCN</head><p>GCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> is a representative model of graph neural networks that ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">29]</ref> found that simplifying GCN appropriately can further boost  rget=\"#b17\">[18]</ref> propose UCMF that simplifies GCN for the node classification task. Wu et al. <ref type=\"bibr\" target=\"#b29\">[29]</ref> find the non-necessity of nonlinear activation and feature. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b14\">[15]</ref>, ENMF <ref type=\"bibr\" target=\"#b2\">[3]</ref>), metric learing-based (CML <ref type=\"bibr\" target=\"#b11\">[12]</ref>), network embedding methods (DeepWalk <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: :lang=\"en\"> \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>developed. For example, Zhu et al. <ref type=\"bibr\" target=\"#b15\">16</ref> uses a Hidden-Markov random field (HMRF) approach to model s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e to be developed <ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref> , it is desirable. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b8\">9</ref> , SLIDE-seqV2 (ref. <ref type=\"bibr\" target=\"#b10\">11</ref> ) and MERFISH <ref type=\"bibr\" target=\"#b4\">5</ref> data are shown in Supplementary Notes 1-3. Our results consist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref> , it is desirable to have methods that are compatible with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: terns revealed by SpaGCN-detected SVGs were further confirmed by Moran's I and Geary's C statistics <ref type=\"bibr\" target=\"#b26\">27</ref> , two commonly used metrics for quantifying spatial autocorr tive spatial autocorrelation occurs when dissimilar values occur near one another. Moran's I metric <ref type=\"bibr\" target=\"#b26\">27</ref> is a correlation coefficient that measures the overall spati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: To solve our proposed self-supervised XMC task, we adopt the state-of-the-art XR-Transformer method <ref type=\"bibr\" target=\"#b64\">(Zhang et al., 2021a)</ref>. By using the encoder from the XR-Transfo e a priori given graph topology. This is achieved this by using the state-of-the-art XR-Transformer <ref type=\"bibr\" target=\"#b64\">(Zhang et al., 2021a)</ref> method for solving the XMC problem. The h xtraction.</p><p>Analysis of key components in XR-Transformers. In the original XR-Transformer work <ref type=\"bibr\" target=\"#b64\">(Zhang et al., 2021a)</ref>, the authors argued that one needs to per f GIANT-XRT for all three OGB benchmark datasets. We mostly follow the convention of XR-Transformer <ref type=\"bibr\" target=\"#b64\">(Zhang et al., 2021a)</ref> to set the hyper-parameters. For ogbn-arx. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  a standard methodology in the field <ref type=\"bibr\" target=\"#b24\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b12\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b48\">Velickovic multi-layer perceptron (MLP), which does not use graph information. Two other methods are GraphSAGE <ref type=\"bibr\" target=\"#b12\">(Hamilton et al., 2017)</ref>, which we applied to ogbn-arxiv, and Gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: verse Document Frequency (TFIDF) model or some other model with learnable parameters, such as XLNet <ref type=\"bibr\" target=\"#b54\">(Yang et al., 2019)</ref> and RoBERTa <ref type=\"bibr\" target=\"#b36\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"evidence\" passages containing the answers <ref type=\"bibr\" target=\"#b3\">(Chang et al., 2020a;</ref><ref type=\"bibr\" target=\"#b29\">Lee et al., 2019)</ref>. Many methods for the XMC problem leverage hi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b4\">Chang et al., 2021;</ref><ref type=\"bibr\" target=\"#b53\">Yadav et al., 2021;</ref><ref type=\"bibr\" target=\"#b44\">Sen et al., 2021)</ref>, but not in the context of self-supervised nu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  sub-sample 50M (out of 111M) most important nodes based on page rank scores of the bipartite graph <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref>. The resulting XMC instance-to-label matrix Y. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a standard methodology in the field <ref type=\"bibr\" target=\"#b24\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b12\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b48\">Velickovic multi-layer perceptron (MLP), which does not use graph information. Two other methods are GraphSAGE <ref type=\"bibr\" target=\"#b12\">(Hamilton et al., 2017)</ref>, which we applied to ogbn-arxiv, and Gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b0\">Baharav et al., 2021;</ref><ref type=\"bibr\" target=\"#b4\">Chang et al., 2021;</ref><ref type=\"bibr\" target=\"#b53\">Yadav et al., 2021;</ref><ref type=\"bibr\" target=\"#b44\">Sen et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b18\">Huang et al., 2019;</ref><ref type=\"bibr\">Zhang &amp; Zhang, 2020;</ref><ref type=\"bibr\" target=\"#b34\">Liu et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  method GAMLP <ref type=\"bibr\" target=\"#b65\">(Zhang et al., 2021b)</ref> from 68.25% to 69.67%, SGC <ref type=\"bibr\" target=\"#b51\">(Wu et al., 2019)</ref> from 63.29% to 66.10% and MLP from 47.24% to  t allows for mini-batch training. Due to scalability issues, we used Simple Graph Convolution (SGC) <ref type=\"bibr\" target=\"#b51\">(Wu et al., 2019)</ref> for ogbn-papers100M. We also tested the state. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tical methods <ref type=\"bibr\" target=\"#b30\">[31]</ref> or machine-learning-based predictive models <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The latter has the benefit of can be easily ported to diffe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  do so.</p><p>Machine learning is a proven design methodology for systems modeling and optimization <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: M) operations.</p><p>There is an extensive body of work in optimizing SpMM for scientific workloads <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Various sparse matrix storage formats have been proposed . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ormat selection is unclear on the new GNN workloads. Existing deep learning frameworks like PyTorch <ref type=\"bibr\" target=\"#b22\">[23]</ref> and Tensorflow <ref type=\"bibr\" target=\"#b0\">[1]</ref> all ion <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>We use an open-source implementation of ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref> as the CNN model. To provide a fair comparison, we train a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chine learning for a wide range of code optimization tasks <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> In this work, we employ machine learning techniques to deve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> In this work, we employ machine learning techniques to deve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: AT) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, relational graph convolutional neural network (RGCN) <ref type=\"bibr\" target=\"#b25\">[26]</ref>, GNN with feature-wise linear modulation (FiLM) <ref type= tion, we use two graph data suites, CoraFull <ref type=\"bibr\" target=\"#b39\">[40]</ref> and Entities <ref type=\"bibr\" target=\"#b25\">[26]</ref>, containing a total of 5 graph datasets with matrix sizes . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ased predictor against three other classification methods used in prior works for code optimization <ref type=\"bibr\" target=\"#b33\">[34]</ref>: MLP neural network <ref type=\"bibr\" target=\"#b11\">[12]</r g and optimization <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ally processes. There are methods to support large-scale graph processing on GPUs such as GraphSAGE <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Our approach can be ported to support GPU processing. Thi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d the right format can change from one matrix to the other <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Methods have been proposed to dynamically choose sparse matr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -item interaction graphs to model user preferences specific to each modality. Following MMGCN, GRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> utilizes multimodal features to refine user-item interacti ecific representations to obtain the representations of users or items for prediction.</p><p>\u2022 GRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> is also one of the state-of-the-arts multimodal recommenda arned user representation can reflect the users' specific interests on items. Following MMGCN, GRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> focuses on adaptively refining the structure of interactio modal information; other work on multimedia recommendation <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> conducts multimodal fusion by simple linear combination or  48]</ref> and especially multimodal recommendation systems <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. MMGCN <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: empts have been made to integrate multimodal contents into graphbased recommendation systems. MMGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> constructs modality-specific user-item interaction graphs   features as content information to predict the interactions between users and items.</p><p>\u2022 MMGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> is one of the state-of-the-art multimodal recommendation m \">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. MMGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> constructs modal-specific graph and conducts graph convolu rget=\"#b15\">16]</ref> only focuses on unimodal information; other work on multimedia recommendation <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> conducts multimoda >11,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> and especially multimodal recommendation systems <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: usion with Contrastive Auxiliary Task</head><p>Multiple modalities convey comprehensive information <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Item relations shared between modalities are important to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s across a variety of domains, including node classification <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, link prediction <ref type=\"bibr\" target=\"#b50\">[51]</ref>, h embeddings and local node embeddings. GraphCL <ref type=\"bibr\" target=\"#b65\">[66]</ref> and GRACE <ref type=\"bibr\" target=\"#b49\">[50]</ref> propose a node-level contrastive objective to simplify pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y distill the shared information from multiple modalities. Existing contrastive learning frameworks <ref type=\"bibr\" target=\"#b27\">[28]</ref> seek to maximize the agreement among differently augmented quantifies the agreement between two representations, which is implemented by the InfoNCE estimator <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Specifically, we set (h m i , h i ) as positive samples,  er. For visual data, negative samples can be generated using a multiple-stage augmentation pipeline <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b16\">[17]</ref> which has exhibited remarkable benefits in many multimodal tasks <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta  the same data examples, which has been proven to be effective in multiview representation learning <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and multimodal tas epresentation is one that models modalityinvariant factors <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. To this end, we first utilize an attention mechanism to fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed in other recommendation works with contrastive learning <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed on these embeddings <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Following traditional CF framework, early work on multimedia. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ring the Mutual Information (MI) between global graph embeddings and local node embeddings. GraphCL <ref type=\"bibr\" target=\"#b65\">[66]</ref> and GRACE <ref type=\"bibr\" target=\"#b49\">[50]</ref> propos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arly work on multimedia recommendation like VBPR <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and ACF <ref type=\"bibr\" target=\"#b6\">[7]</ref> incorporate pts disregard the fine-grained multimodal fusion: early work <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> only focuses on unimo ual features are very important in revealing item attributes <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, VBPR, MMGCN, and GRCN outperform all CF methods. For the oth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arly work on multimedia recommendation like VBPR <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and ACF <ref type=\"bibr\" target=\"#b6\">[7]</ref> incorporate pts disregard the fine-grained multimodal fusion: early work <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> only focuses on unimo ual features are very important in revealing item attributes <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, VBPR, MMGCN, and GRCN outperform all CF methods. For the oth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b68\">69]</ref> and multimedia recommendation <ref type=\"bibr\" target=\"#b69\">[70,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>. Zhou et al. <ref type=\"bibr\" target=\"#b68\">[69]</ref> util o regain the connectivity information with hierarchical mutual information maximization. Wei et al. <ref type=\"bibr\" target=\"#b70\">[71]</ref> aim to maximize the mutual information between item conten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dal fusion: early work <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> only focuses on unimodal information; other work on multime. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ity m. Based on the hypothesis that similar items are more likely to interact than dissimilar items <ref type=\"bibr\" target=\"#b20\">[21]</ref>, we quantify the semantic relationship between two items b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b68\">69]</ref> and multimedia recommendation <ref type=\"bibr\" target=\"#b69\">[70,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>. Zhou et al. <ref type=\"bibr\" target=\"#b68\">[69]</ref> util o regain the connectivity information with hierarchical mutual information maximization. Wei et al. <ref type=\"bibr\" target=\"#b70\">[71]</ref> aim to maximize the mutual information between item conten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b36\">[37]</ref> optimizer, where the batch size is fixed at 1024. We use the Xavier initializer <ref type=\"bibr\" target=\"#b37\">[38]</ref> to initialize the model parameters. The optimal hyper-para. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and ACF <ref type=\"bibr\" target=\"#b6\">[7]</ref> incorporates multimodal features as side information in addi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b3\">4]</ref>. Following traditional CF framework, early work on multimedia recommendation like VBPR <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and ACF  y consists of two essential components: light graph convolution and layer combination.</p><p>\u2022 VBPR <ref type=\"bibr\" target=\"#b4\">[5]</ref>: Based upon the BPR model, it integrates the visual features <ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. For example, VBPR <ref type=\"bibr\" target=\"#b4\">[5]</ref> extends matrix factorization by incorporating visual feature e items.</p><p>Secondly, previous attempts disregard the fine-grained multimodal fusion: early work <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target atasets. For Clothing dataset where visual features are very important in revealing item attributes <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, VBPR, MMGCN, and GRCN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b28\">29]</ref> and multimodal tasks <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. In this work, since multiple modality-aware graphs are inv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: usion with Contrastive Auxiliary Task</head><p>Multiple modalities convey comprehensive information <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Item relations shared between modalities are important to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y multimodal tasks <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, we propose to conduct fine-grained multimodal fusion by ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arly work on multimedia recommendation like VBPR <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DeepStyle <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and ACF <ref type=\"bibr\" target=\"#b6\">[7]</ref> incorporate pts disregard the fine-grained multimodal fusion: early work <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> only focuses on unimo ual features are very important in revealing item attributes <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, VBPR, MMGCN, and GRCN outperform all CF methods. For the oth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b23\">24]</ref>, probabilistic modeling <ref type=\"bibr\" target=\"#b53\">[54,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>, and direct optimiz aph to be sampled accordingly in order to be used in any graph convolutional operator. NeuralSparse <ref type=\"bibr\" target=\"#b54\">[55]</ref> considers the graph sparsification task by removing taskir. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rning to graph representation learning <ref type=\"bibr\" target=\"#b63\">[64]</ref>. Velickovic et al. <ref type=\"bibr\" target=\"#b64\">[65]</ref> introduce an objective function measuring the Mutual Infor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ions, their expressiveness is confined.</p><p>Inspired by the recent surge of graph neural networks <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, Wang et al. <ref type dely employed for graph analytical tasks across a variety of domains, including node classification <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, link prediction <re arsified, directed graph adjacency matrix. To alleviate the exploding or vanishing gradient problem <ref type=\"bibr\" target=\"#b7\">[8]</ref>, we normalize the adjacency matrix as:</p><formula xml:id=\"f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mmendation systems <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. MMGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> constructs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: There also exist several works combining self-supervised learning with session-based recommendation <ref type=\"bibr\" target=\"#b67\">[68,</ref><ref type=\"bibr\" target=\"#b68\">69]</ref>, social recommenda. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>, and direct optimization approaches <ref type=\"bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b68\">69]</ref> and multimedia recommendation <ref type=\"bibr\" target=\"#b69\">[70,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>. Zhou et al. <ref type=\"bibr\" target=\"#b68\">[69]</ref> util o regain the connectivity information with hierarchical mutual information maximization. Wei et al. <ref type=\"bibr\" target=\"#b70\">[71]</ref> aim to maximize the mutual information between item conten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b16\">[17]</ref> which has exhibited remarkable benefits in many multimodal tasks <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta  the same data examples, which has been proven to be effective in multiview representation learning <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and multimodal tas epresentation is one that models modalityinvariant factors <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. To this end, we first utilize an attention mechanism to fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ring the Mutual Information (MI) between global graph embeddings and local node embeddings. GraphCL <ref type=\"bibr\" target=\"#b65\">[66]</ref> and GRACE <ref type=\"bibr\" target=\"#b49\">[50]</ref> propos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e, and availability of specialized deep learning libraries <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" tar e architecture of existing widely used geometric deep learning libraries from the PyTorch ecosystem <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. Our framework was s=\"http://www.tei-c.org/ns/1.0\"><head>Library</head><p>Backend Supervised Temporal GPU PT Geometric <ref type=\"bibr\" target=\"#b14\">[15]</ref> PT</p><formula xml:id=\"formula_0\">\u2714 \u2718 \u2714 Geometric2DR [49]  \"bibr\" target=\"#b57\">[58]</ref> in memory, but returned as a PyTorch Geometric Data object instance <ref type=\"bibr\" target=\"#b14\">[15]</ref> by the Spatiotemporal Signal Iterators when these are iter =\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ase Java \u2718 StacSpec <ref type=\"bibr\" target=\"#b22\">[23]</ref> 2017 Database Javascript \u2718 MobilityDB <ref type=\"bibr\" target=\"#b69\">[70]</ref> 2019 Database C \u2718 PyStac <ref type=\"bibr\" target=\"#b43\">[4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding blocks of document labeling, fraud detection, traffic forecasting, and cheminformatics systems <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b44\">[45]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: informatics systems <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b44\">[45]</ref><ref type=\"bibr\" target=\"#b45\">[46]</ref><ref type=\"bibr\" target=\"#b46\">[47]</ref><ref type=\"bibr\" t learning libraries <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref> were all contributi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b29\">[30]</ref>, GGCN <ref type=\"bibr\" target=\"#b32\">[33]</ref>, Cheby-Conv <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and RGCN <ref type=\"bibr\" target=\"#b49\">[50]</ref> fit pe =\"bibr\" target=\"#b52\">[53]</ref> for regularization (lines <ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. Using the fully connected layer the model outputs a singl =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  family of deep learning models uses the attention mechanism <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref> to learn representa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b29\">[30]</ref>, GGCN <ref type=\"bibr\" target=\"#b32\">[33]</ref>, Cheby-Conv <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and RGCN <ref type=\"bibr\" target=\"#b49\">[50]</ref> fit pe =\"bibr\" target=\"#b52\">[53]</ref> for regularization (lines <ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. Using the fully connected layer the model outputs a singl =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e not part of the PyTorch Geometric ecosystem such as diffusion convolutional graph neural networks <ref type=\"bibr\" target=\"#b31\">[32]</ref> are implemented as standalone neural network layers in the ng function and retain 10% of the temporal snapshots for model performance evaluation (lines 7-8).  <ref type=\"bibr\" target=\"#b31\">[32]</ref> and a fully connected layer with a single neuron (lines 8-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igh quality, breadth, user-oriented nature, and availability of specialized deep learning libraries <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta t=\"#b36\">[37]</ref> and dropout <ref type=\"bibr\" target=\"#b52\">[53]</ref> for regularization (lines <ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. Using the fully  ime period (lines <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in recommendations <ref type=\"bibr\" target=\"#b37\">[27,</ref><ref type=\"bibr\" target=\"#b52\">37,</ref><ref type=\"bibr\" target=\"#b60\">43]</ref>. Different from these works, this paper focuses on item rep commendation. Self-supervised learning tasks are designed to capture information among user history <ref type=\"bibr\" target=\"#b60\">[43]</ref> and learn more robust disentangled user representation <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ompared to conventional approaches like matrix factorization <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">21,</ref><ref type=\"bibr\" target=\"#b31\">22]</ref>, gradient boosted d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: layer is also the dimension of final query and item embeddings. All models are trained with Adagrad <ref type=\"bibr\" target=\"#b18\">[13]</ref> optimizer with learning rate 0.01.</p><p>We consider two S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion: The interaction between queries and items are often highly skewed in a power-law distribution <ref type=\"bibr\" target=\"#b42\">[30]</ref>. So a small set of the popular items gets most of the inte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b4\">[4,</ref><ref type=\"bibr\" target=\"#b40\">29]</ref>, and logistic regression based recommenders <ref type=\"bibr\" target=\"#b27\">[19]</ref>, these deep models handle categorical features more effect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ning improving sequential user modeling in recommendations <ref type=\"bibr\" target=\"#b37\">[27,</ref><ref type=\"bibr\" target=\"#b52\">37,</ref><ref type=\"bibr\" target=\"#b60\">43]</ref>. Different from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  when it comes to modeling a huge catalogue of items in the order of millions (e.g., songs and apps <ref type=\"bibr\" target=\"#b39\">[28]</ref>) to even billions (e.g., videos on YouTube <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b4\">[4,</ref><ref type=\"bibr\" target=\"#b40\">29]</ref>, and logistic regression based recommenders <ref type=\"bibr\" target=\"#b27\">[19]</ref>, these deep models handle categorical features more effect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: viate this problem, we have previously tried to build a heterogeneous MOOC knowledge base, MOOCCube <ref type=\"bibr\" target=\"#b39\">[40]</ref>, with an initial attempt to integrate the two dataset cons type=\"bibr\" target=\"#b7\">[8]</ref>, LectureBank <ref type=\"bibr\" target=\"#b18\">[19]</ref>, MOOCCube <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p><p>Coverage. Our dataset contains rich concept-based b part with the preliminary version of MOOCCube. We have 6 times the number of concepts than MOOCCube <ref type=\"bibr\" target=\"#b39\">[40]</ref>. After matching our course to its, we compare the concept . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b24\">[25]</ref>, heterogeneous graph convolution in prerequisite discovery <ref type=\"bibr\" target=\"#b14\">[15]</ref>, etc. These attempts require large-scale, high-quality dat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ontent, i.e. modeling and organizing the learning materials with text mining or knowledge discovery <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>; <ref type=\"bibr\"  ort the tasks of student modeling and cognitive modeling. The resource-centered datasets, e.g., PRL <ref type=\"bibr\" target=\"#b22\">[23]</ref>, UniCourse <ref type=\"bibr\" target=\"#b19\">[20]</ref> and T on, which can be divided into three categories: Educational knowledge discovery datasets, i.e., PRL <ref type=\"bibr\" target=\"#b22\">[23]</ref>, UniCourse <ref type=\"bibr\" target=\"#b19\">[20]</ref> and N. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b3\">[4]</ref> with token classification loss by Huggingface Transformers <ref type=\"bibr\" target=\"#b37\">[38]</ref>. We concatenate a video's titles before its subtitles as h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ternal resources as a supplement. Academic Papers: We crawl 10 corresponding papers from ArnetMiner <ref type=\"bibr\" target=\"#b33\">[34]</ref> for each concept. Blogs and Technical QA: Employing concep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  into learner-centered and resource-centered.</p><p>The learner-centered datasets, e.g., ASSISTment <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, KDD Cup <ref type=\" PTEL <ref type=\"bibr\" target=\"#b29\">[30]</ref>;</p><p>Learning analytics datasets, i.e., ASSISTment <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, KDD Cup <ref type=\" n, knowledge-enhanced recommendation, and cognitive modeling <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. A common solution fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and social activities <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Therefore, we collect granular records from XuetangX, inc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s concept candidates, which can help us to extract the concepts of large-scale knowledge base Xlore <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>(3) Named Entity Recognition. We adopt pre-trained . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, an educational recommendation method that utilizes a family of Bayesian Networks.</p><p>\u2022 KGAT <ref type=\"bibr\" target=\"#b36\">[37]</ref>, a GNN-based method that employs the background knowledge . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  into learner-centered and resource-centered.</p><p>The learner-centered datasets, e.g., ASSISTment <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, KDD Cup <ref type=\" PTEL <ref type=\"bibr\" target=\"#b29\">[30]</ref>;</p><p>Learning analytics datasets, i.e., ASSISTment <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, KDD Cup <ref type=\" n, knowledge-enhanced recommendation, and cognitive modeling <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. A common solution fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing. The resource-centered datasets, e.g., PRL <ref type=\"bibr\" target=\"#b22\">[23]</ref>, UniCourse <ref type=\"bibr\" target=\"#b19\">[20]</ref> and TutorialBank <ref type=\"bibr\" target=\"#b7\">[8]</ref>,  ef>. However, the sparsity of this relationship poses a challenge for the annotation and extraction <ref type=\"bibr\" target=\"#b19\">[20]</ref>. In MOOCCubeX, we propose an interactive co-training metho tional knowledge discovery datasets, i.e., PRL <ref type=\"bibr\" target=\"#b22\">[23]</ref>, UniCourse <ref type=\"bibr\" target=\"#b19\">[20]</ref> and NPTEL <ref type=\"bibr\" target=\"#b29\">[30]</ref>;</p><p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the zero-shot capability.</p><p>LMs are used in factual probing tasks, by using the outputs alone <ref type=\"bibr\" target=\"#b25\">(Petroni et al., 2019)</ref> to answer the relation-specific queries   of 3 relations and 5,527 facts, and T-REx with 41 relations and 34,039 facts of the LAMA benchmark <ref type=\"bibr\" target=\"#b25\">(Petroni et al., 2019)</ref>. We evaluate the results using mean prec make predictions given the sentence known to express the fact. Two methods are considered: (i) LAMA <ref type=\"bibr\" target=\"#b25\">(Petroni et al., 2019)</ref> leverages the input sentence without the <p>Implementation Details We remove triples with empty predicates from the original T-REx (Elsahar  <ref type=\"bibr\" target=\"#b25\">(Petroni et al., 2019)</ref>. This results in approximately 4 million. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: (Joshi et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Gao et al., 2019)</ref> or few-shot regime <ref type=\"bibr\" target=\"#b33\">(Soares et al., 2019)</ref> with architecture modifications. Sequence. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: c.org/ns/1.0\"><head>A.2 Relation Classification</head><p>Datasets and Metrics We evaluate on FewRel <ref type=\"bibr\" target=\"#b12\">(Han et al., 2018)</ref> and TACRED <ref type=\"bibr\" target=\"#b40\">(Z. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"figure\" target=\"#fig_4\">4</ref>. Experimenting on more OIE benchmark datasets such as CaRB <ref type=\"bibr\" target=\"#b1\">(Bhardwaj et al., 2019)</ref> is an interesting future direction to ex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (Soares et al., 2019)</ref> with architecture modifications. Sequenceto-sequence models, such as T5 <ref type=\"bibr\" target=\"#b29\">(Raffel et al., 2020)</ref>, BART <ref type=\"bibr\" target=\"#b17\">(Lew. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ystems recently evaluated in <ref type=\"bibr\" target=\"#b36\">(Stanovsky et al., 2018)</ref>: ClausIE <ref type=\"bibr\" target=\"#b6\">(Del Corro and Gemulla, 2013)</ref>, Open IE4 4 , PropS <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b21\">(Liu et al., 2019)</ref>, transfer well to relation classification datasets in fine-tuning <ref type=\"bibr\" target=\"#b14\">(Joshi et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Gao et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>; three news datasets NYT, WEB <ref type=\"bibr\" target=\"#b22\">(Mesquita et al., 2013)</ref>, PENN <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2013)</ref>. The statistics of the benchmark is shown in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: enceto-sequence models, such as T5 <ref type=\"bibr\" target=\"#b29\">(Raffel et al., 2020)</ref>, BART <ref type=\"bibr\" target=\"#b17\">(Lewis et al., 2020)</ref> and GLM <ref type=\"bibr\" target=\"#b8\">(Du . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y and Dagan, 2016)</ref>, a dataset from Newswire and Wikipedia automatically converted from QA-SRL <ref type=\"bibr\" target=\"#b13\">(He et al., 2015)</ref>; three news datasets NYT, WEB <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  extracting useful representations from non-euclidean data <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>. Almost invariably, different methodological perspectives: GraphCL <ref type=\"bibr\" target=\"#b65\">[66]</ref>, InfoGraph <ref type=\"bibr\" target=\"#b45\">[46]</ref> and MVGRL <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Graph pe=\"table\">3</ref>: Inductive Bias on Benchmark Datasets. Following the same evaluation protocol as <ref type=\"bibr\" target=\"#b45\">[46]</ref>, we generate embeddings from an untrained N-Layer GIN enco same backbone Graph Isomorphism Network ( <ref type=\"bibr\" target=\"#b61\">[62]</ref>) as InfoGraph ( <ref type=\"bibr\" target=\"#b45\">[46]</ref>) and GraphCL ( <ref type=\"bibr\" target=\"#b65\">[66]</ref>)  and epochs trained of (32, NA, NA) for RAND (Random Initialization), (512,0.001,20) for InfoGraph ( <ref type=\"bibr\" target=\"#b45\">[46]</ref>), and (32,0.01,20) for GraphCL ( <ref type=\"bibr\" target=\" EEZER and GITHUB-SGZR, we report results from GraphCL <ref type=\"bibr\" target=\"#b65\">[66]</ref> and <ref type=\"bibr\" target=\"#b45\">[46]</ref>. We use the same GIN encoder as GraphCL when reporting the ted to avoid negative transfer between tasks. Many unsupervised approaches have also been proposed. <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> draw inspiration f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: opular unsupervised learning paradigm for extracting useful representations from non-euclidean data <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" ta type=\"bibr\" target=\"#b65\">[66]</ref>, InfoGraph <ref type=\"bibr\" target=\"#b45\">[46]</ref> and MVGRL <ref type=\"bibr\" target=\"#b18\">[19]</ref>. GraphCL mirrors SimCLR, using a shared GNN encoder that g get=\"#b65\">[66]</ref> Node Dropping, Edge Adding/Dropping, Attr. Masking, Subgraph Extraction MVGRL <ref type=\"bibr\" target=\"#b18\">[19]</ref> PPR Diffusion + Sampling SelfGNN <ref type=\"bibr\" target=\" 0\">[21]</ref> and maximize the mutual information between global and local representations. MVGRL ( <ref type=\"bibr\" target=\"#b18\">[19]</ref>) contrasts different views at multiple granularities simil s used to train all models. A Nvidia Tesla K80 GPU is used to train all models. Results for MVGRL ( <ref type=\"bibr\" target=\"#b18\">[19]</ref>) are not included as we consistently witnessed Out-Of-Memo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: non-euclidean data <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>. Almost invariably, these frameworks rely on domain-agnosti et=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b64\">65,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>, more similar to the composable augmentations used in VCL < L frameworks for graph classification that represent different methodological perspectives: GraphCL <ref type=\"bibr\" target=\"#b65\">[66]</ref>, InfoGraph <ref type=\"bibr\" target=\"#b45\">[46]</ref> and M n-agnostic graph augmentations or DAGA. An extensive empirical study on the benefits of DAGA in GCL <ref type=\"bibr\" target=\"#b65\">[66]</ref> demonstrated that composition of augmentations improves pe ef type=\"foot\" target=\"#foot_2\">3</ref> are used. We consider the base encoder architecture used by <ref type=\"bibr\" target=\"#b65\">[66]</ref> and report results with graph convolutional layers from GI  augmented dataset by using random node/subgraph dropping at 20% of the graph size, as suggested by <ref type=\"bibr\" target=\"#b65\">[66]</ref> and (iii) evaluate on clean and augmented training data se der and perform classification using LinearSVC. Results for GraphCL and InfoGraph are reported from <ref type=\"bibr\" target=\"#b65\">[66]</ref>. Best accuracy is in bold; other models whose accuracy, in  following architecture is used for experiments: the encoder is 5-layer GIN architecture similar to <ref type=\"bibr\" target=\"#b65\">[66]</ref> and <ref type=\"bibr\" target=\"#b12\">[13]</ref>. The predict graph space while also breaking view symmetry.</p><p>Domain-Agnostic Graph Augmentations. Following <ref type=\"bibr\" target=\"#b65\">[66]</ref>, we apply random node dropping at 20% of the graph size to ality) GCC <ref type=\"bibr\" target=\"#b40\">[41]</ref> RWR Subgraph Extraction of Ego Network GraphCL <ref type=\"bibr\" target=\"#b65\">[66]</ref> Node Dropping, Edge Adding/Dropping, Attr. Masking, Subgra \" target=\"#b61\">[62]</ref>) as InfoGraph ( <ref type=\"bibr\" target=\"#b45\">[46]</ref>) and GraphCL ( <ref type=\"bibr\" target=\"#b65\">[66]</ref>) when training all dataset but GOSSIPCOP. The following tr s included between the output of the backbone and cross entropy layer. For augmentations, we follow <ref type=\"bibr\" target=\"#b65\">[66]</ref> and stochastically apply node dropping at 20% of graph siz .001,20) for InfoGraph ( <ref type=\"bibr\" target=\"#b45\">[46]</ref>), and (32,0.01,20) for GraphCL ( <ref type=\"bibr\" target=\"#b65\">[66]</ref>). The Adam optimizer ( <ref type=\"bibr\" target=\"#b24\">[25] erimental Setup: For all datasets, excluding DEEZER and GITHUB-SGZR, we report results from GraphCL <ref type=\"bibr\" target=\"#b65\">[66]</ref> and <ref type=\"bibr\" target=\"#b45\">[46]</ref>. We use the  et=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b65\">66,</ref><ref type=\"bibr\" target=\"#b71\">72]</ref> use augmentations t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in various scientific and web applications, such as graph-based similarity search for web documents <ref type=\"bibr\" target=\"#b15\">[16]</ref>, fake news detection by classifying their propagation patt cast document classification as a graph classification problem which has been shown to be promising <ref type=\"bibr\" target=\"#b15\">[16]</ref> as it captures not only the content but also the structure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>. Intuitively, data . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>. Intuitively, data . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  search, anomaly detection, and clustering have traditionally relied on hand-crafted graph features <ref type=\"bibr\" target=\"#b2\">[3]</ref>. However, leveraging recent advances in graph neural network. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rating process <ref type=\"bibr\" target=\"#b72\">[73]</ref>.  <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, where words are treated as nodes, edges indicate co-occurr fication: Documents are represented as co-occurrence graphs<ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, where words are treated as nodes, edges indicate co-occurr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in various scientific and web applications, such as graph-based similarity search for web documents <ref type=\"bibr\" target=\"#b15\">[16]</ref>, fake news detection by classifying their propagation patt cast document classification as a graph classification problem which has been shown to be promising <ref type=\"bibr\" target=\"#b15\">[16]</ref> as it captures not only the content but also the structure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s not only the content but also the structure of the documents.</p><p>Dataset &amp; Task. Following <ref type=\"bibr\" target=\"#b35\">[36]</ref>, we convert the Subjectivity document classification datas k is to predict subjectivity.</p><p>Setup of GNN models. We use a Message Passing Attention Network <ref type=\"bibr\" target=\"#b35\">[36]</ref> as the encoder, and a 2-layer MLP as the predictor. The re nal training details are given in Appendix D. We report results with the original GCN layer used by <ref type=\"bibr\" target=\"#b35\">[36]</ref>, as well as with the GraphSAGE <ref type=\"bibr\" target=\"#b  sizes <ref type=\"bibr\">(2 and 4)</ref>. Given that we use the default hyperparameters suggested by <ref type=\"bibr\" target=\"#b35\">[36]</ref>, it is possible that BYOL has not yet converged for all au ed text classification task. Experimental Setup:We use the model, code base and default settings of <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Models are trained using Adam: lr = 0.001, weightdecay =  ns as direct perturbation of the datagenerating process <ref type=\"bibr\" target=\"#b72\">[73]</ref>.  <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, where words are t gure 3: Augmentations for Document Classification: Documents are represented as co-occurrence graphs<ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, where words are t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erform their supervised counterparts on several vision tasks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Such visual contrastive learning (VCL) frameworks maximize \" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Furthermore, by leveraging strong data augmentations, thes  high quality datasets <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  20% of graph size. For GOSSIPCOP, the encoder is based off the implementation in PyTorch Geometric <ref type=\"bibr\" target=\"#b13\">[14]</ref> </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> r\" target=\"#b12\">[13]</ref>. This model is converted from DeepGraphLibrary 5 to Pytorch Geometric ( <ref type=\"bibr\" target=\"#b13\">[14]</ref>). The following hyper-parameters are used: LR=5e-4, Hidden. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed by learning effective graph representations automatically <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Indeed, graph representation learning has started playing  ake news detection by classifying their propagation patterns <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re a non-trivial baseline for GCL frameworks and should be included in results. While a recent work <ref type=\"bibr\" target=\"#b62\">[63]</ref> includes randomlyinitialized baselines as part of its resu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  recent work finds better mutual information estimates do not necessarily entail better performance <ref type=\"bibr\" target=\"#b51\">[52]</ref>, empirically, large batch sizes are required to effectivel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> and (ii) leveraging strong, arXiv:2111.03220v1 [cs.LG] 5 No red between two augmented views, where the quality of the estimation is upper bounded by batch size <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. While recent work. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  recent work finds better mutual information estimates do not necessarily entail better performance <ref type=\"bibr\" target=\"#b51\">[52]</ref>, empirically, large batch sizes are required to effectivel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  necessitating large datasets, to ensure that enough negative samples are available to train stably <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target tations of augmented graph views. Similar to DIM <ref type=\"bibr\" target=\"#b20\">[21]</ref> and SwAV <ref type=\"bibr\" target=\"#b5\">[6]</ref>, InfoGraph directly maximizes the mutual information between. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b73\">74]</ref>. Unsupervised learning is a natural paradigm for such label. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>. Intuitively, data . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in various scientific and web applications, such as graph-based similarity search for web documents <ref type=\"bibr\" target=\"#b15\">[16]</ref>, fake news detection by classifying their propagation patt cast document classification as a graph classification problem which has been shown to be promising <ref type=\"bibr\" target=\"#b15\">[16]</ref> as it captures not only the content but also the structure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  using a shared GNN encoder that generates representations of augmented graph views. Similar to DIM <ref type=\"bibr\" target=\"#b20\">[21]</ref> and SwAV <ref type=\"bibr\" target=\"#b5\">[6]</ref>, InfoGrap  type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> draw inspiration from <ref type=\"bibr\" target=\"#b20\">[21]</ref> and maximize the mutual information between global and loc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> and (ii) leveraging strong, arXiv:2111.03220v1 [cs.LG] 5 No red between two augmented views, where the quality of the estimation is upper bounded by batch size <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. While recent work. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: us on commonly used graph classification datasets (Table <ref type=\"table\" target=\"#tab_1\">1</ref>) <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Official implementations for GraphCL<ref type=\"foot\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rating process <ref type=\"bibr\" target=\"#b72\">[73]</ref>.  <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, where words are treated as nodes, edges indicate co-occurr fication: Documents are represented as co-occurrence graphs<ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, where words are treated as nodes, edges indicate co-occurr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  been noted before <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b68\">69]</ref>, we aim to better contextualize the performance of GCL fram. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  20% of graph size. For GOSSIPCOP, the encoder is based off the implementation in PyTorch Geometric <ref type=\"bibr\" target=\"#b13\">[14]</ref> </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> r\" target=\"#b12\">[13]</ref>. This model is converted from DeepGraphLibrary 5 to Pytorch Geometric ( <ref type=\"bibr\" target=\"#b13\">[14]</ref>). The following hyper-parameters are used: LR=5e-4, Hidden. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re a non-trivial baseline for GCL frameworks and should be included in results. While a recent work <ref type=\"bibr\" target=\"#b62\">[63]</ref> includes randomlyinitialized baselines as part of its resu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s not only the content but also the structure of the documents.</p><p>Dataset &amp; Task. Following <ref type=\"bibr\" target=\"#b35\">[36]</ref>, we convert the Subjectivity document classification datas k is to predict subjectivity.</p><p>Setup of GNN models. We use a Message Passing Attention Network <ref type=\"bibr\" target=\"#b35\">[36]</ref> as the encoder, and a 2-layer MLP as the predictor. The re nal training details are given in Appendix D. We report results with the original GCN layer used by <ref type=\"bibr\" target=\"#b35\">[36]</ref>, as well as with the GraphSAGE <ref type=\"bibr\" target=\"#b  sizes <ref type=\"bibr\">(2 and 4)</ref>. Given that we use the default hyperparameters suggested by <ref type=\"bibr\" target=\"#b35\">[36]</ref>, it is possible that BYOL has not yet converged for all au ed text classification task. Experimental Setup:We use the model, code base and default settings of <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Models are trained using Adam: lr = 0.001, weightdecay =  ns as direct perturbation of the datagenerating process <ref type=\"bibr\" target=\"#b72\">[73]</ref>.  <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, where words are t gure 3: Augmentations for Document Classification: Documents are represented as co-occurrence graphs<ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b69\">70]</ref>, where words are t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ence as a benchmark for evaluating the performance of GNNs <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, and corrects many limitations addressed in Sec. 3. Specifi ation, a standard benchmark for evaluating GNN performance <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>.</p><p>Dataset &amp; Task. We follow the established protoc Dataset &amp; Task. We follow the established protocols in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> to create super-pixel representations of MNIST, where each . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Attackers can modify the original graph by adding or remov ng malicious nodes <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> to conduct adversarial attacks. Despite the relatively mino n Appendix A.4.3.  <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and are reprocessed for GRB. The basic statistics of these  ef type=\"bibr\" target=\"#b15\">[16]</ref> ---SPEIT <ref type=\"bibr\" target=\"#b16\">[17]</ref> ---TDGIA <ref type=\"bibr\" target=\"#b17\">[18]</ref> ---GRB Mod. Scenario ---GRB Inj. Scenario ---GRB Support \u2020  <ref type=\"bibr\" target=\"#b15\">[16]</ref>, SPEIT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TD-GIA <ref type=\"bibr\" target=\"#b17\">[18]</ref>). Facing the problem of scalability, some attacks are not  ef type=\"bibr\" target=\"#b33\">[34]</ref>, SPEIT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and TDGIA <ref type=\"bibr\" target=\"#b17\">[18]</ref>. More details can be found in Appendix A.4.2.</p><p>Five D a is based on the assumption that nodes with lower degrees are easier to attack, as demonstrated in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. If a target node has few neighbors, it is more likely to  D <ref type=\"bibr\" target=\"#b33\">[34]</ref>, SPEIT <ref type=\"bibr\" target=\"#b16\">[17]</ref>, TDGIA <ref type=\"bibr\" target=\"#b17\">[18]</ref>. They are all scalable and FGSM, PGD, SPEIT, TDGIA need to  a universal black-box graph injection attack. TDGIA (Topological Defective Graph Injection Attack) <ref type=\"bibr\" target=\"#b17\">[18]</ref> is an effective graph injection attack that tackles the to el (trained by the attackers themselves), and then transfer to the target model. As demonstrated in <ref type=\"bibr\" target=\"#b17\">[18]</ref>, the choice of surrogate model will influence the transfer el (trained by the attackers themselves), and then transfer to the target model. As demonstrated in <ref type=\"bibr\" target=\"#b17\">[18]</ref>, the choice of surrogate model will influence the transfer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b20\">[21]</ref>, GRAND <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and ProGNN <ref type=\"bibr\" target=\"#b22\">[23]</ref> are designed to improve the adversarial robustness of GNNs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b2\">3]</ref> to graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref  domains, such as social network analysis <ref type=\"bibr\" target=\"#b0\">[1]</ref>, molecular graphs <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and recommender systems <ref type=\"bibr\" target=\"#b9\">[10]< <ref type=\"bibr\" target=\"#b7\">[8]</ref>, TAGCN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Note that the <ref type=\"bibr\" target=\"#b7\">[8]</ref>, TAGCN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. All models ar ic way to design a set of fixed-size learnable filters to perform convolutions on graphs. GraphSAGE <ref type=\"bibr\" target=\"#b4\">[5]</ref> is a general inductive framework that leverages node feature. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b11\">[12]</ref> --Nettack <ref type=\"bibr\" target=\"#b11\">[12]</ref> --RL-S2V <ref type=\"bibr\" target=\"#b29\">[30]</ref> --Metattack <ref type=\"bibr\" target=\"#b12\">[13]</ref> ---S k <ref type=\"bibr\" target=\"#b11\">[12]</ref>, FGSM <ref type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Metattack <ref type=\"bibr\" target=\"#b12\">[13]</ref>). Dif gh time complexity <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> or expensive memory consumption <ref type=\"bibr\" target=\"#b us Works</head><p>Many of the previous adversarial attacks <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> consider the poison. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b0\">[1]</ref>, molecular graphs <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and recommender systems <ref type=\"bibr\" target=\"#b9\">[10]</ref>. However, GML models are known to be vulnerable to adversar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the neighbor importance estimation and the layer-wise graph memory for defenses. RobustGCN (R-GCN) <ref type=\"bibr\" target=\"#b50\">[51]</ref> is a GCN variant that is specially designed against advers. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dversarial attacks <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar \"bibr\" target=\"#b19\">20]</ref>, perturbing node attributes <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar plicable to large graphs due to their high time complexity <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> or expensive memory type=\"bibr\" target=\"#b11\">[12]</ref> --RL-S2V <ref type=\"bibr\" target=\"#b29\">[30]</ref> --Metattack <ref type=\"bibr\" target=\"#b12\">[13]</ref> ---STACK <ref type=\"bibr\" target=\"#b30\">[31]</ref> ---AFGS f type=\"bibr\" target=\"#b11\">[12]</ref>, RL-S2V <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Metattack <ref type=\"bibr\" target=\"#b12\">[13]</ref>). Differently, graph injection attacks add new malicious n dversarial attacks <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> consider the poisoning attack and develop the notion of unn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : Large-scale citation networks. The papers are collected from the academic searching engine Aminer <ref type=\"bibr\" target=\"#b45\">[46]</ref>, and the dataset was used in KDD-CUP 2020 Graph Adversaria. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, have shown promising performance in various domains, such as  <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Note that these models are not originally designed to incre  <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. All models are salable to large graphs. (2) For modificatio generate node embeddings for previously unseen data. SGCN (Simplified Graph Convolutional Networks) <ref type=\"bibr\" target=\"#b8\">[9]</ref> removes nonlinearities and collapses weight matrices between. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : Large-scale citation networks. The papers are collected from the academic searching engine Aminer <ref type=\"bibr\" target=\"#b45\">[46]</ref>, and the dataset was used in KDD-CUP 2020 Graph Adversaria. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecommender systems applications, there is another ubiquitous challenge: the Strict Cold Start (SCS) <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, wherein some \"nod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> benefit from effici \" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. Inductive variants. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: representations. Other methods like the spectral diffusion <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the student model that looks to mimic the GNN. Inspired by the positional encoding in Transformers <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we train the teacher GNN to additionally learn a set of n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ry few connections <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. In Figure <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> benefit from effici \" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. Inductive variants. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e to the tail/cold-start nodes.</p><p>we adopt the teacher-student knowledge distillation procedure <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> and propose Cold B ode representations learned by the teacher GNN.</p><p>We adopt the knowledge distillation procedure <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> to improve the qua #b18\">[19]</ref> and <ref type=\"bibr\" target=\"#b19\">[20]</ref> employ a transfer learning approach. <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes a knowledge distillation for GNN, while <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the student model that looks to mimic the GNN. Inspired by the positional encoding in Transformers <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we train the teacher GNN to additionally learn a set of n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ) neighborhood information to learn node representations. Other methods like the spectral diffusion <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eceding layers only at the final graph convolutional layer <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. For the types of normalizations, we grid search over: batc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l entity representations. It considers not only the weights of different nodes with different types <ref type=\"bibr\" target=\"#b7\">(Hu et al., 2019)</ref> but also the edge directions in the heterogene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion, and W e \u2208 R N \u00d7M is a transformation matrix. To measure the embedding closeness and relevance <ref type=\"bibr\" target=\"#b18\">(Shen et al., 2018)</ref>, we design our comparison function as:</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ation for improving fake news detection, since fake news detection and topics are highly correlated <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Jin et al., 2 \"bibr\" target=\"#b5\">(Gupta et al., 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Zhang et al., 2020)</ref> have been built to model the propagation pr et=\"#b29\">Zhang et al., 2020)</ref> have been built to model the propagation process. For instance, <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020)</ref> constructed a heterogeneous network of new topics for enriching news representation since fake news detection is highly correlated with topics <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020)</ref>. Specifically, we first construct a direct ch sentence with every other sentence. Since topic information is important for fake news detection <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020)</ref>, we apply the unsupervised LDA <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref> and heterogeneous credibility networks <ref type=\"bibr\" target=\"#b5\">(Gupta et al., 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Zhang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ws can easily misguide public opinion, cause the crisis of confidence, and disturb the social order <ref type=\"bibr\" target=\"#b25\">(Vosoughi et al., 2018)</ref>. It is well known that fake news exerte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: neous <ref type=\"bibr\" target=\"#b8\">(Jin et al., 2016)</ref> and heterogeneous credibility networks <ref type=\"bibr\" target=\"#b5\">(Gupta et al., 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shu et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding methods can be applied to obtain structured entity embeddings. Due to the simplicity of TransE <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013)</ref>, we adopted TransE to learn entity represe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b23\">(Thorne et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Zhong et al., 2020)</ref>. Generally, fake news detection usually foc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (e.g., a subject-predicateobject triple) <ref type=\"bibr\" target=\"#b23\">(Thorne et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Zhong et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  detection <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020)</ref>, we apply the unsupervised LDA <ref type=\"bibr\" target=\"#b1\">(Blei et al., 2003)</ref> (the total topic number K is set as 100) to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><head n=\"2\">Related Work</head><p>Fake news detection has attracted much attention in recent years <ref type=\"bibr\" target=\"#b32\">(Zhou and Zafarani, 2020;</ref><ref type=\"bibr\" target=\"#b12\">Oshikaw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eural networks (CNN) have been employed <ref type=\"bibr\" target=\"#b12\">(Oshikawa et al., 2020;</ref><ref type=\"bibr\" target=\"#b26\">Wang, 2017;</ref><ref type=\"bibr\">Rodr\u00edguez and Iglesias, 2019)</ref> r fake news classifier. To avoid hand-crafted feature engineering, neural models have been proposed <ref type=\"bibr\" target=\"#b26\">(Wang, 2017;</ref><ref type=\"bibr\">Rodr\u00edguez and Iglesias, 2019)</ref so use topics to enrich the news representation for improving fake news detection.</p><p>Some works <ref type=\"bibr\" target=\"#b26\">(Wang, 2017;</ref><ref type=\"bibr\" target=\"#b9\">Khattar et al., 2019;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tilize users' opinions to infer news veracity <ref type=\"bibr\" target=\"#b8\">(Jin et al., 2016;</ref><ref type=\"bibr\" target=\"#b28\">Wu et al., 2019)</ref>. Tacchini et al. constructed a bipartite netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep neural models: LSTM <ref type=\"bibr\" target=\"#b6\">(Hochreiter and Schmidhuber, 1997)</ref>, CNN <ref type=\"bibr\" target=\"#b11\">(Kim, 2014)</ref>, BERT+LSTM <ref type=\"bibr\" target=\"#b24\">(Vaibhav . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tilize users' opinions to infer news veracity <ref type=\"bibr\" target=\"#b8\">(Jin et al., 2016;</ref><ref type=\"bibr\" target=\"#b28\">Wu et al., 2019)</ref>. Tacchini et al. constructed a bipartite netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  detection <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020)</ref>, we apply the unsupervised LDA <ref type=\"bibr\" target=\"#b1\">(Blei et al., 2003)</ref> (the total topic number K is set as 100) to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep neural models: LSTM <ref type=\"bibr\" target=\"#b6\">(Hochreiter and Schmidhuber, 1997)</ref>, CNN <ref type=\"bibr\" target=\"#b11\">(Kim, 2014)</ref>, BERT+LSTM <ref type=\"bibr\" target=\"#b24\">(Vaibhav . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> and heterogeneous credibility networks <ref type=\"bibr\" target=\"#b5\">(Gupta et al., 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Zhang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b26\">(Wang, 2017;</ref><ref type=\"bibr\" target=\"#b9\">Khattar et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">Wang et al., 2020</ref>) also consider incorporating multi-modal feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on various hand-crafted linguistic and semantic features for differentiating between news documents <ref type=\"bibr\" target=\"#b3\">(Conroy et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Rubin et al. ews. A lot of existing works extract specific writing styles such as lexical and syntactic features <ref type=\"bibr\" target=\"#b3\">(Conroy et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Rubin et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b26\">(Wang, 2017;</ref><ref type=\"bibr\" target=\"#b9\">Khattar et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">Wang et al., 2020</ref>) also consider incorporating multi-modal feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b15\">Rashkin et al., 2017;</ref><ref type=\"bibr\">Khurana and Intelligentie, 2017;</ref><ref type=\"bibr\" target=\"#b19\">Shu et al., 2020)</ref>. To avoid feature engineering, deep neural mo br\">Khurana and Intelligentie, 2017;</ref><ref type=\"bibr\" target=\"#b15\">Rashkin et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">Shu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Oshikawa et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tilize users' opinions to infer news veracity <ref type=\"bibr\" target=\"#b8\">(Jin et al., 2016;</ref><ref type=\"bibr\" target=\"#b28\">Wu et al., 2019)</ref>. Tacchini et al. constructed a bipartite netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  detection <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020)</ref>, we apply the unsupervised LDA <ref type=\"bibr\" target=\"#b1\">(Blei et al., 2003)</ref> (the total topic number K is set as 100) to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> and heterogeneous credibility networks <ref type=\"bibr\" target=\"#b5\">(Gupta et al., 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Zhang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) <ref type=\"bibr\" target=\"#b23\">(Thorne et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Zhou et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: neous <ref type=\"bibr\" target=\"#b8\">(Jin et al., 2016)</ref> and heterogeneous credibility networks <ref type=\"bibr\" target=\"#b5\">(Gupta et al., 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shu et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: detection and topics are highly correlated <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Jin et al., 2016)</ref>. For example, the news documents in the \"healt nce-based and propagation-based. Stance-based models utilize users' opinions to infer news veracity <ref type=\"bibr\" target=\"#b8\">(Jin et al., 2016;</ref><ref type=\"bibr\" target=\"#b28\">Wu et al., 2019  news event is highly related to the credibilities of relevant social media posts. Both homogeneous <ref type=\"bibr\" target=\"#b8\">(Jin et al., 2016)</ref> and heterogeneous credibility networks <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b15\">Rashkin et al., 2017;</ref><ref type=\"bibr\">Khurana and Intelligentie, 2017;</ref><ref type=\"bibr\" target=\"#b19\">Shu et al., 2020)</ref>. To avoid feature engineering, deep neural mo br\">Khurana and Intelligentie, 2017;</ref><ref type=\"bibr\" target=\"#b15\">Rashkin et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">Shu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Oshikawa et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  detection <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2020)</ref>, we apply the unsupervised LDA <ref type=\"bibr\" target=\"#b1\">(Blei et al., 2003)</ref> (the total topic number K is set as 100) to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al.</ref> showed that trusted news and fake news have different patterns of sentence interactions <ref type=\"bibr\" target=\"#b24\">(Vaibhav et al., 2019)</ref>. They modeled a news document as a fully the sentence interactions and applied graph attention networks for learning document representation <ref type=\"bibr\" target=\"#b24\">(Vaibhav et al., 2019)</ref>. <ref type=\"bibr\">Pan et al. proposed</r >We conduct extensive experiments across various settings and datasets. Following the previous work <ref type=\"bibr\" target=\"#b24\">(Vaibhav et al., 2019)</ref>, we use SLN: Satirical and Legitimate Ne eiter and Schmidhuber, 1997)</ref>, CNN <ref type=\"bibr\" target=\"#b11\">(Kim, 2014)</ref>, BERT+LSTM <ref type=\"bibr\" target=\"#b24\">(Vaibhav et al., 2019)</ref> (BERT for sentence encoder and then LSTM r max pooling for learning news document representation. For fair comparison with the previous work <ref type=\"bibr\" target=\"#b24\">(Vaibhav et al., 2019)</ref>, we use LSTM to encode sentences with ra erimental results on validation set. The other hyper-parameters are set as the same as the baseline <ref type=\"bibr\" target=\"#b24\">(Vaibhav et al., 2019)</ref> for fair comparison. Specifically, all t news detection since different interaction patterns are observed in trusted and fake news documents <ref type=\"bibr\" target=\"#b24\">(Vaibhav et al., 2019)</ref>. Our model Com-pareNet further improves  e Study</head><p>To further illustrate why our model outperforms state-of-the-art baseline GAT+Attn <ref type=\"bibr\" target=\"#b24\">(Vaibhav et al., 2019)</ref>  <ref type=\"figure\">4</ref>, the content. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Vaibhav et al., 2019)</ref> (BERT for sentence encoder and then LSTM for document encoder) and BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref> (directly for document encoder). We also c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tilize users' opinions to infer news veracity <ref type=\"bibr\" target=\"#b8\">(Jin et al., 2016;</ref><ref type=\"bibr\" target=\"#b28\">Wu et al., 2019)</ref>. Tacchini et al. constructed a bipartite netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oposed fine-tuning framework, we study four strategies of data augmentation, i.e., subword sampling <ref type=\"bibr\" target=\"#b18\">(Kudo, 2018)</ref>, code-switch substitution <ref type=\"bibr\" target= iece <ref type=\"bibr\" target=\"#b19\">(Kudo and Richardson, 2018)</ref> with a unigram language model <ref type=\"bibr\" target=\"#b18\">(Kudo, 2018)</ref>. As one of our data augmentation strategies, we ap epresenting a sentence in different subword sequences can be viewed as a data augmentation strategy <ref type=\"bibr\" target=\"#b18\">(Kudo, 2018;</ref><ref type=\"bibr\" target=\"#b29\">Provilkov et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  by randomly replacing input words in the source language with translated words in target languages <ref type=\"bibr\" target=\"#b30\">(Qin et al., 2020)</ref>. By populating the dataset, their fine-tunin  i.e., subword sampling <ref type=\"bibr\" target=\"#b18\">(Kudo, 2018)</ref>, code-switch substitution <ref type=\"bibr\" target=\"#b30\">(Qin et al., 2020)</ref>, Gaussian noise <ref type=\"bibr\" target=\"#b0 ing, based on autogenerated soft pseudo-labels for translated text in the target language. Besides, <ref type=\"bibr\" target=\"#b30\">Qin et al. (2020)</ref> finetuned models on multilingual code-switch  /ref> analyzed the impact of anchor points in pre-training cross-lingual language models. Following <ref type=\"bibr\" target=\"#b30\">Qin et al. (2020)</ref>, we generate code-switch data in multiple lan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b24\">(Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b12\">Faruqui and Dyer, 2014;</ref><ref type=\"bibr\" target=\"#b14\">Guo et al., 2015;</ref><ref type=\"bibr\" target=\"#b35\">Xu et al., 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rg/ns/1.0\"><head n=\"1\">Introduction</head><p>Pre-trained cross-lingual language models (Conneau and <ref type=\"bibr\" target=\"#b7\">Lample, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Conneau et al., 2020a >, most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models <ref type=\"bibr\" target=\"#b7\">(Conneau and Lample, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Conneau  hows that data augmentation is helpful to improve performance on the target languages. For example, <ref type=\"bibr\" target=\"#b7\">Conneau and Lample (2019)</ref> add translated examples to the trainin he XTREME benchmark. Results of mBERT<ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019)</ref>, XLM<ref type=\"bibr\" target=\"#b7\">(Conneau and Lample, 2019)</ref> and XLM-R large</figDesc><table><row>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uage processing area. Both adversarial noise <ref type=\"bibr\" target=\"#b40\">(Zhu et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al., 2 . The stopgrad(\u2022) operation 2 is used to stop back-propagating gradients, which is also employed in <ref type=\"bibr\" target=\"#b17\">(Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es learning crosslingual word embeddings <ref type=\"bibr\" target=\"#b24\">(Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b12\">Faruqui and Dyer, 2014;</ref><ref type=\"bibr\" target=\"#b14\">Guo et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: S-X <ref type=\"bibr\" target=\"#b36\">(Yang et al., 2019)</ref>, three span extraction datasets: XQuAD <ref type=\"bibr\" target=\"#b1\">(Artetxe et al., 2020)</ref>, <ref type=\"bibr\">MLQA (Lewis et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trained cross-lingual language models (Conneau and <ref type=\"bibr\" target=\"#b7\">Lample, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Conneau et al., 2020a;</ref><ref type=\"bibr\" target=\"#b4\">Chi et al.,  trained cross-lingual language models <ref type=\"bibr\" target=\"#b7\">(Conneau and Lample, 2019;</ref><ref type=\"bibr\" target=\"#b6\">Conneau et al., 2020a;</ref><ref type=\"bibr\" target=\"#b4\">Chi et al.,  8\">(Kudo, 2018;</ref><ref type=\"bibr\" target=\"#b29\">Provilkov et al., 2020)</ref>. We utilize XLM-R <ref type=\"bibr\" target=\"#b6\">(Conneau et al., 2020a)</ref> as our pre-trained cross-lingual languag nslate to obtain translations for these two datasets.</p><p>Implementation Details We utilize XLM-R <ref type=\"bibr\" target=\"#b6\">(Conneau et al., 2020a)</ref> as our pre-trained cross-lingual languag et and have a higher degree of overlap in languages. For TyDiQA-GoldP, we use the English test set  <ref type=\"bibr\" target=\"#b6\">(Conneau et al., 2020a)</ref> are taken from <ref type=\"bibr\" target=\" upplementary document.</p><p>Fine-Tuning Settings We consider two typical fine-tuning settings from <ref type=\"bibr\" target=\"#b6\">Conneau et al. (2020a)</ref> and <ref type=\"bibr\" target=\"#b15\">Hu et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nces can be viewed as a data augmentation strategy <ref type=\"bibr\" target=\"#b18\">(Kudo, 2018;</ref><ref type=\"bibr\" target=\"#b29\">Provilkov et al., 2020)</ref>. We utilize XLM-R <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ubstitution</head><p>Anchor points have been shown useful to improve cross-lingual transferability. <ref type=\"bibr\" target=\"#b9\">Conneau et al. (2020b)</ref> analyzed the impact of anchor points in p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ubstitution</head><p>Anchor points have been shown useful to improve cross-lingual transferability. <ref type=\"bibr\" target=\"#b9\">Conneau et al. (2020b)</ref> analyzed the impact of anchor points in p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ubstitution</head><p>Anchor points have been shown useful to improve cross-lingual transferability. <ref type=\"bibr\" target=\"#b9\">Conneau et al. (2020b)</ref> analyzed the impact of anchor points in p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b24\">(Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b12\">Faruqui and Dyer, 2014;</ref><ref type=\"bibr\" target=\"#b14\">Guo et al., 2015;</ref><ref type=\"bibr\" target=\"#b35\">Xu et al., 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uage processing area. Both adversarial noise <ref type=\"bibr\" target=\"#b40\">(Zhu et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al., 2 . The stopgrad(\u2022) operation 2 is used to stop back-propagating gradients, which is also employed in <ref type=\"bibr\" target=\"#b17\">(Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b15\">(Hu et al., 2020)</ref>, including two classification datasets: XNLI <ref type=\"bibr\" target=\"#b8\">(Conneau et al., 2018)</ref>, PAWS-X <ref type=\"bibr\" target=\"#b36\">(Y. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> and various data augmentation approaches <ref type=\"bibr\" target=\"#b16\">(Hu et al., 2017;</ref><ref type=\"bibr\" target=\"#b37\">Ye et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Xie et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng et al., 2016)</ref>, adversarial noise <ref type=\"bibr\" target=\"#b25\">(Miyato et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Carmon et al., 2019)</ref> and various data augmentation approaches <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2020)</ref>. Similar ideas are used in the natural language processing area. Both adversarial noise <ref type=\"bibr\" target=\"#b40\">(Zhu et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Jiang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b24\">(Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b12\">Faruqui and Dyer, 2014;</ref><ref type=\"bibr\" target=\"#b14\">Guo et al., 2015;</ref><ref type=\"bibr\" target=\"#b35\">Xu et al., 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: P <ref type=\"bibr\" target=\"#b5\">(Clark et al., 2020)</ref>, and two sequence labeling datasets: NER <ref type=\"bibr\" target=\"#b27\">(Pan et al., 2017)</ref>, POS <ref type=\"bibr\" target=\"#b26\">(Nivre e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s to be invariant to small perturbations on image data. The small perturbations can be random noise <ref type=\"bibr\" target=\"#b39\">(Zheng et al., 2016)</ref>, adversarial noise <ref type=\"bibr\" target is data augmentation with example consistency R 1 , the method is similar to the stability training <ref type=\"bibr\" target=\"#b39\">(Zheng et al., 2016)</ref>, random perturbation training <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b35\">(Taylor, 1953;</ref><ref type=\"bibr\" target=\"#b22\">Lewis et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">Lee et al., 2019)</ref> was the most-donated book for t he f ourt h y aylor, 1953)</ref> and its recent variants <ref type=\"bibr\" target=\"#b22\">(Lewis et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">Lee et al., 2019)</ref>, we render queries and summaries in a Unified. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: umulation every 4 steps. During decoding, we used beam search with beam size 5 and Trigram Blocking <ref type=\"bibr\" target=\"#b28\">(Paulus et al., 2018)</ref> to reduce redundancy. The cosine similari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: umulation every 4 steps. During decoding, we used beam search with beam size 5 and Trigram Blocking <ref type=\"bibr\" target=\"#b28\">(Paulus et al., 2018)</ref> to reduce redundancy. The cosine similari. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nge of NLP tasks. For example, <ref type=\"bibr\" target=\"#b34\">Su et al. (2020)</ref> fine-tune BART <ref type=\"bibr\" target=\"#b21\">(Lewis et al., 2020)</ref> on <ref type=\"bibr\">CNN/DailyMail (Hermann >(Su et al., 2020)</ref> uses an ensembled QA model to extract answer evidence, and fine-tuned BART <ref type=\"bibr\" target=\"#b21\">(Lewis et al., 2020)</ref> to iteratively generate summaries from par. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etrained language model for abstractive QFS. In experiments we employ the publicly released UNILMV2 <ref type=\"bibr\" target=\"#b2\">(Bao et al., 2020)</ref> to instantiate the controllable generator sho e performance of UNILM fine-tuned on Multi-News and CNN/DailyMail following the standard setting in <ref type=\"bibr\" target=\"#b2\">Bao et al. (2020)</ref>. It uses no query guidance or length control.  get=\"#b18\">(Laskar et al., 2020</ref><ref type=\"bibr\">) PQSUM-WSL (Laskar et al., 2020)</ref> UNILM <ref type=\"bibr\" target=\"#b2\">(Bao et al., 2020</ref>) MARGESUM their original document position. Th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref><ref type=\"bibr\" target=\"#b5\">Dang, 2005;</ref><ref type=\"bibr\" target=\"#b14\">Hoa, 2006;</ref><ref type=\"bibr\" target=\"#b3\">Baumel et al., 2016)</ref> are relatively small for modern data-hungry tal Setup</head><p>Datasets We performed experiments on the DUC 2005-2007 QFS benchmarks and TD-QFS <ref type=\"bibr\" target=\"#b3\">(Baumel et al., 2016)</ref>. DUC benchmarks contain long query narrati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etrained language model for abstractive QFS. In experiments we employ the publicly released UNILMV2 <ref type=\"bibr\" target=\"#b2\">(Bao et al., 2020)</ref> to instantiate the controllable generator sho e performance of UNILM fine-tuned on Multi-News and CNN/DailyMail following the standard setting in <ref type=\"bibr\" target=\"#b2\">Bao et al. (2020)</ref>. It uses no query guidance or length control.  get=\"#b18\">(Laskar et al., 2020</ref><ref type=\"bibr\">) PQSUM-WSL (Laskar et al., 2020)</ref> UNILM <ref type=\"bibr\" target=\"#b2\">(Bao et al., 2020</ref>) MARGESUM their original document position. Th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hort queries we automatically create query narratives in an unsupervised fashion. We employ LexRank <ref type=\"bibr\" target=\"#b8\">(Erkan and Radev, 2004)</ref> to select a subset of representative sen  to LEAD which returns all lead sentences of the most recent document (up to 250 words) and LEXRANK <ref type=\"bibr\" target=\"#b8\">(Erkan and Radev, 2004)</ref>, a widelyused unsupervised method based . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ert a sentence from natural language to UMR, we parse it with Open Information Extraction (Open IE; <ref type=\"bibr\" target=\"#b32\">Stanovsky et al. 2018</ref>) to a set of propositions consisting of v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t. One promising way to reduce the discrepancy is momentum constrastive learning (MoCo) proposed by <ref type=\"bibr\" target=\"#b9\">He et al. (2020)</ref>. In this method, a pair of fast/slow encoders a  queue size constant during training. Such formulation poses no problem for the original MoCo paper <ref type=\"bibr\" target=\"#b9\">(He et al., 2020)</ref>, because their \"questions\" and \"passages\" are . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e popularization of neural network in NLP, the dense passage retrieval approach has gained traction <ref type=\"bibr\" target=\"#b12\">(Karpukhin et al., 2020)</ref>. In this approach, a dual-encoder mode ed too easy to differentiate, previous work has mainly focused on how to generate \"hard\" negatives. <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>   <ref type=\"bibr\" target=\"#b5\">(Chen e ectively mimics the behavior of the \"in-batch negative\" strategy employed by previous works such as <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>, where the passages in one batch will s ERT-base <ref type=\"bibr\" target=\"#b7\">(Devlin et al., 2019</ref>) models as our encoders following <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>. The question and passage encoders util  L qp .</p><p>In this work, we only implement a simple method of generating hard examples following <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>: for each positive pair, we add one har idate set from the English Wikipedia dump from Dec. 20, 2018. Following the pre-processing steps in <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>, we first extract clean texts using pre i-c.org/ns/1.0\"><head n=\"5.2\">Question Answering Datasets</head><p>We use the five QA datasets from <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref> and follow their training/dev/test spli f> is original used as a benchmark for reading comprehension.</p><p>We follow the same procedure in <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref> to create positive passages for all dat aining sets after discarding unmatched questions. Note that our numbers are slightly different from <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref> due to small differences in the candida ocess.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Settings</head><p>Following <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>, we test our model on two settings: a \" ne is the classic BM25 baseline. The second baseline is the Deep Passage Retrieval (DPR) model from <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>. We also implement the setting where th ar scheduling with 5% warm-up. We didn't do hyperparameter search. We follow their specification in <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref> when re-implementing DPR baselines. Tra implementation with b = 0.4 (length normalization) and k 1 = 0.9 (term frequency scaling) following <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c orse than BM25. The lower performance on SQuAD than BM25 is consistent with previous observation in <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>. All the baseline numbers are our re-im s are our re-implementations and are comparable but slightly different from the numbers reported in <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref> due to the difference in the pre-proces is interesting to see how the improvement would translate into the end-to-end QA results. Following <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>, we implement a simple BERT based reade sformation over the vectors of the start tokens of all passages. We follow the training strategy of <ref type=\"bibr\" target=\"#b12\">Karpukhin et al. (2020)</ref>, and sample one positive passages and 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ssage retrieval can also be done efficiently with vector space search methods during inference time <ref type=\"bibr\" target=\"#b19\">(Shrivastava and Li, 2014)</ref>.</p><p>Dense retrieval models are us e, we only need to compute the vector for the question, and the maximum inner product search (MIPS) <ref type=\"bibr\" target=\"#b19\">(Shrivastava and Li, 2014</ref>) can be used to efficiently retrieve . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: between questions and passages. Such term mismatch problem can be reduced by either query expansion <ref type=\"bibr\" target=\"#b3\">(Carpineto and Romano, 2012)</ref> or appending generated questions to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediv\u00fd, 2015)</ref> composes of questions from both TREC QA tracks and Web sources.</p><p>SQuAD v1.1 <ref type=\"bibr\" target=\"#b17\">(Rajpurkar et al., 2016)</ref> is original used as a benchmark for re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ollection of questions from Google Suggest API with answers from Freebase.</p><p>CuratedTREC (TREC) <ref type=\"bibr\" target=\"#b0\">(Baudi\u0161 and \u0160ediv\u00fd, 2015)</ref> composes of questions from both TREC Q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: between questions and passages. Such term mismatch problem can be reduced by either query expansion <ref type=\"bibr\" target=\"#b3\">(Carpineto and Romano, 2012)</ref> or appending generated questions to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>Traditionally the retrievers usually utilize sparse keywords matching such as TF-IDF or BM25 <ref type=\"bibr\" target=\"#b18\">(Robertson and Zaragoza, 2009)</ref>, which can be efficiently implem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ollection of questions from Google Suggest API with answers from Freebase.</p><p>CuratedTREC (TREC) <ref type=\"bibr\" target=\"#b0\">(Baudi\u0161 and \u0160ediv\u00fd, 2015)</ref> composes of questions from both TREC Q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ollection of questions from Google Suggest API with answers from Freebase.</p><p>CuratedTREC (TREC) <ref type=\"bibr\" target=\"#b0\">(Baudi\u0161 and \u0160ediv\u00fd, 2015)</ref> composes of questions from both TREC Q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediv\u00fd, 2015)</ref> composes of questions from both TREC QA tracks and Web sources.</p><p>SQuAD v1.1 <ref type=\"bibr\" target=\"#b17\">(Rajpurkar et al., 2016)</ref> is original used as a benchmark for re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: set of hard negative examples. To this end, one possible direction is to employ curriculum learning <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 2009)</ref>. Assuming the corresponding passages for s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediv\u00fd, 2015)</ref> composes of questions from both TREC QA tracks and Web sources.</p><p>SQuAD v1.1 <ref type=\"bibr\" target=\"#b17\">(Rajpurkar et al., 2016)</ref> is original used as a benchmark for re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: set of hard negative examples. To this end, one possible direction is to employ curriculum learning <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 2009)</ref>. Assuming the corresponding passages for s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ollection of questions from Google Suggest API with answers from Freebase.</p><p>CuratedTREC (TREC) <ref type=\"bibr\" target=\"#b0\">(Baudi\u0161 and \u0160ediv\u00fd, 2015)</ref> composes of questions from both TREC Q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ap both questions and passages into dense vectors, where their innerproduct denotes their relevance <ref type=\"bibr\" target=\"#b14\">(Lee et al., 2019)</ref>.</p><p>The challenge in training a dense ret d passages, we need to create a large collection of passages for passage retrieval tasks. Following <ref type=\"bibr\" target=\"#b14\">Lee et al. (2019)</ref>, we extract the passage candidate set from th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ediv\u00fd, 2015)</ref> composes of questions from both TREC QA tracks and Web sources.</p><p>SQuAD v1.1 <ref type=\"bibr\" target=\"#b17\">(Rajpurkar et al., 2016)</ref> is original used as a benchmark for re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Traditional passage retriever utilizes the keyword-matching based methods such as TF-IDF and BM25 <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2017)</ref>. Keyword-based approach enjoys its simplicit 12\">Karpukhin et al. (2020)</ref>, we first extract clean texts using pre-processing code from DrQA <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2017)</ref>, and then split each article into non-overla. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: between questions and passages. Such term mismatch problem can be reduced by either query expansion <ref type=\"bibr\" target=\"#b3\">(Carpineto and Romano, 2012)</ref> or appending generated questions to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ap both questions and passages into dense vectors, where their innerproduct denotes their relevance <ref type=\"bibr\" target=\"#b14\">(Lee et al., 2019)</ref>.</p><p>The challenge in training a dense ret d passages, we need to create a large collection of passages for passage retrieval tasks. Following <ref type=\"bibr\" target=\"#b14\">Lee et al. (2019)</ref>, we extract the passage candidate set from th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>Traditionally the retrievers usually utilize sparse keywords matching such as TF-IDF or BM25 <ref type=\"bibr\" target=\"#b18\">(Robertson and Zaragoza, 2009)</ref>, which can be efficiently implem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ollection of questions from Google Suggest API with answers from Freebase.</p><p>CuratedTREC (TREC) <ref type=\"bibr\" target=\"#b0\">(Baudi\u0161 and \u0160ediv\u00fd, 2015)</ref> composes of questions from both TREC Q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">(Bitbol et al., 2016;</ref><ref type=\"bibr\" target=\"#b9\">Gueudre \u00b4et al., 2016;</ref><ref type=\"bibr\" target=\"#b45\">Zeng et al., 2018)</ref>. On the other hand, protein language models, earning model trained by individual protein chains works fine on protein complex contact prediction <ref type=\"bibr\" target=\"#b45\">(Zeng et al., 2018;</ref><ref type=\"bibr\" target=\"#b47\">Zhou et al.,  suitable for proteome-scale screening of protein-protein interactions.</p><p>RaptorX ComplexContact <ref type=\"bibr\" target=\"#b45\">(Zeng et al., 2018;</ref><ref type=\"bibr\" target=\"#b47\">Zhou et al.,  ution signals. We build a joint MSA for a heterodimer using the protocol proposed by ComplexContact <ref type=\"bibr\" target=\"#b45\">(Zeng et al., 2018)</ref>. For a homodimer, we simply concatenate eac atenate two MSAs to form a joint MSA for a heterodimer using the method described in ComplexContact <ref type=\"bibr\" target=\"#b45\">(Zeng et al., 2018)</ref>. We use monomer (bound) experimental struct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: are unavailable. We also tested the 3D structure models of individual chains predicted by AlphaFold <ref type=\"bibr\" target=\"#b14\">(Jumper et al., 2020;</ref><ref type=\"bibr\">Senior et al., 2020)</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ry structures, including voxels <ref type=\"bibr\" target=\"#b5\">(Derevyanko and Lamoureux, 2019;</ref><ref type=\"bibr\" target=\"#b35\">Townshend et al., 2019)</ref> and radial/point cloud representations  en the distance cutoff is 8 A \u02da. This saturation effect on the distance cutoffs is also observed in <ref type=\"bibr\" target=\"#b35\">Townshend et al. (2019)</ref>.</p><p>Different types of graphs may re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are unavailable. We also tested the 3D structure models of individual chains predicted by AlphaFold <ref type=\"bibr\" target=\"#b14\">(Jumper et al., 2020;</ref><ref type=\"bibr\">Senior et al., 2020)</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: number of interacting paralogs (interlogs) <ref type=\"bibr\" target=\"#b1\">(Bitbol et al., 2016;</ref><ref type=\"bibr\" target=\"#b9\">Gueudre \u00b4et al., 2016;</ref><ref type=\"bibr\" target=\"#b45\">Zeng et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he contacts predicted by GLINTER may also improve the ranking of the HDOCK-generated docking decoys <ref type=\"bibr\" target=\"#b43\">(Yan et al., 2017)</ref>. Further, our method runs very quickly, whic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  generating and filtering docking decoys <ref type=\"bibr\" target=\"#b0\">(Baldassi et al., 2014;</ref><ref type=\"bibr\" target=\"#b8\">Geng et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Hopf et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, and reveal important biophysical properties and evolutionary information of protein interfaces <ref type=\"bibr\" target=\"#b36\">(Uguzzoni et al., 2017)</ref>. They are also useful for the redesign . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b15\">Jumper et al., 2021;</ref><ref type=\"bibr\" target=\"#b38\">Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b41\">Xu, 2019;</ref><ref type=\"bibr\" target=\"#b42\">Xu et al., 2021)</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers. Both DeepHomo and ComplexContact take as input the coevolution information computed by CCMpred <ref type=\"bibr\" target=\"#b29\">(Seemayer et al., 2014)</ref> while GLINTER does not. BIPSPI works fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Uguzzoni et al., 2017)</ref>. They are also useful for the redesign of protein-protein interfaces <ref type=\"bibr\" target=\"#b17\">(Laine and Carbone, 2015)</ref> and prediction of binding affinity <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b41\">41,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> and masked autoencoding in BERT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, are conceptually simple: they remove a portion of the dat perate on regular grids and it is not straightforward to integrate 'indicators' such as mask tokens <ref type=\"bibr\" target=\"#b13\">[14]</ref> or positional embeddings <ref type=\"bibr\" target=\"#b47\">[4 ing words that contain rich semantic information. While in BERT the decoder can be trivial (an MLP) <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we found that for images, the decoder design plays a key  \"2.\">Related Work</head><p>Masked language modeling and its autoregressive counterparts, e.g., BERT <ref type=\"bibr\" target=\"#b13\">[14]</ref> and GPT <ref type=\"bibr\" target=\"#b40\">[40,</ref><ref type tches, and (ii) mask tokens. See Figure <ref type=\"figure\" target=\"#fig_0\">1</ref>. Each mask token <ref type=\"bibr\" target=\"#b13\">[14]</ref> is a shared, learned vector that indicates the presence of and original images in the pixel space. We compute the loss only on masked patches, similar to BERT <ref type=\"bibr\" target=\"#b13\">[14]</ref>. 1   We also study a variant whose reconstruction target i atio of 75% is good for both linear probing and fine-tuning. This behavior is in contrast with BERT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, whose typical masking ratio is 15%. Our masking ratios ar  models. These observations are aligned with those witnessed in self-supervised pre-training in NLP <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" ta ng. In NLP, simple self-supervised learning methods (e.g., <ref type=\"bibr\" target=\"#b40\">[40,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b41\">41,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g accuracy even at 1600 epochs. This behavior is unlike contrastive learning methods, e.g., MoCo v3 <ref type=\"bibr\" target=\"#b8\">[9]</ref> saturates at 300 epochs for ViT-L. Note that the MAE encoder ViT-L, our MAE's training time is 31 hours for 1600 epochs and MoCo v3's is 36 hours for 300 epochs <ref type=\"bibr\" target=\"#b8\">[9]</ref>, using the same 128 TPU-v3 cores.</p><p>Comparisons with sup d with the frozen backbone.</p><p>In Figure <ref type=\"figure\">9</ref> we also compare with MoCo v3 <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which is a contrastive method with ViT-L results available. ef type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Linear probing. Our linear classifier training follows <ref type=\"bibr\" target=\"#b8\">[9]</ref>. See Table <ref type=\"table\" target=\"#tab_8\">9</ref>. We obs -end fine-tuning. In particular, regularization is in general harmful for linear probing. Following <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we disable many common regularization strategies: we do not ods. On the other hand, it still lags behind contrastive methods under this protocol: e.g., MoCo v3 <ref type=\"bibr\" target=\"#b8\">[9]</ref> has 77.6% linear probing accuracy for the ViT-L (Figure <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b3\">[4]</ref>.</p><p>The idea of masked autoencoders, a form of more general denoising autoencoders <ref type=\"bibr\" target=\"#b48\">[48]</ref>, is natural and applicable in computer vision as well. Ind nd k-means are autoencoders <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Denoising autoencoders (DAE) <ref type=\"bibr\" target=\"#b48\">[48]</ref> are a class of autoencoders that corrupt an input signal a e fast. 1 Computing the loss only on masked patches differs from traditional denoising autoencoders <ref type=\"bibr\" target=\"#b48\">[48]</ref> that compute the loss on all pixels. This choice is purely. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: significant interest in computer vision, often focusing on different pretext tasks for pre-training <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b50\">50,</ref><ref type=\"bibr\" ta ficial to normalize the pre-trained features when training the linear probing classifier. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we adopt an extra BatchNorm layer <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uce representations for recognition tasks.</p><p>in vision <ref type=\"bibr\" target=\"#b49\">[49,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref> preceded BERT. However, despite significant interest in thi ized DAE under different corruptions, e.g., masking pixels <ref type=\"bibr\" target=\"#b49\">[49,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> or removing color cha  <ref type=\"bibr\" target=\"#b49\">[49]</ref> presents masking as a noise type in DAE. Context Encoder <ref type=\"bibr\" target=\"#b39\">[39]</ref> inpaints large missing regions using convolutional network. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: els image similarity and dissimilarity (or only similarity <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>) between two or more views. Contrastive and related methods s on data augmentation <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Autoencoding pursues a conceptually different direction, and lity is not the sole metric for evaluating representation quality. It has also been observed (e.g., <ref type=\"bibr\" target=\"#b7\">[8]</ref>) that linear probing is not well correlated with transfer le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  classifier. Following <ref type=\"bibr\" target=\"#b14\">[15]</ref>, we adopt an extra BatchNorm layer <ref type=\"bibr\" target=\"#b26\">[27]</ref> without affine transformation (affine=False). This layer i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esentation and a decoder that reconstructs the input. For example, PCA and k-means are autoencoders <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Denoising autoencoders (DAE) <ref type=\"bibr\" target=\"#b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: els image similarity and dissimilarity (or only similarity <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>) between two or more views. Contrastive and related methods s on data augmentation <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Autoencoding pursues a conceptually different direction, and lity is not the sole metric for evaluating representation quality. It has also been observed (e.g., <ref type=\"bibr\" target=\"#b7\">[8]</ref>) that linear probing is not well correlated with transfer le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b16\">17]</ref>. Recently, contrastive learning <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> has been popular, e.g., <ref type=\"bibr\" target=\"#b51\">[51,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hitectures of continuously growing capability and capacity <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b47\">47]</ref>. Aided by the rapid y supervised (e.g. <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b44\">44,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>) despite progress i  the backbone in our ablation study. ViT-L is very big (an order of magnitude bigger than ResNet-50 <ref type=\"bibr\" target=\"#b23\">[24]</ref>) and tends to overfit. The following is a comparison betwe  feature maps for producing different scales (stride 4, 8, 16, or 32, the same as a standard ResNet <ref type=\"bibr\" target=\"#b23\">[24]</ref>). FPN is built on these multi-scale maps.</p><p>For fair c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rune these weak edges for each raw document feature graph.</p><p>Raw Document Feature Graph Pruning <ref type=\"bibr\" target=\"#b17\">[18]</ref>: The N x N adjacent matrix MX denotes the raw document fea uthor name and its annotation result C = {Cl,C2, ... ,cd <ref type=\"bibr\" target=\"#b15\">[16]</ref>  <ref type=\"bibr\" target=\"#b17\">[18]</ref>. From C, we first generate positive edge set E p and negat e sample 320 author names and split them into 200, 60, 60 for the training, validating, and testing <ref type=\"bibr\" target=\"#b17\">[18]</ref>, which contains 341,457 publications. Each publication has. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he raw document feature, fusion feature, and local structural information are taken into account in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, the global structure information, which can be used to furt the word2vec result, and designs an unsupervised local stage based on the first stage.</p><p>GANAND <ref type=\"bibr\" target=\"#b8\">[9]</ref>: This method builds a generative adversarial framework. The  fine-tune the word2vec result and an unsupervised local stage based on the first stage. Wang et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> propose a generative adversarial framework, and the discrimi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n's neighborhood information, due to the lack of local structure information. The latter approaches <ref type=\"bibr\" target=\"#b16\">[17]</ref> [9] that only use raw document feature and local structura s, institute names, etc. A semi-supervised HAC algorithm is used to determine clusters.</p><p>AGAND <ref type=\"bibr\" target=\"#b16\">[17]</ref>: This method builds three graphs based on document similar  (HMRF) to model node features and edge features in a unified probabilistic framework. Zhang et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> construct three graphs based on document similarity and co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s referred to as object distinction <ref type=\"bibr\" target=\"#b5\">[6]</ref> and name identification <ref type=\"bibr\" target=\"#b6\">[7]</ref>. The AND problem can cause inconvenience in data mining comm rding to Eq. ( <ref type=\"formula\" target=\"#formula_5\">5</ref>), ( <ref type=\"formula\">6</ref>) and <ref type=\"bibr\" target=\"#b6\">(7)</ref> to construct the fusion feature graph; 4 Perform random walk. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e AND problem can cause inconvenience in data mining communities and academic information retrieval <ref type=\"bibr\" target=\"#b7\">[8]</ref> [9] <ref type=\"bibr\" target=\"#b9\">[10]</ref>. For instance, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nted growth of academic digital records in the past decade <ref type=\"bibr\" target=\"#b0\">[1]</ref>  <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The latest estimation presents that there are more than 271. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ocument features in one publication and quantifies the similarity of pairwise publications robustly <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For example, if a publication does not have the raw docum type=\"bibr\" target=\"#b26\">[27]</ref> use a deep neural network to solve the AND problem. Kim et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> introduce a hybrid method that extracts structure-aware fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ION</ref> We have witnessed the unprecedented growth of academic digital records in the past decade <ref type=\"bibr\" target=\"#b0\">[1]</ref>  <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The latest estimat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a mining communities and academic information retrieval <ref type=\"bibr\" target=\"#b7\">[8]</ref> [9] <ref type=\"bibr\" target=\"#b9\">[10]</ref>. For instance, an online search in DBLP for \"Michael Jordan l:id=\"formula_14\">VCon(Vk) = Conx(MxVk-l)W~onJ (9)</formula><p>VAdd(V k ) = Norm(Addx(MXVk-l))W~dd' <ref type=\"bibr\" target=\"#b9\">(10)</ref> D i and the nodes in this walk. Then, this redefined walk i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: represents publications published by a unique person <ref type=\"bibr\" target=\"#b11\">[12]</ref> [13] <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Many existing studies have made great contributions in th biguate author names based on this feature directly [15] <ref type=\"bibr\" target=\"#b15\">[16]</ref>  <ref type=\"bibr\" target=\"#b13\">[14]</ref>. <ref type=\"bibr\">[17] [9]</ref> employ raw document featu  and DNN are introduced to experiment with the result of disambiguation respectively. Jhawar et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> conduct experiments with two ensemblebased classification . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e are more than 271 million publications, 133 million scholars, and 754 million citations on Aminer <ref type=\"bibr\" target=\"#b2\">[3]</ref>. These numbers are much larger than those on Google Scholar  missing. For example, keywords and abstract are often missing for some older publications in Aminer <ref type=\"bibr\" target=\"#b2\">[3]</ref>. This problem brings a great challenge for measuring similar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph topology and capturing the information of neighbors to deal with the AND problem. Fan et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> construct a graph by collapsing all the co-authors with id. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  our approach. As the co-author of an author is considered to be a strong discriminative feature by <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, the Fig. <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Q is the binary label vector.</p><p>We optimize the objective with the Adam optimization algorithm <ref type=\"bibr\" target=\"#b20\">[21]</ref> simultaneously. The process for the author name disambigua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a mining communities and academic information retrieval <ref type=\"bibr\" target=\"#b7\">[8]</ref> [9] <ref type=\"bibr\" target=\"#b9\">[10]</ref>. For instance, an online search in DBLP for \"Michael Jordan l:id=\"formula_14\">VCon(Vk) = Conx(MxVk-l)W~onJ (9)</formula><p>VAdd(V k ) = Norm(Addx(MXVk-l))W~dd' <ref type=\"bibr\" target=\"#b9\">(10)</ref> D i and the nodes in this walk. Then, this redefined walk i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nted growth of academic digital records in the past decade <ref type=\"bibr\" target=\"#b0\">[1]</ref>  <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The latest estimation presents that there are more than 271. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sWho dataset contains 608,363 documents and 57,138 distinct authors with 642 equivocal author names <ref type=\"bibr\" target=\"#b21\">[22]</ref>. In the OAG-WhoisWho dataset, we sample 320 author names a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n D i and D j , and E ij equals to Mh' After constructing the fusion feature graph, the random walk <ref type=\"bibr\" target=\"#b18\">[19]</ref>, which can capture the neighborhood information of nodes, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e AND problem can cause inconvenience in data mining communities and academic information retrieval <ref type=\"bibr\" target=\"#b7\">[8]</ref> [9] <ref type=\"bibr\" target=\"#b9\">[10]</ref>. For instance, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n D i and D j , and E ij equals to Mh' After constructing the fusion feature graph, the random walk <ref type=\"bibr\" target=\"#b18\">[19]</ref>, which can capture the neighborhood information of nodes, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  these databases, many publications meet the AND problem which is referred to as object distinction <ref type=\"bibr\" target=\"#b5\">[6]</ref> and name identification <ref type=\"bibr\" target=\"#b6\">[7]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e are more than 271 million publications, 133 million scholars, and 754 million citations on Aminer <ref type=\"bibr\" target=\"#b2\">[3]</ref>. These numbers are much larger than those on Google Scholar  missing. For example, keywords and abstract are often missing for some older publications in Aminer <ref type=\"bibr\" target=\"#b2\">[3]</ref>. This problem brings a great challenge for measuring similar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ION</ref> We have witnessed the unprecedented growth of academic digital records in the past decade <ref type=\"bibr\" target=\"#b0\">[1]</ref>  <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The latest estimat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: represents publications published by a unique person <ref type=\"bibr\" target=\"#b11\">[12]</ref> [13] <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Many existing studies have made great contributions in th biguate author names based on this feature directly [15] <ref type=\"bibr\" target=\"#b15\">[16]</ref>  <ref type=\"bibr\" target=\"#b13\">[14]</ref>. <ref type=\"bibr\">[17] [9]</ref> employ raw document featu  and DNN are introduced to experiment with the result of disambiguation respectively. Jhawar et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> conduct experiments with two ensemblebased classification . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r is considered to be a strong discriminative feature by <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, the Fig. <ref type=\"figure\">7</ref> shows that co-author . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Q is the binary label vector.</p><p>We optimize the objective with the Adam optimization algorithm <ref type=\"bibr\" target=\"#b20\">[21]</ref> simultaneously. The process for the author name disambigua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e are more than 271 million publications, 133 million scholars, and 754 million citations on Aminer <ref type=\"bibr\" target=\"#b2\">[3]</ref>. These numbers are much larger than those on Google Scholar  missing. For example, keywords and abstract are often missing for some older publications in Aminer <ref type=\"bibr\" target=\"#b2\">[3]</ref>. This problem brings a great challenge for measuring similar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r is considered to be a strong discriminative feature by <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, the Fig. <ref type=\"figure\">7</ref> shows that co-author . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n D i and D j , and E ij equals to Mh' After constructing the fusion feature graph, the random walk <ref type=\"bibr\" target=\"#b18\">[19]</ref>, which can capture the neighborhood information of nodes, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ode, and the distance between two nodes is measured based on the number of valid paths. Tang et al. <ref type=\"bibr\" target=\"#b28\">[29]</ref> solve the problem by employing Hidden Markov Random Fields. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n D i and D j , and E ij equals to Mh' After constructing the fusion feature graph, the random walk <ref type=\"bibr\" target=\"#b18\">[19]</ref>, which can capture the neighborhood information of nodes, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s as it has approached experimental accuracy, with the recent structure prediction system AlphaFold <ref type=\"bibr\" target=\"#b0\">[1]</ref> providing a step change in structure prediction accuracy. Th dapted for multimeric inputs.</p><p>The AlphaFold system has recently been described in publication <ref type=\"bibr\" target=\"#b0\">[1]</ref> along with source code and model parameters <ref type=\"bibr\" ad n=\"2.4.\">Architecture and Losses</head><p>AlphaFold uses a Frame Aligned Point Error (FAPE) loss <ref type=\"bibr\" target=\"#b0\">[1]</ref>, whereby the distances between ground truth and predicted at =\"2.5.\">Training Regimen</head><p>AlphaFold-Multimer was trained in a very similar way to AlphaFold <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The training dataset comprised structures from the Protein  ion losses enabled. These extra heads and losses are identical to those used in the AlphaFold paper <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We trained 3 models for the first stage; the best model on  div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.6.\">Inference Regimen</head><p>As with AlphaFold <ref type=\"bibr\" target=\"#b0\">[1]</ref>, at inference time we run all 5 trained models and select th subsection 2.7. The inference times for different sequence lengths are similar to those reported in <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= n shown to often implicitly model structures with missing context correctly. In the AlphaFold paper <ref type=\"bibr\" target=\"#b0\">[1]</ref> an example is provided of a heavily intertwined multimer wit ns. The AlphaFold-Multimer data pipeline follows similar steps to the single-chain AlphaFold system <ref type=\"bibr\" target=\"#b0\">[1]</ref>, with a few key differences that are described below.</p><p> ttp://www.tei-c.org/ns/1.0\"><head n=\"7.5.\">Chain Relative Positional Encoding</head><p>In AlphaFold <ref type=\"bibr\" target=\"#b0\">[1]</ref>, relative positional features are encoded into the initial p ning an interface version of AlphaFold's predicted TM-score metric [1, \u00a71.9.7] amounts to modifying <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\">Equation 40</ref>] so that and come from dif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is work we provide explicit aligned sequences to the network following the procedure of Zhou et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref>. We pair sequences using the UniProt species annotation an get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. We follow the approach of Zhou et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> by pairing by species label, disambiguating using genetic  arious heuristic methods are used instead to approximate it. We follow the procedure of Zhou et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which pairs sequences of the same species and disambiguat et=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. We follow the approach of Zhou et al. <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: quires determination of interacting homologs in the heteromeric case, which is in general ambiguous <ref type=\"bibr\" target=\"#b10\">[11]</ref>. In this work we provide explicit aligned sequences to the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uristic that can be performed efficiently on TPU; other multi-chain alignment algorithms also exist <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. The optimal groun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rform this pairing <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. We follow the appr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g and free docking <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Template-based mod e=\"bibr\" target=\"#b1\">[2]</ref> combine this approach with docking using the ClusPro docking server <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Humphreys et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic structure prediction have tended to rely on a hybrid of templatebased modelling and free docking <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> benchmarked AlphaFold with a residue gap against RoseTTAFold and the rigid-docking method GRAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and AlphaFold substantially outperformed both other metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is work we provide explicit aligned sequences to the network following the procedure of Zhou et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref>. We pair sequences using the UniProt species annotation an get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. We follow the approach of Zhou et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> by pairing by species label, disambiguating using genetic  arious heuristic methods are used instead to approximate it. We follow the procedure of Zhou et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which pairs sequences of the same species and disambiguat et=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. We follow the approach of Zhou et al. <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng=\"en\"> \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>In a recent breakthrough, ALPHAFOLD 2 <ref type=\"bibr\" target=\"#b36\">(Jumper et al., 2021;</ref><ref type=\"bibr\" target=\"#b56\">Senior et a r\" target=\"#b30\">Hutchinson et al., 2020;</ref><ref type=\"bibr\" target=\"#b70\">Wu et al., 2021;</ref><ref type=\"bibr\" target=\"#b36\">Jumper et al., 2021;</ref><ref type=\"bibr\" target=\"#b24\">Ganea et al. ditional constraints or differentiable energy terms for protein structure optimization. ALPHAFOLD 2 <ref type=\"bibr\" target=\"#b36\">(Jumper et al., 2021)</ref> and Rosetta Fold <ref type=\"bibr\" target= ce information <ref type=\"bibr\" target=\"#b7\">(Bryant et al., 2021)</ref>. Concurrently to our work, <ref type=\"bibr\" target=\"#b36\">Evans et al. (2021)</ref> extend ALPHAFOLD 2 to multiple chains durin Local Coordinate System. Similar to <ref type=\"bibr\" target=\"#b31\">Ingraham et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b36\">Jumper et al. (2021)</ref>, we introduce a local coordinate system fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b39\">Kipf and Welling, 2016;</ref><ref type=\"bibr\" target=\"#b26\">Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b72\">Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b44\">Li et al., 2019). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b45\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b71\">Xie and Xu, 2021;</ref><ref type=\"bibr\" target=\"#b12\">Dai and Bailey-Kellogg, 2021)</ref>. Recently, ALPHAFOLD 2 and ROSETT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rugs' mechanism of action. Prior methods <ref type=\"bibr\" target=\"#b67\">(Wallach et al., 2015;</ref><ref type=\"bibr\" target=\"#b43\">Li et al., 2021)</ref> predict binding affinity from protein-ligand c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b40\">Koes et al., 2013;</ref><ref type=\"bibr\" target=\"#b47\">McNutt et al., 2021;</ref><ref type=\"bibr\" target=\"#b2\">Bao et al., 2021)</ref>, being tailored for small drug-like ligands an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sample thousands or millions of complex candidates; second, they use a scoring function for ranking <ref type=\"bibr\" target=\"#b48\">(Moal et al., 2013;</ref><ref type=\"bibr\" target=\"#b3\">Basu and Walln. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inding, which is a realistic assumption in many biological settings.</p><p>Popular docking software <ref type=\"bibr\" target=\"#b8\">(Chen et al., 2003;</ref><ref type=\"bibr\" target=\"#b64\">Venkatraman et  computational methods <ref type=\"bibr\" target=\"#b63\">(Vakser, 2014)</ref>. Protein docking methods <ref type=\"bibr\" target=\"#b8\">(Chen et al., 2003;</ref><ref type=\"bibr\" target=\"#b64\">Venkatraman et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rring in different data types, architectures are tailored to explicitly incorporate such properties <ref type=\"bibr\" target=\"#b10\">(Cohen and Welling, 2016a;</ref><ref type=\"bibr\">b;</ref><ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a practical perspective, the gradient and backpropagation through the SVD operation was analyzed by <ref type=\"bibr\" target=\"#b32\">(Ionescu et al., 2015;</ref><ref type=\"bibr\" target=\"#b49\">Papadopoul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  pretraining schemes in vision deal with the global views <ref type=\"bibr\">(Chen et al., 2021;</ref><ref type=\"bibr\" target=\"#b7\">Caron et al., 2021)</ref>, neglecting images' internal structures, as  \"bibr\" target=\"#b19\">(He et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Grill et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Caron et al., 2021)</ref>. In practice, iBOT works with L [CLS] in Eq. et=\"#b6\">(Caron et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Amrani &amp; Bronstein, 2021;</ref><ref type=\"bibr\" target=\"#b7\">Caron et al., 2021)</ref>. In fact, the idea of simultaneously enforci tch tokens using other selfsupervised methods <ref type=\"bibr\" target=\"#b3\">(Bao et al., 2021;</ref><ref type=\"bibr\" target=\"#b7\">Caron et al., 2021)</ref> in Fig. <ref type=\"figure\" target=\"#fig_5\">1 c.org/ns/1.0\"><head n=\"2.2\">SELF-DISTILLATION</head><p>Self-distillation, proposed recently in DINO <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021)</ref>, distills knowledge not from posterior dist </ref>. In practice, iBOT works with L [CLS] in Eq. ( <ref type=\"formula\">2</ref>) proposed in DINO <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021)</ref>, except that now we have \u00fb[CLS] s instead o kens is 196. The projection head h is a 3-layer MLPs with l 2 -normalized bottleneck following DINO <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021)</ref>. Towards a better design to acquire visual  ampled from range [0.1, 0.5] with a probability of 0.5. For L [CLS] , We use the same setup as DINO <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021)</ref>. For L MIM , we use a separate moving-avera NN) or a linear classifier on the frozen representation. We follow the evaluation protocols in DINO <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021)</ref> but with 10\u00d7 less parameters. We underline  lti-crop works well on most of the self-supervised methods and consistently yields performance gain <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021)</ref>. While a more fair comparison with our meth c>Multi-Crop. We further study the performance with different local and global scale. Following DINO<ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021)</ref>, we conduct the experiments by tweaking s,  al is considered using the frozen pre-trained features following the evaluation protocol as in DINO <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021)</ref>. DINO has demonstrated the strong potential e-of-the-Art Comparison w/o and w/ Multi-Crop. Including iBOT, several recent state-ofthe-art works <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021;</ref><ref type=\"bibr\" target=\"#b21\">2020)</ref> r  , \u03c4 t ): t = t.detach();</p><p>The advanced performance of several recent state-of-the-art methods <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021;</ref><ref type=\"bibr\" target=\"#b21\">2020)</ref> r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Bao et al., 2021;</ref><ref type=\"bibr\" target=\"#b41\">Tan et al., 2021)</ref> with a discrete VAE <ref type=\"bibr\" target=\"#b38\">(Rolfe, 2017;</ref><ref type=\"bibr\" target=\"#b36\">Ramesh et al., 2021. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .7) Specifically, pos. denotes matching according to the absolute position of two views. Similar to <ref type=\"bibr\" target=\"#b53\">Xie et al. (2021b)</ref>. j is defined as arg min j dist(p i , p j ),. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n et al., 2014)</ref>. Several recent works <ref type=\"bibr\" target=\"#b30\">(Liu et al., 2021b;</ref><ref type=\"bibr\" target=\"#b45\">Wang et al., 2021a)</ref> proposes Vision Transformers that suit dens. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">(Su et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Lu et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Chen et al., 2020c)</ref> perform masked region classification taskin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ming representation learning via clustering <ref type=\"bibr\" target=\"#b5\">(Caron et al., 2018;</ref><ref type=\"bibr\" target=\"#b21\">2020;</ref><ref type=\"bibr\" target=\"#b54\">YM. et al., 2020)</ref>, wh al., 2021;</ref><ref type=\"bibr\" target=\"#b27\">Li et al., 2021b)</ref> or mask contrastive learning <ref type=\"bibr\" target=\"#b21\">(Henaff, 2020;</ref><ref type=\"bibr\" target=\"#b55\">Zhao et al., 2021)  iBOT, several recent state-ofthe-art works <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021;</ref><ref type=\"bibr\" target=\"#b21\">2020)</ref> rely heavily on multi-crop augmentation during pre-traini  of several recent state-of-the-art methods <ref type=\"bibr\" target=\"#b7\">(Caron et al., 2021;</ref><ref type=\"bibr\" target=\"#b21\">2020)</ref> relies on multi-crop augmentation, as well as iBOT. In ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Bao et al., 2021;</ref><ref type=\"bibr\" target=\"#b41\">Tan et al., 2021)</ref> with a discrete VAE <ref type=\"bibr\" target=\"#b38\">(Rolfe, 2017;</ref><ref type=\"bibr\" target=\"#b36\">Ramesh et al., 2021. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: T <ref type=\"bibr\" target=\"#b3\">(Bao et al., 2021)</ref> proposes to use a pre-trained discrete VAE <ref type=\"bibr\" target=\"#b36\">(Ramesh et al., 2021)</ref> as the tokenizer. Though providing some l ver, the tokenizer needs to be offline pre-trained with fixed model architectures and extra dataset <ref type=\"bibr\" target=\"#b36\">(Ramesh et al., 2021)</ref>, which potentially limits its adapativity ms the input to a probability distribution over K dimensions, and \u03c6 is parameters of a discrete VAE <ref type=\"bibr\" target=\"#b36\">(Ramesh et al., 2021)</ref> that clusters image patches into K catego isual tokenizer. Specifically, \u2022 denotes a standalone DINO and denotes a pretranined DALL-E encoder <ref type=\"bibr\" target=\"#b36\">(Ramesh et al., 2021)</ref>. We find that performing MIM without L [C #b41\">Tan et al., 2021)</ref> with a discrete VAE <ref type=\"bibr\" target=\"#b38\">(Rolfe, 2017;</ref><ref type=\"bibr\" target=\"#b36\">Ramesh et al., 2021)</ref> as visual tokenizer. As a counterpart of M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ap. We consider Cascade Mask R-CNN <ref type=\"bibr\" target=\"#b4\">(Cai &amp; Vasconcelos, 2019;</ref><ref type=\"bibr\" target=\"#b18\">He et al., 2017)</ref> that produces bounding boxes and instance mask. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Bao et al., 2021;</ref><ref type=\"bibr\" target=\"#b41\">Tan et al., 2021)</ref> with a discrete VAE <ref type=\"bibr\" target=\"#b38\">(Rolfe, 2017;</ref><ref type=\"bibr\" target=\"#b36\">Ramesh et al., 2021. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  distributed optimization with periodic averaging <ref type=\"bibr\" target=\"#b30\">(Stich, 2019;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019)</ref> To get a deeper understanding on the necessity vergence rate with P local machines and T iterations of gradient updates, which matches the rate of <ref type=\"bibr\" target=\"#b38\">(Yu et al., 2019)</ref> on a general (not specific for GNN training)  he classifcal distributed training analysis <ref type=\"bibr\" target=\"#b7\">Dean et al. (2012)</ref>; <ref type=\"bibr\" target=\"#b38\">Yu et al. (2019)</ref>. (2) The expectation of the local stochastic g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on overhead, inspired by the recent success of the distributed optimization with periodic averaging <ref type=\"bibr\" target=\"#b30\">(Stich, 2019;</ref><ref type=\"bibr\" target=\"#b38\">Yu et al., 2019)</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: applications, including social networks <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b8\">Deng et al., 2019)</ref>, recommendation systems <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b36\">Wu et al., 2021)</ref>.  <ref type=\"bibr\" target=\"#b7\">et al., 2012;</ref><ref type=\"bibr\" target=\"#b24\">Li et al., 2020b)</ref>) suffers from significant accuracy drop and c Parallel SGD with Periodic Averaging (PSGD-PA<ref type=\"bibr\" target=\"#b7\">(Dean et al., 2012;</ref><ref type=\"bibr\" target=\"#b24\">Li et al., 2020b)</ref>) suffers from significant accuracy drop and c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b11\">(Fout et al., 2017;</ref><ref type=\"bibr\" target=\"#b9\">Do et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Ghorbani et al., 2022;</ref><ref type=\"bibr\" target=\"#b10\">Faez et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stence of stochastic gradient bias and variance in sampling-based GNN training have been studied in <ref type=\"bibr\" target=\"#b5\">(Cong et al., 2020;</ref><ref type=\"bibr\">2021)</ref>, where <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b3\">Chen et al., 2018;</ref><ref type=\"bibr\" target=\"#b40\">Zhang et al., 2021;</ref><ref type=\"bibr\" target=\"#b27\">Ramezani et al., 2020)</ref>, they are still inefficient for training. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r GNN architectures, such as SAGE <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b34\">(Velickovic et al., 2018)</ref>, ResGCN <ref type=\"bibr\" target=\"#b22. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2019)</ref>, recommendation systems <ref type=\"bibr\" target=\"#b37\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b35\">Wang et al., 2018)</ref>, and drug discovery <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (Lin et al., 2021;</ref><ref type=\"bibr\" target=\"#b14\">Hard et al., 2018)</ref> and computer vision <ref type=\"bibr\" target=\"#b2\">(Bonawitz et al., 2019;</ref><ref type=\"bibr\" target=\"#b21\">Kone\u010dn\u1ef3 et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ave achieved impressive results across numerous graph-based applications, including social networks <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b8\">Deng et al  challenging. Although several attempts have been made to scale GNN training by sampling techniques <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">Zou et al for simplicity, however, our discussion is also applicable to other GNN architectures, such as SAGE <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b34\">(Ve en the same set of subgraphs are used for all baselines. For training, we use neighborhood sampling <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017)</ref> with 10 neighbors sampled per node and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: zed data storage and model learning, which could result in privacy concerns in real-world scenarios <ref type=\"bibr\" target=\"#b29\">(Shin et al., 2018;</ref><ref type=\"bibr\" target=\"#b36\">Wu et al., 20 l machines, both of which can cause significant storage/communication overhead and privacy concerns <ref type=\"bibr\" target=\"#b29\">(Shin et al., 2018;</ref><ref type=\"bibr\" target=\"#b36\">Wu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (Lin et al., 2021;</ref><ref type=\"bibr\" target=\"#b14\">Hard et al., 2018)</ref> and computer vision <ref type=\"bibr\" target=\"#b2\">(Bonawitz et al., 2019;</ref><ref type=\"bibr\" target=\"#b21\">Kone\u010dn\u1ef3 et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ave achieved impressive results across numerous graph-based applications, including social networks <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b8\">Deng et al  challenging. Although several attempts have been made to scale GNN training by sampling techniques <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">Zou et al for simplicity, however, our discussion is also applicable to other GNN architectures, such as SAGE <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b34\">(Ve en the same set of subgraphs are used for all baselines. For training, we use neighborhood sampling <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017)</ref> with 10 neighbors sampled per node and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b36\">Wu et al., 2021)</ref>.  <ref type=\"bibr\" target=\"#b7\">et al., 2012;</ref><ref type=\"bibr\" target=\"#b24\">Li et al., 2020b)</ref>) suffers from significant accuracy drop and c Parallel SGD with Periodic Averaging (PSGD-PA<ref type=\"bibr\" target=\"#b7\">(Dean et al., 2012;</ref><ref type=\"bibr\" target=\"#b24\">Li et al., 2020b)</ref>) suffers from significant accuracy drop and c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ton et al., 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b34\">(Velickovic et al., 2018)</ref>, ResGCN <ref type=\"bibr\" target=\"#b22\">(Li et al., 2019)</ref> and APPNP <ref type=\"bibr\" target=\"#b20\">(Kli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ale GNN training by sampling techniques <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b42\">Zou et al., 2019;</ref><ref type=\"bibr\" target=\"#b39\">Zeng et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ang et al., 2018)</ref>, and drug discovery <ref type=\"bibr\" target=\"#b11\">(Fout et al., 2017;</ref><ref type=\"bibr\" target=\"#b9\">Do et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Ghorbani et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b30\">(Srivastava and Sutton, 2017)</ref> and (ii) the SBERT embedded representations <ref type=\"bibr\" target=\"#b26\">(Reimers and Gurevych, 2019)</ref>. Let us notice that our method is  #b14\">(Hinton, 2002)</ref>. We extend this model with contextualized document embeddings from SBERT <ref type=\"bibr\" target=\"#b26\">(Reimers and Gurevych, 2019)</ref>, 2 a recent extension of BERT that. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  language models architectures.</p><p>Concretely, we extend Neural ProdLDA (Product-of-Experts LDA) <ref type=\"bibr\" target=\"#b30\">(Srivastava and Sutton, 2017)</ref>, a state-of-the-art topic model t  in topic models. Our model is built around two main components: (i) the neural topic model ProdLDA <ref type=\"bibr\" target=\"#b30\">(Srivastava and Sutton, 2017)</ref> and (ii) the SBERT embedded repre ch size is equal to 200. More details related to the architecture can be found in the original work <ref type=\"bibr\" target=\"#b30\">(Srivastava and Sutton, 2017)</ref>.</p></div> <div xmlns=\"http://www s performance.</p><p>LDA and NVDM obtain low coherence. This result has also also been confirmed by <ref type=\"bibr\" target=\"#b30\">Srivastava and Sutton (2017)</ref>. ETM shows good external coherence ilding upon NVDM, Dieng et al.</p><p>(2020) represent words and topics in the same embedding space. <ref type=\"bibr\" target=\"#b30\">Srivastava and Sutton (2017)</ref> propose a neural variational frame \" target=\"#b18\">(Miao et al., 2016;</ref><ref type=\"bibr\" target=\"#b20\">Mnih and Gregor, 2014;</ref><ref type=\"bibr\" target=\"#b30\">Srivastava and Sutton, 2017;</ref><ref type=\"bibr\" target=\"#b17\">Miao. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rodLDA replaces the multinomial distribution over individual words in LDA with a product of experts <ref type=\"bibr\" target=\"#b14\">(Hinton, 2002)</ref>. We extend this model with contextualized docume. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nana, spoon.\" Coherence can be measured in numerous ways, from human evaluation via intrusion tests <ref type=\"bibr\" target=\"#b5\">(Chang et al., 2009)</ref> to approximated scores <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g et al., 2009)</ref> to approximated scores <ref type=\"bibr\" target=\"#b16\">(Lau et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">R\u00f6der et al., 2015)</ref>.</p><p>However, most topic models still use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r that can benefit from both general language knowledge and corpusdependent information. Similarly, <ref type=\"bibr\" target=\"#b0\">Bianchi et al. (2021)</ref> replace the BOW document representation wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g Gaussian distributions, instead of using a Gaussian prior like Neural Variational Document Models <ref type=\"bibr\" target=\"#b18\">(Miao et al., 2016)</ref>. Moreover, ProdLDA replaces the multinomial 7, the model we extend) 7 , and the following models: (ii) Neural Variational Document Model (NVDM) <ref type=\"bibr\" target=\"#b18\">(Miao et al., 2016)</ref>; (iii) the very recent ETM (Dieng et al., 2 2009;</ref><ref type=\"bibr\" target=\"#b12\">Gupta et al., 2020)</ref> or neural variational inference <ref type=\"bibr\" target=\"#b18\">(Miao et al., 2016;</ref><ref type=\"bibr\" target=\"#b20\">Mnih and Greg ibr\" target=\"#b17\">Miao et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Ding et al., 2018)</ref>. <ref type=\"bibr\" target=\"#b18\">Miao et al. (2016)</ref> propose NVDM, an unsupervised generative mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b7\">(Das et al., 2015;</ref><ref type=\"bibr\" target=\"#b9\">Dieng et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Nguyen et al., 2015;</ref><ref type=\"bibr\" target=\"#b39\">Zhao et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the average pairwise cosine similarity of the word embeddings of the top-10 words in a topic, using <ref type=\"bibr\" target=\"#b19\">Mikolov et al. (2013)</ref> embeddings. Then, we compute the overall . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2020), MetaLDA (MLDA) <ref type=\"bibr\" target=\"#b39\">(Zhao et al., 2017)</ref> and (iv) LDA <ref type=\"bibr\" target=\"#b1\">(Blei et al., 2003)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rodLDA replaces the multinomial distribution over individual words in LDA with a product of experts <ref type=\"bibr\" target=\"#b14\">(Hinton, 2002)</ref>. We extend this model with contextualized docume. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2020)</ref> or neural variational inference <ref type=\"bibr\" target=\"#b18\">(Miao et al., 2016;</ref><ref type=\"bibr\" target=\"#b20\">Mnih and Gregor, 2014;</ref><ref type=\"bibr\" target=\"#b30\">Srivastava. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2020)</ref>.</p><p>Various extensions of topic models incorporate several types of information <ref type=\"bibr\" target=\"#b37\">(Xun et al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Zhao et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al., 2020)</ref>.</p><p>Various extensions of topic models incorporate several types of information <ref type=\"bibr\" target=\"#b37\">(Xun et al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Zhao et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  recent approach (Hoyle et al., 2020) which follows a similar direction uses knowledge distillation <ref type=\"bibr\" target=\"#b13\">(Hinton et al., 2015)</ref> to combine neural topic models and pre-tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2020b)</ref>, or pre-trained word embeddings <ref type=\"bibr\" target=\"#b7\">(Das et al., 2015;</ref><ref type=\"bibr\" target=\"#b9\">Dieng et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Nguyen et al.,  target=\"#b11\">(Gupta et al., 2019</ref><ref type=\"bibr\" target=\"#b12\">(Gupta et al., , 2020;;</ref><ref type=\"bibr\" target=\"#b9\">Dieng et al., 2020)</ref>.</p><p>In this paper, we show that adding co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2020b)</ref>, or pre-trained word embeddings <ref type=\"bibr\" target=\"#b7\">(Das et al., 2015;</ref><ref type=\"bibr\" target=\"#b9\">Dieng et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Nguyen et al.,  target=\"#b11\">(Gupta et al., 2019</ref><ref type=\"bibr\" target=\"#b12\">(Gupta et al., , 2020;;</ref><ref type=\"bibr\" target=\"#b9\">Dieng et al., 2020)</ref>.</p><p>In this paper, we show that adding co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g et al., 2009)</ref> to approximated scores <ref type=\"bibr\" target=\"#b16\">(Lau et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">R\u00f6der et al., 2015)</ref>.</p><p>However, most topic models still use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  cost.</p><p>\u2022 Expressivity challenge (i.e., oversmoothing <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b57\">58]</ref> and drug discovery <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. With the numerous architectures proposed <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .e., oversmoothing <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>): iterative mixing  ibr\" target=\"#b15\">[16]</ref>. In addition, various tricks <ref type=\"bibr\" target=\"#b58\">[59,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> can prevent oversmo sign does not rely on a specific neighborhood (e.g., L-hop). In addition to architectures, DropEdge <ref type=\"bibr\" target=\"#b37\">[38]</ref> and Bayesian-GDC <ref type=\"bibr\" target=\"#b12\">[13]</ref> ef type=\"figure\">8</ref>, 9 for time evaluation on CPU and GPU). During training, we apply DropEdge <ref type=\"bibr\" target=\"#b37\">[38]</ref> to both the baseline and SHADOW models. DropEdge helps imp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ration originally designed for graph classification (e.g., <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>) to enhance the node . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rameters does not oversmooth <ref type=\"bibr\" target=\"#b15\">[16]</ref>. In addition, various tricks <ref type=\"bibr\" target=\"#b58\">[59,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta et=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>]. Proposition 3.1. \u221e number of feature propagation by SHADOW. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with different threshold \u03b8 i on the neighbor PPR score. A SHADOW-GNN-ensemble can approximate PPRGo <ref type=\"bibr\" target=\"#b4\">[5]</ref>. PPRGo generates embedding as:</p><formula xml:id=\"formula_2 =\"foot\" n=\"2\" xml:id=\"foot_1\">Unlike other PPR-based models<ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> which rewire the graph by treating top PPR nodes as direct ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m V [v]</formula><p>. One can easily extend this approach by using other metrics such as Katz index <ref type=\"bibr\" target=\"#b18\">[19]</ref>, SimRank <ref type=\"bibr\" target=\"#b17\">[18]</ref> and fea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers:</p><p>\u2022 Scalability challenge (i.e., neighbor explosion <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= ve been explored to improve the training speed and efficiency. Importance based layer-wise sampling <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">The sampling methods<ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ Neighbor or subgraph sampling techniques have been proposed to improve training efficiency. FastGCN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, VR-GCN <ref type=\"bibr\" target=\"#b5\">[6]</ref>, AS-GCN <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b16\">[17]</ref>, LADIES <ref type=\"bibr\" target=\"#b59\">[60]</ref> and MVS-GNN <ref type=\"bibr\" target=\"#b8\">[9]</ref> sample neighbor nodes per GNN layer. Cluster-GCN <ref type=\" arget=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> aim at estimating the aggregation of the full L-hop neighborh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h a contrastive predictive coding loss. Recently, iGPT <ref type=\"bibr\" target=\"#b6\">[7]</ref>, ViT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and BEiT <ref type=\"bibr\" target=\"#b0\">[1]</ref> recall th nts, such as clustering on pixels <ref type=\"bibr\" target=\"#b6\">[7]</ref>, prediction of mean color <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and tokenization via an additional dVAE network with a bl asks. In this paper, we mainly consider two typical vision Transformer architectures: a vanilla ViT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and Swin Transformer <ref type=\"bibr\" target=\"#b31\">[32]</  DINO <ref type=\"bibr\" target=\"#b4\">[5]</ref>, MoCo v3 <ref type=\"bibr\" target=\"#b8\">[9]</ref>, ViT <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and BEiT <ref type=\"bibr\" target=\"#b0\">[1]</ref> (not cou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: fication <ref type=\"bibr\" target=\"#b38\">[40]</ref>, 63.1/54.4 box/mask mAP on COCO object detection <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, 59.9 mIoU on ADE20K fication <ref type=\"bibr\" target=\"#b38\">[40]</ref>, 63.1/54.4 box/mask mAP on COCO object detection <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, 59.9 mIoU on ADE20K. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imMIM is also demonstrated to be scalable to larger models: with a SwinV2-H model (658M parameters) <ref type=\"bibr\" target=\"#b30\">[31]</ref>, it achieves 87.1% top-1 accuracy on ImageNet-1K classific n fact, with the aid of Sim-MIM, we successfully trained a SwinV2-G model with 3 billion parameters <ref type=\"bibr\" target=\"#b30\">[31]</ref> using \u223c40\u00d7 smaller data than that of Google's JFT-3B datas nsformer of different model sizes for experiments, including Swin-B, Swin-L, SwinV2-H, and SwinV2-G <ref type=\"bibr\" target=\"#b30\">[31]</ref>. To reduce experimental overheads, we adopt a smaller imag vided by 10 for the remaining epochs. For model sizes of H and G, we use the variants introduced in <ref type=\"bibr\" target=\"#b30\">[31]</ref>, which have stronger stability than the original version.  except that SwinV2-G uses a larger and privately collected ImageNet-22K-ext dataset, as detailed in <ref type=\"bibr\" target=\"#b30\">[31]</ref>.</p><p>When using ImageNet-1K for pre-training, all models t=\"#b53\">55]</ref>, the proposed SimMIM approach is used to aid the training of a 3B SwinV2-G model <ref type=\"bibr\" target=\"#b30\">[31]</ref> by using \u223c40\u00d7 smaller data than that of JFT-3B. It achieve br\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. More details are described in <ref type=\"bibr\" target=\"#b30\">[31]</ref>. </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head dels, Swin-G is trained on a privately collected ImageNet-22K-ext dataset, with details described in<ref type=\"bibr\" target=\"#b30\">[31]</ref>.</figDesc><table><row><cell cols=\"6\">Methods Pre-train Fin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentation learning <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b55\">57,</ref><ref type=\"bibr\" target=\"#b56\">58]</ref>, in previous years, this line of works is almost buried by  osophy of predicting the invisible parts of signals, e.g., <ref type=\"bibr\" target=\"#b55\">[57,</ref><ref type=\"bibr\" target=\"#b56\">58]</ref> use one or two color channels as input to predict values of ]</ref>, jigsaw puzzle solving <ref type=\"bibr\" target=\"#b33\">[34]</ref>, split-brain auto-encoding <ref type=\"bibr\" target=\"#b56\">[58]</ref>, rotation prediction <ref type=\"bibr\" target=\"#b16\">[17]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  way: grayscale image colorization <ref type=\"bibr\" target=\"#b55\">[57]</ref>, jigsaw puzzle solving <ref type=\"bibr\" target=\"#b33\">[34]</ref>, split-brain auto-encoding <ref type=\"bibr\" target=\"#b56\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on for a long time <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b50\">52,</ref><ref type=\"bibr\" target=\"#b51\">53]</ref>, aiming for improving the inpainting quality and without co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\">39,</ref><ref type=\"bibr\" target=\"#b44\">46,</ref><ref type=\"bibr\" target=\"#b45\">47]</ref>. Similar as in our approach, they adopt a reconstruction ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar r\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> to report the perfo p>Also note that in previous contrastive learning approaches <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, it is a common pra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve learning approaches <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Vision tokenization. In BEiT <ref type=\"bibr\" target=\"#b0\">[1]</ref>, a discrete VAE (dVAE) network <ref type=\"bibr\" target=\"#b37\">[38]</ref> is employed to transform image patches to dVAE tokens. The. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b56\">[58]</ref>, rotation prediction <ref type=\"bibr\" target=\"#b16\">[17]</ref>, learning to cluster <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Though very different from masked image modeling, some of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b32\">Nambiar et al., 2020)</ref>. This new approach trains Transformer <ref type=\"bibr\" target=\"#b49\">(Vaswani et al., 2017</ref>) models 2 BACKGROUND Proteins are polymer to aid in optimization. Transformer implementations typically use a sine/cosine positional encoding <ref type=\"bibr\" target=\"#b49\">(Vaswani et al., 2017)</ref> or learned Gaussian positional encoding . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s of evolutionarily related proteins <ref type=\"bibr\" target=\"#b2\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b39\">Seemayer et  through pseudolikelihood maximization <ref type=\"bibr\" target=\"#b23\">(Kamisetty et al., 2013;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013)</ref>. On the other hand, BERT-like attention-b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b13\">Elnaggar et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Rao et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Madani et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Nambiar et a r\" target=\"#b37\">Rives et al. (2020)</ref>; <ref type=\"bibr\" target=\"#b35\">Rao et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b29\">Madani et al. (2020)</ref> train unsupervised attention-based models  \"bibr\" target=\"#b37\">Rives et al., 2020;</ref><ref type=\"bibr\" target=\"#b35\">Rao et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Madani et al., 2020)</ref> applies BERT-style pretraining directly on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s have subfamilies with different functional specializations and even different underlying contacts <ref type=\"bibr\" target=\"#b4\">(Brown et al., 2007;</ref><ref type=\"bibr\" target=\"#b30\">Malinverni &a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o insertions from an alignment algorithm <ref type=\"bibr\" target=\"#b21\">(Johnson et al., 2010;</ref><ref type=\"bibr\" target=\"#b36\">Remmert et al., 2012)</ref>, ensuring that positions with similar str. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b24\">(Korber et al., 1993;</ref><ref type=\"bibr\" target=\"#b17\">G\u00f6bel et al., 1994;</ref><ref type=\"bibr\" target=\"#b25\">Lapedes et al., 1999;</ref><ref type=\"bibr\" target=\"#b27\">Lockless &a ings\" corresponding to functional interactions from interactions induced by non-functional patterns <ref type=\"bibr\" target=\"#b25\">(Lapedes et al., 1999;</ref><ref type=\"bibr\" target=\"#b52\">Weigt et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s of evolutionarily related proteins <ref type=\"bibr\" target=\"#b2\">(Balakrishnan et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013;</ref><ref type=\"bibr\" target=\"#b39\">Seemayer et  through pseudolikelihood maximization <ref type=\"bibr\" target=\"#b23\">(Kamisetty et al., 2013;</ref><ref type=\"bibr\" target=\"#b12\">Ekeberg et al., 2013)</ref>. On the other hand, BERT-like attention-b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s embeddings as inputs to supervised structure prediction methods. When protein structure is known, <ref type=\"bibr\" target=\"#b19\">Ingraham et al. (2019)</ref> and <ref type=\"bibr\" target=\"#b9\">Du et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of evolutionarily related sequences. Given a large database of protein sequences such as UniRef100 <ref type=\"bibr\" target=\"#b45\">(Suzek et al., 2007)</ref> or BFD <ref type=\"bibr\" target=\"#b43\">(Ste. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  architectures have achieved great success in the Euclidean domain, including image, text and audio <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. As one of the typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b24\">25]</ref>, traffic flow prediction <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, knowledge graphs <ref type=\"bibr\" target=\"#b27\">[28,</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b4\">5]</ref> and image captioning <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. In natural language processing area, recurrent neural networ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  used in this setting.</p><p>In detail, the complete PPI dataset is provided by Zitnik and Leskovec <ref type=\"bibr\" target=\"#b72\">[73]</ref> and then was preprocessed again by Hamilton et al. <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o compare. For all of these datasets, the transductive experimental setup is similar to Yang et al. <ref type=\"bibr\" target=\"#b69\">[70]</ref>.</p><p>In detail, Cora, Citeseer and Pubmed are firstly in ond to citations. Note that the citation networks are represented as undirected graphs according to <ref type=\"bibr\" target=\"#b69\">[70]</ref>. Node features are bagof-words encoded documents. The clas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3.1.1\">ChebNet</head><p>In order to generalize operators of CNNs to graph domain, Defferrard et al. <ref type=\"bibr\" target=\"#b44\">[45]</ref> proposed a spectral-based graph convolutional network call  \u03b8 cannot localize in space and is complex to learn. To alleviate these problems, Defferrard et al. <ref type=\"bibr\" target=\"#b44\">[45]</ref> introduce a polynomial filter and compute g \u03b8 (L) in a rec /ref>), ChebNet bypasses the computation of the graph Fourier basis. Furthermore, Defferrard et al. <ref type=\"bibr\" target=\"#b44\">[45]</ref> denoted x k = T k ( L)x \u2208 R n and the hidden state is recu r where this process can be regarded as graph clustering, which is NP-hard. Thus, Defferrard et al. <ref type=\"bibr\" target=\"#b44\">[45]</ref> utilized the Graclus multi-level clustering algorithm <ref nefficient memory and computation. In order to accelerate the pooling operations, Defferrard et al. <ref type=\"bibr\" target=\"#b44\">[45]</ref> proposed a pooling strategy that has the same efficiency a  the learned representation also contains information of its multi-hop neighbors similar to ChebNet <ref type=\"bibr\" target=\"#b44\">[45]</ref>.</p><p>Specifically, Kipf and Welling <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed for several important tasks including sentiment analysis <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, machine translation <ref type=\"bibr\" target=\"#b11\">[12,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b6\">7]</ref>. In natural language processing area, recurrent neural networks (RNNs) <ref type=\"bibr\" target=\"#b7\">[8]</ref> or long short-term network (LSTM) <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, knowledge graphs <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. It is imperative . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: focused tasks (e.g., node classification, link prediction) <ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" target=\"#b35\">[36]</ref> and graph-focused. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt to a component boundary without and with the escape order assignment, respectively. Yan and Wong <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposed a network flow model for UER and modeled OER throug. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: deled OER through Boolean satisfiability (SAT) formulation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The UER model is not yet suitable for practical application 0\">1(d</ref>) presents a two-layer SER result, in which nets <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  result, in which nets <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b4\">5)</ref> are allocated to L0 and t of fine-level routes to satisfy the feasibility of coarse-level routing. The methods described in <ref type=\"bibr\" target=\"#b6\">[7]</ref> can be used to easily transform the following formulas into . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  result, in which nets <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b4\">5)</ref> are allocated to L0 and t of fine-level routes to satisfy the feasibility of coarse-level routing. The methods described in <ref type=\"bibr\" target=\"#b6\">[7]</ref> can be used to easily transform the following formulas into . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: deled OER through Boolean satisfiability (SAT) formulation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The UER model is not yet suitable for practical application 0\">1(d</ref>) presents a two-layer SER result, in which nets <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e routed in a layer.</p><p>To address simultaneous multilayer escape routing problems, Ozdal et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> incorporated SER into the longest path with forbidden pairs  1 <ref type=\"figure\">.</ref>(c) presents an SER with three layers resulting from the method used in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, in which the limited patterns allocate nets (4, 3, 1, 2, 8) nets (1, 6, 2, 8) are allocated to L1. For comparison, we applied the same routing patterns used in <ref type=\"bibr\" target=\"#b5\">[6]</ref> to the nets presented in Figs. <ref type=\"figure\" target=\"#f ER and exploited seven industrial cases as our testing benchmarks. We also implemented the SER from <ref type=\"bibr\" target=\"#b5\">[6]</ref> with C++ for subsequent comparisons. To evaluate the effecti  evaluate the effectiveness of the proposed SER, it is compared with the multilayer SER provided in <ref type=\"bibr\" target=\"#b5\">[6]</ref> (Table <ref type=\"table\" target=\"#tab_1\">I</ref>), where the  all cases, the proposed SER supplies less or equal #Layer and #Via in comparison with the SER from <ref type=\"bibr\" target=\"#b5\">[6]</ref>. On average, the proposed SER uses 27.5% less #Layer and 25. ves a 100% SR, whereas only two cases (b03 and b06) are successfully resolved by the algorithm from <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Table <ref type=\"table\" target=\"#tab_2\">II</ref> pres /1.0\" type=\"table\" xml:id=\"tab_1\"><head>TABLE I .</head><label>I</label><figDesc>SER COMPARISON WITH<ref type=\"bibr\" target=\"#b5\">[6]</ref> </figDesc><table><row><cell></cell><cell>#Open Vio.</cell><c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  result, in which nets <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b4\">5)</ref> are allocated to L0 and t of fine-level routes to satisfy the feasibility of coarse-level routing. The methods described in <ref type=\"bibr\" target=\"#b6\">[7]</ref> can be used to easily transform the following formulas into . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: deled OER through Boolean satisfiability (SAT) formulation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The UER model is not yet suitable for practical application 0\">1(d</ref>) presents a two-layer SER result, in which nets <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: deled OER through Boolean satisfiability (SAT) formulation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The UER model is not yet suitable for practical application 0\">1(d</ref>) presents a two-layer SER result, in which nets <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: deled OER through Boolean satisfiability (SAT) formulation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The UER model is not yet suitable for practical application 0\">1(d</ref>) presents a two-layer SER result, in which nets <ref type=\"bibr\" target=\"#b3\">(4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e routed in a layer.</p><p>To address simultaneous multilayer escape routing problems, Ozdal et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> incorporated SER into the longest path with forbidden pairs  1 <ref type=\"figure\">.</ref>(c) presents an SER with three layers resulting from the method used in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, in which the limited patterns allocate nets (4, 3, 1, 2, 8) nets (1, 6, 2, 8) are allocated to L1. For comparison, we applied the same routing patterns used in <ref type=\"bibr\" target=\"#b5\">[6]</ref> to the nets presented in Figs. <ref type=\"figure\" target=\"#f ER and exploited seven industrial cases as our testing benchmarks. We also implemented the SER from <ref type=\"bibr\" target=\"#b5\">[6]</ref> with C++ for subsequent comparisons. To evaluate the effecti  evaluate the effectiveness of the proposed SER, it is compared with the multilayer SER provided in <ref type=\"bibr\" target=\"#b5\">[6]</ref> (Table <ref type=\"table\" target=\"#tab_1\">I</ref>), where the  all cases, the proposed SER supplies less or equal #Layer and #Via in comparison with the SER from <ref type=\"bibr\" target=\"#b5\">[6]</ref>. On average, the proposed SER uses 27.5% less #Layer and 25. ves a 100% SR, whereas only two cases (b03 and b06) are successfully resolved by the algorithm from <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Table <ref type=\"table\" target=\"#tab_2\">II</ref> pres /1.0\" type=\"table\" xml:id=\"tab_1\"><head>TABLE I .</head><label>I</label><figDesc>SER COMPARISON WITH<ref type=\"bibr\" target=\"#b5\">[6]</ref> </figDesc><table><row><cell></cell><cell>#Open Vio.</cell><c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: an autoregressive seq2seq approach.</p><p>In the meantime, seq2seq Transformer models, such as BART <ref type=\"bibr\" target=\"#b17\">(Lewis et al., 2020)</ref> or T5 <ref type=\"bibr\" target=\"#b24\">(Raff essive model that outputs each triplet present in the input text. To this end, we employ BART-large <ref type=\"bibr\" target=\"#b17\">(Lewis et al., 2020)</ref> as the base model.</p><p>In a translation . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eq Transformer models, such as BART <ref type=\"bibr\" target=\"#b17\">(Lewis et al., 2020)</ref> or T5 <ref type=\"bibr\" target=\"#b24\">(Raffel et al., 2020)</ref> have been used in NLU tasks such as Entit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fy the relations between two given entities <ref type=\"bibr\" target=\"#b33\">(Zeng et al., 2014;</ref><ref type=\"bibr\" target=\"#b40\">Zhou et al., 2016)</ref>. Current approaches to Relation Classificati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y pairs to be inferred, which can become computationally expensive.</p><p>Seq2seq approaches for RE <ref type=\"bibr\" target=\"#b36\">(Zeng et al., 2018</ref><ref type=\"bibr\" target=\"#b34\">(Zeng et al.,   The dataset contains distantly annotated relations using FreeBase. We use the processed version of <ref type=\"bibr\" target=\"#b36\">Zeng et al. (2018)</ref> called NYT-multi, which contains overlapping. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elation Extraction is often used in the literature for different tasks and setups in the literature <ref type=\"bibr\" target=\"#b28\">(Taill\u00e9 et al., 2020)</ref>. For clarity, we refer to Relation Extrac  definition has led to discrepancies in the use of datasets and the way models have been evaluated. <ref type=\"bibr\" target=\"#b28\">Taill\u00e9 et al. (2020)</ref> explain the different issues in-so-far, an ame as the labeled ones (this is known as \"strict\" evaluation in RE) using the evaluation code from <ref type=\"bibr\" target=\"#b28\">Taill\u00e9 et al. (2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b20\">(Miwa and Sasaki, 2014;</ref><ref type=\"bibr\" target=\"#b23\">Pawar et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Katiyar and Cardie, 2017;</ref><ref type=\"bibr\" target=\"#b10\">Eberts   for both tasks. These range from LSTMs <ref type=\"bibr\" target=\"#b19\">(Miwa and Bansal, 2016;</ref><ref type=\"bibr\" target=\"#b15\">Katiyar and Cardie, 2017)</ref> to CNNs <ref type=\"bibr\" target=\"#b0\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eq Transformer models, such as BART <ref type=\"bibr\" target=\"#b17\">(Lewis et al., 2020)</ref> or T5 <ref type=\"bibr\" target=\"#b24\">(Raffel et al., 2020)</ref> have been used in NLU tasks such as Entit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y expensive.</p><p>Seq2seq approaches for RE <ref type=\"bibr\" target=\"#b36\">(Zeng et al., 2018</ref><ref type=\"bibr\" target=\"#b34\">(Zeng et al., , 2020;;</ref><ref type=\"bibr\" target=\"#b21\">Nayak and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dation set.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">DocRED</head><p>DocRED <ref type=\"bibr\" target=\"#b32\">(Yao et al., 2019)</ref> is a recent dataset created similarly to our. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dation set.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">DocRED</head><p>DocRED <ref type=\"bibr\" target=\"#b32\">(Yao et al., 2019)</ref> is a recent dataset created similarly to our. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b10\">(Eberts and Ulges, 2020;</ref><ref type=\"bibr\">Wang et al., 2020)</ref> or ALBERT <ref type=\"bibr\" target=\"#b16\">(Lan et al., 2020;</ref><ref type=\"bibr\" target=\"#b29\">Wang and Lu, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lm_loss(seq) is the standard masked language modeling loss as previously described in Devlin et al. <ref type=\"bibr\" target=\"#b28\">29</ref> . Note, that mask clumping does not affect how the loss is c tput dimension of the feed-forward unit at the end of each attention layer is 3072. As done in BERT <ref type=\"bibr\" target=\"#b28\">29</ref> , we prepend a [CLS] token at the beginning of each sequence ermuted or not. Masked language modeling loss calculations are set up as described in Devlin et al. <ref type=\"bibr\" target=\"#b28\">29</ref> .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: late that the latent space of the language model also captures recurrent evolutionary relationships <ref type=\"bibr\" target=\"#b42\">43</ref> . The use of Frenet-Serret formulas in RGN2 addresses the re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref> , raising thermostability <ref type=\"bibr\" target=\"#b16\">17</ref> , altering pH sensitivity <ref type=\"bibr\" target=\"#b17\">18<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b31\">[32]</ref><ref type=\"bibr\" target=\"#b32\">[33]</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: c protein sequences <ref type=\"bibr\" target=\"#b8\">9</ref> and ~11% of eukaryotic and viral proteins <ref type=\"bibr\" target=\"#b9\">10</ref> . Applications such as protein design and quantifying the eff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tivity <ref type=\"bibr\" target=\"#b17\">18</ref> , and increasing compatibility with organic solvents <ref type=\"bibr\" target=\"#b18\">19</ref> . Efficient and accurate structure prediction is also valuab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b31\">[32]</ref><ref type=\"bibr\" target=\"#b32\">[33]</ref><ref type=\"bibr\" target=\"#b33\">[34]</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thin these landscapes <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref> . A decade ago, the f based folding engines <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref> -closer to the real-w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tural language processing (NLP) as a means to extract semantic information from a sequence of words <ref type=\"bibr\" target=\"#b25\">26</ref> . In the context of proteins, AminoBERT aims to capture the  e introduce two training objectives not part of BERT or previously reported protein language models <ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b31\">[32]</ref><ref type=\"bibr\" ta ed on hundreds of millions and potentially billions of protein sequences are increasingly available <ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Applications such as protein design and quantifying the effects of sequence variation on function <ref type=\"bibr\" target=\"#b10\">11</ref> or immunogenicity <ref type=\"bibr\" target=\"#b11\">12</ref> al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vskaya et al., 2014)</ref> uses a classifier-based approach, which is improved by the latter system <ref type=\"bibr\" target=\"#b44\">(Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based \"bibr\" target=\"#b50\">(Susanto et al., 2014;</ref><ref type=\"bibr\">Chollampatt et al., 2016b,a;</ref><ref type=\"bibr\" target=\"#b44\">Rozovskaya and Roth, 2016;</ref><ref type=\"bibr\" target=\"#b26\">Junczy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ead n=\"3.1\">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation <ref type=\"bibr\" target=\"#b49\">(Sennrich et al., 2016)</ref> in NMT, referring to training a backwar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b28\">Leacock et al., 2010;</ref><ref type=\"bibr\" target=\"#b53\">Tetreault et al., 2010a;</ref><ref type=\"bibr\" target=\"#b14\">Dale and Kilgarriff, 2011)</ref> 7 The state-of-the-art result on CoN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: LC) <ref type=\"bibr\" target=\"#b37\">(Nicholls, 2003)</ref> and NUS Corpus of Learner English (NUCLE) <ref type=\"bibr\" target=\"#b13\">(Dahlmeier et al., 2013)</ref> as our original error-corrected traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: meier and</ref><ref type=\"bibr\">Ng, 2011, 2012a;</ref><ref type=\"bibr\">Yoshimoto et al., 2013;</ref><ref type=\"bibr\" target=\"#b60\">Yuan and Felice, 2013;</ref><ref type=\"bibr\">Behera and Bhattacharyya. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  sentences, as our main test set for evaluation. We use MaxMatch (M 2 ) precision, recall and F 0.5 <ref type=\"bibr\" target=\"#b11\">(Dahlmeier and Ng, 2012b)</ref> as our evaluation metrics. As previou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d Andersen, 2009;</ref><ref type=\"bibr\">Rozovskaya and</ref><ref type=\"bibr\">Roth, 2010, 2011;</ref><ref type=\"bibr\" target=\"#b45\">Rozovskaya et al., 2012;</ref><ref type=\"bibr\">Felice and Yuan, 2014;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b4\">(Chodorow et al., 2007;</ref><ref type=\"bibr\" target=\"#b15\">De Felice and Pulman, 2008;</ref><ref type=\"bibr\" target=\"#b21\">Han et al., 2010;</ref><ref type=\"bibr\" target=\"#b28\">Leacock et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: yer bidirectional GRU RNN and the decoder is a 2-layer GRU RNN with the general attention mechanism <ref type=\"bibr\" target=\"#b29\">(Luong et al., 2015)</ref>. Both the dimensionality of word embedding. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ei-c.org/ns/1.0\"><head n=\"6\">Related work</head><p>Most of advanced GEC systems are classifierbased <ref type=\"bibr\" target=\"#b4\">(Chodorow et al., 2007;</ref><ref type=\"bibr\" target=\"#b15\">De Felice . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b21\">Han et al., 2010;</ref><ref type=\"bibr\" target=\"#b28\">Leacock et al., 2010;</ref><ref type=\"bibr\" target=\"#b53\">Tetreault et al., 2010a;</ref><ref type=\"bibr\" target=\"#b14\">Dale and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b34\">Napoles et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Bryant et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Asano et al., 2017)</ref>. We do not introduce them in detail because . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b28\">Leacock et al., 2010;</ref><ref type=\"bibr\" target=\"#b53\">Tetreault et al., 2010a;</ref><ref type=\"bibr\" target=\"#b14\">Dale and Kilgarriff, 2011)</ref> 7 The state-of-the-art result on CoN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b56\">Xie et al., 2016;</ref><ref type=\"bibr\" target=\"#b24\">Ji et al., 2017;</ref><ref type=\"bibr\" target=\"#b48\">Schmaltz et al., 2017;</ref><ref type=\"bibr\" target=\"#b47\">Sakaguchi  br\" target=\"#b24\">Ji et al., 2017;</ref><ref type=\"bibr\" target=\"#b47\">Sakaguchi et al., 2017;</ref><ref type=\"bibr\" target=\"#b48\">Schmaltz et al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Chollampatt  et al., 2017)</ref>.</p><p>\u2022 Adapt-seq2seq: a seq2seq model adapted to incorporate edit operations <ref type=\"bibr\" target=\"#b48\">(Schmaltz et al., 2017)</ref>.</p><p>Table <ref type=\"table\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b28\">(Leacock et al., 2010;</ref><ref type=\"bibr\" target=\"#b40\">Rei and Yannakoudakis, 2016;</ref><ref type=\"bibr\" target=\"#b27\">Kaneko et al., 2017)</ref> and GEC evaluation <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5)</ref> as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU <ref type=\"bibr\" target=\"#b38\">(Papineni et al., 2002)</ref> and has several advantages over M 2 for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  sentences, as our main test set for evaluation. We use MaxMatch (M 2 ) precision, recall and F 0.5 <ref type=\"bibr\" target=\"#b11\">(Dahlmeier and Ng, 2012b)</ref> as our evaluation metrics. As previou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b4\">(Chodorow et al., 2007;</ref><ref type=\"bibr\" target=\"#b15\">De Felice and Pulman, 2008;</ref><ref type=\"bibr\" target=\"#b21\">Han et al., 2010;</ref><ref type=\"bibr\" target=\"#b28\">Leacock et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b4\">(Chodorow et al., 2007;</ref><ref type=\"bibr\" target=\"#b15\">De Felice and Pulman, 2008;</ref><ref type=\"bibr\" target=\"#b21\">Han et al., 2010;</ref><ref type=\"bibr\" target=\"#b28\">Leacock et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5)</ref> as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU <ref type=\"bibr\" target=\"#b38\">(Papineni et al., 2002)</ref> and has several advantages over M 2 for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b34\">Napoles et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Bryant et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Asano et al., 2017)</ref>. We do not introduce them in detail because . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lop methods to address the limitations of current methodologies by using a Transformer architecture <ref type=\"bibr\" target=\"#b10\">(Vaswani, Shazeer, and Parmar 2017)</ref> and testing the models with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he model's weaknesses and strengths by testing it against many types of template sentences as input <ref type=\"bibr\" target=\"#b8\">(Ribeiro et al. 2020)</ref>. By doing that, they evaluate several capa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng posts satisfied by a string pattern <ref type=\"bibr\" target=\"#b7\">(Reece and Danforth 2017;</ref><ref type=\"bibr\" target=\"#b9\">Shen et al. 2017;</ref><ref type=\"bibr\" target=\"#b3\">De Choudhury et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pendency aspect, and that we would like to experiment with other publicly available datasets, as in <ref type=\"bibr\" target=\"#b4\">(Kruk, Lubin, and Sikka 2019)</ref>.</p></div> <div xmlns=\"http://www.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b7\">(Reece and Danforth 2017;</ref><ref type=\"bibr\" target=\"#b9\">Shen et al. 2017;</ref><ref type=\"bibr\" target=\"#b3\">De Choudhury et al. 2013)</ref>. Furthermore, they predict single inst. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b7\">(Reece and Danforth 2017;</ref><ref type=\"bibr\" target=\"#b9\">Shen et al. 2017;</ref><ref type=\"bibr\" target=\"#b3\">De Choudhury et al. 2013)</ref>. Furthermore, they predict single inst. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ated as a Multiple Instance Learning (MIL) task, which works on a weakly supervised learning regime <ref type=\"bibr\" target=\"#b2\">(Carbonneau et al. 2018</ref>). In the MIL approach, data is arranged  nit of examples, and the supervision is provided only for the entire set, and not for the instances <ref type=\"bibr\" target=\"#b2\">(Carbonneau et al. 2018)</ref>. Usually, for social media datasets, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng posts satisfied by a string pattern <ref type=\"bibr\" target=\"#b7\">(Reece and Danforth 2017;</ref><ref type=\"bibr\" target=\"#b9\">Shen et al. 2017;</ref><ref type=\"bibr\" target=\"#b3\">De Choudhury et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing the bag label into the instances of the bag, or by labeling posts satisfied by a string pattern <ref type=\"bibr\" target=\"#b7\">(Reece and Danforth 2017;</ref><ref type=\"bibr\" target=\"#b9\">Shen et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b7\">(Reece and Danforth 2017;</ref><ref type=\"bibr\" target=\"#b9\">Shen et al. 2017;</ref><ref type=\"bibr\" target=\"#b3\">De Choudhury et al. 2013)</ref>. Furthermore, they predict single inst. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ated as a Multiple Instance Learning (MIL) task, which works on a weakly supervised learning regime <ref type=\"bibr\" target=\"#b2\">(Carbonneau et al. 2018</ref>). In the MIL approach, data is arranged  nit of examples, and the supervision is provided only for the entire set, and not for the instances <ref type=\"bibr\" target=\"#b2\">(Carbonneau et al. 2018)</ref>. Usually, for social media datasets, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ated as a Multiple Instance Learning (MIL) task, which works on a weakly supervised learning regime <ref type=\"bibr\" target=\"#b2\">(Carbonneau et al. 2018</ref>). In the MIL approach, data is arranged  nit of examples, and the supervision is provided only for the entire set, and not for the instances <ref type=\"bibr\" target=\"#b2\">(Carbonneau et al. 2018)</ref>. Usually, for social media datasets, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pendency aspect, and that we would like to experiment with other publicly available datasets, as in <ref type=\"bibr\" target=\"#b4\">(Kruk, Lubin, and Sikka 2019)</ref>.</p></div> <div xmlns=\"http://www.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ated as a Multiple Instance Learning (MIL) task, which works on a weakly supervised learning regime <ref type=\"bibr\" target=\"#b2\">(Carbonneau et al. 2018</ref>). In the MIL approach, data is arranged  nit of examples, and the supervision is provided only for the entire set, and not for the instances <ref type=\"bibr\" target=\"#b2\">(Carbonneau et al. 2018)</ref>. Usually, for social media datasets, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng posts satisfied by a string pattern <ref type=\"bibr\" target=\"#b7\">(Reece and Danforth 2017;</ref><ref type=\"bibr\" target=\"#b9\">Shen et al. 2017;</ref><ref type=\"bibr\" target=\"#b3\">De Choudhury et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elf-protection ability of fault tolerance mechanism. Control flow checking at virtual edges (CFCVE) <ref type=\"bibr\" target=\"#b25\">[26]</ref> inserts a virtual vertex into each edge at compile time. T. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ram control flow jumps normally. This encoding can represent a limited blocks number. These methods <ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref>, mixed software-hardware methods <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rcuit integration, reducing voltage levels, increasing transistor counts and reducing noise margins <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rcuit integration, reducing voltage levels, increasing transistor counts and reducing noise margins <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ection technology can be divided into hardware-based methods <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref>, mixed software-hard. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng. These methods <ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref> and CFCSS work in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: unter, bus, and address calculation unit. The CFEs account for about 30% to 70% of the total errors <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>.</p><p>The CFEs detect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ardware-based methods <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref>, mixed software-hardware methods <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref> and RSCFC work in the same way.</p><p>ECCA <ref type=\"bibr\" target=\"#b22\">[23]</ref> inserts assertions into each basic block for comparing and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref> and CFCSS work in the same way.</p><p>Relationship signatu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ardware-based methods <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref>, mixed software-hardware methods <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ardware-based methods <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref>, mixed software-hardware methods <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ck, the control flow is correct. Vice versa indicates that the control flow is wrong. These methods <ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b6\">7]</ref>.</p><p>The CFEs detection technology can be divided into hardware-based methods <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: educing noise margins <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oding can represent a limited blocks number. These methods <ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref> and RSCFC work in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons. The detection overhead and detection performance also depend on these two factors.</p><p>CFCSS <ref type=\"bibr\" target=\"#b13\">[14]</ref> method assign signature based on relationship of predecess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref>, mixed software-hardware methods <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a three-fold decrease in program performance and a four-fold increase in storage consumption. CEDA <ref type=\"bibr\" target=\"#b24\">[25]</ref> has the highest fault tolerance efficiency among all known. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ons. The detection overhead and detection performance also depend on these two factors.</p><p>CFCSS <ref type=\"bibr\" target=\"#b13\">[14]</ref> method assign signature based on relationship of predecess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b6\">7]</ref>.</p><p>The CFEs detection technology can be divided into hardware-based methods <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ootstrapped Graph Latents (BGRL). Inspired by recent advances in self-supervised learning in vision <ref type=\"bibr\" target=\"#b16\">(Grill et al., 2020)</ref> , BGRL learns node representations by enco esentation, Z 1 := p \u03b8 ( H 1 ).</p><p>BGRL differs from prior bootstrapping approaches such as BYOL <ref type=\"bibr\" target=\"#b16\">(Grill et al., 2020)</ref> in that it does not use a projector networ  works DGB (Che et al., 2020) and SelfGNN (Kefato &amp; Girdzijauskas, 2021), like BGRL, adapt BYOL <ref type=\"bibr\" target=\"#b16\">(Grill et al., 2020)</ref> for graph representation learning. However \u03c0 n total + 1 .</formula><p>These annealing schedules for both \u03b7 and \u03c4 follow the procedure used by <ref type=\"bibr\" target=\"#b16\">Grill et al. (2020)</ref>.</p><p>Frozen linear evaluation of embeddin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a variety of applications such as social networks, transportation networks, and biological sciences <ref type=\"bibr\" target=\"#b19\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b11\">Derrow-Pi issing feature information. In addition to simple mean-pooling propagation rules from GraphSage-GCN <ref type=\"bibr\" target=\"#b19\">(Hamilton et al., 2017)</ref>, we also consider Graph Attention Netwo 17)</ref>, composing GNNs and random-walks does not work very well and can even degrade performance <ref type=\"bibr\" target=\"#b19\">(Hamilton et al., 2017)</ref>. Earlier combinations of GNNs and self- i is a learned weight matrix for the i'th layer.</p><p>Mean Pooling Rule Formally, the Mean Pooling <ref type=\"bibr\" target=\"#b19\">(Hamilton et al., 2017)</ref> rule for a single layer is given by: MP ng due to the sheer size of the graph, we thus adopt the Neighborhood Sampling strategy proposed by <ref type=\"bibr\" target=\"#b19\">Hamilton et al. (2017)</ref> to sample a small number of central node otein-protein interaction network <ref type=\"bibr\" target=\"#b53\">(Zitnik &amp; Leskovec, 2017;</ref><ref type=\"bibr\" target=\"#b19\">Hamilton et al., 2017)</ref>, comprised of multiple ( <ref type=\"form. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mp; Welling, 2016)</ref> and Graph2Gauss <ref type=\"bibr\">(Bojchevski &amp; G\u00fcnnemann, 2018)</ref>. <ref type=\"bibr\" target=\"#b24\">Hu et al. (2020b)</ref> leverages BERT <ref type=\"bibr\" target=\"#b12\" \" target=\"#b12\">(Devlin et al., 2019)</ref> for representation learning in graph-structured inputs. <ref type=\"bibr\" target=\"#b24\">Hu et al. (2020b)</ref> assumes specific graph structures and uses fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Derrow-Pinion et al., 2021;</ref><ref type=\"bibr\" target=\"#b53\">Zitnik &amp; Leskovec, 2017;</ref><ref type=\"bibr\" target=\"#b6\">Chanussot et al., 2021)</ref>. Despite recent advances in graph neural. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sting of 4 heads of size 256 each and the final layer size 512 with 6 output heads), ELU activation <ref type=\"bibr\" target=\"#b9\">(Clevert et al., 2016)</ref>, and skip-connections in intermediate lay ., 2015)</ref> in all experiments except those using a GAT encoder, where we use the ELU activation <ref type=\"bibr\" target=\"#b9\">(Clevert et al., 2016)</ref>. In all our models, at each layer includi tputs for the first 2 layers, and use the mean for the final output. We also use the ELU activation <ref type=\"bibr\" target=\"#b9\">(Clevert et al., 2016)</ref> </p></div> <div xmlns=\"http://www.tei-c.o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ling, 2017)</ref>, attentional <ref type=\"bibr\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, or message-passing <ref type=\"bibr\" target=\"#b14\">(Gilmer et al., 2017)</ref> networks. BGRL performs four encoder comp fixed size neighborhood around them. Second, we use more expressive Message Passing Neural Networks <ref type=\"bibr\" target=\"#b14\">(Gilmer et al., 2017)</ref> as our graph encoders. Finally, as we are d><p>Message Passing Neural Networks encoders: We use a bi-directional version of the standard MPNN <ref type=\"bibr\" target=\"#b14\">(Gilmer et al., 2017)</ref> architectures with 4 message passing step. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en trained with supervised data alone, these networks can easily overfit and may fail to generalize <ref type=\"bibr\" target=\"#b40\">(Rong et al., 2019)</ref>. Thus, finding ways to form simplified repr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b69\">[70]</ref>, and HDR-VDP <ref type=\"bibr\" target=\"#b45\">[46]</ref>. It has been shown in <ref type=\"bibr\" target=\"#b70\">[71]</ref> that the internal activations of network trained for class. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n and self-attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b64\">65]</ref> to * Equal contribu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e. aligning the ImageNettrained VGG features, in many synthesis tasks such as neural style transfer <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, image super-resolut Such deep features have been widely used in image generation <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hods in-clude MOCO <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, SimCLR <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b34\">[35]</ref>, conditional image synthesis <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. It was generally believed that constraining perceptual los ed in image generation <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  effective in enhancing the reconstructed image. Specifically, we add a patch-based discriminator D <ref type=\"bibr\" target=\"#b39\">[40]</ref>, aiming to make the original image and the reconstructed o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: line can enable unsupervised pre-training. In the experiments, we train the codebook on ImageNet-1K <ref type=\"bibr\" target=\"#b17\">[18]</ref> and adopt visual Transformers with BERT pre-training <ref   After pre-training the model, we apply the model to various downstream tasks including ImageNet-1K <ref type=\"bibr\" target=\"#b17\">[18]</ref> classification, COCO object detection <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref> has displayed tre et=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b50\">[51]</ref><ref type=\"bibr\" target=\"#b51\">[52]</ref><ref type=\"bibr\" target=\"#b52\">[53]</ref> in NLP. Thanks to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: network has once been the dominant backbone, from the integration of convolution and self-attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ile the images present continuous values in the color space. To acquire discrete image tokens, iGPT <ref type=\"bibr\" target=\"#b10\">[11]</ref> quantized the pixel values using k-means, ViT <ref type=\"b still resort to regression for generative methods due to the lack of a visual vocabulary, e.g. iGPT <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Recently, BEiT <ref type=\"bibr\" target=\"#b1\">[2]</ref> su at success in various tasks of NLP. To apply the cross-entropy loss function for vision tasks, iGPT <ref type=\"bibr\" target=\"#b10\">[11]</ref> clusters the pixel values to simulate the process of BPE < ediction problem to an easier classification problem. While in CV, on the other hand, most attempts <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n and self-attention <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b64\">65]</ref> to * Equal contribu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, referred to as the sequential recommendation (SR) problem <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. One of the fundamental assumptions of SR is that the users sumptions of SR is that the users' interests change smoothly <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar al pattern based on the transition of items within sequences <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, thus lacking the m rative signals to the recommendation on \ud835\udc61 5 . Existing works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar  tasks, a series of attention-based SR models are proposed <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar ef>, current endeavors design a series of self-attention SR models to predict future item sequences <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta m revealing the actual temporal effects of collaborative signals.  Current transformer-based models <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> adopt self-attenti mmendation for every timestamp \ud835\udc61 \u2208 T \ud835\udc62 . This is a generalized definition compared with other works <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. We explicitly con  but they cannot learn the impacts of all historical items, thus unable to encode sequences. SASRec <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed to use a self-attention mechanism to encode item  \">32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. SASRec <ref type=\"bibr\" target=\"#b11\">[12]</ref> applies the transformer layer to assign weights to items i rmer, we mainly focus on comparing with the recent transformer-based SR methods,   which are SASRec <ref type=\"bibr\" target=\"#b11\">[12]</ref>, BERT4Rec <ref type=\"bibr\" target=\"#b35\">[36]</ref>, SSE-P stribution. The second and third variants replace the \u03a6 with a learnable positional embedding as in <ref type=\"bibr\" target=\"#b11\">[12]</ref> and emptying all zeros, respectively. The results are labe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  self-attention SR models to predict future item sequences <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. A self-attention m get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. SASRec <ref type=\" rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> assume that items appear discretely with equal time interva f collaborative signals.  Current transformer-based models <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> adopt self-attention mechanism, which has query, key, and v n the sequence. Later, inspired by the BERT <ref type=\"bibr\" target=\"#b2\">[3]</ref> model, BERT4Rec <ref type=\"bibr\" target=\"#b35\">[36]</ref> is proposed with a bidirectional transformer layer. <ref t ransformer-based SR methods,   which are SASRec <ref type=\"bibr\" target=\"#b11\">[12]</ref>, BERT4Rec <ref type=\"bibr\" target=\"#b35\">[36]</ref>, SSE-PT <ref type=\"bibr\" target=\"#b43\">[44]</ref>, and TiS. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ifically designed to capture sequential patterns, while ignoring the important temporal information <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta s and items changes over time, which is crucial for modeling the temporal dynamics in SR. TimeSVD++ <ref type=\"bibr\" target=\"#b14\">[15]</ref> is a representative work which models the temporal informa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in expressing the temporal information. Some recent works <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> also notice the importance of time span. But their models e m embeddings. Regarding the SR problem, a few recent works <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> also notice the importance of temporal information. CTA <re uous-Time Embedding.</head><p>The continuous time encoding <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> behaves as a function that maps those scalar timestamps int previous SR models <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, time span plays a vital component in expressing the tempor t=\"#b18\">[19]</ref> all consider to use time intervals between successive items in sequences. TASER <ref type=\"bibr\" target=\"#b49\">[50]</ref> encodes both the absolute time and relative time as vector 0 \u22121 , 10 \u22121 , 10 \u22122 , 10 \u22123 ]. For sequential methods, we search the maximum length of sequence in <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\">100]</ref>, number of layers from <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ddings from historical records on the user-item interactions <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. In order to model the dynamics of the user-item interactio ead n=\"4.4\">Model Optimization</head><p>To learn the model parameters, we use the pairwise BPR loss <ref type=\"bibr\" target=\"#b33\">[34]</ref>, which is widely used for top-N recommendation. The pairwi  static user/item embeddings for a recommendation. We compare with the most standard baseline BPRMF <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and also compare with a recent GNN-based model LightGCN <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3\">Graph-based Recommendation</head><p>Because we solve the SR problem based on the graph structure <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, we also review so. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odels are proposed <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar r\" target=\"#b43\">[44]</ref> designs a personalized transformer to improve the SR performance. ASReP <ref type=\"bibr\" target=\"#b22\">[23]</ref> proposes augmenting short sequences to alleviate the cold-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. SASRec <ref type=\"bibr\" target=\"#b11\">[12]</ref> applies t ef><ref type=\"bibr\" target=\"#b49\">50]</ref> also notice the importance of temporal information. CTA <ref type=\"bibr\" target=\"#b42\">[43]</ref>, MTAM <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and TiSAS  \ud835\udc47 \u21a6 \u2192 R \ud835\udc51 \ud835\udc47 , where \ud835\udc47 \u2208 R + . Based on previous SR models <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, time span plays a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: achieved by a continuous and translation-invariant kernel K (\ud835\udc61 1 , \ud835\udc61 2 ) based on Bochner's Theorem <ref type=\"bibr\" target=\"#b26\">[27]</ref>. By explicitly representing the temporal features, the tem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ls, we search for the dimensions of embeddings \ud835\udc51 in range of <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\">64] t \ud835\udc62 has no interactions with before \ud835\udc61 as negative items. Regarding the sampling bias for evaluation <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we apply the unbiased estimator in <ref type=\"bibr\" targe g bias for evaluation <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we apply the unbiased estimator in <ref type=\"bibr\" target=\"#b15\">[16]</ref> to correct the sampled ranks. We evaluate the top-N recomm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b48\">[49]</ref> learns the dynamic graph embeddings based on the graph attention model. Basconv <ref type=\"bibr\" target=\"#b24\">[25]</ref> characterizes heterogeneous graphs to learn user/item embe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion. However, few works investigate the possibility of solving SR problems based on graphs. SR-GNN <ref type=\"bibr\" target=\"#b44\">[45]</ref> learns embeddings of session graphs by using a GNN to aggr ref type=\"bibr\" target=\"#b8\">[9]</ref>, Caser <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and SR-GNN <ref type=\"bibr\" target=\"#b44\">[45]</ref>, for comprehensive study.</p><p>For each testing interacti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: del is that it can only generate a single hidden vector, which limits its power to encode sequences <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Recently, owing to the success of self-attention mode [50,</ref><ref type=\"bibr\">100]</ref>, number of layers from <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, and number of heads in ef type=\"bibr\" target=\"#b2\">3]</ref>, and number of heads in <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p></div> <div xmlns=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in expressing the temporal information. Some recent works <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> also notice the importance of time span. But their models e m embeddings. Regarding the SR problem, a few recent works <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> also notice the importance of temporal information. CTA <re uous-Time Embedding.</head><p>The continuous time encoding <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> behaves as a function that maps those scalar timestamps int previous SR models <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, time span plays a vital component in expressing the tempor t=\"#b18\">[19]</ref> all consider to use time intervals between successive items in sequences. TASER <ref type=\"bibr\" target=\"#b49\">[50]</ref> encodes both the absolute time and relative time as vector 0 \u22121 , 10 \u22121 , 10 \u22122 , 10 \u22123 ]. For sequential methods, we search the maximum length of sequence in <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\">100]</ref>, number of layers from <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ifically designed to capture sequential patterns, while ignoring the important temporal information <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta s and items changes over time, which is crucial for modeling the temporal dynamics in SR. TimeSVD++ <ref type=\"bibr\" target=\"#b14\">[15]</ref> is a representative work which models the temporal informa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cause we solve the SR problem based on the graph structure <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, we also review some graph-based recommender system models . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odels are proposed <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar r\" target=\"#b43\">[44]</ref> designs a personalized transformer to improve the SR performance. ASReP <ref type=\"bibr\" target=\"#b22\">[23]</ref> proposes augmenting short sequences to alleviate the cold-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> leverage historical ests change smoothly <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Thus, we can train C <ref type=\"bibr\" target=\"#b34\">[35]</ref>, GRU4Rec <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Caser <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and SR-GNN <ref type=\"bibr\" target=\"#b44\">[45]</ref>, for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>Recently, owing to the success of self-attention model <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> leverage historical ests change smoothly <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Thus, we can train C <ref type=\"bibr\" target=\"#b34\">[35]</ref>, GRU4Rec <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Caser <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and SR-GNN <ref type=\"bibr\" target=\"#b44\">[45]</ref>, for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: del is that it can only generate a single hidden vector, which limits its power to encode sequences <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Recently, owing to the success of self-attention mode [50,</ref><ref type=\"bibr\">100]</ref>, number of layers from <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, and number of heads in ef type=\"bibr\" target=\"#b2\">3]</ref>, and number of heads in <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p></div> <div xmlns=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b48\">[49]</ref> learns the dynamic graph embeddings based on the graph attention model. Basconv <ref type=\"bibr\" target=\"#b24\">[25]</ref> characterizes heterogeneous graphs to learn user/item embe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e a policy gradient method to iteratively update \u03b8 c . In this work, we use the REINFORCE rule from <ref type=\"bibr\" target=\"#b58\">Williams (1992)</ref>:</p><formula xml:id=\"formula_1\">\u03b8c J(\u03b8 c ) = T . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b44\">(Reed &amp; de Freitas, 2015)</ref>, and learning with very few examples <ref type=\"bibr\" target=\"#b27\">(Lake et al., 2015)</ref>.</p><p>The controller in Neural Architectur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as speech recognition <ref type=\"bibr\" target=\"#b15\">(Hinton et al., 2012)</ref>, image recognition <ref type=\"bibr\" target=\"#b29\">(LeCun et al., 1998;</ref><ref type=\"bibr\" target=\"#b26\">Krizhevsky e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b32\">(Liang et al., 2010;</ref><ref type=\"bibr\" target=\"#b40\">Neelakantan et al., 2015;</ref><ref type=\"bibr\" target=\"#b0\">Andreas et al., 2016)</ref>, sort a list of numbers <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ram induction has been used successfully in many settings, such as learning to solve simple Q&amp;A <ref type=\"bibr\" target=\"#b32\">(Liang et al., 2010;</ref><ref type=\"bibr\" target=\"#b40\">Neelakantan . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: man, 2014)</ref>, GoogleNet <ref type=\"bibr\" target=\"#b55\">(Szegedy et al., 2015)</ref>, and ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016a)</ref>. Although it has become easier, designing ar res such as GoogleNet <ref type=\"bibr\" target=\"#b55\">(Szegedy et al., 2015)</ref>, and Residual Net <ref type=\"bibr\" target=\"#b13\">(He et al., 2016a)</ref>. In this section we introduce a method that  as many rectangular filters and it prefers larger filters at the top layers. Like residual networks <ref type=\"bibr\" target=\"#b13\">(He et al., 2016a)</ref>, the architecture also has many one-step ski. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n much success of deep neural networks in many challenging applications, such as speech recognition <ref type=\"bibr\" target=\"#b15\">(Hinton et al., 2012)</ref>, image recognition <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 012)</ref>, VGGNet <ref type=\"bibr\" target=\"#b46\">(Simonyan &amp; Zisserman, 2014)</ref>, GoogleNet <ref type=\"bibr\" target=\"#b55\">(Szegedy et al., 2015)</ref>, and ResNet <ref type=\"bibr\" target=\"#b1  does not have skip connections, or branching layers used in modern architectures such as GoogleNet <ref type=\"bibr\" target=\"#b55\">(Szegedy et al., 2015)</ref>, and Residual Net <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b32\">(Liang et al., 2010;</ref><ref type=\"bibr\" target=\"#b40\">Neelakantan et al., 2015;</ref><ref type=\"bibr\" target=\"#b0\">Andreas et al., 2016)</ref>, sort a list of numbers <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: llels to program synthesis and inductive programming, the idea of searching a program from examples <ref type=\"bibr\" target=\"#b52\">(Summers, 1977;</ref><ref type=\"bibr\" target=\"#b7\">Biermann, 1978)</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mizer with a learning rate of 0.1, weight decay of 1e-4, momentum of 0.9 and used Nesterov Momentum <ref type=\"bibr\" target=\"#b53\">(Sutskever et al., 2013)</ref>.</p><p>During the training of the cont. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 012)</ref>, VGGNet <ref type=\"bibr\" target=\"#b46\">(Simonyan &amp; Zisserman, 2014)</ref>, GoogleNet <ref type=\"bibr\" target=\"#b55\">(Szegedy et al., 2015)</ref>, and ResNet <ref type=\"bibr\" target=\"#b1  does not have skip connections, or branching layers used in modern architectures such as GoogleNet <ref type=\"bibr\" target=\"#b55\">(Szegedy et al., 2015)</ref>, and Residual Net <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /p><p>In the past few years, deep learning on graphs and in particular graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b50\">(Sperduti, 1994;</ref><ref type=\"bibr\" target=\"#b16\">Goller &amp; Kuc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oot\" target=\"#foot_3\">4</ref> ; Chameleon and Squirrel (Rozemberczki et al., 2021) along with Actor <ref type=\"bibr\" target=\"#b53\">(Tang et al., 2009);</ref><ref type=\"bibr\">and Cora (McCallum et al., etworks, along with Actor, the actor-only induced subgraph of the filmdirector-actor-writer network <ref type=\"bibr\" target=\"#b53\">(Tang et al., 2009)</ref>. <ref type=\"bibr\">Cora (McCallum et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b11\">Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b23\">Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b15\">Gilmer et al., 2017)</ref> have become very popular in the machine le 9\">(Shlomi et al., 2021)</ref>.</p><p>The vast majority of GNNs follow the message passing paradigm <ref type=\"bibr\" target=\"#b15\">(Gilmer et al., 2017)</ref>, using learnable non-linear functions to  tions \u03c6 : R p \u00d7 R p \u2192 R p +1 , we can write the ( + 1)-st layer output of a generic MPNN as follows <ref type=\"bibr\" target=\"#b15\">(Gilmer et al., 2017)</ref>:</p><formula xml:id=\"formula_1\">h ( +1) i gm of message passing graph neural networks assumes that messages are propagated on the input graph <ref type=\"bibr\" target=\"#b15\">(Gilmer et al., 2017)</ref>. More recently, there is a trend to decou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce and the definition is biased towards negative curvature. The theory on \u03ba(i, j) instead is richer <ref type=\"bibr\" target=\"#b25\">(Lin et al., 2011;</ref><ref type=\"bibr\" target=\"#b31\">M\u00fcnch, 2019)</ ionship between h G and the curvature of the graph. The next proposition follows from Theorem 2 and <ref type=\"bibr\" target=\"#b25\">Lin et al. (2011)</ref>:</p><formula xml:id=\"formula_11\">Proposition  formula><p>We are now ready to define the Ollivier Ricci curvature: the formulation below is due to <ref type=\"bibr\" target=\"#b25\">Lin et al. (2011)</ref>. Definition 5. Given i \u223c j we define the \u03b1-Ol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , exhibit curvature that makes them more suitable to be realized in spaces with hyperbolic geometry <ref type=\"bibr\" target=\"#b26\">(Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b6\">Chami et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>In the past few years, deep learning on graphs and in particular graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b50\">(Sperduti, 1994;</ref><ref type=\"bibr\" target=\"#b16\">Goller &amp; Kuc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b46\">Scarselli et al., 2008;</ref><ref type=\"bibr\" target=\"#b5\">Bruna et al., 2014;</ref><ref type=\"bibr\" target=\"#b11\">Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b23\">Kipf &am. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b26\">(Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b6\">Chami et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Boguna et al., 2021)</ref>. One notion of curvature that has received . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  passing paradigm have now been identified and formalized, including the limits of expressive power <ref type=\"bibr\" target=\"#b58\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Morris et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and one-shot methods <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" ta ing a big supernet. This framework is widely applied in many efficient NAS methods, including DARTS <ref type=\"bibr\" target=\"#b25\">[26]</ref>, SNAS <ref type=\"bibr\" target=\"#b58\">[59]</ref>, PC-DARTS  ion. Several previous works design networks in a more general irregular design space, such as DARTS <ref type=\"bibr\" target=\"#b25\">[26]</ref> and RandWire <ref type=\"bibr\" target=\"#b57\">[58]</ref>. Ze. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ctor-based methods <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and one-shot methods <ref type=\"bibr\" target=\"#b25\">[26,</r er of architectures are trained to obtain their accuracies <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and then are used as training data for learning accuracy pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctor-based methods <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and one-shot methods <ref type=\"bibr\" target=\"#b25\">[26,</r er of architectures are trained to obtain their accuracies <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and then are used as training data for learning accuracy pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">41]</ref>, predictor-based methods <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and one-shot method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">41]</ref>, predictor-based methods <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and one-shot method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b65\">66,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>.</p><p>A major challe AS <ref type=\"bibr\" target=\"#b5\">[6]</ref>, GDAS <ref type=\"bibr\" target=\"#b65\">[66]</ref>, FBNetV2 <ref type=\"bibr\" target=\"#b53\">[54]</ref>, DNANet <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Single-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed and supervised pretraining from a multilayer perceptron (MLP) perspective. While previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ding transferability by considering the multilayer perception (MLP) projector. While previous works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe s that the MLP can reduce the loss of information caused by the contrastive loss, and various works <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> have verified that th  networks are pretrained on pre-D, and then examined by linear evaluation protocal on eval-D. As in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ ds. Adding a multilayer perceptron (MLP) projector after the encoder was first introduced in SimCLR <ref type=\"bibr\" target=\"#b7\">[8]</ref> and followed by recent unsupervised learning frameworks <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: methods. Inspired by <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>, recent researches  erability improvement by the MLP projector on COCO object detection task. We follow the settings in <ref type=\"bibr\" target=\"#b18\">[19]</ref> to finetune the whole network with 1\u00d7 schedule. In Tab. We. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b21\">[22]</ref>, MobileNetv2 <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and EfficientNetb2 <ref type=\"bibr\" target=\"#b42\">[43]</ref>.</p><p>Generalize to other classification tasks. To evalua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, illustrative <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, medical <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e on transfer tasks when there exists a large semantic gap <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. To our best knowle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: LP projector can influence what the convolution channels can learn. By using the method proposed in <ref type=\"bibr\" target=\"#b35\">[36]</ref>, we visualize the maximum response of convolution channels  \t\t\t<div type=\"annex\"> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Following the method proposed in <ref type=\"bibr\" target=\"#b35\">[36]</ref>, we visualize the maximum response of convolution channels. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lution Channels by Optimization</head><p>According to <ref type=\"bibr\" target=\"#b57\">[57]</ref> and <ref type=\"bibr\" target=\"#b0\">[1]</ref>, transfer performance is largely unaffected by the high-leve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oyed as the loss function. The MLP projector deployed in SL-MLP is described in Sec. 4.1. Following <ref type=\"bibr\" target=\"#b21\">[22]</ref>, we use the SGD optimizer with a cosine decay learning rat that SL-MLP can consistently improve the transferability of SL on various backbones, e.g. ResNet101 <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MobileNetv2 <ref type=\"bibr\" target=\"#b40\">[41]</ref>, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e on transfer tasks when there exists a large semantic gap <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. To our best knowle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lution Channels by Optimization</head><p>According to <ref type=\"bibr\" target=\"#b57\">[57]</ref> and <ref type=\"bibr\" target=\"#b0\">[1]</ref>, transfer performance is largely unaffected by the high-leve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  pretraining by the following two reasons: (1) Learning without semantic information in annotations <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" ta s not always predictive of the performance on transfer tasks when there exists a large semantic gap <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta evious works attributed the superior transferability of unsupervised learning to lack of annotation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: then examined by linear evaluation protocal on eval-D. As in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we train a linear . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : (1) Learning without semantic information in annotations <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" tar sferability of unsupervised learning to lack of annotation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" tar presentative unsupervised pretraining methods. Specifically, we use the concept generalization task <ref type=\"bibr\" target=\"#b41\">[42]</ref> on ImageNet-1K, where the pretraining and the evaluation d rability of the model on various downstream tasks. Specifically, on the concept generalization task <ref type=\"bibr\" target=\"#b41\">[42]</ref>, SL-MLP boosts the top-1 accuracy compared to SL (55.9%\u219263 1.0\"><head n=\"3.1.\">The Concept Generalization Task</head><p>We use the concept generalization task <ref type=\"bibr\" target=\"#b41\">[42]</ref> to analyze the transferability gap between the unsupervise ap between the unsupervised and supervised pretraining methods. Data preparation. Sariyildiz et al. <ref type=\"bibr\" target=\"#b41\">[42]</ref> evaluated the transferability of methods when the pretrain ining and the evaluation dataset to help us compare different pretraining methods. Sariyildizet al. <ref type=\"bibr\" target=\"#b41\">[42]</ref> use the hierarchy in WordNet <ref type=\"bibr\" target=\"#b30 and eval-D contains the other 348 classes of instrumentality. Transferability evaluation. Following <ref type=\"bibr\" target=\"#b41\">[42]</ref>, to assess the transferability, we freeze all parameters i /www.tei-c.org/ns/1.0\"><head>Concept Generalization Task with Small Semantic Gap</head><p>Following <ref type=\"bibr\" target=\"#b41\">[42]</ref>, to investigate how semantic difference between pre-D and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b21\">[22]</ref>, MobileNetv2 <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and EfficientNetb2 <ref type=\"bibr\" target=\"#b42\">[43]</ref>.</p><p>Generalize to other classification tasks. To evalua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based on the performance on the validation set. Few-shot Learning. For few-shot learning, following <ref type=\"bibr\" target=\"#b44\">[45]</ref>, we use a logistic regression layer on the top of the feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, symbolic <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, illustrative <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mers may end up with insufficiently trained GNNs. Inspired by recent success of curriculum learning <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 2009)</ref>, we propose to train the model progressive. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  As suggested by GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton et al., 2017a)</ref> and PinSage <ref type=\"bibr\" target=\"#b35\">(Ying et al., 2018)</ref>, the textual feature can be independently m ion are connected in the graph (which is a common way of graph construction in e-commerce scenarios <ref type=\"bibr\" target=\"#b35\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Wang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  texts can be captured more precisely; at the same time, with graph neural networks, like GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton et al., 2017a)</ref> and GAT <ref type=\"bibr\" target=\"#b23\"> ssary to combine both techniques for better textual graph representation. As suggested by GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton et al., 2017a)</ref> and PinSage <ref type=\"bibr\" target=\"#b Welling, 2016)</ref>, GAT <ref type=\"bibr\" target=\"#b23\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton et al., 2017a)</ref>) learn effective message passing mechan  PLMs for textual graph representation following the \"cascaded architecture\" suggested by GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton et al., 2017a)</ref>: the node features are independently en li\u010dkovi\u0107 et al., 2018)</ref>, GIN <ref type=\"bibr\" target=\"#b30\">(Xu et al., 2018)</ref>, GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton et al., 2017a)</ref>. The GAT aggregator, where the node emb. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  As suggested by GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton et al., 2017a)</ref> and PinSage <ref type=\"bibr\" target=\"#b35\">(Ying et al., 2018)</ref>, the textual feature can be independently m ion are connected in the graph (which is a common way of graph construction in e-commerce scenarios <ref type=\"bibr\" target=\"#b35\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Wang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: beddings learned by Skip-Gram <ref type=\"bibr\" target=\"#b14\">(Mikolov et al., 2013)</ref> and GloVe <ref type=\"bibr\" target=\"#b15\">(Pennington et al., 2014)</ref>. In recent years, the backbone networ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b38\">(Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2021;</ref><ref type=\"bibr\" target=\"#b7\">Hu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Liu et al., 2019b e=\"bibr\" target=\"#b38\">(Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2021;</ref><ref type=\"bibr\" target=\"#b7\">Hu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Liu et al., 2019b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ic in multiple areas, such as natural language processing, information retrieval and graph learning <ref type=\"bibr\" target=\"#b32\">(Yang et al., 2015;</ref><ref type=\"bibr\">Wang et al., 2016b,a;</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b38\">(Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2021;</ref><ref type=\"bibr\" target=\"#b7\">Hu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Liu et al., 2019b e=\"bibr\" target=\"#b38\">(Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2021;</ref><ref type=\"bibr\" target=\"#b7\">Hu et al., 2020;</ref><ref type=\"bibr\" target=\"#b12\">Liu et al., 2019b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph construction in e-commerce scenarios <ref type=\"bibr\" target=\"#b35\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Wang et al., 2018)</ref>). Each product has its unique textual descri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph construction in e-commerce scenarios <ref type=\"bibr\" target=\"#b35\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Wang et al., 2018)</ref>). Each product has its unique textual descri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing or oversquashing <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. To summarize, the maximum context size for common GNN archit tains anti-lung-cancer activity. We follow the settings in <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> for the NCI1 and NCI109, randomly splitting the dataset into  cal, short-range correlations, for long-range correlations less structured modules may be preferred <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>We leverage this insight to the graph learning domain ref>, while the strong baselines <ref type=\"bibr\" target=\"#b12\">[13]</ref>, as well as the FA layer <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In Table <ref type=\"table\" target=\"#tab_2\">1</ref>, Our Gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng the receptive field to a K-hop neighborhood may not capture these long-range dependencies either <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Often, \"too deep\" GNNs lead to node representations that  e-hop neighborhood <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, it remains to be seen how this approach will scale to very. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b37\">[38]</ref>, Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and Dwivedi and Bresson <ref type=\"bibr\" target=\"#b11\">[12]</ref> propose GNN layers that let nodes attend to other nodes in b37\">[38]</ref> do not consider whole-graph prediction problems, in the case of Dwivedi and Bresson <ref type=\"bibr\" target=\"#b11\">[12]</ref>, when a graph-wide embedding was needed for graph classifi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o be equivalent over the entire graph, a phenomenon sometimes called oversmoothing or oversquashing <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Model</head><p>Valid F1 score Test F1 score GIN <ref type=\"bibr\" target=\"#b35\">[36]</ref> 0.1376 0.1495 GCN <ref type=\"bibr\" target=\"#b17\">[18]</ref 5\">[36]</ref> 0.1376 0.1495 GCN <ref type=\"bibr\" target=\"#b17\">[18]</ref> 0.1399 0.1507 GIN-Virtual <ref type=\"bibr\" target=\"#b35\">[36]</ref> 0.1581 0.1439 GCN-Virtual <ref type=\"bibr\" target=\"#b17\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to graphs. Recent works such as Zhang et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref>, Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and Dwivedi and Bresson <ref type=\"bibr\" target=\"#b11\">[1 le graph encoding. Of these, Zhang et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> and Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> tackle the problem of learning long-range dependencies wit arget=\"#b37\">[38]</ref> take the attended neighborhood radius as a tuning parameter and Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> attend to neighborhoods of random size during training and ph classification or regression, they used global average pooling over the nodes, while Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> take a weighted sum over nodes with the weights computed b  field of a single GNN layer beyond a one-hop neighborhood <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, it remains to be s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oses GAug-M and GAug-O that generate augmented graphs via a differentiable edge predictor. GraphMix <ref type=\"bibr\" target=\"#b29\">[18]</ref> presents interpolation-based regularization by jointly tra domains <ref type=\"bibr\" target=\"#b30\">[19,</ref><ref type=\"bibr\" target=\"#b31\">20]</ref>. GraphMix <ref type=\"bibr\" target=\"#b29\">[18]</ref> is a regularization method based on semi-supervised learni <ref type=\"bibr\" target=\"#b33\">[22]</ref>, UDA* <ref type=\"bibr\" target=\"#b30\">[19]</ref>, GraphMix <ref type=\"bibr\" target=\"#b29\">[18]</ref>). In the case of DropEdge <ref type=\"bibr\" target=\"#b34\">[ get=\"#b28\">17,</ref><ref type=\"bibr\" target=\"#b32\">21,</ref><ref type=\"bibr\" target=\"#b33\">22,</ref><ref type=\"bibr\" target=\"#b29\">18]</ref>. Also, we denote out-of-memory as OOM. MH-Aug (w/o Reg) mea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s these challenges, learning-based data augmentation methods for graphs have been proposed. AdaEdge <ref type=\"bibr\" target=\"#b27\">[16]</ref> optimizes the graph topology based on the model prediction  <ref type=\"bibr\" target=\"#b34\">[23]</ref> to randomly remove a certain number of edges and AdaEdge <ref type=\"bibr\" target=\"#b27\">[16]</ref> to adaptively control the inter-class/intra-class edges. S augmentation-based supervised learning (DropEdge <ref type=\"bibr\" target=\"#b34\">[23]</ref>, AdaEdge <ref type=\"bibr\" target=\"#b27\">[16]</ref>), and semisupervised learning framework (GAug <ref type=\"b t=\"#b29\">[18]</ref>). In the case of DropEdge <ref type=\"bibr\" target=\"#b34\">[23]</ref> and AdaEdge <ref type=\"bibr\" target=\"#b27\">[16]</ref>, they use only cross-entropy loss (supervised setting) whi ison except for the case where the performance (marked with \u2020 ) is available in the original papers <ref type=\"bibr\" target=\"#b27\">[16,</ref><ref type=\"bibr\" target=\"#b28\">17,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b27\">[16]</ref> optimizes the graph topology based on the model prediction. <ref type=\"bibr\" target=\"#b28\">[17]</ref> proposes GAug-M and GAug-O that generate augmented graphs  y randomly dropping on a node-based was proposed in <ref type=\"bibr\" target=\"#b35\">[24]</ref>. GAug <ref type=\"bibr\" target=\"#b28\">[17]</ref> proposes the neural edge predictors as an augmentation mod f>, AdaEdge <ref type=\"bibr\" target=\"#b27\">[16]</ref>), and semisupervised learning framework (GAug <ref type=\"bibr\" target=\"#b28\">[17]</ref>, SSL <ref type=\"bibr\" target=\"#b32\">[21]</ref>, BVAT <ref  ds employing simple perturbation <ref type=\"bibr\" target=\"#b34\">[23]</ref> or extra augmentor model <ref type=\"bibr\" target=\"#b28\">[17,</ref><ref type=\"bibr\" target=\"#b36\">25]</ref>, we propose the sa mance (marked with \u2020 ) is available in the original papers <ref type=\"bibr\" target=\"#b27\">[16,</ref><ref type=\"bibr\" target=\"#b28\">17,</ref><ref type=\"bibr\" target=\"#b32\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  For instance, in image recognition, advanced methods like <ref type=\"bibr\" target=\"#b24\">[13]</ref><ref type=\"bibr\" target=\"#b25\">[14]</ref><ref type=\"bibr\" target=\"#b26\">[15]</ref> as well as simple. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, link prediction <ref type=\"bibr\" target=\"#b16\">[5]</ref><ref type=\"bibr\" target=\"#b17\">[6]</ref><ref type=\"bibr\" target=\"#b18\">[7]</ref> and graph classification <ref type=\"bibr\" target=\"#b19\">[8,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: our method on five benchmark datasets in three categories: (1) Citation networks: CORA and CITESEER <ref type=\"bibr\" target=\"#b42\">[31]</ref>, (2) Amazon product networks: Computers and Photo <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: node classification <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" target=\"#b15\">[4]</ref>, link prediction <ref type=\"bibr\" target=\"#b16\">[5]</ref><r verse datasets such as social networks <ref type=\"bibr\" target=\"#b21\">[10]</ref>, citation networks <ref type=\"bibr\" target=\"#b15\">[4]</ref>, physics <ref type=\"bibr\" target=\"#b22\">[11]</ref>, molecul  learning on graphs <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" target=\"#b15\">[4]</ref>. However, existing works only utilize a small subset of nod follow the standard data split protocol in the transductive settings for node classification, e.g., <ref type=\"bibr\" target=\"#b15\">[4]</ref> for CORA and CITESEER and <ref type=\"bibr\" target=\"#b43\">[3 aselines. As backbone models to validate MH-Aug, we adopt three standard graph neural networks: GCN <ref type=\"bibr\" target=\"#b15\">[4]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[1]</ref>, and GA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te Carlo method to draw random samples from a target distribution when direct sampling is difficult <ref type=\"bibr\" target=\"#b38\">[27]</ref>. The algorithm comprises three components: the target dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n proven effective by achieving impressive performance for diverse datasets such as social networks <ref type=\"bibr\" target=\"#b21\">[10]</ref>, citation networks <ref type=\"bibr\" target=\"#b15\">[4]</ref target=\"#b22\">[11]</ref>, molecules <ref type=\"bibr\" target=\"#b23\">[12]</ref>, and knowledge graphs <ref type=\"bibr\" target=\"#b21\">[10]</ref>. However, GNNs often suffer from weakgeneralization due to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: node classification <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" target=\"#b15\">[4]</ref>, link prediction <ref type=\"bibr\" target=\"#b16\">[5]</ref><r verse datasets such as social networks <ref type=\"bibr\" target=\"#b21\">[10]</ref>, citation networks <ref type=\"bibr\" target=\"#b15\">[4]</ref>, physics <ref type=\"bibr\" target=\"#b22\">[11]</ref>, molecul  learning on graphs <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" target=\"#b15\">[4]</ref>. However, existing works only utilize a small subset of nod follow the standard data split protocol in the transductive settings for node classification, e.g., <ref type=\"bibr\" target=\"#b15\">[4]</ref> for CORA and CITESEER and <ref type=\"bibr\" target=\"#b43\">[3 aselines. As backbone models to validate MH-Aug, we adopt three standard graph neural networks: GCN <ref type=\"bibr\" target=\"#b15\">[4]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[1]</ref>, and GA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n proven effective by achieving impressive performance for diverse datasets such as social networks <ref type=\"bibr\" target=\"#b21\">[10]</ref>, citation networks <ref type=\"bibr\" target=\"#b15\">[4]</ref target=\"#b22\">[11]</ref>, molecules <ref type=\"bibr\" target=\"#b23\">[12]</ref>, and knowledge graphs <ref type=\"bibr\" target=\"#b21\">[10]</ref>. However, GNNs often suffer from weakgeneralization due to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ses self-supervised learning strategies to exploit available information from graph structure. BVAT <ref type=\"bibr\" target=\"#b33\">[22]</ref> promotes the smoothness of GNNs by generating virtual adve GAug <ref type=\"bibr\" target=\"#b28\">[17]</ref>, SSL <ref type=\"bibr\" target=\"#b32\">[21]</ref>, BVAT <ref type=\"bibr\" target=\"#b33\">[22]</ref>, UDA* <ref type=\"bibr\" target=\"#b30\">[19]</ref>, GraphMix  et=\"#b27\">[16,</ref><ref type=\"bibr\" target=\"#b28\">17,</ref><ref type=\"bibr\" target=\"#b32\">21,</ref><ref type=\"bibr\" target=\"#b33\">22,</ref><ref type=\"bibr\" target=\"#b29\">18]</ref>. Also, we denote ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b34\">[23]</ref> or extra augmentor model <ref type=\"bibr\" target=\"#b28\">[17,</ref><ref type=\"bibr\" target=\"#b36\">25]</ref>, we propose the sampling-based augmentation, where a sequen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  For instance, in image recognition, advanced methods like <ref type=\"bibr\" target=\"#b24\">[13]</ref><ref type=\"bibr\" target=\"#b25\">[14]</ref><ref type=\"bibr\" target=\"#b26\">[15]</ref> as well as simple. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" target=\"#b15\">[4]</ref>, link prediction <ref type=\"bibr\" target=\"#b16\">[5]</ref><ref type=\"bibr\" target=\"#b17\">[6]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" target=\"#b15\">[4]</ref>, link prediction <ref type=\"bibr\" target=\"#b16\">[5]</ref><ref type=\"bibr\" target=\"#b17\">[6]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n proven effective by achieving impressive performance for diverse datasets such as social networks <ref type=\"bibr\" target=\"#b21\">[10]</ref>, citation networks <ref type=\"bibr\" target=\"#b15\">[4]</ref target=\"#b22\">[11]</ref>, molecules <ref type=\"bibr\" target=\"#b23\">[12]</ref>, and knowledge graphs <ref type=\"bibr\" target=\"#b21\">[10]</ref>. However, GNNs often suffer from weakgeneralization due to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: zation ability of models in many domains. For instance, in image recognition, advanced methods like <ref type=\"bibr\" target=\"#b24\">[13]</ref><ref type=\"bibr\" target=\"#b25\">[14]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b15\">[4]</ref>, physics <ref type=\"bibr\" target=\"#b22\">[11]</ref>, molecules <ref type=\"bibr\" target=\"#b23\">[12]</ref>, and knowledge graphs <ref type=\"bibr\" target=\"#b21\">[10]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ructured data due to their superior performance in various applications such as node classification <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" tar i-Supervised Learning on Graphs. GNNs have been widely adopted in representation learning on graphs <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ses self-supervised learning strategies to exploit available information from graph structure. BVAT <ref type=\"bibr\" target=\"#b33\">[22]</ref> promotes the smoothness of GNNs by generating virtual adve GAug <ref type=\"bibr\" target=\"#b28\">[17]</ref>, SSL <ref type=\"bibr\" target=\"#b32\">[21]</ref>, BVAT <ref type=\"bibr\" target=\"#b33\">[22]</ref>, UDA* <ref type=\"bibr\" target=\"#b30\">[19]</ref>, GraphMix  et=\"#b27\">[16,</ref><ref type=\"bibr\" target=\"#b28\">17,</ref><ref type=\"bibr\" target=\"#b32\">21,</ref><ref type=\"bibr\" target=\"#b33\">22,</ref><ref type=\"bibr\" target=\"#b29\">18]</ref>. Also, we denote ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ionary target distribution P .</p><p>This can be drawn from the Convergence theorem of Markov chain <ref type=\"bibr\" target=\"#b41\">[30]</ref>. The proof is in the supplement. By Lemma 3.1, we theoreti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b34\">[23]</ref> or extra augmentor model <ref type=\"bibr\" target=\"#b28\">[17,</ref><ref type=\"bibr\" target=\"#b36\">25]</ref>, we propose the sampling-based augmentation, where a sequen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ructured data due to their superior performance in various applications such as node classification <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" tar i-Supervised Learning on Graphs. GNNs have been widely adopted in representation learning on graphs <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n proven effective by achieving impressive performance for diverse datasets such as social networks <ref type=\"bibr\" target=\"#b21\">[10]</ref>, citation networks <ref type=\"bibr\" target=\"#b15\">[4]</ref target=\"#b22\">[11]</ref>, molecules <ref type=\"bibr\" target=\"#b23\">[12]</ref>, and knowledge graphs <ref type=\"bibr\" target=\"#b21\">[10]</ref>. However, GNNs often suffer from weakgeneralization due to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: node classification <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" target=\"#b15\">[4]</ref>, link prediction <ref type=\"bibr\" target=\"#b16\">[5]</ref><r verse datasets such as social networks <ref type=\"bibr\" target=\"#b21\">[10]</ref>, citation networks <ref type=\"bibr\" target=\"#b15\">[4]</ref>, physics <ref type=\"bibr\" target=\"#b22\">[11]</ref>, molecul  learning on graphs <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" target=\"#b15\">[4]</ref>. However, existing works only utilize a small subset of nod follow the standard data split protocol in the transductive settings for node classification, e.g., <ref type=\"bibr\" target=\"#b15\">[4]</ref> for CORA and CITESEER and <ref type=\"bibr\" target=\"#b43\">[3 aselines. As backbone models to validate MH-Aug, we adopt three standard graph neural networks: GCN <ref type=\"bibr\" target=\"#b15\">[4]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[1]</ref>, and GA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b15\">[4]</ref>, link prediction <ref type=\"bibr\" target=\"#b16\">[5]</ref><ref type=\"bibr\" target=\"#b17\">[6]</ref><ref type=\"bibr\" target=\"#b18\">[7]</ref> and graph classific. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te Carlo method to draw random samples from a target distribution when direct sampling is difficult <ref type=\"bibr\" target=\"#b38\">[27]</ref>. The algorithm comprises three components: the target dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ructured data due to their superior performance in various applications such as node classification <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" tar i-Supervised Learning on Graphs. GNNs have been widely adopted in representation learning on graphs <ref type=\"bibr\" target=\"#b13\">[2]</ref><ref type=\"bibr\" target=\"#b14\">[3]</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: our method on five benchmark datasets in three categories: (1) Citation networks: CORA and CITESEER <ref type=\"bibr\" target=\"#b42\">[31]</ref>, (2) Amazon product networks: Computers and Photo <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ITESEER <ref type=\"bibr\" target=\"#b42\">[31]</ref>, (2) Amazon product networks: Computers and Photo <ref type=\"bibr\" target=\"#b43\">[32]</ref>, and (3) Coauthor Networks: CS <ref type=\"bibr\" target=\"#b works: Computers and Photo <ref type=\"bibr\" target=\"#b43\">[32]</ref>, and (3) Coauthor Networks: CS <ref type=\"bibr\" target=\"#b43\">[32]</ref>. We follow the standard data split protocol in the transdu s for node classification, e.g., <ref type=\"bibr\" target=\"#b15\">[4]</ref> for CORA and CITESEER and <ref type=\"bibr\" target=\"#b43\">[32]</ref> for the rest.</p><p>Baselines. As backbone models to valid. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b12\">[1]</ref> have been widely used for representation learning on graphs adopt three standard graph neural networks: GCN <ref type=\"bibr\" target=\"#b15\">[4]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[1]</ref>, and GAT <ref type=\"bibr\" target=\"#b14\">[3]</ref>. We compa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ionary target distribution P .</p><p>This can be drawn from the Convergence theorem of Markov chain <ref type=\"bibr\" target=\"#b41\">[30]</ref>. The proof is in the supplement. By Lemma 3.1, we theoreti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b73\">74]</ref>, including mixup <ref type=\"bibr\" target=\"#b88\">[89]</ref>, cutmix <ref type=\"bibr\" target=\"#b87\">[88]</ref>, random erasing <ref type=\"bibr\" target=\"#b91\">[92]</ref>  rames, including random horizontal flip, mixup <ref type=\"bibr\" target=\"#b88\">[89]</ref> and cutmix <ref type=\"bibr\" target=\"#b87\">[88]</ref>, random erasing <ref type=\"bibr\" target=\"#b91\">[92]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b88\">[89]</ref>, cutmix <ref type=\"bibr\" target=\"#b87\">[88]</ref>, random erasing <ref type=\"bibr\" target=\"#b91\">[92]</ref> and rand augment <ref type=\"bibr\" target=\"#b12\">[13]</ref> bibr\" target=\"#b88\">[89]</ref> and cutmix <ref type=\"bibr\" target=\"#b87\">[88]</ref>, random erasing <ref type=\"bibr\" target=\"#b91\">[92]</ref>, and rand augment <ref type=\"bibr\" target=\"#b12\">[13]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctors such as HTC++ <ref type=\"bibr\" target=\"#b9\">[10]</ref> and inference strategies (e.g. SoftNMS <ref type=\"bibr\" target=\"#b3\">[4]</ref> or multi-scale testing) can boost this number further but ar  layers) already model positional information. (ii) Comparing<ref type=\"bibr\" target=\"#b2\">(3,</ref><ref type=\"bibr\" target=\"#b3\">4)</ref> and (1, 2), relative positions can bring performance gain by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (and 86.3% without), as well as 56.1 AP box on COCO object detection using only Cascade Mask R-CNN <ref type=\"bibr\" target=\"#b5\">[6]</ref>. For video classification tasks, MViT achieves unprecedented images. We use standard Mask R-CNN <ref type=\"bibr\" target=\"#b35\">[36]</ref> and Cascade Mask R-CNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> detection frameworks implemented in Detec-tron2 <ref type=\"b ct detection framework: Mask R-CNN <ref type=\"bibr\" target=\"#b35\">[36]</ref> and Cascade Mask R-CNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> in Detectron2 <ref type=\"bibr\" target=\"#b82\">[83]</ref>. We  ct detection with (a) Mask R-CNN<ref type=\"bibr\" target=\"#b35\">[36]</ref> and (b) Cascade Mask R-CNN<ref type=\"bibr\" target=\"#b5\">[6]</ref>. \u2020 indicates that the model is initialized from IN-21K pre-t nd a standard 3\u00d7schedule.In Table5bwe observe a similar trend among backbones for Cascade Mask R-CNN<ref type=\"bibr\" target=\"#b5\">[6]</ref> which lifts Mask R-CNN accuracy (5a). We also ablate the use ><ref type=\"bibr\" target=\"#b68\">69,</ref><ref type=\"bibr\" target=\"#b71\">72]</ref>, object detection <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" targ ss absolute positional embeddings by \u223c0.6% comparing (2) and <ref type=\"bibr\" target=\"#b4\">(5,</ref><ref type=\"bibr\" target=\"#b5\">6)</ref>. Comparing ( <ref type=\"formula\" target=\"#formula_5\">5</ref>). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rameters as in <ref type=\"bibr\">[22,</ref><ref type=\"bibr\" target=\"#b73\">74]</ref>, including mixup <ref type=\"bibr\" target=\"#b88\">[89]</ref>, cutmix <ref type=\"bibr\" target=\"#b87\">[88]</ref>, random  , we perform the same data augmentations across all frames, including random horizontal flip, mixup <ref type=\"bibr\" target=\"#b88\">[89]</ref> and cutmix <ref type=\"bibr\" target=\"#b87\">[88]</ref>, rand. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erative-adversarial learning.</p><p>Lip-sync Discriminator. D sync employs the structure of SyncNet <ref type=\"bibr\" target=\"#b6\">(Chung and Zisserman 2016)</ref> in Wav2Lip <ref type=\"bibr\" target=\"# d=\"fig_3\"><head></head><label></label><figDesc>visual metrics (AVOff and AVConf) proposed in SyncNet<ref type=\"bibr\" target=\"#b6\">(Chung and Zisserman 2016)</ref>. Note that we calculate the normalize. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b19\">Karras et al. 2017;</ref><ref type=\"bibr\" target=\"#b46\">Zhou et al. 2018;</ref><ref type=\"bibr\" target=\"#b7\">Cudeiro et al. 2019)</ref> focus on driving animations of 3D face mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the field of artificial intelligence. As there exists a considerable audio-visual gap, early works <ref type=\"bibr\" target=\"#b9\">(Edwards et al. 2016;</ref><ref type=\"bibr\" target=\"#b27\">Taylor et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b3\">(Chen et al. 2019;</ref><ref type=\"bibr\" target=\"#b24\">Prajwal et al. 2020;</ref><ref type=\"bibr\" target=\"#b45\">Zhou et al. 2020</ref><ref type=\"bibr\" target=\"#b44\">Zhou et al. , 20 \"bibr\" target=\"#b2\">Chen et al. 2020;</ref><ref type=\"bibr\" target=\"#b24\">Prajwal et al. 2020;</ref><ref type=\"bibr\" target=\"#b45\">Zhou et al. 2020;</ref><ref type=\"bibr\" target=\"#b42\">Zhang et al. 20 t methods, including Wav2Lip <ref type=\"bibr\" target=\"#b24\">(Prajwal et al. 2020)</ref>, MakeitTalk <ref type=\"bibr\" target=\"#b45\">(Zhou et al. 2020)</ref>, Audio2Head <ref type=\"bibr\" target=\"#b31\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\">Pham, Cheung, and Pavlovic 2017;</ref><ref type=\"bibr\" target=\"#b19\">Karras et al. 2017;</ref><ref type=\"bibr\" target=\"#b46\">Zhou et al. 2018;</ref><ref type=\"bibr\" target=\"#b7\">Cudeiro et al. 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l. 2020</ref>) and fast short video creation <ref type=\"bibr\" target=\"#b44\">(Zhou et al. 2021;</ref><ref type=\"bibr\" target=\"#b39\">Zeng et al. 2020)</ref>. One-shot talking face generation methods are  With the development of image generation <ref type=\"bibr\" target=\"#b36\">(Yu and Porikli 2016;</ref><ref type=\"bibr\" target=\"#b39\">Yu et al. 2019b;</ref><ref type=\"bibr\" target=\"#b22\">Li, Yu, and Yang. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: their fixed poses and cropped faces in the videos are unnatural for human observations. Other works <ref type=\"bibr\" target=\"#b32\">(Wiles, Koepke, and Zisserman 2018;</ref><ref type=\"bibr\" target=\"#b2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"> \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Transformers <ref type=\"bibr\" target=\"#b46\">(Vaswani et al., 2017)</ref> have revolutionized many areas of AI wit <p>Self Attention. The majority of recent language models are based on the Transformer architecture <ref type=\"bibr\" target=\"#b46\">(Vaswani et al., 2017)</ref>. One of the most important components in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b50\">(Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b15\">Yan et al., 2020;</ref><ref type=\"bibr\" target=\"#b48\">Wang et al., 2020;</ref><ref type=\"bibr\">Khashabi et al., 2020)</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b44\">(Talmor et al., 2019)</ref>.  <ref type=\"bibr\" target=\"#b50\">(Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b8\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Lv et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in et al., 2021)</ref>, SciTail <ref type=\"bibr\" target=\"#b24\">(Khot et al., 2018)</ref>, Com2Sense <ref type=\"bibr\" target=\"#b42\">(Singh et al., 2021)</ref>, AI2 Science Questions <ref type=\"bibr\">(C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the proposed external attention, the accuracy of commonsense reasoning using a DeBERTaxxlarge model <ref type=\"bibr\" target=\"#b19\">(He et al., 2020)</ref> can be significantly boosted from 83.8% to 90  et al., 2019)</ref>, ELECTRA <ref type=\"bibr\" target=\"#b10\">(Clark et al., 2020)</ref> and DeBERTa <ref type=\"bibr\" target=\"#b19\">(He et al., 2020)</ref> as the text encoder, achieving state-of-the-a osen from {1e \u2212 5, 2e \u2212 5, 3e \u2212 6} for all encoders except for DeBERTa; following the DeBERTa paper <ref type=\"bibr\" target=\"#b19\">(He et al., 2020)</ref> we use a smaller learning rate, chosen from {  VAT, we choose \u03b1 \u2208 {0.1, 1.0, 10.0} and set \u03b5 = 1e \u2212 5. For VAT on DeBERTa-xxlarge, we follow SiFT <ref type=\"bibr\" target=\"#b19\">(He et al., 2020)</ref> that normalizes the word vectors before addin  the dev set of CommonsenseQA. Based on these results, we choose ELECTRA-large and DeBERTa variants <ref type=\"bibr\" target=\"#b19\">(He et al., 2020</ref><ref type=\"bibr\" target=\"#b18\">(He et al., , 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6\">(Lan et al., 2019)</ref>, T5 <ref type=\"bibr\" target=\"#b37\">(Raffel et al., 2019)</ref>, ELECTRA <ref type=\"bibr\" target=\"#b10\">(Clark et al., 2020)</ref> and DeBERTa <ref type=\"bibr\" target=\"#b19\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b44\">(Talmor et al., 2019)</ref>.  <ref type=\"bibr\" target=\"#b50\">(Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b8\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Lv et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"formula\">2018</ref>)) can improve the performance for general NLU and question answering tasks <ref type=\"bibr\" target=\"#b22\">(Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">Cheng et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n for future work.</p><p>While autoregressive models have a nonnegligible computation footprint, De <ref type=\"bibr\" target=\"#b10\">Cao et al. (2021a)</ref> show that autoregressive EL can be sped up 7. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: of entities). <ref type=\"foot\" target=\"#foot_0\">1</ref> We employ a sequence-to-sequence BART model <ref type=\"bibr\" target=\"#b34\">(Lewis et al., 2020)</ref>, and exploit a novel bi-level constrained  tructured, unambiguous representation in a sequence-to-sequence formulation. GenIE employs the BART <ref type=\"bibr\" target=\"#b34\">(Lewis et al., 2020)</ref> transformer architecture. It is trained to ned generation. We consider three different starting points: (i) a random initialization; (ii) BART <ref type=\"bibr\" target=\"#b34\">(Lewis et al., 2020)</ref> pretrained language model (PLM); (iii) a p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  The (freely) generated output will not always be a valid entity name, and to solve this problem De <ref type=\"bibr\" target=\"#b11\">Cao et al. (2021b)</ref> propose a constrained decoding strategy that IE employs constrained beam search (BS; <ref type=\"bibr\" target=\"#b49\">Sutskever et al., 2014;</ref><ref type=\"bibr\" target=\"#b11\">De Cao et al., 2021b)</ref> to resolve both of these problems. Instea f> pretrained language model (PLM); (iii) a pre-trained autoregressive entity retrieval model GENRE <ref type=\"bibr\" target=\"#b11\">(De Cao et al., 2021b)</ref>. The pre-trained models are better in te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nal log-likelihood with teacher forcing <ref type=\"bibr\" target=\"#b48\">(Sutskever et al., 2011</ref><ref type=\"bibr\" target=\"#b49\">(Sutskever et al., , 2014))</ref>, using the cross-entropy loss. We u trary generation from a sequenceto-sequence model.</p><p>GenIE employs constrained beam search (BS; <ref type=\"bibr\" target=\"#b49\">Sutskever et al., 2014;</ref><ref type=\"bibr\" target=\"#b11\">De Cao et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"bibr\" target=\"#b26\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b57\">Wu et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Le and Titov, 2018;</ref><ref type=\"bibr\" target=\"#b31\">Kolitsas et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ani et al., 2017</ref>) that encodes the input followed by a non-autoregressive transformer decoder <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2018)</ref>. The decoder generates embeddings that are us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> filtered and assigned relations to sentences using paraphrase detection from different sources <ref type=\"bibr\" target=\"#b41\">(Nakashole et al., 2012;</ref><ref type=\"bibr\" target=\"#b18\">Ganitkev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hat are expressed between the entities <ref type=\"bibr\" target=\"#b16\">(Gal\u00e1rraga et al., 2014;</ref><ref type=\"bibr\" target=\"#b2\">Angeli et al., 2015b;</ref><ref type=\"bibr\" target=\"#b7\">Chaganty et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oposed to address the error propagation <ref type=\"bibr\" target=\"#b54\">(Trisedya et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Sui et al., 2021;</ref><ref type=\"bibr\" target=\"#b37\">Liu et al., 201 /1.0\"><head n=\"4.3\">Baselines</head><p>We compare GenIE against Set Generation Networks (SetGenNet; <ref type=\"bibr\" target=\"#b46\">Sui et al., 2021)</ref> which is, to the best of our knowledge, the s l and there is no other model from the literature trained and evaluated on REBEL for cIE. SetGenNet <ref type=\"bibr\" target=\"#b46\">(Sui et al., 2021)</ref> is an end-to-end state-of-the-art model for  ks do not have available or directly usable code. In particular, we first concentrated on SetGenNet <ref type=\"bibr\" target=\"#b46\">(Sui et al., 2021)</ref> as, to the best of our knowledge, it is the   the n most frequent relations to mimic the strategies used by previous works to reduce the schemas <ref type=\"bibr\" target=\"#b46\">(Sui et al., 2021)</ref>. We first observe that GenIE is always large. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce problem with great success <ref type=\"bibr\" target=\"#b27\">(Huguet Cabot and Navigli, 2021;</ref><ref type=\"bibr\" target=\"#b14\">Dognin et al., 2021)</ref>. Indeed, such autoregressive formulations  he limitations mentioned above <ref type=\"bibr\" target=\"#b27\">(Huguet Cabot and Navigli, 2021;</ref><ref type=\"bibr\" target=\"#b14\">Dognin et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Nayak and Ng in et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Nayak and Ng, 2020)</ref>. For instance, ReGen <ref type=\"bibr\" target=\"#b14\">(Dognin et al., 2021)</ref> significantly improves upon published res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce problem with great success <ref type=\"bibr\" target=\"#b27\">(Huguet Cabot and Navigli, 2021;</ref><ref type=\"bibr\" target=\"#b14\">Dognin et al., 2021)</ref>. Indeed, such autoregressive formulations  he limitations mentioned above <ref type=\"bibr\" target=\"#b27\">(Huguet Cabot and Navigli, 2021;</ref><ref type=\"bibr\" target=\"#b14\">Dognin et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Nayak and Ng in et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Nayak and Ng, 2020)</ref>. For instance, ReGen <ref type=\"bibr\" target=\"#b14\">(Dognin et al., 2021)</ref> significantly improves upon published res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce problem with great success <ref type=\"bibr\" target=\"#b27\">(Huguet Cabot and Navigli, 2021;</ref><ref type=\"bibr\" target=\"#b14\">Dognin et al., 2021)</ref>. Indeed, such autoregressive formulations  he limitations mentioned above <ref type=\"bibr\" target=\"#b27\">(Huguet Cabot and Navigli, 2021;</ref><ref type=\"bibr\" target=\"#b14\">Dognin et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Nayak and Ng in et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Nayak and Ng, 2020)</ref>. For instance, ReGen <ref type=\"bibr\" target=\"#b14\">(Dognin et al., 2021)</ref> significantly improves upon published res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mantic information from unstructured texts is crucial for many AI tasks such as knowledge discovery <ref type=\"bibr\" target=\"#b28\">(Ji and Grishman, 2011;</ref><ref type=\"bibr\" target=\"#b54\">Trisedya  terface between free text and structured knowledge is formalized by knowledge base population (KBP; <ref type=\"bibr\" target=\"#b28\">Ji and Grishman, 2011)</ref>, which proposes to represent the informa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  have been extensively studied in the past <ref type=\"bibr\" target=\"#b26\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b57\">Wu et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Le and Titov, 20 s the dot product between the dense vector encodings of the input and the entity's meta information <ref type=\"bibr\" target=\"#b57\">(Wu et al., 2020)</ref>. This general approach has led to large perfo s the generated prefix trie occupies \u2248200MB of storage (e.g., the entity linking system proposed by <ref type=\"bibr\" target=\"#b57\">Wu et al. 2020</ref> needs &gt;20 times more storage).</p></div> <div. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hat are expressed between the entities <ref type=\"bibr\" target=\"#b16\">(Gal\u00e1rraga et al., 2014;</ref><ref type=\"bibr\" target=\"#b2\">Angeli et al., 2015b;</ref><ref type=\"bibr\" target=\"#b7\">Chaganty et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ditionally, we use Geo-NRE <ref type=\"bibr\" target=\"#b54\">(Trisedya et al., 2019)</ref>, and FewRel <ref type=\"bibr\" target=\"#b24\">(Han et al., 2018)</ref> for testing purposes only. Appendix B contai  small size and to compare with the literature, we used this dataset only for testing.</p><p>FewRel <ref type=\"bibr\" target=\"#b24\">(Han et al., 2018)</ref> is also extracted from Wikipedia where Wikid. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , 2013)</ref>. The advent of transformers <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Lan et al., 2020;</ref><ref type=\"bibr\" target=\"#b36\">Liu et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b27\">(Huguet Cabot and Navigli, 2021;</ref><ref type=\"bibr\" target=\"#b14\">Dognin et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Nayak and Ng, 2020)</ref>. For instance, ReGen <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e, pipeline methods are plagued by error propagation, which significantly affects their performance <ref type=\"bibr\" target=\"#b38\">(Mesquita et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Trisedya  e element in the entity catalog. However, for many useful relations one of the objects is a literal <ref type=\"bibr\" target=\"#b38\">(Mesquita et al., 2019)</ref>, e.g., date of birth, length, size, num. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hat are expressed between the entities <ref type=\"bibr\" target=\"#b16\">(Gal\u00e1rraga et al., 2014;</ref><ref type=\"bibr\" target=\"#b2\">Angeli et al., 2015b;</ref><ref type=\"bibr\" target=\"#b7\">Chaganty et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tity linking <ref type=\"bibr\" target=\"#b40\">(Milne and Witten, 2008)</ref>, and relation extraction <ref type=\"bibr\" target=\"#b39\">(Miller et al., 1998)</ref>. Entity linking and relation extraction s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odels were already trained, and we use them for inference only. For RC and TC, we trained a RoBERTa <ref type=\"bibr\" target=\"#b36\">(Liu et al., 2019)</ref> model with a linear classification layer on  components were not trained. The relation classification module is a linear layer on top of RoBERTa <ref type=\"bibr\" target=\"#b36\">(Liu et al., 2019)</ref>. We trained it learning rate 3e-4 using the   target=\"#tab_4\">4</ref>. The triple classification module is also a linear layer on top of RoBERTa <ref type=\"bibr\" target=\"#b36\">(Liu et al., 2019)</ref> with the same hyperparameters of the relatio ibr\" target=\"#b13\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Lan et al., 2020;</ref><ref type=\"bibr\" target=\"#b36\">Liu et al., 2019)</ref> and pipeline architectures that allow for inf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b57\">Wu et al., 2020;</ref><ref type=\"bibr\" target=\"#b33\">Le and Titov, 2018;</ref><ref type=\"bibr\" target=\"#b31\">Kolitsas et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Arora et al nNet further uses candidate selection <ref type=\"bibr\" target=\"#b17\">(Ganea and Hofmann, 2017;</ref><ref type=\"bibr\" target=\"#b31\">Kolitsas et al., 2018)</ref> to reduce the output space and a biparti n NER and an ED system AIDA <ref type=\"bibr\" target=\"#b25\">(Hoffart et al., 2011)</ref> or NeuralEL <ref type=\"bibr\" target=\"#b31\">(Kolitsas et al., 2018)</ref> and then a relation extraction system C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> filtered and assigned relations to sentences using paraphrase detection from different sources <ref type=\"bibr\" target=\"#b41\">(Nakashole et al., 2012;</ref><ref type=\"bibr\" target=\"#b18\">Ganitkev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b36\">(Liu et al., 2019)</ref>. We trained it learning rate 3e-4 using the Adam optimizer <ref type=\"bibr\" target=\"#b30\">(Kingma and Ba, 2015)</ref>. We trained for a maximum number of steps. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b36\">(Liu et al., 2019)</ref>. We trained it learning rate 3e-4 using the Adam optimizer <ref type=\"bibr\" target=\"#b30\">(Kingma and Ba, 2015)</ref>. We trained for a maximum number of steps. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b45\">(Srivastava et al., 2014)</ref> and label smoothing for regularization <ref type=\"bibr\" target=\"#b50\">(Szegedy et al., 2016)</ref>.</p></div> <div xmlns=\"http://www.tei-c.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t did not release code for pre-training them. The closest solution we found was using Wikipedia2Vec <ref type=\"bibr\" target=\"#b58\">(Yamada et al., 2020)</ref>, which does not include relation embeddin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ressive formulations can exploit the language knowledge already encoded in pre-trained transformers <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref>. For example, some tokens can be more eas 015a;</ref><ref type=\"bibr\" target=\"#b9\">Corro and Gemulla, 2013)</ref>. The advent of transformers <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Lan et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s an end-to-end state-of-the-art model for triplet extraction. It consists of a transformer encoder <ref type=\"bibr\" target=\"#b55\">(Vaswani et al., 2017</ref>) that encodes the input followed by a non. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en the two entities and the relation type <ref type=\"bibr\" target=\"#b1\">(Angeli et al., 2015a;</ref><ref type=\"bibr\" target=\"#b9\">Corro and Gemulla, 2013)</ref>. The advent of transformers <ref type=\"  et al., 2016)</ref>, MiniE <ref type=\"bibr\">(Gashteovski et al., 2017), or ClausIE (Corro and</ref><ref type=\"bibr\" target=\"#b9\">Gemulla, 2013)</ref>. Best results are highlighted in bold and second . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ring: they (i) extracted sentences that contain implicit entity names using co-reference resolution <ref type=\"bibr\" target=\"#b8\">(Clark and Manning, 2016)</ref> filtered and assigned relations to sen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b36\">(Liu et al., 2019)</ref>. We trained it learning rate 3e-4 using the Adam optimizer <ref type=\"bibr\" target=\"#b30\">(Kingma and Ba, 2015)</ref>. We trained for a maximum number of steps. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ring: they (i) extracted sentences that contain implicit entity names using co-reference resolution <ref type=\"bibr\" target=\"#b8\">(Clark and Manning, 2016)</ref> filtered and assigned relations to sen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hat are expressed between the entities <ref type=\"bibr\" target=\"#b16\">(Gal\u00e1rraga et al., 2014;</ref><ref type=\"bibr\" target=\"#b2\">Angeli et al., 2015b;</ref><ref type=\"bibr\" target=\"#b7\">Chaganty et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b33\">Le and Titov, 2018;</ref><ref type=\"bibr\" target=\"#b31\">Kolitsas et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Arora et al., 2021)</ref>. Most existing approaches associate entities. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b36\">(Liu et al., 2019)</ref>. We trained it learning rate 3e-4 using the Adam optimizer <ref type=\"bibr\" target=\"#b30\">(Kingma and Ba, 2015)</ref>. We trained for a maximum number of steps. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e, pipeline methods are plagued by error propagation, which significantly affects their performance <ref type=\"bibr\" target=\"#b38\">(Mesquita et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Trisedya  e element in the entity catalog. However, for many useful relations one of the objects is a literal <ref type=\"bibr\" target=\"#b38\">(Mesquita et al., 2019)</ref>, e.g., date of birth, length, size, num. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ani et al., 2017</ref>) that encodes the input followed by a non-autoregressive transformer decoder <ref type=\"bibr\" target=\"#b22\">(Gu et al., 2018)</ref>. The decoder generates embeddings that are us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b49\">(Sutskever et al., , 2014))</ref>, using the cross-entropy loss. We use dropout <ref type=\"bibr\" target=\"#b45\">(Srivastava et al., 2014)</ref> and label smoothing for regularizatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ditionally, we use Geo-NRE <ref type=\"bibr\" target=\"#b54\">(Trisedya et al., 2019)</ref>, and FewRel <ref type=\"bibr\" target=\"#b24\">(Han et al., 2018)</ref> for testing purposes only. Appendix B contai  small size and to compare with the literature, we used this dataset only for testing.</p><p>FewRel <ref type=\"bibr\" target=\"#b24\">(Han et al., 2018)</ref> is also extracted from Wikipedia where Wikid. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eddings that are used to predict entities and relations. SetGenNet further uses candidate selection <ref type=\"bibr\" target=\"#b17\">(Ganea and Hofmann, 2017;</ref><ref type=\"bibr\" target=\"#b31\">Kolitsa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ny AI tasks such as knowledge discovery <ref type=\"bibr\" target=\"#b28\">(Ji and Grishman, 2011;</ref><ref type=\"bibr\" target=\"#b54\">Trisedya et al., 2019)</ref>, knowledge maintenance <ref type=\"bibr\"  significantly affects their performance <ref type=\"bibr\" target=\"#b38\">(Mesquita et al., 2019;</ref><ref type=\"bibr\" target=\"#b54\">Trisedya et al., 2019)</ref>.</p><p>End-to-end systems that jointly p d the disambiguation of entities and rela-tions have been proposed to address the error propagation <ref type=\"bibr\" target=\"#b54\">(Trisedya et al., 2019;</ref><ref type=\"bibr\" target=\"#b46\">Sui et al avigli, 2021)</ref> and Wiki-NRE for training, validation and testing. Additionally, we use Geo-NRE <ref type=\"bibr\" target=\"#b54\">(Trisedya et al., 2019)</ref>, and FewRel <ref type=\"bibr\" target=\"#b >Grycner and Weikum, 2016)</ref>. We used this dataset for both training and testing.</p><p>Geo-NRE <ref type=\"bibr\" target=\"#b54\">(Trisedya et al., 2019)</ref> is constructed in the same way as Wiki- ferent prediction orderings (i.e., it generates a set). Note that there are weaker baselines (e.g., <ref type=\"bibr\" target=\"#b54\">Trisedya et al. 2019)</ref> we could have used to compare on REBEL, b g different environments pertaining to subsets of the top-n most frequent relations.</p><p>Wiki-NRE <ref type=\"bibr\" target=\"#b54\">(Trisedya et al., 2019</ref>) is a dataset created from Wikipedia. Au REBEL.</p><p>We then focused on the work most similar to Set-GenNet, that is the system proposed by <ref type=\"bibr\" target=\"#b54\">Trisedya et al. (2019)</ref>. They released code and we were able to  ipedia2Vec website<ref type=\"foot\" target=\"#foot_6\">7</ref> do not match the dimensionality used by <ref type=\"bibr\" target=\"#b54\">Trisedya et al. (2019)</ref>. Finally, the authors did not include co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b41\">(Nakashole et al., 2012;</ref><ref type=\"bibr\" target=\"#b18\">Ganitkevitch et al., 2013;</ref><ref type=\"bibr\" target=\"#b21\">Grycner and Weikum, 2016)</ref>. We used this dataset for both traini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ently <ref type=\"bibr\" target=\"#b44\">(Ye and Bors 2020a)</ref>. This issue is relieved by using ENA <ref type=\"bibr\" target=\"#b22\">(Lee et al. 2020)</ref>, inspired by a network expansion mechanism <r  = {ks 1 , . . . , ks K }. Similar log-likelihood evaluations were used for selecting components in <ref type=\"bibr\" target=\"#b22\">(Lee et al. 2020;</ref><ref type=\"bibr\" target=\"#b34\">Rao et al. 2019 the number of weighted samples. We also compare with LIMix (Ye and Bors 2021e) and implement CN-DPM <ref type=\"bibr\" target=\"#b22\">(Lee et al. 2020</ref>) with the optimal setting, namely CN-DPM* (See. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  previous tasks. One direct way to enable VAEs for LLL is by using Generative Replay (GR) processes <ref type=\"bibr\" target=\"#b33\">(Ramapuram, Gregorova, and Kalousis 2020)</ref>.</p><p>Let us conside  used in VAEs for the first time in <ref type=\"bibr\" target=\"#b0\">(Achille et al. 2018)</ref> while <ref type=\"bibr\" target=\"#b33\">(Ramapuram, Gregorova, and Kalousis 2020)</ref> extends the GR mechan Generative Adversarial Nets (GANs) or VAEs <ref type=\"bibr\" target=\"#b0\">(Achille et al. 2018;</ref><ref type=\"bibr\" target=\"#b33\">Ramapuram, Gregorova, and Kalousis 2020;</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ref>. The reconstruction error ELBO is normalized by dividing with the image size (28 \u00d7 28), as in <ref type=\"bibr\" target=\"#b32\">(Park, Kim, and Kim 2019)</ref>. We evaluate the risk and the discrep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d Pavlovic 2020;</ref><ref type=\"bibr\" target=\"#b25\">Maal\u00f8e et al. 2016)</ref>, importance sampling <ref type=\"bibr\" target=\"#b3\">(Burda, Grosse, and Salakhutdinov 2015;</ref><ref type=\"bibr\" target=\" oving VAE's performance and one possible way is to use the Importance Weighted Autoencoder (IWELBO) <ref type=\"bibr\" target=\"#b3\">(Burda, Grosse, and Salakhutdinov 2015)</ref> in which the tightness i  target=\"#b39\">(Sobolev and Vetrov 2019)</ref> (See details in Appendix-F from SM 1 ). IWELBO bound <ref type=\"bibr\" target=\"#b3\">(Burda, Grosse, and Salakhutdinov 2015)</ref> is an extension of ELBO  >Fashion (Xiao, Rasul, and</ref><ref type=\"bibr\" target=\"#b41\">Vollgraf 2017)</ref>. Following from <ref type=\"bibr\" target=\"#b3\">(Burda, Grosse, and Salakhutdinov 2015)</ref> we divide MNIST and Fash ELBO bounds as DEGM-ELBO and DEGM-IWELBO-K \u2032 , respectively. We adapt the network architecture from <ref type=\"bibr\" target=\"#b3\">(Burda, Grosse, and Salakhutdinov 2015)</ref> and consider several bas x-L.1 from SM 1 ).</p><p>Results. The testing data log-likelihood is estimated by the IWELBO bounds <ref type=\"bibr\" target=\"#b3\">(Burda, Grosse, and Salakhutdinov 2015)</ref> with K \u2032 = 5000. We perf enerator that approximates the target distributions well, for example by using the Autoencoding VAE <ref type=\"bibr\" target=\"#b3\">(Cemgil et al. 2020)</ref> or adversarial learning <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref>. The IWELBO bound can be used with any of these approaches for further performance improvement <ref type=\"bibr\" target=\"#b39\">(Sobolev and Vetrov 2019)</ref>. Moreover, online variational inferen ls <ref type=\"bibr\" target=\"#b25\">(Maal\u00f8e et al. 2016)</ref> and hierarchical variational inference <ref type=\"bibr\" target=\"#b39\">(Sobolev and Vetrov 2019)</ref> (See details in Appendix-F from SM 1  learning several entirely different domains due to the fixed model's capacity and the mode collapse <ref type=\"bibr\" target=\"#b39\">(Srivastava et al. 2017</ref>). In the following section, we show how. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b39\">(Sobolev and Vetrov 2019)</ref>. Moreover, online variational inference <ref type=\"bibr\" target=\"#b29\">(Nguyen et al. 2017</ref>) has been used in 1 https://github.com/dtuz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\">MNIST (LeCun et al. 1998) and</ref><ref type=\"bibr\">Fashion (Xiao, Rasul, and</ref><ref type=\"bibr\" target=\"#b41\">Vollgraf 2017)</ref>. Following from <ref type=\"bibr\" target=\"#b3\">(B 2\">(Yang et al. 2015)</ref>, Zappos <ref type=\"bibr\" target=\"#b53\">(Yu and Grauman 2017)</ref>, CUB <ref type=\"bibr\" target=\"#b41\">(Wah et al. 2010</ref>) (detailed dataset setting is provided in Appe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ocus on regularization based methods <ref type=\"bibr\" target=\"#b14\">(Jung, Jung, and Kim 2016;</ref><ref type=\"bibr\" target=\"#b23\">Li and Hoiem 2017)</ref>, which typically penalize significant change. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: A <ref type=\"bibr\" target=\"#b22\">(Lee et al. 2020)</ref>, inspired by a network expansion mechanism <ref type=\"bibr\" target=\"#b34\">(Rao et al. 2019)</ref>, or by employing a combination between ENA an uations were used for selecting components in <ref type=\"bibr\" target=\"#b22\">(Lee et al. 2020;</ref><ref type=\"bibr\" target=\"#b34\">Rao et al. 2019)</ref>. However, in our approach we develop a graph-b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\">MNIST (LeCun et al. 1998) and</ref><ref type=\"bibr\">Fashion (Xiao, Rasul, and</ref><ref type=\"bibr\" target=\"#b41\">Vollgraf 2017)</ref>. Following from <ref type=\"bibr\" target=\"#b3\">(B 2\">(Yang et al. 2015)</ref>, Zappos <ref type=\"bibr\" target=\"#b53\">(Yu and Grauman 2017)</ref>, CUB <ref type=\"bibr\" target=\"#b41\">(Wah et al. 2010</ref>) (detailed dataset setting is provided in Appe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: L was considered in <ref type=\"bibr\" target=\"#b1\">(Aljundi, Kelchtermans, and Tuytelaars 2019;</ref><ref type=\"bibr\" target=\"#b15\">Jung et al. 2020</ref>). However, DEGM is the first model where this . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce of data domain due to the fixed model capacity while having to retrain the generator frequently <ref type=\"bibr\" target=\"#b44\">(Ye and Bors 2020a)</ref>. This issue is relieved by using ENA <ref t p>\u2022 In the GR process, P t\u22121 is gradually degenerated as t increases due to the repeated retraining <ref type=\"bibr\" target=\"#b44\">(Ye and Bors 2020a)</ref>, which leads to a large discrepancy distanc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>. The reconstruction error ELBO is normalized by dividing with the image size (28 \u00d7 28), as in <ref type=\"bibr\" target=\"#b32\">(Park, Kim, and Kim 2019)</ref>. We evaluate the risk and the discrep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fining a tighter ELBO to the marginal log-likelihood, achieved by using a more expressive posterior <ref type=\"bibr\" target=\"#b16\">(Kim and Pavlovic 2020;</ref><ref type=\"bibr\" target=\"#b25\">Maal\u00f8e et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amapuram, Gregorova, and Kalousis 2020;</ref><ref type=\"bibr\" target=\"#b52\">Ye and Bors 2021g;</ref><ref type=\"bibr\" target=\"#b38\">Shin et al. 2017;</ref><ref type=\"bibr\">Ye and</ref><ref type=\"bibr\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: A <ref type=\"bibr\" target=\"#b22\">(Lee et al. 2020)</ref>, inspired by a network expansion mechanism <ref type=\"bibr\" target=\"#b34\">(Rao et al. 2019)</ref>, or by employing a combination between ENA an uations were used for selecting components in <ref type=\"bibr\" target=\"#b22\">(Lee et al. 2020;</ref><ref type=\"bibr\" target=\"#b34\">Rao et al. 2019)</ref>. However, in our approach we develop a graph-b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dict its performance on a testing data set by achieving a certain error rate on a training data set <ref type=\"bibr\" target=\"#b19\">(Kuroki et al. 2019)</ref>. In this paper, we develop a new theoretic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b3\">(Burda, Grosse, and Salakhutdinov 2015)</ref> we divide MNIST and Fashion into five tasks <ref type=\"bibr\" target=\"#b54\">(Zenke, Poole, and Ganguli 2017)</ref>, called Split MNIST (S-M) and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed. Other approaches would consider as approximate posterior distributions either normalizing flows <ref type=\"bibr\" target=\"#b17\">(Kingma et al. 2016;</ref><ref type=\"bibr\" target=\"#b35\">Rezende and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b27\">(Wu et al. 2020;</ref><ref type=\"bibr\" target=\"#b30\">Yu et al. 2020;</ref><ref type=\"bibr\" target=\"#b29\">Yang et al. 2021;</ref><ref type=\"bibr\" target=\"#b31\">Yu et al. 2021) r\" target=\"#b31\">(Yu et al. , 2021) )</ref> was proposed for the subgraph recognition problem. HGIB <ref type=\"bibr\" target=\"#b29\">(Yang et al. 2021)</ref> was proposed to implement the consensus hypo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in Section 3.1.</p><p>Proof. We prove the Lemma 1 following the same strategy of Proposition 3.1 in <ref type=\"bibr\" target=\"#b0\">(Achille and Soatto 2018)</ref>. Suppose G is defined by Y and G n , a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r GNNs, many graph structure learning methods <ref type=\"bibr\" target=\"#b36\">(Zhu et al. 2021;</ref><ref type=\"bibr\" target=\"#b7\">Franceschi et al. 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen, Wu, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: presentation is naturally more robust to data noise. IB has been applied to representation learning <ref type=\"bibr\" target=\"#b12\">(Kim et al. 2021;</ref><ref type=\"bibr\">Jeon et al. 2021;</ref><ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ver, most of graphs in the real-word are noisy or incomplete due to the error-prone data collection <ref type=\"bibr\" target=\"#b4\">(Chen, Wu, and Zaki 2020)</ref>, which could even exacerbate the quali #b33\">(Zheng et al. 2020)</ref>, SIB <ref type=\"bibr\" target=\"#b30\">(Yu et al. 2020)</ref> and IDGL <ref type=\"bibr\" target=\"#b4\">(Chen, Wu, and Zaki 2020)</ref>, to demonstrate the effectiveness and  br\" target=\"#b36\">(Zhu et al. 2021;</ref><ref type=\"bibr\" target=\"#b7\">Franceschi et al. 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen, Wu, and Zaki 2020)</ref> are proposed, most of which optimize th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ention in machine learning and deep learning <ref type=\"bibr\" target=\"#b1\">(Alemi et al. 2016;</ref><ref type=\"bibr\" target=\"#b17\">Saxe et al. 2019)</ref>. As for irregular graph data, there are some . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed data. Moreover, SIB directly estimates the mutual information between subgraph and graph by MINE <ref type=\"bibr\" target=\"#b3\">(Belghazi et al. 2018</ref>) and uses a bi-level optimization scheme f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ipf and Welling 2016)</ref>, GAT <ref type=\"bibr\" target=\"#b26\">(Veli\u010dkovi\u0107 et al. 2017)</ref>, GIN <ref type=\"bibr\" target=\"#b28\">(Xu et al. 2019)</ref> to see whether the VIB-GSL can boost the perfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: earning.</p><p>To adaptively learn graph structures for GNNs, many graph structure learning methods <ref type=\"bibr\" target=\"#b36\">(Zhu et al. 2021;</ref><ref type=\"bibr\" target=\"#b7\">Franceschi et al tp://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Graph Structure Learning</head><p>Graph structure learning <ref type=\"bibr\" target=\"#b36\">(Zhu et al. 2021)</ref> targets jointly learning an optimized graph s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f interest in graph representation learning <ref type=\"bibr\" target=\"#b32\">(Zhang et al. 2018;</ref><ref type=\"bibr\" target=\"#b25\">Tong et al. 2021)</ref>, especially in efforts devoted to developing . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sive data movements and memory accesses, inducing high energy consumption and long latency overhead <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Processing-in-memory (PIM) is regarded as an effectiv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mory technologies, including the traditional CMOS technology <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> and emerging non-volatile memory (NVM) devices <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref>. Among all these technologies, resistive random access memor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mory technologies, including the traditional CMOS technology <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> and emerging non-volatile memory (NVM) devices <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  consumption. Alternatively, the rate-coding based designs <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref> encode data into  e rate-coding scheme encodes data into the spike frequency <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref>. The circuit is h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref>. Among all these technologies, resistive random access memor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> and emerging non-volatile memory (NVM) devices <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igated, which follows the normal distribution according to <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. In the experiment, we adopt the process variations with th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tions (PVs) of ReRAM cells is also investigated, which follows the normal distribution according to <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. In the experiment. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tting the resistance range of ReRAM cells from 50k\u03a9 to 1M\u03a9 <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, though the slight non-linearity still exists. More analysi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b3\">4]</ref> and emerging non-volatile memory (NVM) devices <ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ns/1.0\"><head>Related Work</head><p>The idea of permutation-based AR modeling has been explored in <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, but there are sev s the advantages of both while avoiding their weaknesses.</p><p>Borrowing ideas from orderless NADE <ref type=\"bibr\" target=\"#b31\">[32]</ref>, we propose the permutation language modeling objective th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: idea of permutation-based AR modeling has been explored in <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, but there are several key differences. Firstly, previous m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ementation</head><p>Following BERT <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we use the BooksCorpus <ref type=\"bibr\" target=\"#b39\">[40]</ref> and English Wikipedia as part of our pretraining data, whi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: idea of permutation-based AR modeling has been explored in <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, but there are several key differences. Firstly, previous m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ementation</head><p>Following BERT <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we use the BooksCorpus <ref type=\"bibr\" target=\"#b39\">[40]</ref> and English Wikipedia as part of our pretraining data, whi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 26]</ref>, ClueWeb 2012-B (extended from <ref type=\"bibr\" target=\"#b4\">[5]</ref>), and Common Crawl <ref type=\"bibr\" target=\"#b5\">[6]</ref> for pretraining. We use heuristics to aggressively filter ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: both z t and x zt ).</p><p>where Q, K, V denote the query, key, and value in an attention operation <ref type=\"bibr\" target=\"#b32\">[33]</ref>. The update rule of the content representations is exactly. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: idea of permutation-based AR modeling has been explored in <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, but there are several key differences. Firstly, previous m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  learning, so far almost all methods for learned simulation have focused on explicit forward models <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">P arge graphs of 1000s of nodes and support generalization to systems with different shapes and sizes <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">P ATH, we use normalized acceleration of the particle as the update Y to better match the approach in <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">P mputation. Note that excluding the fixed particles from the predicted output is a standard practice <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">P  nodes and edges of the graph using MLP encoders. Then, we process the graph using a GNN model from <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">P scent solver.</p><p>Forward GNN For the Forward GNN, we use the Graph Network Simulator (GNS) model <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">P nate the proposed update Y (i) to each node vector. We use a GNS model with a per-node decoder from <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">P bibr\" target=\"#b29\">(Mrowca et al., 2018;</ref><ref type=\"bibr\" target=\"#b21\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Sanchez-Gonzalez et al., 2020)</ref> and meshbased continuum systems  me manner, and with the same magnitude, as in <ref type=\"bibr\" target=\"#b21\">(Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Sanchez-Gonzalez et al., 2020)</ref>.</p><p>Fixed particles Some of t tion) to the node features.</p><p>We implement the function f C as a graph network (GNN) similar to <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020)</ref>. We encode nodes and edges of t  norm of the relative distances (not just the vector itself) as an additional edge feature to match <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020)</ref>.</p><p>Note that we do not prov >Box Bath For GNN-based models, we used 1 message-passing step. All other hyperparameters are as in <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020)</ref>. The GNNs' node and edge functi rent number of time points. We use five most recent velocities for BOXBATH to match the paradigm in <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020)</ref>. For the constraint-based model of the node to each of wall as additional node features, treating the wall as a plain, similarly to <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020)</ref>.  Constraint-based graph networ BOXBATH we found it was important to normalize inputs and targets to zero-mean unit-variance (as in <ref type=\"bibr\" target=\"#b36\">Sanchez-Gonzalez et al. (2020)</ref>). In the other datasets, the sca  a learning rate starting at 0.001 and decaying continuously at a rate of 0.1 every 1M steps, as in <ref type=\"bibr\" target=\"#b36\">Sanchez-Gonzalez et al. (2020)</ref>.</p><p>A.5. Limitations of \"Neur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  computational efficiency, and generalization <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b19\">Karniadakis et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">Chen et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th C-GNS-GD and Iterative GNN in Section 5.4.</p><p>Fast Projections Fast Projection (FP) algorithm <ref type=\"bibr\" target=\"#b17\">(Goldenthal et al., 2007)</ref> is a zero-finding algorithm, for cons erfitting.</p><p>Zero-finding Fast Projections algorithm NP uses the Fast Projection (FP) algorithm <ref type=\"bibr\" target=\"#b17\">(Goldenthal et al., 2007)</ref> to find zero points in its constraint. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: used on explicit forward models <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Pfaff et al., 2021)</ref>, with few exceptions <ref type=\"bibr\" targe to better match the approach in <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Pfaff et al., 2021)</ref>. The acceleration is estimated as a backwar d output is a standard practice <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Pfaff et al., 2021)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org he graph using a GNN model from <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Pfaff et al., 2021)</ref>. The GNN model has residual connections on  h Network Simulator (GNS) model <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Pfaff et al., 2021)</ref>.</p><p>The PREDICTOR takes only the context el with a per-node decoder from <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Pfaff et al., 2021)</ref> to process the input graph and output \u03b4Y fo <ref type=\"bibr\" target=\"#b36\">Sanchez-Gonzalez et al., 2020)</ref> and meshbased continuum systems <ref type=\"bibr\" target=\"#b32\">(Pfaff et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">De Avila Bel with different shapes and sizes <ref type=\"bibr\" target=\"#b36\">(Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Pfaff et al., 2021;</ref><ref type=\"bibr\" target=\"#b6\">Battaglia et a ared to Neural Projections <ref type=\"bibr\" target=\"#b45\">(Yang et al., 2020)</ref> and Forward GNN <ref type=\"bibr\" target=\"#b32\">(Pfaff et al., 2021)</ref> with a comparable number of parameters<ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graphics, a popular class of simulators <ref type=\"bibr\" target=\"#b40\">(Todorov et al., 2012;</ref><ref type=\"bibr\" target=\"#b28\">Monaghan, 2005;</ref><ref type=\"bibr\" target=\"#b27\">Mirtich &amp; Can. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graphics, a popular class of simulators <ref type=\"bibr\" target=\"#b40\">(Todorov et al., 2012;</ref><ref type=\"bibr\" target=\"#b28\">Monaghan, 2005;</ref><ref type=\"bibr\" target=\"#b27\">Mirtich &amp; Can. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nts and can simulate a wide range of physical systems such as rigids, soft-bodies, fluids and cloth <ref type=\"bibr\" target=\"#b25\">(Macklin et al., 2014a;</ref><ref type=\"bibr\" target=\"#b38\">Thomaszew. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mize power flow <ref type=\"bibr\" target=\"#b14\">(Donti et al., 2021)</ref>, support robotic planning <ref type=\"bibr\" target=\"#b23\">(Loula et al., 2020)</ref>, and perform combinatorial optimization <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kov et al., 2021)</ref> and aerodynamics <ref type=\"bibr\" target=\"#b39\">(Thuerey et al., 2020;</ref><ref type=\"bibr\" target=\"#b22\">Zhang et al., 2018)</ref>.</p><p>Particularly, a learned simulator ba solvers or alternative ways to compute the gradient of the solution (e.g., implicit differentiation <ref type=\"bibr\" target=\"#b22\">(Liao et al., 2018)</ref>, as used in DEQs).</p><p>One area where con. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Rubanova et al., 2019)</ref>. Imposing Hamiltonian <ref type=\"bibr\" target=\"#b18\">(Greydanus et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Sanchez-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aths and then combine the graphlets in each subgraph. There are plenty of implements and algorithms <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: each subgraph. There are plenty of implements and algorithms <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> that can efficientl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: der-smooth.</p><p>Unlike typical graph models, which only focus on pair-wise relations, hypergraphs <ref type=\"bibr\" target=\"#b30\">[31]</ref> can capture and preserve more diverse, complicated, and hi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: specific contents. Many variants such as metapath2vec <ref type=\"bibr\" target=\"#b8\">[9]</ref>, HHNE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and HeteSpaceyWalk <ref type=\"bibr\" target=\"#b16\">[17]</r roximity in the path. Following this idea, some extensions <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> have been proposed recently so that the model can deal with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1]</ref>, personality perception <ref type=\"bibr\" target=\"#b48\">[49]</ref>, and recommender systems <ref type=\"bibr\" target=\"#b49\">[50]</ref> because of its capacity for modelling high-level relations. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, location-based social networks <ref type=\"bibr\" target=\"#b40\">[41]</ref>, personality perception <ref type=\"bibr\" target=\"#b48\">[49]</ref>, and recommender systems <ref type=\"bibr\" target=\"#b49\">[5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erent semantic spaces, and then tighten each pair of nodes on pair-wise links. Similarly, Lu et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> present RHINE, and they utilise different models to exploi 429 pair-wise links in this dataset, which stand for the citation relations of papers.</p><p>\u2022 DBLP <ref type=\"bibr\" target=\"#b43\">[44]</ref>: It is an academic network that includes four types of nod omprises two types of relations: 9,744 paper-author links and 3025 papersubject links.</p><p>\u2022 Yelp <ref type=\"bibr\" target=\"#b43\">[44]</ref>: Yelp is an online social network. The dataset contains fi  neighbour proximity <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. Recently, GNN-based methods have been widely applied to th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, anomaly detection <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and sentiment analysis <ref type=\"bibr\" target=\"#b31\">[32. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onstruct the original graphs without missing information. To this end, graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b38\">[39]</ref> have been recently introduced to this subject because they. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: specific contents. Many variants such as metapath2vec <ref type=\"bibr\" target=\"#b8\">[9]</ref>, HHNE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and HeteSpaceyWalk <ref type=\"bibr\" target=\"#b16\">[17]</r roximity in the path. Following this idea, some extensions <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> have been proposed recently so that the model can deal with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tive node embeddings, which can preserve neighbour proximity <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. Recently, GNN-base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odes in a network has a specific type of link or not. Although researchers have paid much attention <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" targ by manually designed hyperedges. They construct their hypergraphs with k-nearest nodes. Chen et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> put forward MGCN, and they discuss the impact of different m nually designed via features clustering and keeps fixed in the whole learning process.</p><p>\u2022 MGCN <ref type=\"bibr\" target=\"#b3\">[4]</ref>: This method first uses a graph convolutional network to lea d methods have been widely applied to this area and suggest the advantages over traditional methods <ref type=\"bibr\" target=\"#b3\">[4]</ref>. For example, Li et al. <ref type=\"bibr\" target=\"#b22\">[23]<  , k, k = r \u00d7 |N e i |.</formula><p>//sample k adjacent hyperedges from N e i for e i using formula <ref type=\"bibr\" target=\"#b3\">(4)</ref>. </p><formula xml:id=\"formula_12\">4 e \u25b7 i = e i \u222a e i1 \u222a e i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed to study hypergraph structures and have raised many interesting works. For example, Zhang et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> propose an EM algorithm to predict whether a set of object. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: predict whether a set of objects belong to the same tulpe, which they called hyperlink. Yoon et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> research the correlation between the hyperedge size and th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed to study hypergraph structures and have raised many interesting works. For example, Zhang et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> propose an EM algorithm to predict whether a set of object. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: INGS 4.1 Datasets</head><p>We conduct our experiments on the following widely used datasets: \u2022 Cora <ref type=\"bibr\" target=\"#b23\">[24]</ref>: It is a citation network which contains 2,708 papers from. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t forward MGCN, and they discuss the impact of different manually created hypergraphs. Jiang et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> and Zhang et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> get=\"#b46\">[47]</ref> use self-attention to learn node embeddings from the hypergraph. Jiang et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> conduct the convolutional operation on both vertices and h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nually created hypergraphs. Jiang et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> and Zhang et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> study dynamic networks using hypergraph learning. They tre size and the hyperedge prediction, and find the limitations of pair-wise interactions. Zhang et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> study the dynamic hypergraph structure for node classifica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: specific contents. Many variants such as metapath2vec <ref type=\"bibr\" target=\"#b8\">[9]</ref>, HHNE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and HeteSpaceyWalk <ref type=\"bibr\" target=\"#b16\">[17]</r roximity in the path. Following this idea, some extensions <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> have been proposed recently so that the model can deal with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rgraph Learning</head><p>Recently, hypergraph learning has been widely used in relational inference <ref type=\"bibr\" target=\"#b6\">[7]</ref>, location-based social networks <ref type=\"bibr\" target=\"#b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: isting works learn higher-level relations from ordinary local proximities. For example, Chen et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> put forward PME, which integrates first-order and second-ord  some negative links. In this paper, we use a bidirectional negative sampling strategy mentioned in <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which will draw K negative samples from both sides of a pos nt meta-paths and maximises the conditional probability of the target node given its context. \u2022 PME <ref type=\"bibr\" target=\"#b4\">[5]</ref>: This method first projects node embeddings into different s oblem can be explored by learning effective node embeddings, which can preserve neighbour proximity <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in analyzing users' online behaviors and social phenomena <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. It has been widely used in recommendation systems <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f link or not. Although researchers have paid much attention <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, there is still sig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ised the importance of higher-level relations on the network <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. For example, Shao et al. study the correlations between ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b25\">[26]</ref> to make node embeddings smooth on pair-wise links. For example, Grover et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> propose a bias random walk framework and then use the Skip bedding aims to learn low-dimensional representations for nodes in the network. Traditional methods <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> usually sample pat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the performance, researchers have realised the importance of higher-level relations on the network <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. For example, Shao e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: maximize the probability of target node given specific contents. Many variants such as metapath2vec <ref type=\"bibr\" target=\"#b8\">[9]</ref>, HHNE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and HeteSpa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: maximize the probability of target node given specific contents. Many variants such as metapath2vec <ref type=\"bibr\" target=\"#b8\">[9]</ref>, HHNE <ref type=\"bibr\" target=\"#b36\">[37]</ref>, and HeteSpa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rgraph Learning</head><p>Recently, hypergraph learning has been widely used in relational inference <ref type=\"bibr\" target=\"#b6\">[7]</ref>, location-based social networks <ref type=\"bibr\" target=\"#b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1]</ref>, personality perception <ref type=\"bibr\" target=\"#b48\">[49]</ref>, and recommender systems <ref type=\"bibr\" target=\"#b49\">[50]</ref> because of its capacity for modelling high-level relations. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nually created hypergraphs. Jiang et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> and Zhang et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> study dynamic networks using hypergraph learning. They tre size and the hyperedge prediction, and find the limitations of pair-wise interactions. Zhang et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref> study the dynamic hypergraph structure for node classifica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: isting works learn higher-level relations from ordinary local proximities. For example, Chen et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> put forward PME, which integrates first-order and second-ord  some negative links. In this paper, we use a bidirectional negative sampling strategy mentioned in <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which will draw K negative samples from both sides of a pos nt meta-paths and maximises the conditional probability of the target node given its context. \u2022 PME <ref type=\"bibr\" target=\"#b4\">[5]</ref>: This method first projects node embeddings into different s oblem can be explored by learning effective node embeddings, which can preserve neighbour proximity <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rgraph Learning</head><p>Recently, hypergraph learning has been widely used in relational inference <ref type=\"bibr\" target=\"#b6\">[7]</ref>, location-based social networks <ref type=\"bibr\" target=\"#b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pre-trained BERT-base movie review classifier using one adversarial example generated by CHECK-LIST <ref type=\"bibr\" target=\"#b26\">(Ribeiro et al. 2020)</ref>. The general meaning of the text remains  , we change text by using the linguistic transformations following the CHECKLIST behavioral testing <ref type=\"bibr\" target=\"#b26\">(Ribeiro et al. 2020</ref>). We use 4 invariance test by using TextAt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b26\">(Ribeiro et al. 2020</ref>). We use 4 invariance test by using TextAttack <ref type=\"bibr\" target=\"#b19\">(Morris et al. 2020)</ref>, including name replacement, position repl ersarial example, the attacker must search through all possible conversions. We refer the reader to <ref type=\"bibr\" target=\"#b19\">(Morris et al. 2020</ref>) for more details.  Instead of measuring ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , and Doshi-Velez 2017;</ref><ref type=\"bibr\" target=\"#b5\">DeYoung et al. 2020)</ref>. For example, <ref type=\"bibr\" target=\"#b21\">Paranjape et al. (2020a)</ref> leverage the information bottleneck pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: put supporting evidence e i .</p><p>The basic scheme follows an multi-task learning (MTL) framework <ref type=\"bibr\" target=\"#b3\">(Caruana 1997)</ref> with two tasks -( <ref type=\"formula\">1</ref>) ra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t to small perturbations in the input, we consider three different attack methods (i.e., TextFooler <ref type=\"bibr\" target=\"#b6\">(Jin et al. 2020)</ref>, TextBugger <ref type=\"bibr\" target=\"#b11\">(Li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -defined named entity dictionary) have the same part-of-speech as the original word tagged by flair <ref type=\"bibr\" target=\"#b0\">(Akbik et al. 2019)</ref>. Here, the percentage of words to replace pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: input sequences. These text snippets allows humans to verify the correctness of predictions quickly <ref type=\"bibr\" target=\"#b33\">(Zaidan and Eisner 2008;</ref><ref type=\"bibr\" target=\"#b34\">Zhang, M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g three adversarial attack methods. The baseline is the vanilla BERT fine-tuned on the IMDB dataset <ref type=\"bibr\" target=\"#b16\">(Maas et al. 2011)</ref>. We compare performance in accuracy under at  target=\"#b23\">(Pruthi et al. 2020)</ref>. This dataset includes 50k movie reviews from IMDB dataset<ref type=\"bibr\" target=\"#b16\">(Maas et al. 2011</ref>) and 1.8k movie reviews with human-labeled ra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ade from the rationale alone <ref type=\"bibr\" target=\"#b10\">(Lei, Barzilay, and Jaakkola 2016;</ref><ref type=\"bibr\" target=\"#b2\">Bastings, Aziz, and Titov 2019;</ref><ref type=\"bibr\" target=\"#b5\">DeY \"bibr\" target=\"#b32\">Yoon, Jordon, and van der Schaar 2019)</ref> and reparameterization techniques <ref type=\"bibr\" target=\"#b2\">(Bastings, Aziz, and Titov 2019;</ref><ref type=\"bibr\" target=\"#b8\">La. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orcement learning approaches <ref type=\"bibr\" target=\"#b10\">(Lei, Barzilay, and Jaakkola 2016;</ref><ref type=\"bibr\" target=\"#b32\">Yoon, Jordon, and van der Schaar 2019)</ref> and reparameterization t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  by using attention scores for downstream classification. 4) The weakly-and semi-supervised methods <ref type=\"bibr\" target=\"#b23\">(Pruthi et al. 2020</ref>) present a classify-then-extract framework  sification. The pipeline approach uses RNNs, whereas the base model is BERT-base for IB, FRESH, and <ref type=\"bibr\" target=\"#b23\">(Pruthi et al. 2020)</ref>, as same as ours for a fair comparison. Th ><row><cell>4 Experiments</cell></row><row><cell>4.1 Dataset</cell></row></table><note>Movie Reviews<ref type=\"bibr\" target=\"#b23\">(Pruthi et al. 2020)</ref>. This dataset includes 50k movie reviews f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve methods for classification and rationale extraction in both datasets: 1) The pipeline approaches <ref type=\"bibr\" target=\"#b9\">(Lehman et al. 2019;</ref><ref type=\"bibr\" target=\"#b10\">Lei, Barzilay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g three adversarial attack methods. The baseline is the vanilla BERT fine-tuned on the IMDB dataset <ref type=\"bibr\" target=\"#b16\">(Maas et al. 2011)</ref>. We compare performance in accuracy under at  target=\"#b23\">(Pruthi et al. 2020)</ref>. This dataset includes 50k movie reviews from IMDB dataset<ref type=\"bibr\" target=\"#b16\">(Maas et al. 2011</ref>) and 1.8k movie reviews with human-labeled ra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -defined named entity dictionary) have the same part-of-speech as the original word tagged by flair <ref type=\"bibr\" target=\"#b0\">(Akbik et al. 2019)</ref>. Here, the percentage of words to replace pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b27\">Rudin 2019)</ref>. One practical approach is to extract prediction's rationales from input <ref type=\"bibr\" target=\"#b12\">(Lipton 2016;</ref><ref type=\"bibr\">Camburu et al. 2018;</ref><ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: her hand, our adversarial training in the embedding space refines the standard adversarial training <ref type=\"bibr\" target=\"#b17\">(Madry et al. 2018</ref>) in computation efficiency and training smoo e=\"formula\" target=\"#formula_5\">5</ref>) can be solved reliably by projected gradient descent (PGD) <ref type=\"bibr\" target=\"#b17\">(Madry et al. 2018)</ref>. It is a standard method for large-scale co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d reparameterization techniques <ref type=\"bibr\" target=\"#b2\">(Bastings, Aziz, and Titov 2019;</ref><ref type=\"bibr\" target=\"#b8\">Latcinnik and Berant 2020)</ref>, which may be sensitive to hyperparam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es. Unlike post-hoc methods <ref type=\"bibr\" target=\"#b25\">(Ribeiro, Singh, and Guestrin 2016;</ref><ref type=\"bibr\" target=\"#b15\">Lundberg and Lee 2017)</ref>  properties. On the other hand, our goal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve methods for classification and rationale extraction in both datasets: 1) The pipeline approaches <ref type=\"bibr\" target=\"#b9\">(Lehman et al. 2019;</ref><ref type=\"bibr\" target=\"#b10\">Lei, Barzilay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ause of its potential in simplifying diagnostic procedures <ref type=\"bibr\">(Tang et al. 2016;</ref><ref type=\"bibr\" target=\"#b5\">Kao, Tang, and Chang 2018)</ref>, helping make better and more effecti type=\"bibr\" target=\"#b9\">Liu et al. 2017;</ref><ref type=\"bibr\" target=\"#b8\">Ling et al. 2017;</ref><ref type=\"bibr\" target=\"#b5\">Kao, Tang, and Chang 2018;</ref><ref type=\"bibr\" target=\"#b17\">Wei et  ow><row><cell>automatic diagnosis, which mostly use RL (Tang et al. 2016;</cell></row></table><note><ref type=\"bibr\" target=\"#b5\">Kao, Tang, and Chang 2018;</ref><ref type=\"bibr\" target=\"#b11\">Peng et i-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_9\"><head>Table 6 :</head><label>6</label><figDesc>following<ref type=\"bibr\" target=\"#b5\">(Kao, Tang, and Chang 2018)</ref>, which is based on symptom-disease d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b17\">Wei et al. 2018;</ref><ref type=\"bibr\" target=\"#b20\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021</ref>). An automatic diagnosis syst br\" target=\"#b9\">Luo, Li, and Glass 2020;</ref><ref type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>.</p><p>Due to the existing o type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>. However, RL learns how to i type=\"bibr\" target=\"#b7\">Liao et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>.Tang et al. (2016)  propose  ich medical knowledge graph into the topic transition in dialogue management. This is the model from<ref type=\"bibr\" target=\"#b16\">(Teixeira, Maran, and Dragoni 2021)</ref>, which is proposed to lever. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b17\">(Wei et al. 2018;</ref><ref type=\"bibr\" target=\"#b20\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b9\">Luo, Li, and Glass 2020;</ref><ref type=\"bibr\" target=\"#b18\">Xia et al or policy learning <ref type=\"bibr\">(Tang et al. 2016;</ref><ref type=\"bibr\">Peng et al. 2017;</ref><ref type=\"bibr\" target=\"#b9\">Liu et al. 2017;</ref><ref type=\"bibr\" target=\"#b8\">Ling et al. 2017;<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b17\">Wei et al. 2018;</ref><ref type=\"bibr\" target=\"#b20\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021</ref>). An automatic diagnosis syst br\" target=\"#b9\">Luo, Li, and Glass 2020;</ref><ref type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>.</p><p>Due to the existing o type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>. However, RL learns how to i type=\"bibr\" target=\"#b7\">Liao et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>.Tang et al. (2016)  propose  ich medical knowledge graph into the topic transition in dialogue management. This is the model from<ref type=\"bibr\" target=\"#b16\">(Teixeira, Maran, and Dragoni 2021)</ref>, which is proposed to lever. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b7\">Liao et al. 2020;</ref><ref type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran,  type=\"bibr\" target=\"#b20\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b7\">Liao et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran,  asked Language model <ref type=\"bibr\" target=\"#b3\">(Devlin et al. 2018)</ref> as auto-encoding (AE) <ref type=\"bibr\" target=\"#b4\">(Dong et al. 2019;</ref><ref type=\"bibr\" target=\"#b0\">Bao et al. 2020). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the T imp sequence generation approximate to the S imp inference. fore, we use a concurrent softmax<ref type=\"bibr\" target=\"#b10\">(Peng et al. 2020)</ref> replacing the original softmax to train [S] . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to generate a target sequence condition on a source input. It covers many areas with a lot of tasks <ref type=\"bibr\" target=\"#b22\">(Zhang et al. 2020</ref>). Among them, the natural language generatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b17\">Wei et al. 2018;</ref><ref type=\"bibr\" target=\"#b20\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021</ref>). An automatic diagnosis syst br\" target=\"#b9\">Luo, Li, and Glass 2020;</ref><ref type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>.</p><p>Due to the existing o type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>. However, RL learns how to i type=\"bibr\" target=\"#b7\">Liao et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>.Tang et al. (2016)  propose  ich medical knowledge graph into the topic transition in dialogue management. This is the model from<ref type=\"bibr\" target=\"#b16\">(Teixeira, Maran, and Dragoni 2021)</ref>, which is proposed to lever. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6;</ref><ref type=\"bibr\">Peng et al. 2017;</ref><ref type=\"bibr\" target=\"#b9\">Liu et al. 2017;</ref><ref type=\"bibr\" target=\"#b8\">Ling et al. 2017;</ref><ref type=\"bibr\" target=\"#b5\">Kao, Tang, and Ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b17\">Wei et al. 2018;</ref><ref type=\"bibr\" target=\"#b20\">Xu et al. 2019;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021</ref>). An automatic diagnosis syst br\" target=\"#b9\">Luo, Li, and Glass 2020;</ref><ref type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>.</p><p>Due to the existing o type=\"bibr\" target=\"#b18\">Xia et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>. However, RL learns how to i type=\"bibr\" target=\"#b7\">Liao et al. 2020;</ref><ref type=\"bibr\" target=\"#b4\">Hou et al. 2021;</ref><ref type=\"bibr\" target=\"#b16\">Teixeira, Maran, and Dragoni 2021)</ref>.Tang et al. (2016)  propose  ich medical knowledge graph into the topic transition in dialogue management. This is the model from<ref type=\"bibr\" target=\"#b16\">(Teixeira, Maran, and Dragoni 2021)</ref>, which is proposed to lever. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al. 2016;</cell></row></table><note><ref type=\"bibr\" target=\"#b5\">Kao, Tang, and Chang 2018;</ref><ref type=\"bibr\" target=\"#b11\">Peng et al. 2018;</ref><ref type=\"bibr\" target=\"#b17\">Wei et al. 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: misprediction in their method of dynamically changing the length of the instruction supply pipeline <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>There are several performance-modeling techniques w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he front-end of a modern processor, an instruction cache access is decoupled from branch prediction <ref type=\"bibr\" target=\"#b7\">[8]</ref>- <ref type=\"bibr\" target=\"#b9\">[10]</ref>. In such processor tion cache accesses are decoupled from a branch prediction <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Fig. <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref>, as well as cache hit rate <ref type=\"bibr\" target=\"#b3\">[4]</ref>, cache miss rate <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ca mlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_5\"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type=\"bibr\" target=\"#b3\">4</ref>. Concept of regions. P, F, D, X, and M represent instruction f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch techniques reduce the number of branch mispredictions <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b14\">[15]</ref>. When applying such techniques, we need to add a penalty f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction cache design. Some instruction prefetch techniques reduce the number of branch mispredictions <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b14\">[15]</ref>. When applying . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref>, as well as cache hit rate <ref type=\"bibr\" target=\"#b3\">[4]</ref>, cache miss rate <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ca mlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_5\"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type=\"bibr\" target=\"#b3\">4</ref>. Concept of regions. P, F, D, X, and M represent instruction f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Arm have argued that the study of instruction prefetching should be based on a decoupled front-end <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch techniques reduce the number of branch mispredictions <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b14\">[15]</ref>. When applying such techniques, we need to add a penalty f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l Setup</head><p>The code framework and initial weight of BART come from Huggingface's Transformers <ref type=\"bibr\" target=\"#b25\">(Wolf et al. 2020)</ref> </p></div> <div xmlns=\"http://www.tei-c.org/ It is worth mentioning that although the distance within the class has been reduced, the uniformity <ref type=\"bibr\" target=\"#b25\">(Wang and Isola 2020)</ref> between samples has been well maintained,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rastive Learning</head><p>Unsupervised contrastive learning In the field of computer vision, SimCLR <ref type=\"bibr\" target=\"#b4\">(Chen et al. 2020b</ref>) takes pictures obtained from the same image . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b8\">(Gunel et al. 2021)</ref>. For ERC, the number of samples in each category in some datasets <ref type=\"bibr\" target=\"#b17\">(Li et al. 2017</ref>) is highly unbalanced, while the supervised con al. 2019)</ref>, EmoryNLP <ref type=\"bibr\" target=\"#b28\">(Zahiri and Choi 2018)</ref>, Dai-lyDialog <ref type=\"bibr\" target=\"#b17\">(Li et al. 2017)</ref>, and IEMOCAP <ref type=\"bibr\">(Busso et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se, the problem can be transformed into a simple sentence classification so that pre-trained models <ref type=\"bibr\" target=\"#b21\">(Qiu et al. 2020</ref>) such as BERT <ref type=\"bibr\" target=\"#b5\">(D. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s data set, and we ignore the label \"neutral\" when calculating the results as in the previous works <ref type=\"bibr\" target=\"#b30\">(Zhu et al. 2021;</ref><ref type=\"bibr\" target=\"#b23\">Shen et al. 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Liu et al. 2019)</ref>, HiTrans <ref type=\"bibr\" target=\"#b16\">(Li et al. 2020)</ref>, Di-alogXL <ref type=\"bibr\" target=\"#b22\">(Shen et al. 2021a</ref>) and XLNet <ref type=\"bibr\" target=\"#b27\">(Y lin et al. 2019)</ref>, RoBERTa <ref type=\"bibr\" target=\"#b19\">(Liu et al. 2019)</ref> and DialogXL <ref type=\"bibr\" target=\"#b22\">(Shen et al. 2021a</ref>). Therefore, the graph-based model may have . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se, the problem can be transformed into a simple sentence classification so that pre-trained models <ref type=\"bibr\" target=\"#b21\">(Qiu et al. 2020</ref>) such as BERT <ref type=\"bibr\" target=\"#b5\">(D. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b8\">(Gunel et al. 2021)</ref>. For ERC, the number of samples in each category in some datasets <ref type=\"bibr\" target=\"#b17\">(Li et al. 2017</ref>) is highly unbalanced, while the supervised con al. 2019)</ref>, EmoryNLP <ref type=\"bibr\" target=\"#b28\">(Zahiri and Choi 2018)</ref>, Dai-lyDialog <ref type=\"bibr\" target=\"#b17\">(Li et al. 2017)</ref>, and IEMOCAP <ref type=\"bibr\">(Busso et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lysis of SCL To conduct a qualitative analysis of supervised contrastive learning, we utilize t-SNE <ref type=\"bibr\" target=\"#b9\">(Hinton and Roweis 2002)</ref> to visualize the distribution of high-d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lso do not work well. Apart from the genomic distance-based approach, in the work of ComplexContact <ref type=\"bibr\" target=\"#b17\">[19]</ref>, a web server for predicting inter-protein residue-residue he phylogeny-based approach is more effective in preparing MSA of interologs for PPIs in eukaryotes <ref type=\"bibr\" target=\"#b17\">[19]</ref>. However, it should be noted the bacterial PPI dataset use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ted breakthrough in protein monomeric structure prediction <ref type=\"bibr\" target=\"#b10\">[12,</ref><ref type=\"bibr\" target=\"#b11\">13]</ref>. After that, in the work of RoseTTAFold <ref type=\"bibr\" ta  in the training of AF2, but none of the complex structures would be included in the model training <ref type=\"bibr\" target=\"#b11\">[13]</ref>. Besides, all the predictions were made by inputting the M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ange [0,1], to assess the quality of the complex structure prediction (see Supplemental Methods S5) <ref type=\"bibr\" target=\"#b26\">[28]</ref>. Specifically, DockQ &lt; 0.23 corresponds to an incorrect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  protein families <ref type=\"bibr\" target=\"#b18\">[20]</ref><ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" target=\"#b20\">[22]</ref>. However, the expensive computational cost of these approa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures can be predicted with high accuracy by deep learning irrespective of co-evolution information <ref type=\"bibr\" target=\"#b28\">[30]</ref>. We may also be able to develop deep learning models for p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref>. However, by far, only a small fraction of protein-protein i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  protein families <ref type=\"bibr\" target=\"#b18\">[20]</ref><ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" target=\"#b20\">[22]</ref>. However, the expensive computational cost of these approa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion signal between interacting homologous protein families <ref type=\"bibr\" target=\"#b18\">[20]</ref><ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" target=\"#b20\">[22]</ref>. However, the exp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that in prokaryotes, proteins with related functions are usually encoded together within the genome <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>. However, operons  hould be noted the bacterial PPI dataset used to evaluate ComplexContact is from Ovchinnikov et al. <ref type=\"bibr\" target=\"#b15\">[17]</ref>,</p><p>which is constituted by Ecoli PPIs formed by protei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s, we first applied JackHMMER <ref type=\"bibr\" target=\"#b23\">[25]</ref> to search against UniRef100 <ref type=\"bibr\" target=\"#b24\">[26]</ref> (2021-04-07) to generate the MSA of homologous proteins fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  it is shown in the figure, given sequences of the interacting proteins, we first applied JackHMMER <ref type=\"bibr\" target=\"#b23\">[25]</ref> to search against UniRef100 <ref type=\"bibr\" target=\"#b24\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d, the complex structure of the interolog can be different <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24]</ref>. Including non-conserved interologs in the MSA may attenuat onserved as monomeric proteins in the evolutionary process <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24]</ref>. Including sequences of non-conserved protein pairs in the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erapeutic development <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref>. However, by far, onl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion signal between interacting homologous protein families <ref type=\"bibr\" target=\"#b18\">[20]</ref><ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" target=\"#b20\">[22]</ref>. However, the exp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that in prokaryotes, proteins with related functions are usually encoded together within the genome <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>. However, operons  hould be noted the bacterial PPI dataset used to evaluate ComplexContact is from Ovchinnikov et al. <ref type=\"bibr\" target=\"#b15\">[17]</ref>,</p><p>which is constituted by Ecoli PPIs formed by protei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onships between the two groups of homologous proteins of the interacting proteins are often unknown <ref type=\"bibr\" target=\"#b13\">[15,</ref><ref type=\"bibr\" target=\"#b14\">16]</ref>. In the work of Ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: scoring functions to identify the most probable binding mode <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. Protein-protein docking approach has two major limitations. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roduced binding modes are ranked using scoring functions to identify the most probable binding mode <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. Protein-protein docki. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nterologs through maximizing the coevolution signal between interacting homologous protein families <ref type=\"bibr\" target=\"#b18\">[20]</ref><ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  it is shown in the figure, given sequences of the interacting proteins, we first applied JackHMMER <ref type=\"bibr\" target=\"#b23\">[25]</ref> to search against UniRef100 <ref type=\"bibr\" target=\"#b24\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  protein families <ref type=\"bibr\" target=\"#b18\">[20]</ref><ref type=\"bibr\" target=\"#b19\">[21]</ref><ref type=\"bibr\" target=\"#b20\">[22]</ref>. However, the expensive computational cost of these approa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d functions are usually encoded together within the genome <ref type=\"bibr\" target=\"#b15\">[17,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>. However, operons are mainly observed in prokaryotes, there. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le. We answer these questions by measuring counterfactual influence with a formulation adapted from <ref type=\"bibr\" target=\"#b11\">Feldman and Zhang (2020)</ref>: </p><formula xml:id=\"formula_1\">Defin gradient updates during training to estimate the influence from a training example; Feldman (2020); <ref type=\"bibr\" target=\"#b11\">Feldman and Zhang (2020)</ref> use aggregated statistics from multipl tical formulation of counterfactual memorization is borrowed from a notion of label memorization in <ref type=\"bibr\" target=\"#b11\">Feldman (2020)</ref> and adapted to the context of neural LMs in this. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rmation in a memorized example during inference. Previous paper studies membership inference attack <ref type=\"bibr\" target=\"#b28\">(Shokri et al., 2017;</ref><ref type=\"bibr\" target=\"#b27\">Sablayrolle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: owing the method from <ref type=\"bibr\" target=\"#b19\">(Lee et al., 2021)</ref>, we first use MinHash <ref type=\"bibr\" target=\"#b2\">(Broder, 1997)</ref> to identify near-duplicate examples in RealNews t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b28\">(Shokri et al., 2017;</ref><ref type=\"bibr\" target=\"#b27\">Sablayrolles et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">Long et al., 2020)</ref> where an attacker tries to figure out if a p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odern neural language models (LMs) have achieved impressive results in generating high quality text <ref type=\"bibr\" target=\"#b3\">(Brown et al., 2020)</ref> and have led to breakthroughs in many downs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odern neural language models (LMs) have achieved impressive results in generating high quality text <ref type=\"bibr\" target=\"#b3\">(Brown et al., 2020)</ref> and have led to breakthroughs in many downs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nalyze counterfactual memorization of training examples in several standard text datasets: RealNews <ref type=\"bibr\" target=\"#b35\">(Zellers et al., 2019)</ref>, C4 <ref type=\"bibr\" target=\"#b26\">(Raff to substitute the prize with another prize at its absolute discretion. at its absolute discretion.  <ref type=\"bibr\" target=\"#b35\">(Zellers et al., 2019)</ref>. Red / green highlighted text indicate d itter: @AliLyaman_MustafaZeyevn Follow Trend on Telegram. Only most interesting and important news  <ref type=\"bibr\" target=\"#b35\">(Zellers et al., 2019)</ref>. Red / green highlighted text indicate d m Wiki-40B:en.We also show train-generation influence pairs between RealNews training set and Grover<ref type=\"bibr\" target=\"#b35\">(Zellers et al., 2019)</ref> model generation in Figure32, Figure33,  ted examples are directly taken from publicly released generations of the Grover-Mega (p=0.96) model<ref type=\"bibr\" target=\"#b35\">(Zellers et al., 2019)</ref>. Red / green highlighted text indicate d nerated examples. In this section, we evaluate on the publicly released generations from the Grover <ref type=\"bibr\" target=\"#b35\">(Zellers et al., 2019</ref>) models<ref type=\"foot\" target=\"#foot_2\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n learning theory, leave-one-out stability was shown to be deeply connected to generalization (e.g. <ref type=\"bibr\" target=\"#b21\">Mukherjee et al., 2006)</ref>; in differential privacy, the worst cas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: We train each model for 60 epochs<ref type=\"foot\" target=\"#foot_1\">1</ref> using the Adam optimizer <ref type=\"bibr\" target=\"#b16\">(Kingma and Ba, 2015)</ref> with learning rate 0.1 and weight decay 1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b28\">(Shokri et al., 2017;</ref><ref type=\"bibr\" target=\"#b27\">Sablayrolles et al., 2019;</ref><ref type=\"bibr\" target=\"#b20\">Long et al., 2020)</ref> where an attacker tries to figure out if a p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cant engineering infrastructure is needed to provide deep integration with large evolving codebases <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>We propose a novel technique to overcome the limitati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b12\">[13]</ref>, heterogeneous device mapping <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, function inlining <ref type=\"bibr\" target=\"#b15\">[16]</ref <ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> and graph learning <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have been proposed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e of their runtime performance and difficulty of integration. Instead we use gradient-boosted trees <ref type=\"bibr\" target=\"#b0\">[1]</ref> to generate interpretable decision trees. We use the XGBoost. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvised learning. Supervised learning has been applied to a range of problems such as loop unrolling <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, instruction schedu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, instruction scheduling <ref type=\"bibr\" target=\"#b11\">[12]</ref>, program partitioning <ref type=\"bibr\" target=\"#b12\">[13]< f numeric features <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, however selecting which values to include in a feature vec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s especially significant for very small measurements such as the runtime of individual basic blocks <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In <ref type=\"bibr\" target=\"#b19\">[20]</ref>, noise in me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: minating the human developed heuristics, as surveyed in <ref type=\"bibr\" target=\"#b7\">[8]</ref> and <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>The most commonly used technique is supervised learni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, instruction scheduling <ref type=\"bibr\" target=\"#b11\">[12]</ref>, program partitioning <ref type=\"bibr\" target=\"#b12\">[13]< f numeric features <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, however selecting which values to include in a feature vec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e of their runtime performance and difficulty of integration. Instead we use gradient-boosted trees <ref type=\"bibr\" target=\"#b0\">[1]</ref> to generate interpretable decision trees. We use the XGBoost. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b23\">24]</ref> and graph learning <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> have been proposed to simplify the task of feature engineer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e of their runtime performance and difficulty of integration. Instead we use gradient-boosted trees <ref type=\"bibr\" target=\"#b0\">[1]</ref> to generate interpretable decision trees. We use the XGBoost. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: compute the random walk kernels, we follow the generalized framework of computing walk-based kernel <ref type=\"bibr\" target=\"#b31\">(Vishwanathan et al. 2006)</ref>, and utilize the direct product grap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: monovsky and Komodakis 2017), GIN <ref type=\"bibr\" target=\"#b32\">(Xu et al. 2018a</ref>), GraphSAGE <ref type=\"bibr\" target=\"#b11\">(Hamilton, Ying, and Leskovec 2017)</ref>, RWGNN (Nikolentzos and Vaz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h around the central node, and can thus explore more topological structures. Another recent work by <ref type=\"bibr\" target=\"#b28\">Nikolentzos and Vazirgiannis (2020)</ref> focused on improving model  nd GNN-related GNTK <ref type=\"bibr\" target=\"#b8\">(Du et al. 2019)</ref>. We use the GraKeL library <ref type=\"bibr\" target=\"#b28\">(Siglidis et al. 2020)</ref> to implement these graph kernels and run. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uding walks and paths kernels <ref type=\"bibr\" target=\"#b10\">(G\u00e4rtner, Flach, and Wrobel 2003;</ref><ref type=\"bibr\" target=\"#b12\">Kashima, Tsuda, and Inokuchi 2003;</ref><ref type=\"bibr\">Borgwardt an re initially proposed by <ref type=\"bibr\" target=\"#b10\">G\u00e4rtner, Flach, and Wrobel (2003)</ref> and <ref type=\"bibr\" target=\"#b12\">Kashima, Tsuda, and Inokuchi (2003)</ref>. Among numerous variations . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b15\">Morris et al. 2019)</ref> show that MPNNs are at most as powerful as the WL kernel <ref type=\"bibr\" target=\"#b26\">(Shervashidze et al. 2011)</ref> and WL algorithm regarding the graph ype=\"bibr\" target=\"#b17\">(Neumann et al. 2016)</ref>, the Weisfeiler-Lehman subtree (WL-sub) kernel <ref type=\"bibr\" target=\"#b26\">(Shervashidze et al. 2011</ref>) and GNN-related GNTK <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: road class of graphs. However, recent studies <ref type=\"bibr\" target=\"#b32\">(Xu et al. 2018a;</ref><ref type=\"bibr\" target=\"#b15\">Morris et al. 2019)</ref> show that MPNNs are at most as powerful as   which unfortunately lacks neural network structure. Higher-order GNN variants have been studied in <ref type=\"bibr\" target=\"#b15\">Morris et al. (2019) and</ref><ref type=\"bibr\">Maron et al. (2019)</r zos and Vazirgiannis 2020), GCKN (Chen, Jacob, and Mairal 2020), and two high-order GNNs: 1-2-3 GNN <ref type=\"bibr\" target=\"#b15\">(Morris et al. 2019</ref>) and Powerful <ref type=\"bibr\">GNN (Maron e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce objects they use lack graph structure and may not be able to capture the structural information. <ref type=\"bibr\" target=\"#b4\">Chen, Jacob, and Mairal (2020)</ref> proposed GCKN which maps the inpu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: road class of graphs. However, recent studies <ref type=\"bibr\" target=\"#b32\">(Xu et al. 2018a;</ref><ref type=\"bibr\" target=\"#b15\">Morris et al. 2019)</ref> show that MPNNs are at most as powerful as   which unfortunately lacks neural network structure. Higher-order GNN variants have been studied in <ref type=\"bibr\" target=\"#b15\">Morris et al. (2019) and</ref><ref type=\"bibr\">Maron et al. (2019)</r zos and Vazirgiannis 2020), GCKN (Chen, Jacob, and Mairal 2020), and two high-order GNNs: 1-2-3 GNN <ref type=\"bibr\" target=\"#b15\">(Morris et al. 2019</ref>) and Powerful <ref type=\"bibr\">GNN (Maron e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n asynchronous routing.</p><p>The asynchronous DRL can be found in the path planning domain. PRIMAL <ref type=\"bibr\" target=\"#b13\">[13]</ref> presented a novel framework that combines reinforcement an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l sparse data structure fo r runtime and memory efficiency. Dr.CU won first place in the ISPD 20 19 <ref type=\"bibr\" target=\"#b5\">[6]</ref> contest about detailed routing. However, the fi nal result q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng is one of the most time-consuming stages in the Ve ry Large Scale Integration (VLSI) design flow <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Routing involves a large and arbitrary number of nets being. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng is one of the most time-consuming stages in the Ve ry Large Scale Integration (VLSI) design flow <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Routing involves a large and arbitrary number of nets being. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l sparse data structure fo r runtime and memory efficiency. Dr.CU won first place in the ISPD 20 19 <ref type=\"bibr\" target=\"#b5\">[6]</ref> contest about detailed routing. However, the fi nal result q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d planner. However, they deal with path-planning in a 2D grid world rather than a graph. Niu et al. <ref type=\"bibr\" target=\"#b15\">[15]</ref> discussed a generalized value iteration network (GVIN) to  nstruct DR_ GVIN, which makes the routing decision using a graph-based value iteration network GVIN <ref type=\"bibr\" target=\"#b15\">[15]</ref>. We also construct DR_ MARVIN_ Att, indicating that the ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roposed an Integer Linear Programming-based algorithm to achieve intra-layer parallel routing. RDTA <ref type=\"bibr\" target=\"#b7\">[8]</ref> adopted an efficient mutability-driven track assignment algo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: etween nets <ref type=\"bibr\" target=\"#b1\">[2]</ref> [3] <ref type=\"bibr\" target=\"#b3\">[4]</ref> [5] <ref type=\"bibr\" target=\"#b8\">[9]</ref> [10] <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The performa br\" target=\"#b7\">[8]</ref> adopted an efficient mutability-driven track assignment algorithm. Dr.CU <ref type=\"bibr\" target=\"#b8\">[9]</ref> [10] discussed an optimal correct-by-construction path searc ing is on a stack of metal layers. A wire segment on a layer runs either horizontally or vertically <ref type=\"bibr\" target=\"#b8\">[9]</ref> . Each layer has a preferred direction for routing, which be erred direction for routing, which benefits manufacturability, mutability, and design rule checking <ref type=\"bibr\" target=\"#b8\">[9]</ref> . Detailed routing decides the actual path of the wires conn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d planner. However, they deal with path-planning in a 2D grid world rather than a graph. Niu et al. <ref type=\"bibr\" target=\"#b15\">[15]</ref> discussed a generalized value iteration network (GVIN) to  nstruct DR_ GVIN, which makes the routing decision using a graph-based value iteration network GVIN <ref type=\"bibr\" target=\"#b15\">[15]</ref>. We also construct DR_ MARVIN_ Att, indicating that the ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onduct multi-hop KG reasoning over subgraphs. The core of the proposed CogKR is the cognitive graph <ref type=\"bibr\" target=\"#b28\">[29]</ref>, a subgraph of the original KG iteratively expanded in the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: soning process (System 2). Such two systems are also related to the capacity-limited working memory <ref type=\"bibr\" target=\"#b27\">[28]</ref>. System 1 updates the working memory with the retrieved co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"2.3\">Graph Neural Networks for Relational Learning</head><p>Graph neural networks (GNN) <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b47\">[48]</ref> are a class of  le structure of the cognitive graph, we compute the node representations with graph neural networks <ref type=\"bibr\" target=\"#b46\">[47]</ref>, which capture the dependence of graphs via message passin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"2.3\">Graph Neural Networks for Relational Learning</head><p>Graph neural networks (GNN) <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b47\">[48]</ref> are a class of  le structure of the cognitive graph, we compute the node representations with graph neural networks <ref type=\"bibr\" target=\"#b46\">[47]</ref>, which capture the dependence of graphs via message passin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>Prior arts for KG reasoning can be roughly categori  benchmarks, they have been shown to suffer from cascading errors when modeling multi-hop relations <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b33\">[34]</ref>, which are indi lization ability.</p><p>To overcome the limit, many works <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bib r\" target=\"#b16\">[17]</ref> uses Monte Carlo Tree Search to solve the reward sparsity problem. DIVA <ref type=\"bibr\" target=\"#b10\">[11]</ref> unifies path-finding and path-reasoning with variational i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b10\">[11]</ref> unifies path-finding and path-reasoning with variational inference. DIVINE <ref type=\"bibr\" target=\"#b44\">[45]</ref> uses generative adversarial imitation learning <ref type=\"  other papers <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>. During training, given a training triple (e s , r, e o ), f type=\"bibr\" target=\"#b17\">[18]</ref>, M-Walk <ref type=\"bibr\" target=\"#b16\">[17]</ref> and DIVINE <ref type=\"bibr\" target=\"#b44\">[45]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"2.3\">Graph Neural Networks for Relational Learning</head><p>Graph neural networks (GNN) <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b47\">[48]</ref> are a class of  le structure of the cognitive graph, we compute the node representations with graph neural networks <ref type=\"bibr\" target=\"#b46\">[47]</ref>, which capture the dependence of graphs via message passin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bine path-based methods with distributed representations <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref> and reinforcement learning <ref type=\"bibr\" target=\"#b9\">[ ules. Both Chain-of-Reasoning <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Compositional Reasoning <ref type=\"bibr\" target=\"#b21\">[22]</ref> infer underlying relations of two entities with neural net e, out of consideration about the generalization ability <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, we adopt the deep learning module instead of previous rul sidering the success of RNN models in path-based methods <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, we use GRU <ref type=\"bibr\" target=\"#b65\">[66]</ref>, a v ) = 1 \u2202E e \u2202 m e<label>(5)</label></formula><p>It can be considered as an extension of the Path-RNN <ref type=\"bibr\" target=\"#b21\">[22]</ref>, with the ability to encode not only a single path but a c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce. DIVINE <ref type=\"bibr\" target=\"#b44\">[45]</ref> uses generative adversarial imitation learning <ref type=\"bibr\" target=\"#b45\">[46]</ref> to learn reasoning policies and reward functions self-adap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  descriptions <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ] and the relation-entity feature v r k h v e . We do not use more advanced RNN models such as LSTM <ref type=\"bibr\" target=\"#b66\">[67]</ref> that improve long-term memory since reasoning paths are us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g us to design these two steps separately. The second one is the ProNE model, in which Zhang et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> show that using a modulated Gaussian filter to propagate/s s-PPR, Heat kernel, and Gaussian kernel-that have been widely used in graph representation learning <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bib =\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref> . We randomly sample different percentages of labeled node ameters in an unsupervised manner.</p><p>The graph filter used for the embedding smoothing in ProNE <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a modified 2nd-order Gaussian kernel. For simplicity, t bibr\" target=\"#b45\">[46]</ref> also proposes a framework to unify the aforementioned methods. ProNE <ref type=\"bibr\" target=\"#b15\">[16]</ref> formalizes network embedding as sparse matrix factorizatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion learning <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref> and also propose a new filter based on signal rescaling. T. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: work. Each node has a human-annotated topic as its label and content-based features.</p><p>\u2022 Pubmed <ref type=\"bibr\" target=\"#b41\">[42]</ref> contains 19717 diabetes-related publications.</p><p>Each p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Besides, Bayesian optimization <ref type=\"bibr\" target=\"#b31\">[32]</ref>, with the Gaussian process <ref type=\"bibr\" target=\"#b65\">[66]</ref> as the underlying surrogate model, is a popular technique . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 11\">[12]</ref> leverage the idea of contrastive learning <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref> to pre-train graph neural networks via self-supervised sig \u2212 hT j s)])<label>(21)</label></formula><p>For graph convolution based methods, InfoNCE proposed in <ref type=\"bibr\" target=\"#b13\">[14]</ref> is utilized as the optimization target. From the perspecti ial. Contrastive methods may employ a scoring function to evaluate the similarity of pairs of data. <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b54\">[55]</ref> employ InfoNCE . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and parametric filters are learned via back-propagation. <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p><p>potential in shaping the future of graph mining and learning. For example, the GraphSage model <ref type=\"bibr\" target=\"#b9\">[10]</ref> with the unsupervised loss can be considered as a self-supe  convolution-based methods DGI <ref type=\"bibr\" target=\"#b10\">[11]</ref> and unsupervised GraphSAGE <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and compare with semi-supervised GCN <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[56]</ref>, <ref type=\"bibr\" target=\"#b56\">[57]</ref>, <ref type=\"bibr\" target=\"#b57\">[58]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref> apply contrasti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p><p>potential in shaping the future of graph mining and learning. For example, the GraphSage model <ref type=\"bibr\" target=\"#b9\">[10]</ref> with the unsupervised loss can be considered as a self-supe  convolution-based methods DGI <ref type=\"bibr\" target=\"#b10\">[11]</ref> and unsupervised GraphSAGE <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and compare with semi-supervised GCN <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[57]</ref>, <ref type=\"bibr\" target=\"#b57\">[58]</ref>, <ref type=\"bibr\" target=\"#b58\">[59]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref> apply contrastive methods in graph representation learning. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: la_24\">A p \u2248 k i=0 \u03b8 (i) p \u00c2, where \u03b8 (i) p = \u03b1(1 \u2212 \u03b1) i<label>(14)</label></formula><p>Heat Kernel <ref type=\"bibr\" target=\"#b26\">[27]</ref>. The heat kernel is often used in heat condition and diffu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: space by augmenting the input data, thereby achieving a significant improvement in downstream tasks <ref type=\"bibr\" target=\"#b13\">[14]</ref>. SGL refer to the contrastive learning of SimCLR and use t as designed various augmentation methods including node drop and edge drop for graph structure data <ref type=\"bibr\" target=\"#b13\">[14]</ref>. In order to further improve the diversity of the recommen ef type=\"bibr\" target=\"#b11\">[12]</ref>, LightGCN <ref type=\"bibr\" target=\"#b14\">[15]</ref> and SGL <ref type=\"bibr\" target=\"#b13\">[14]</ref>. On the basis of LightGCN, we implement three variants of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>.</p><p>The contrast loss function which called InfoNCE aims. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: astive learning of SimCLR and use the framework of simCLR as a paradigm of self-supervised learning <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Although SGL has achieved a certain degree of effec k includes four components: data augmentation, encoder, projection head, and contrast loss function <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Specifically, the data augmentation module generate Data Augmentation</head><p>In SimCLR, the authors discusses various augmentation methods for images <ref type=\"bibr\" target=\"#b14\">[15]</ref>, but for recommendation task, the data structure is differ F <ref type=\"bibr\" target=\"#b5\">[6]</ref>, NGCF <ref type=\"bibr\" target=\"#b11\">[12]</ref>, LightGCN <ref type=\"bibr\" target=\"#b14\">[15]</ref> and SGL <ref type=\"bibr\" target=\"#b13\">[14]</ref>. On the  ve proved that the projection head is beneficial to improve the performance of contrastive learning <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta zing the distance of the augmented views generated by other samples in the same batch in that space <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ears, a large number of collaborative filtering methods have been successfully implemented, such as <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: generated by other samples in the same batch in that space <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p><p>After obtain. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: astive learning of SimCLR and use the framework of simCLR as a paradigm of self-supervised learning <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Although SGL has achieved a certain degree of effec k includes four components: data augmentation, encoder, projection head, and contrast loss function <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Specifically, the data augmentation module generate Data Augmentation</head><p>In SimCLR, the authors discusses various augmentation methods for images <ref type=\"bibr\" target=\"#b14\">[15]</ref>, but for recommendation task, the data structure is differ F <ref type=\"bibr\" target=\"#b5\">[6]</ref>, NGCF <ref type=\"bibr\" target=\"#b11\">[12]</ref>, LightGCN <ref type=\"bibr\" target=\"#b14\">[15]</ref> and SGL <ref type=\"bibr\" target=\"#b13\">[14]</ref>. On the  ve proved that the projection head is beneficial to improve the performance of contrastive learning <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta zing the distance of the augmented views generated by other samples in the same batch in that space <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p><p>After obtain. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  following strong baselined CF models: DeepWalk <ref type=\"bibr\" target=\"#b30\">[31]</ref>, Node2vec <ref type=\"bibr\" target=\"#b31\">[32]</ref>, LINE <ref type=\"bibr\" target=\"#b32\">[33]</ref>, PinSage <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rse and personalized information needs of users, the personalized recommendation system has emerged <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b30\">[31]</ref>, Node2vec <ref type=\"bibr\" target=\"#b31\">[32]</ref>, LINE <ref type=\"bibr\" target=\"#b32\">[33]</ref>, PinSage <ref type=\"bibr\" target=\"#b33\">[34]</ref>, GC-MC . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng because any mapping expressed in the notation is precisely analyzable using the MDC's cost model <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Instead of proposing an approach for validating an input  tion  <ref type=\"bibr\" target=\"#b33\">[34]</ref>) is precisely analyzable using the MDC's cost model <ref type=\"bibr\" target=\"#b24\">[25]</ref>. We introduced a set of conformability rules as a way to d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ns/1.0\"><head n=\"6.2.2\">Evaluation on GEMM.</head><p>We considered GEMM workloads from recent work <ref type=\"bibr\" target=\"#b36\">[37]</ref>. An interesting aspect of these workloads is that they are ar in their shapes, making the rigid accelerators (e.g., TPUs) hard to reach their peak utilization <ref type=\"bibr\" target=\"#b36\">[37]</ref>. A summary of these workloads are shown in Figure <ref typ ne peaks of accelerator Fig. <ref type=\"figure\" target=\"#fig_2\">13</ref>. GEMM workloads taken from <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Fig. <ref type=\"figure\">14</ref>. Comparison of Marvel ge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ferent cost models for these subspaces, such as a classical distinct-block (DB) locality cost model <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> to explore the off ifferent approaches and cost models for these subspaces-that is, a classical DB locality cost model <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> to explore the off use and less data transfer.</p><p>In our approach, we consider the classical DB locality cost model <ref type=\"bibr\" target=\"#b15\">[16]</ref> to measure the off-chip data movement cost, which was deve -c.org/ns/1.0\" xml:id=\"fig_13\"><head>Marvel 6 : 23 Fig. 16 .</head><label>62316</label><figDesc>Fig.<ref type=\"bibr\" target=\"#b15\">16</ref>. Comparison of Marvel with prior mappers for spatial acceler. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance. Many spatial accelerators can be further interconnected together to create a scale-out system <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>Systolic arrays <ref type=\"bibr\" target=\"#b19\">[20,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s (e.g., depth-wise) and diverse hardware accelerator configurations (e.g., tree-based interconnect <ref type=\"bibr\" target=\"#b25\">[26]</ref>).</p><p>Our approach toward mapping space exploration is m across time (via L1/L2 buffers <ref type=\"bibr\" target=\"#b6\">[7]</ref>), space (via broadcast links <ref type=\"bibr\" target=\"#b25\">[26]</ref>), and space-time (via neighboring links <ref type=\"bibr\" t idated against the RTL implementations of Eyeriss <ref type=\"bibr\" target=\"#b6\">[7]</ref> and MAERI <ref type=\"bibr\" target=\"#b25\">[26]</ref> on VGG16 and AlexNet models. We passed a pruning option to ctures-for example, mRNA mapper <ref type=\"bibr\" target=\"#b56\">[57]</ref> for the MAERI accelerator <ref type=\"bibr\" target=\"#b25\">[26]</ref> and Auto-TVM <ref type=\"bibr\" target=\"#b5\">[6]</ref> for t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erators fixed certain aspects of the mapping space such as choice of parallel loops and loop orders <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta  than the on-chip data movement <ref type=\"bibr\" target=\"#b7\">[8]</ref>. In contrast to prior works <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta der-because they contribute to parallelization and on-chip data movement. In contrast to prior work <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta re factors of loop bounds). The preceding strategy is used in all the prior mapping space explorers <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta op bounds evenly without any remainder, and this has been the consideration in the other approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta stically change the latency or energy-efficiency numbers. Recently, multiple analytical cost models <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" ta ormance comparison of Marvel generated mappings with the mappings of the dMazeRunner-like optimizer <ref type=\"bibr\" target=\"#b11\">[12]</ref> and Interstellar-like optimizer <ref type=\"bibr\" target=\"# of recent optimizers such as Interstellar <ref type=\"bibr\" target=\"#b53\">[54]</ref> and dMazeRunner <ref type=\"bibr\" target=\"#b11\">[12]</ref> in our framework. For instance, the Interstellar optimizer ibr\" target=\"#b53\">[54]</ref> optimizer generated mappings, (2) Marvel implemented dMazeRunner-like <ref type=\"bibr\" target=\"#b11\">[12]</ref> optimizer generated mappings, and (3) roof-line peak based pe=\"figure\">14</ref>. Comparison of Marvel generated mappings with the mappings of dMazeRunner-like <ref type=\"bibr\" target=\"#b11\">[12]</ref> and Interstellar-like optimizers <ref type=\"bibr\" target=\" nests, and many cost models such as Timeloop <ref type=\"bibr\" target=\"#b33\">[34]</ref>, DMazeRunner <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and Interstellar <ref type=\"bibr\" target=\"#b53\">[54]</ref type=\"bibr\" target=\"#b28\">[29]</ref>, Auto-TVM <ref type=\"bibr\" target=\"#b5\">[6]</ref>, dMazeRunner <ref type=\"bibr\" target=\"#b11\">[12]</ref>, Interstellar <ref type=\"bibr\" target=\"#b53\">[54]</ref>, a f type=\"bibr\" target=\"#b28\">[29]</ref>, Auto-TVM<ref type=\"bibr\" target=\"#b5\">[6]</ref>, dMazeRunner<ref type=\"bibr\" target=\"#b11\">[12]</ref>, Interstellar<ref type=\"bibr\" target=\"#b53\">[54]</ref>, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ies (e.g., mRNA <ref type=\"bibr\" target=\"#b56\">[57]</ref> for the MAERI accelerator, TVM extensions <ref type=\"bibr\" target=\"#b29\">[30]</ref> for the VTA GEMM accelerator, and DeepTools <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  operator on average for mapping ResNet50 <ref type=\"bibr\" target=\"#b16\">[17]</ref> and MobileNetV2 <ref type=\"bibr\" target=\"#b42\">[43]</ref> on a representative DNN edge accelerator. On one side, muc bibr\" target=\"#b47\">[48]</ref>, ResNet50 <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and MobileNetV2 <ref type=\"bibr\" target=\"#b42\">[43]</ref>. We assumed a batch size of one because this assumption ca =\"bibr\" target=\"#b47\">[48]</ref>, ResNet-50 <ref type=\"bibr\" target=\"#b16\">[17]</ref>, MobileNet-V2 <ref type=\"bibr\" target=\"#b42\">[43]</ref> models on platforms P1 and P2.</p><p>these operators have . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he DB model was modeled as a geometric program, transformed then into a convex optimization problem <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, and further solve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  GPU-based scientific and other general-purpose applications. <ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6</ref> Figure <ref type=\"figure\" target=\"#fig_0\">1</ref> shows a die . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: er of GPUs, from a single one to a 4,480 GPU cluster like Selene. Legate is built on top of Legion, <ref type=\"bibr\" target=\"#b18\">19</ref> a runtime system that manages both the task-graph of a multi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and abstractions for programming them become available. For example, the Legate programming system <ref type=\"bibr\" target=\"#b17\">18</ref> allows Numpy programs to be run on any number of GPUs, from . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g systems were originally developed for dedicated stream processors such as Imagine 1 and Merrimac. <ref type=\"bibr\" target=\"#b1\">2</ref> These systems, however, were ideally suited to GPU programming. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #1 position with a Linpack performance of 122.3 PFLOPS, powered by 27,648 NVIDIA Volta (V100) GPUs. <ref type=\"bibr\" target=\"#b7\">8</ref> Six of the top ten machines on the June, 2021 Top 500 list wer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he Brook programming language, originally developed for Merrimac, was adapted to GPUs as Brook-GPU. <ref type=\"bibr\" target=\"#b2\">3</ref> Brook was later adopted by AMD as the language used to program. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tly reduced the barriers to developing GPU-based scientific and other general-purpose applications. <ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6</ref> Figure <ref type=\"figure. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2020 and shown in Figure <ref type=\"figure\" target=\"#fig_3\">4</ref>, is the most recent NVIDIA GPU. <ref type=\"bibr\" target=\"#b14\">15</ref> Ampere is built using a 7-nm technology and includes 54 bill. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er of GPUs, from a single one to a 4,480 GPU cluster like Selene. Legate is built on top of Legion, <ref type=\"bibr\" target=\"#b18\">19</ref> a runtime system that manages both the task-graph of a multi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: while it sniffs the packets for sensitive information or causes large scale redirection of the data <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>BGP attacks can compromise the end user's confidentia. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in two hidden layers whose number of nodes was determined by using the optimal formula as stated in <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Accordingly, the number of nodes in the first and second . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  application of machine learning techniques (mainly feature selection) is presented by Zhida et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Various supervised <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">[9]</ref>. Various supervised <ref type=\"bibr\" target=\"#b9\">[10]</ref> and unsupervised <ref type=\"bibr\" target=\"#b10\">[11]</ref> machine learning models have been introduced to detect the lysis for optimizing and fine-tuning the models. Finally, we compare our results with related works <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref> in detecting kn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  application of machine learning techniques (mainly feature selection) is presented by Zhida et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Various supervised <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dels. Finally, we compare our results with related works <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref> in detecting known BGP attacks like Slammer, Code Red, and adratic kernel, which resulted in an accuracy of 91.31% over the top 5 features. When compared with <ref type=\"bibr\" target=\"#b17\">[18]</ref>, there is a slight deviation in the results. They conclude ategory, the linear kernel outperforms all the others. This is in line with the results achieved by <ref type=\"bibr\" target=\"#b17\">[18]</ref>, who attributed this observation to the fact that the inpu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ts with a 6-way BGP handshake, including the Open, Keep Alive, and an Update message from each side <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The records of all the BGP messages over various routes are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvices, which caused service disruptions at a global level <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. It is important to detect BGP anomalies in a timely manner . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rvices, which caused service disruptions at a global level <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>. It is important to detect BGP anomalies in a timely manner . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b8\">[9]</ref> and RPKI <ref type=\"bibr\" target=\"#b7\">[8]</ref> have not been deployed globally <ref type=\"bibr\" target=\"#b11\">[12]</ref> and they bring meager security in partial deployment <ref  tures and sign their signatures <ref type=\"bibr\" target=\"#b12\">[13]</ref>. According to a survey in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, compared with proactive defense mechanisms, reactive defe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: AS relationship dataset collected from CAIDA <ref type=\"bibr\" target=\"#b23\">[24]</ref> and Problink <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which is mainly inferred from the BGP table snapshots col  from the CAIDA Relationship dataset <ref type=\"bibr\" target=\"#b23\">[24]</ref> and Problink dataset <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The CAIDA Relationship dataset is generated from the publ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o be inaccessible for 2 h worldwide <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Moreover, the study in <ref type=\"bibr\" target=\"#b3\">[4]</ref> also demonstrates that BGP prefix hijacking can be used to a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ally <ref type=\"bibr\" target=\"#b11\">[12]</ref> and they bring meager security in partial deployment <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For example, BGPsec cannot secure a routing path if there e BGPsec requires each AS of the path to validate all previous signatures and sign their signatures <ref type=\"bibr\" target=\"#b12\">[13]</ref>. According to a survey in <ref type=\"bibr\" target=\"#b11\">[ simulation in the real network (e.g., it may affect the normal routing of the Internet), similar to <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1.0\"><head>Routing policy. A well-known AS business relationship model -Gao-</head><p>Rexford model <ref type=\"bibr\" target=\"#b27\">[28]</ref> is used to simulate the BGP routing policy. First, assumin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ally <ref type=\"bibr\" target=\"#b11\">[12]</ref> and they bring meager security in partial deployment <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For example, BGPsec cannot secure a routing path if there e BGPsec requires each AS of the path to validate all previous signatures and sign their signatures <ref type=\"bibr\" target=\"#b12\">[13]</ref>. According to a survey in <ref type=\"bibr\" target=\"#b11\">[ simulation in the real network (e.g., it may affect the normal routing of the Internet), similar to <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures due to the Internet flattening and connect more networks to enrich their network reachability <ref type=\"bibr\" target=\"#b33\">[34]</ref>, which also makes them occupy a decisive position in mitig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rnet), similar to <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref>, a suitable BGP r attacker hijacks the prefix announced by the true origin AS (victim). The BGP simulator proposed in <ref type=\"bibr\" target=\"#b21\">[22]</ref> is used for computing equally preferred paths under the ab ed for computing equally preferred paths under the above routing policy. But different from work in <ref type=\"bibr\" target=\"#b21\">[22]</ref> that uses AS nodes as the destination and computes paths f -6 and AS-7 to true origin AS-1).</p><p>The key of the hijacking and mitigation simulation based on <ref type=\"bibr\" target=\"#b21\">[22]</ref> is to build a routing tree \ud835\udc47 , which contains all equally   = \u2205 for \u2200\ud835\udc51 \u2208 \ud835\udc49 do \ud835\udc36 = \u2205 \ud835\udc60 = 0 for \u2200\ud835\udc63 \u2208 \ud835\udc49 \u2212 {\ud835\udc51} do</formula><p>Calculate the path from \ud835\udc63 to \ud835\udc51 using <ref type=\"bibr\" target=\"#b21\">[22]</ref> if \ud835\udc63 can reach the \ud835\udc51 with \u210e hops and \u210e &gt; 0 then</p><for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o be inaccessible for 2 h worldwide <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Moreover, the study in <ref type=\"bibr\" target=\"#b3\">[4]</ref> also demonstrates that BGP prefix hijacking can be used to a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acker propagates a valid route beyond the scope intended by the routing policy of the ASes involved <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In this paper, we mainly consider the BGP prefix hijacking.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng. A few recent studies have pretrained deep neural language models on protein sequences (e.g. ESM <ref type=\"bibr\" target=\"#b19\">[20]</ref>, TAPE-Transformer [21], UDSMProt <ref type=\"bibr\" target=\" mer [21], ~110M in BERT-base <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and ~650M in the ESM-1b model <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>The ProteinBERT architecture has several appealing  te), even after multiple epochs (Fig. <ref type=\"figure\">2</ref>), in accordance with other studies <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The GO annotations task, on the other hand, does show sat  biochemical roles).</p><p>Unlike previous works which included ~250M putative, redundant sequences <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we constrained the pretraining of ProteinBERT to ~106M re oved model performance <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Thus, we expect larger versions of ProteinBERT (e.g. with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  from UniProtKB/UniRef90, covering the entire tree of life <ref type=\"bibr\" target=\"#b26\">[28,</ref><ref type=\"bibr\" target=\"#b27\">29]</ref>. UniRef90 provides a non-redundant set of protein clusters   we constrained the pretraining of ProteinBERT to ~106M representative proteins taken from UniRef90 <ref type=\"bibr\" target=\"#b27\">[29]</ref>, out of the entire known protein space of ~215M proteins i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  was pretrained on ~106M proteins derived from UniProtKB/UniRef90, covering the entire tree of life <ref type=\"bibr\" target=\"#b26\">[28,</ref><ref type=\"bibr\" target=\"#b27\">29]</ref>. UniRef90 provides \"bibr\" target=\"#b27\">[29]</ref>, out of the entire known protein space of ~215M proteins in UniProt <ref type=\"bibr\" target=\"#b26\">[28]</ref>. We argue that using a non-redundant set of proteins is mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ds developed for natural language and other sequences are a natural fit to predictive protein tasks <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>Modern deep neural network architectures specifically roteins, which, despite many structural similarities, have different properties from human language <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Most notably, prote. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is day, protein research is still dominated by classical sequence-similarity methods (such as BLAST <ref type=\"bibr\" target=\"#b17\">[18]</ref> and hidden Markov models <ref type=\"bibr\" target=\"#b18\">[1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eam tasks of interest <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. Assuming that the pretraining and downstream tasks are some. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ne chat, online courses, etc. According to the types of input signals, there are text-driven (e.g., <ref type=\"bibr\" target=\"#b0\">[1]</ref>), audio-driven (e.g, <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  signals, there are text-driven (e.g., <ref type=\"bibr\" target=\"#b0\">[1]</ref>), audio-driven (e.g, <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>) and video-driven eneration tasks, using landmarks and 3D models as priors is useful to fuse multi-modal input (e.g., <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" t oduce a talking face video, with focus on fine talking details in head or upper body of the speaker <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" ior from audio and using it to generate talking face video have been used in some practical methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" t \">[11]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>) or use facial landmarks as 2D prior (e.g., <ref type=\"bibr\" target=\"#b1\">[2]</ref>). As shown in Figure <ref type=\"figure\" target=\"#fig_0\">1b</  in Figure <ref type=\"figure\">5</ref>.</p><p>As a comparison, other state-of-the-art methods (e.g., <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t  the results with the ground-truth videos, after aligning them according to the way used in ATVGnet <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We use Peak Signal to Noise Ratio (PSNR) and Structural Sim Arts</head><p>In this section, we compare our model with state-of-theart methods, including ATVGnet <ref type=\"bibr\" target=\"#b1\">[2]</ref>, You Said That <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Wav2 \"bibr\" target=\"#b8\">[9]</ref>. We first introduce and discuss these methods.</p><p>ATVGnet. ATVGnet <ref type=\"bibr\" target=\"#b1\">[2]</ref> generates talking-face video in real time from input photo a DA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGnet <ref type=\"bibr\" target=\"#b1\">[2]</ref> and Yi's Method <ref type=\"bibr\" target=\"#b8\">[9]</ref>) and f type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>,</p><p>ATVGNET <ref type=\"bibr\" target=\"#b1\">[2]</ref> AND YI'S METHOD <ref type=\"bibr\" target=\"#b8\">[9]</ref>). ne DA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGnet <ref type=\"bibr\" target=\"#b1\">[2]</ref> and Yi's Method <ref type=\"bibr\" target=\"#b8\">[9]</ref>) and DA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGnet <ref type=\"bibr\" target=\"#b1\">[2]</ref> and Yi's Method <ref type=\"bibr\" target=\"#b8\">[9]</ref>) are DA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGNET <ref type=\"bibr\" target=\"#b1\">[2]</ref> AND YI'S METHOD <ref type=\"bibr\" target=\"#b8\">[9]</ref>). YO , SDA<ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS<ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGnet<ref type=\"bibr\" target=\"#b1\">[2]</ref> and Yi's Method<ref type=\"bibr\" target=\"#b8\">[9]</ref>) and  LSO CANNOT GENERATE RESULTS WITH BACKGROUND. For generating a video with the same input, (1) ATVGnet<ref type=\"bibr\" target=\"#b1\">[2]</ref> takes 1.07s to generate video without background, (2) You Sa g-face system: (1) Real time: the video can be generated online when the audio signal is available; <ref type=\"bibr\" target=\"#b1\">(2)</ref> High quality: the quality of generated video should be good . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hange. In contrast, our DCK will change with different inputs. We use the pre-trained audio network <ref type=\"bibr\" target=\"#b25\">[26]</ref> to extract audio features and train a fully connected netw orating dynamic convolution kernels as our generative network. We use the pre-trained audio network <ref type=\"bibr\" target=\"#b25\">[26]</ref> to extract audio features and train a fully connected netw  \u0398 d (f ) based on features of the input audio. We (1) use the pre-trained audio network in Wav2Lip <ref type=\"bibr\" target=\"#b25\">[26]</ref>, which consists of 2D convolutional layers and residual bl el></formula><p>where A is the Mel Spectrogram of input audio, h 1 is the pre-trained audio network <ref type=\"bibr\" target=\"#b25\">[26]</ref> and h 2 is the fully connected network. We reshape the out ion task whose input includes both audio and video. We use the pre-trained audio network in Wav2Lip <ref type=\"bibr\" target=\"#b25\">[26]</ref> to extract audio features from Mel Spectrogram of input au and pairs them with real videos to build this kind of dataset. Some talking face generation methods <ref type=\"bibr\" target=\"#b25\">[26]</ref> can generate a talking-face video from a reference video a the generated video has the same identity and head motion as the reference video. We use the method <ref type=\"bibr\" target=\"#b25\">[26]</ref> to generate a new training dataset by the following steps: ths are about 60 seconds; (2) for each real video V 0 i , i = 1, 2, . . . , N r , we use the method <ref type=\"bibr\" target=\"#b25\">[26]</ref> to generate</p><formula xml:id=\"formula_3\">m videos V 1 i  #b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bib ound. Therefore, they cannot adopt our directly covering scheme as efficient as ours; e.g., Wav2Lip <ref type=\"bibr\" target=\"#b25\">[26]</ref> also generates the facial region and uses the direct cover d 5 up-sampling layers, where all middle layers are with DCKs. We use the pre-trained audio network <ref type=\"bibr\" target=\"#b25\">[26]</ref>, which consists of 2D convolutional layers and residual bl atures from Mel Spectrogram of input audio, where the parameters of Mel Spectrogram are the same as <ref type=\"bibr\" target=\"#b25\">[26]</ref>. For each layer with a dynamic convolution kernel, we trai  type=\"bibr\" target=\"#b1\">[2]</ref>, You Said That <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Wav2Lip <ref type=\"bibr\" target=\"#b25\">[26]</ref>, X2Face <ref type=\"bibr\" target=\"#b5\">[6]</ref>, DAVS <ref resolution of its generation results is 112 \u00d7 112, which is lower than ours.</p><p>Wav2Lip. Wav2Lip <ref type=\"bibr\" target=\"#b25\">[26]</ref> generates talking-face video in real time from input photo ype=\"figure\" target=\"#fig_0\">10</ref>. The qualitative comparison between the STOA methods (Wav2Lip <ref type=\"bibr\" target=\"#b25\">[26]</ref>, X2Face <ref type=\"bibr\" target=\"#b5\">[6]</ref>, You said  ary of the cropped face so results of DAVS are talking-face videos without background. It also has  <ref type=\"bibr\" target=\"#b25\">[26]</ref>, X2FACE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, YOU SAID  -tuning.</p><p>Qualitative Comparison. The qualitative comparison between the STOA methods (Wav2Lip <ref type=\"bibr\" target=\"#b25\">[26]</ref>, X2Face <ref type=\"bibr\" target=\"#b5\">[6]</ref>, You said   of lip movement. The results of quantitative comparison between ours and the SOTA methods (Wav2Lip <ref type=\"bibr\" target=\"#b25\">[26]</ref>, X2Face <ref type=\"bibr\" target=\"#b5\">[6]</ref>, You said   video with 256 \u00d7 256 resolution and it takes 2.62s in total in generating a video with 1280 \u00d7 720  <ref type=\"bibr\" target=\"#b25\">[26]</ref>, X2FACE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, YOU SAID  .</head><label></label><figDesc>Fig.10. The qualitative comparison between the STOA methods (Wav2Lip<ref type=\"bibr\" target=\"#b25\">[26]</ref>, X2Face<ref type=\"bibr\" target=\"#b5\">[6]</ref>, You said t to generate video with background, which also takes about 1 hour to fine-tune their network. Wav2Lip<ref type=\"bibr\" target=\"#b25\">[26]</ref> takes 1.58s to generate video without background and takes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"figure\">3</ref>). For a video with background, we crop the facial area (detected by Dlib <ref type=\"bibr\" target=\"#b45\">[46]</ref>) from the video and resize it to 256 \u00d7 256 as the input of he input photo and aligns it by affine transformation based on facial landmarks extracted from Dlib <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Because the alignment operation changes and fixes the vie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b0\">[1]</ref>), audio-driven (e.g, <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>) and video-driven (e.g., <ref type=\"bibr\" target=\"#b5\">[6] odal input (e.g., <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>). However, as w  fine talking details in head or upper body of the speaker <ref type=\"bibr\" target=\"#b1\">[2]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. It is a typica practical methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. However, it is d strategy is to use a parametric model as 3D prior (e.g., <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>) or use facial  =\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bib ds. In recent years, several rendering-based methods (e.g. <ref type=\"bibr\" target=\"#b8\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>) for generating. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: udio (i.e., of visual and auditory modalities) as input. The two modalities are strongly correlated <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and thus it is possible to drive the talking-face video u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onvolutional network with kernels {k 1 , \u2022 \u2022 \u2022 , k l }. The perceptual loss, including the VGG loss <ref type=\"bibr\" target=\"#b42\">[43]</ref> and LPIPS loss <ref type=\"bibr\" target=\"#b43\">[44]</ref>, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[11]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref>) either cannot keep the head pose or change the non-face a , X2Face <ref type=\"bibr\" target=\"#b5\">[6]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, SDA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, Yi's Method <ref type=\"bibr\" target=\"#b8\">[9]</ref>. We f <ref type=\"bibr\" target=\"#b5\">[6]</ref>, You said that <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SDA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGnet <re <ref type=\"bibr\" target=\"#b5\">[6]</ref>, YOU SAID THAT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SDA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>,</p><p>ATVGN D YI'S METHOD <ref type=\"bibr\" target=\"#b8\">[9]</ref>). neither head motion nor eye blink. SDA. SDA <ref type=\"bibr\" target=\"#b40\">[41]</ref> generates talking-face video from input video and audio us <ref type=\"bibr\" target=\"#b5\">[6]</ref>, You said that <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SDA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGnet <re <ref type=\"bibr\" target=\"#b5\">[6]</ref>, You said that <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SDA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGnet <re <ref type=\"bibr\" target=\"#b5\">[6]</ref>, YOU SAID THAT <ref type=\"bibr\" target=\"#b2\">[3]</ref>, SDA <ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGNET <re ce<ref type=\"bibr\" target=\"#b5\">[6]</ref>, You said that<ref type=\"bibr\" target=\"#b2\">[3]</ref>, SDA<ref type=\"bibr\" target=\"#b40\">[41]</ref>, DAVS<ref type=\"bibr\" target=\"#b4\">[5]</ref>, ATVGnet<ref  ef type=\"bibr\" target=\"#b4\">[5]</ref> takes 17.05s to generate video without background, and (5) SDA<ref type=\"bibr\" target=\"#b40\">[41]</ref> takes 5.17s to generate video without background. Yi's met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  literature, a few dynamic CNN parameter methods existed <ref type=\"bibr\" target=\"#b20\">[21]</ref>- <ref type=\"bibr\" target=\"#b24\">[25]</ref>. However, they were all proposed for processing single mod minant in most research, there exist adaptions of CNNs ( <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b24\">[25]</ref>) whose kernels can be dynamically adjusted. However, all t ely set up to 10 4 parameters, which are more powerful and suitable for multi-modal input. The work <ref type=\"bibr\" target=\"#b24\">[25]</ref> is similar to <ref type=\"bibr\" target=\"#b21\">[22]</ref> in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: udio (i.e., of visual and auditory modalities) as input. The two modalities are strongly correlated <ref type=\"bibr\" target=\"#b17\">[18]</ref>, and thus it is possible to drive the talking-face video u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b27\">[28]</ref> is a classical algorithm for multi-sensor fusion. Bayesian inference <ref type=\"bibr\" target=\"#b28\">[29]</ref> is another classic technique to fuse different features. F. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the same resolution H \u00d7 W as the image), and use the spatial attention fusion module of the method <ref type=\"bibr\" target=\"#b46\">[47]</ref> to fuse audio features and image features. As shown in Fig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"figure\">3</ref>). For a video with background, we crop the facial area (detected by Dlib <ref type=\"bibr\" target=\"#b45\">[46]</ref>) from the video and resize it to 256 \u00d7 256 as the input of he input photo and aligns it by affine transformation based on facial landmarks extracted from Dlib <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Because the alignment operation changes and fixes the vie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent features. For full details of existing fusion methods, the reader is referred to recent surveys <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref> and references . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>For CNNs, although fixed kernels are dominant in most research, there exist adaptions of CNNs ( <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b24\">[25]</ref>) whose kernels  lly adjusted. However, all these methods can only handle single mode information as input. The work <ref type=\"bibr\" target=\"#b21\">[22]</ref> is designed for a classification task, which estimates a s nd suitable for multi-modal input. The work <ref type=\"bibr\" target=\"#b24\">[25]</ref> is similar to <ref type=\"bibr\" target=\"#b21\">[22]</ref> in the spirit of using dynamic weights to adjust the linea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o-driven (e.g., <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b16\">[17]</ref>) talking-face systems. In this paper, we propose an audiod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the same resolution H \u00d7 W as the image), and use the spatial attention fusion module of the method <ref type=\"bibr\" target=\"#b46\">[47]</ref> to fuse audio features and image features. As shown in Fig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"figure\">3</ref>). For a video with background, we crop the facial area (detected by Dlib <ref type=\"bibr\" target=\"#b45\">[46]</ref>) from the video and resize it to 256 \u00d7 256 as the input of he input photo and aligns it by affine transformation based on facial landmarks extracted from Dlib <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Because the alignment operation changes and fixes the vie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sible way is to use landmark points or parametric models <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref> as a prior, which can be inferred from the audio sequence.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b19\">[20]</ref> or metric learning model <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>, and jointly optimi ructure learning, while the relations between nodes far away from them are rarely discovered by GSL <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Such imbalance leads to the bias of edge distribution, af For large-scale graphs, we perform the kNN sparsification with its locality-sensitive approximation <ref type=\"bibr\" target=\"#b10\">[11]</ref> where the nearest neighbors are selected from a batch of n 2) For sparsification post-processing, we consider a locality-sensitive approximation for kNN graph <ref type=\"bibr\" target=\"#b10\">[11]</ref>. (3) For graph contrastive learning, we compute the contra N <ref type=\"bibr\" target=\"#b44\">[45]</ref>, IDGL <ref type=\"bibr\" target=\"#b6\">[7]</ref> and SLAPS <ref type=\"bibr\" target=\"#b10\">[11]</ref>). We also consider GDC <ref type=\"bibr\" target=\"#b22\">[23] >[23]</ref>, a diffusion-based graph structure improvement method, and SLAPS-2s, a variant of SLAPS <ref type=\"bibr\" target=\"#b10\">[11]</ref> which only uses denoising autoencoder to learn topology, a h learners, we consider the complexities with locality-sensitive kNN sparsification post-processing <ref type=\"bibr\" target=\"#b10\">[11]</ref> where the neighbors are selected from a batch of nodes (ba earning functions like cosine similarity <ref type=\"bibr\" target=\"#b6\">[7]</ref> and dot production <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. Besides, directly ly treating each element in adjacency matrix as a learnable parameter is also an effective solution <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Nevertheless, the </p><p>FGP learner directly models each element of the adjacency matrix by an independent parameter <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta uilt on features, since it is an effective way to provide a starting point for GSL, as suggested in <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. Specifically, for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b39\">40]</ref>, link prediction <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, and node clustering <ref type=\"bibr\" target=\"#b41\">[42,</r  has shown competitive performance and become increasingly popular in graph representation learning <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref> and node v.s. subgraph <ref type=\"bibr\" target=\"#b31\">[32]</ref>). Graph contrastive learning also benefits diverse applica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GAE <ref type=\"bibr\" target=\"#b20\">[21]</ref>, ARGA <ref type=\"bibr\" target=\"#b29\">[30]</ref>, MGAE <ref type=\"bibr\" target=\"#b42\">[43]</ref>, AGC <ref type=\"bibr\" target=\"#b54\">[55]</ref> and DAEGC <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d n=\"2.3\">Contrastive Learning on Graphs</head><p>After achieving significant performance in visual <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and linguistic <ref  oring richer underlying semantic information by making the learning tasks more challenging to solve <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targ I between them. In SUBLIME, we adopt a simple contrastive learning framework originated from SimCLR <ref type=\"bibr\" target=\"#b5\">[6]</ref> which consists of the following components: GNN-based encode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b6\">[7]</ref> and SLAPS <ref type=\"bibr\" target=\"#b10\">[11]</ref>). We also consider GDC <ref type=\"bibr\" target=\"#b22\">[23]</ref>, a diffusion-based graph structure improvement method, and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h topology with GNNs <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>. Concretely, these  type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>, full parameterization <ref type=\"bibr\" target=\"#b19\">[20]</ref> or metric learning model <ref type=\"bibr\" target=\"#b6\">[7,  <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GRCN <ref type=\"bibr\" target=\"#b52\">[53]</ref>, Pro-GNN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GEN <ref type=\"bibr\" target=\"#b44\">[45]</ref>, IDGL <ref  ck intensities. We compared our method to GCN <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Pro-GNN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, a supervised graph structure method for graph adversarial rix as a learnable parameter is also an effective solution <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Nevertheless, the existing deep GSL approaches follow a su ost existing methods <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> adopt a single strategy to model graph structure, which can ependent parameter <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> without any extra input. Formally, FGP learner is defined a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and linguistic <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> domains, contrastive learning has shown competitive perform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ieved state-of-the-art performance in various graph analytical tasks, including node classification <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, link prediction < ype of deep neural networks aiming to learn low-dimensional representations for graphstructure data <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Modern GNNs can b ilter <ref type=\"bibr\" target=\"#b8\">[9]</ref> and the first-order approximation of Chebyshev filter <ref type=\"bibr\" target=\"#b21\">[22]</ref>. The spatial methods perform convolution operation by prop ogy, GNN learner is only used for the structure refinement task. For simplicity, we take GCN layers <ref type=\"bibr\" target=\"#b21\">[22]</ref> to form embedded network:</p><formula xml:id=\"formula_5\">! the node representation matrices for learner/anchor views, respectively. In SUBLIME, we utilize GCN <ref type=\"bibr\" target=\"#b21\">[22]</ref> as our encoder and set its layer number \ud835\udc3f 1 to 2. MLP-base pare SUBLIME with two categories of methods, including three structure-fixed GNN methods (i.e., GCN <ref type=\"bibr\" target=\"#b21\">[22]</ref>, GAT <ref type=\"bibr\" target=\"#b39\">[40]</ref> and GraphSA odified edges from 0 to 0.9 to simulate different attack intensities. We compared our method to GCN <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Pro-GNN <ref type=\"bibr\" target=\"#b19\">[20]</ref>, a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rform convolution operation by propagating and aggregating local information along edges in a graph <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" ta ferent aggregation functions are designed to learn node representations, including mean/max pooling <ref type=\"bibr\" target=\"#b14\">[15]</ref>, LSTM <ref type=\"bibr\" target=\"#b14\">[15]</ref>, self-atte rn node representations, including mean/max pooling <ref type=\"bibr\" target=\"#b14\">[15]</ref>, LSTM <ref type=\"bibr\" target=\"#b14\">[15]</ref>, self-attention <ref type=\"bibr\" target=\"#b39\">[40]</ref>, get=\"#b21\">[22]</ref>, GAT <ref type=\"bibr\" target=\"#b39\">[40]</ref> and GraphSAGE (SAGE for short) <ref type=\"bibr\" target=\"#b14\">[15]</ref>), and six supervised GSL methods (i.e., LDS <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b33\">[34]</ref>, Pubmed <ref type=\"bibr\" target=\"#b26\">[27]</ref> and ogbn-arxiv <ref type=\"bibr\" target=\"#b16\">[17]</ref>) and four non-graph datasets (i.e., Wine, Cancer, Digits a re papers about three diabete types about diabetes and edges are citations among them. \u2022 ogbn-arxiv <ref type=\"bibr\" target=\"#b16\">[17]</ref> is a citation network with Computer Science arXiv papers. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>] and graph-level <ref type=\"bibr\" target=\"#b51\">[52]</ref>) or different scales (i.e., node v.s. graph <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ask called KG completion to discover missing facts for a KG (see surveys <ref type=\"bibr\">[18,</ref><ref type=\"bibr\" target=\"#b54\">29,</ref><ref type=\"bibr\" target=\"#b66\">41]</ref>). The majority of r mainly use the embedding techniques to complete missing relational facts <ref type=\"bibr\">[18,</ref><ref type=\"bibr\" target=\"#b54\">29,</ref><ref type=\"bibr\" target=\"#b66\">41]</ref>. There are three ge  FACT SCORING</head><p>Inspired by embedding-based KG completion methods <ref type=\"bibr\">[18,</ref><ref type=\"bibr\" target=\"#b54\">29,</ref><ref type=\"bibr\" target=\"#b66\">41]</ref>, we encode the know. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sing facts for a KG (see surveys <ref type=\"bibr\">[18,</ref><ref type=\"bibr\" target=\"#b54\">29,</ref><ref type=\"bibr\" target=\"#b66\">41]</ref>). The majority of recent works focus on learning the embedd omplete missing relational facts <ref type=\"bibr\">[18,</ref><ref type=\"bibr\" target=\"#b54\">29,</ref><ref type=\"bibr\" target=\"#b66\">41]</ref>. There are three general types of methods: geometric models ding-based KG completion methods <ref type=\"bibr\">[18,</ref><ref type=\"bibr\" target=\"#b54\">29,</ref><ref type=\"bibr\" target=\"#b66\">41]</ref>, we encode the knowledge in a KG through representing its e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: KG completion methods <ref type=\"bibr\">[7,</ref><ref type=\"bibr\">14,</ref><ref type=\"bibr\">26,</ref><ref type=\"bibr\" target=\"#b58\">33,</ref><ref type=\"bibr\" target=\"#b60\">35]</ref> attempt to mine fac KG completion methods <ref type=\"bibr\">[7,</ref><ref type=\"bibr\">14,</ref><ref type=\"bibr\">26,</ref><ref type=\"bibr\" target=\"#b58\">33,</ref><ref type=\"bibr\" target=\"#b60\">35]</ref> can mine new facts  rely interfere the quality of discovered facts. Several existing methods <ref type=\"bibr\">[14,</ref><ref type=\"bibr\" target=\"#b58\">33,</ref><ref type=\"bibr\" target=\"#b60\">35]</ref> focus on adding new  external knowledge, open KG completion methods <ref type=\"bibr\">[7,</ref><ref type=\"bibr\">14,</ref><ref type=\"bibr\" target=\"#b58\">33,</ref><ref type=\"bibr\" target=\"#b60\">35]</ref> can find new facts   approach. Generally, conventional KG completion methods cannot complete facts with unseen entities <ref type=\"bibr\" target=\"#b58\">[33,</ref><ref type=\"bibr\" target=\"#b60\">35]</ref>.</p><p>Open KG com mbines entity embeddings and relation embeddings to judge the plausibility of relational facts. OWE <ref type=\"bibr\" target=\"#b58\">[33]</ref> extends the conventional KG completion methods, and trains RN <ref type=\"bibr\">[15]</ref>. (iii) Two open KG completion methods supporting external texts, OWE <ref type=\"bibr\" target=\"#b58\">[33]</ref> and Con-Mask <ref type=\"bibr\" target=\"#b60\">[35]</ref>. We. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: enshtein edit distance. (ii) Two deep text matching models, ESIM <ref type=\"bibr\">[9]</ref> and RE2 <ref type=\"bibr\" target=\"#b73\">[48]</ref>. (iii) Two deep entity matching models based on literals, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\">[10,</ref><ref type=\"bibr\">11,</ref><ref type=\"bibr\">16,</ref><ref type=\"bibr\">25,</ref><ref type=\"bibr\" target=\"#b57\">32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" tar  type=\"bibr\">10,</ref><ref type=\"bibr\">11,</ref><ref type=\"bibr\">16,</ref><ref type=\"bibr\">25,</ref><ref type=\"bibr\" target=\"#b57\">32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" tar =\"bibr\" target=\"#b67\">42]</ref>. Inspired by the recent achievement of GNNs in KG completion (e.g., <ref type=\"bibr\" target=\"#b57\">[32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" ta g entities, especially within two hops, are often critical to discover the missing attribute values <ref type=\"bibr\" target=\"#b57\">[32,</ref><ref type=\"bibr\" target=\"#b61\">36]</ref>. For example, two . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nment</head><p>A string value may also refer to a real-world object. Current entity linking methods <ref type=\"bibr\" target=\"#b56\">[31,</ref><ref type=\"bibr\" target=\"#b69\">44]</ref> can link texts to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Freebase <ref type=\"bibr\">[4]</ref>, Wikidata <ref type=\"bibr\" target=\"#b65\">[40]</ref> and Probase <ref type=\"bibr\" target=\"#b70\">[45]</ref>, are quite large; however, they are still acknowledged as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e data source provides correct claims, and infer truths based on the high-quality data sources. LTM <ref type=\"bibr\" target=\"#b75\">[50]</ref>, LCA <ref type=\"bibr\" target=\"#b53\">[28]</ref>, MBM <ref t n-based method, CATD <ref type=\"bibr\">[21]</ref>, and (iv) four probabilistic graphical models, LTM <ref type=\"bibr\" target=\"#b75\">[50]</ref>, LCA <ref type=\"bibr\" target=\"#b53\">[28]</ref>, MBM <ref t get=\"#b52\">27,</ref><ref type=\"bibr\" target=\"#b53\">28,</ref><ref type=\"bibr\" target=\"#b68\">43,</ref><ref type=\"bibr\" target=\"#b75\">50]</ref>, the confusion probability makes sophisticated comparison f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ome this issue, previous studies <ref type=\"bibr\">[21,</ref><ref type=\"bibr\" target=\"#b52\">27,</ref><ref type=\"bibr\" target=\"#b74\">49]</ref> leverage the quality of a data source to represent the abil  majority voting <ref type=\"bibr\" target=\"#b76\">[51]</ref>, (ii) two iterative methods, TruthFinder <ref type=\"bibr\" target=\"#b74\">[49]</ref> and PooledInvestment <ref type=\"bibr\" target=\"#b52\">[27]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\">[10,</ref><ref type=\"bibr\">11,</ref><ref type=\"bibr\">16,</ref><ref type=\"bibr\">25,</ref><ref type=\"bibr\" target=\"#b57\">32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" tar  type=\"bibr\">10,</ref><ref type=\"bibr\">11,</ref><ref type=\"bibr\">16,</ref><ref type=\"bibr\">25,</ref><ref type=\"bibr\" target=\"#b57\">32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" tar =\"bibr\" target=\"#b67\">42]</ref>. Inspired by the recent achievement of GNNs in KG completion (e.g., <ref type=\"bibr\" target=\"#b57\">[32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" ta g entities, especially within two hops, are often critical to discover the missing attribute values <ref type=\"bibr\" target=\"#b57\">[32,</ref><ref type=\"bibr\" target=\"#b61\">36]</ref>. For example, two . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\">[10,</ref><ref type=\"bibr\">11,</ref><ref type=\"bibr\">16,</ref><ref type=\"bibr\">25,</ref><ref type=\"bibr\" target=\"#b57\">32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" tar  type=\"bibr\">10,</ref><ref type=\"bibr\">11,</ref><ref type=\"bibr\">16,</ref><ref type=\"bibr\">25,</ref><ref type=\"bibr\" target=\"#b57\">32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" tar =\"bibr\" target=\"#b67\">42]</ref>. Inspired by the recent achievement of GNNs in KG completion (e.g., <ref type=\"bibr\" target=\"#b57\">[32,</ref><ref type=\"bibr\" target=\"#b59\">34,</ref><ref type=\"bibr\" ta g entities, especially within two hops, are often critical to discover the missing attribute values <ref type=\"bibr\" target=\"#b57\">[32,</ref><ref type=\"bibr\" target=\"#b61\">36]</ref>. For example, two . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng the receptive field to a K-hop neighborhood may not capture these long-range dependencies either <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Often, \"too deep\" GNNs lead to node representations that  e-hop neighborhood <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, it remains to be seen how this approach will scale to very. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: it positional encodings to ensure permutation invariance.</p><p>Efficient Transformers. Transformer <ref type=\"bibr\" target=\"#b27\">[28]</ref> has been widely used in sequence modeling. Recently, modif GIN-Virtual <ref type=\"bibr\" target=\"#b35\">[36]</ref> 0.2798\u00b10.0025 0.2703\u00b10.0023</p><p>Transformer <ref type=\"bibr\" target=\"#b27\">[28]</ref> 0.1316\u00b10.0012 0.1281\u00b10.0039</p><p>GraphTrans (GIN) 0.2893\u00b1 AGNN (SOTA) <ref type=\"bibr\" target=\"#b26\">[27]</ref> 0.1607\u00b10.0040 0.1751\u00b10.0049</p><p>Transformer <ref type=\"bibr\" target=\"#b27\">[28]</ref> 0.1546\u00b10.0018 0.1670\u00b10.0015</p><p>GraphTrans (GCN) 0.1599\u00b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tions like attention <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>.</p><p>Our method, which we call Graph Transformer (GraphTran scribed in section 4.</p><p>for traditional CNN convolutions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>: attention layers can learn to reproduce the strong relationa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rks</head><p>Datasets. We choose two commonly used graph classification benchmarks, NCI1 and NCI109 <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Each of them contains about 4000 graphs with around 30 no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: phenomenon sometimes called oversmoothing or oversquashing <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. Therefore, the maximum. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, and some results suggest neighborhood-local coarsening may be unnecessary or counterproductive <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p><p>In this work, we take a different approach at graph owever, the effectiveness or necessity of hierarchical, coarsening-based pooling in GNNs is unclear <ref type=\"bibr\" target=\"#b22\">[23]</ref>. On the other hand, the most common global, whole-graph po. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the GNN receptive field.</p><p>However, GNN performance drops dramatically when its depth increases <ref type=\"bibr\" target=\"#b20\">[21]</ref>. This limitation has hurt the performance of GNNs on whole o be equivalent over the entire graph, a phenomenon sometimes called oversmoothing or oversquashing <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmer architecture emerge to further improve the efficiency <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. The LiteTransformer  \">6]</ref>. The LiteTransformer <ref type=\"bibr\" target=\"#b33\">[34]</ref> with less FLOPs, Reformer <ref type=\"bibr\" target=\"#b18\">[19]</ref> with complexity, and Performer <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to graphs. Recent works such as Zhang et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref>, Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and Dwivedi and Bresson <ref type=\"bibr\" target=\"#b11\">[1 le graph encoding. Of these, Zhang et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> and Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> tackle the problem of learning long-range dependencies wit arget=\"#b37\">[38]</ref> take the attended neighborhood radius as a tuning parameter and Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> attend to neighborhoods of random size during training and ph classification or regression, they used global average pooling over the nodes, while Rong et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> take a weighted sum over nodes with the weights computed b  field of a single GNN layer beyond a one-hop neighborhood <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, it remains to be s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: enerality and effectiveness of the framework. All of our models are trained with the Adam optimizer <ref type=\"bibr\" target=\"#b16\">[17]</ref> with a learning rate of 0.0001, a weight decay of 0.0001, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r graph classification datasets. We find significant improvements in accuracy on OpenGraphBenchmark <ref type=\"bibr\" target=\"#b14\">[15]</ref> where we achieve state-of-the-art results on two graph cla ur novel architecture GraphTrans, we obtain state-of-the-art results on several OpenGraph-Benchmark <ref type=\"bibr\" target=\"#b14\">[15]</ref> datasets and the NCI biomolecular datasets <ref type=\"bibr te our GraphTrans on a dataset larger than NCI dataset, molpcba from the Open Graph Benchmark (OGB) <ref type=\"bibr\" target=\"#b14\">[15]</ref>. It contains 437929 graphs with 28 nodes on average. Each  chmark. The performance on the GIN and GIN-Virtual baselines are as reported on the OGB leaderboard <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>Training Setups. All the GNN modules in the experim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rhood-local aggregation leverages the relational inductive bias encoded by the graph's connectivity <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Similar to convolutional neural networks (CNNs), GNNs can a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elines for long-range modeling in graphs via hierarchical clustering such as self-attention pooling <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>Our contributions are as follows:</p><p>\u2022 We show t h and depth in the small GraphTrans model are copied from the simple baseline, i.e. the settings in <ref type=\"bibr\" target=\"#b19\">[20]</ref>, which has a hidden dimension of 128 and 3 GNN layers. The #tab_2\">1</ref>. The simple baselines, including GCN Set2Set, SortPool, and SAGPool, are taken from <ref type=\"bibr\" target=\"#b19\">[20]</ref>, while the strong baselines <ref type=\"bibr\" target=\"#b12\" ed aggregation methods like graph-specific pooling methods <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> and \"virtual node\" approaches.</p><p>\u2022 Using our novel arch task is to predict whether a compound contains anti-lung-cancer activity. We follow the settings in <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> for the NCI1 and NCI. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hods like global pooling as well as learned aggregation methods like graph-specific pooling methods <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> and \"virtual node\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rhood-local aggregation leverages the relational inductive bias encoded by the graph's connectivity <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Similar to convolutional neural networks (CNNs), GNNs can a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tions like attention <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>.</p><p>Our method, which we call Graph Transformer (GraphTran scribed in section 4.</p><p>for traditional CNN convolutions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>: attention layers can learn to reproduce the strong relationa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the GNN receptive field.</p><p>However, GNN performance drops dramatically when its depth increases <ref type=\"bibr\" target=\"#b20\">[21]</ref>. This limitation has hurt the performance of GNNs on whole o be equivalent over the entire graph, a phenomenon sometimes called oversmoothing or oversquashing <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arch (NAS) was also applied to Transformer to fulfill the resource constraints for the edge devices <ref type=\"bibr\" target=\"#b31\">[32]</ref>. These off-the-shelf architectures are orthogonal to our G. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ar property prediction task, many GNNs (MPNN <ref type=\"bibr\" target=\"#b17\">[18]</ref>, AttentiveFP <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and Pre-trained GIN <ref type=\"bibr\" target=\"#b19\">[20]</ egating.</p><p>AttentiveFP is an interpretable graph attention network for molecular representation <ref type=\"bibr\" target=\"#b18\">[19]</ref>. AttentiveFP uses the attention mechanism to learn atom-le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a sequence format-SMILES, sequence-based models are used for molecule modeling, such as SMILES-BERT <ref type=\"bibr\" target=\"#b15\">[16]</ref> and MG-BERT <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Mor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ches for molecular property prediction gained great interest <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. In this way, scientists were enthusiastic in designing molec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed for molecule modeling, such as SMILES-BERT <ref type=\"bibr\" target=\"#b15\">[16]</ref> and MG-BERT <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Moreover, due to consisting of atoms and bonds, molecular. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: matics technologies have promoted the field of drug discovery such as molecular property prediction <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. In the early years, m type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and drug repositioning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. For molecular prope. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ADMET) prediction <ref type=\"bibr\" target=\"#b2\">[3]</ref>, drug-target interaction prediction (DTI) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, etc. However, the information loss due to the incomplete mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: The frameworks of curriculum learning always include a difficulty measurer and a training scheduler <ref type=\"bibr\" target=\"#b21\">[22]</ref>. The difficulty measurer is used for calculating the diffi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: noamine, JAK2, and HERG, The molecular property benchmark datasets were downloaded from MoleculeNet <ref type=\"bibr\" target=\"#b27\">[28]</ref> and the QSAR benchmark datasets were acquired from Cort\u00e9s-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b10\">[11]</ref>, biomarker discovery <ref type=\"bibr\" target=\"#b11\">[12]</ref>, drug discovery <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and drug repositio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d physicochemical properties to computer-readable formats like molecule fingerprint and descriptors <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Combining such molecular representations and a machine lear. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: The frameworks of curriculum learning always include a difficulty measurer and a training scheduler <ref type=\"bibr\" target=\"#b21\">[22]</ref>. The difficulty measurer is used for calculating the diffi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: riments were assessed to increase productivity. On the other hand, Trigueiro de Sousa Junior et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> presented a systematic review on discrete optimization metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hallenges involved in implementing the improvement changes <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>In this context, discrete-event simulation arises as  ete-event simulation arises as a suitable tool for decision support. Hosseinpour &amp; Hajihosseini <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Asprion et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sions must avoid putting their position in the market at risk. As Fullana Belda &amp; Urqu\u00eda Grande <ref type=\"bibr\" target=\"#b0\">[1]</ref> declare, organizations now face even more vigorous competiti sions must avoid putting their position in the market at risk. As Fullana Belda &amp; Urqu\u00eda Grande <ref type=\"bibr\" target=\"#b0\">[1]</ref> declare, organizations now face even more vigorous competiti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  support. Hosseinpour &amp; Hajihosseini <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Asprion et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> indicate that simulation has gained importance in recent yea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a tool for decision-making and increasing the productivity of organizations like in Zahraee et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Medvedev et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ttleneck by adjusting the calendar, which increased productivity by 15.81%.</p><p>Onofrejova et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> used simulation as a decision-making tool in a chimney produ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of organizations like in Zahraee et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Medvedev et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>.Krishnan et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref> carr behavior continues to meet the average time of delivery of the current system.</p><p>: \u03bc \u2264 33 hours <ref type=\"bibr\" target=\"#b5\">(6)</ref> : \u03bc <ref type=\"bibr\" target=\"#b6\">(7)</ref> Initially, the m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hallenges involved in implementing the improvement changes <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>In this context, discrete-event simulation arises as  ete-event simulation arises as a suitable tool for decision support. Hosseinpour &amp; Hajihosseini <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Asprion et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hallenges involved in implementing the improvement changes <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>In this context, discrete-event simulation arises as  ete-event simulation arises as a suitable tool for decision support. Hosseinpour &amp; Hajihosseini <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Asprion et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b4\">[5]</ref> and Medvedev et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>.Krishnan et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref> carried out a study at a pneumatic manufacturing company to  of delivery of the current system.</p><p>: \u03bc \u2264 33 hours <ref type=\"bibr\" target=\"#b5\">(6)</ref> : \u03bc <ref type=\"bibr\" target=\"#b6\">(7)</ref> Initially, the model was run, including the changes with ten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ttleneck by adjusting the calendar, which increased productivity by 15.81%.</p><p>Onofrejova et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> used simulation as a decision-making tool in a chimney produ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b61\">[63,</ref><ref type=\"bibr\" target=\"#b41\">43]</ref> and Microsoft DeepSpeed <ref type=\"bibr\" target=\"#b55\">[57,</ref><ref type=\"bibr\" target=\"#b63\">65]</ref>, we created an eff ref type=\"bibr\" target=\"#b43\">(45)</ref> , mental <ref type=\"bibr\" target=\"#b48\">(50)</ref> , sweet <ref type=\"bibr\" target=\"#b55\">(57)</ref> , charitable <ref type=\"bibr\" target=\"#b58\">(60)</ref> , p  <ref type=\"bibr\" target=\"#b45\">(47)</ref> , Allah <ref type=\"bibr\" target=\"#b53\">(55)</ref> , face <ref type=\"bibr\" target=\"#b55\">(57)</ref> , mosque <ref type=\"bibr\" target=\"#b57\">(59)</ref> , count. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ncluding more examples in the few-shot setting substantially increases performance.</p><p>PiQA PiQA <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a binary-choice question answering dataset targeting unde. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to on these 3 datasets is UNICORN <ref type=\"bibr\" target=\"#b36\">[38]</ref>.  Winogrande Winogrande <ref type=\"bibr\" target=\"#b56\">[58]</ref> is a dataset that seeks to expand the Winograd Schema Chal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pipeline parallelism) divides the layers of the model into stages that can be processed in parallel <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b40\">42]</ref>. As one stage comp  represent the word's ordinal position in the top 100 most frequent words list.</p><p>Asian Chinese <ref type=\"bibr\" target=\"#b21\">(23)</ref> , slim <ref type=\"bibr\" target=\"#b27\">(29)</ref> , yellow  a <ref type=\"bibr\" target=\"#b15\">(17)</ref> , monk <ref type=\"bibr\" target=\"#b19\">(21)</ref> , mind <ref type=\"bibr\" target=\"#b21\">(23)</ref> , robes <ref type=\"bibr\" target=\"#b22\">(24)</ref> , calm <  type=\"bibr\" target=\"#b10\">(11)</ref> , India <ref type=\"bibr\" target=\"#b12\">(14)</ref> , tolerance <ref type=\"bibr\" target=\"#b21\">(23)</ref> , caste <ref type=\"bibr\" target=\"#b42\">(44)</ref> , tradit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oiting existing spurious correlations in training datasets <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: his is a distinct trend compared to what we see in reading comprehension.</p><p>HellaSWAG HellaSWAG <ref type=\"bibr\" target=\"#b74\">[76]</ref> is a commonsense reasoning dataset where a goal is given a pe=\"bibr\" target=\"#b60\">(62)</ref> , existence <ref type=\"bibr\" target=\"#b61\">(63)</ref> , thinking <ref type=\"bibr\" target=\"#b74\">(76)</ref> , angry (80) , human (81) Buddhism compassion (13) , mindf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b13\">(15)</ref> , Buddha <ref type=\"bibr\" target=\"#b15\">(17)</ref> , monk <ref type=\"bibr\" target=\"#b19\">(21)</ref> , mind <ref type=\"bibr\" target=\"#b21\">(23)</ref> , robes < ble and at least partially driven by exploiting existing spurious correlations in training datasets <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing the most likely follow-up actions. The examples are mined from Wikihow and Activitynet Captions <ref type=\"bibr\" target=\"#b27\">[29]</ref> dataset. During evaluation, we prompt the model with the g  100 most frequent words list.</p><p>Asian Chinese <ref type=\"bibr\" target=\"#b21\">(23)</ref> , slim <ref type=\"bibr\" target=\"#b27\">(29)</ref> , yellow <ref type=\"bibr\" target=\"#b37\">(39)</ref> , Japan > , desirable <ref type=\"bibr\" target=\"#b75\">(77)</ref> , feminine (88) , pleasant (91) Black civil <ref type=\"bibr\" target=\"#b27\">(29)</ref> , lazy <ref type=\"bibr\" target=\"#b42\">(44)</ref> , immoral. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t <ref type=\"bibr\" target=\"#b32\">(34)</ref> , law <ref type=\"bibr\" target=\"#b33\">(35)</ref> , cover <ref type=\"bibr\" target=\"#b45\">(47)</ref> , Allah <ref type=\"bibr\" target=\"#b53\">(55)</ref> , face < et=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" target=\"#b45\">47,</ref><ref type=\"bibr\" target=\"#b73\">75]</ref>. The reason why lar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ERT <ref type=\"bibr\" target=\"#b66\">[68]</ref>.</p><p>HANS Heuristic Analysis for NLI Systems (HANS) <ref type=\"bibr\" target=\"#b38\">[40]</ref> is an NLI dataset designed to evaluate the tendency of mod ei-c.org/ns/1.0\"><head n=\"6.2\">Evaluating Grasp of Language Systematicity</head><p>The HANS dataset <ref type=\"bibr\" target=\"#b38\">[40]</ref> allows us to evaluate to what extent language models can c =\"annex\"> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>The Heuristic Analysis for NLI Systems (HANS) <ref type=\"bibr\" target=\"#b38\">[40]</ref> dataset is an NLI dataset designed to check the reliance o s} are\", \"The {religious practitio Atheism belief <ref type=\"bibr\" target=\"#b18\">(20)</ref> , think <ref type=\"bibr\" target=\"#b38\">(40)</ref> , science <ref type=\"bibr\" target=\"#b41\">(43)</ref> , lack ref type=\"bibr\" target=\"#b16\">(18)</ref> , money <ref type=\"bibr\" target=\"#b17\">(19)</ref> , Israel <ref type=\"bibr\" target=\"#b38\">(40)</ref> , black <ref type=\"bibr\" target=\"#b40\">(42)</ref> , bad <r  training datasets <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" target=\"#b45\">47,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ERT <ref type=\"bibr\" target=\"#b66\">[68]</ref>.</p><p>HANS Heuristic Analysis for NLI Systems (HANS) <ref type=\"bibr\" target=\"#b38\">[40]</ref> is an NLI dataset designed to evaluate the tendency of mod ei-c.org/ns/1.0\"><head n=\"6.2\">Evaluating Grasp of Language Systematicity</head><p>The HANS dataset <ref type=\"bibr\" target=\"#b38\">[40]</ref> allows us to evaluate to what extent language models can c =\"annex\"> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>The Heuristic Analysis for NLI Systems (HANS) <ref type=\"bibr\" target=\"#b38\">[40]</ref> dataset is an NLI dataset designed to check the reliance o s} are\", \"The {religious practitio Atheism belief <ref type=\"bibr\" target=\"#b18\">(20)</ref> , think <ref type=\"bibr\" target=\"#b38\">(40)</ref> , science <ref type=\"bibr\" target=\"#b41\">(43)</ref> , lack ref type=\"bibr\" target=\"#b16\">(18)</ref> , money <ref type=\"bibr\" target=\"#b17\">(19)</ref> , Israel <ref type=\"bibr\" target=\"#b38\">(40)</ref> , black <ref type=\"bibr\" target=\"#b40\">(42)</ref> , bad <r  training datasets <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b38\">40,</ref><ref type=\"bibr\" target=\"#b45\">47,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a seasonal-trend decomposition approach <ref type=\"bibr\" target=\"#b6\">(Cleveland et al., 1990;</ref><ref type=\"bibr\" target=\"#b33\">Wen et al., 2019)</ref>, which is widely used in time series analysis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: el is not parallelizable and unable to handle long dependencies. The temporal convolutional network <ref type=\"bibr\" target=\"#b29\">(Sen et al., 2019)</ref> is another family efficient in sequential ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ll obey RIP, so similar conclusions could be drawn.</p><p>Starting from Johnson-Lindenstrauss lemma <ref type=\"bibr\" target=\"#b14\">(Johnson, 1984)</ref> and using the version from <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns the electricity consumption of clients with each column corresponding to one client. 3) Exchange <ref type=\"bibr\" target=\"#b16\">(Lai et al., 2018)</ref> contains the current exchange of 8 countries. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imation of attention matrix with O(n) complexity. Some work uses a combination of patterns (BIGBIRD <ref type=\"bibr\" target=\"#b36\">(Zaheer et al., 2020)</ref>) mentioned above. Another strategy is to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ugh this idea has been exploited before <ref type=\"bibr\" target=\"#b21\">(Oreshkin et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wu et al., 2021)</ref>, we present a special design of network that i ted, and applied in time series forecasting <ref type=\"bibr\" target=\"#b37\">(Zhou et al., 2021;</ref><ref type=\"bibr\" target=\"#b34\">Wu et al., 2021)</ref>. In sequence to sequence time series forecasti erform relatively inferior as shown in <ref type=\"bibr\" target=\"#b37\">(Zhou et al., 2021)</ref> and <ref type=\"bibr\" target=\"#b34\">(Wu et al., 2021)</ref>, we mainly include four state-of-theart trans >, we mainly include four state-of-theart transformer-based models for comparison, i.e., Autoformer <ref type=\"bibr\" target=\"#b34\">(Wu et al., 2021)</ref>, Informer <ref type=\"bibr\" target=\"#b37\">(Zho 1.\">Main Results</head><p>For better comparison, we follow the experiment settings of Autoformer in <ref type=\"bibr\" target=\"#b34\">(Wu et al., 2021)</ref> where the input length is fixed to 96, and th  select top-k in attention matrix. This sparser matrix costs only N log N in complexity. Autoformer <ref type=\"bibr\" target=\"#b34\">(Wu et al., 2021)</ref> introduces an auto-correlation block in place cient computing of auto-correlation function, which can be used as a building neural networks block <ref type=\"bibr\" target=\"#b34\">(Wu et al., 2021)</ref> and also useful in numerous anomaly detection. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Devlin et al., 2019)</ref> and computer vision tasks <ref type=\"bibr\" target=\"#b8\">(Dosovitskiy et al., 2021;</ref><ref type=\"bibr\" target=\"#b27\">Rao et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b32\">(Serrano &amp; Smith, 2019;</ref><ref type=\"bibr\" target=\"#b17\">Jain &amp; Wallace, 2019;</ref><ref type=\"bibr\" target=\"#b26\">Mohankumar et al., 2020)</ref> have shown that the attention may not . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ining, and the rank of the learned weights can be interpreted as the importance of certain features <ref type=\"bibr\" target=\"#b2\">(Bahdanau et al., 2014;</ref><ref type=\"bibr\">Xu et al., 2015)</ref>.  chanism has been widely used in inherently interpretable neural network models for NLP and CV tasks <ref type=\"bibr\" target=\"#b2\">(Bahdanau et al., 2014;</ref><ref type=\"bibr\">Xu et al., 2015;</ref><r .</p><p>There are two types of attention models: One normalizes the attention weights to sum to one <ref type=\"bibr\" target=\"#b2\">(Bahdanau et al., 2014)</ref>, while the other learns weights between . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y-used technique to provide inherent interpretability, often cannot provide faithful interpretation <ref type=\"bibr\" target=\"#b24\">(Lipton, 2018)</ref>. The rationale of the attention mechanism is to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rediction performance with the backbone models GIN <ref type=\"bibr\">(Xu et al., 2019)</ref> and PNA <ref type=\"bibr\" target=\"#b8\">(Corso et al., 2020)</ref>, and inherently interpretable models DIR <r Xu et al., 2019)</ref> is used as the backbone model for both baselines and GSAT. We also apply PNA <ref type=\"bibr\" target=\"#b8\">(Corso et al., 2020)</ref> to further test the wide applicability of G r\">(Xu et al., 2019)</ref> with 64 hidden dimensions and 0.3 dropout ratio. We use the setting from <ref type=\"bibr\" target=\"#b8\">(Corso et al., 2020)</ref> for PNA, which has 4 layers with 80 hidden  n dimensions, 0.3 dropout ratio, and no scalars are used. For OGBG-Mol datasets, we directly follow <ref type=\"bibr\" target=\"#b8\">(Corso et al., 2020)</ref> using (mean, min, max, std) aggregators for r\">(Yuan et al., 2020b)</ref> and <ref type=\"bibr\" target=\"#b43\">(Wu et al., 2022)</ref>. Following <ref type=\"bibr\" target=\"#b8\">(Corso et al., 2020)</ref>, edge features are not used for all OGBG-Mo rious-Motifs and 0.001 for all other datasets. PNA uses 0.01 learning rate with scheduler following <ref type=\"bibr\" target=\"#b8\">(Corso et al., 2020)</ref>, 0.003 learning rate for Graph-SST2 and Spu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b27\">(Pearl et al., 2016;</ref><ref type=\"bibr\" target=\"#b1\">Arjovsky et al., 2019;</ref><ref type=\"bibr\" target=\"#b6\">Chang et al., 2020)</ref>: Although G * S uniquely determines Y , in t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b31\">Schlichtkrull et al., 2021;</ref><ref type=\"bibr\">Yuan et al., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Lin et al., 2021;</ref><ref type=\"bibr\" target=\"#b14\">Henderson et al mlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_0\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b23\">Lin et al., 2021)</ref> checking Granger causality and Graphlime<ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  science, such as physics <ref type=\"bibr\" target=\"#b4\">(Bapst et al., 2020)</ref> and biochemistry <ref type=\"bibr\" target=\"#b19\">(Jumper et al., 2021)</ref>. In many such disciplines, building more . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he widely-used graph attention models <ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2015)</ref> seem unable to provide any reliable interpreta ef type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref> while GGNN adopts the unnormalized one <ref type=\"bibr\" target=\"#b22\">(Li et al., 2015)</ref>. Our method belongs to the second category.</ ntion mechanisms such as GAT <ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref> or GGNN <ref type=\"bibr\" target=\"#b22\">(Li et al., 2015)</ref> may not yield good model interpretation.</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b40\">Vu &amp; Thai, 2020;</ref><ref type=\"bibr\" target=\"#b25\">Luo et al., 2020;</ref><ref type=\"bibr\" target=\"#b31\">Schlichtkrull et al., 2021;</ref><ref type=\"bibr\">Yuan et al., 2021;< els <ref type=\"bibr\">(Ying et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Luo et al., 2020;</ref><ref type=\"bibr\" target=\"#b31\">Schlichtkrull et al., 2021;</ref><ref type=\"bibr\">Yu et al., 2020)</r efine a few notations and concepts.</p><p>Graph. An attributed graph can be denoted as G = (A, X)   <ref type=\"bibr\" target=\"#b31\">(Schlichtkrull et al., 2021)</ref> (second row) on a motif example, w al., 2019)</ref>, PGExplainer <ref type=\"bibr\" target=\"#b25\">(Luo et al., 2020)</ref> and GraphMask <ref type=\"bibr\" target=\"#b31\">(Schlichtkrull et al., 2021)</ref>. Given a pre-trained predictor f \u03b8 ype=\"bibr\">(Ying et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Luo et al., 2020)</ref>, 0 -norm <ref type=\"bibr\" target=\"#b31\">(Schlichtkrull et al., 2021)</ref> or 2 -regression to {0, 1} (Yu et  (Ying et al., 2019), PGExplainer <ref type=\"bibr\" target=\"#b25\">(Luo et al., 2020)</ref>, GraphMask <ref type=\"bibr\" target=\"#b31\">(Schlichtkrull et al., 2021)</ref>, and inherently interpretable mode esc>Figure3. Visualizing attention (normalized to [0, 1]) of GSAT (first row) and masks of GraphMask<ref type=\"bibr\" target=\"#b31\">(Schlichtkrull et al., 2021)</ref> (second row) on a motif example, w Mask also have stochasticity in their models <ref type=\"bibr\" target=\"#b25\">(Luo et al., 2020;</ref><ref type=\"bibr\" target=\"#b31\">Schlichtkrull et al., 2021)</ref>. However, their main goal is to ena. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -75sp <ref type=\"bibr\" target=\"#b21\">(Knyazev et al., 2019</ref>) is a image classifica- Graph-SST2 <ref type=\"bibr\" target=\"#b34\">(Socher et al., 2013;</ref><ref type=\"bibr\">Yuan et al., 2020b</ref>) at the subgraphs that provide explanations are of different sizes in this dataset.</p><p>Graph-SST2 <ref type=\"bibr\" target=\"#b34\">(Socher et al., 2013;</ref><ref type=\"bibr\">Yuan et al., 2020b)</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: attribute masking; as well as two approaches for graph-level pretraining. More recently, Dai et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> present AdaGCN: a framework for transfer learning based on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch graph for each class has an attribute vector that can be assigned to it. We follow Morris et al. <ref type=\"bibr\" target=\"#b44\">[44]</ref> in generating this attribute level task using the scikit-l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rld contexts, such as social networks <ref type=\"bibr\" target=\"#b29\">[30]</ref> or knowledge graphs <ref type=\"bibr\" target=\"#b31\">[31]</ref>. Below we describe the datasets and experimental methodolo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2]</ref> and specialised models such as recurrent and convolutional neural networks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> have been designed to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  useful as a trained network's <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>However, Lee et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> do propose a framework to transfer spectral information be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ationship between them. Practical applications include real-world contexts, such as social networks <ref type=\"bibr\" target=\"#b29\">[30]</ref> or knowledge graphs <ref type=\"bibr\" target=\"#b31\">[31]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ep learning approaches have been extended to graph-based domains using graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, which leverage certain topological structures and propertie R C is the attribute vector for node v i . Similarly, a graph may have an edge attribute matrix X e <ref type=\"bibr\" target=\"#b4\">[5]</ref>. An important measure of a node's connectivity is its degree. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch graph for each class has an attribute vector that can be assigned to it. We follow Morris et al. <ref type=\"bibr\" target=\"#b44\">[44]</ref> in generating this attribute level task using the scikit-l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aph similarity <ref type=\"bibr\" target=\"#b40\">[40]</ref>, the most common being graph-edit distance <ref type=\"bibr\" target=\"#b41\">[41]</ref>. Another approach is to compare graph spectra. However, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: asure of a node's connectivity is its degree, which is the number of edges the node is connected to <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We denote the degree of the i th node as d i . The degree m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rld contexts, such as social networks <ref type=\"bibr\" target=\"#b29\">[30]</ref> or knowledge graphs <ref type=\"bibr\" target=\"#b31\">[31]</ref>. Below we describe the datasets and experimental methodolo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve label model, which aggregates over the four GPT-3 1-shot outputs without using any labeled data. <ref type=\"bibr\" target=\"#b27\">(Ratner et al., 2016</ref><ref type=\"bibr\" target=\"#b26\">(Ratner et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b28\">(Sanh et al., 2022)</ref>, which achieves zero-shot generalization by fine-tuning T5 <ref type=\"bibr\" target=\"#b25\">(Raffel et al., 2020)</ref> on multiple tasks whose labeled examples . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: old between the models being trained (essentially, they are \"different enough\"). In a similar vein, <ref type=\"bibr\" target=\"#b37\">Wei et al. (2020)</ref> give a theoretical explanation of why (and wh complementary models (though it can also happen with a single model with appropriate regularization <ref type=\"bibr\" target=\"#b37\">(Wei et al., 2020)</ref>).</p><p>Figure <ref type=\"figure\">3</ref> sh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve label model, which aggregates over the four GPT-3 1-shot outputs without using any labeled data. <ref type=\"bibr\" target=\"#b27\">(Ratner et al., 2016</ref><ref type=\"bibr\" target=\"#b26\">(Ratner et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ters while attaining similar performance <ref type=\"bibr\" target=\"#b18\">(Li &amp; Liang, 2021;</ref><ref type=\"bibr\" target=\"#b17\">Lester et al., 2021)</ref>. The parameter space for model h 0 is the  arameterized by a continuous soft prompt <ref type=\"bibr\" target=\"#b18\">(Li &amp; Liang, 2021;</ref><ref type=\"bibr\" target=\"#b17\">Lester et al., 2021)</ref>. Concretely, letting d = 2048 be the dimen fers from the usual soft prompting setup <ref type=\"bibr\" target=\"#b18\">(Li &amp; Liang, 2021;</ref><ref type=\"bibr\" target=\"#b17\">Lester et al., 2021)</ref>. We discuss this issue in more depth in Se fers from the usual soft prompting setup <ref type=\"bibr\" target=\"#b18\">(Li &amp; Liang, 2021;</ref><ref type=\"bibr\" target=\"#b17\">Lester et al., 2021)</ref>, where the input is encoded more neutrally nstrained to human-readable prompts, <ref type=\"bibr\" target=\"#b18\">Li &amp; Liang (2021)</ref> and <ref type=\"bibr\" target=\"#b17\">Lester et al. (2021)</ref> instead learn a continuous soft task-speci ing, for training the soft prompt over view \u03c6 0 (x) we mainly used the hyperparameters suggested by <ref type=\"bibr\" target=\"#b17\">Lester et al. (2021)</ref>, which were obtained by performing gold so with the maximum likelihood objective. This is identical to the soft prompt training technique from <ref type=\"bibr\" target=\"#b17\">Lester et al. (2021)</ref>.</p><p>Caveat. As noted by <ref type=\"bibr www.tei-c.org/ns/1.0\" place=\"foot\" n=\"3\" xml:id=\"foot_2\">We use L = 20 in our experiments, following<ref type=\"bibr\" target=\"#b17\">Lester et al. (2021)</ref>, so the soft prompt has 20 \u00d7 2048 =</note> oft prompt tuning can match the performance of full-fine-tuning with far fewer trainable parameters <ref type=\"bibr\" target=\"#b17\">(Lester et al., 2021;</ref><ref type=\"bibr\" target=\"#b15\">Le Scao &am. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eled data, and then iteratively includes the confidently pseudo-labeled data as new training labels <ref type=\"bibr\" target=\"#b31\">(Scudder, 1965)</ref>. In the context of few-shot text classification. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tatistic is a ranking heuristic that uses the view geometry more than the model confidence approach <ref type=\"bibr\" target=\"#b21\">(Muhlenbach et al., 2004;</ref><ref type=\"bibr\" target=\"#b40\">Zhang &. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and showed significantly better performance than traditional machine learning methods. Hamad et al. <ref type=\"bibr\" target=\"#b67\">[68]</ref> presented a computational framework for automatic detectio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ndicate that many people on social media tend to share or give advice on health-related information <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with mental health issues on social media as a way of relief <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Therefore, social media is an excellent resource to automa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t that is extracted in the form of statistical knowledge such as those based on count-based methods <ref type=\"bibr\" target=\"#b24\">[25]</ref>. These features describe the linguistic content of the pos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref> on Twitter by finding depression relevant words, antidepressants, and depression symptoms. In <ref type=\"bibr\" target=\"#b39\">[40]</ref> the authors used post-level behaviour for detecting anorex guistic features of the users for psychometric properties which resembles the settings described in <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> where the authors . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of attention, owing to the requirement to explain the internal mechanics of a deep learning system <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>. Many recent studies. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, the authors monitored different symptoms of depression that are mentioned in a user's tweet. In <ref type=\"bibr\" target=\"#b44\">[45]</ref>, they study users' behaviour on both Twitter and Weibo. To etric properties which resembles the settings described in <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> where the authors have extracted 70 features from two diffe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tage that this method has is that it can learn from user-level labels to identify post-level labels <ref type=\"bibr\" target=\"#b60\">[61]</ref>.</p><p>Recently, Lin et al. <ref type=\"bibr\" target=\"#b11\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  health problems is depression that is more dominant than other mental illness conditions worldwide <ref type=\"bibr\" target=\"#b64\">[65]</ref>. Diagnosis of depression is usually a difficult task becau. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, traffic flow and traffic accident risk predictions <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, and mental illness. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  COVID-19 pandemic using LSTM and fastText <ref type=\"bibr\" target=\"#b30\">[31]</ref> embeddings. In <ref type=\"bibr\" target=\"#b45\">[46]</ref>, the authors also propose a multi-model RNN-based model fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  adaptation of language models to ad-hoc tasks and improving sample efficiency in low-data settings <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Schick and Sc lly improve language models' few-shot and zero-shot performance in a wide range of downstream tasks <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b27\">Schick and Sc ake predictions for these tasks by scoring only the valid completions and returning the highest one <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020;</ref><ref type=\"bibr\">Sanh et al., 2021;</ref><re included in an input sequence in order to prime models to make predictions without gradient updates <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020)</ref>.</p><p>PromptSource was originally designed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s labels and entity tags. Since then, many Webbased systems for annotating text have been developed <ref type=\"bibr\" target=\"#b30\">(Stenetorp et al., 2012;</ref><ref type=\"bibr\" target=\"#b25\">Salgado . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  words in the model's original vocabulary <ref type=\"bibr\" target=\"#b11\">(Lester et al., 2021;</ref><ref type=\"bibr\" target=\"#b24\">Qin and Eisner, 2021)</ref> When using human-written prompts, there a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ormance in a wide range of downstream tasks <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b27\">Schick and Sch\u00fctze, 2021a;</ref><ref type=\"bibr\">Sanh et al., 2021;</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , using techniques such as ac- tive learning <ref type=\"bibr\" target=\"#b15\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Li et al., 2021)</ref> and example recommendation <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the level of individual examples <ref type=\"bibr\" target=\"#b20\">(Neves and \u0160eva, 2021)</ref>. GATE <ref type=\"bibr\" target=\"#b4\">(Cunningham et al., 2002</ref>) was an early system for annotating tex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: available.</p><p>There has also been some work on collecting annotations other than labels. AlvisAE <ref type=\"bibr\" target=\"#b21\">(Papazian et al., 2012)</ref> and TreeAnnotator <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b23\">Putra et al., 2020)</ref>. Other systems support collaboration among multiple annotators <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Stewart et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ving sample efficiency in low-data settings <ref type=\"bibr\" target=\"#b2\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Schick and Sch\u00fctze, 2021b;</ref><ref type=\"bibr\" target=\"#b9\">Le Scao. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Li et al., 2021)</ref> and example recommendation <ref type=\"bibr\" target=\"#b10\">(Lee et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Kiela et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: begun to incorporate learned models to improve workflow, using techniques such as ac- tive learning <ref type=\"bibr\" target=\"#b15\">(Lin et al., 2019;</ref><ref type=\"bibr\" target=\"#b14\">Li et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ure such as GraphSAGE <ref type=\"bibr\" target=\"#b29\">(Hamilton et al., 2017)</ref>, Graph Attention <ref type=\"bibr\" target=\"#b57\">Velickovic et al. (2018</ref><ref type=\"bibr\">), Graph Convolution De /ref>). In this paper, we focus on two particularly popular choices: Attentional message passing of <ref type=\"bibr\" target=\"#b57\">Velickovic et al. (2018)</ref>:</p><formula xml:id=\"formula_4\">F \u03b8 (X th standard GNN baselines: GCN <ref type=\"bibr\" target=\"#b31\">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b57\">(Velickovic et al., 2018)</ref>, MoNet <ref type=\"bibr\" target=\"#b36\"  al. (2019)</ref> 98.5 JKNet <ref type=\"bibr\" target=\"#b62\">(Xu et al., 2018b)</ref> 97.6</p><p>GAT <ref type=\"bibr\" target=\"#b57\">(Velickovic et al., 2018)</ref> 97.3 GraphCON-GAT 99.4</p><p>GCN <ref br\" target=\"#b31\">(Kipf &amp; Welling, 2017)</ref> 0.47 \u00b1 0.002 GraphCON-GCN 0.22 \u00b1 0.004</p><p>GAT <ref type=\"bibr\" target=\"#b57\">(Velickovic et al., 2018)</ref> 0.46 \u00b1 0.002 GraphCON-GAT 0.23 \u00b1 0.00  <ref type=\"bibr\" target=\"#b31\">(Kipf &amp; Welling, 2017)</ref> 88.89 GraphCON-GCN 98.68</p><p>GAT <ref type=\"bibr\" target=\"#b57\">(Velickovic et al., 2018)</ref> 96.19 GraphCON-GAT 98.91</p></div> <d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resentations <ref type=\"bibr\" target=\"#b62\">Xu et al. (2018b)</ref>, or adding residual connections <ref type=\"bibr\" target=\"#b11\">Chen et al. (2020)</ref>.</p><p>We will show that GraphCON allows to  l., 2017)</ref> 61.2 PDE-GCN <ref type=\"bibr\" target=\"#b19\">(Eliasof et al., 2021)</ref> 99.2 GCNII <ref type=\"bibr\" target=\"#b11\">(Chen et al., 2020)</ref> 99.5 Cluster-GCN <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b22\">Frasconi et al. (1998)</ref>; <ref type=\"bibr\" target=\"#b26\">Gori et al. (2005)</ref>; <ref type=\"bibr\" target=\"#b47\">Scarselli et al. (2008)</ref>; <ref type=\"bibr\" target=\"#b6\">Bruna et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uggested by <ref type=\"bibr\" target=\"#b36\">Monti et al. (2017)</ref>, is based on the MNIST dataset <ref type=\"bibr\" target=\"#b32\">(LeCun et al., 1998)</ref>, where the grey-scale images are transform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ure such as GraphSAGE <ref type=\"bibr\" target=\"#b29\">(Hamilton et al., 2017)</ref>, Graph Attention <ref type=\"bibr\" target=\"#b57\">Velickovic et al. (2018</ref><ref type=\"bibr\">), Graph Convolution De /ref>). In this paper, we focus on two particularly popular choices: Attentional message passing of <ref type=\"bibr\" target=\"#b57\">Velickovic et al. (2018)</ref>:</p><formula xml:id=\"formula_4\">F \u03b8 (X th standard GNN baselines: GCN <ref type=\"bibr\" target=\"#b31\">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b57\">(Velickovic et al., 2018)</ref>, MoNet <ref type=\"bibr\" target=\"#b36\"  al. (2019)</ref> 98.5 JKNet <ref type=\"bibr\" target=\"#b62\">(Xu et al., 2018b)</ref> 97.6</p><p>GAT <ref type=\"bibr\" target=\"#b57\">(Velickovic et al., 2018)</ref> 97.3 GraphCON-GAT 99.4</p><p>GCN <ref br\" target=\"#b31\">(Kipf &amp; Welling, 2017)</ref> 0.47 \u00b1 0.002 GraphCON-GCN 0.22 \u00b1 0.004</p><p>GAT <ref type=\"bibr\" target=\"#b57\">(Velickovic et al., 2018)</ref> 0.46 \u00b1 0.002 GraphCON-GAT 0.23 \u00b1 0.00  <ref type=\"bibr\" target=\"#b31\">(Kipf &amp; Welling, 2017)</ref> 88.89 GraphCON-GCN 98.68</p><p>GAT <ref type=\"bibr\" target=\"#b57\">(Velickovic et al., 2018)</ref> 96.19 GraphCON-GAT 98.91</p></div> <d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al. (2020)</ref>; <ref type=\"bibr\" target=\"#b60\">Xhonneux et al. (2020b)</ref>, including diffusion <ref type=\"bibr\" target=\"#b9\">Chamberlain et al. (2021b)</ref> and wave <ref type=\"bibr\" target=\"#b1 =\"#b12\">Chen et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b27\">Haber &amp; Ruthotto (2018)</ref>; <ref type=\"bibr\" target=\"#b9\">Chamberlain et al. (2021b)</ref>, one can view GraphCON as a wrapper a d-order time derivative X is removed from (1) and \u03b1 = \u03b3 = 1, we recover the graph diffusion-PDEs of <ref type=\"bibr\" target=\"#b9\">Chamberlain et al. (2021b)</ref>. Hence, the presence of the temporal  training, validation, and test splits of <ref type=\"bibr\" target=\"#b49\">Shchur et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b9\">Chamberlain et al. (2021b)</ref>, using only on the largest connected  neux et al., 2020a)</ref>, GDE <ref type=\"bibr\" target=\"#b42\">(Poli et al., 2019a)</ref>, and GRAND <ref type=\"bibr\" target=\"#b9\">Chamberlain et al. (2021b)</ref>. We observe that GraphCON-GCN and Gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b52\">Sperduti (1994)</ref>; <ref type=\"bibr\" target=\"#b25\">Goller &amp; Ku. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b31\">Kipf &amp; Welling (2017)</ref>; <ref type=\"bibr\" target=\"#b36\">Monti et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b24\">Gilmer et al. (2017)</ref> are a widely-used class of models for lear portation <ref type=\"bibr\" target=\"#b17\">Derrow-Pinion et al. (2021)</ref>, computational chemistry <ref type=\"bibr\" target=\"#b24\">(Gilmer et al., 2017)</ref>, drug discovery <ref type=\"bibr\">Gaudelet tion of this nomenclature). The coupling function F \u03b8 plays the role of a message passing mechanism <ref type=\"bibr\" target=\"#b24\">(Gilmer et al. (2017)</ref>, also referred to, in various contexts, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rard et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b31\">Kipf &amp; Welling (2017)</ref>, SplineCNN <ref type=\"bibr\" target=\"#b20\">(Fey et al., 2018)</ref>, or MoNet <ref type=\"bibr\" target=\"#b36\">(Mo f> 98.76 GatedGCN <ref type=\"bibr\" target=\"#b4\">(Bresson &amp; Laurent, 2017)</ref> 97.95 SplineCNN <ref type=\"bibr\" target=\"#b20\">(Fey et al., 2018)</ref> 95.22</p><p>GCN <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b18\">(DeGrave et al., 2020)</ref>.</p><p>Recent studies of the OOD generalization problem like <ref type=\"bibr\" target=\"#b44\">Rojas-Carulla et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b10\">B\u00fch ons since one only has access to data from limited environments in the training set. Recent studies <ref type=\"bibr\" target=\"#b44\">(Rojas-Carulla et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Arjov pired by Weisfeiler-Lehman test, we can adapt the definition of invariance assumption in prior arts <ref type=\"bibr\" target=\"#b44\">(Rojas-Carulla et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Gong. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b9\">Blanchard et al., 2011;</ref><ref type=\"bibr\" target=\"#b40\">Muandet et al., 2013;</ref><ref type=\"bibr\" target=\"#b22\">Gong et al., 2016)</ref> occupies a central role in the ML community. get=\"#b44\">Rojas-Carulla et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b10\">B\u00fchlmann (2018)</ref>; <ref type=\"bibr\" target=\"#b22\">Gong et al. (2016)</ref>; <ref type=\"bibr\" target=\"#b3\">Arjovsky et a nvariance assumption in prior arts <ref type=\"bibr\" target=\"#b44\">(Rojas-Carulla et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Gong et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Arjovsky et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  problem is how to specify g w k (G). Following recent advances in adversarial robustness on graphs <ref type=\"bibr\" target=\"#b60\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Jin et al., 202 s but differently harness energy function to enforce the constraints. From a different perspective, <ref type=\"bibr\" target=\"#b60\">Xu et al. (2019)</ref> attempts to attack the graph topology for impr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  advances in adversarial robustness on graphs <ref type=\"bibr\" target=\"#b60\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Jin et al., 2020)</ref>, we consider editing graph structures by addi mmon practice is to enforce a certain regularization for the learned graph structures. For example, <ref type=\"bibr\" target=\"#b29\">Jin et al. (2020)</ref> proposes to constrain the sparsity and smooth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  out-ofdistribution (OOD) generalization <ref type=\"bibr\" target=\"#b39\">(Mansour et al., 2009;</ref><ref type=\"bibr\" target=\"#b9\">Blanchard et al., 2011;</ref><ref type=\"bibr\" target=\"#b40\">Muandet et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b51\">Su et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b43\">Recht et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b38\">Mancini et al. (2020)</ref>. A more concerning example is that a mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ol, 2018</ref><ref type=\"bibr\">), medical diagnosis (AlBadawy et al., 2018)</ref>, criminal justice <ref type=\"bibr\" target=\"#b7\">(Berk et al., 2018)</ref>, etc.</p><p>In this paper, we endeavor to 1). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing in-the-wild unseen instances draws increasing concerns, out-ofdistribution (OOD) generalization <ref type=\"bibr\" target=\"#b39\">(Mansour et al., 2009;</ref><ref type=\"bibr\" target=\"#b9\">Blanchard e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(c), we compare the test accuracy averaged on eight graphs when using different GNNs e.g. GCN, SGC <ref type=\"bibr\" target=\"#b57\">(Wu et al., 2019)</ref> and GAT <ref type=\"bibr\" target=\"#b53\">(Velic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n using different GNNs e.g. GCN, SGC <ref type=\"bibr\" target=\"#b57\">(Wu et al., 2019)</ref> and GAT <ref type=\"bibr\" target=\"#b53\">(Velickovic et al., 2018)</ref>, for data generation (See Appendix G . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b6\">Beery et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b51\">Su et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b43\">Recht et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b38\">Mancini et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: here exist distribution shifts between training and testing data.</p><p>There is a very recent work <ref type=\"bibr\" target=\"#b5\">(Baranwal et al., 2021)</ref> that endeavors to understand the out-of- oviding a new perspective and methodology for node-level prediction on graphs. Third, compared with <ref type=\"bibr\" target=\"#b5\">Baranwal et al. (2021)</ref> that focus on synthetic datasets and simu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amples <ref type=\"bibr\">(Rosenfeld et al., 2021;</ref><ref type=\"bibr\">Nagarajan et al., 2021;</ref><ref type=\"bibr\" target=\"#b30\">Kamath et al., 2021)</ref>. A recent work <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b1\">Ahuja et al., 2020;</ref><ref type=\"bibr\">Krueger et al., 2021;</ref><ref type=\"bibr\" target=\"#b36\">Liu et al., 2021;</ref><ref type=\"bibr\" target=\"#b16\">Creager et al., \"#b3\">Arjovsky et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Koyama &amp; Yamaguchi, 2021;</ref><ref type=\"bibr\" target=\"#b36\">Liu et al., 2021)</ref> to accommodate structural information in grap z] is the solution to OOD problem in Eq. 2.</p><p>The proof for Theorem 2 follows a similar line of <ref type=\"bibr\" target=\"#b36\">Liu et al. (2021)</ref>. The above theorems indicate that the objecti n inputs from the same object. Also, <ref type=\"bibr\" target=\"#b16\">Creager et al. (2021)</ref> and <ref type=\"bibr\" target=\"#b36\">Liu et al. (2021)</ref> point out that in most real situations, one h at satisfies p e (y, z, z) = p e (y, z)p e (z).</p><p>Then we reproduce the proof of Theorem 2.1 in <ref type=\"bibr\" target=\"#b36\">Liu et al. (2021)</ref> to prove the result by showing that for arbit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  out-ofdistribution (OOD) generalization <ref type=\"bibr\" target=\"#b39\">(Mansour et al., 2009;</ref><ref type=\"bibr\" target=\"#b9\">Blanchard et al., 2011;</ref><ref type=\"bibr\" target=\"#b40\">Muandet et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b6\">Beery et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b51\">Su et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b43\">Recht et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b38\">Mancini et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uture works.</p><p>There are a few recent studies that focus on out-of-graph problems. For example, <ref type=\"bibr\" target=\"#b27\">Hu et al. (2020a)</ref> considers transferable active learning over m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2021)</ref> or adaptively sparsifying the structures <ref type=\"bibr\">(Zheng et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Hasanzadeh et al., 2020)</ref>. While in our model, the introduced co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ol, 2018</ref><ref type=\"bibr\">), medical diagnosis (AlBadawy et al., 2018)</ref>, criminal justice <ref type=\"bibr\" target=\"#b7\">(Berk et al., 2018)</ref>, etc.</p><p>In this paper, we endeavor to 1). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amples <ref type=\"bibr\">(Rosenfeld et al., 2021;</ref><ref type=\"bibr\">Nagarajan et al., 2021;</ref><ref type=\"bibr\" target=\"#b30\">Kamath et al., 2021)</ref>. A recent work <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b1\">Ahuja et al., 2020;</ref><ref type=\"bibr\">Krueger et al., 2021;</ref><ref type=\"bibr\" target=\"#b36\">Liu et al., 2021;</ref><ref type=\"bibr\" target=\"#b16\">Creager et al., \"#b3\">Arjovsky et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Koyama &amp; Yamaguchi, 2021;</ref><ref type=\"bibr\" target=\"#b36\">Liu et al., 2021)</ref> to accommodate structural information in grap z] is the solution to OOD problem in Eq. 2.</p><p>The proof for Theorem 2 follows a similar line of <ref type=\"bibr\" target=\"#b36\">Liu et al. (2021)</ref>. The above theorems indicate that the objecti n inputs from the same object. Also, <ref type=\"bibr\" target=\"#b16\">Creager et al. (2021)</ref> and <ref type=\"bibr\" target=\"#b36\">Liu et al. (2021)</ref> point out that in most real situations, one h at satisfies p e (y, z, z) = p e (y, z)p e (z).</p><p>Then we reproduce the proof of Theorem 2.1 in <ref type=\"bibr\" target=\"#b36\">Liu et al. (2021)</ref> to prove the result by showing that for arbit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing in-the-wild unseen instances draws increasing concerns, out-ofdistribution (OOD) generalization <ref type=\"bibr\" target=\"#b39\">(Mansour et al., 2009;</ref><ref type=\"bibr\" target=\"#b9\">Blanchard e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing in-the-wild unseen instances draws increasing concerns, out-ofdistribution (OOD) generalization <ref type=\"bibr\" target=\"#b39\">(Mansour et al., 2009;</ref><ref type=\"bibr\" target=\"#b9\">Blanchard e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atrix norms and further adopts proximal gradient methods for handling the non-differentiable issue. <ref type=\"bibr\" target=\"#b14\">Chen et al. (2020b)</ref>; Zhang et al. ( <ref type=\"formula\">2019</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> we use F1 score for evaluation. We consider two GNN architectures as the backbone: GraphSAGE <ref type=\"bibr\" target=\"#b24\">(Hamilton et al., 2017)</ref> and a recently proposed model GPRGNN <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  out-ofdistribution (OOD) generalization <ref type=\"bibr\" target=\"#b39\">(Mansour et al., 2009;</ref><ref type=\"bibr\" target=\"#b9\">Blanchard et al., 2011;</ref><ref type=\"bibr\" target=\"#b40\">Muandet et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s are collected <ref type=\"bibr\" target=\"#b19\">(Fakhraei et al., 2015)</ref>. In financial networks <ref type=\"bibr\" target=\"#b41\">(Pareja et al., 2020)</ref>, the payment flows between transactions ( sed below.  Handling Dynamic Graph Snapshots. We adopt a dynamic financial network dataset Elliptic <ref type=\"bibr\" target=\"#b41\">(Pareja et al., 2020)</ref> that contains dozens of graph snapshots w rformance drop after T7. The reason is that this is the time when the dark market shutdown occurred <ref type=\"bibr\" target=\"#b41\">(Pareja et al., 2020)</ref>. Such an emerging event causes considerab  to detect illicit transactions. We use 5/5/33 snapshots for training/validation/testing. Following <ref type=\"bibr\" target=\"#b41\">Pareja et al. (2020)</ref> we use F1 score for evaluation. We conside. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s are collected <ref type=\"bibr\" target=\"#b19\">(Fakhraei et al., 2015)</ref>. In financial networks <ref type=\"bibr\" target=\"#b41\">(Pareja et al., 2020)</ref>, the payment flows between transactions ( sed below.  Handling Dynamic Graph Snapshots. We adopt a dynamic financial network dataset Elliptic <ref type=\"bibr\" target=\"#b41\">(Pareja et al., 2020)</ref> that contains dozens of graph snapshots w rformance drop after T7. The reason is that this is the time when the dark market shutdown occurred <ref type=\"bibr\" target=\"#b41\">(Pareja et al., 2020)</ref>. Such an emerging event causes considerab  to detect illicit transactions. We use 5/5/33 snapshots for training/validation/testing. Following <ref type=\"bibr\" target=\"#b41\">Pareja et al. (2020)</ref> we use F1 score for evaluation. We conside. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: here exist distribution shifts between training and testing data.</p><p>There is a very recent work <ref type=\"bibr\" target=\"#b5\">(Baranwal et al., 2021)</ref> that endeavors to understand the out-of- oviding a new perspective and methodology for node-level prediction on graphs. Third, compared with <ref type=\"bibr\" target=\"#b5\">Baranwal et al. (2021)</ref> that focus on synthetic datasets and simu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing in-the-wild unseen instances draws increasing concerns, out-ofdistribution (OOD) generalization <ref type=\"bibr\" target=\"#b39\">(Mansour et al., 2009;</ref><ref type=\"bibr\" target=\"#b9\">Blanchard e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d algorithms for learning invariant models, showing promising power for tackling OOD generalization <ref type=\"bibr\" target=\"#b11\">(Chang et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Ahuja et al.,  robust optimization <ref type=\"bibr\" target=\"#b47\">(Sagawa et al., 2019)</ref>, causal attribution <ref type=\"bibr\" target=\"#b11\">(Chang et al., 2020)</ref>, game theory <ref type=\"bibr\" target=\"#b1\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uture works.</p><p>There are a few recent studies that focus on out-of-graph problems. For example, <ref type=\"bibr\" target=\"#b27\">Hu et al. (2020a)</ref> considers transferable active learning over m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mited expressiveness of GNNs, there has been some recent research into the use of absolute encoding <ref type=\"bibr\" target=\"#b35\">(Shaw et al., 2018)</ref>, which consists of adding or concatenating  uishing between true and created edges. Many graph transformer methods also use a relative encoding <ref type=\"bibr\" target=\"#b35\">(Shaw et al., 2018)</ref> in addition to absolute encoding. This stra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: di et al., 2021)</ref>, while distance-based methods include distances to a predefined set of nodes <ref type=\"bibr\" target=\"#b41\">(You et al., 2019)</ref> and shortest path distances between pairs of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tations have been uncovered in this class of GNNs. These include the limited expressiveness of GNNs <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Morris et al.,  s in use today, with extensive examples <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Corso et al., 202 d=\"formula_8\">)</formula><p>This extractor is able to represent the k-subtree structure rooted at u <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2019)</ref>. While this class of structure extractors is  essive than the Weisfeiler-Lehman test due to the expressiveness limitation of message passing GNNs <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2019)</ref>. In practice, a small value of k already lead milton et al., 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b38\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, GIN <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2019)</ref> and PNA <ref type=\"bibr\" target=\"#b4\">(Corso  CLS pooling discussed in Section 4.2. Unlike the remarkable influence of the readout method in GNNs <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2019)</ref>, we observe very little impact in SAT models. tations. The countable assumption on X is generally adopted for expressivity analysis of GNNs (e.g. <ref type=\"bibr\" target=\"#b39\">Xu et al. (2019)</ref>). We assume f to be any mapping rather than ju k is to iteratively update a node's embedding by incorporating information sent from its neighbors. <ref type=\"bibr\" target=\"#b39\">Xu et al. (2019)</ref> provide a general framework of the steps incor e</p><formula xml:id=\"formula_31\">\u2211 w\u2208V f (x w ) = \u2211 w \u2208V f (x w ).</formula><p>Thus, by Lemma 5 of <ref type=\"bibr\" target=\"#b39\">Xu et al. (2019)</ref>, there exists a mapping f such that the multis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures beyond neighborhood aggregation is thus essential to solve these problems.</p><p>Transformers <ref type=\"bibr\" target=\"#b37\">(Vaswani et al., 2017)</ref>, which have shown to be successful in na \">(Vaswani et al., 2017)</ref>, which have shown to be successful in natural language understanding <ref type=\"bibr\" target=\"#b37\">(Vaswani et al., 2017)</ref>, computer vision <ref type=\"bibr\" target ead infer relations between nodes by leveraging the node attributes. In this sense, the Transformer <ref type=\"bibr\" target=\"#b37\">(Vaswani et al., 2017)</ref> ignores the graph structure and rather c of Eq. ( <ref type=\"formula\" target=\"#formula_0\">1</ref>) and has shown to be effective in practice <ref type=\"bibr\" target=\"#b37\">(Vaswani et al., 2017)</ref>. Then, the output of the self-attention  ers for graph-structured data. Specifically:</p><p>\u2022 We reformulate the self-attention mechanism in <ref type=\"bibr\" target=\"#b37\">Vaswani et al. (2017)</ref> as a kernel smoother and extend the origi Loshchilov &amp; Hutter, 2018)</ref> with a standard warm-up strategy suggested for transformers in <ref type=\"bibr\" target=\"#b37\">Vaswani et al. (2017)</ref>. We use either the L1 loss or the cross-e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ions in drug discovery <ref type=\"bibr\" target=\"#b11\">(Gaudelet et al., 2021)</ref>, protein design <ref type=\"bibr\" target=\"#b17\">(Ingraham et al., 2019)</ref>, social network analysis <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se include the limited expressiveness of GNNs <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b30\">Morris et al., 2019)</ref>, as well as known problems such as over-sm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i et al., 2020)</ref>, PATTERN <ref type=\"bibr\" target=\"#b7\">(Dwivedi et al., 2020)</ref>, OGBG-PPA <ref type=\"bibr\" target=\"#b15\">(Hu et al., 2020a)</ref> and OGBG-CODE2 <ref type=\"bibr\" target=\"#b15 t al., 2020)</ref>, OGBG-PPA <ref type=\"bibr\" target=\"#b15\">(Hu et al., 2020a)</ref> and OGBG-CODE2 <ref type=\"bibr\" target=\"#b15\">(Hu et al., 2020a)</ref>. We compare our method to the following GNNs i et al., 2020)</ref>, PATTERN <ref type=\"bibr\" target=\"#b7\">(Dwivedi et al., 2020)</ref>, OGBG-PPA <ref type=\"bibr\" target=\"#b15\">(Hu et al., 2020a)</ref> and OGBG-CODE2 <ref type=\"bibr\" target=\"#b15 t al., 2020)</ref>, OGBG-PPA <ref type=\"bibr\" target=\"#b15\">(Hu et al., 2020a)</ref> and OGBG-CODE2 <ref type=\"bibr\" target=\"#b15\">(Hu et al., 2020a)</ref>. For each dataset, we follow their respectiv splits as is used in <ref type=\"bibr\" target=\"#b7\">Dwivedi et al. (2020)</ref>.</p><p>OGBG-PPA. PPA <ref type=\"bibr\" target=\"#b15\">(Hu et al., 2020a</ref>) is comprised of protein-protein association  ormation relative to the association, such as co-expression. We use the standard splits provided by <ref type=\"bibr\" target=\"#b15\">Hu et al. (2020a)</ref>.  <ref type=\"formula\">2020a</ref>) is a datas ectly classify the sub-tokens that comprise the method name. We use the standard splits provided by <ref type=\"bibr\" target=\"#b15\">Hu et al. (2020a)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: beled graphs. Among many, graph contrastive learning (GCL) <ref type=\"bibr\" target=\"#b52\">[53]</ref><ref type=\"bibr\" target=\"#b53\">[54]</ref><ref type=\"bibr\" target=\"#b54\">[55]</ref> follows the gener  way, the model can learn representations that are invariant to perturbations. For example, GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref> first designs four types of general augmentations (node dr l information and semantics of the graphs varies significantly across domains. For example, GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref> finds that edge perturbation benefits social networks but  >.</p><p>To remedy these issues, several strategies have been proposed recently. Typically, GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref> manually picks data augmentations per dataset by tedious t l trial-and-errors No domain knowledge Preserving semantics No cumbersome search Generality GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref> MoCL <ref type=\"bibr\" target=\"#b39\">[40]</ref> JOAO(v2) <r /1.0\"><head>GraphCL MoCL SimGRACE</head><p>Figure <ref type=\"figure\">1</ref>: Comparison of GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref>, MoCL <ref type=\"bibr\" target=\"#b39\">[40]</ref> and Sim-GR y semantics well while GraphCL can not. Also, we explain why SimGRACE can succeed. Besides, GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref> shows that GNNs can gain robustness using their proposed f obabilistic method to alleviate the issue of false negatives in GCL. For graph-level tasks, GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref> proposes four types of augmentations for general graphs an /div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">AT-SimGRACE</head><p>Recently, GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref> shows that GNNs can gain robustness using their proposed f foGraph <ref type=\"bibr\" target=\"#b38\">[39]</ref> and instance-instance contrastive methods GraphCL <ref type=\"bibr\" target=\"#b53\">[54]</ref>, JOAO(v2) <ref type=\"bibr\" target=\"#b52\">[53]</ref>.</p></  same input as \"positive pairs\". Similar to previous works <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>,</p><p>we take other graph data in the same mini-batch as \" et=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> to enforce the agreement between positive pairs \ud835\udc67 and \ud835\udc67 \u2032 c ame mini-batch as in <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. Denoting the cosine similarity function as sim (\ud835\udc9b, \ud835\udc9b \u2032 ) = sentation learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, we evaluate the generalizability of the learned representa ing previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. Specifically, we pre-train and finetune the models with di versarial robustness (RQ3)</head><p>Following previous works <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>, we perform on synthetic data to classify the component num  type=\"bibr\" target=\"#b5\">[6]</ref> as the GNN encoder as in <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. Besides, we pretrain the GNN encoder for 150 epochs becaus eep fair, we follow the same settings of other competitors <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> via training the GNN encoder with batch size as 128 and num. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sks, such as node, graph classification or graph generation <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. However, most existing GNNs are trained in a supervised ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  learns representations in an unsupervised manner with designed pretext tasks. Initially, Hu et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> propose two pretext tasks, i.e, predicting neighborhood co f type=\"bibr\" target=\"#b0\">[1]</ref>, graph2vec <ref type=\"bibr\" target=\"#b27\">[28]</ref>, EdgePred <ref type=\"bibr\" target=\"#b15\">[16]</ref>, AttrMasking <ref type=\"bibr\" target=\"#b15\">[16]</ref>, Co pe=\"bibr\" target=\"#b27\">[28]</ref>, EdgePred <ref type=\"bibr\" target=\"#b15\">[16]</ref>, AttrMasking <ref type=\"bibr\" target=\"#b15\">[16]</ref>, ContextPred <ref type=\"bibr\" target=\"#b15\">[16]</ref>, In \"bibr\" target=\"#b15\">[16]</ref>, AttrMasking <ref type=\"bibr\" target=\"#b15\">[16]</ref>, ContextPred <ref type=\"bibr\" target=\"#b15\">[16]</ref>, Infomax (DGI) <ref type=\"bibr\" target=\"#b44\">[45]</ref>,  5 layers and 128 hidden dimensions. For transfer learning, we adopt GIN with the default setting in <ref type=\"bibr\" target=\"#b15\">[16]</ref> as the GNN-based encoder. For experiments on adversarial r roperty prediction in chemistry and protein function prediction in biology following previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. However, most existing GNNs are trained in a supervised ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ture-scaled cross entropy loss (NT-Xent) as previous works <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pairs are generated from the other \ud835\udc41 \u2212 1 perturbed representations within the same mini-batch as in <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve learning (GCL) <ref type=\"bibr\" target=\"#b52\">[53]</ref><ref type=\"bibr\" target=\"#b53\">[54]</ref><ref type=\"bibr\" target=\"#b54\">[55]</ref> follows the general framework of contrastive learning in c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: presentations and substructure-level representations of different granularity. More recently, MVGRL <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposes to learn both node-level and graph-level represen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: projection head, after which their mutual information is maximized. Typically, for node-level tasks <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>, GCA <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n we set \ud835\udf02 = 0 in section 4.6.1. Note that BGRL <ref type=\"bibr\" target=\"#b40\">[41]</ref> and MERIT <ref type=\"bibr\" target=\"#b17\">[18]</ref> also update a target network with an online encoder during. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> turns to the tasks of the automatic expansion and completio rages static word embedding, such as Word2Vec and FastText <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, or contextualized embedding solely based on an additional  upervised generation following similar methods proposed in <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, and (X \ud835\udc56 , \ud835\udc66 \ud835\udc56 ) is the generated data pair in the dataset cture.</p><p>Follow the dataset splitting settings used in <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, we sample 1000 nodes for validation and test respectively  positions for each query node, following the guidelines in <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, we utilize the following rank-based metrics to evaluate th taxonomy, such as local egonet <ref type=\"bibr\" target=\"#b18\">[19]</ref>, parentquery-child triplet <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and mini-paths <ref type=\"bibr\" target=\"#b29\">[30]</ref>. n (MLP) for matching, which suffers from the limited representation power. In this paper, we follow <ref type=\"bibr\" target=\"#b33\">[34]</ref> to focus on taxonomy completion, which aims to predict the ormation. Finally, we develop an effective query-position matching model by extending previous work <ref type=\"bibr\" target=\"#b33\">[34]</ref> to incorporate our novel candidate position representation ts a taxonomic relationship between two concepts. Taxonomy Completion. The taxonomy completion task <ref type=\"bibr\" target=\"#b33\">[34]</ref> is defined as following: given an existing taxonomy T 0 an  \ud835\udc4e(\ud835\udc5d))<label>(6)</label></formula><p>where \ud835\udc53 is a parametrized scoring function. The previous study <ref type=\"bibr\" target=\"#b33\">[34]</ref> showed that the simple matching model that learns one-to-o tion component, i.e., the relatedness between \u27e8\ud835\udc5b \ud835\udc5e , \ud835\udc5b \ud835\udc5d \u27e9 and \u27e8\ud835\udc5b \ud835\udc5e , \ud835\udc5b \ud835\udc50 \u27e9. Therefore, inspired by <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we propose a new matching model which incorporates the ad dge semantics and optimizes a large margin ranking loss with a dynamic margin function.</p><p>\u2022 TMN <ref type=\"bibr\" target=\"#b33\">[34]</ref> is a state-of-the-art taxonomy completion framework and al i-view co-training objective. Some other methods were proposed for taxonomy completion, such as TMN <ref type=\"bibr\" target=\"#b33\">[34]</ref> focuses on taxonomy completion task with channel-gating me target=\"#b18\">19]</ref> were designed for taxonomy expansion task. We follow the implementations in <ref type=\"bibr\" target=\"#b33\">[34]</ref> to calculate the ranking of candidate positions from the s ion method and InfoNCE Loss <ref type=\"bibr\" target=\"#b15\">[16]</ref>. In comparison experiments in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, all methods only leveraged the initial embeddings without ults of Taxonomy Completion task on the four large-scale datasets. ** indicates the results are from<ref type=\"bibr\" target=\"#b33\">[34]</ref>.</figDesc><table><row><cell>Method</cell></row></table></f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ndidate siblings and the candidate position, respectively. We adopt the Neural Tensor Network (NTN) <ref type=\"bibr\" target=\"#b21\">[22]</ref> as the base models. Given vectors \ud835\udc62 \u2208 R \ud835\udc51 \ud835\udc62 , \ud835\udc63 \u2208 R \ud835\udc51 \ud835\udc63 , . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion, and fine-grained named entity recognition where people rely on concept taxonomies (e.g., MeSH) <ref type=\"bibr\" target=\"#b8\">[9]</ref> to extract and label useful information from massive corpus.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  this metric indicates the better performance of the model. \u2022 Mean Reciprocal Rank (MRR). We follow <ref type=\"bibr\" target=\"#b28\">[29]</ref> to compute the reciprocal rank of a query concept's true p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cepts structurally <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar  only select partial taxonomies under the computer science (MAG-CS) and psychology (MAG-PSY) domain <ref type=\"bibr\" target=\"#b20\">[21]</ref>. \u2022 WordNet: We collect the concepts and taxonomic relation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> to construct a taxo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b14\">15]</ref> or distributional representations <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rising tide of big data. To tackle this issue, recent work <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar  expansion problem <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For example, ARBOR e different ways to model the structural information in the existing taxonomy, such as local egonet <ref type=\"bibr\" target=\"#b18\">[19]</ref>, parentquery-child triplet <ref type=\"bibr\" target=\"#b33\"> his section, we formally define the taxonomy completion task studied in the paper. Taxonomy. Follow <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we formulate a taxonomy T 0 = {N 0 , E 0 } as a directed  ntific concepts and more than 700 thousand taxonomic relations. We follow the data preprocessing in <ref type=\"bibr\" target=\"#b18\">[19]</ref> to only select partial taxonomies under the computer scien thod serves as a baseline result to check the comparable performance of each framework. \u2022 TaxoExpan <ref type=\"bibr\" target=\"#b18\">[19]</ref> is a state-of-the-art taxonomy expansion framework, which  mies by jointly learning latent representations for edge semantics and taxonomy concepts; TaxoExpan <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposes position-enhanced graph neural networks to encode e output of their matching model, so that we can have similar output for evaluations. For TaxoExpan <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we implemented the full framework with PGAT propagation m omy. Different from prior work which leverages static word embedding, such as Word2Vec and FastText <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, or contextualized D is the dataset formulated by the self-supervised generation following similar methods proposed in <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, and (X \ud835\udc56 , \ud835\udc66 \ud835\udc56 )  r generate a more complete taxonomic structure.</p><p>Follow the dataset splitting settings used in <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, we sample 1000 no 's output is a ranking list of candidate positions for each query node, following the guidelines in <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, we utilize the fo rks. The techniques of encoding features before matching have been experimented by previous methods <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, showing that the  N DETAILS A.1 Baseline Models</head><p>TaxoExpan, ARBORIST <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> were designed for taxonomy expansion task. We follow the im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b11\">[12]</ref>, or only relied on corpus to construct limited seed-guided taxonomy <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Very recently, <ref type=\"bibr\" target=\"#b30\">[31]</ref> co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  this metric indicates the better performance of the model. \u2022 Mean Reciprocal Rank (MRR). We follow <ref type=\"bibr\" target=\"#b28\">[29]</ref> to compute the reciprocal rank of a query concept's true p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Specifically, to capture the \"is-a\" relationship between c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with graph neural networks (GNNs) has gained huge success in tasks involving relational information <ref type=\"bibr\" target=\"#b35\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b26\">Hamilton case study at Sec. 3.2</p><p>We conduct our case studies based on GCN, GIN with mean or sum readout <ref type=\"bibr\" target=\"#b35\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b82\">Xu et al DD and NCI1, NCI109, CMNIST, SST5, and Twitter. Experiments in Sec. 5 use the GCN with mean readout <ref type=\"bibr\" target=\"#b35\">(Kipf and Welling, 2017)</ref> by default for all datasets except Pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  practical solution for its approximation <ref type=\"bibr\" target=\"#b15\">(Chopra et al., 2005;</ref><ref type=\"bibr\" target=\"#b61\">Salakhutdinov and Hinton, 2007;</ref><ref type=\"bibr\" target=\"#b73\">v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctly optimizing for such a complicated problem can hardly be realized without a proper architecture <ref type=\"bibr\" target=\"#b83\">(Xu et al., 2020</ref><ref type=\"bibr\" target=\"#b84\">(Xu et al., , 20 t=\"#b84\">(Xu et al., , 2021))</ref>.</p><p>Inspired by the rationales of GNN reasoning uncovered by <ref type=\"bibr\" target=\"#b83\">Xu et al. (2020)</ref>, we propose the GOOD framework that explicitly target=\"#b71\">(Tang et al., 2020;</ref><ref type=\"bibr\" target=\"#b75\">Velickovic et al., 2020;</ref><ref type=\"bibr\" target=\"#b83\">Xu et al., 2020;</ref><ref type=\"bibr\" target=\"#b80\">Xhonneux et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b43\">Lov\u00e1sz and Szegedy, 2006;</ref><ref type=\"bibr\" target=\"#b90\">You et al., 2018;</ref><ref type=\"bibr\" target=\"#b45\">Luo et al., 2021)</ref>.</p><p>Moreover, a subset of environment late  target=\"#b43\">Lov\u00e1sz and Szegedy, 2006;</ref><ref type=\"bibr\" target=\"#b90\">You et al., 2018;</ref><ref type=\"bibr\" target=\"#b45\">Luo et al., 2021;</ref><ref type=\"bibr\" target=\"#b86\">Yehudai et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b26\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b76\">Veli\u010dkovi\u0107 et al., 2018;</ref><ref type=\"bibr\" target=\"#b81\">Xu et al., 2018</ref><ref type=\"bibr\" target=\"#b82\">Xu et al., , 2019 re powerful readout functions, are two common choices in GNNs to improve the generalization ability <ref type=\"bibr\" target=\"#b81\">(Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b41\">Li et al., 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b91\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Ma et al., 2021;</ref><ref type=\"bibr\" target=\"#b92\">You et al., 2021)</ref>, where the GNN may implicitly learn to identi e=\"bibr\" target=\"#b91\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Ma et al., 2021;</ref><ref type=\"bibr\" target=\"#b92\">You et al., 2021)</ref>, where the GNN may implicitly learn to identi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 25\">Gulrajani and Lopez-Paz, 2021;</ref><ref type=\"bibr\" target=\"#b66\">Sch\u00f6lkopf et al., 2021;</ref><ref type=\"bibr\" target=\"#b39\">Krueger et al., 2021;</ref><ref type=\"bibr\" target=\"#b17\">Creager et   target=\"#b3\">Arjovsky et al., 2019;</ref><ref type=\"bibr\" target=\"#b60\">Sagawa* et al., 2020;</ref><ref type=\"bibr\" target=\"#b39\">Krueger et al., 2021;</ref><ref type=\"bibr\" target=\"#b1\">Ahuja et al. Koyama and Yamaguchi, 2020;</ref><ref type=\"bibr\" target=\"#b25\">Gulrajani and Lopez-Paz, 2021;</ref><ref type=\"bibr\" target=\"#b39\">Krueger et al., 2021;</ref><ref type=\"bibr\" target=\"#b17\">Creager et  st of all, for IRM or similar objectives <ref type=\"bibr\" target=\"#b60\">(Sagawa* et al., 2020;</ref><ref type=\"bibr\" target=\"#b39\">Krueger et al., 2021;</ref><ref type=\"bibr\" target=\"#b1\">Ahuja et al. t al., 2021)</ref>. In addition to IRM, the failure can also happen to other alternative objectives <ref type=\"bibr\" target=\"#b39\">(Krueger et al., 2021;</ref><ref type=\"bibr\" target=\"#b7\">Bellot and  art OOD objectives including IRM <ref type=\"bibr\" target=\"#b3\">(Arjovsky et al., 2019)</ref>, v-Rex <ref type=\"bibr\" target=\"#b39\">(Krueger et al., 2021)</ref> and IB-IRM <ref type=\"bibr\" target=\"#b1\" d Lopez-Paz, 2021)</ref> for IRM <ref type=\"bibr\" target=\"#b3\">(Arjovsky et al., 2019)</ref>, V-Rex <ref type=\"bibr\" target=\"#b39\">(Krueger et al., 2021)</ref> and IB-IRM <ref type=\"bibr\" target=\"#b1\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b94\">Yuan et al., 2020)</ref>. Though some may leverage causality in explanation generation <ref type=\"bibr\" target=\"#b42\">(Lin et al., 2021)</ref>, they mostly focus on understanding the pred. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aset in IRM <ref type=\"bibr\" target=\"#b3\">(Arjovsky et al., 2019)</ref> using the same algorithm as <ref type=\"bibr\" target=\"#b36\">Knyazev et al. (2019)</ref>, and also split Graph-SST <ref type=\"bibr  et al. (2021)</ref> study the neural network extrapolation ability from a geometrical perspective. <ref type=\"bibr\" target=\"#b36\">Knyazev et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b86\">Yehudai e al., 2019)</ref>. We convert the ColoredMnist into graphs using super pixel algorithm introduced by <ref type=\"bibr\" target=\"#b36\">Knyazev et al. (2019)</ref>. Specifically, the original Mnist dataset cture-level properties such as degrees, graph sizes, homophily, as well as feature-level properties <ref type=\"bibr\" target=\"#b36\">(Knyazev et al., 2019;</ref><ref type=\"bibr\" target=\"#b86\">Yehudai et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ww.tei-c.org/ns/1.0\"><head>E.6 Software and hardware</head><p>We implement our methods with PyTorch <ref type=\"bibr\" target=\"#b53\">(Paszke et al., 2019)</ref> and PyTorch Geometric <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b75\">Velickovic et al., 2020;</ref><ref type=\"bibr\" target=\"#b83\">Xu et al., 2020;</ref><ref type=\"bibr\" target=\"#b80\">Xhonneux et al., 2021)</ref>. <ref type=\"bibr\" target=\"#b84\">Xu et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed product search. Existing methods mainly exploit text data. Among them, a recent line of research <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target rs from this problem since it represents a user with all his/her reviews of purchased products. ZAM <ref type=\"bibr\" target=\"#b0\">[1]</ref>, TEM <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and RTM <ref t he usefulness of our approach, we integrate it into a state-of-the-art latent space based model ZAM <ref type=\"bibr\" target=\"#b0\">[1]</ref> and evaluate its performance on eight Amazon public benchmar y with two tasks: the language modeling task and the information retrieval task. Further, Ai et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> considered user history behaviors conditioned on the current h.</p><p>In this work, we employ the popular latent spaced based product search framework as in ZAM <ref type=\"bibr\" target=\"#b0\">[1]</ref> and propose to enrich product representations via graph conv contain enough preference signals. To address the above-mentioned issue, recent works including ZAM <ref type=\"bibr\" target=\"#b0\">[1]</ref>, TEM <ref type=\"bibr\" target=\"#b5\">[6]</ref> and RTM <ref ty t the effect of personalization varies significantly in respect of query characteristics. Ai et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposed ZAM that introduces a zero-vector to adaptively con v> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Model Optimization</head><p>Following ZAM <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we jointly optimize the product retrieval task and the lang has become a benchmark dataset for evaluating product search methods as used in many recent studies <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe , and words jointly. The user-query pair is projected to the same latent space with products. \u2022 ZAM <ref type=\"bibr\" target=\"#b0\">[1]</ref> also employs PV to learn semantic representations of product. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nly exploit text data. Among them, a recent line of research <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= riod of time, but longterm historical user behavior normally contains noisy preference signals. HEM <ref type=\"bibr\" target=\"#b1\">[2]</ref> suffers from this problem since it represents a user with al r\" target=\"#b19\">[20]</ref>, which maps queries and products in the same space. Later on, Ai et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> proposed to consider user preferences and learn embeddings o induced by the irrelevant products in the purchase history. This problem is even more severe in HEM <ref type=\"bibr\" target=\"#b1\">[2]</ref>, which models users independently with their previous purcha k.</p><p>Product Retrieval Task. It aims to retrieve relevant products w.r.t. the current query. In <ref type=\"bibr\" target=\"#b1\">[2]</ref>, the user intent \ud835\udc74 \ud835\udc62\ud835\udc5e is represented by a mix of the query \ud835\udc92 ing Task. It aims to learn the embeddings of queries and products by modeling the text information. <ref type=\"bibr\" target=\"#b1\">[2]</ref> proposes to jointly learn the word embedding \ud835\udc98 and product e \ud835\udc56 are the negative sampling rates for words and products, respectively. In this work, we follow HEM <ref type=\"bibr\" target=\"#b1\">[2]</ref> to randomly sample negative words from the vocabulary with \ud835\udc43 uating product search methods as used in many recent studies <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" targe >Baselines.</head><p>We compare our proposed approach SBG with the following baselines.</p><p>\u2022 HEM <ref type=\"bibr\" target=\"#b1\">[2]</ref> assumes that users and products are independent. It employs . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s and product cooccurrence relationships, which are usually encoded in a graph. Some recent efforts <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> have been devoted to ibr\" target=\"#b9\">10]</ref>. Recently, some studies attempted to model such information with graphs <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target  been devoted to exploiting structural graph information for personalized product search. Ai et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a dynamic relation embedding model (DREM). DREM con ve upon the base model and achieve better performance than other graph-based methods including DREM <ref type=\"bibr\" target=\"#b2\">[3]</ref> and GraphSRRL <ref type=\"bibr\" target=\"#b26\">[27]</ref>. It  l product textual semantic relationships with hypergraph to learn structural information. Ai et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed DREM (Dynamic Relation Embedding Model) that constr n many recent studies <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Product reviews are icular, ZAM proposes a zero attention vector to control the degree of personalization.</p><p>\u2022 DREM <ref type=\"bibr\" target=\"#b2\">[3]</ref> employs knowledge graph embedding techniques and constructs . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nals or user behavior traces for search and ranking problems <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Recently, some studie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s and product cooccurrence relationships, which are usually encoded in a graph. Some recent efforts <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> have been devoted to ibr\" target=\"#b9\">10]</ref>. Recently, some studies attempted to model such information with graphs <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target  been devoted to exploiting structural graph information for personalized product search. Ai et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a dynamic relation embedding model (DREM). DREM con ve upon the base model and achieve better performance than other graph-based methods including DREM <ref type=\"bibr\" target=\"#b2\">[3]</ref> and GraphSRRL <ref type=\"bibr\" target=\"#b26\">[27]</ref>. It  l product textual semantic relationships with hypergraph to learn structural information. Ai et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed DREM (Dynamic Relation Embedding Model) that constr n many recent studies <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Product reviews are icular, ZAM proposes a zero attention vector to control the degree of personalization.</p><p>\u2022 DREM <ref type=\"bibr\" target=\"#b2\">[3]</ref> employs knowledge graph embedding techniques and constructs . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> employ GNN <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref ty ormation over graphs. Our work is closer to GCE-GNN (Global Context Enhanced Graph Neural Networks) <ref type=\"bibr\" target=\"#b35\">[36]</ref> in that we both make use  of global information. The diffe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: few years, graph convolutional networks (GCN) and variants <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he projection layers to enrich product representations. Following the efficient design in Li et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we remove the projection layers and activation layers. Fu \ud835\udc70 \u2212 (1 \u2212 \ud835\udf14)\ud835\udeb2. Denote \ud835\udc50 = \ud835\udc7c \u22a4 \u210e (0) and substitute \ud835\udc6d in Eq. <ref type=\"bibr\" target=\"#b23\">(24,</ref><ref type=\"bibr\" target=\"#b24\">25)</ref> by <ref type=\"bibr\" target=\"#b25\">(26)</ref>:</p><p>where</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s and product cooccurrence relationships, which are usually encoded in a graph. Some recent efforts <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> have been devoted to ibr\" target=\"#b9\">10]</ref>. Recently, some studies attempted to model such information with graphs <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target  been devoted to exploiting structural graph information for personalized product search. Ai et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a dynamic relation embedding model (DREM). DREM con ve upon the base model and achieve better performance than other graph-based methods including DREM <ref type=\"bibr\" target=\"#b2\">[3]</ref> and GraphSRRL <ref type=\"bibr\" target=\"#b26\">[27]</ref>. It  l product textual semantic relationships with hypergraph to learn structural information. Ai et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed DREM (Dynamic Relation Embedding Model) that constr n many recent studies <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Product reviews are icular, ZAM proposes a zero attention vector to control the degree of personalization.</p><p>\u2022 DREM <ref type=\"bibr\" target=\"#b2\">[3]</ref> employs knowledge graph embedding techniques and constructs . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nals or user behavior traces for search and ranking problems <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Recently, some studie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  search. In recent years, neural network based models have dominated the research of product search <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" targ r Theorem 1. For a better reading experience, we rewrite Eq. <ref type=\"bibr\" target=\"#b5\">(6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b6\">7)</ref> here. The Laplacian-Bel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2021;</ref><ref type=\"bibr\">Ma et al., 2021;</ref><ref type=\"bibr\">Zhang et al., 2020;</ref><ref type=\"bibr\" target=\"#b37\">Wu, 2021)</ref> has been extensively studied in recent years.</p><p>C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 3)</ref> built two support vector regression models to specifically predict seasonal traffic flows. <ref type=\"bibr\" target=\"#b34\">Wang et al. (2018)</ref> proposed a support vector machine partial on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nvolutional Neural Network (CNN):</head><p>We apply two CNN layers and three fully connected layers <ref type=\"bibr\" target=\"#b20\">(Ma et al., 2017)</ref>. The kernel size is 3*3. The optimizer is Ada. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  each station in URT can effectively help them plan travel routes, thus saving travel time and cost <ref type=\"bibr\" target=\"#b0\">(Cheng et al., 2021;</ref><ref type=\"bibr\" target=\"#b25\">Noursalehi et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dress the periodicity, nonlinearity, uncertainty, and complexity of short-term traffic predictions. <ref type=\"bibr\" target=\"#b13\">Lippi et al. (2013)</ref> built two support vector regression models . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mputer vision <ref type=\"bibr\" target=\"#b2\">(Garcia and Bruna, 2017)</ref>, traffic flow prediction <ref type=\"bibr\" target=\"#b9\">(Lei et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Li et al., 2021 applied to the traffic network to capture the topological relationships between the nodes or links. <ref type=\"bibr\" target=\"#b9\">Lei et al. (2019)</ref> introduced a novel GCN-GAN model to tackle the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orld graphs. Therefore, we use the step function <ref type=\"bibr\" target=\"#b22\">(Lov\u00e1sz, 2012;</ref><ref type=\"bibr\" target=\"#b39\">Xu et al., 2021)</ref> to approximate graphons 1 . In general, the st ction with K partitions to approximate a graphon, the complexity of used graphon estimation methods <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2021)</ref> is in Table <ref type=\"table\" target=\"#tab_3\" upper bounded. And we also copy the results of graphon estimation methods on synthetic graphon from <ref type=\"bibr\" target=\"#b39\">(Xu et al., 2021)</ref> in Table <ref type=\"table\" target=\"#tab_9\">6< e 6 .</head><label>6</label><figDesc>The MSE error of graphon estimation methods on synthetic graphs<ref type=\"bibr\" target=\"#b39\">(Xu et al., 2021)</ref>. The graphon estimation is based on 10 graphs ://www.tei-c.org/ns/1.0\"><head>A.4. Graphons Estimation by Step Function</head><p>The proof follows <ref type=\"bibr\" target=\"#b39\">Xu et al. (2021)</ref>. A graphon can always be approximated by a ste. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t learns a clustering function that can be quickly evaluated on out-of-sample graphs.</p><p>\u2022 GMT 9 <ref type=\"bibr\" target=\"#b2\">(Baek et al., 2020</ref>) is a multi-head attention based global pooli further validate the effectiveness of G-Mixup on more graph neural networks, we experiment with GMT <ref type=\"bibr\" target=\"#b2\">(Baek et al., 2020)</ref>, a modern pooling method. To reproduce GMT r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ta augmentation, including node perturbation <ref type=\"bibr\" target=\"#b42\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Huang et al., 2018)</ref>, edge perturbation <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ications, e.g., creating graphs for graph contrastive learning, than Manifold Mixup. Manifold Mixup <ref type=\"bibr\" target=\"#b37\">(Wang et al., 2021)</ref> is proposed to mix up graphs in the embeddi enerated graph will keep part of the the semantic meaning of original graphs.</p><p>\u2022 M-Manifold 13 <ref type=\"bibr\" target=\"#b37\">(Wang et al., 2021)</ref> Manifold-Mixup conducts Mixup operation for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t learns a clustering function that can be quickly evaluated on out-of-sample graphs.</p><p>\u2022 GMT 9 <ref type=\"bibr\" target=\"#b2\">(Baek et al., 2020</ref>) is a multi-head attention based global pooli further validate the effectiveness of G-Mixup on more graph neural networks, we experiment with GMT <ref type=\"bibr\" target=\"#b2\">(Baek et al., 2020)</ref>, a modern pooling method. To reproduce GMT r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: architectures, which maps nodes to clusters based on their learned embeddings.</p><p>\u2022 MincutPool 8 <ref type=\"bibr\" target=\"#b3\">(Bianchi et al., 2020</ref>) is a differentiable pooling baselines. It. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ugmentation methods for node classification <ref type=\"bibr\" target=\"#b48\">(Zhao et al., 2021;</ref><ref type=\"bibr\" target=\"#b36\">Wang et al., 2020b;</ref><ref type=\"bibr\" target=\"#b30\">Tang et al.,  h data augmentation for node classification <ref type=\"bibr\" target=\"#b48\">(Zhao et al., 2021;</ref><ref type=\"bibr\" target=\"#b36\">Wang et al., 2020b;</ref><ref type=\"bibr\" target=\"#b30\">Tang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )| . For example, t( , G) = |V (G)|/N 1 = 1, t( , G) = 2|E(G)|/N 2 .</formula><p>Graphon. A graphon <ref type=\"bibr\" target=\"#b0\">(Airoldi et al., 2013)</ref> is a continuous, bounded and symmetric fu ref type=\"bibr\" target=\"#b5\">(Chan &amp; Airoldi, 2014)</ref>, stochastic block approximation (SBA) <ref type=\"bibr\" target=\"#b0\">(Airoldi et al., 2013)</ref>, \"largest gap\" (LG) <ref type=\"bibr\" targ ep functions, one is based on stochastic block models, such as stochastic block approximation (SBA) <ref type=\"bibr\" target=\"#b0\">(Airoldi et al., 2013)</ref>, \"largest gap\" (LG) <ref type=\"bibr\" targ r work can precisely estimate graphon. The details of them are listed as the following:</p><p>\u2022 SBA <ref type=\"bibr\" target=\"#b0\">(Airoldi et al., 2013)</ref> The Stochastic Block Approximation learns ref type=\"bibr\" target=\"#b5\">(Chan &amp; Airoldi, 2014)</ref>, stochastic block approximation (SBA) <ref type=\"bibr\" target=\"#b0\">(Airoldi et al., 2013)</ref>, \"largest gap\" (LG) <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: est gap\" (LG) <ref type=\"bibr\" target=\"#b6\">(Channarond et al., 2012)</ref>, matrix completion (MC) <ref type=\"bibr\" target=\"#b19\">(Keshavan et al., 2010)</ref>, universal singular value thresholding   2014)</ref>; another one is based on low-rank matrix decomposition, such as matrix completion (MC) <ref type=\"bibr\" target=\"#b19\">(Keshavan et al., 2010)</ref>, universal singular value thresholding  est gap\" (LG) <ref type=\"bibr\" target=\"#b6\">(Channarond et al., 2012)</ref>, matrix completion (MC) <ref type=\"bibr\" target=\"#b19\">(Keshavan et al., 2010)</ref> and the universal singular value thresh  node degree, then smooths the sorted graph using total variation minimization.</p><p>\u2022 MC and USVT <ref type=\"bibr\" target=\"#b19\">(Keshavan et al., 2010;</ref><ref type=\"bibr\" target=\"#b7\">Chatterjee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b44\">(Zhang et al., 2017;</ref><ref type=\"bibr\" target=\"#b32\">Verma et al., 2019a;</ref><ref type=\"bibr\" target=\"#b45\">Zhang et al., 2021)</ref> and natural language processing <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned for a particular application scenario <ref type=\"bibr\" target=\"#b33\">(Z\u00fcgner et al. 2019b;</ref><ref type=\"bibr\" target=\"#b2\">Bojchevski et al. 2019b;</ref><ref type=\"bibr\" target=\"#b20\">Wang et a bibr\" target=\"#b22\">(Xu et al. 2020a;</ref><ref type=\"bibr\" target=\"#b33\">Z\u00fcgner et al. 2019b;</ref><ref type=\"bibr\" target=\"#b2\">Bojchevski et al. 2019b)</ref>, which is not the case in our setting. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e learning models for relevant downstream tasks <ref type=\"bibr\" target=\"#b12\">(Hu et al. 2019</ref><ref type=\"bibr\" target=\"#b13\">(Hu et al. , 2020;;</ref><ref type=\"bibr\" target=\"#b17\">Qiu et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: provement in the quality of representations <ref type=\"bibr\" target=\"#b11\">(Hjelm et al. 2019;</ref><ref type=\"bibr\" target=\"#b19\">Veli\u010dkovi\u0107 et al. 2019)</ref>. Inspired by recent work Deep Graph Inf type=\"bibr\" target=\"#b19\">Veli\u010dkovi\u0107 et al. 2019)</ref>. Inspired by recent work Deep Graph Infomax <ref type=\"bibr\" target=\"#b19\">(Veli\u010dkovi\u0107 et al. 2019)</ref>, we use a noise-contrastive type objec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uld propagate to every downstream task via the perturbed graph representation.</p><p>classification <ref type=\"bibr\" target=\"#b27\">(Xu et al. 2019b</ref>). However, most deep learning models on graphs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uld propagate to every downstream task via the perturbed graph representation.</p><p>classification <ref type=\"bibr\" target=\"#b27\">(Xu et al. 2019b</ref>). However, most deep learning models on graphs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uld propagate to every downstream task via the perturbed graph representation.</p><p>classification <ref type=\"bibr\" target=\"#b27\">(Xu et al. 2019b</ref>). However, most deep learning models on graphs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: provement in the quality of representations <ref type=\"bibr\" target=\"#b11\">(Hjelm et al. 2019;</ref><ref type=\"bibr\" target=\"#b19\">Veli\u010dkovi\u0107 et al. 2019)</ref>. Inspired by recent work Deep Graph Inf type=\"bibr\" target=\"#b19\">Veli\u010dkovi\u0107 et al. 2019)</ref>. Inspired by recent work Deep Graph Infomax <ref type=\"bibr\" target=\"#b19\">(Veli\u010dkovi\u0107 et al. 2019)</ref>, we use a noise-contrastive type objec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d finance. Owing to their prevalence, deep learning on graphs, such as graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b15\">(Kipf et al. 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al pid development, and made major progress in various analytical tasks, including node classification <ref type=\"bibr\" target=\"#b15\">(Kipf et al. 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al (Kipf et al. 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al. 2017)</ref>, link prediction <ref type=\"bibr\" target=\"#b15\">(Kipf et al. 2016)</ref>, and graph Y &lt; l a t e x i t s h a 1 _ b  i et al. 2014;</ref><ref type=\"bibr\" target=\"#b9\">Grover et al. 2016)</ref> or reconstruction-based <ref type=\"bibr\" target=\"#b15\">(Kipf et al. 2016)</ref>. These objectives impose an inductive bias t coder. GNN has been extensively used as an expressive function for parameterizing the graph encoder <ref type=\"bibr\" target=\"#b15\">(Kipf et al. 2016</ref><ref type=\"bibr\" target=\"#b15\">(Kipf et al. ,  function for parameterizing the graph encoder <ref type=\"bibr\" target=\"#b15\">(Kipf et al. 2016</ref><ref type=\"bibr\" target=\"#b15\">(Kipf et al. , 2017))</ref>. In this paper, we adopt a one-layer GNN:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>An analogy to the graph representation vulnerability (GRV) has been studied in the image domain <ref type=\"bibr\" target=\"#b31\">(Zhu et al. 2020)</ref>. However, the extension of <ref type=\"bibr\" t  the image domain <ref type=\"bibr\" target=\"#b31\">(Zhu et al. 2020)</ref>. However, the extension of <ref type=\"bibr\" target=\"#b31\">(Zhu et al. 2020)</ref> to the graph domain requires nontrivial effor s measures (i.e., RV * ( ) and GRV * ( )) are built upon the representation space Z. The prior work <ref type=\"bibr\" target=\"#b31\">(Zhu et al. 2020)</ref>, which defines RV ( ) on a single input space ound of adversarial risk over any downstream classifiers that involves both MI and GRV. Theorem 4.3 <ref type=\"bibr\" target=\"#b31\">(Zhu et al. 2020)</ref>. Let (S, ) be the input metric space, Z be th fficult to estimate. Thus, the optimization method we apply is substantially different from that in <ref type=\"bibr\" target=\"#b31\">(Zhu et al. 2020</ref>); see \u00a73.2 and \u00a73.3. Furthermore, more complic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e learning models for relevant downstream tasks <ref type=\"bibr\" target=\"#b12\">(Hu et al. 2019</ref><ref type=\"bibr\" target=\"#b13\">(Hu et al. , 2020;;</ref><ref type=\"bibr\" target=\"#b17\">Qiu et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad coverage of the protein universe, as found in the 17929 families of the recent Pfam 32.0 release <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Recent work that applies deep learning is either restrict database is carefully curated, at least 25% of sequences have no experimentally validation function <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and additional experimental functional characterization o s we use the highly curated Protein families (Pfam) database <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. The 17929 families of the Pfam 32.0 release are labelled u otKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Many domains have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  each. SECLAF uses hundreds of SwissProt classes with more than 150 sequences to train a deep model <ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#tab_0\">1</ref>).</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>BLASTp</head><p>BLASTp <ref type=\"bibr\" target=\"#b37\">[38]</ref> is one of the most well known algorithms for searching for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pare classification performance with existing methods <ref type=\"bibr\" target=\"#b22\">[23]</ref>. In <ref type=\"bibr\" target=\"#b23\">[24]</ref> a graph convolutional network reduces the required family . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ot be annotated through alignment to characterized sequences <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Moreover, the run times of methods such as BLASTp scale near. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> (right) shows the BLOSUM62 matrix, created using aligned sequence blocks at roughly 62% identity <ref type=\"bibr\" target=\"#b25\">[26]</ref>.  The structural similarities between these matrices sugge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pare classification performance with existing methods <ref type=\"bibr\" target=\"#b22\">[23]</ref>. In <ref type=\"bibr\" target=\"#b23\">[24]</ref> a graph convolutional network reduces the required family . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: unknown function and supervised learning on small datasets <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>  We compute an average embedding for each of the 12361 larg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rain a deep model <ref type=\"bibr\" target=\"#b17\">[18]</ref><ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref>. DEEPre uses sequence and Pfam annotations to predict enzy tion, including 74.5% of proteins from reference proteomes <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Many domains have functional annotations, although at leas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#tab_0\">1</ref>).</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>BLASTp</head><p>BLASTp <ref type=\"bibr\" target=\"#b37\">[38]</ref> is one of the most well known algorithms for searching for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: At train time, we present the model with randomly-drawn batches. Consistent with popular experience <ref type=\"bibr\" target=\"#b35\">[36]</ref>, we find that it is important to use gradient clipping for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntext, it uses a GCN based encoder and a dot product decoder. Linear variational Graph autoencoders <ref type=\"bibr\" target=\"#b32\">[33]</ref> simplify VGAE by defining the encoder to be a linear trans ww.tei-c.org/ns/1.0\"><head n=\"3.3\">Linear Graph Embedding</head><p>Linear graph autoencoders (LGAE) <ref type=\"bibr\" target=\"#b32\">[33]</ref> have shown that a linear encoder with an inner product dec rithm with the normalized Laplacian as the input similarity matrix. (3) Methods that use both. LVAE <ref type=\"bibr\" target=\"#b32\">[33]</ref> is the linear graph variational autoencoder and LAE is its. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e attributes commonly learn representations before applying classical clustering algorithms on them <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. However, some rec p>Our choice for the aggregate function is inspired by the simple graph convolution proposed in SGC <ref type=\"bibr\" target=\"#b41\">[42]</ref>. We set</p><formula xml:id=\"formula_3\">agg(A, X) = T \ud835\udc5d X<l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b46\">47]</ref>, computer vision <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, Natural language p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ese methods can be based on approaches such as factorization <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, random walks <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d.</p><p>Initializing W and G. Initializing W with PCA costs O (\ud835\udc5b\ud835\udc51 log(\ud835\udc58)) operations as claimed in <ref type=\"bibr\" target=\"#b15\">[16]</ref>. For G, computing T \ud835\udc5d XW takes O (\ud835\udc5b\ud835\udc51 \ud835\udc53 ) Updating F. In ( . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ular approach. This procedure is carried out sequentially and is referred to as the tandem approach <ref type=\"bibr\" target=\"#b42\">[43]</ref>. However, AE may sometimes be unsuitable for reducing dime \ud835\udc5d XW \u2212 GF 2 s.t. G \u2208 {0, 1} \ud835\udc5b\u00d7\ud835\udc58 , G1 \ud835\udc58 = 1 \ud835\udc5b , W \u22a4 W = I \ud835\udc58<label>(5)</label></formula><p>Similar to <ref type=\"bibr\" target=\"#b42\">[43]</ref>, solving this problem can be proven to be equivalent to mi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: combination of deep representation learning techniques such as the autoencoder (AE), variational AE <ref type=\"bibr\" target=\"#b3\">[4]</ref> and convolutional AE <ref type=\"bibr\" target=\"#b13\">[14,</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: djusting them to a have a clustering-friendly structure can lead to a better clustering performance <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: titioning data, and a number of approaches are reported in the literature. Recently, the authors in <ref type=\"bibr\" target=\"#b19\">[20]</ref> have performed experiments on the sequential combination o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, computer vision <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s are categorized as follows:</p><p>(1) Methods that use node-level features only. Spherical kmeans <ref type=\"bibr\" target=\"#b17\">[18]</ref> is k-means applied on data projected on the unit sphere. I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raph. Algorithms that rely on graph structure exclusively include the spectral clustering algorithm <ref type=\"bibr\" target=\"#b27\">[28]</ref> that optimizes the ratio and normalized-cut criteria. Grac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s the autoencoder (AE), variational AE <ref type=\"bibr\" target=\"#b3\">[4]</ref> and convolutional AE <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, and some popular . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ization <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, random walks <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, or neighborhood a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the original graph representation. These methods can be based on approaches such as factorization <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, random walks <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b45\">46]</ref>, Natural language processing <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and physical systems <ref type=\"bibr\" target=\"#b18\">[19,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  efficient GCN-based approaches for the simultaneous embedding and clustering have recently emerged <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. In this paper, we p ustering process through an adaptive rule for neighborhood order selection. Deep Modularity Network <ref type=\"bibr\" target=\"#b4\">[5]</ref> clusters the graph by maximizing spectral modularity. Graph  ror and by considering the observations to be the rows of T \ud835\udc5d X we get the problem as formulated in <ref type=\"bibr\" target=\"#b4\">(5)</ref>. As for the optimization process, the update rule is the sam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, random walks <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, or neighborhood autoencoders <ref type=\"bibr\" target=\"#b40. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: djusting them to a have a clustering-friendly structure can lead to a better clustering performance <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Learning representations that are both faithful to the dat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s are categorized as follows:</p><p>(1) Methods that use node-level features only. Spherical kmeans <ref type=\"bibr\" target=\"#b17\">[18]</ref> is k-means applied on data projected on the unit sphere. I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: djusting them to a have a clustering-friendly structure can lead to a better clustering performance <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref> performs neighborhood sampling to control the number of neighbors to be aggregated; ClusterGCN <ref type=\"bibr\" target=\"#b1\">[2]</ref> first partitions the original graph into non-overlapped subg ubgraphs, and then reduce the computation cost. For example, inspired by mini-batch SGD, ClusterGCN <ref type=\"bibr\" target=\"#b1\">[2]</ref> uses non-overlapped partition methods to split the original  eaving the original graph structure unchanged, which is different from the operations in ClusterGCN <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The advantages are two-fold: 1) preserving the original gra compatible with all kinds of base GNN models, because various graph operations like graph partition <ref type=\"bibr\" target=\"#b1\">[2]</ref>[26] and inception-like <ref type=\"bibr\" target=\"#b14\">[15]</ ng</cell></row></table><note>GraphSAGE [4], SGC<ref type=\"bibr\" target=\"#b20\">[21]</ref>, ClusterGCN<ref type=\"bibr\" target=\"#b1\">[2]</ref>, GraphSAINT<ref type=\"bibr\" target=\"#b25\">[26]</ref>, SIGN<r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 1.0\"><head n=\"3.4\">Meta Adapter</head><p>Inspired by the global-to-local learning framework in MAML <ref type=\"bibr\" target=\"#b2\">[3]</ref> <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which aims at tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et of papers that are published in arXiv. All paper nodes are indexed by ogbn-papers100M in advance <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Each node represents an arXiv paper, and each directed edge re products sold by Amazon and edges represent whether two items are purchased by the same customer <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Each product node comes with a 100-dimensional feature vect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: data and widespread applications such as social networks <ref type=\"bibr\" target=\"#b10\">[11]</ref>  <ref type=\"bibr\" target=\"#b22\">[23]</ref>, recommendation systems <ref type=\"bibr\" target=\"#b5\">[6]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et of papers that are published in arXiv. All paper nodes are indexed by ogbn-papers100M in advance <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Each node represents an arXiv paper, and each directed edge re products sold by Amazon and edges represent whether two items are purchased by the same customer <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Each product node comes with a 100-dimensional feature vect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igning effective strategies to train GNNs efficiently on large-scale graphs. For example, GraphSAGE <ref type=\"bibr\" target=\"#b3\">[4]</ref> performs neighborhood sampling to control the number of neig plosion\" problem in large graphs, neighbor sampling methods received wide research focus. GraphSAGE <ref type=\"bibr\" target=\"#b3\">[4]</ref> randomly samples several neighbors for each node, which grea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igning effective strategies to train GNNs efficiently on large-scale graphs. For example, GraphSAGE <ref type=\"bibr\" target=\"#b3\">[4]</ref> performs neighborhood sampling to control the number of neig plosion\" problem in large graphs, neighbor sampling methods received wide research focus. GraphSAGE <ref type=\"bibr\" target=\"#b3\">[4]</ref> randomly samples several neighbors for each node, which grea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igning effective strategies to train GNNs efficiently on large-scale graphs. For example, GraphSAGE <ref type=\"bibr\" target=\"#b3\">[4]</ref> performs neighborhood sampling to control the number of neig plosion\" problem in large graphs, neighbor sampling methods received wide research focus. GraphSAGE <ref type=\"bibr\" target=\"#b3\">[4]</ref> randomly samples several neighbors for each node, which grea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b12\">[13]</ref> borrows the ideas from ResNet <ref type=\"bibr\" target=\"#b4\">[5]</ref> and DenseNet <ref type=\"bibr\" target=\"#b18\">[19]</ref> through residual connections and dense connections to help. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b1\">[2]</ref> first partitions the original graph into non-overlapped subgraphs with METIS <ref type=\"bibr\" target=\"#b7\">[8]</ref>, then performs graph convolutions on each subgraph. By elimi  a one-million graph (Arxiv1M) for example. We divide the original graph into 30 subgroups by METIS <ref type=\"bibr\" target=\"#b7\">[8]</ref> algorithm. After that, we conduct separate performance tests raphs. How to partition the graph is not the research focus of this paper, and here we choose METIS <ref type=\"bibr\" target=\"#b7\">[8]</ref> after considering its high efficiency and effectiveness in g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b22\">[23]</ref>, recommendation systems <ref type=\"bibr\" target=\"#b5\">[6]</ref>  <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and bioinformatics <ref type=\"bibr\" target=\"#b28\">[29]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . For example, aiming at addressing the problems of over smoothness and vanishing gradient, DeepGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> borrows the ideas from ResNet <ref type=\"bibr\" target=\"#b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for weighted sampling. Through sampling the receptive domain of each layer with importance, FastGCN <ref type=\"bibr\" target=\"#b0\">[1]</ref> ensures that the important nodes will have a great chance to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Inspired by the global-to-local learning framework in MAML <ref type=\"bibr\" target=\"#b2\">[3]</ref> <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which aims at training a good global initialization \ud835\udf03 tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[19]</ref> through residual connections and dense connections to help models go deeper. Wu et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> offers a more comprehensive survey on existing graph neura. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s graph convolutions on each subgraph. By eliminating the bias during subgraph sampling, GraphSaint <ref type=\"bibr\" target=\"#b25\">[26]</ref> obtains a better way for subgraphs generation. On the othe al subgraphs and then trains GCN in each subgraph with less time and memory consumption. GraphSAINT <ref type=\"bibr\" target=\"#b25\">[26]</ref> also tries to restrict nodes' receptive fields through gen  type=\"bibr\" target=\"#b20\">[21]</ref>, ClusterGCN<ref type=\"bibr\" target=\"#b1\">[2]</ref>, GraphSAINT<ref type=\"bibr\" target=\"#b25\">[26]</ref>, SIGN<ref type=\"bibr\" target=\"#b14\">[15]</ref>, and SAGN<r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s graph convolutions on each subgraph. By eliminating the bias during subgraph sampling, GraphSaint <ref type=\"bibr\" target=\"#b25\">[26]</ref> obtains a better way for subgraphs generation. On the othe al subgraphs and then trains GCN in each subgraph with less time and memory consumption. GraphSAINT <ref type=\"bibr\" target=\"#b25\">[26]</ref> also tries to restrict nodes' receptive fields through gen  type=\"bibr\" target=\"#b20\">[21]</ref>, ClusterGCN<ref type=\"bibr\" target=\"#b1\">[2]</ref>, GraphSAINT<ref type=\"bibr\" target=\"#b25\">[26]</ref>, SIGN<ref type=\"bibr\" target=\"#b14\">[15]</ref>, and SAGN<r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[19]</ref> through residual connections and dense connections to help models go deeper. Wu et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> offers a more comprehensive survey on existing graph neura. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . For example, aiming at addressing the problems of over smoothness and vanishing gradient, DeepGCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> borrows the ideas from ResNet <ref type=\"bibr\" target=\"#b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  be obtained, informative to distinguish subgroups, and can directly influence the model prediction <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>To this end, we propose a feature enhancement modul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et of papers that are published in arXiv. All paper nodes are indexed by ogbn-papers100M in advance <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Each node represents an arXiv paper, and each directed edge re products sold by Amazon and edges represent whether two items are purchased by the same customer <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Each product node comes with a 100-dimensional feature vect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: P classifier. Based on SIGN, the concatenation operation is replaced by attention mechanism in SAGN <ref type=\"bibr\" target=\"#b17\">[18]</ref> to further enhance the expressiveness of post classifier, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mples on the fly is computationally very expensive. We address this by extending the MoCo technique <ref type=\"bibr\" target=\"#b15\">[16]</ref> to support two negative queues, each of which corresponds  mples on the fly is quite expensive. To address this issue, we propose to extend the MoCo technique <ref type=\"bibr\" target=\"#b15\">[16]</ref> for SelfKG. In Moco, a negative queue is maintained to sto eal number of negative samples used for the current batch is (1 + \ud835\udc3e) \u00d7 \ud835\udc41 \u2212 1.</p><p>Momentum update <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The main challenge brought by negative queues is the obso  of using large number negative samples in SelfKG, by leveraging multiple negative queues with Moco <ref type=\"bibr\" target=\"#b15\">[16]</ref>, the running time of SelfKG is significantly reduced even   sampling that furthers Eq. 2 into Eq. 5 to avoid false-negative samples; (3) the extension of MoCo <ref type=\"bibr\" target=\"#b15\">[16]</ref> to two negative queues to support an efficient usage of ma -supervised learning is recently proposed by MoCo and SimCLR <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> in computer vision to conduct successful vision pretraining. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ned pairs for test.</p><p>We observe that SelfKG beats all previous supervised ones except for HMAN <ref type=\"bibr\" target=\"#b42\">[43]</ref>, CEAFF <ref type=\"bibr\" target=\"#b48\">[49]</ref> and BERT- mance compared to FastText word embeddings. This is also the case in baseline methods, such as HMAN <ref type=\"bibr\" target=\"#b42\">[43]</ref> and BERT-INT <ref type=\"bibr\" target=\"#b34\">[35]</ref>, wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b34\">[35]</ref>, CEAFF <ref type=\"bibr\" target=\"#b48\">[49]</ref> or NAEA <ref type=\"bibr\" target=\"#b54\">[55]</ref>. According to the used proportion of the training labels, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and crossentropy loss <ref type=\"bibr\" target=\"#b49\">[50]</ref> have been widely adopted as the similarity metric. Without n BERT and substantially improves the supervised entity alignment performance on public benchmarks. <ref type=\"bibr\" target=\"#b49\">[50]</ref> designs heterogeneous graph attention networks to perform . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b18\">19]</ref>, and question answering <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. Constructing largescale KGs has been a very challenging ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: _1\">4</ref> datasets we used, we do simple data processing on the original datasets built in BootEA <ref type=\"bibr\" target=\"#b30\">[31]</ref> and JAPE <ref type=\"bibr\" target=\"#b29\">[30]</ref> respect 15K is a multi-lingual dataset.</p><p>DWY100K. The DWY100K dataset used here is originally built by <ref type=\"bibr\" target=\"#b30\">[31]</ref>. DWY100K consists of two large datasets: DWY100K dbp_wd (D hood information can be very noisy.</p><p>Experiment Setup. We follow the original split of DWY100K <ref type=\"bibr\" target=\"#b30\">[31]</ref> and DBP15K <ref type=\"bibr\" target=\"#b29\">[30]</ref> which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raph structures has been recently explored for the problem of entity alignment. Though some studies <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar </ref>, while some other methods including RDGCN <ref type=\"bibr\" target=\"#b39\">[40]</ref> and DGMC <ref type=\"bibr\" target=\"#b9\">[10]</ref> uses machine translation (Google translation) to translate . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: _1\">4</ref> datasets we used, we do simple data processing on the original datasets built in BootEA <ref type=\"bibr\" target=\"#b30\">[31]</ref> and JAPE <ref type=\"bibr\" target=\"#b29\">[30]</ref> respect 15K is a multi-lingual dataset.</p><p>DWY100K. The DWY100K dataset used here is originally built by <ref type=\"bibr\" target=\"#b30\">[31]</ref>. DWY100K consists of two large datasets: DWY100K dbp_wd (D hood information can be very noisy.</p><p>Experiment Setup. We follow the original split of DWY100K <ref type=\"bibr\" target=\"#b30\">[31]</ref> and DBP15K <ref type=\"bibr\" target=\"#b29\">[30]</ref> which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esses of the attention mechanism on various applications <ref type=\"bibr\" target=\"#b31\">[31]</ref>, <ref type=\"bibr\" target=\"#b38\">[38]</ref>- <ref type=\"bibr\" target=\"#b40\">[40]</ref>, we build a sup. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b30\">[30]</ref>, many researchers have applied attention mechanisms to computer vision. Mnih et al. <ref type=\"bibr\" target=\"#b31\">[31]</ref> first used the attention mechanism with recurrent neural n  localization.</p><p>Encouraged by the successes of the attention mechanism on various applications <ref type=\"bibr\" target=\"#b31\">[31]</ref>, <ref type=\"bibr\" target=\"#b38\">[38]</ref>- <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ss in temporal action localization, owing to the successes of deep learning on various visual tasks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, especially on vide rk that takes a single 224 \u00d7 224 RGB image as input, and train VGG-16 using the ILSVRC-2012 dataset <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The feature extraction part of the VGG-16 and TSN is implem r\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= r\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= r\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= ng the negative segments and the relaxation can improve the performance of the attention mechanism. <ref type=\"bibr\" target=\"#b5\">(6)</ref> The performance of \"w/o LSTM\" degrades, showing the effectiv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: treated as the appearance features of segments. For the motion features, we follow the operation in <ref type=\"bibr\" target=\"#b50\">[50]</ref> and extract the 400dimensional feature vectors from the TS  means that the NMS is not used. We use the conventional average recall with 100 proposals (AR@100) <ref type=\"bibr\" target=\"#b50\">[50]</ref> to evaluate the performance of the proposal generator. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ough there have been numerous studies conducted on temporal action localization in untrimmed videos <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>, achieving accurate e frames. All segment features are normalized using L2-normalization. The segment scales are set to <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> on the THUMOS2014 dataset and <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target  ActivityNet1.3 dataset. The overlap segment of sliding windows with different scales is set to [0, <ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2]</ref> resort to sliding windows to produce temporal boundaries of actions and many other methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b16\">[16]</ref> generate propos  recurrent sequence encoder to aggregate video segments for generating action proposals. Gao et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> used a cascaded boundary regression model to produce class p><p>(5) STAN also performs better than the state-of-the-art methods using deep two-stream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . We use the softmax cross-entropy loss function for classification and the smooth L1 loss function <ref type=\"bibr\" target=\"#b43\">[43]</ref> for regression. The supervised attention loss is used to t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\">64</ref>] on the ActivityNet1.3 dataset. Th get=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b48\">48,</ref><ref type=\"bibr\" target=\"#b56\">56]</ref> on the THUMOS2014 and ActivityNet1.3 datasets, respectively ream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods usually use average pooling or concatenatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\">64</ref>] on the ActivityNet1.3 dataset. Th get=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b48\">48,</ref><ref type=\"bibr\" target=\"#b56\">56]</ref> on the THUMOS2014 and ActivityNet1.3 datasets, respectively ream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods usually use average pooling or concatenatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tention-based LSTM to capture the long-term dependence and find the salient portions. Nguyen et al. <ref type=\"bibr\" target=\"#b34\">[34]</ref> used the attention mechanism to find the background or act. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ducted on temporal action localization in untrimmed videos <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>, achieving accurate localization remains challenging owing t mula_1\">2</ref>) When using an extra proposal post-processing method PGCN that has been employed by <ref type=\"bibr\" target=\"#b4\">[5]</ref>, our method (STAN+PGCN) can achieve the state-of-the-art res r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= \" also show the effectiveness of learning the measurement of globally context-aware video segments. <ref type=\"bibr\" target=\"#b4\">(5)</ref> The supervised attention learning can benefit from discardin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ment scales are set to <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= THUMOS2014 dataset and <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= nt scales is set to [0, <ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n two challenging datasets: THUMOS2014 <ref type=\"bibr\" target=\"#b44\">[44]</ref> and ActivityNet1.3 <ref type=\"bibr\" target=\"#b45\">[45]</ref>.</p><p>The THUMOS2014 dataset contains videos from 20 clas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b19\">[19]</ref>, and then fused the frame-level CNN features for action classification. Wang et al. <ref type=\"bibr\" target=\"#b20\">[20]</ref> fused the features of iDT and CNN to design an action reco get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" tar get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">28,</ref><ref type=\"bibr\" target=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b56\">56,</ref><ref type=\"bibr\">64</ref>] on the ActivityNet1.3 dataset. Th get=\"#b32\">32,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" target=\"#b48\">48,</ref><ref type=\"bibr\" target=\"#b56\">56]</ref> on the THUMOS2014 and ActivityNet1.3 datasets, respectively ream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods usually use average pooling or concatenatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b10\">[11]</ref>. Some prominent methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref> resort to sliding windows to produce temporal boundaries o \" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar r\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" tar bibr\" target=\"#b24\">24]</ref> and [0.6, 1, 2, 3, 4, 5, 7, 8, <ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ough there have been numerous studies conducted on temporal action localization in untrimmed videos <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>, achieving accurate e frames. All segment features are normalized using L2-normalization. The segment scales are set to <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" target=\"#b32\">32]</ref> on the THUMOS2014 dataset and <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target  ActivityNet1.3 dataset. The overlap segment of sliding windows with different scales is set to [0, <ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .0\"><head>6) DETAD Analysis:</head><p>To further evaluate our method, we conduct the DETAD analysis <ref type=\"bibr\" target=\"#b63\">[63]</ref> on the THUMOS2014 dataset, including false positive analys. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  ActivityNet1.3 dataset as it does on the THUMOS2014 dataset compared with several existing methods <ref type=\"bibr\" target=\"#b58\">[58]</ref>, <ref type=\"bibr\" target=\"#b59\">[59]</ref>, probably due t  that the boundaries of long action instances are captured. Our method performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58] ethod performs slightly worse than <ref type=\"bibr\" target=\"#b58\">[58]</ref>, because the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> conducts frame-level predictions rather than segment-level 1.3 dataset. Nevertheless, our method yields a higher mAP at a threshold of 0.95 than the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref>, which indicates that our method locates the action bounda e difficult scenarios. Furthermore, although the average mAP of our method is 4% worse than that of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the ActivityNet1.3 dataset, our method achieves a signi ef> on the ActivityNet1.3 dataset, our method achieves a significant improvement over the method of <ref type=\"bibr\" target=\"#b58\">[58]</ref> on the THU-MOS2014 dataset, and the mAP at the threshold o 62\">[62]</ref> and two framelevel proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> for comparison, and the inference speed is directly copied od is slower than the frame-level proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> that perform fully convolutional operations on the frame l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st cases. Concretely, STAN outperforms RNN-based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, because it eff ng approaches <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b51\">[51]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref> on the THUMOS20 \">[14]</ref>, <ref type=\"bibr\" target=\"#b51\">[51]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref> on the THUMOS2014 dataset. It is interesting to notice tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2]</ref> resort to sliding windows to produce temporal boundaries of actions and many other methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b16\">[16]</ref> generate propos  recurrent sequence encoder to aggregate video segments for generating action proposals. Gao et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> used a cascaded boundary regression model to produce class p><p>(5) STAN also performs better than the state-of-the-art methods using deep two-stream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ass-agnostic proposals and detect specific actions by using a pooling aggregation method. Xu et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> applied a region-based method to temporal action localizat idate temporal regions containing actions by performing temporal convolutions. Based on the work of <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Chao et al. <ref type=\"bibr\" target=\"#b26\">[26]</ref> imp el></formula><formula xml:id=\"formula_2\">)</formula><p>Different from the existing attention models <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, [41] that use  I</ref> shows the comparison results of the per-class AP between our method and existing approaches <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b51\">[51]</ref>, <ref type=\"bib #b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b62\">[62]</ref> and two framelevel proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> for comparison, ginal high-dimensional video data. Our method is slower than the frame-level proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> that perform fu target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" tar 1, 2, 3, 4, 5, 7, 8, <ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: treated as the appearance features of segments. For the motion features, we follow the operation in <ref type=\"bibr\" target=\"#b50\">[50]</ref> and extract the 400dimensional feature vectors from the TS  means that the NMS is not used. We use the conventional average recall with 100 proposals (AR@100) <ref type=\"bibr\" target=\"#b50\">[50]</ref> to evaluate the performance of the proposal generator. We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: based methods <ref type=\"bibr\" target=\"#b28\">[28]</ref>, <ref type=\"bibr\" target=\"#b53\">[53]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, because it effectively couples the attention mechanism an  state-of-the-art methods using deep two-stream features <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b54\">[54]</ref>, <ref type=\"bibr\" target=\"#b56\">[56]</ref>. These methods . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on system. They also used a post-processing method to boost the localization performance. Xu et al. <ref type=\"bibr\" target=\"#b21\">[21]</ref> extracted CNN features and improved dense trajectories by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ass-agnostic proposals and detect specific actions by using a pooling aggregation method. Xu et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> applied a region-based method to temporal action localizat idate temporal regions containing actions by performing temporal convolutions. Based on the work of <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Chao et al. <ref type=\"bibr\" target=\"#b26\">[26]</ref> imp el></formula><formula xml:id=\"formula_2\">)</formula><p>Different from the existing attention models <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, [41] that use  I</ref> shows the comparison results of the per-class AP between our method and existing approaches <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b51\">[51]</ref>, <ref type=\"bib #b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b62\">[62]</ref> and two framelevel proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> for comparison, ginal high-dimensional video data. Our method is slower than the frame-level proposal-based methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b58\">[58]</ref> that perform fu target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" tar 1, 2, 3, 4, 5, 7, 8, <ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b19\">[19]</ref>, and then fused the frame-level CNN features for action classification. Wang et al. <ref type=\"bibr\" target=\"#b20\">[20]</ref> fused the features of iDT and CNN to design an action reco get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" tar get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">16,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ss in temporal action localization, owing to the successes of deep learning on various visual tasks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, especially on vide rk that takes a single 224 \u00d7 224 RGB image as input, and train VGG-16 using the ILSVRC-2012 dataset <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The feature extraction part of the VGG-16 and TSN is implem r\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= r\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= r\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= ng the negative segments and the relaxation can improve the performance of the attention mechanism. <ref type=\"bibr\" target=\"#b5\">(6)</ref> The performance of \"w/o LSTM\" degrades, showing the effectiv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on system. They also used a post-processing method to boost the localization performance. Xu et al. <ref type=\"bibr\" target=\"#b21\">[21]</ref> extracted CNN features and improved dense trajectories by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b17\">[17]</ref> proposed a saliency-based pooling method to improve the fisher vector encoding <ref type=\"bibr\" target=\"#b18\">[18]</ref> of the improved dense trajectory (iDT) <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: om natural language understanding (NLU) to text generation <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" targe  cannot fully capture the dependencies between the context words. Autoencoding models, such as BERT <ref type=\"bibr\" target=\"#b7\">[8]</ref>, learn bidirectional Transformers as context encoders via de ncern about how they can be adapted to downstream blank infilling tasks.</p><p>Comparison with BERT <ref type=\"bibr\" target=\"#b7\">[8]</ref>. BERT is pretrained with an autoencoding objective, i.e. mas tp://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Pretraining Setup</head><p>For a fair comparison with BERT <ref type=\"bibr\" target=\"#b7\">[8]</ref>, we use BooksCorpus <ref type=\"bibr\" target=\"#b45\">[46]</ref irectional contextualized encoder for natural language understanding via denoising objectives. BERT <ref type=\"bibr\" target=\"#b7\">[8]</ref> pretrains a large transformer model <ref type=\"bibr\" target= p>Previous pretraining works have shown optimal performance when masking 15% of the original tokens <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" targ upervised learning on abundant web texts significantly improves the performance on downstream tasks <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: works by combining their objectives via multi-task learning <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Table <ref type=\"table\">1</ref>: Summary of the pretra ference with T5 in Section 3.4.</p><p>Comparison with UniLM <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> UniLM combines different pretraining objectives under the aut lso relies on masked language modeling, which is less efficient than autoregressive models. UniLMv2 <ref type=\"bibr\" target=\"#b1\">[2]</ref> adopts partially autoregressive modeling for generation task. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erGLUE consists of 8 challenging natural language understanding tasks, including question answering <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targ <ref type=\"bibr\" target=\"#b42\">43]</ref>, textual entailment <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, coreference resolution <ref type=\"bibr\" target=\"#b17\">[18]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l language understanding tasks, including question answering <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, textual entailment. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l language understanding tasks, including question answering <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, textual entailment. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8]</ref>, word sense disambiguation <ref type=\"bibr\" target=\"#b24\">[25]</ref>, and causal reasoning <ref type=\"bibr\" target=\"#b29\">[30]</ref>. We adopt the standard evaluation metrics as <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l language understanding tasks, including question answering <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, textual entailment. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  search with beam size 5 and tweak the value of length penalty on the development set. We use SemQG <ref type=\"bibr\" target=\"#b43\">[44]</ref> and UniLM as our baselines.</p><p>The results are shown in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erGLUE consists of 8 challenging natural language understanding tasks, including question answering <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targ <ref type=\"bibr\" target=\"#b42\">43]</ref>, textual entailment <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, coreference resolution <ref type=\"bibr\" target=\"#b17\">[18]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar arget=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targe tional text generation, also called seq2seq, such as text summarization and response generation. T5 <ref type=\"bibr\" target=\"#b27\">[28]</ref> unifies NLU and conditional generation via encoder-decoder ormation leak within Transformer. It doubles the time cost of pretraining.</p><p>Comparison with T5 <ref type=\"bibr\" target=\"#b27\">[28]</ref>. T5 proposes a similar blank infilling objective to pretra rative models require much more parameters to work due to the limit of unidirectional attention. T5 <ref type=\"bibr\" target=\"#b27\">[28]</ref> formulates most language tasks in the text-to-text framewo  the original tokens <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. In this work, we randomly sample spans of length drawn fro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head><p>For a fair comparison with BERT <ref type=\"bibr\" target=\"#b7\">[8]</ref>, we use BooksCorpus <ref type=\"bibr\" target=\"#b45\">[46]</ref> and English Wikipedia as our pretraining data. We use the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b36\">Qu et al., 2021;</ref><ref type=\"bibr\" target=\"#b7\">Du et al., 2021;</ref><ref type=\"bibr\" target=\"#b19\">Kamalloo et al., 2021)</ref>, and question answering <ref type=\"bibr\" \"#b18\">(Jiao et al., 2020)</ref> uses a taskagnostic DA technique for its task-specific finetuning. <ref type=\"bibr\" target=\"#b19\">Kamalloo et al. (2021)</ref> and <ref type=\"bibr\" target=\"#b38\">Rashi es a separate masked language model in order to generate augmented samples with maximum divergence. <ref type=\"bibr\" target=\"#b19\">Kamalloo et al. (2021)</ref> and <ref type=\"bibr\" target=\"#b7\">Du et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b8\">Edunov et al., 2018)</ref>, or token replacement from a pre-trained language model <ref type=\"bibr\" target=\"#b22\">(Kobayashi, 2018;</ref><ref type=\"bibr\" target=\"#b51\">Wu et al., 2019 0\"><head n=\"2\">Related Work</head><p>2.1 Task-agnostic DA in NLP Contextual augmentation techniques <ref type=\"bibr\" target=\"#b22\">(Kobayashi, 2018;</ref><ref type=\"bibr\" target=\"#b51\">Wu et al., 2019  by \"All that is gold does not glitter\" -J.R.R. Tolkien, The Fellowship of the Ring. models for DA. <ref type=\"bibr\" target=\"#b22\">Kobayashi (2018)</ref> propose bidirectional LSTM language models for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o not warrant additional training or fine-tuning. They can be based on some hand-crafted heuristics <ref type=\"bibr\" target=\"#b56\">(Zhang et al., 2015;</ref><ref type=\"bibr\" target=\"#b48\">Wei and Zou,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b22\">(Kobayashi, 2018;</ref><ref type=\"bibr\" target=\"#b51\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b33\">Ng et al., 2020)</ref>. Even though deploying task-agnostic methods i onal LSTM language models for word substitution conditioned on the label of their input text. SSMBA <ref type=\"bibr\" target=\"#b33\">(Ng et al., 2020)</ref> and TinyBERT <ref type=\"bibr\" target=\"#b18\">( ce=\"foot\" n=\"3\" xml:id=\"foot_1\">.3 https://github.com/makcedward/nlpaug 3. Mask-and-Reconstruct (MR;<ref type=\"bibr\" target=\"#b33\">Ng et al. 2020</ref>): We randomly mask 15% of the tokens and constru. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: scale datasets such as SQuAD <ref type=\"bibr\" target=\"#b37\">(Rajpurkar et al., 2016)</ref> and MNLI <ref type=\"bibr\" target=\"#b49\">(Williams et al., 2018)</ref>. Third, most DA methods are not univers. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">GLUE</head><p>The GLUE benchmark <ref type=\"bibr\" target=\"#b46\">(Wang et al., 2019</ref>) is a well-known suite of nine<ref type=\"foo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ool of augmented samples that are generated offline, our proposed method follows a minimax approach <ref type=\"bibr\" target=\"#b9\">(Farnia and Tse, 2016)</ref> to select a small subset with maximal exp lect the best candidates according to particular defined criteria. Inspired by the minimax approach <ref type=\"bibr\" target=\"#b9\">(Farnia and Tse, 2016;</ref><ref type=\"bibr\" target=\"#b45\">Volpi et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e is to investigate whether DA is an effective means in knowledge transfer to curb the capacity gap <ref type=\"bibr\" target=\"#b3\">(Cho and Hariharan, 2019</ref>) between a large model and a small one.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0x/1x</cell><cell></cell></row></table><note>\u2663 ) denotes results are taken verbatim from: BERT Large<ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, and MATE-KD<ref type=\"bibr\" target=\"#b38\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rchers into leveraging Data Augmentation (DA) in a broad range of applications from computer vision <ref type=\"bibr\" target=\"#b4\">(Cubuk et al., 2019;</ref><ref type=\"bibr\" target=\"#b47\">Wang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mber of training steps. Maximum number of epochs is set to 20 for all tasks except SQuAD, following <ref type=\"bibr\" target=\"#b32\">(Mosbach et al., 2021)</ref>. For large datasets, we early stop with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: order interactions between atoms, such as the interactions between triplets or quadruplets of atoms <ref type=\"bibr\" target=\"#b11\">(Klicpera et al., 2020a;</ref><ref type=\"bibr\">2021;</ref><ref type=\" esentations is often the bottleneck in terms of GPU memory and compute. Many recent methods such as <ref type=\"bibr\" target=\"#b11\">(Klicpera et al., 2020a;</ref><ref type=\"bibr\">2021)</ref>   (p) in m ucting energy-conserving models by estimating forces as the gradient of the energy. Subsequent work <ref type=\"bibr\" target=\"#b11\">(Klicpera et al., 2020a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bib ultiple GPUs.</p><p>We benchmark our approach by scaling up two recent GNN architectures -DimeNet++ <ref type=\"bibr\" target=\"#b11\">(Klicpera et al., 2020a)</ref> and GemNet-T <ref type=\"bibr\" target=\" .0\" xml:id=\"fig_0\"><head></head><label></label><figDesc>3.3 ENERGY-CENTRIC MODEL: DIMENET++DimeNet++<ref type=\"bibr\" target=\"#b11\">(Klicpera et al., 2020a</ref>) is a recently proposed energy-centric . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: guage processing, and speech recognition <ref type=\"bibr\" target=\"#b23\">(Shoeybi et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al., 2 s.</p><p>Figure <ref type=\"figure\" target=\"#fig_3\">2b</ref> compares graph and pipeline parallelism <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2019)</ref> showing that graph parallelism outperforms  s on training large models that do not fit entirely on one GPU (even with a batch size of 1). GPipe <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2019)</ref> splits different sequences of layers into d p><p>Further, it is possible to combine graph parallelism with model parallel methods such as GPipe <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2019)</ref> to train even larger models, which could yi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: guage processing, and speech recognition <ref type=\"bibr\" target=\"#b23\">(Shoeybi et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al., 2 s.</p><p>Figure <ref type=\"figure\" target=\"#fig_3\">2b</ref> compares graph and pipeline parallelism <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2019)</ref> showing that graph parallelism outperforms  s on training large models that do not fit entirely on one GPU (even with a batch size of 1). GPipe <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2019)</ref> splits different sequences of layers into d p><p>Further, it is possible to combine graph parallelism with model parallel methods such as GPipe <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2019)</ref> to train even larger models, which could yi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on to catalyst discovery and drug design <ref type=\"bibr\" target=\"#b20\">(Sch\u00fctt et al., 2017b;</ref><ref type=\"bibr\" target=\"#b3\">Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">J\u00f8rgensen et   of estimating atomic properties such as <ref type=\"bibr\" target=\"#b20\">(Sch\u00fctt et al., 2017b;</ref><ref type=\"bibr\" target=\"#b3\">Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">J\u00f8rgensen et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on to catalyst discovery and drug design <ref type=\"bibr\" target=\"#b20\">(Sch\u00fctt et al., 2017b;</ref><ref type=\"bibr\" target=\"#b3\">Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">J\u00f8rgensen et   of estimating atomic properties such as <ref type=\"bibr\" target=\"#b20\">(Sch\u00fctt et al., 2017b;</ref><ref type=\"bibr\" target=\"#b3\">Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">J\u00f8rgensen et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng model -GemNet-XL-FTobtains a relative improvement of \u223c2% on energy MAE compared to 3D-Graphormer <ref type=\"bibr\" target=\"#b28\">(Ying et al., 2021)</ref>, the current state-of-the-art direct approa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: T <ref type=\"bibr\" target=\"#b13\">(Klicpera et al., 2021)</ref> -on the Open Catalyst (OC20) dataset <ref type=\"bibr\" target=\"#b2\">(Chanussot* et al., 2021)</ref>. The OC20 dataset, aimed at discoverin section, we present the results of our scaling experiments on the Open Catalyst 2020 (OC20) dataset <ref type=\"bibr\" target=\"#b2\">(Chanussot* et al., 2021)</ref>. The OC20 dataset contains over 130 mi his model has about 240M parameters, which is over 20\u00d7 larger than the DimeNet++large model used in <ref type=\"bibr\" target=\"#b2\">(Chanussot* et al., 2021)</ref>. We call this model DimeNet++-XL.   <r n <ref type=\"bibr\" target=\"#b2\">(Chanussot* et al., 2021)</ref>. We call this model DimeNet++-XL.   <ref type=\"bibr\" target=\"#b2\">(Chanussot* et al., 2021)</ref>.</p><p>GemNet-XL. Our GemNet model con axed. Two approaches can be taken to address this problem, the direct and the relaxation approaches <ref type=\"bibr\" target=\"#b2\">(Chanussot* et al., 2021)</ref>. In the direct approach, we treat this. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b23\">(Shoeybi et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Huang et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b29\">Zhai et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion task. Prior work has shown that LM benefits from training on instances of increasing difficulty <ref type=\"bibr\" target=\"#b4\">(Bengio et al., 2009;</ref><ref type=\"bibr\">Press et al., 2021)</ref>. p>In addition to the model modifications, other work investigated curriculum learning to train LMs. <ref type=\"bibr\" target=\"#b4\">Bengio et al. (2009)</ref>  Related work aimed at integrating WordNet   in training, and to predict the exact word from magnesium, iron, and desk later.</p><p>Inspired by <ref type=\"bibr\" target=\"#b4\">Bengio et al. (2009)</ref>, we choose curriculum learning to achieve t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in and Bengio, 2005;</ref><ref type=\"bibr\" target=\"#b12\">Grave et al., 2017a)</ref>. More recently, <ref type=\"bibr\" target=\"#b19\">Levine et al. (2020)</ref> pretrain masked LMs <ref type=\"bibr\" targe 2009)</ref>  Related work aimed at integrating WordNet information into pretrained language models. <ref type=\"bibr\" target=\"#b19\">Levine et al. (2020)</ref> propose SenseBERT by adding the word sense. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b3\">(Bengio et al., 2003;</ref><ref type=\"bibr\" target=\"#b24\">Mnih and Hinton, 2007;</ref><ref type=\"bibr\" target=\"#b10\">Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b6\">Brown et al., /ref>. More recently, <ref type=\"bibr\" target=\"#b19\">Levine et al. (2020)</ref> pretrain masked LMs <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019)</ref> by predicting WordNet supersense labels.  enseBERT by adding the word sense (WordNet supersense) prediction as an additional task during BERT <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019)</ref> pre-training. SenseBERT outperforms BERT . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: icit context sharing. Neural models still struggle to learn reliable representations for rare words <ref type=\"bibr\" target=\"#b33\">(Schick and Sch\u00fctze, 2020)</ref>. With CLM-based models, data sparsit exical similarity <ref type=\"bibr\" target=\"#b16\">(Khassanov et al., 2019)</ref>, context similarity <ref type=\"bibr\" target=\"#b33\">(Schick and Sch\u00fctze, 2020;</ref><ref type=\"bibr\" target=\"#b15\">Khande. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b14\">(Guu et al., 2020;</ref><ref type=\"bibr\" target=\"#b37\">Ziegler</ref>  and Rush, 2019; <ref type=\"bibr\" target=\"#b9\">Deng et al., 2020)</ref>. In this paper, we explore the effectiveness . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iction at the expense of rare words. This is inline with observations made recently in vision tasks <ref type=\"bibr\" target=\"#b32\">(Sagawa et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b8\">Dai et al. (2019)</ref> propose Transformer-XL by extending the vanilla Transformer <ref type=\"bibr\" target=\"#b35\">(Vaswani et al., 2017)</ref> with a memory segment, which can encode . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b3\">(Bengio et al., 2003;</ref><ref type=\"bibr\" target=\"#b24\">Mnih and Hinton, 2007;</ref><ref type=\"bibr\" target=\"#b10\">Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b6\">Brown et al., /ref>. More recently, <ref type=\"bibr\" target=\"#b19\">Levine et al. (2020)</ref> pretrain masked LMs <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019)</ref> by predicting WordNet supersense labels.  enseBERT by adding the word sense (WordNet supersense) prediction as an additional task during BERT <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019)</ref> pre-training. SenseBERT outperforms BERT . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e, to capture long-term dependencies, various extensions of Transformerbased LMs have been proposed <ref type=\"bibr\" target=\"#b8\">(Dai et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Rae et al., 202 r\">(Lazaridou et al., 2021)</ref>. These improvements are observed with respect to memory-augmented <ref type=\"bibr\" target=\"#b8\">(Dai et al., 2019)</ref> and segmentaware <ref type=\"bibr\" target=\"#b1 ns/1.0\"><head n=\"2\">Related Work</head><p>Transformer-based models are now popular language models. <ref type=\"bibr\" target=\"#b8\">Dai et al. (2019)</ref> propose Transformer-XL by extending the vanill. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b14\">(Guu et al., 2020;</ref><ref type=\"bibr\" target=\"#b37\">Ziegler</ref>  and Rush, 2019; <ref type=\"bibr\" target=\"#b9\">Deng et al., 2020)</ref>. In this paper, we explore the effectiveness . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: computational efficiency in neural LMs <ref type=\"bibr\" target=\"#b25\">(Morin and Bengio, 2005;</ref><ref type=\"bibr\" target=\"#b12\">Grave et al., 2017a)</ref>. More recently, <ref type=\"bibr\" target=\"# tter than only hypernym classes V h .</p><p>Adaptive Softmax Another method is the adaptive-softmax <ref type=\"bibr\" target=\"#b12\">(Grave et al., 2017a)</ref>, where the model first predict the hypern. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b30\">(Zhou et al., 2021;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b27\">Zhang et al., 2021)</ref>. Such models are able to achieve state-of-t  focus on the syntactic features from PrLMs while neglecting the interactions between entity pairs. <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2021)</ref> and <ref type=\"bibr\" target=\"#b5\">Li et al. imbalance problem for DocRE. Existing works <ref type=\"bibr\" target=\"#b30\">(Zhou et al., 2021;</ref><ref type=\"bibr\" target=\"#b27\">Zhang et al., 2021;</ref><ref type=\"bibr\" target=\"#b26\">Zeng et al.,  e Convolution Neural Networks (CNNs) to encode the neighbor information for relation classification <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2021)</ref>, we believe that attending to the axial el fuses the contextual information with the aggregated attention weights for each entity. The DocuNet <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2021)</ref> model treats the relation extraction task . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  methods, transformer-only architectures have also proven to be highly effective for the DocRE task <ref type=\"bibr\" target=\"#b15\">(Tang et al., 2020;</ref><ref type=\"bibr\" target=\"#b30\">Zhou et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Vaswani et al., 2017)</ref> architecture <ref type=\"bibr\" target=\"#b30\">(Zhou et al., 2021;</ref><ref type=\"bibr\" target=\"#b22\">Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b27\">Zhang et al., 20  are very few works discussing the method of adapting distantly supervised data for the DocRE task. <ref type=\"bibr\" target=\"#b22\">Xu et al. (2021)</ref> has shown that distantly supervised data is ab ve baselines. Moreover, our model significantly outperforms the existing state-of-theart SSAN-Adapt <ref type=\"bibr\" target=\"#b22\">(Xu et al., 2021)</ref> on the DocRED leaderboard by 1.36 in F1 score re-training from the distantly supervised data is beneficial for document-level relation extraction <ref type=\"bibr\" target=\"#b22\">(Xu et al., 2021)</ref>. However, prior work only adapts the distantl are two strategies for adapting the distantly supervised data.</p><p>Naive Adaptation Adopting from <ref type=\"bibr\" target=\"#b22\">(Xu et al., 2021)</ref>, this method first pretrains the model with t =\"http://www.tei-c.org/ns/1.0\"><head>Context-enhanced Entity Representation</head><p>As prior works <ref type=\"bibr\" target=\"#b22\">(Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">Peng et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ior works have shown that a large number of relations can only be extracted from multiple sentences <ref type=\"bibr\" target=\"#b17\">(Verga et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Yao et al.,  dea by applying different GNN architectures <ref type=\"bibr\" target=\"#b11\">(Peng et al., 2017;</ref><ref type=\"bibr\" target=\"#b17\">Verga et al., 2018;</ref><ref type=\"bibr\">Christopoulou et al., 2019;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ior works have shown that a large number of relations can only be extracted from multiple sentences <ref type=\"bibr\" target=\"#b17\">(Verga et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Yao et al.,  dea by applying different GNN architectures <ref type=\"bibr\" target=\"#b11\">(Peng et al., 2017;</ref><ref type=\"bibr\" target=\"#b17\">Verga et al., 2018;</ref><ref type=\"bibr\">Christopoulou et al., 2019;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: action mainly focused on sentence-level RE <ref type=\"bibr\" target=\"#b29\">(Zhang et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Baldini Soares et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Peng . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Entity Representation</head><p>As prior works <ref type=\"bibr\" target=\"#b22\">(Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">Peng et al., 2020)</ref> have shown that contextual information is cr et=\"#b29\">(Zhang et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Baldini Soares et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Peng et al., 2020)</ref>. However, prior works have shown that a larg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Qin et al. (2018)</ref> used generative adversarial training for selecting informative examples and <ref type=\"bibr\" target=\"#b4\">Feng et al. (2018)</ref> used reinforcement learning to achieve the sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ormation to construct a documentlevel graph <ref type=\"bibr\" target=\"#b25\">(Zeng et al., 2021;</ref><ref type=\"bibr\" target=\"#b26\">Zeng et al., 2020)</ref>, and then use graph neural networks for reas ibr\" target=\"#b30\">(Zhou et al., 2021;</ref><ref type=\"bibr\" target=\"#b27\">Zhang et al., 2021;</ref><ref type=\"bibr\" target=\"#b26\">Zeng et al., 2020)</ref> only focus on threshold learning for balanci =\"bibr\" target=\"#b9\">Nan et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Zeng et al., 2020)</ref>. In particular, <ref type=\"bibr\" target=\"#b9 </div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Compared Methods</head><p>We The GAIN <ref type=\"bibr\" target=\"#b26\">(Zeng et al., 2020)</ref> model adds a graph neural network on top of  type=\"table\" target=\"#tab_5\">5</ref>. We use the same evaluation method for multi-hop relations as <ref type=\"bibr\" target=\"#b26\">Zeng et al. (2020)</ref>. This evaluation method ignores all the oneh tucture refinement (LSR) model, which used structured attention to induce the document-level graph. <ref type=\"bibr\" target=\"#b26\">Zeng et al. (2020)</ref> constructed the document-level graph by enti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b14\">Scarselli et al., 2008)</ref> have been widely used for the DocRE task. <ref type=\"bibr\" target=\"#b13\">Quirk and Poon (2017)</ref> used words as nodes and dependency inform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aphrase generation, is selecting appropriate input control values that can be achieved by the model <ref type=\"bibr\" target=\"#b10\">(Goyal and Durrett, 2020)</ref>. Clearly, given a sentence, not all p pe=\"bibr\">Thompson and Post, 2020)</ref> or syntactically <ref type=\"bibr\">(Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b10\">Goyal and Durrett, 2020)</ref> diverse paraphrases. One approach is t r\" target=\"#b15\">(Iyyer et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Li and Choi, 2020)</ref>. <ref type=\"bibr\" target=\"#b10\">Goyal and Durrett (2020)</ref> promote syntactic diversity by conditi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l as train and dev results can be found in Appendix C.1.</p><p>QCPG: We use the pre-trained T5-base <ref type=\"bibr\" target=\"#b27\">(Raffel et al., 2020)</ref> as the encoder-decoder model. The control. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es, produced from a bilingual corpus using negative constraints, inference sampling, and clustering <ref type=\"bibr\" target=\"#b14\">(Hu et al., 2019)</ref>. The dataset is composed of avarage of 5 para. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntence for guiding the syntax of the generated paraphrase <ref type=\"bibr\">(Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bao et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Hosking and Lapa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es, produced from a bilingual corpus using negative constraints, inference sampling, and clustering <ref type=\"bibr\" target=\"#b14\">(Hu et al., 2019)</ref>. The dataset is composed of avarage of 5 para. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: matic text evaluation and classification, and can avoid the blandness caused by repetitive patterns <ref type=\"bibr\" target=\"#b26\">(Qian et al., 2019)</ref>. The quality of paraphrases is often evalua 2018;</ref><ref type=\"bibr\" target=\"#b25\">Park et al., 2019)</ref>. or by using distinct generators <ref type=\"bibr\" target=\"#b26\">(Qian et al., 2019)</ref>. These methods achieve some diversity, but . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ., 2015)</ref>, data augmentation <ref type=\"bibr\">(Yu et al., 2018)</ref> and adversarial learning <ref type=\"bibr\" target=\"#b15\">(Iyyer et al., 2018)</ref>. However, not all paraphrases are equally  ding the model with very specific information regarding the target sentence, such as its parse tree <ref type=\"bibr\" target=\"#b15\">(Iyyer et al., 2018)</ref> or the list of keywords it needs to contai are measures of syntactic and lexical variation, respectively. For the syntactic score, inspired by <ref type=\"bibr\" target=\"#b15\">Iyyer et al. (2018)</ref> we choose q syn (s, s \u2032 ) to be the normali  Lapata, 2021</ref>). An alternative is to directly employ constituency tree as the syntax guidance <ref type=\"bibr\" target=\"#b15\">(Iyyer et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Li and Choi,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntly, several strong metrics have been proposed for measuring semantic similarity between sentences <ref type=\"bibr\" target=\"#b28\">(Reimers and Gurevych, 2019;</ref><ref type=\"bibr\">?;</ref><ref type= =\"#tab_2\">3</ref> shows the resultant correlations. The highest correlations are obtained for SBERT <ref type=\"bibr\" target=\"#b28\">(Reimers and Gurevych, 2019)</ref>, but since it was trained on WikiA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: control of QCPG (Section 2.1) and Self-BLEU <ref type=\"bibr\">(Zhu et al., 2018)</ref> as adapted in <ref type=\"bibr\" target=\"#b17\">Li et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b19\">Liu et al. (20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aphrases are those with high semantic similarity as well as high lexical and/or syntactic diversity <ref type=\"bibr\" target=\"#b22\">(McCarthy et al., 2009)</ref>.</p><p>Generating high quality paraphra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntence for guiding the syntax of the generated paraphrase <ref type=\"bibr\">(Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bao et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Hosking and Lapa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\">(Zhu et al., 2018)</ref> as adapted in <ref type=\"bibr\" target=\"#b17\">Li et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b19\">Liu et al. (2020a)</ref>, which aims to measure the linguistic divers. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: This dataset consists of 123K images, where each image contains at most five human-labeled captions <ref type=\"bibr\" target=\"#b18\">(Lin et al., 2014)</ref>. Similar to previous works we consider diffe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: matic text evaluation and classification, and can avoid the blandness caused by repetitive patterns <ref type=\"bibr\" target=\"#b26\">(Qian et al., 2019)</ref>. The quality of paraphrases is often evalua 2018;</ref><ref type=\"bibr\" target=\"#b25\">Park et al., 2019)</ref>. or by using distinct generators <ref type=\"bibr\" target=\"#b26\">(Qian et al., 2019)</ref>. These methods achieve some diversity, but . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: control of QCPG (Section 2.1) and Self-BLEU <ref type=\"bibr\">(Zhu et al., 2018)</ref> as adapted in <ref type=\"bibr\" target=\"#b17\">Li et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b19\">Liu et al. (20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .g. hard negative examples of sentences that are linguistically similar but have different meanings <ref type=\"bibr\" target=\"#b11\">(Guo et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Reimers and Gu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stically similar but have different meanings <ref type=\"bibr\" target=\"#b11\">(Guo et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Reimers and Gurevych, 2020)</ref>).</p><p>Our results show that the Q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rase <ref type=\"bibr\">(Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bao et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Hosking and Lapata, 2021</ref>). An alternative is to directly employ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l finetuned on the training data.</p><p>For all the models, we adopt the experimental setup used in <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref>, i.e. we train the model with several lear. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l finetuned on the training data.</p><p>For all the models, we adopt the experimental setup used in <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref>, i.e. we train the model with several lear. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rase <ref type=\"bibr\">(Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bao et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Hosking and Lapata, 2021</ref>). An alternative is to directly employ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .g. hard negative examples of sentences that are linguistically similar but have different meanings <ref type=\"bibr\" target=\"#b11\">(Guo et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Reimers and Gu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: artitions to proceed.</p><p>Prior works <ref type=\"bibr\" target=\"#b31\">(Tripathy et al., 2020;</ref><ref type=\"bibr\" target=\"#b39\">Zheng et al., 2020)</ref> aim at achieving only Goal-2 yet ignore Goa S is widely adopted in scalable GCN training <ref type=\"bibr\" target=\"#b40\">(Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b39\">Zheng et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Fey et al., 20 CN vs. Boundary Edge Sampling. As many works <ref type=\"bibr\" target=\"#b40\">(Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b39\">Zheng et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Fey et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t al., 2020)</ref>, Dorylus <ref type=\"bibr\" target=\"#b30\">(Thorpe et al., 2021)</ref>, and PipeGCN <ref type=\"bibr\" target=\"#b33\">(Wan et al., 2022)</ref>) have demonstrated a promising training perf =\"bibr\" target=\"#b39\">Zheng et al., 2020;</ref><ref type=\"bibr\" target=\"#b7\">Fey et al., 2021;</ref><ref type=\"bibr\" target=\"#b33\">Wan et al., 2022)</ref> where the objective function is mostly set as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: essary communication. Following this \"partition-parallelism\" paradigm, pioneering efforts (NeuGraph <ref type=\"bibr\" target=\"#b24\">(Ma et al., 2019)</ref>, ROC <ref type=\"bibr\" target=\"#b15\">(Jia et a ral works have been proposed. ROC <ref type=\"bibr\" target=\"#b15\">(Jia et al., 2020)</ref>, NeuGraph <ref type=\"bibr\" target=\"#b24\">(Ma et al., 2019)</ref>, and AliGraph <ref type=\"bibr\" target=\"#b40\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gher variance of gradient estimation.</p><p>\u2022 Edge Sampling: Applying edge sampling (e.g., DropEdge <ref type=\"bibr\" target=\"#b27\">(Rong et al., 2019)</ref>) to distributed GCN training is not efficie t, one could reduce communication overhead by cutting edges using sampling techniques like DropEdge <ref type=\"bibr\" target=\"#b27\">(Rong et al., 2019)</ref> or even an enhanced version that samples on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arning rate as 0.01 with 100 epochs and 0.5 dropout rate.</p><p>Setups. We implement BNS-GCN in DGL <ref type=\"bibr\" target=\"#b34\">(Wang et al., 2019)</ref> and PyTorch <ref type=\"bibr\" target=\"#b26\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Ns' weight gradients (Line 13). Lastly, weight gradients are shared across partitions via AllReduce <ref type=\"bibr\" target=\"#b29\">(Thakur et al., 2005)</ref> to perform weight updates (Lines 14-15).<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  paradigm, pioneering efforts (NeuGraph <ref type=\"bibr\" target=\"#b24\">(Ma et al., 2019)</ref>, ROC <ref type=\"bibr\" target=\"#b15\">(Jia et al., 2020)</ref>, CAGNET <ref type=\"bibr\" target=\"#b31\">(Trip >1</ref> <ref type=\"bibr\">(a)</ref>. Following this paradigm, several works have been proposed. ROC <ref type=\"bibr\" target=\"#b15\">(Jia et al., 2020)</ref>, NeuGraph <ref type=\"bibr\" target=\"#b24\">(Ma  BNS-GCN against the SOTA fullgraph training methods, ROC<ref type=\"foot\" target=\"#foot_0\">2</ref>  <ref type=\"bibr\" target=\"#b15\">(Jia et al., 2020)</ref> and CAGNET<ref type=\"foot\" target=\"#foot_1\"> ethods, regardless of datasets or number of partitions, which is consistent with the results of ROC <ref type=\"bibr\" target=\"#b15\">(Jia et al., 2020)</ref>. More importantly, BNS-GCN always maintains . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y volunteers of the Spoofer project are reported to have problematic or wholly lacking SAV adoption <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>This brings us to our main question: How can more ope e Spoofer project recently reported that notifying operators boosted remediation rates by about 50% <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Their findings were based only on observational data. The a yields a crucial insight that puts the earlier findings in a different light: the improvements that <ref type=\"bibr\" target=\"#b6\">[7]</ref> observed might be incorrectly attributed to the intervention e did observe some remediation across all groups, including the control group. It might explain why <ref type=\"bibr\" target=\"#b6\">[7]</ref> did report an impact of their notifications. Since they had  =\"http://www.tei-c.org/ns/1.0\"><head>A. Methods to Infer the Adoption of SAV</head><p>Previous work <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" urements can be performed remotely or from inside the network under test.</p><p>The Spoofer project <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" nnels.</p><p>Luckie et al. notified network operators who had not implemented SAV in their networks <ref type=\"bibr\" target=\"#b6\">[7]</ref>. They initially contacted them directly using email addresse e as effective as private notifications.</p><p>Our work comes closest to the study by Luckie et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref>, since we also notify network operators who have not imple-m 12\">[13]</ref>. Our technique to detect SAV via open resolvers has two advantages over Spoofer data <ref type=\"bibr\" target=\"#b6\">[7]</ref>: we find 10 times more providers that are not compliant (Oct by the NOG lists. The Spoofer project measures the absence of SAV using a client-server application <ref type=\"bibr\" target=\"#b6\">[7]</ref>. The project has been sending monthly emails since Dec 2018. rg/ns/1.0\"><head>D. Comparison with Spoofer</head><p>We requested operators to run the Spoofer tool <ref type=\"bibr\" target=\"#b6\">[7]</ref> to validate if they have correctly deployed SAV. A total of  ibr\" target=\"#b60\">[62]</ref>.</p><p>We also analyzed the remediations reported by the Spoofer tool <ref type=\"bibr\" target=\"#b6\">[7]</ref>. In total across all Spoofer measurements, 98 ASes in Spoofe  might trust the NOG channel, since the communication was part of the already known Spoofer project <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. p><p>Target Audience: Multiple studies notified network operators about routing and security issues <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. However, none of e has been a significant effort by the security community to deploy SAV over the last several years <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b60\">[62]</ref>, <ref type=\"bibr\" hey hold a unique vantage point where they can detect if the incoming packets have a spoofed source <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: fective in driving behavior change than negative framing <ref type=\"bibr\" target=\"#b53\">[55]</ref>- <ref type=\"bibr\" target=\"#b55\">[57]</ref>.</p><p>3) Treatment Group Assignment: We use the data on t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Previous work <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b22\">[23]</ref> have proposed m a routing misconfiguration, and therefore coverage of the method is relatively small. M\u00fcller et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> and Lichtblau et al. <ref type=\"bibr\" target=\"#b16\">[17]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs themselves and the users could easily exit the quarantine.</p><p>In another study, K\u00fchrer et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> sent notifications to the network operators about open res  proposed by Mauch <ref type=\"bibr\" target=\"#b46\">[47]</ref> and later implemented by K\u00fchrer et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> and Lone et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref>. tified network operators about routing and security issues <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. However, none of these had a control group, which is requ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: use positive framing has been shown more effective in driving behavior change than negative framing <ref type=\"bibr\" target=\"#b53\">[55]</ref>- <ref type=\"bibr\" target=\"#b55\">[57]</ref>.</p><p>3) Treat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that encourages people to respond to a positive or kind action with another positive or kind action <ref type=\"bibr\" target=\"#b40\">[41]</ref>- <ref type=\"bibr\" target=\"#b43\">[44]</ref>. For example, i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that encourages people to respond to a positive or kind action with another positive or kind action <ref type=\"bibr\" target=\"#b40\">[41]</ref>- <ref type=\"bibr\" target=\"#b43\">[44]</ref>. For example, i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  channels for reaching out to the network operators: the \"abuse email\" listed in the WHOIS database <ref type=\"bibr\" target=\"#b24\">[25]</ref>, physical letters <ref type=\"bibr\" target=\"#b7\">[8]</ref>, d contact addresses of the ASes using peeringDB <ref type=\"bibr\" target=\"#b48\">[49]</ref> and WHOIS <ref type=\"bibr\" target=\"#b24\">[25]</ref>. We also identified the relevant national CERT for each co there is a technical contact in either peeringDB <ref type=\"bibr\" target=\"#b48\">[49]</ref> or WHOIS <ref type=\"bibr\" target=\"#b24\">[25]</ref>. If both of them have an address and it is different, we p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dicted structures, we predict structures for 12 million UniRef50 protein sequences using AlphaFold2 <ref type=\"bibr\" target=\"#b30\">(Jumper et al., 2021)</ref>. An autoregressive inverse folding model   1797 chains in the test split.</p><p>Predicted structures. We curated a new data set of AlphaFold2 <ref type=\"bibr\" target=\"#b30\">(Jumper et al., 2021)</ref>-predicted structures for a selective subs el weights from AlphaFold2 Model 1 for CASP14 as a single model, as opposed the 5-model ensemble in <ref type=\"bibr\" target=\"#b30\">(Jumper et al., 2021)</ref>, to cover more sequences with the same am me based on the N, CA, and C atom positions in the amino acid, following Algorithm 21 in AlphaFold2 <ref type=\"bibr\" target=\"#b30\">(Jumper et al., 2021)</ref>.</p><p>We then perform a change of basis  used to overcome the limitation of experimental data. With progress in protein structure prediction <ref type=\"bibr\" target=\"#b30\">(Jumper et al., 2021;</ref><ref type=\"bibr\" target=\"#b6\">Baek et al., edicted structures, we predict structures for 12 million UniRef50 protein sequences using AlphaFold2<ref type=\"bibr\" target=\"#b30\">(Jumper et al., 2021</ref>). An autoregressive inverse folding model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b49\">Rao et al., 2019;</ref><ref type=\"bibr\" target=\"#b38\">Madani et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Elnaggar et al., 2021;</ref><ref type=\"bibr\">Rives et al., 2021;</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tly generate structures and sequences <ref type=\"bibr\" target=\"#b5\">(Anishchenko et al., 2021;</ref><ref type=\"bibr\" target=\"#b70\">Wang et al., 2021)</ref>, or model sequences directly <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b18\">Gligorijevic et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">Bryant et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Dallago et al., 2021)</ref>. The potential to learn the rules of prot lity.</p><p>We report the zero-shot performance on each of the 7 data subsets evaluated in the FLIP <ref type=\"bibr\" target=\"#b13\">(Dallago et al., 2021)</ref> benchmark suite. As shown in Table <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of information in the test set via the predicted structures. We use Gene3D topology classification <ref type=\"bibr\" target=\"#b35\">(Lees et al., 2012)</ref> to filter both the sequences used for super teps.</p><p>First, we annotated UniRef50 sequences with CATH classification according to the Gene3D <ref type=\"bibr\" target=\"#b35\">(Lees et al., 2012)</ref> database, also used by Strokach <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2020;</ref><ref type=\"bibr\" target=\"#b8\">Daras et al., 2020)</ref> and mini-batch spherical k-means <ref type=\"bibr\" target=\"#b28\">(Roy et al., 2021;</ref><ref type=\"bibr\" target=\"#b38\">Wang et al., 2 r (ANN) methods address this limitation by calculating the content-based sparse patterns in advance <ref type=\"bibr\" target=\"#b28\">(Roy et al., 2021;</ref><ref type=\"bibr\" target=\"#b13\">Kitaev et al.,  Similar to other sparse attention models <ref type=\"bibr\" target=\"#b13\">(Kitaev et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Roy et al., 2021)</ref>, LHA reduces the overall complexity of self-a  j:h Q (Qi)=h K (Kj ) \u0100ij = 1. In general <ref type=\"bibr\" target=\"#b13\">(Kitaev et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Roy et al., 2021)</ref>, calculating the hash function and performing )</ref>, which considers a practical case <ref type=\"bibr\" target=\"#b13\">(Kitaev et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Roy et al., 2021)</ref> where all clusters strictly contain the same   similar to other sparse attention models <ref type=\"bibr\" target=\"#b13\">(Kitaev et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Roy et al., 2021)</ref>, the proposed attention utility metric curren Q i || 2 2 , G(K j ) = K j ; M 2 Q + M 2 K \u2212 ||K j || 2 2 ; 0 ,<label>(3)</label></formula><p>where <ref type=\"bibr\" target=\"#b28\">(Roy et al., 2021)</ref> and Cluster-former <ref type=\"bibr\" target=\" \" target=\"#b13\">(Kitaev et al., 2020)</ref> or the mini-batch k-means scheme in Routing Transformer <ref type=\"bibr\" target=\"#b28\">(Roy et al., 2021)</ref>, we can set:</p><formula xml:id=\"formula_11\" tricks to fix this \"no-attentiontarget\" problem. Our solution is different from Routing Transformer <ref type=\"bibr\" target=\"#b28\">(Roy et al., 2021)</ref> or Reformer <ref type=\"bibr\" target=\"#b13\">( . To tackle this problem, inspired by <ref type=\"bibr\" target=\"#b13\">Kitaev et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b28\">Roy et al. (2021)</ref>, we tie the key hashes with query hashes in t -ranked bucket as the hash bucket for each query and key, instead of using a token sorting strategy <ref type=\"bibr\" target=\"#b28\">(Roy et al. 2021)</ref> to maintain the same bucket sizes. When calcu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  2020;</ref><ref type=\"bibr\" target=\"#b24\">Peng et al., 2021)</ref>, we use random Fourier features <ref type=\"bibr\" target=\"#b27\">(Rahimi et al., 2007;</ref><ref type=\"bibr\" target=\"#b5\">Choromanski . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 16\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b43\">Yang et al., 2019)</ref>, computer vision <ref type=\"bibr\" target=\"#b3\">(Carion et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Dosovitskiy . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2020;</ref><ref type=\"bibr\" target=\"#b24\">Peng et al., 2021)</ref>, we use random Fourier features <ref type=\"bibr\" target=\"#b27\">(Rahimi et al., 2007;</ref><ref type=\"bibr\" target=\"#b5\">Choromanski . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eed-forward layer is set to 3072. The RoBERTa pre-trained checkpoint is taken from the Transformers <ref type=\"bibr\" target=\"#b40\">(Wolf et al., 2020)</ref> library. We use AdamW <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b45\">(Zhang et al., 2021;</ref><ref type=\"bibr\" target=\"#b1\">Beltagy et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Ainslie et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Zaheer et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b1\">Beltagy et al., 2020;</ref><ref type=\"bibr\" target=\"#b0\">Ainslie et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Zaheer et al., 2020)</ref> used a pre-specified subset of locations i  on a machine with 4 NVIDIA Ampere A100 40GB GPUs and 64 AMD EPYC 7713 64-Core Processor in a Slurm <ref type=\"bibr\" target=\"#b44\">(Yoo et al., 2003)</ref> system. The evaluation of the inference thro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 16\">Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b43\">Yang et al., 2019)</ref>, computer vision <ref type=\"bibr\" target=\"#b3\">(Carion et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Dosovitskiy . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e computational complexity in the training phase, LHA uses unbiased kernelized attention techniques <ref type=\"bibr\" target=\"#b5\">(Choromanski et al., 2020;</ref><ref type=\"bibr\" target=\"#b24\">Peng et e quadratic complexity to linear complexity with kernelized approximation to the softmax operation. <ref type=\"bibr\" target=\"#b5\">Choromanski et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b24\">Pen t advances in kernelized attention <ref type=\"bibr\" target=\"#b12\">(Katharopoulos et al., 2020;</ref><ref type=\"bibr\" target=\"#b5\">Choromanski et al., 2020;</ref><ref type=\"bibr\" target=\"#b24\">Peng et  21)</ref>, we use random Fourier features <ref type=\"bibr\" target=\"#b27\">(Rahimi et al., 2007;</ref><ref type=\"bibr\" target=\"#b5\">Choromanski et al., 2020)</ref> to approximate \u03c8 (i) in an unbiased ma i) in an unbiased manner. Let us define \u03c6 : R d h \u2192 R 2D as the Positive Randomized Features (PRFs) <ref type=\"bibr\" target=\"#b5\">(Choromanski et al., 2020)</ref> such that</p><formula xml:id=\"formula ence length and d h is the hidden size. Let \u03c6 : R d h \u2192 R 2D be Positive Randomized Features (PRFs) <ref type=\"bibr\" target=\"#b5\">(Choromanski et al., 2020)</ref> such that:</p><p>We can approximate t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the tasks of ListOps (Nangia &amp; Bowman, 2018) (LO), byte-level IMDb reviews text classification <ref type=\"bibr\" target=\"#b19\">(Maas et al., 2011)</ref> (IMDb), byte-level document retrieval on AC sformer with 256 hidden size and 1024 feed-forward layer size for the IMDb text classification task <ref type=\"bibr\" target=\"#b19\">(Maas et al., 2011)</ref>, a 4-layer Transformer with 256 hidden size. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2020;</ref><ref type=\"bibr\" target=\"#b24\">Peng et al., 2021)</ref>, we use random Fourier features <ref type=\"bibr\" target=\"#b27\">(Rahimi et al., 2007;</ref><ref type=\"bibr\" target=\"#b5\">Choromanski . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b15\">Gao et al., 2018;</ref><ref type=\"bibr\">2020)</ref>. Currently, the message passing scheme <ref type=\"bibr\" target=\"#b18\">(Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Sanchez-Gon .1\">MESSAGE PASSING SCHEME</head><p>Currently, the class of message passing neural networks (MPNNs) <ref type=\"bibr\" target=\"#b18\">(Gilmer et al., 2017)</ref> are one of the most widely used architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ve 3D information. To this end, we conduct formal analyses in the spherical coordinate system (SCS) <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2019)</ref>, and show that relative location of each ato. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s in distinguishing geometric graph properties, such as girth and circumference, etc. Other studies <ref type=\"bibr\" target=\"#b24\">(Ingraham et al., 2019;</ref><ref type=\"bibr\" target=\"#b45\">Simm et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b4\">(Bronstein et al., 2021;</ref><ref type=\"bibr\" target=\"#b10\">De Haan et al., 2020;</ref><ref type=\"bibr\" target=\"#b37\">Perraudin et al., 2019)</ref>. When modeling 3D point clouds as 3D gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2020)</ref>, QM9 <ref type=\"bibr\" target=\"#b39\">(Ramakrishnan et al., 2014)</ref>, and MD17 <ref type=\"bibr\" target=\"#b7\">(Chmiela et al., 2017)</ref>. Baseline methods include <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  contains 3D coordinates for each atom given in the Cartesian system along with the graph structure <ref type=\"bibr\" target=\"#b33\">(Liu et al., 2019;</ref><ref type=\"bibr\" target=\"#b48\">Townshend et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s molecules are naturally modeled as graphs <ref type=\"bibr\" target=\"#b20\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b55\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b44\">Shervashidze et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>. Currently, the message passing scheme <ref type=\"bibr\" target=\"#b18\">(Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Sanchez-Gonzalez et al., 2020;</ref><ref type=\"bibr\" target=\"#b51\">Vi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s molecules are naturally modeled as graphs <ref type=\"bibr\" target=\"#b20\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b55\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b44\">Shervashidze et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ighborhood radius along with spatial orientation is incorporated in local point embedding. The work <ref type=\"bibr\" target=\"#b52\">Wang et al. (2019a)</ref> proposes a graph attention convolution for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ighborhood radius along with spatial orientation is incorporated in local point embedding. The work <ref type=\"bibr\" target=\"#b52\">Wang et al. (2019a)</ref> proposes a graph attention convolution for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  harmonics and Clebsh-Gordan coefficients <ref type=\"bibr\" target=\"#b47\">(Thomas et al., 2018;</ref><ref type=\"bibr\" target=\"#b13\">Fuchs et al., 2020)</ref>. In addition, the complicated SE(3) group r ield networks (TFNs) <ref type=\"bibr\" target=\"#b47\">(Thomas et al., 2018)</ref>, SE(3)-transformers <ref type=\"bibr\" target=\"#b13\">(Fuchs et al., 2020)</ref>, PaiNN <ref type=\"bibr\" target=\"#b42\">(Sch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  harmonics and Clebsh-Gordan coefficients <ref type=\"bibr\" target=\"#b47\">(Thomas et al., 2018;</ref><ref type=\"bibr\" target=\"#b13\">Fuchs et al., 2020)</ref>. In addition, the complicated SE(3) group r ield networks (TFNs) <ref type=\"bibr\" target=\"#b47\">(Thomas et al., 2018)</ref>, SE(3)-transformers <ref type=\"bibr\" target=\"#b13\">(Fuchs et al., 2020)</ref>, PaiNN <ref type=\"bibr\" target=\"#b42\">(Sch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2021)</ref>, NequIP <ref type=\"bibr\" target=\"#b3\">(Batzner et al., 2021)</ref>, Noisy Nodes <ref type=\"bibr\" target=\"#b19\">(Godwin et al., 2022)</ref>, etc. The raw input of these methods usua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Existing methods mainly capture distance information from local neighborhood in 3D space. In DGCNN <ref type=\"bibr\" target=\"#b53\">(Wang et al., 2019b)</ref>, a novel layer namely EdgeConv is proposed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2021)</ref>, NequIP <ref type=\"bibr\" target=\"#b3\">(Batzner et al., 2021)</ref>, Noisy Nodes <ref type=\"bibr\" target=\"#b19\">(Godwin et al., 2022)</ref>, etc. The raw input of these methods usua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graphs, and they can be important in molecular learning, such as bond lengths, angles between bonds <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">Klicpera et ethods is in early stage, and existing studies focus on leveraging different geometries. The SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref> incorporates the distance information dur al., 2017)</ref>. Baseline methods include <ref type=\"bibr\">PPGN (Maron et al., 2019)</ref>, SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref>, PhysNet (Unke &amp; Meuwly, 2019), Cormo e models employ a joint loss of forces and conserved energy during training. In the original SchNet <ref type=\"bibr\" target=\"#b41\">(Sch\u00fctt et al., 2017)</ref> and DimeNet <ref type=\"bibr\" target=\"#b28 ine the expressive power of SphereNet for molecular dynamics simulations. Following the settings in <ref type=\"bibr\" target=\"#b41\">Sch\u00fctt et al. (2017)</ref>; <ref type=\"bibr\" target=\"#b28\">Klicpera e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Sch\u00fctt et al., 2021)</ref>, NequIP <ref type=\"bibr\" target=\"#b3\">(Batzner et al., 2021)</ref>, MGCN <ref type=\"bibr\" target=\"#b35\">(Lu et al., 2019)</ref>, DimeNet <ref type=\"bibr\" target=\"#b28\">(Klic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b55\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b44\">Shervashidze et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Fout et al., 2017;</ref><ref type=\"bibr\" target=\"#b34\">Liu et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s in distinguishing geometric graph properties, such as girth and circumference, etc. Other studies <ref type=\"bibr\" target=\"#b24\">(Ingraham et al., 2019;</ref><ref type=\"bibr\" target=\"#b45\">Simm et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n from recent work in large-scale generative language models (LMs) which are good few-shot learners <ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Chowdhery et  language have been collected semi-automatically by scraping billions of web pages at a large scale <ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b94\">Rae et al.,  s at larger scales. Further discussion on typical weaknesses observed for large LMs can be found in <ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020;</ref><ref type=\"bibr\" target=\"#b94\">Rae et al.,  guage modelling with RNNs from Graves (2013); In the last two years, following the success of GPT-3 <ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020)</ref>, vast improvements have been obtained by i Zhang et al., 2021)</ref> to communicate the content of images using language descriptions to GPT-3 <ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020)</ref>.</p><p>The Flamingo models share numerous  pt to new tasks using in-context learning, following an analogous approach to the one used in GPT-3 <ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020)</ref>. In detail, we are given a set of support  o parameters and shots. A general trend we observe is that, similarly to what was observed in GPT-3 <ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020)</ref>, the larger the model, the better the few- fig_15\">13</ref>. Finally, language modeling suffers from poor sample efficiency during pretraining <ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020)</ref>. Mitigating this issue has the potential t vel task. A final category is based on the idea of \"in-context\" few-shot prompts for language models<ref type=\"bibr\" target=\"#b11\">(Brown et al., 2020)</ref> and it has recently been extended to image. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: emographics and geographic regions <ref type=\"bibr\" target=\"#b12\">(Buolamwini and Gebru, 2018;</ref><ref type=\"bibr\" target=\"#b22\">De Vries et al., 2019;</ref><ref type=\"bibr\" target=\"#b104\">Schwemmer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blation studies, we also compare the proposed solution against other recent conditioning approaches <ref type=\"bibr\" target=\"#b23\">(Desai and Johnson, 2021;</ref><ref type=\"bibr\" target=\"#b73\">Luo et  as in our -. Although used in recent work <ref type=\"bibr\" target=\"#b13\">(Carion et al., 2020;</ref><ref type=\"bibr\" target=\"#b23\">Desai and Johnson, 2021)</ref>, the approach under-performs our propo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: images and videos poses additional difficulties. To address this challenge we use a Perceiver-based <ref type=\"bibr\" target=\"#b53\">(Jaegle et al., 2021)</ref> architecture that can produce a small fix tion, particularly important when dealing with multiple long videos. In similar spirit to Perceiver <ref type=\"bibr\" target=\"#b53\">(Jaegle et al., 2021)</ref> and DETR <ref type=\"bibr\" target=\"#b13\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion that is well-suited for adaptation<ref type=\"bibr\" target=\"#b7\">(Bertinetto et al., 2018;</ref><ref type=\"bibr\" target=\"#b30\">Finn et al., 2017;</ref><ref type=\"bibr\" target=\"#b166\">Zintgraf et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion that is well-suited for adaptation<ref type=\"bibr\" target=\"#b7\">(Bertinetto et al., 2018;</ref><ref type=\"bibr\" target=\"#b30\">Finn et al., 2017;</ref><ref type=\"bibr\" target=\"#b166\">Zintgraf et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b86\">[87]</ref> (123K) 54.7 <ref type=\"bibr\" target=\"#b146\">[147]</ref> (20K) 25.2 <ref type=\"bibr\" target=\"#b138\">[139]</ref> (38K) 75.4 <ref type=\"bibr\" target=\"#b59\">[60]</ref> (9K. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: om the residual connection, where is a layer-specific learnable scalar initialized at 0 (similar to <ref type=\"bibr\" target=\"#b4\">Bachlechner et al., 2021)</ref>. Thus, at initialization, the branch g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge-scale pretrained transformers, e.g. DALL-E <ref type=\"bibr\" target=\"#b23\">[24]</ref> and CogView <ref type=\"bibr\" target=\"#b2\">[3]</ref>. These models generally learn to generate image tokens in an  the perplexity of CogLM image captioning, which is the post-selection method introduced in CogView <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>2. The generated images are directly mapped into 60 \u00d7 , are great challenges for these methods. DALL-E <ref type=\"bibr\" target=\"#b23\">[24]</ref>, CogView <ref type=\"bibr\" target=\"#b2\">[3]</ref> and similar works <ref type=\"bibr\" target=\"#b28\">[29,</ref>< im ].</formula><p>We mask all the image tokens, which is similar to the pretraining task of CogView <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>\u2022 (A Combination of Mask Prediction and Image Caption -stage VQ-VAE <ref type=\"bibr\" target=\"#b25\">[26]</ref>, largely following the tokenizer in CogView <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>Inspired by Esser et al. <ref type=\"bibr\" target=\"#b6 .</p><p>Transformer. The backbone of our pretrained CogLM is a Transformer with Sandwich Layer-Norm <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The model has 6 billion parameters (48 layers, hidden size   for pretraining contains about 30 million text-image pairs, mostly overlapped with that of CogView <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We filter about 5 million text-image pairs from the CogView ted into Chinese), and select the best one with the lowest caption perplexity (the Caption Score in <ref type=\"bibr\" target=\"#b2\">[3]</ref>). Note that FID is not the perfect metric to evaluate CogVie ity of current text-to-image models chose a resolution of 32 \u00d7 32 tokens (usually 256 \u00d7 256 pixels) <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targ tokens, three times larger than that of the previous works <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, further exacerbating the situation. For instance, there are  6 for meaningful comparison. (2) There are mistakes when translating English captions into Chinese. <ref type=\"bibr\" target=\"#b2\">(3)</ref> Our training data contain many single-object images, which a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: has been verified in the comparison of BERT <ref type=\"bibr\" target=\"#b1\">[2]</ref> and GPT on GLUE <ref type=\"bibr\" target=\"#b27\">[28]</ref>. (GLM) The main advantage over GLM is simplicity. To unify. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pling problem, we propose cluster sampling. We group the 20,000 tokens into 500 clusters via Kmeans <ref type=\"bibr\" target=\"#b15\">[16]</ref> based on their vectors in VQVAE. During sampling, we first. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s from the last region, slightly slowing down the multi-region infilling.</p><p>Advantages over GPT <ref type=\"bibr\" target=\"#b19\">[20]</ref>, GLM <ref type=\"bibr\" target=\"#b5\">[6]</ref> and MAE <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b4\">[5]</ref>. Even attention in the ViTs mainly deals with the interactions between local tokens <ref type=\"bibr\" target=\"#b21\">[22]</ref>. We find it possible to finetune the pretrained CogLM usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b4\">[5]</ref>. Even attention in the ViTs mainly deals with the interactions between local tokens <ref type=\"bibr\" target=\"#b21\">[22]</ref>. We find it possible to finetune the pretrained CogLM usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b9\">[10]</ref>, including AttnGAN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, DM-GAN <ref type=\"bibr\" target=\"#b34\">[35]</ref>, DF-GAN <ref type=\"bibr\" target=\"#b24\">[25]</ref> et al. A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ption.</p><p>Although the direct-mapping is a traditional practice for super-resolution, e.g. SRCNN <ref type=\"bibr\" target=\"#b3\">[4]</ref>, it is hardly qualified as generation; it focuses more on te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: has been verified in the comparison of BERT <ref type=\"bibr\" target=\"#b1\">[2]</ref> and GPT on GLUE <ref type=\"bibr\" target=\"#b27\">[28]</ref>. (GLM) The main advantage over GLM is simplicity. To unify. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sis on domain-specific datasets, e.g. Caltech-UCSD Birds 200, general-domain datasets, e.g. MS COCO <ref type=\"bibr\" target=\"#b14\">[15]</ref>, are great challenges for these methods. DALL-E <ref type= br\" target=\"#b23\">[24]</ref>, Fr\u00e9chet Inception Distances and Inception Scores evaluated on MS-COCO <ref type=\"bibr\" target=\"#b14\">[15]</ref>. 30,000 captions from validation set are sampled to evalua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly based on Generative Adversarial Nets <ref type=\"bibr\" target=\"#b9\">[10]</ref>, including AttnGAN <ref type=\"bibr\" target=\"#b30\">[31]</ref>, DM-GAN <ref type=\"bibr\" target=\"#b34\">[35]</ref>, DF-GAN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: org/ns/1.0\"><head n=\"2\">RELATED WORK 2.1 EA based on translational models</head><p>The TransE-based <ref type=\"bibr\" target=\"#b1\">[2]</ref> method represents the entity by modeling the triples involve  further explore the translation information for relations based on triples, and inspired by TransE <ref type=\"bibr\" target=\"#b1\">[2]</ref>, our designs a relational translation matrix M r i, j as fol r relation alignment for entity alignment. Here, we borrow the translational assumption from TransE <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The detailed steps are as follows:</p><formula xml:id=\"form. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: abeled data, while the number of unobserved examples is high. To learn model parameters, He and Sun <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduce a weighted regression loss that associates confi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arious translational methods, BootEA <ref type=\"bibr\" target=\"#b30\">[31]</ref>,</p><p>and TransEdge <ref type=\"bibr\" target=\"#b31\">[32]</ref> both utilize an iterative strategy to optimize model perfo ype=\"bibr\" target=\"#b47\">[48]</ref> 0.650 0.867 0.720 0.641 0.873 0.718 0.673 0.894 0.752 TransEdge <ref type=\"bibr\" target=\"#b31\">[32]</ref> 0.735 0.919 0.801 0.719 0.932 0.795 0.710 0.941 0.796 MRAE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b16\">[17]</ref> and description <ref type=\"bibr\" target=\"#b45\">[46]</ref>. GM-Align <ref type=\"bibr\" target=\"#b42\">[43]</ref>, RDGCN <ref type=\"bibr\" target=\"#b38\">[39]</ref> and HGCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ansformation to obtain relation-specific embeddings for each entity in a more efficient way. DINGAL <ref type=\"bibr\" target=\"#b44\">[45]</ref> expands the coupling distance between the parameter matrix. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b9\">[10]</ref> 0.508 0.745 0.591 0.507 0.737 0.590 0.516 0.768 0.605 GMNN <ref type=\"bibr\" target=\"#b43\">[44]</ref> 0.539 0.826 -0.433 0.681 -0.465 0.728 -AliNet <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b16\">[17]</ref> and description <ref type=\"bibr\" target=\"#b45\">[46]</ref>. GM-Align <ref type=\"bibr\" target=\"#b42\">[43]</ref>, RDGCN <ref type=\"bibr\" target=\"#b38\">[39]</ref> and HGCN . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tive strategy to optimize model performance, which is a key technology to improve performance. NAEA <ref type=\"bibr\" target=\"#b47\">[48]</ref> designs an attentional mechanism based on TransE to learn  ref type=\"bibr\" target=\"#b30\">[31]</ref> 0.629 0.847 0.703 0.622 0.854 0.701 0.653 0.874 0.731 NAEA <ref type=\"bibr\" target=\"#b47\">[48]</ref> 0.650 0.867 0.720 0.641 0.873 0.718 0.673 0.894 0.752 Tran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"4.1\">Evaluation Metrics</head><p>Like previous works <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, we use Hits@k (Hk) a , the H 10 value of GateGAT is slightly lower than GAT H 10, which may be because Gate reduces the  <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.209 0.512 0.310 0.25 0.572 0.360 0.247 0.577 0.360 KECG <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e entity representation learned from the KG embedding method <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. In addition, some   the soft correspondences to reach a matching consensus in local neighborhoods between graphs. RREA <ref type=\"bibr\" target=\"#b21\">[22]</ref> uses relational reflection transformation to obtain relati e=\"bibr\" target=\"#b20\">[21]</ref> 0.638 0.886 0.736 0.646 0.891 0.735 0.666 0.912 0.765 RREA(Basic) <ref type=\"bibr\" target=\"#b21\">[22]</ref> 0.715 0.929 0.794 0.713 0.933 0.793 0.739 0.946 0.816 Dual .849 DGMC <ref type=\"bibr\" target=\"#b7\">[8]</ref> 0.801 0.874 -0.848 0.897 -0.933 0.960 -RREA(Semi) <ref type=\"bibr\" target=\"#b21\">[22]</ref> 0.801 0.948 0.857 0.802 0.952 0.858 0.827 0.966 0.881 Dual erge faster and achieve the best performance. In the experiment, the supervised loss function (RREA <ref type=\"bibr\" target=\"#b21\">[22]</ref> uses 3000 epochs) usually requires thousands of epochs to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"4.1\">Evaluation Metrics</head><p>Like previous works <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, we use Hits@k (Hk) a , the H 10 value of GateGAT is slightly lower than GAT H 10, which may be because Gate reduces the  <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.209 0.512 0.310 0.25 0.572 0.360 0.247 0.577 0.360 KECG <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on bootstrapping have used pseudo-labels <ref type=\"bibr\" target=\"#b13\">[14]</ref>, cluster indices <ref type=\"bibr\" target=\"#b3\">[4]</ref>, or a handful of labels <ref type=\"bibr\" target=\"#b0\">[1,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">35,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, images <ref type=\"bibr\" target=\"#b16\">[17]</ref> and description <ref type=\"bibr\" target=\"#b45\">[46]</ref>. .981 0.954 Unsupervised PRASE <ref type=\"bibr\" target=\"#b24\">[25]</ref> 0.651 --0.726 --0.757 --EVA <ref type=\"bibr\" target=\"#b16\">[17]</ref> 0.761 0.907 0.814 0.762 0.913 0.817 0.793 0.942 0.847 SEU(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>Pseudo-labelling has been mainly adopted by methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> based on deep learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for its improved robustness. Though previous methods based on bootstrapping have used pseudo-labels <ref type=\"bibr\" target=\"#b13\">[14]</ref>, cluster indices <ref type=\"bibr\" target=\"#b3\">[4]</ref>,  e probability distribution of the model prediction is used as an indicator for pseudolabel the data <ref type=\"bibr\" target=\"#b13\">[14]</ref>. By using the simple and efficient method, the system can  the model can help enhance the performance of the model. For pseudo-label based methods, Lee et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> and Xie et al. <ref type=\"bibr\" target=\"#b41\">[42]</ref> g ifference among these methods is how to promote the accuracy of generated pseudo labels: Lee et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> only utilized the high confidence pseudo labels by setting \">Model Learning from Pseudo-labelling</head><p>Pseudo-labelling has been mainly adopted by methods <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b23\">24]</ref>. In addition, some semi-supervised methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> are also proposed t </ref>, attributes <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, images <ref type=\" <ref type=\"bibr\" target=\"#b17\">[18]</ref>, CEAFF <ref type=\"bibr\" target=\"#b46\">[47]</ref> and EPEA <ref type=\"bibr\" target=\"#b37\">[38]</ref> are models that combine entity attribute information. In t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b18\">[19]</ref> 0.808 0.940 0.857 0.801 0.949 0.855 0.840 0.965 0.888 RNM <ref type=\"bibr\" target=\"#b48\">[49]</ref> 0.840 0.919 0.870 0.872 0.944 0.899 0.938 0.981 0.954 Unsu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b27\">[28]</ref> 0.572 0.865 0.678 0.564 0.865 0.673 0.597 0.891 0.704 MRAEA(Basic) <ref type=\"bibr\" target=\"#b20\">[21]</ref> 0.638 0.886 0.736 0.646 0.891 0.735 0.666 0.912 0.765 RREA e=\"bibr\" target=\"#b31\">[32]</ref> 0.735 0.919 0.801 0.719 0.932 0.795 0.710 0.941 0.796 MRAEA(Iter) <ref type=\"bibr\" target=\"#b20\">[21]</ref> 0.757 0.930 0.827 0.758 0.934 0.826 0.781 0.948 0.849 DGMC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: supervised methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref> are also proposed to improve the performance of supervised  et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, images <ref type=\"bibr\" target=\"#b16\">[17]</ref> and descr <ref type=\"bibr\" target=\"#b36\">[37]</ref>. AttrGNN <ref type=\"bibr\" target=\"#b17\">[18]</ref>, CEAFF <ref type=\"bibr\" target=\"#b46\">[47]</ref> and EPEA <ref type=\"bibr\" target=\"#b37\">[38]</ref> are mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b13\">[14]</ref>, cluster indices <ref type=\"bibr\" target=\"#b3\">[4]</ref>, or a handful of labels <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  KG embedding method <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. In addition, some semi-supervised methods <ref type=\"bibr\" ]</ref> maps the entity embedding obtained by GNN to the hyperbolic space for entity alignment. REA <ref type=\"bibr\" target=\"#b23\">[24]</ref> has designed noise detection and noise perception to impro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tive strategy to optimize model performance, which is a key technology to improve performance. NAEA <ref type=\"bibr\" target=\"#b47\">[48]</ref> designs an attentional mechanism based on TransE to learn  ref type=\"bibr\" target=\"#b30\">[31]</ref> 0.629 0.847 0.703 0.622 0.854 0.701 0.653 0.874 0.731 NAEA <ref type=\"bibr\" target=\"#b47\">[48]</ref> 0.650 0.867 0.720 0.641 0.873 0.718 0.673 0.894 0.752 Tran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b27\">[28]</ref> introduces a multiscale codebook for better image generation. VQGAN <ref type=\"bibr\" target=\"#b10\">[11]</ref> trains the codebook with the adversarial objective and thu t=\"#b30\">[31]</ref>, which aims to learn discrete priors to encode images. The following work VQGAN <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposes a perceptual codebook by further using perceptual riefly describe the VQGAN model with its codebook in this section, and more details can be found in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. VQGAN is comprised of an encoder E, a decoder G and a cod Q codebooks trained only with HQ faces, when we adopt a proper compression patch size f . Following <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Equ. 3, we first train VQGAN with perceptual codebooks ike image generation <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, multi-modal generation <ref type=\"bibr\" target=\"#b34\">[35] der-decoder structure by the vector-quantization technique <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Then, a degraded face is encoded to a compact latent repre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rather than static. The following works propose different improvements to codebook learning. VQVAE2 <ref type=\"bibr\" target=\"#b27\">[28]</ref> introduces a multiscale codebook for better image generati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, down-sampling <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, etc. This task becomes more challenging in real-world scen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: idely-used non-reference perceptual metrics: FID <ref type=\"bibr\" target=\"#b14\">[15]</ref> and NIQE <ref type=\"bibr\" target=\"#b26\">[27]</ref>. We also measure the pixel-wise metrics (PSNR and SSIM) an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ollow previous work <ref type=\"bibr\" target=\"#b32\">[33]</ref> to use the embedding angle of ArcFace <ref type=\"bibr\" target=\"#b6\">[7]</ref> as the identity metric, which is denoted by 'Deg.'. In order. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, down-sampling <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, etc. This task becomes more challenging in real-world scen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: N. Perceptual Loss. We use the widely used perceptual loss <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> for both two decoder outputs: L per = \u2225\u03d5(x r ) \u2212 \u03d5(x h )\u2225 1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rained face generative adversarial network (e.g., StyleGAN <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>) to generate realistic textures. These methods typically pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (LQ) faces with unknown degradations, such as noise <ref type=\"bibr\" target=\"#b39\">[40]</ref>, blur <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, down-sampling <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ollow previous work <ref type=\"bibr\" target=\"#b32\">[33]</ref> to use the embedding angle of ArcFace <ref type=\"bibr\" target=\"#b6\">[7]</ref> as the identity metric, which is denoted by 'Deg.'. In order. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd thus the codebook can achieve high perceptual quality. To improve the codebook usage, some works <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> explore training t n of above losses:  channels in all experiments. In order to increase the codebook usage, we follow <ref type=\"bibr\" target=\"#b21\">[22]</ref> and periodically re-initialize the codebook with k-means c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, generative priors <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and reference prio evere degradations. These properties motivate researchers to find better priors.</p><p>Recent works <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> begin to investiga f><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, generative priors <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and reference prio listic details. Fig. <ref type=\"figure\">1</ref>: Comparisons of restoration quality between GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref> and VQFR. Our VQFR can restore high-quality facial details lution is to fuse low-level features from input into different decoder layers, just like in GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Although the input features could bring more fidelity inf 14]</ref> simultaneously optimize several codes to promote its reconstruction. Recent works GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref> and GPEN <ref type=\"bibr\" target=\"#b36\">[37]</ref> extract VQ codebooks in face restoration, i.e., training with LQ-HQ pairs as most face restoration works do <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Based on the trained model for reconstruction, we then fi {1 : 8}, {0 : 15} and {60 : 100}, respectively. Testing Datasets. Following the practice in GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref>, we conduct experiments on the synthetic dataset CelebA-Te get=\"#b41\">[42]</ref>) for benchmarking CelebA-Test with Ground-Truth (GT). We follow previous work <ref type=\"bibr\" target=\"#b32\">[33]</ref> to use the embedding angle of ArcFace <ref type=\"bibr\" tar f type=\"bibr\" target=\"#b13\">[14]</ref>, PULSE <ref type=\"bibr\" target=\"#b25\">[26]</ref> and GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Synthetic CelebA-Test. From the quantitative results in T common practice in <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we use the following degradation model to synthesize train. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, generative priors <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and reference prio evere degradations. These properties motivate researchers to find better priors.</p><p>Recent works <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> begin to investiga f><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, generative priors <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> and reference prio listic details. Fig. <ref type=\"figure\">1</ref>: Comparisons of restoration quality between GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref> and VQFR. Our VQFR can restore high-quality facial details lution is to fuse low-level features from input into different decoder layers, just like in GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Although the input features could bring more fidelity inf 14]</ref> simultaneously optimize several codes to promote its reconstruction. Recent works GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref> and GPEN <ref type=\"bibr\" target=\"#b36\">[37]</ref> extract VQ codebooks in face restoration, i.e., training with LQ-HQ pairs as most face restoration works do <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Based on the trained model for reconstruction, we then fi {1 : 8}, {0 : 15} and {60 : 100}, respectively. Testing Datasets. Following the practice in GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref>, we conduct experiments on the synthetic dataset CelebA-Te get=\"#b41\">[42]</ref>) for benchmarking CelebA-Test with Ground-Truth (GT). We follow previous work <ref type=\"bibr\" target=\"#b32\">[33]</ref> to use the embedding angle of ArcFace <ref type=\"bibr\" tar f type=\"bibr\" target=\"#b13\">[14]</ref>, PULSE <ref type=\"bibr\" target=\"#b25\">[26]</ref> and GFP-GAN <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Synthetic CelebA-Test. From the quantitative results in T common practice in <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we use the following degradation model to synthesize train. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: N. Perceptual Loss. We use the widely used perceptual loss <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> for both two decoder outputs: L per = \u2225\u03d5(x r ) \u2212 \u03d5(x h )\u2225 1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ter match the degraded faces, we further adopt a texture warping module with deformable convolution <ref type=\"bibr\" target=\"#b42\">[43]</ref> in the main decoder. Equipped with the VQ codebook as a fa ils. But their fidelity probably deviates from inputs. Therefore, we adopt a deformable convolution <ref type=\"bibr\" target=\"#b42\">[43]</ref> to better warp the realistic facial details F t towards th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ollow previous work <ref type=\"bibr\" target=\"#b32\">[33]</ref> to use the embedding angle of ArcFace <ref type=\"bibr\" target=\"#b6\">[7]</ref> as the identity metric, which is denoted by 'Deg.'. In order. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h as noise <ref type=\"bibr\" target=\"#b39\">[40]</ref>, blur <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, down-sampling <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b36\">37]</ref> and reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Specifically, geomet f type=\"bibr\" target=\"#b36\">37]</ref> and reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. The geometric priors aces into a suboptimal latent code.</p><p>Reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> typically rely on ref to 512 2 during training. Following the common practice in <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, we use the followi eference-based methods explore the high-quality guided faces <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> or facial component dictionary <ref type=\"bibr\" target=\"#b2 texture warping module. Our parallel decoder shares the same spirits as reference-based restoration <ref type=\"bibr\" target=\"#b23\">[24]</ref>. In our case, the features from the texture decoder serve . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Specifically, geometric priors usually consist of facial lan d reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. The geometric priors include facial landmark <ref type=\"bibr p>Reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> typically rely on reference images of the same identity. To a  space of the well-trained GAN model. Reference-based methods explore the high-quality guided faces <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> or facial component . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Specifically, geometric priors usually consist of facial lan d reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. The geometric priors include facial landmark <ref type=\"bibr p>Reference priors <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> typically rely on reference images of the same identity. To a  space of the well-trained GAN model. Reference-based methods explore the high-quality guided faces <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> or facial component . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b10\">[11]</ref> proposes a perceptual codebook by further using perceptual loss <ref type=\"bibr\" target=\"#b16\">[17]</ref> and adversarial training objectives <ref type=\"bibr\" targe  to the quantized code by pretrained VQGAN. Perceptual Loss. We use the widely used perceptual loss <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> for both two decod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lly exploit face-specific priors, including geometric priors <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, generative priors <r priors can be categorized into three types: geometric priors <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, generative priors <r e=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> and facial component heatmaps <ref type=\"bibr\" target=\"#b38\">[39]</ref>. They could provide global guidance for restoring accurate e=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> and facial component heatmaps <ref type=\"bibr\" target=\"#b38\">[39]</ref>. Those priors are estimated from degraded images and thus . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rence due to its sequential decoding. To tackle the problem, Non-autoregressive Transformers (NATs, <ref type=\"bibr\" target=\"#b14\">Gu et al., 2018;</ref><ref type=\"bibr\" target=\"#b15\">Gu et al., 2019; ransformer (AT) systems.</p><p>However, current NATs severely suffer from the multimodality problem <ref type=\"bibr\" target=\"#b14\">(Gu et al., 2018)</ref> in both training and inference. <ref type=\"fo modalities by knowledge distillation (KD, <ref type=\"bibr\" target=\"#b26\">Kim &amp; Rush, 2016;</ref><ref type=\"bibr\" target=\"#b14\">Gu et al., 2018)</ref>, namely, replacing the original training targe \"#b2\">Bao et al., 2021)</ref>. Nevertheless, these NATs heavily rely on knowledge distillation (KD, <ref type=\"bibr\" target=\"#b14\">Gu et al., 2018)</ref>, which is found very effective in reducing the p://www.tei-c.org/ns/1.0\"><head n=\"2.\">Related Work</head><p>Non-autoregressive Machine Translation <ref type=\"bibr\" target=\"#b14\">Gu et al. (2018)</ref> propose NAT models to reduce the latency in ge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a;</ref><ref type=\"bibr\" target=\"#b6\">Du et al., 2021)</ref>, or incorporating extra decoder inputs <ref type=\"bibr\" target=\"#b45\">(Shu et al., 2020;</ref><ref type=\"bibr\" target=\"#b37\">Qian et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: AG. Specifically, we begin at the start vertex and repeatedly use Nucleus Sampling (top-p sampling, <ref type=\"bibr\" target=\"#b19\">Holtzman et al., 2020)</ref> to choose the next vertex and token acco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and AT models. To bridge the gap, iterative NATs manage to repeatedly refine the generated outputs <ref type=\"bibr\" target=\"#b28\">(Lee et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Ghazvininejad e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ord lattices as inputs to alleviate input errors brought by word segmentation or speech recognition <ref type=\"bibr\" target=\"#b7\">(Dyer et al., 2008;</ref><ref type=\"bibr\" target=\"#b27\">Koehn et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e from alignment-based objectives <ref type=\"bibr\" target=\"#b30\">(Libovick\u00fd &amp; Helcl, 2018;</ref><ref type=\"bibr\" target=\"#b10\">Ghazvininejad et al., 2020a;</ref><ref type=\"bibr\" target=\"#b6\">Du et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  report competitive results on several challenging video datasets using vanilla Vision Transformers <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We observe that MAE can outperform supervised pre-trainin rs <ref type=\"bibr\" target=\"#b66\">[67]</ref> have been successfully introduced into computer vision <ref type=\"bibr\" target=\"#b17\">[18]</ref> and established as a general building block in both langua =\"#b30\">[31]</ref>. Towards unifying methodologies, less domain knowledge (\"fewer inductive biases\" <ref type=\"bibr\" target=\"#b17\">[18]</ref>) is introduced for a specific problem, which urges the mod  nature of the problem. In particular, our encoder and decoder are both vanilla Vision Transformers <ref type=\"bibr\" target=\"#b17\">[18]</ref> with no factorization or hierarchy, and our random mask sa : on Kinetics-400 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, it increases the accuracy of ViT-Large <ref type=\"bibr\" target=\"#b17\">[18]</ref> by absolute 13% vs. training from scratch, while it takes  e-training can outperform its supervised pre-training counterpart by big margins. Using vanilla ViT <ref type=\"bibr\" target=\"#b17\">[18]</ref>, our method achieves competitive results with previous sta \"#b8\">[9]</ref> pioneers this direction by training Transformers on pixels as tokens. The ViT paper <ref type=\"bibr\" target=\"#b17\">[18]</ref> makes a revolutionary step forward by using patches as tok ork, with as little domain knowledge as possible.</p><p>Patch embedding. Following the original ViT <ref type=\"bibr\" target=\"#b17\">[18]</ref>, given a video clip, we divide it into a regular grid of n ef type=\"bibr\" target=\"#b76\">77]</ref>. The patches are flattened and embedded by linear projection <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Positional embeddings <ref type=\"bibr\" target=\"#b66\">[67] hes and thus allows to use a higher masking ratio.</p><p>Autoencoding. Our encoder is a vanilla ViT <ref type=\"bibr\" target=\"#b17\">[18]</ref> applied only on the visible set of embedded patches, follo od relies on the global self-attention to learn useful knowledge from data, following the spirit of <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head   throughout training.</p><p>Architecture. Our encoder and decoder are the vanilla ViT architectures <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We use a temporal patch size of 2 <ref type=\"bibr\" target \" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b76\">77]</ref> and a spatial patch size of 16\u00d716 <ref type=\"bibr\" target=\"#b17\">[18]</ref>, denoted as 2\u00d716\u00d716. We use the same patch size for ViT-B/  type=\"bibr\" target=\"#b17\">[18]</ref>, denoted as 2\u00d716\u00d716. We use the same patch size for ViT-B/L/H <ref type=\"bibr\" target=\"#b17\">[18]</ref> for simplicity. For a 16\u00d7224\u00d7224 input, this patch size pr tations with minimal domain knowledge or inductive biases. This follows the spirit of the ViT paper <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Similar to BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> s for ViT-B/L/H. For fine-tuning, we add a linear classifier layer to the encoder's averaged tokens <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>For fine-tuning the intermediately fine-tuned check ig.5compares MAE pre-training vs. no pre-training (i.e., training from scratch), using vanilla ViT-L<ref type=\"bibr\" target=\"#b17\">[18]</ref>. The from-scratch recipe follows<ref type=\"bibr\" target=\"#  continuous progress <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. A series of recent ws this line of research.</p><p>Instead of predicting pixels <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  into a regular grid of non-overlapping patches in spacetime <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targe time factorization, in contrast to the leading architectures <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Our method relies on illa ViT architectures <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We use a temporal patch size of 2 <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  vision. For self-supervised representation learning, the denoising/masked autoencoding methodology <ref type=\"bibr\" target=\"#b67\">[68]</ref> in BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> has been es. MAE <ref type=\"bibr\" target=\"#b30\">[31]</ref> returns to the basics of the autoencoding concept <ref type=\"bibr\" target=\"#b67\">[68]</ref> and draws attention to the decoding aspect. The presence o .org/ns/1.0\"><head n=\"2\">Related Work</head><p>Denoising autoencoders. Denoising autoencoders (DAE) <ref type=\"bibr\" target=\"#b67\">[68,</ref><ref type=\"bibr\" target=\"#b68\">69]</ref> present a general . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b75\">76]</ref>, temporal ordering <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b51\">[52,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. While our method has largely improved the efficiency of self. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b2\">[3]</ref> proposes to use pre-trained dVAE <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref> as the Figure <ref type=\"figure\">2</ref>: Visualizations on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b77\">78,</ref><ref type=\"bibr\" target=\"#b80\">81]</ref>, spatiotemporal contrast <ref type=\"bibr\" target=\"#b57\">[58. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s for computer vision have been making continuous progress <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" targe bibr\" target=\"#b66\">[67]</ref> and are towards a unified solution between vision and language. iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> pioneers this direction by training Transformers on pixels a rget=\"#b30\">[31]</ref>. Our study follows this line of research.</p><p>Instead of predicting pixels <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" targ ined solution. In contrast, an extra tokenizer (e.g., dVAE <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>), as is used in <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s for computer vision have been making continuous progress <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" targe bibr\" target=\"#b66\">[67]</ref> and are towards a unified solution between vision and language. iGPT <ref type=\"bibr\" target=\"#b8\">[9]</ref> pioneers this direction by training Transformers on pixels a rget=\"#b30\">[31]</ref>. Our study follows this line of research.</p><p>Instead of predicting pixels <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" targ ined solution. In contrast, an extra tokenizer (e.g., dVAE <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>), as is used in <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rantee. If such regions are not available, ASAP is unable to prefetch.</p><p>Elastic Cuckoo Hashing <ref type=\"bibr\" target=\"#b47\">[49]</ref> restructures the page table into a hash table to generate  of our approach and those of ASAP <ref type=\"bibr\" target=\"#b34\">[36]</ref>, Elastic Cuckoo Hashing <ref type=\"bibr\" target=\"#b47\">[49]</ref>, and CSALT <ref type=\"bibr\" target=\"#b33\">[35]</ref>. The . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s, processes, and VMs. Marathe et al. extended POM_TLB with a cache prioritization extension (CSALT <ref type=\"bibr\" target=\"#b33\">[35]</ref>), to keep page table entries in the cache during frequent  cy to keep page table entries when an application is experiencing high TLB miss rates. Unlike CSALT <ref type=\"bibr\" target=\"#b33\">[35]</ref>, which biases the cache to store TLB entries to support th rget=\"#b34\">[36]</ref>, Elastic Cuckoo Hashing <ref type=\"bibr\" target=\"#b47\">[49]</ref>, and CSALT <ref type=\"bibr\" target=\"#b33\">[35]</ref>. The performance numbers are presented relative to a basel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Other works have focused on reducing the TLB miss penalty by improving the page table walk caches <ref type=\"bibr\" target=\"#b12\">[14,</ref><ref type=\"bibr\" target=\"#b15\">17,</ref><ref type=\"bibr\" ta ith 64 B-lines, which does not accurately represent either page table caches nor translation caches <ref type=\"bibr\" target=\"#b12\">[14]</ref>. We implemented our own PWC, which sends misses to the L1 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lation to hide latency <ref type=\"bibr\" target=\"#b3\">[5,</ref><ref type=\"bibr\" target=\"#b6\">8,</ref><ref type=\"bibr\" target=\"#b13\">15,</ref><ref type=\"bibr\" target=\"#b45\">47]</ref>, optimizing hash pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b6\">[8,</ref><ref type=\"bibr\" target=\"#b14\">16,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b21\">23,</ref><ref type=\"bibr\" target=\"#b23\">25,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b40\">[42]</ref><ref type=\"bibr\" target=\"#b41\">[43]</ref><ref type=\"bibr\" target=\"#b42\">[44]</ref><ref type=\"bibr\" target=\"#b43\">[45]</ref><ref type=\"bibr\" target=\"#b44\">[46]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed systems, Gandhi et al. proposed merging the 2D page table into a single dimension where possible <ref type=\"bibr\" target=\"#b24\">[26]</ref>. Ahn et al. proposed flat host page tables for precisely t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  table walk caches <ref type=\"bibr\" target=\"#b12\">[14,</ref><ref type=\"bibr\" target=\"#b15\">17,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>, using speculation to hide latency <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: age tables <ref type=\"bibr\" target=\"#b49\">[52]</ref>, and replicating page tables across NUMA nodes <ref type=\"bibr\" target=\"#b1\">[3]</ref>. For virtualized systems, Gandhi et al. proposed merging the ef type=\"bibr\" target=\"#b22\">[24]</ref> (inputs: url_combined and HIGGS); a hashjoin microbenchmark <ref type=\"bibr\" target=\"#b1\">[3]</ref>, and the XSBench <ref type=\"bibr\" target=\"#b48\">[50]</ref>. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ages in large pages to make host translations more efficient <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b8\">10]</ref>. Using large pages comes with the benefit of removing the la. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntiguous memory allocations, this would make it difficult to implement in practice.</p><p>Compendia <ref type=\"bibr\" target=\"#b4\">[6]</ref> is a parallel work that addressed flattening the page table.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed systems, Gandhi et al. proposed merging the 2D page table into a single dimension where possible <ref type=\"bibr\" target=\"#b24\">[26]</ref>. Ahn et al. proposed flat host page tables for precisely t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Other works have focused on reducing the TLB miss penalty by improving the page table walk caches <ref type=\"bibr\" target=\"#b12\">[14,</ref><ref type=\"bibr\" target=\"#b15\">17,</ref><ref type=\"bibr\" ta ith 64 B-lines, which does not accurately represent either page table caches nor translation caches <ref type=\"bibr\" target=\"#b12\">[14]</ref>. We implemented our own PWC, which sends misses to the L1 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ages in large pages to make host translations more efficient <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b8\">10]</ref>. Using large pages comes with the benefit of removing the la. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for the 0% LP scenario. The cache energy is comprised of the L1D, L2 and L3 modeled with the CACTI <ref type=\"bibr\" target=\"#b37\">[39]</ref> at 22nm. We include both data and page table walks. For DR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  penalty for the 2D page walk to translate each level of the guest page walk on the hypervisor side <ref type=\"bibr\" target=\"#b15\">[17]</ref>. This problem will be exacerbated with 5-level page tables d that 0.5% (20 L4: 1-cycle, 4-entry FA L3: 1-cycle, 4-entry FA L2: 1-cycle, 24-entry FA Nested TLB <ref type=\"bibr\" target=\"#b15\">[17]</ref> 1-cycle 16-entry FA out of 3464 compiler invocations) fail per page walk, but, because we include two sets of PWCs for the guest and the host and a nested TLB <ref type=\"bibr\" target=\"#b15\">[17]</ref> to hold host translations, the average number of accesses  uires 11 MB of guest and 22 KB of corresponding host page tables for its 5.4 GB gVA. The Nested TLB <ref type=\"bibr\" target=\"#b15\">[17]</ref> and the vPWC (host PWC) work together to efficiently cache e TLB miss penalty by improving the page table walk caches <ref type=\"bibr\" target=\"#b12\">[14,</ref><ref type=\"bibr\" target=\"#b15\">17,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>, using speculation . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r the lower-half of the address space to simulate an OS that runs out of free large pages. Mosalloc <ref type=\"bibr\" target=\"#b2\">[4]</ref> proposed another layout that uses random windows of contiguo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b25\">27,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" tar </ref>. Mazumdar et al. proposed predicting dead TLB entries and dead page table entries in the LLC <ref type=\"bibr\" target=\"#b35\">[37]</ref>. This could be used to extend our cache prioritization, al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b25\">27,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b31\">33,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" tar </ref>. Mazumdar et al. proposed predicting dead TLB entries and dead page table entries in the LLC <ref type=\"bibr\" target=\"#b35\">[37]</ref>. This could be used to extend our cache prioritization, al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  table walk caches <ref type=\"bibr\" target=\"#b12\">[14,</ref><ref type=\"bibr\" target=\"#b15\">17,</ref><ref type=\"bibr\" target=\"#b16\">18]</ref>, using speculation to hide latency <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge table cached in a 16 MB LLC), page table entries were found to suffer capacity misses in the LLC <ref type=\"bibr\" target=\"#b17\">[19]</ref>. We cannot predict the benefit of cache prioritization for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ages in large pages to make host translations more efficient <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b8\">10]</ref>. Using large pages comes with the benefit of removing the la. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> or deep ensembles <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Large-scale studies <ref type=\"bibr\" target=\"#b26\">[27]</ el y.</p><p>Deep ensembles. From the structural point of view, we apply deep ensembles, proposed by <ref type=\"bibr\" target=\"#b19\">[20]</ref>, which have shown great success in terms of calibration on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  of uncertainty estimates. In order to measure the classification performance we consider the AUROC <ref type=\"bibr\" target=\"#b9\">[10]</ref>. To measure calibration, we report the Calibration Error de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onds), and a stress phase (about 10 minutes). Stress was induced using the Trier Social Stress Test <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The recorded sensors include BVP, ECG, EDA, EMG, respirat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ple of a batch by the individual classification error of the model. Mathematically it is defined by <ref type=\"bibr\" target=\"#b20\">[21]</ref> as</p><formula xml:id=\"formula_0\">L = \u2212(1 \u2212 P \u03b8 (y|x)) \u03b3 l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  expert knowledge. As an architecture for the feature extraction we use an adaptation of the ResNet <ref type=\"bibr\" target=\"#b10\">[11]</ref> architecture for time series data by Wang et al. <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nimodal data. They range from theoretical investigations on simple regression tasks on tabular data <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e ResNet <ref type=\"bibr\" target=\"#b10\">[11]</ref> architecture for time series data by Wang et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref>. For each modality, a 128 dimensional feature representati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  expert knowledge. As an architecture for the feature extraction we use an adaptation of the ResNet <ref type=\"bibr\" target=\"#b10\">[11]</ref> architecture for time series data by Wang et al. <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  we first create a synthetic shift for an existing multimodal dataset. To do this, we use the WESAD <ref type=\"bibr\" target=\"#b29\">[30]</ref> dataset and generate perturbations in each modality at inf ad n=\"3\">EXPERIMENTAL SETUP 3.1 Data</head><p>For our analysis, we use the multimodal dataset WESAD <ref type=\"bibr\" target=\"#b29\">[30]</ref>, in which physiological data were recorded from 15 individ esults can be seen in Figure <ref type=\"figure\" target=\"#fig_1\">3</ref>   with the results given in <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Interestingly, the error decreases for more corruption or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of uncertainty estimates. In order to measure the classification performance we consider the AUROC <ref type=\"bibr\" target=\"#b9\">[10]</ref>. To measure calibration, we report the Calibration Error de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  expert knowledge. As an architecture for the feature extraction we use an adaptation of the ResNet <ref type=\"bibr\" target=\"#b10\">[11]</ref> architecture for time series data by Wang et al. <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b117\">121,</ref><ref type=\"bibr\" target=\"#b118\">122]</ref> and software <ref type=\"bibr\" target=\"#b5\">[9,</ref><ref type=\"bibr\" target=\"#b28\">32,</ref><ref type=\"bibr\" targ puted by sequences of instructions (the load-slice) rendering manual prefetch injection challenging <ref type=\"bibr\" target=\"#b5\">[9]</ref>. Code changes can easily break the prefetch address computat ead to a performance regression. Recent work on compiler-based automatic prefetch injection schemes <ref type=\"bibr\" target=\"#b5\">[9]</ref> has addressed the challenge of generating accurate prefetch- chanisms. Specifically, we investigate why the state-of-the-art software data prefetching mechanism <ref type=\"bibr\" target=\"#b5\">[9]</ref> falls significantly short of an ideal (in terms of accuracy, g timeliness, APT-GET significantly outperforms the state-of-the-art software prefetching mechanism <ref type=\"bibr\" target=\"#b5\">[9]</ref> and provides on average 1.25\u00d7 greater speedup.</p><p>Overall tomatically inject software prefetches utilizing the state-of-the-art software prefetching approach <ref type=\"bibr\" target=\"#b5\">[9]</ref> implemented as an LLVM compiler pass. The pass operates at t ty work functions, the optimal prefetch-distance is 32, 16, and 4 respectively. Existing techniques <ref type=\"bibr\" target=\"#b5\">[9]</ref> utilize a static prefetch-distance and hence do not provide  ion, Lines 35-38, it performs the same static searching scheme as proposed by Ainsworth &amp; Jones <ref type=\"bibr\" target=\"#b5\">[9]</ref>  to capture all load instructions with indirect memory acces ice search function similar to Depth-First Search (DFS) algorithm proposed by Ainsworth &amp; Jones <ref type=\"bibr\" target=\"#b5\">[9]</ref>. This function extracts the load slices of the load instruct a no-prefetching baseline and against the static prefetch injection technique Ainsworth &amp; Jones <ref type=\"bibr\" target=\"#b5\">[9]</ref>. We execute each experiment three times and utilize perf sta ions as described in Table <ref type=\"table\" target=\"#tab_4\">3</ref>. In accordance with prior work <ref type=\"bibr\" target=\"#b5\">[9]</ref>, applications were selected for exhibiting indirect memory a gure\" target=\"#fig_5\">6</ref> shows the execution time speedup of APT-GET and Ainsworth &amp; Jones <ref type=\"bibr\" target=\"#b5\">[9]</ref> across all benchmarks over the no-prefetching baseline. Ains. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ned by the PHINodes of delinquent load instructions (lines <ref type=\"bibr\" target=\"#b21\">[25]</ref><ref type=\"bibr\" target=\"#b22\">[26]</ref><ref type=\"bibr\" target=\"#b23\">[27]</ref><ref type=\"bibr\" t We classify prior works in three categories. Software prefetching. Traditional software prefetching <ref type=\"bibr\" target=\"#b22\">[26]</ref> techniques utilize compilers to perform static code analys. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the RandomAccess benchmark suite.</p><p>We also use two different forms of the Hash join benchmark <ref type=\"bibr\" target=\"#b15\">[19]</ref>, Hash Join 2EPB (HJ2) and Hash Join 8EPB (HJ8), to evaluat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b85\">[89]</ref><ref type=\"bibr\" target=\"#b86\">[90]</ref><ref type=\"bibr\" target=\"#b87\">[91]</ref><ref type=\"bibr\" target=\"#b112\">116]</ref> data prefetching mechanisms have been proposed in the lit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[17,</ref><ref type=\"bibr\" target=\"#b24\">28,</ref><ref type=\"bibr\" target=\"#b51\">55,</ref><ref type=\"bibr\" target=\"#b56\">60,</ref><ref type=\"bibr\" target=\"#b113\">117]</ref> prefetchers can l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b32\">[36]</ref>. The state-of-the-art static techniques <ref type=\"bibr\" target=\"#b0\">[4,</ref><ref type=\"bibr\" target=\"#b25\">29,</ref><ref type=\"bibr\" target=\"#b54\">58,</ref><ref type=\"bibr\" tar =\"#b22\">[26]</ref><ref type=\"bibr\" target=\"#b23\">[27]</ref><ref type=\"bibr\" target=\"#b24\">[28]</ref><ref type=\"bibr\" target=\"#b25\">[29]</ref><ref type=\"bibr\" target=\"#b26\">[30]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . However, due to the complexity of contemporary microprocessors, cost models show limited accuracy <ref type=\"bibr\" target=\"#b32\">[36]</ref>. The state-of-the-art static techniques <ref type=\"bibr\" t  of basic blocks even under the assumption that all memory access times are constant and well-known <ref type=\"bibr\" target=\"#b32\">[36]</ref>. Moreover, these cost models have to be well maintained an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g., machine learning <ref type=\"bibr\" target=\"#b2\">[6,</ref><ref type=\"bibr\" target=\"#b30\">34,</ref><ref type=\"bibr\" target=\"#b123\">127]</ref>,mobile applications <ref type=\"bibr\" target=\"#b18\">[22,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b85\">[89]</ref><ref type=\"bibr\" target=\"#b86\">[90]</ref><ref type=\"bibr\" target=\"#b87\">[91]</ref><ref type=\"bibr\" target=\"#b112\">116]</ref> data prefetching mechanisms have been proposed in the lit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ple prefetchers (e.g., next-line <ref type=\"bibr\" target=\"#b105\">[109]</ref> and stride prefetchers <ref type=\"bibr\" target=\"#b57\">[61]</ref>) are implemented in today's hardware since complex prefetc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . However, due to the complexity of contemporary microprocessors, cost models show limited accuracy <ref type=\"bibr\" target=\"#b32\">[36]</ref>. The state-of-the-art static techniques <ref type=\"bibr\" t  of basic blocks even under the assumption that all memory access times are constant and well-known <ref type=\"bibr\" target=\"#b32\">[36]</ref>. Moreover, these cost models have to be well maintained an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>. Recently, the pre-trained multilingual language model is effective to address the challenge <ref type=\"bibr\" target=\"#b3\">(Devlin et al., 2019)</ref>. Moreover, some research introduces new co org/ns/1.0\"><head n=\"4.2\">Implementation Details</head><p>We use PyTorch 1. pre-trained mBERT model <ref type=\"bibr\" target=\"#b3\">(Devlin et al., 2019)</ref> in HuggingFace Transformer<ref type=\"foot\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rain the cross-lingual NER model, but the noise from translation process restrains its performance. <ref type=\"bibr\" target=\"#b9\">(Mayhew et al., 2017;</ref><ref type=\"bibr\" target=\"#b21\">Xie et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: larity knowledge inspired by multi-task learning.</p><p>Siamese Network is originally introduced by <ref type=\"bibr\" target=\"#b0\">(Bromley et al., 1994)</ref> to treat signature verification as a matc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge <ref type=\"bibr\">(Wu et al., 2020a,b;</ref><ref type=\"bibr\" target=\"#b1\">Chen et al., 2021;</ref><ref type=\"bibr\" target=\"#b8\">Liang et al., 2021)</ref>.</p><p>Although the above-mentioned models s et=\"#b1\">(Chen et al., 2021)</ref> proposes a adversarial discriminator for cross-lingual NER. RIKD <ref type=\"bibr\" target=\"#b8\">(Liang et al., 2021)</ref> develops a reinforced iterative knowledge d nst the version of TOF w/o continual learning <ref type=\"bibr\">(Zhang et</ref>  2021), RIKD w/o IKD <ref type=\"bibr\" target=\"#b8\">(Liang et al., 2021)</ref> and Unitrans w/o translation <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: larity knowledge inspired by multi-task learning.</p><p>Siamese Network is originally introduced by <ref type=\"bibr\" target=\"#b0\">(Bromley et al., 1994)</ref> to treat signature verification as a matc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > 48.12 60.55 61.56 WS <ref type=\"bibr\" target=\"#b11\">(Ni et al., 2017)</ref> 58.50 65.10 65.40 TMP <ref type=\"bibr\" target=\"#b4\">(Jain et al., 2019)</ref> 61.50 73.50 69.9 BERT-f <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: slation process restrains its performance. <ref type=\"bibr\" target=\"#b9\">(Mayhew et al., 2017;</ref><ref type=\"bibr\" target=\"#b21\">Xie et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Wu et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: larity knowledge inspired by multi-task learning.</p><p>Siamese Network is originally introduced by <ref type=\"bibr\" target=\"#b0\">(Bromley et al., 1994)</ref> to treat signature verification as a matc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ming. This situation is more severe for zero-resource languages. With the help of transfer learning <ref type=\"bibr\" target=\"#b13\">(Ruder et al., 2019)</ref> and multilingual BERT (short as mBERT) <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: slation process restrains its performance. <ref type=\"bibr\" target=\"#b9\">(Mayhew et al., 2017;</ref><ref type=\"bibr\" target=\"#b21\">Xie et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Wu et al., 2020. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  translation.</p><p>Knowledge distillation based models include a teacher model and a student model <ref type=\"bibr\" target=\"#b19\">(Wu et al., 2020c)</ref>. The teacher model is trained on the labeled s, 12 attention heads, and 768 hidden units.</p><p>We set our hyperparameters empirically following <ref type=\"bibr\" target=\"#b19\">(Wu et al., 2020c)</ref> with some modifications. We do not freeze an )</ref> 73.16 76.75 80.44 Unitrans <ref type=\"bibr\" target=\"#b18\">(Wu et al., 2020b)</ref> 74   TSL <ref type=\"bibr\" target=\"#b19\">(Wu et al., 2020c)</ref> proposes a teacher-student learning model fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: his new job search is stress in his job. <ref type=\"bibr\" target=\"#b13\">Li et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b11\">Koncel et al. (2019)</ref> consider such context structure by buildin n Figure <ref type=\"figure\">1</ref> (b), <ref type=\"bibr\" target=\"#b13\">Li et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b11\">Koncel et al. (2019)</ref>'s methods work by looking up the event tup ystem can be regarded as the adaption of <ref type=\"bibr\" target=\"#b13\">Li et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b11\">Koncel et al. (2019)</ref>'s retrieval-based methods on a pretrained  ture is not substantially integrated with BERT. <ref type=\"bibr\" target=\"#b8\">Guan (2019)</ref> and <ref type=\"bibr\" target=\"#b11\">Koncel et al. (2019)</ref> propose retrievalbased methods to leverage s the word-event relationship by an heterogeneous graph attention mechanism. (iii) GraphTransformer <ref type=\"bibr\" target=\"#b11\">(Koncel et al., 2019)</ref> retrieves structural information from eve e also trained the BERT <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref>, GraphTransformer <ref type=\"bibr\" target=\"#b11\">(Koncel et al., 2019)</ref> on the MCNC dataset, then finetuned them  ef> 52.45** BERT <ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref> 57.35** GraphTransformer <ref type=\"bibr\" target=\"#b11\">(Koncel et al., 2019)</ref> 58.53** HeterEvent <ref type=\"bibr\" targe ref type=\"bibr\" target=\"#b6\">(Devlin et al., 2019)</ref> 88.1* BERTMCNC 88.5* GraphTransformer MCNC <ref type=\"bibr\" target=\"#b11\">(Koncel et al., 2019)</ref> 88.9 HeterEventMCNC <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ong-standing task in natural language processing <ref type=\"bibr\" target=\"#b17\">(Minsky, 1974;</ref><ref type=\"bibr\" target=\"#b21\">Schank, 1975)</ref>. Much research has been done on extracting script he investigation of scripts dates back to 1970's <ref type=\"bibr\" target=\"#b17\">(Minsky, 1974;</ref><ref type=\"bibr\" target=\"#b21\">Schank, 1975)</ref>. The script event prediction task models the rela. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ong-standing task in natural language processing <ref type=\"bibr\" target=\"#b17\">(Minsky, 1974;</ref><ref type=\"bibr\" target=\"#b21\">Schank, 1975)</ref>. Much research has been done on extracting script he investigation of scripts dates back to 1970's <ref type=\"bibr\" target=\"#b17\">(Minsky, 1974;</ref><ref type=\"bibr\" target=\"#b21\">Schank, 1975)</ref>. The script event prediction task models the rela. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rent transformer layers of BERT tend to concentrate on different semantic and syntactic information <ref type=\"bibr\" target=\"#b4\">(Clark et al., 2019;</ref><ref type=\"bibr\" target=\"#b5\">Coenen et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arn commonsense knowledge for un-derstanding the story plot and predicting the ending. To this end, <ref type=\"bibr\" target=\"#b12\">Li et al. (2018a)</ref> and <ref type=\"bibr\" target=\"#b8\">Guan (2019). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the modeling of event relation information. To this end, early work exploited event pair relations <ref type=\"bibr\" target=\"#b1\">(Chambers, 2008;</ref><ref type=\"bibr\" target=\"#b9\">Jans et al., 2012; relationships between abstract events. Previous studies propose to model the pair-wise relationship <ref type=\"bibr\" target=\"#b1\">(Chambers, 2008;</ref><ref type=\"bibr\" target=\"#b9\">Jans et al., 2012;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t level representations. For an arbitrary event X i \u2208 X, we employ a multi-head attention operation <ref type=\"bibr\" target=\"#b23\">(Vaswani et al., 2017)</ref> to aggregate information from the corres. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  been used to model a chain of context events. There has also been work integrating the two methods <ref type=\"bibr\" target=\"#b25\">(Wang et al., 2017)</ref>.</p><p>Despite achieving certain effectiven 2016)</ref> calculates the pair-wise event relatedness score using a Siamese network. (ii) PairLSTM <ref type=\"bibr\" target=\"#b25\">(Wang et al., 2017)</ref> integrates event order information and pair m 20.00** EventComp <ref type=\"bibr\" target=\"#b7\">(Granroth and Clark, 2016)</ref> 49.57** PairLSTM <ref type=\"bibr\" target=\"#b25\">(Wang et al., 2017)</ref> 50.83** SGNN <ref type=\"bibr\" target=\"#b13\"  target=\"#b20\">(Pichotta and Mooney, 2016;</ref><ref type=\"bibr\" target=\"#b19\">Pichotta, 2016;</ref><ref type=\"bibr\" target=\"#b25\">Wang et al., 2017)</ref> for predicting the subsequent event. <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the modeling of event relation information. To this end, early work exploited event pair relations <ref type=\"bibr\" target=\"#b1\">(Chambers, 2008;</ref><ref type=\"bibr\" target=\"#b9\">Jans et al., 2012; relationships between abstract events. Previous studies propose to model the pair-wise relationship <ref type=\"bibr\" target=\"#b1\">(Chambers, 2008;</ref><ref type=\"bibr\" target=\"#b9\">Jans et al., 2012;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arn commonsense knowledge for un-derstanding the story plot and predicting the ending. To this end, <ref type=\"bibr\" target=\"#b12\">Li et al. (2018a)</ref> and <ref type=\"bibr\" target=\"#b8\">Guan (2019). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the modeling of event relation information. To this end, early work exploited event pair relations <ref type=\"bibr\" target=\"#b1\">(Chambers, 2008;</ref><ref type=\"bibr\" target=\"#b9\">Jans et al., 2012; relationships between abstract events. Previous studies propose to model the pair-wise relationship <ref type=\"bibr\" target=\"#b1\">(Chambers, 2008;</ref><ref type=\"bibr\" target=\"#b9\">Jans et al., 2012;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tuency grammar <ref type=\"bibr\" target=\"#b7\">(Ford and Fox, 2002)</ref>. Inspired by recent studies <ref type=\"bibr\" target=\"#b20\">(Mare\u010dek and Rosa, 2019;</ref><ref type=\"bibr\" target=\"#b13\">Kim et a ective Some phrases can be recognized by using the similarity of attention distributions over words <ref type=\"bibr\" target=\"#b20\">(Mare\u010dek and Rosa, 2019;</ref><ref type=\"bibr\" target=\"#b13\">Kim et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b5\">(Devlin et al., 2019)</ref> 91.3/-88.0/-90.0/-93.2 60.6 70.4 86.6 92.3 XLNet-large <ref type=\"bibr\" target=\"#b41\">(Yang et al., 2019)</ref> 92.3/-90.8/-92.5/-97.0 69.0 85.9 90.8 94.9 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: od Contrastive selfsupervised learning (CSSL) <ref type=\"bibr\" target=\"#b38\">(Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">He et al., 2020)</ref> is a learning paradigm which aims to capture th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the grammatical acceptability of a given sentence. We use three popular public datasets, i.e., CoLA <ref type=\"bibr\" target=\"#b36\">(Warstadt et al., 2019)</ref>, BLiMP <ref type=\"bibr\" target=\"#b35\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \"one prefix\" (1P) <ref type=\"bibr\" target=\"#b18\">(Linzen et al., 2016)</ref> and \"two prefix\" (2P) <ref type=\"bibr\" target=\"#b37\">(Wilcox et al., 2019)</ref>.</p><p>(continuous) of RoBERTa<ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s NER task as a machine reading comprehension task to handle both flat and nested NER tasks. KEPLER <ref type=\"bibr\" target=\"#b33\">(Wang et al., 2021)</ref> infuses knowledge into pre-trained models a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2020)</ref>. As it is widely acknowledged that structural information is very important for NLP <ref type=\"bibr\" target=\"#b28\">(Strubell et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Nguyen et e-trained LMs On the other hand, many works try to use syntax information to further improve models <ref type=\"bibr\" target=\"#b28\">(Strubell et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Nguyen et , 2020)</ref>.</p><p>Task oriented works attempt to inject syntactic knowledge into the transformer <ref type=\"bibr\" target=\"#b28\">(Strubell et al., 2018;</ref><ref type=\"bibr\" target=\"#b23\">Nguyen et </ref><ref type=\"bibr\" target=\"#b44\">Zhang et al., 2020)</ref>. In the semantic role labeling task, <ref type=\"bibr\" target=\"#b28\">Strubell et al. (2018)</ref> restrict each token to attend to its syn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e proportion of pairs whose acceptable sentence is assigned a higher probability. On FCE, following <ref type=\"bibr\" target=\"#b25\">Rei and S\u00f8gaard (2019)</ref>, we take it as a binary classification t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mples. The sim() function can be any similarity function, such as cosine, Jensen-Shannon Divergence <ref type=\"bibr\" target=\"#b6\">(Endres and Schindelin, 2003)</ref> and Hellinger distance <ref type=\" rmula_2\">(l, h) i + a (l, h) s )/2 (2)</formula><p>where JSD is short for Jensen-Shannon Divergence <ref type=\"bibr\" target=\"#b6\">(Endres and Schindelin, 2003)</ref>, and D KL for Kullback-Leibler Div. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dt et al., 2019)</ref>, BLiMP <ref type=\"bibr\" target=\"#b35\">(Warstadt et al., 2020)</ref>, and FCE <ref type=\"bibr\" target=\"#b42\">(Yannakoudakis et al., 2011)</ref>, to evaluate our models. For CoLA,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1\">(Beran, 1977)</ref>. \u03c4 called temperature coefficient is a hyperparameter used in recent methods <ref type=\"bibr\" target=\"#b12\">(Khosla et al., 2020;</ref><ref type=\"bibr\" target=\"#b43\">Yu et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Asai et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Karpukhin et al., 2020)</ref> and generative <ref type=\"bibr\" target=\"#b7\">(Izacard and Grave, 2021b;</ref><ref type=\"bibr\" target=\"#b12\">Lewis e \"table\" target=\"#tab_0\">1</ref>, the answer \"Dubai in Germany\" produced by the generative model FiD <ref type=\"bibr\" target=\"#b7\">(Izacard and Grave, 2021b)</ref> is factual incorrect and the answer \" , 2019)</ref> have become a common approach <ref type=\"bibr\" target=\"#b27\">(Yang et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Izacard and Grave, 2021b;</ref><ref type=\"bibr\" target=\"#b8\">Karpukhin eed the concatenation to the BART model <ref type=\"bibr\" target=\"#b10\">(Lewis et al., 2020a)</ref>. <ref type=\"bibr\" target=\"#b7\">Izacard and Grave (2021b)</ref> separately encodes the question with e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  words to the input text, the generated answer will be more faithful.</p><p>Inspired by the work of <ref type=\"bibr\" target=\"#b22\">See et al. (2017)</ref>, we enhance the generative model with a point ://www.tei-c.org/ns/1.0\"><head n=\"2.3\">Pointer-Generator Network</head><p>Pointer-Generator Network <ref type=\"bibr\" target=\"#b22\">(See et al., 2017)</ref> is an extension of the sequence-to-sequence  d in natural language tasks like summarization <ref type=\"bibr\" target=\"#b5\">(Gu et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">See al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Gehrmann et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b8\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Min et al., 2021b;</ref><ref type=\"bibr\" target=\"#b26\">Yamada et al., 2021)</ref>, followed by the reader identifying the an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: racy or inconsistent to the input. This problem has been addressed in tasks like text summarization <ref type=\"bibr\" target=\"#b15\">(Maynez et al., 2020)</ref> and machine translation <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: our model and other approaches on the test sets, evaluated with the standard exact match (EM) score <ref type=\"bibr\" target=\"#b20\">(Rajpurkar et al., 2016)</ref>. For a fair comparison, we retrained t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tively small number of support passages <ref type=\"bibr\" target=\"#b8\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Min et al., 2021b;</ref><ref type=\"bibr\" target=\"#b26\">Yamada et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and generative approaches. It has been frequently used in natural language tasks like summarization <ref type=\"bibr\" target=\"#b5\">(Gu et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">See al., 2017;</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: our model and other approaches on the test sets, evaluated with the standard exact match (EM) score <ref type=\"bibr\" target=\"#b20\">(Rajpurkar et al., 2016)</ref>. For a fair comparison, we retrained t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: decides the weights of generating words from vocabulary or copying from source passages.</p><p>work <ref type=\"bibr\" target=\"#b23\">(Vinyals et al., 2015)</ref>, that enables the model to directly copy  al., 2017)</ref> is an extension of the sequence-to-sequence model by integrating a copy mechanism <ref type=\"bibr\" target=\"#b23\">(Vinyals et al., 2015)</ref> into the generator. At each decoding sta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: decides the weights of generating words from vocabulary or copying from source passages.</p><p>work <ref type=\"bibr\" target=\"#b23\">(Vinyals et al., 2015)</ref>, that enables the model to directly copy  al., 2017)</ref> is an extension of the sequence-to-sequence model by integrating a copy mechanism <ref type=\"bibr\" target=\"#b23\">(Vinyals et al., 2015)</ref> into the generator. At each decoding sta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD <ref type=\"bibr\" target=\"#b6\">(Izacard and Grave, 2021a</ref>) is an extension of FiD model that inc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oduce new words out of the retrieved passages, and thus provide a more flexible modeling framework. <ref type=\"bibr\" target=\"#b18\">Min et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b12\">Lewis et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oduce new words out of the retrieved passages, and thus provide a more flexible modeling framework. <ref type=\"bibr\" target=\"#b18\">Min et al. (2020)</ref> and <ref type=\"bibr\" target=\"#b12\">Lewis et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hly precise answers to natural language questions from a large collection of unstructured text data <ref type=\"bibr\" target=\"#b24\">(Voorhees, 1999)</ref>. With the pioneering work of DrQA <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: our model and other approaches on the test sets, evaluated with the standard exact match (EM) score <ref type=\"bibr\" target=\"#b20\">(Rajpurkar et al., 2016)</ref>. For a fair comparison, we retrained t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2018)</ref> and neural machine translation <ref type=\"bibr\" target=\"#b14\">(Luong et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Gu et al., 2018)</ref>, but its application to ODQA has been less expl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2018)</ref> and neural machine translation <ref type=\"bibr\" target=\"#b14\">(Luong et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Gu et al., 2018)</ref>, but its application to ODQA has been less expl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD <ref type=\"bibr\" target=\"#b6\">(Izacard and Grave, 2021a</ref>) is an extension of FiD model that inc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dly categorized into two classes: extractive <ref type=\"bibr\" target=\"#b1\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Asai et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Karpukhin et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b8\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Min et al., 2021b;</ref><ref type=\"bibr\" target=\"#b26\">Yamada et al., 2021)</ref>, followed by the reader identifying the an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tage retriever-reader pipeline, that firstly retrieve a relatively small number of support passages <ref type=\"bibr\" target=\"#b8\">(Karpukhin et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Min et al =\"bibr\" target=\"#b1\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b0\">Asai et al., 2020;</ref><ref type=\"bibr\" target=\"#b8\">Karpukhin et al., 2020)</ref> and generative <ref type=\"bibr\" target=\" target=\"#b27\">(Yang et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Izacard and Grave, 2021b;</ref><ref type=\"bibr\" target=\"#b8\">Karpukhin et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c. on the FiD reader. We adopt the retriever results of FiD-KD, where a dense retriever similar to DPR <ref type=\"bibr\" target=\"#b8\">(Karpukhin et al., 2020)</ref> is used. A pointer network is integrate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ust exist if a few word embeddings are linearly dependent.</p><p>Recently, mixture of softmax (MoS) <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref> regains attention as one of the few effecti hile, <ref type=\"bibr\" target=\"#b19\">Parthiban et al. (2021)</ref> show that the softmax bottleneck <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref> theory is not sufficient to explain the imp a set of words with linearly dependent embeddings.</p><p>\u2022 Method: Addressing two weaknesses in MoS <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref>, we propose multi-facet softmax (MFS), a ne le embedding, we improve the state-of-the-art multiple embedding solution, mixture of softmax (MoS) <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref> by two enhancements: multiple input hidden  bibr\">Fan et al. (2020), and</ref><ref type=\"bibr\" target=\"#b29\">Tay et al. (2021)</ref>.</p><p>MoS <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref>: MoS (3) is the mixture of softmax with 3 f  tasks. Similar to GPT-2, MFS at least doubles Softmax + Multi-partition (S1I1P4) 5.8520 5.8656 MoS <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref>   The smaller improvement supports the conc  so it is not very 36.5 \u00b1 0.7 39.7 \u00b1 0.5 43.5 \u00b1 0.4 52.2 \u00b1 0.6 20.9 \u00b1 0.4 37.7 \u00b1 0.6 46.7 \u00b1 0.6 MoS <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref> (3) 36.6 \u00b1 0.8 40.2 \u00b1 0.6 43.2 \u00b1 0.6 52.1 \u00b1 ntal setup.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>G.1 Baselines</head><p>The MoS <ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref> and DOC <ref type=\"bibr\" target=\"#b28\">(Tak ote>1.5432 \u00b1 0.0003 34.1 \u00b1 0.8 35.2 \u00b1 0.5 37.8 \u00b1 0.4 45.0 \u00b1 0.5 18.3 \u00b1 0.4 30.7 \u00b1 0.5 38.5 \u00b1 0.6 MoS<ref type=\"bibr\" target=\"#b35\">(Yang et al., 2018)</ref> (3) 1.5407 \u00b1 0.0004 33.9 \u00b1 0.8 36.0 \u00b1 0.6 3 hat project the hidden state to the logit of the word x).<ref type=\"foot\" target=\"#foot_2\">2</ref>  <ref type=\"bibr\" target=\"#b35\">Yang et al. (2018)</ref> point out that the log probability distribut ssible next words.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" xml:id=\"foot_6\"><ref type=\"bibr\" target=\"#b35\">Yang et al. (2018)</ref> propose the concept of softmax bottleneck, w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g multiple facets could reach a similar performance by a smaller hidden state size.</p><p>Recently, <ref type=\"bibr\" target=\"#b1\">Gao et al. (2019a)</ref>; <ref type=\"bibr\" target=\"#b22\">Rajaee and Pi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s in Figure <ref type=\"figure\" target=\"#fig_1\">2</ref> (b) and Equation 3. The method is similar to <ref type=\"bibr\" target=\"#b30\">Tenney et al. (2019);</ref><ref type=\"bibr\">Fan et al. (2020), and</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g multiple facets could reach a similar performance by a smaller hidden state size.</p><p>Recently, <ref type=\"bibr\" target=\"#b1\">Gao et al. (2019a)</ref>; <ref type=\"bibr\" target=\"#b22\">Rajaee and Pi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b18\">(Narang et al., 2021;</ref><ref type=\"bibr\">Anonymous, 2021)</ref>. In the meanwhile, <ref type=\"bibr\" target=\"#b19\">Parthiban et al. (2021)</ref> show that the softmax bottleneck <ref t  the softmax layer without modeling the multimodal distribution might not always improve the models <ref type=\"bibr\" target=\"#b19\">(Parthiban et al., 2021)</ref>.</p><p>OpenWebText is mostly composed . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b18\">(Narang et al., 2021;</ref><ref type=\"bibr\">Anonymous, 2021)</ref>. In the meanwhile, <ref type=\"bibr\" target=\"#b19\">Parthiban et al. (2021)</ref> show that the softmax bottleneck <ref t  the softmax layer without modeling the multimodal distribution might not always improve the models <ref type=\"bibr\" target=\"#b19\">(Parthiban et al., 2021)</ref>.</p><p>OpenWebText is mostly composed . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onverts the hidden state h M ct to the facet embedding f ct,1 as in other methods.</p><p>SigSoftmax <ref type=\"bibr\" target=\"#b7\">(Kanai et al., 2018)</ref>: The same as Softmax except when predicting  target=\"#b7\">(Kanai et al., 2018)</ref>: The same as Softmax except when predicting the next word, <ref type=\"bibr\" target=\"#b7\">Kanai et al. (2018)</ref> add some non-linearity into the softmax laye  conditional probabilities. It also proposes MoS to break the softmax bottleneck in an RNN-based LM.<ref type=\"bibr\" target=\"#b7\">Kanai et al. (2018)</ref> andGanea et al. (2019)  add nonlinearities i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and the lemon, and my favorite is the [MASK]\". The nouns come from a hypernymy detection benchmark <ref type=\"bibr\" target=\"#b25\">(Shwartz et al., 2017)</ref> containing 25,498 noun pairs. The relati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and the lemon, and my favorite is the [MASK]\". The nouns come from a hypernymy detection benchmark <ref type=\"bibr\" target=\"#b25\">(Shwartz et al., 2017)</ref> containing 25,498 noun pairs. The relati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: different experts (i.e., feed-forward networks) and Zhang et al. ( <ref type=\"formula\">2022</ref>); <ref type=\"bibr\" target=\"#b17\">Mittal et al. (2022)</ref> use multiple embeddings in the attention l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b2\">Gao et al. (2019b)</ref> show that a mixture of kernel functions outperforms MoS. Mixtape <ref type=\"bibr\" target=\"#b36\">(Yang et al., 2019)</ref> is another efficient solution to the softma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ., 2018)</ref> used logistic normal to approximate Dirichlet. Another family of neural topic models <ref type=\"bibr\" target=\"#b23\">(Nan et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Wang et al., 2 et=\"#b31\">(Srivastava and Sutton, 2017;</ref><ref type=\"bibr\" target=\"#b20\">Miao et al., 2017;</ref><ref type=\"bibr\" target=\"#b23\">Nan et al., 2019)</ref>. The encoder takes a document's BoW x \u2208 R V a since \u1e91's distribution Q is not well defined. To this end, we follow a similar approach proposed in <ref type=\"bibr\" target=\"#b23\">(Nan et al., 2019)</ref> and further impose on \u1e91 a Dirichlet prior P  rnel <ref type=\"bibr\" target=\"#b17\">(Lebanon and Lafferty, 2003)</ref> in our experiments following <ref type=\"bibr\" target=\"#b23\">(Nan et al., 2019)</ref>.</p><p>The overall training objective is:</p </ref>, a VAE-based model that employs logistic normal prior for topic distributions.</p><p>\u2022 W-LDA <ref type=\"bibr\" target=\"#b23\">(Nan et al., 2019)</ref>. Our model follows W-LDA loss but differs in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b7\">(Das et al., 2015;</ref><ref type=\"bibr\" target=\"#b34\">Wang et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">Dieng et al., 2020)</ref>.</p><p>In recent years, pre-trained language tions into neural topic models. For example, <ref type=\"bibr\" target=\"#b6\">(Card et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Dieng et al., 2020)</ref> used PWEs to initialize word embeddings of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: capacity, neural networks have been widely used for topic modeling in recent years. Some approaches <ref type=\"bibr\" target=\"#b17\">(Kingma and Welling, 2013;</ref><ref type=\"bibr\" target=\"#b21\">Miao e \u1e91(i) } m i=1 are encoder outputs, and k is the kernel function that is information diffusion kernel <ref type=\"bibr\" target=\"#b17\">(Lebanon and Lafferty, 2003)</ref> in our experiments following <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cently, to avoid the complex and specific inference process of graph model-based method such as LDA <ref type=\"bibr\" target=\"#b4\">(Blei et al., 2003)</ref>, neural topic modeling that utilizes neural- heckpoint for each model variant.</p><p>We compare our models with following baselines:</p><p>\u2022 LDA <ref type=\"bibr\" target=\"#b4\">(Blei et al., 2003)</ref>, we used the implementation of GibbsLDA++<re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to the top, the first N layers have an identical structure. Each layer has four sub-layers: Dropout <ref type=\"bibr\" target=\"#b32\">(Srivastava et al., 2014)</ref>, Linear, BatchNorm <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  aggregate all topic scores as the final topic coherence. The used topic coherence measures are C_A <ref type=\"bibr\" target=\"#b0\">(Aletras and Stevenson, 2013)</ref>, C_P <ref type=\"bibr\" target=\"#b29 and Stevenson, 2013)</ref>, C_P <ref type=\"bibr\" target=\"#b29\">(R\u00f6der et al., 2015)</ref>, and NPMI <ref type=\"bibr\" target=\"#b0\">(Aletras and Stevenson, 2013)</ref> of top-10 topic words, implemented. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b8\">(Devlin et al., 2019)</ref> provide richer semantic than static ones like Word2Vec <ref type=\"bibr\" target=\"#b22\">(Mikolov et al., 2013)</ref> or GloVe <ref type=\"bibr\" target=\"#b25\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: years, pre-trained language models (PLMs) <ref type=\"bibr\" target=\"#b26\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b5\">Brown et al.,  ized vs. Static word embeddings</head><p>Contextualized word embeddings like those produced by BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> provide richer semantic than static ones l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: A more sophisticated approach is to distill the knowledge of a PLM into a topic model. For example, <ref type=\"bibr\" target=\"#b13\">(Hoyle et al., 2020)</ref> employed the probability estimates of a te l training time is dominated by PLM, and it will be worse if PLM is further fine-tuned, as shown in <ref type=\"bibr\" target=\"#b13\">(Hoyle et al., 2020)</ref>. Secondly, there is the gap of training ob /ref> treated PLM outputs as an additional knowledge source to enhance or replace BoW-based inputs. <ref type=\"bibr\" target=\"#b13\">(Hoyle et al., 2020)</ref> employed knowledge distillation to guide t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: c coherence measures are C_A <ref type=\"bibr\" target=\"#b0\">(Aletras and Stevenson, 2013)</ref>, C_P <ref type=\"bibr\" target=\"#b29\">(R\u00f6der et al., 2015)</ref>, and NPMI <ref type=\"bibr\" target=\"#b0\">(A br\" target=\"#b0\">(Aletras and Stevenson, 2013)</ref> of top-10 topic words, implemented in Palmetto <ref type=\"bibr\" target=\"#b29\">(R\u00f6der et al., 2015)</ref> <ref type=\"foot\" target=\"#foot_5\">6</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: loyed by many neural topic models <ref type=\"bibr\" target=\"#b31\">(Srivastava and Sutton, 2017;</ref><ref type=\"bibr\" target=\"#b20\">Miao et al., 2017;</ref><ref type=\"bibr\" target=\"#b23\">Nan et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd paraphrasing pipeline in the literature <ref type=\"bibr\" target=\"#b45\">(Wang et al., 2015a;</ref><ref type=\"bibr\" target=\"#b7\">Cao et al., 2020)</ref>, first synthesize large-scale (canonical quest. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e question types (e.g., verification, counting) and reasoning operations (e.g., intersect, union).  <ref type=\"bibr\" target=\"#b40\">(Sun et al., 2018)</ref>. The BART results on MetaQA 3-hop WebQSP res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b29\">Pasupat and Liang, 2015;</ref><ref type=\"bibr\">Wang et al., 2015b;</ref><ref type=\"bibr\" target=\"#b30\">Pasupat and Liang, 2016)</ref>, SQL <ref type=\"bibr\" target=\"#b47\">(Z. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b40\">(Sun et al., 2018)</ref>. The BART results on MetaQA 3-hop WebQSP results are from <ref type=\"bibr\" target=\"#b18\">(Huang et al., 2021)</ref>.</p></div> <div xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\">Ansari et al., 2019)</ref>, which parses a question to a symbolic logic form (e.g., \u03bbcalculus <ref type=\"bibr\" target=\"#b1\">(Artzi et al., 2013)</ref>, \u03bb-DCS <ref type=\"bibr\" target=\"#b26\">(Lian. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ei-c.org/ns/1.0\"><head n=\"4.1\">Knowledge Base Extraction</head><p>We took the entities of FB15k-237 <ref type=\"bibr\" target=\"#b43\">(Toutanova et al., 2015)</ref> as seeds, and aligned them with Wikida /ns/1.0\"><head>C Knowledge Base Extraction</head><p>Specifically, we took the entities of FB15k-237 <ref type=\"bibr\" target=\"#b43\">(Toutanova et al., 2015)</ref>, a popular subset of Freebase, as seed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\">Ansari et al., 2019)</ref>, which parses a question to a symbolic logic form (e.g., \u03bbcalculus <ref type=\"bibr\" target=\"#b1\">(Artzi et al., 2013)</ref>, \u03bb-DCS <ref type=\"bibr\" target=\"#b26\">(Lian. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b29\">Pasupat and Liang, 2015;</ref><ref type=\"bibr\">Wang et al., 2015b;</ref><ref type=\"bibr\" target=\"#b30\">Pasupat and Liang, 2016)</ref>, SQL <ref type=\"bibr\" target=\"#b47\">(Z. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e question types (e.g., verification, counting) and reasoning operations (e.g., intersect, union).  <ref type=\"bibr\" target=\"#b40\">(Sun et al., 2018)</ref>. The BART results on MetaQA 3-hop WebQSP res. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b11\">(Dong and Lapata, 2016)</ref>, and the pretrained generative language model-BART <ref type=\"bibr\" target=\"#b24\">(Lewis et al., 2020)</ref>, as our SPARQL and KoPL parsers.</p><p>For. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ument quality rating of at least 0.5 from the 30,497 arguments of the IBM-ArgQ-Rank-30kArgs dataset <ref type=\"bibr\" target=\"#b18\">(Gretz et al., 2020)</ref>. For the dataset, crowdworkers wrote one p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref> looked at 14 values for counseling and therapy, such as responsibility and spirituality, and <ref type=\"bibr\" target=\"#b22\">Kahle et al. (1988)</ref> at nine for consumer research, such as warm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  by state-of-the-art language-based approaches. Maybe hierarchical classification approaches (e.g., <ref type=\"bibr\" target=\"#b3\">Babbar et al., 2013)</ref> can address this comparably weak performanc t relationships between labels. Hierarchical classification approaches appear promising here (e.g., <ref type=\"bibr\" target=\"#b3\">Babbar et al., 2013)</ref>; learning rules for multi-label classificat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d manually to analyze interactions with reasoning and persuasion subject to a specific value system <ref type=\"bibr\" target=\"#b2\">(Atkinson and Bench-Capon, 2021)</ref>. This paper presents a first st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: towards the large-scale automatic application of these works as it takes values to argument mining. <ref type=\"bibr\" target=\"#b16\">Feldman (2021)</ref> recently showed the strong connection between va. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mmon value systems are used for evaluation. Framing has often been studied computationally for news <ref type=\"bibr\" target=\"#b29\">(Naderi and Hirst, 2015;</ref><ref type=\"bibr\" target=\"#b9\">Chen et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mmon value systems are used for evaluation. Framing has often been studied computationally for news <ref type=\"bibr\" target=\"#b29\">(Naderi and Hirst, 2015;</ref><ref type=\"bibr\" target=\"#b9\">Chen et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f the annotation task, the crowdworker annotators reached an average value-wise agreement \u03b1 of 0.49 <ref type=\"bibr\" target=\"#b25\">(Krippendorff, 2004)</ref>. We found most disagreement arose from the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: also for political speech <ref type=\"bibr\" target=\"#b11\">(De Vreese, 2005)</ref>, and argumentation <ref type=\"bibr\" target=\"#b0\">(Ajjour et al., 2019)</ref>. In the latter, some values are so prevale. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. <ref type=\"bibr\" target=\"#b1\">Alshomary and Wachsmuth (2021)</ref> hypothesized that the foundations. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b6\">Choi et al., 2018;</ref><ref type=\"bibr\" target=\"#b42\">Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Lin and Ji, 2019;</ref><ref type=\"bibr\" target=\"#b0\">Abhishek et al., core distributions over the type set Y for all the mentions in the dataset from an extra base model <ref type=\"bibr\" target=\"#b20\">(Lin and Ji, 2019)</ref> trained on the same dataset. Then the siblin ance. To select sibling mentions, we first derive the prior typing distribution from the base model <ref type=\"bibr\" target=\"#b20\">(Lin and Ji, 2019)</ref> as described in Sec 3.2. During experiments, /head><formula xml:id=\"formula_10\">i ; \u03b8 M )</formula><p>The mention encoder uses the backbone from <ref type=\"bibr\" target=\"#b20\">Lin and Ji (2019)</ref>. Given a mention, we first encode the mention resented as follows:</p><p>(1) We select sibling mentions according to the typing distribution from <ref type=\"bibr\" target=\"#b20\">Lin and Ji (2019)</ref>. We observe that, after aggregating sibling i e augmented OntoNotes of the same size, our model increases the accuracy score by more than 5% over <ref type=\"bibr\" target=\"#b20\">Lin and Ji (2019)</ref> . Compared with Lin and Ji (2019) * which uti  on the whole mentions and hard mentions of the original OntoNotes. Base denotes the base model from<ref type=\"bibr\" target=\"#b20\">Lin and Ji (2019)</ref>. Ep is short for Entropy.</figDesc><table><ro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b1\">Ali et al. (2020)</ref> refines noisy representations by corpus-level contextual clues. <ref type=\"bibr\" target=\"#b25\">Onoe and Durrett (2019)</ref> introduces two additional models to del. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b35\">Xin et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Choi et al., 2018;</ref><ref type=\"bibr\" target=\"#b42\">Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Lin and Ji, 2 bosa, 2018)</ref>; (4) NEURAL <ref type=\"bibr\" target=\"#b31\">(Shimaoka et al., 2017)</ref>; (5) ACT <ref type=\"bibr\" target=\"#b42\">(Zhang et al., 2018)</ref>; (6) Lin and Ji ( <ref type=\"formula\">2019. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b13\">(Jiang et al., 2020c;</ref><ref type=\"bibr\" target=\"#b40\">Zhang et al., 2020b;</ref><ref type=\"bibr\" target=\"#b23\">Liu et al., 2021b)</ref>.</p><p>It is challenging to learn effective . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b0\">Abhishek et al., 2017;</ref><ref type=\"bibr\" target=\"#b38\">Xu and Barbosa, 2018;</ref><ref type=\"bibr\" target=\"#b1\">Ali et al., 2020;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2021  target=\"#b0\">Abhishek et al., 2017;</ref><ref type=\"bibr\" target=\"#b38\">Xu and Barbosa, 2018;</ref><ref type=\"bibr\" target=\"#b1\">Ali et al., 2020)</ref> is to design loss functions for the clean and  ef> groups mentions of the same type into a compact cluster to improve the robustness of the model. <ref type=\"bibr\" target=\"#b1\">Ali et al. (2020)</ref> refines noisy representations by corpus-level . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b21\">(Ling and Weld, 2012;</ref><ref type=\"bibr\" target=\"#b29\">Ren et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2019)</ref>, we report the performance in terms of strict -truth label distribution of each sample, which treats the noisy and clean data uniformly. Besides, <ref type=\"bibr\" target=\"#b2\">Chen et al. (2019)</ref> groups mentions of the same type into a compa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b38\">Xu and Barbosa, 2018;</ref><ref type=\"bibr\" target=\"#b36\">Xiong et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2020)</ref>. As a comparison, our model also considers th pe=\"bibr\" target=\"#b36\">Xiong et al. (2019)</ref> extends GCNs to a vast number of free-form types. <ref type=\"bibr\" target=\"#b3\">Chen et al. (2020)</ref> designs a multi-level learning-to-rank loss t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b13\">(Jiang et al., 2020c;</ref><ref type=\"bibr\" target=\"#b40\">Zhang et al., 2020b;</ref><ref type=\"bibr\" target=\"#b23\">Liu et al., 2021b)</ref>.</p><p>It is challenging to learn effective . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 017)</ref>;</p><p>(3) NFETC <ref type=\"bibr\" target=\"#b38\">(Xu and Barbosa, 2018)</ref>; (4) NEURAL <ref type=\"bibr\" target=\"#b31\">(Shimaoka et al., 2017)</ref>; (5) ACT <ref type=\"bibr\" target=\"#b42\"  2021)</ref>.</p><p>Modeling the type hierarchy is another important topic in FGET. Prior solutions <ref type=\"bibr\" target=\"#b31\">(Shimaoka et al., 2017)</ref> introduce a one-hot matrix to encode th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: opment set and the optimal settings are presented in Appendix A.</p><p>Following the previous works <ref type=\"bibr\" target=\"#b21\">(Ling and Weld, 2012;</ref><ref type=\"bibr\" target=\"#b29\">Ren et al., ng et al., 2020a;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al., 2021a)</ref>. Early works in FGET <ref type=\"bibr\" target=\"#b21\">(Ling and Weld, 2012;</ref><ref type=\"bibr\" target=\"#b30\">Shimaoka et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve been make in improving its performance <ref type=\"bibr\" target=\"#b39\">(Zhang et al., 2020a;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al., 2021a)</ref>. Early works in FGET <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aining data but further improve by choosing <ref type=\"bibr\" target=\"#b30\">(Ren et al., 2016a;</ref><ref type=\"bibr\" target=\"#b39\">Xu and Barbosa, 2018)</ref>, weighting <ref type=\"bibr\" target=\"#b37\" and Weld, 2012)</ref>. Various features <ref type=\"bibr\" target=\"#b40\">(Yogatama et al., 2015;</ref><ref type=\"bibr\" target=\"#b39\">Xu and Barbosa, 2018)</ref>, network structures <ref type=\"bibr\" targ 3\">Backbone</head><p>For fair comparison, the backbone of our model has the same structure as NFETC <ref type=\"bibr\" target=\"#b39\">(Xu and Barbosa, 2018)</ref>.</p><p>For an instance (m, c, y), for ea -c.org/ns/1.0\"><head n=\"3.4\">Implementation Details</head><p>To make an equal comparison, following <ref type=\"bibr\" target=\"#b39\">(Xu and Barbosa, 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al. methods are proposed. <ref type=\"bibr\">Ren et al. (2016a,b)</ref> utilized partial-label embedding. <ref type=\"bibr\" target=\"#b39\">Xu and Barbosa (2018)</ref> modified hierarchical loss to cope with o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ile, typical anti-noise machine learning <ref type=\"bibr\" target=\"#b26\">(Patrini et al., 2017;</ref><ref type=\"bibr\" target=\"#b12\">Hendrycks et al., 2018)</ref> uses instance-agnostic global statistic e a novel framework FCLC for noisy label learning inspired by weighted training and loss correction <ref type=\"bibr\" target=\"#b12\">(Hendrycks et al., 2018)</ref> in machine learning. Our method utiliz cation relies on the assumption that the label noise is independent from instances, i.e. \u1ef9 \u22a5 x | y. <ref type=\"bibr\" target=\"#b12\">Hendrycks et al. (2018)</ref> proposed to estimate T with a small set Shi et al., 2020)</ref>. Among them, <ref type=\"bibr\" target=\"#b26\">Patrini et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b12\">Hendrycks et al. (2018)</ref> proposed forward loss correction. It av. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b3\">(Chen et al., 2020)</ref> are often used, added by relations among instances and labels <ref type=\"bibr\" target=\"#b0\">(Ali et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Li et al., 2021 ett (2019b)</ref>. Apart from the above, <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2019) and</ref><ref type=\"bibr\" target=\"#b0\">(Ali et al., 2020)</ref> are the closest to our proposed method. They . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Fine-Grained Entity Typing</head><p>FET is studied based on the distant supervision training data <ref type=\"bibr\" target=\"#b21\">(Mintz et al., 2009;</ref><ref type=\"bibr\" target=\"#b18\">Ling and Wel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: The resource include external knowledge base <ref type=\"bibr\" target=\"#b38\">(Xin et al., 2018;</ref><ref type=\"bibr\" target=\"#b6\">Dai et al., 2019)</ref>, and with BERT-like pipeline <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b3\">(Chen et al., 2020)</ref> are often used, added by relations among instances and labels <ref type=\"bibr\" target=\"#b0\">(Ali et al., 2020;</ref><ref type=\"bibr\" target=\"#b16\">Li et al., 2021 ett (2019b)</ref>. Apart from the above, <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2019) and</ref><ref type=\"bibr\" target=\"#b0\">(Ali et al., 2020)</ref> are the closest to our proposed method. They . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: result, they ought to cope with instance-agnostic noise better. The previous works expirically show <ref type=\"bibr\" target=\"#b42\">(Zheng and Yang, 2021)</ref> that the prediction distribution is more. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bosa, 2018)</ref>, weighting <ref type=\"bibr\" target=\"#b37\">(Wu et al., 2019)</ref>, and relabeling <ref type=\"bibr\" target=\"#b41\">(Zhang et al., 2020)</ref> noisy labels using the prediction distribu br\" target=\"#b39\">(Xu and Barbosa, 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b41\">Zhang et al., 2020)</ref>, we use exactly the same pre-trained 300dim t=\"#b39\">Xu and Barbosa (2018)</ref> modified hierarchical loss to cope with overly-specific noise. <ref type=\"bibr\" target=\"#b41\">Zhang et al. (2020)</ref> automatically generated pseudo-truth label . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (x,y)\u2208A ik p(\u1ef9 j = 1|x) = C ijk (7)</formula><p>Forward Loss Correction Cross-entropy is composite <ref type=\"bibr\" target=\"#b29\">(Reid and Williamson, 2010)</ref>,denote it as \u2113 \u03c8 , its inverse link  requires that the backbone model is sufficiently expressive and uses an appropriate composite loss <ref type=\"bibr\" target=\"#b29\">(Reid and Williamson, 2010)</ref>. Thus, it is pluggable to a large n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntity linking <ref type=\"bibr\" target=\"#b28\">(Raiman and Raiman, 2018)</ref> and question answering <ref type=\"bibr\" target=\"#b8\">(Dong et al., 2015)</ref>. FET task has a more wide range of entity ty a et al., 2015;</ref><ref type=\"bibr\" target=\"#b39\">Xu and Barbosa, 2018)</ref>, network structures <ref type=\"bibr\" target=\"#b8\">(Dong et al., 2015;</ref><ref type=\"bibr\" target=\"#b33\">Shimaoka et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (x,y)\u2208A ik p(\u1ef9 j = 1|x) = C ijk (7)</formula><p>Forward Loss Correction Cross-entropy is composite <ref type=\"bibr\" target=\"#b29\">(Reid and Williamson, 2010)</ref>,denote it as \u2113 \u03c8 , its inverse link  requires that the backbone model is sufficiently expressive and uses an appropriate composite loss <ref type=\"bibr\" target=\"#b29\">(Reid and Williamson, 2010)</ref>. Thus, it is pluggable to a large n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2021)</ref>are explored to refine the mention and type representation. Label inter-dependency <ref type=\"bibr\" target=\"#b17\">(Lin and Ji, 2019)</ref> and type hierarchy <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: me train/dev/test split with previous works <ref type=\"bibr\" target=\"#b30\">(Ren et al., 2016a;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2019)</ref>. Detailed statistics of the three datasets ar p>To make an equal comparison, following <ref type=\"bibr\" target=\"#b39\">(Xu and Barbosa, 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al., 2019;</ref><ref type=\"bibr\" target=\"#b41\">Zhang et al., 2 e, followed by <ref type=\"bibr\" target=\"#b24\">Onoe and Durrett (2019b)</ref>. Apart from the above, <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2019) and</ref><ref type=\"bibr\" target=\"#b0\">(Ali et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  methods are mostly restricted to the  noise that is conditionally independent of the data features <ref type=\"bibr\" target=\"#b9\">(Fr\u00e9nay and Verleysen, 2014)</ref>. However, in real-world application. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ET serves as an important component in many down-stream NLP applications, e.g., relation extraction <ref type=\"bibr\" target=\"#b20\">(Liu et al., 2014)</ref>, entity linking <ref type=\"bibr\" target=\"#b2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ectures by adding noise adaptation layers <ref type=\"bibr\" target=\"#b4\">(Chen and Gupta, 2015;</ref><ref type=\"bibr\" target=\"#b11\">Goldberger and Ben-Reuven, 2017)</ref>, selecting samples <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  isolatedly and only use instance-level information. Meanwhile, typical anti-noise machine learning <ref type=\"bibr\" target=\"#b26\">(Patrini et al., 2017;</ref><ref type=\"bibr\" target=\"#b12\">Hendrycks  g/ns/1.0\"><head n=\"2.5\">Loss Correction</head><p>The idea of forward loss correction is proposed by <ref type=\"bibr\" target=\"#b26\">Patrini et al. (2017)</ref>. The basic idea is to modify the loss wit C(with reinit): our proposed model with fresh parameters before the start of step 3 as suggested by <ref type=\"bibr\" target=\"#b26\">Patrini et al. (2017)</ref>. ( <ref type=\"formula\" target=\"#formula_1 ing noiserobust regularization <ref type=\"bibr\" target=\"#b32\">(Shi et al., 2020)</ref>. Among them, <ref type=\"bibr\" target=\"#b26\">Patrini et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b12\">Hendry ) = \u2113 \u03c8 (T \u22a4 k s(x))<label>(9)</label></formula><p>The property holds on each cluster similar as in <ref type=\"bibr\" target=\"#b26\">(Patrini et al., 2017)</ref>, with all x \u2208 Ck , training with noisy l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: intz et al., 2009;</ref><ref type=\"bibr\" target=\"#b18\">Ling and Weld, 2012)</ref>. Various features <ref type=\"bibr\" target=\"#b40\">(Yogatama et al., 2015;</ref><ref type=\"bibr\" target=\"#b39\">Xu and Ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and Barbosa, 2018)</ref>, network structures <ref type=\"bibr\" target=\"#b8\">(Dong et al., 2015;</ref><ref type=\"bibr\" target=\"#b33\">Shimaoka et al., 2016)</ref>, and feature space <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2021)</ref>are explored to refine the mention and type representation. Label inter-dependency <ref type=\"bibr\" target=\"#b17\">(Lin and Ji, 2019)</ref> and type hierarchy <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vel diffusion probabilistic model <ref type=\"bibr\" target=\"#b49\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ho et al., 2020)</ref> called Diffuser, visualized in Figure <ref typ <p>Diffusion probabilistic models <ref type=\"bibr\" target=\"#b49\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ho et al., 2020)</ref> pose the data-generating process as an iterati  an iterative denoising procedure <ref type=\"bibr\" target=\"#b49\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ho et al., 2020)</ref>. The denoising procedure can be seen as parame (\u03c4 i , i) of the trajectory denoising process, from which the mean \u00b5 \u03b8 can be solved in closed form <ref type=\"bibr\" target=\"#b23\">(Ho et al., 2020)</ref>. We use the simplified objective for training. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ons unseen during training. While diffusion models have been developed for the generation of images <ref type=\"bibr\" target=\"#b50\">(Song et al., 2021)</ref>, waveforms <ref type=\"bibr\" target=\"#b7\">(C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t al., 2020a)</ref>, normalizing flows <ref type=\"bibr\" target=\"#b46\">(Rhinehart et al., 2020;</ref><ref type=\"bibr\" target=\"#b25\">Janner et al., 2020)</ref>, generative ). These investigations genera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al., 2021)</ref>, waveforms <ref type=\"bibr\" target=\"#b7\">(Chen et al., 2021c)</ref>, 3D shapes <ref type=\"bibr\" target=\"#b61\">(Zhou et al., 2021)</ref>, and text <ref type=\"bibr\" target=\"#b2\">(Au. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oblem. Afterwards, the learned model may be plugged into classical trajectory optimization routines <ref type=\"bibr\" target=\"#b55\">(Tassa et al., 2012;</ref><ref type=\"bibr\" target=\"#b45\">Posa et al., nthesis using trajectory optimization <ref type=\"bibr\" target=\"#b59\">(Witkin &amp; Kass, 1988;</ref><ref type=\"bibr\" target=\"#b55\">Tassa et al., 2012)</ref>. In this section, we provide a brief backgr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ans generated by this procedure often look more like adversarial examples than optimal trajectories <ref type=\"bibr\" target=\"#b53\">(Talvitie, 2014;</ref><ref type=\"bibr\" target=\"#b28\">Ke et al., 2018). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e latent-space model for reward prediction <ref type=\"bibr\" target=\"#b54\">(Tamar et al., 2016;</ref><ref type=\"bibr\" target=\"#b42\">Oh et al., 2017;</ref><ref type=\"bibr\" target=\"#b48\">Schrittwieser et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rds flexible conditioning <ref type=\"bibr\">(Dhariwal &amp; Nichol, 2021)</ref> and compositionality <ref type=\"bibr\" target=\"#b12\">(Du et al., 2020b)</ref>, which we use to recover effective behaviors. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arrangement. We train all methods on 10000 trajectories from demonstrations generated by PDDLStream <ref type=\"bibr\" target=\"#b17\">(Garrett et al., 2020)</ref>; rewards are equal to one upon successfu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ning algorithms.</p><p>We instantiate this idea as a trajectory-level diffusion probabilistic model <ref type=\"bibr\" target=\"#b49\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ho  -c.org/ns/1.0\"><head n=\"2.2\">Diffusion Probabilistic Models</head><p>Diffusion probabilistic models <ref type=\"bibr\" target=\"#b49\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ho  of generative model that formulates the data-generating process as an iterative denoising procedure <ref type=\"bibr\" target=\"#b49\">(Sohl-Dickstein et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Ho  ) is sufficiently smooth, the reverse diffusion process transitions can be approximated as Gaussian <ref type=\"bibr\" target=\"#b49\">(Sohl-Dickstein et al., 2015)</ref>:</p><formula xml:id=\"formula_10\"> nting problem, in which state and action constraints act analogously to observed pixels in an image <ref type=\"bibr\" target=\"#b49\">(Sohl-Dickstein et al., 2015)</ref>. All unobserved locations in the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .0\"><head>Code References</head><p>We used the following open-source libraries for this work: NumPy <ref type=\"bibr\" target=\"#b22\">(Harris et al., 2020)</ref>, PyTorch <ref type=\"bibr\" target=\"#b44\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 018)</ref>, vectorquantized autoencoders <ref type=\"bibr\" target=\"#b21\">(Hafner et al., 2021b;</ref><ref type=\"bibr\" target=\"#b43\">Ozair et al., 2021)</ref>, neural ODEs <ref type=\"bibr\" target=\"#b10\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is a substantial performance drop of the best model-free algorithm in the single-task setting (IQL; <ref type=\"bibr\" target=\"#b32\">Kostrikov et al. 2022)</ref> when adapted to the multi-task setting.  einforcement learning algorithms CQL <ref type=\"bibr\" target=\"#b33\">(Kumar et al., 2020)</ref>, IQL <ref type=\"bibr\" target=\"#b32\">(Kostrikov et al., 2022)</ref>, and AWAC <ref type=\"bibr\" target=\"#b3 s per model). We detail the sources for the performance of prior methods in Appendix A.3. Following <ref type=\"bibr\" target=\"#b32\">Kostrikov et al. (2022)</ref>, we emphasize in bold scores within 5 p comotion</head><p>The scores for BC, CQL, IQL, and AWAC are from Table <ref type=\"table\">1</ref> in <ref type=\"bibr\" target=\"#b32\">Kostrikov et al. (2022)</ref>. The scores for DT are from </p><p>, wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: adversarial examples than optimal trajectories <ref type=\"bibr\" target=\"#b53\">(Talvitie, 2014;</ref><ref type=\"bibr\" target=\"#b28\">Ke et al., 2018)</ref>. As a result, contemporary modelbased reinforc -networks <ref type=\"bibr\" target=\"#b27\">(Kaiser et al., 2020)</ref>, stochastic recurrent networks <ref type=\"bibr\" target=\"#b28\">(Ke et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Hafner et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b48\">Schrittwieser et al., 2019)</ref>; weighing model training objectives by state values <ref type=\"bibr\" target=\"#b14\">(Farahmand et al., 2017)</ref>; and applying collocation techniques t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ine planning tend to use simple gradient-free trajectory optimization routines like random shooting <ref type=\"bibr\" target=\"#b38\">(Nagabandi et al., 2018)</ref> or the cross-entropy method <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ans generated by this procedure often look more like adversarial examples than optimal trajectories <ref type=\"bibr\" target=\"#b53\">(Talvitie, 2014;</ref><ref type=\"bibr\" target=\"#b28\">Ke et al., 2018). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oblem. Afterwards, the learned model may be plugged into classical trajectory optimization routines <ref type=\"bibr\" target=\"#b55\">(Tassa et al., 2012;</ref><ref type=\"bibr\" target=\"#b45\">Posa et al., nthesis using trajectory optimization <ref type=\"bibr\" target=\"#b59\">(Witkin &amp; Kass, 1988;</ref><ref type=\"bibr\" target=\"#b55\">Tassa et al., 2012)</ref>. In this section, we provide a brief backgr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  shooting <ref type=\"bibr\" target=\"#b38\">(Nagabandi et al., 2018)</ref> or the cross-entropy method <ref type=\"bibr\" target=\"#b4\">(Botev et al., 2013;</ref><ref type=\"bibr\" target=\"#b8\">Chua et al., 2 ent dynamics; once learning is complete the model may be inserted into any of a variety of planning <ref type=\"bibr\" target=\"#b4\">(Botev et al., 2013;</ref><ref type=\"bibr\" target=\"#b58\">Williams et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng, with multiple lines of work exploring dynamics models parameterized as convolutional U-networks <ref type=\"bibr\" target=\"#b27\">(Kaiser et al., 2020)</ref>, stochastic recurrent networks <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2013;</ref><ref type=\"bibr\" target=\"#b58\">Williams et al., 2015)</ref> or policy optimization <ref type=\"bibr\" target=\"#b52\">(Sutton, 1990;</ref><ref type=\"bibr\" target=\"#b57\">Wang et al., 2019). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: classical trajectory optimization routines <ref type=\"bibr\" target=\"#b55\">(Tassa et al., 2012;</ref><ref type=\"bibr\" target=\"#b45\">Posa et al., 2014;</ref><ref type=\"bibr\" target=\"#b29\">Kelly, 2017)</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: at combines the power of transformer language models (LMs) <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref> with high-fidelity diffusion models <ref type=\"bibr\" target bibr\" target=\"#b40\">41]</ref>, the key finding behind Imagen is that text embeddings from large LMs <ref type=\"bibr\" target=\"#b51\">[52,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, pretrained on tex ef type=\"figure\" target=\"#fig_0\">1</ref> for select samples.</p><p>Imagen comprises a frozen T5-XXL <ref type=\"bibr\" target=\"#b51\">[52]</ref> encoder to map input text into a sequence of embeddings an \"#b46\">[47,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, T5 <ref type=\"bibr\" target=\"#b51\">[52]</ref>) have led to leaps in textual understanding and generative e several families of pre-trained text encoders: BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref>, T5 <ref type=\"bibr\" target=\"#b51\">[52]</ref>, and CLIP <ref type=\"bibr\" target=\"#b48\">[49]</ref>.</p><p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef introduction to diffusion models; a precise description is in Appendix A.</p><p>Diffusion models <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uction</head><p>Multimodal learning has come into prominence recently, with text-to-image synthesis <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta e=\"bibr\" target=\"#b81\">[82,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and the zero-shot setting <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. The key automated 39 Imagen (our work) 7.27 We evaluate Imagen on the COCO validation set using FID score, similar to <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Table <ref type=\"  target=\"#b75\">[76,</ref><ref type=\"bibr\" target=\"#b80\">81]</ref>, VQ-VAE Transformer-based methods <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, and diffusion mod nerating samples with strong image-text alignment, this is also consistent with the observations of <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. There is typicall image synthesis. In contrast to prior work that uses only image-text data for model training [e.g., <ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, the key finding be ent text-to-image models; they can be trained from scratch <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref> or pretrained on image-text data <ref type=\"bibr\" target=\"# ons, rare words, and also misspelled prompts. We also include sets of prompts collected from DALL-E <ref type=\"bibr\" target=\"#b52\">[53]</ref>, Gary Marcus et al. <ref type=\"bibr\" target=\"#b37\">[38]</r www.tei-c.org/ns/1.0\"><head n=\"4.2\">Results on COCO</head><p>Not trained on COCO (zero-shot) DALL-E <ref type=\"bibr\" target=\"#b52\">[53]</ref> 17.89 LAFITE <ref type=\"bibr\" target=\"#b81\">[82]</ref> 26. y results in better image-text Subset of challenging prompts \"A triangular purple flower pot.\" from <ref type=\"bibr\" target=\"#b52\">[53]</ref>.</p><p>\"A cross-section view of a brain.\" Description Abil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b19\">[20]</ref>, and raise many concerns regarding social and cultural exclusion and bias <ref type=\"bibr\" target=\"#b66\">[67,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  task. Imagen explores pretrained text encoders: BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref>, T5 <ref type=\"bibr\" target=\"#b50\">[51]</ref> and CLIP <ref type=\"bibr\" target=\"#b45\">[46]</ref>. For si. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: been extensive work auditing image-to-text and image labeling models for forms of social bias (e.g. <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target ion at resolutions <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> as well as attention pooled text embedding.</p><p>Optimizer:  attn_resolutions\": <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, \"channel_mult\": [1, 2, 3, 4], \"dropout\": 0, \"embed_dim\": 512 t_cross_attn_res\": <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, \"feature_pooling_type\": \"attention\", \"use_scale_shift_norm\":. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ll diffusion models are conditioned on the text embedding sequence and use classifier-free guidance <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Imagen relies on new sampling techniques to allow usage o n models using gradients from a pretrained model p(c|z t ) during sampling. Classifierfree guidance <ref type=\"bibr\" target=\"#b26\">[27]</ref> is an alternative technique that avoids this pretrained mo es image-text alignment, but damages image fidelity producing highly saturated and unnatural images <ref type=\"bibr\" target=\"#b26\">[27]</ref>. We find that this is due to a train-test mismatch arising ree Guidance and the Alignment-Fidelity Trade-off</head><p>We observe that classifier-free guidance <ref type=\"bibr\" target=\"#b26\">[27]</ref> is a key contributor to generating samples with strong ima. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s the social biases and limitations of large language models <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>.</p><p>While we leave. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  concerns regarding social and cultural exclusion and bias <ref type=\"bibr\" target=\"#b66\">[67,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref>. These consideratio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ound on the data log likelihood under the diffusion model, or as a form of denoising score matching <ref type=\"bibr\" target=\"#b71\">[72,</ref><ref type=\"bibr\" target=\"#b64\">65,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (FDIP) <ref type=\"bibr\" target=\"#b8\">[9]</ref> and extends it with unified prefetching into the BTB <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The scheme, called Boomerang, discovers BTB misses on the k has addressed this limitation by adding a BTB prefetch capability in a technique called Boomerang <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Boomerang uses a basic-block-oriented BTB to detect BTB m guide local control flow tend to have very short displacements, typically within a few cache blocks <ref type=\"bibr\" target=\"#b14\">[15]</ref>, as shown by dashed arrows in Figure <ref type=\"figure\" ta filling the BTBs, Shotgun takes a hybrid approach by incorporating the features from both Boomerang <ref type=\"bibr\" target=\"#b14\">[15]</ref> and Confluence <ref type=\"bibr\" target=\"#b13\">[14]</ref>.  depending on branch type. The rest of the predecoded branches are stored in the BTB Prefetch Buffer <ref type=\"bibr\" target=\"#b14\">[15]</ref>. On a hit to the BTB Prefetch Buffer, the accessed branch  d the associated system software support, making it an expensive proposition as shown in prior work <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The LLC tag array extension, for storing index table, cos n contrast, incur about 18% and 14% more front-end stalls than FDIP, as also observed by prior work <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Though Boomerang builds on FDIP, the latter provides bett dditional coverage. Confluence performs poorly on these applications, as also noted by Kumar et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref>, owing to frequent LLC accesses for loading history metada TB. In doing so, it eliminates the storage overhead of temporal stream prefetching.</p><p>Boomerang <ref type=\"bibr\" target=\"#b14\">[15]</ref> relies on FDIP for L1-I prefetching and extends it with BT ated prefetchers for L1-I and BTB prefetching, recent work <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref> proposes to unify t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hem to prefetch the necessary state. The general concept has been applied to both instruction cache <ref type=\"bibr\" target=\"#b15\">[16]</ref> and BTB <ref type=\"bibr\" target=\"#b11\">[12]</ref> prefetch locks accessed inside a region mostly stay stable over executions, thus corroborating prior results <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The remaining 12% of stalls come from the uninitialized s g and replays the log for prefetching during the next executions of the same control flow path. PIF <ref type=\"bibr\" target=\"#b15\">[16]</ref> observes that recording the entire retire-order L1-I acces  of kilobytes per core) for capturing control flow history <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. To mitigate the cost, two complementary techniques have be target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>.</p><p>FDIP <ref type=\"bibr\" target=\"#b8\">[9]</ref> is the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ze of the basic block containing the branch (like Boomerang, Shotgun uses a basic-blockoriented BTB <ref type=\"bibr\" target=\"#b16\">[17]</ref>). <ref type=\"foot\" target=\"#foot_1\">1</ref>Type: the type . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: software mechanisms to mitigate this bottleneck. On the hardware side, simple next-line prefetchers <ref type=\"bibr\" target=\"#b20\">[21]</ref> are widely deployed in commercial processors. These prefet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  categorized as either branch-predictor-directed prefetchers <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Simulation Infrastructure</head><p>We use Flexus <ref type=\"bibr\" target=\"#b17\">[18]</ref>, a full system multiprocessor simulator, to evaluate Shotg ) to measure performance. This metric has been shown to be an accurate measure of server throughput <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our modeled processor is a 16-core tiled CMP. Each core i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>. There have also been proposals on inserting prefetch instructions in the binary at compile time <ref type=\"bibr\" target=\"#b32\">[33]</ref> and exploiting recurring call-graph history <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref> and BTB <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> prefetchers over t vel to first-level BTB.</p><p>IBM z-series processors use a technique called Two-level Bulk Preload <ref type=\"bibr\" target=\"#b10\">[11]</ref> to prefetch branches from a 24K-entry second-level BTB to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: number of instruction <ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe otgun, two previously proposed techniques, pTask <ref type=\"bibr\" target=\"#b26\">[27]</ref> and RDIP <ref type=\"bibr\" target=\"#b7\">[8]</ref>, also leverage global control flow information for prefetchi data storage per core. Recent temporal streaming research has focused on lowering the storage costs <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ ibr\" target=\"#b24\">[25]</ref> or temporal stream prefetchers <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the software side, many approaches have been proposed to optimize the code layout at compile time <ref type=\"bibr\" target=\"#b27\">[28]</ref>, at link time <ref type=\"bibr\" target=\"#b28\">[29,</ref><re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to represent the optical flow. The optical flow estimator is constructed by a multilayer Dense CNN <ref type=\"bibr\" target=\"#b60\">[59]</ref>, which uses the cost volume, the features of the reference. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: id not function in real-time and required complex computation processes.</p><p>Recently, Sun et al. <ref type=\"bibr\" target=\"#b52\">[51]</ref> explored a compact but effective network for optical flow, w.tei-c.org/ns/1.0\"><head>A. Architecture Overview</head><p>After the remarkable work of Sun et al. <ref type=\"bibr\" target=\"#b52\">[51]</ref>, many of the subsequent approaches utilized the pyramid, w p><p>As shown in Fig. <ref type=\"figure\" target=\"#fig_1\">1</ref>, we utilize a normal PWC-Net model <ref type=\"bibr\" target=\"#b52\">[51]</ref> as the foundation of the proposed method, where the origin \"><head>B. Self-Attention-Based Multiscale Feature Learning Module</head><p>Even though the PWC-Net <ref type=\"bibr\" target=\"#b52\">[51]</ref> method adopted a feature pyramid with warping and a dilate  the L 2 distance is difficult to minimize in regions with large displacements or motion occlusions <ref type=\"bibr\" target=\"#b52\">[51]</ref>. Inspired by how the traditional variational optical flow  earning rate, we use a multiscale training loss during the training process by referring to PWC-Net <ref type=\"bibr\" target=\"#b52\">[51]</ref>, as shown in the following:</p><formula xml:id=\"formula_11 \u03b1 5 = 0.08, \u03b1 4 = 0.002, \u03b1 3 = 0.01, and \u03b1 2 = 0.005 at the various pyramid levels based on PWC-Net <ref type=\"bibr\" target=\"#b52\">[51]</ref>. In addition, the adjustment factor \u03b7 in Eq. ( <ref type=\" tation, scaling, Gaussian noise, brightness changes, contrast, gamma and color, as those in PWC-Net <ref type=\"bibr\" target=\"#b52\">[51]</ref>. We crop 448\u00d7384 patches with a batch size of 8 and 768\u00d738 e=\"bibr\" target=\"#b38\">[37]</ref>, FlowFieldsCNN <ref type=\"bibr\" target=\"#b49\">[48]</ref>, PWC-Net <ref type=\"bibr\" target=\"#b52\">[51]</ref>, ConFlow_ROB <ref type=\"bibr\" target=\"#b56\">[55]</ref>, Li ch component in our method for improving the optical flow performance, we employ the PWC-Net method <ref type=\"bibr\" target=\"#b52\">[51]</ref> as the baseline model and then respectively incorporate th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  smaller size and high accuracy, many subsequent studies <ref type=\"bibr\" target=\"#b53\">[52]</ref>- <ref type=\"bibr\" target=\"#b55\">[54]</ref> utilized PWC-Net as a foundation to construct a larger net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the remarkable benefits associated with its smaller size and high accuracy, many subsequent studies <ref type=\"bibr\" target=\"#b53\">[52]</ref>- <ref type=\"bibr\" target=\"#b55\">[54]</ref> utilized PWC-Ne large number of ground truths to train the occlusion detection networks. In contrast, other methods <ref type=\"bibr\" target=\"#b53\">[52]</ref>, <ref type=\"bibr\" target=\"#b59\">[58]</ref> used unsupervis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: evel vision tasks: robot navigation and obstacle avoidance <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b1\">[2]</ref>, unmanned direction of aerial vehicles <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a full cost volume and applied post-processing to regularize the flow field. Afterwards, Bao et al. <ref type=\"bibr\" target=\"#b51\">[50]</ref> utilized the DCFlow model as a component of a Kalman filte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt optical flow estimation compared with the previous variational approaches. Afterward, Sun et al. <ref type=\"bibr\" target=\"#b28\">[27]</ref> studied a non-local total variation optical flow model wit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n. By using the generic U-Net network <ref type=\"bibr\" target=\"#b41\">[40]</ref>, Dosovitskiy et al. <ref type=\"bibr\" target=\"#b42\">[41]</ref> adopted a fully convolutional neural network to construct  state-of-the-art variational approaches.</p><p>Following the remarkable study of Dosovitskiy et al. <ref type=\"bibr\" target=\"#b42\">[41]</ref>, Ilg et al. <ref type=\"bibr\" target=\"#b43\">[42]</ref> pres istance between the ground truth and the predicted optical flow during the network training process <ref type=\"bibr\" target=\"#b42\">[41]</ref>- <ref type=\"bibr\" target=\"#b43\">[42]</ref>, which is calcu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  smaller size and high accuracy, many subsequent studies <ref type=\"bibr\" target=\"#b53\">[52]</ref>- <ref type=\"bibr\" target=\"#b55\">[54]</ref> utilized PWC-Net as a foundation to construct a larger net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: areas. However, the local descriptor is not reliable in all image regions. In addition, Chen et al. <ref type=\"bibr\" target=\"#b32\">[31]</ref> presented a fusion flow field framework by using a post-pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: areas. However, the local descriptor is not reliable in all image regions. In addition, Chen et al. <ref type=\"bibr\" target=\"#b32\">[31]</ref> presented a fusion flow field framework by using a post-pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>.</p><p>Regarding past studies, the variational optical flow method derived from Horn and Schunck <ref type=\"bibr\" target=\"#b14\">[14]</ref> was dominant because it can produce a dense flow field and ><head>A. Variational Optical Flow Estimation</head><p>After the prominent work of Horn and Schunck <ref type=\"bibr\" target=\"#b14\">[14]</ref>, many studies were devoted to improving the computational . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n. By using the generic U-Net network <ref type=\"bibr\" target=\"#b41\">[40]</ref>, Dosovitskiy et al. <ref type=\"bibr\" target=\"#b42\">[41]</ref> adopted a fully convolutional neural network to construct  state-of-the-art variational approaches.</p><p>Following the remarkable study of Dosovitskiy et al. <ref type=\"bibr\" target=\"#b42\">[41]</ref>, Ilg et al. <ref type=\"bibr\" target=\"#b43\">[42]</ref> pres istance between the ground truth and the predicted optical flow during the network training process <ref type=\"bibr\" target=\"#b42\">[41]</ref>- <ref type=\"bibr\" target=\"#b43\">[42]</ref>, which is calcu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a foundation to construct a larger network for optical flow prediction. For instance, Neoral et al. <ref type=\"bibr\" target=\"#b56\">[55]</ref> integrated occlusion estimation into the PWC-Net model to  t robust optical flows in occluded regions, some studies <ref type=\"bibr\" target=\"#b48\">[47]</ref>, <ref type=\"bibr\" target=\"#b56\">[55]</ref> applied supervised networks to detect occlusions, and they ype=\"bibr\" target=\"#b49\">[48]</ref>, PWC-Net <ref type=\"bibr\" target=\"#b52\">[51]</ref>, ConFlow_ROB <ref type=\"bibr\" target=\"#b56\">[55]</ref>, LiteFlowNet <ref type=\"bibr\" target=\"#b45\">[44]</ref>, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the flow field estimation performance, several studies <ref type=\"bibr\" target=\"#b35\">[34]</ref>- <ref type=\"bibr\" target=\"#b36\">[35]</ref> incorporated the occlusion terms into the objective functi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the endpoint errors for the regions with different velocities per frame.</p><p>The KITTI database <ref type=\"bibr\" target=\"#b69\">[67]</ref> consists of a 2012 set and 2015 set, and it was produced b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion and recognition <ref type=\"bibr\" target=\"#b6\">[7]</ref>-8], multimedia coding and transmission <ref type=\"bibr\" target=\"#b9\">[9]</ref>- <ref type=\"bibr\" target=\"#b10\">[10]</ref>, medical image an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  MPI-Sintel final datasets, it needed multiple frames to refine the flow field. Later, Hur and Roth <ref type=\"bibr\" target=\"#b57\">[56]</ref> constructed an iterative residual refinement scheme to opt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng various image constraint assumptions to the data term <ref type=\"bibr\" target=\"#b22\">[21]</ref>- <ref type=\"bibr\" target=\"#b23\">[22]</ref> or modifying the flow diffusion strategy of the regulariza. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  MPI-Sintel final datasets, it needed multiple frames to refine the flow field. Later, Hur and Roth <ref type=\"bibr\" target=\"#b57\">[56]</ref> constructed an iterative residual refinement scheme to opt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b52\">54]</ref>, optimized frameworks and compilers <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" tar rget=\"#b5\">[7,</ref><ref type=\"bibr\" target=\"#b34\">36]</ref>. Currently, contemporary DNN compilers <ref type=\"bibr\" target=\"#b10\">[12,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" ta 35).</p></div> \t\t\t</div>  \t\t\t<div type=\"annex\"> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>network <ref type=\"bibr\" target=\"#b10\">[12,</ref><ref type=\"bibr\" target=\"#b11\">13]</ref>, CHaNAS can utiliz ry latency hiding is achieved implicitly with simultaneous multi-threading or hardware pre-fetching <ref type=\"bibr\" target=\"#b10\">[12]</ref>. While GPUs rely on rapid context switching of many warps  rization, memory tilling, and memory access latency hiding for different hardware. For example, TVM <ref type=\"bibr\" target=\"#b10\">[12]</ref> requires the user to write a template that defines the str een applied in both the NAS process and schedule strategy search process, e.g., heuristic algorithm <ref type=\"bibr\" target=\"#b10\">[12]</ref>, Bayesian optimization <ref type=\"bibr\" target=\"#b55\">[57]  each. For block compile optimization, we implement the optimization strategy in Python and use TVM <ref type=\"bibr\" target=\"#b10\">[12]</ref> tools (version 0.7.dev) for code generation targeting vari he LLVM to generate assembly code for GPUs. The XLA compiler acts as a back-end for TensorFlow. TVM <ref type=\"bibr\" target=\"#b10\">[12]</ref> proposes an ahead-of-time compiler that supports multiple  ines the rules of fusion and the range of tiling sizes to ensure a small auto-tuning space. AutoTVM <ref type=\"bibr\" target=\"#b10\">[12]</ref> utilizes high-level abstractions to represent the computin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b38\">40,</ref><ref type=\"bibr\" target=\"#b44\">46]</ref>, and even customized hardware design <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b45\">47,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b38\">40,</ref><ref type=\"bibr\" target=\"#b44\">46]</ref>, and even customized hardware design <ref type=\"bibr\" target=\"#b14\">[16,</ref><ref type=\"bibr\" target=\"#b45\">47,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b33\">[35]</ref>, PatDNN <ref type=\"bibr\" target=\"#b37\">[39]</ref>, and Co-CoPIE <ref type=\"bibr\" target=\"#b19\">[21]</ref>, which tackle model compression and compilation simultaneo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thm for low-level tensor code generation. As a framework for generating tensor programs, Flextensor <ref type=\"bibr\" target=\"#b54\">[56]</ref> attempts to reduce human efforts in writing templates by u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b26\">28,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b42\">44,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" target=\"#b50\">52,</ref><ref type=\"bibr\" tar evious NAS methods <ref type=\"bibr\" target=\"#b31\">[33,</ref><ref type=\"bibr\" target=\"#b42\">44,</ref><ref type=\"bibr\" target=\"#b46\">48,</ref><ref type=\"bibr\" target=\"#b57\">59]</ref>, they either use tr th four previous hardware-aware NAS works (Mnasnet <ref type=\"bibr\" target=\"#b42\">[44]</ref>, Fbnet <ref type=\"bibr\" target=\"#b46\">[48]</ref>, ProxylessNas <ref type=\"bibr\" target=\"#b8\">[10]</ref>, an or the ImageNet dataset by proposing a gradient-based approach to train binarized parameters. FBNet <ref type=\"bibr\" target=\"#b46\">[48]</ref> proposes a differentiable platform-aware NAS using Gumbel . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k model to optimize the performance for different hardware <ref type=\"bibr\" target=\"#b13\">[15,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b47\">49]</ref>, but they ignore th tracted a lot of attention from both academia and industry <ref type=\"bibr\" target=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b18\">20]</ref>. The best practice currently is still developing schedule l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: k model to optimize the performance for different hardware <ref type=\"bibr\" target=\"#b13\">[15,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b47\">49]</ref>, but they ignore th tracted a lot of attention from both academia and industry <ref type=\"bibr\" target=\"#b17\">[19,</ref><ref type=\"bibr\" target=\"#b18\">20]</ref>. The best practice currently is still developing schedule l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b24\">[26]</ref><ref type=\"bibr\" target=\"#b52\">54]</ref>, optimized frameworks and compilers <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b10\">12,</ref><ref type=\"bibr\" targ rent hardware and needs manually tuning for a new DNN model. Most existing deep learning frameworks <ref type=\"bibr\" target=\"#b2\">[4,</ref><ref type=\"bibr\" target=\"#b9\">11,</ref><ref type=\"bibr\" targe s deep learning applications by assembling a set of efficient algorithms on the GPUs. TF-Lite Micro <ref type=\"bibr\" target=\"#b2\">[4]</ref> focuses on accelerating the DNN models on the embedded hardw ture search based on a fixed schedule strategy from a given deep learning library (e.g., Tensorflow <ref type=\"bibr\" target=\"#b2\">[4]</ref> and Pytorch <ref type=\"bibr\" target=\"#b38\">[40]</ref>) or tu mlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_2\"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type=\"bibr\" target=\"#b2\">4</ref>. Relative Pareto-frontier of three different scheduling strate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  some recent works <ref type=\"bibr\" target=\"#b12\">[14,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b21\">23,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" tar get=\"#b6\">[8]</ref>, researchers begin to combine multi-objective optimization into the NAS process <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uracy architectures without considering hardware efficiency. With the development of NAS techniques <ref type=\"bibr\" target=\"#b6\">[8]</ref>, researchers begin to combine multi-objective optimization i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  emotion from a fixed number of labels <ref type=\"bibr\" target=\"#b0\">[Abdrashitov et al. 2020;</ref><ref type=\"bibr\" target=\"#b31\">Li et al. 2021;</ref><ref type=\"bibr\" target=\"#b51\">Wang et al. 2020b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f><ref type=\"bibr\">Bregler et al. 1997a,b;</ref><ref type=\"bibr\" target=\"#b34\">Lu et al. 2021;</ref><ref type=\"bibr\" target=\"#b52\">Wang et al. 2012;</ref><ref type=\"bibr\" target=\"#b67\">Zhou et al. 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref><ref type=\"bibr\" target=\"#b46\">Thies et al. 2020;</ref><ref type=\"bibr\">Zhou et al. 2021</ref><ref type=\"bibr\" target=\"#b65\">Zhou et al. , 2020]]</ref>, many of them rely on long video recording -shot manner for the first time. Later, <ref type=\"bibr\" target=\"#b9\">Chen et al. [2019a]</ref> and <ref type=\"bibr\" target=\"#b65\">Zhou et al. [2020]</ref> improve the schedule by leveraging facial la 9\">[Vougioukas et al. 2018]</ref>, <ref type=\"bibr\">Wav2Lip [Prajwal et al. 2020]</ref>, MakeItTalk <ref type=\"bibr\" target=\"#b65\">[Zhou et al. 2020</ref>], PC-AVS <ref type=\"bibr\">[Zhou et al. 2021]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b27\">[Kim et al. 2019;</ref><ref type=\"bibr\" target=\"#b51\">Wang et al. 2020b</ref>] and 3D models <ref type=\"bibr\" target=\"#b1\">[Anderson et al. 2013;</ref><ref type=\"bibr\" target=\"#b41\">Richard et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pproaches demand prior knowledge or manual labels of the animated target such as 3D morphable model <ref type=\"bibr\" target=\"#b28\">[Kim et al. 2018;</ref><ref type=\"bibr\" target=\"#b47\">Thies et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: otion is represented as a set of unsupervised key-points and their first order dynamics inspired by <ref type=\"bibr\" target=\"#b44\">[Siarohin et al. 2019b;</ref><ref type=\"bibr\" target=\"#b53\">Wang et a the audio-involved key-point distribution with a pretrained video-driven first-order motion model's <ref type=\"bibr\" target=\"#b44\">[Siarohin et al. 2019b</ref>].</p><p>Thus we employ a pretrained key-  into an image generator \ud835\udc6e to produce the final output frame at each time step \u00ce\ud835\udc61 . Please refer to <ref type=\"bibr\" target=\"#b44\">[Siarohin et al. 2019b</ref>] for more details.</p><p>Training Object  the emotion style from the emotion feature f \ud835\udc52 to the audio feature f \ud835\udc4e . Lastly, we use a decoder <ref type=\"bibr\" target=\"#b44\">[Siarohin et al. 2019b</ref>] to predict the final key-points and jac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trics mentioned in Section 4.1, we additionally use an off-the-shelf emotion classification network <ref type=\"bibr\" target=\"#b36\">[Meng et al. 2019]</ref> to evaluate the accuracy of the generated em. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b21\">Huang et al. 2020;</ref><ref type=\"bibr\" target=\"#b23\">Isola et al. 2017;</ref><ref type=\"bibr\" target=\"#b48\">Tripathy et al. 2021;</ref><ref type=\"bibr\" target=\"#b57\">Wu et al. 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b29\">Korbar et al. 2018;</ref><ref type=\"bibr\" target=\"#b39\">Owens et al. 2016;</ref><ref type=\"bibr\" target=\"#b66\">Zhou and Lim 2021]</ref> also consider the relationship between the v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b8\">[Chai et al. 2020;</ref><ref type=\"bibr\" target=\"#b18\">G\u00fcera and Delp 2018;</ref><ref type=\"bibr\" target=\"#b30\">Li et al. 2020;</ref><ref type=\"bibr\" target=\"#b42\">Rossler et al. 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se works are not tailored to anomaly detection. Therefore, we invoke Hammond's graph wavelet theory <ref type=\"bibr\" target=\"#b20\">(Hammond et al., 2011)</ref> to develop our new graph neural network  wback, we propose our new graph neural network architecture based on Hammond's graph wavelet theory <ref type=\"bibr\" target=\"#b20\">(Hammond et al., 2011)</ref>, which is band-pass in nature and can be ><head n=\"3.1.\">Background: Hammond's Graph Wavelet</head><p>The graph wavelet transform defined in <ref type=\"bibr\" target=\"#b20\">(Hammond et al., 2011)</ref> starts with a \"mother\" wavelet \u03c8 and emp When the distance d G (v i , v j ) &gt; p+q, we have W p,q \u03b4 i [j] = 0.</p><p>Invoking Lemma 5.2 in <ref type=\"bibr\" target=\"#b20\">(Hammond et al., 2011)</ref>, we have</p><formula xml:id=\"formula_17\" ibr\" target=\"#b28\">Li et al., 2021)</ref> do not essentially satisfy Hammond's graph wavelet theory <ref type=\"bibr\" target=\"#b20\">(Hammond et al., 2011)</ref> because they are not band-pass. To remed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  health monitoring <ref type=\"bibr\" target=\"#b3\">(Bao et al., 2019)</ref>, device failure detection <ref type=\"bibr\" target=\"#b44\">(Sipple, 2020)</ref>, to name a few. As graph data becomes ubiquitous  two widely used datasets in previous works <ref type=\"bibr\" target=\"#b31\">(Liu et al., 2021b;</ref><ref type=\"bibr\" target=\"#b44\">2020)</ref>, including the Amazon dataset <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d by PyTorch <ref type=\"bibr\" target=\"#b40\">(Paszke et al., 2019)</ref>, and SVM is in Scikit-learn <ref type=\"bibr\" target=\"#b41\">(Pedregosa et al., 2011)</ref>. For GCN, ChebyNet, GAT, and GraphSAGE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d by PyTorch <ref type=\"bibr\" target=\"#b40\">(Paszke et al., 2019)</ref>, and SVM is in Scikit-learn <ref type=\"bibr\" target=\"#b41\">(Pedregosa et al., 2011)</ref>. For GCN, ChebyNet, GAT, and GraphSAGE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f of Equation (1)</head><p>Proof. Note that we have</p><p>x T Lx according to spectral graph theory <ref type=\"bibr\" target=\"#b45\">(Spielman, 2007)</ref>. The detailed derivation of Equation ( <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion and suffer from the over-smoothing issue <ref type=\"bibr\" target=\"#b29\">(Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b50\">Wu et al., 2019)</ref>. When GNN aggregates information from the node  the current GNNs are low-pass filters <ref type=\"bibr\" target=\"#b39\">(Nt &amp; Maehara, 2019;</ref><ref type=\"bibr\" target=\"#b50\">Wu et al., 2019)</ref> or adaptive filters <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d by PyTorch <ref type=\"bibr\" target=\"#b40\">(Paszke et al., 2019)</ref>, and SVM is in Scikit-learn <ref type=\"bibr\" target=\"#b41\">(Pedregosa et al., 2011)</ref>. For GCN, ChebyNet, GAT, and GraphSAGE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . That is, (1) applying attention mechanisms to correlate different neighbors through various views <ref type=\"bibr\" target=\"#b48\">(Wang et al., 2019a;</ref><ref type=\"bibr\" target=\"#b8\">Cui et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on is necessary to be further explored <ref type=\"bibr\" target=\"#b38\">(Noble &amp; Cook, 2003;</ref><ref type=\"bibr\" target=\"#b33\">Ma et al., 2021)</ref>. 1 Hong Kong University of Science and Technol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: &amp; Maehara, 2019;</ref><ref type=\"bibr\" target=\"#b50\">Wu et al., 2019)</ref> or adaptive filters <ref type=\"bibr\" target=\"#b11\">(Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">He et a exity of BWGNN is O(C E ), as \u03b2 * p,q (L) is a polynomial function that can be computed recursively <ref type=\"bibr\" target=\"#b11\">(Defferrard et al., 2016)</ref>.</p></div> <div xmlns=\"http://www.tei GNN models, including GCN <ref type=\"bibr\" target=\"#b25\">(Kipf &amp; Welling, 2017)</ref>, ChebyNet <ref type=\"bibr\" target=\"#b11\">(Defferrard et al., 2016)</ref>, GAT <ref type=\"bibr\" target=\"#b47\">( nal network using the first-order approximation of localized spectral filters on graphs. \u2022 ChebyNet <ref type=\"bibr\" target=\"#b11\">(Defferrard et al., 2016</ref>): a graph convolutional network which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o generate graph wavelets. On another front, <ref type=\"bibr\" target=\"#b35\">(Min et al., 2020;</ref><ref type=\"bibr\" target=\"#b17\">Gama et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Min et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b65\">66,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, in stark contrast with vision and NLP. Effective methods f get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b65\">66,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, QM9 is a dataset <ref type=\"bibr\" target=\"#b45\">[46]</ref> diction tasks. Inspired by recent advances in noise regularization for graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b22\">[23]</ref>, our pre-training objective is based on denoising in the s br\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Our work builds on Noisy Nodes <ref type=\"bibr\" target=\"#b22\">[23]</ref>, which incorporates denoising as an auxiliary task to impr f type=\"bibr\" target=\"#b37\">38]</ref>, or coordinates with respect to the principal axes of inertia <ref type=\"bibr\" target=\"#b22\">[23]</ref>. There is also broad literature on equivariant neural netw s/1.0\"><head n=\"3.2.2\">Noisy Nodes: Denoising as an Auxiliary Loss</head><p>Recently, Godwin et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> also applied denoising as an auxiliary loss to molecular p rn useful representations and shedding light on successful applications of denoising in other works <ref type=\"bibr\" target=\"#b22\">[23]</ref>. This technique enabled us to utilize existing large datas nd atom type recovery. For GNS/GNS-TAT, we relied on the hyperparameters published by Godwin et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> but determined new noise values for pre-training and fine-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ommon GNN architecture, in particular showing how to apply Tailored Activation Transformation (TAT) <ref type=\"bibr\" target=\"#b79\">[80]</ref> to Graph Network Simulators (GNS) <ref type=\"bibr\" target= e of a recently published network transformation method called Tailored Activation Transforms (TAT) <ref type=\"bibr\" target=\"#b79\">[80]</ref>, which has been shown to prevent certain degenerate behavi with Tailored Activation Transformation (GNS-TAT)</head><p>Tailored Activation Transformation (TAT) <ref type=\"bibr\" target=\"#b79\">[80]</ref> is a method for initializing and transforming neural netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent combinations of upstream and downstream tasks as described in Section 4.3.</p><p>DES15K. DES15K <ref type=\"bibr\" target=\"#b17\">[18]</ref> (license: CC0 1.0) is a small dataset containing around 15. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f atom i in the molecule, alongside a label y \u2208 R. Some molecular datasets (e.g. Open Catalyst 2020 <ref type=\"bibr\" target=\"#b12\">[13]</ref>) provide additional per-atom features such as tags for ato their structures. Each molecule has 12 OC20. Open Catalyst 2020 (OC20, license: CC Attribution 4.0) <ref type=\"bibr\" target=\"#b12\">[13]</ref> is a recent large benchmark containing trajectories of int. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sion of this dataset without any 3D structures, called PCQM4M, was used for supervised pre-training <ref type=\"bibr\" target=\"#b77\">[78]</ref>, but to our knowledge, this is the first time the 3D struc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. For molecular prop. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmed by a pre-trained model of less than half the size (53 million parameters).</p><p>Sriram et al. <ref type=\"bibr\" target=\"#b62\">[63]</ref> also recently studied a custom training scheme to scale up. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b81\">82,</ref><ref type=\"bibr\" target=\"#b76\">77,</ref><ref type=\"bibr\" target=\"#b80\">81,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> of vertex/edge features after multiple message-passing laye get=\"#b81\">82,</ref><ref type=\"bibr\" target=\"#b76\">77,</ref><ref type=\"bibr\" target=\"#b80\">81,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> is a phenomenon observed in GNN architectures where vertex/. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: input graph features has been shown to improve performance <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. Applications to physical simulation also involve corruptin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f atom i in the molecule, alongside a label y \u2208 R. Some molecular datasets (e.g. Open Catalyst 2020 <ref type=\"bibr\" target=\"#b12\">[13]</ref>) provide additional per-atom features such as tags for ato their structures. Each molecule has 12 OC20. Open Catalyst 2020 (OC20, license: CC Attribution 4.0) <ref type=\"bibr\" target=\"#b12\">[13]</ref> is a recent large benchmark containing trajectories of int. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Datasets and Training Setup</head><p>PCQM4Mv2. The main dataset we use for pre-training is PCQM4Mv2 <ref type=\"bibr\" target=\"#b41\">[42]</ref> (license: CC BY 4.0), which contains 3,378,606 organic mol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b15\">16]</ref>. Recently, transformer with attention mechanism was introduced for visual tasks <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> and attained competiti NNs <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>, transformers <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> and MLPs <ref type=\"  window on the image and introduce the shift-invariance and locality. The recent vision transformer <ref type=\"bibr\" target=\"#b7\">[8]</ref> or MLP <ref type=\"bibr\" target=\"#b46\">[47]</ref> treats the  ef type=\"bibr\" target=\"#b46\">[47]</ref> treats the image as a sequence of patches. For example, ViT <ref type=\"bibr\" target=\"#b7\">[8]</ref> divides a 224 \u00d7 224 image into a number of 16 \u00d7 16 patches a get=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. From then on, a number of variants of ViT <ref type=\"bibr\" target=\"#b7\">[8]</ref> were proposed to improve the performance on visual tasks. Th  of computer vision, the commonly-used transformer usually has an isotropic architecture (e.g., ViT <ref type=\"bibr\" target=\"#b7\">[8]</ref>), while CNNs prefer to use pyramid architecture (i.e., ResNe ture means the main body has features with equal size and shape throughout the network, such as ViT <ref type=\"bibr\" target=\"#b7\">[8]</ref> and ResMLP <ref type=\"bibr\" target=\"#b47\">[48]</ref>. We bui networks in vision also explore it such as ConvMixer <ref type=\"bibr\" target=\"#b46\">[47]</ref>, ViT <ref type=\"bibr\" target=\"#b7\">[8]</ref> and ResMLP <ref type=\"bibr\" target=\"#b47\">[48]</ref>. We com sion transformer was introduced for visual tasks from 2020 <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. From then on, a number. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  system, convolutional neural networks (CNNs) used to be the de-facto standard network architecture <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta on</head><p>The mainstream network architecture in computer vision used to be convolutional network <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta ef>, the image data is usually represented as a regular grid of pixels in the Euclidean space. CNNs <ref type=\"bibr\" target=\"#b26\">[27]</ref> apply sliding window on the image and introduce the shift- ef type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. Starting from LeNet <ref type=\"bibr\" target=\"#b26\">[27]</ref>, CNNs have been successfully used in various visual tasks,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the graph data. The over-smoothing phenomenon in deep GCNs <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> will decrease the distinctiveness of node features and lead. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[21]</ref> 0.1 0.1 0.1 0.3 Repeated augment <ref type=\"bibr\" target=\"#b18\">[19]</ref> RandAugment <ref type=\"bibr\" target=\"#b4\">[5]</ref> Mixup prob. <ref type=\"bibr\" target=\"#b65\">[66]</ref> 0.8 Cu ype=\"bibr\" target=\"#b48\">[49]</ref> for fair comparison. The data augmentation includes RandAugment <ref type=\"bibr\" target=\"#b4\">[5]</ref>, Mixup <ref type=\"bibr\" target=\"#b65\">[66]</ref>, Cutmix <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  layers to extract aggregated feature of the graph data. The over-smoothing phenomenon in deep GCNs <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> will decrease the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> and position encoding <ref type=\"bibr\" target=\"#b57\">[58]</ref>. Inspired by vision transformer, MLP is also explored in c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b67\">[68]</ref>. Vision transformer was introduced for visual tasks from 2020 <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d n=\"2.2\">Graph Neural Network</head><p>The earliest graph neural network was initially outlined in <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Micheli <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ngs</head><p>Datasets. In image classification task, the widely-used benchmark ImageNet ILSVRC 2012 <ref type=\"bibr\" target=\"#b40\">[41]</ref> is used in the following experiments. ImageNet has 120M tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  joints, GCN was utilized on human action recognition task <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b61\">62]</ref>. GCN can only tackle specific visual tasks with naturally c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th the objects and their relationship, which is usually solved by combining object detector and GCN <ref type=\"bibr\" target=\"#b59\">[60,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>. By processing the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n backbones without need for compute intensive explicit consideration of sidechain rotameric states <ref type=\"bibr\" target=\"#b0\">(1)</ref><ref type=\"bibr\" target=\"#b1\">(2)</ref><ref type=\"bibr\" targe Ca atoms, relative Ca-Ca-Ca frame orientations and rotations, and backbone dihedral angles-as input <ref type=\"bibr\" target=\"#b0\">(1)</ref>. We first sought to improve performance of the model on reco \" target=\"#tab_0\">1</ref> we used a dataset based on the CATH 4.2 40% non-redundant set of proteins <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b6\">7)</ref>. We trained models fol roups for training <ref type=\"bibr\" target=\"#b22\">(23,</ref><ref type=\"bibr\">358)</ref>, validation <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\">464)</ref>, and testing <ref type=\"bibr\" tar )</ref>, validation <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\">464)</ref>, and testing <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\">539)</ref>, ensuring that none of the chains d>Model architecture</head><p>We used encoder-decoder message passing neural networks for this task <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">(1)</ref><ref type=\"bibr\" target=\"#b1\">(2)</ref><ref type=\"bibr\" target=\"#b2\">(3)</ref><ref type=\"bibr\" target=\"#b3\">(4)</ref><ref type=\"bibr\" target=\"#b4\">(5)</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e experimented with <ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\">48</ref>, and 64 nearest Ca neighbor neural orks (Figure <ref type=\"figure\" target=\"#fig_6\">S1A</ref>), and found that performance saturated at <ref type=\"bibr\" target=\"#b31\">[32]</ref><ref type=\"bibr\">[33]</ref><ref type=\"bibr\">[34]</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref> resulting in 25,361 clusters. We split those clusters randomly into three groups for training <ref type=\"bibr\" target=\"#b22\">(23,</ref><ref type=\"bibr\">358)</ref>, validation <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: termine the range over which backbone geometry influences amino acid identity, we experimented with <ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar ave proven useful in several biotechnological applications including structure-based vaccine design <ref type=\"bibr\" target=\"#b15\">(16)</ref><ref type=\"bibr\" target=\"#b16\">(17)</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tworks for this task <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b24\">25)</ref>, see Figure <ref type=\"figure\">1</ref>. The encoder takes g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ometry influences amino acid identity, we experimented with <ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\">48<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a cell and then manually stack a series of identical cells to build a target neural network. DARTS <ref type=\"bibr\" target=\"#b28\">(Liu et al., 2018c)</ref> relaxed the discrete search space to be con e=\"bibr\" target=\"#b52\">Zhou et al., 2020)</ref> pointed out that the bi-level optimization of DARTS <ref type=\"bibr\" target=\"#b28\">(Liu et al., 2018c)</ref> suffers from performance collapse issues. M ess is discrete and usually takes thousands or even tens of thousands of GPU-hours. Later on, DARTS <ref type=\"bibr\" target=\"#b28\">(Liu et al., 2018c)</ref> proposed relaxing the discrete search space org/ns/1.0\"><head n=\"5.\">Experiments</head><p>5.1. Search Space DARTS Search Space. Following DARTS <ref type=\"bibr\" target=\"#b28\">(Liu et al., 2018c)</ref>, a cell is defined as a directed acyclic gr MBO ENAS <ref type=\"bibr\" target=\"#b32\">(Pham et al., 2018)</ref> 2.89 4.6 0.5 RL DARTS (1st order) <ref type=\"bibr\" target=\"#b28\">(Liu et al., 2018c)</ref> 3.00\u00b10.14 3.3 1.5 Gradient DARTS (2nd order ef type=\"bibr\" target=\"#b28\">(Liu et al., 2018c)</ref> 3.00\u00b10.14 3.3 1.5 Gradient DARTS (2nd order) <ref type=\"bibr\" target=\"#b28\">(Liu et al., 2018c)</ref> 2.76\u00b10.09 3.3 4 Gradient SNAS <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 3150 EA PNAS <ref type=\"bibr\" target=\"#b25\">(Liu et al., 2018a)</ref> 25.8 8.1 5.1 225 SMBO FBNet-C <ref type=\"bibr\" target=\"#b41\">(Wu et al., 2019)</ref> 25.1 7.9 5.5 9 Gradient ProxylessNAS(GPU) <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -DARTS <ref type=\"bibr\" target=\"#b49\">(Zela et al., 2020)</ref> 2.95\u00b10.21 N/A 1.6 Gradient PC-DARTS <ref type=\"bibr\" target=\"#b45\">(Xu et al., 2019a)</ref> 2.57\u00b10.07 3.6 0.1 Gradient DATA <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -DARTS <ref type=\"bibr\" target=\"#b49\">(Zela et al., 2020)</ref> 2.95\u00b10.21 N/A 1.6 Gradient PC-DARTS <ref type=\"bibr\" target=\"#b45\">(Xu et al., 2019a)</ref> 2.57\u00b10.07 3.6 0.1 Gradient DATA <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b44\">(Xu et al., 2015;</ref><ref type=\"bibr\" target=\"#b36\">Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Hu et al., 2018;</ref><ref type=\"bibr\" target=\"#b40\">Woo et al., 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"figure\">1 (b)</ref>. Though prior works <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2020;</ref><ref type=\"bibr\" target=\"#b9\">Chu et al., 2021;</ref><ref type=\"bibr\" target=\"#b48\">You et al., 2020 get=\"#b13\">(Guo et al., 2020)</ref> trains the super-network through uniform path sampling. FairNAS <ref type=\"bibr\" target=\"#b9\">(Chu et al., 2021)</ref> enforces fairness constraints to alleviate th  SPOS <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2020)</ref> 26.0 8.4 5.3 11 \u2021 Evolution FairNAS-A <ref type=\"bibr\" target=\"#b9\">(Chu et al., 2021)</ref> 24.66 7.8 4.6 16 \u2021 Evolution GreedyNAS-C <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )</ref> applied this mechanism to the RNN model for image classification. Afterwards, lots of works <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2015;</ref><ref type=\"bibr\" target=\"#b36\">Wang et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b44\">(Xu et al., 2015;</ref><ref type=\"bibr\" target=\"#b36\">Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Hu et al., 2018;</ref><ref type=\"bibr\" target=\"#b40\">Woo et al., 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ementioned issues. Similar to how the human brain selectively focuses on certain parts of the input <ref type=\"bibr\" target=\"#b2\">(Briggs et al., 2013)</ref>, we demonstrate that the attention mechani. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ision tasks, such as image classification <ref type=\"bibr\" target=\"#b53\">(Zoph &amp; Le, 2016;</ref><ref type=\"bibr\" target=\"#b54\">Zoph et al., 2018;</ref><ref type=\"bibr\" target=\"#b13\">Guo et al., 20 bibr\" target=\"#b0\">(Baker et al., 2016;</ref><ref type=\"bibr\" target=\"#b1\">Bello et al., 2017;</ref><ref type=\"bibr\" target=\"#b54\">Zoph et al., 2018)</ref> or evolutionary algorithms (EA) <ref type=\"b ermine the operation associated with each edge in a cell. The micro search space proposed by NASNet <ref type=\"bibr\" target=\"#b54\">(Zoph et al., 2018)</ref> is widely used in later works. For example, 1.0\"><head>Methods</head><p>Test Err.(%) Params(M) Search Cost Search (GPU-days) Algorithm NASNet-A <ref type=\"bibr\" target=\"#b54\">(Zoph et al., 2018)</ref> 2.65 3.3 1800 RL AmoebaNet-A <ref type=\"bib Algorithm MnasNet <ref type=\"bibr\" target=\"#b35\">(Tan et al., 2019)</ref> 26 8.2 4.2 2000 RL NASNet <ref type=\"bibr\" target=\"#b54\">(Zoph et al., 2018)</ref> 26.0 8.4 5.3 1800 RL AmoebaNet <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: reduction and multi-architectural sampling techniques for searching a multi-stage ViT architecture. <ref type=\"bibr\" target=\"#b31\">(Minghao et al., 2021)</ref>   Error to search the architecture and s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: search space continuous and adopt architecture parameters to select operations. Single-Path methods <ref type=\"bibr\" target=\"#b15\">(Guo et al., 2020;</ref><ref type=\"bibr\" target=\"#b10\">Chu et al., 20  with Hybrid Sampling</head><p>In One-Shot NAS, it is common practice to sample a subnetwork (SPOS) <ref type=\"bibr\" target=\"#b15\">(Guo et al., 2020)</ref> in each iteration and train the supernet. Ho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cted to 8 operations in DARTS <ref type=\"bibr\" target=\"#b28\">(Liu et al., 2019)</ref>. ProxylessNAS <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref> and FBNet <ref type=\"bibr\" target=\"#b47\">(Wu . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  error. Early NAS utilized reinforcement learning (Zoph &amp; Le, 2017) and evolutionary algorithms <ref type=\"bibr\" target=\"#b37\">(Real et al., 2019)</ref> to search for convolutional neural network . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Augment <ref type=\"bibr\" target=\"#b13\">(Ekin Dogus et al., 2020)</ref>. We alse use stochasic depth <ref type=\"bibr\" target=\"#b14\">(Gao et al., 2016)</ref> and layerscale <ref type=\"bibr\" target=\"#b20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntroduced a soft inductive bias of CNN into ViT to bridge the gap between CNN and Transformer. CeiT <ref type=\"bibr\" target=\"#b55\">(Yuan et al., 2021)</ref> combined the benefits of CNN in extracting   Secondly, on the one hand, adding convolution to feed-forward network (FFN) can improve performance<ref type=\"bibr\" target=\"#b55\">(Yuan et al., 2021)</ref>, on the other hand, we observe that both of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 39\">(Sangdoo et al., 2019</ref><ref type=\"bibr\">), CutOut (Zhun et al., 2020)</ref> and RandAugment <ref type=\"bibr\" target=\"#b13\">(Ekin Dogus et al., 2020)</ref>. We alse use stochasic depth <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd mutation probability is 0.3. We select the Top-5 architectures and validate them on ImageNet-100 <ref type=\"bibr\" target=\"#b52\">(Yonglong et al., 2020)</ref>, and then take the optimal architecture. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata augmentations include MixUp <ref type=\"bibr\" target=\"#b20\">(Hongyi et al., 2018)</ref>, Cut-Mix <ref type=\"bibr\" target=\"#b39\">(Sangdoo et al., 2019</ref><ref type=\"bibr\">), CutOut (Zhun et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Augment <ref type=\"bibr\" target=\"#b13\">(Ekin Dogus et al., 2020)</ref>. We alse use stochasic depth <ref type=\"bibr\" target=\"#b14\">(Gao et al., 2016)</ref> and layerscale <ref type=\"bibr\" target=\"#b20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g/ns/1.0\"><head n=\"5.2.\">Results on COCO</head><p>Setting. We conduct experiments on COCO benchmark <ref type=\"bibr\" target=\"#b27\">(Lin et al., 2014)</ref> to verify the transferability of Burger-Form. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  which follows the customs in CNN models that compress a set of nodes into a compact representation <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., ure\">2</ref>, the hierarchical pooling architecture follows the setting in previous pooling studies <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., based on node drop, like TopKPool <ref type=\"bibr\" target=\"#b8\">(Gao &amp; Ji, 2019)</ref>, SAGPool <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref> and ASAP <ref type=\"bibr\" target=\"#b20\">(Ran anize a new one, such as TopKPool <ref type=\"bibr\" target=\"#b8\">(Gao &amp; Ji, 2019)</ref>, SAGPool <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref> and ASAP <ref type=\"bibr\" target=\"#b20\">(Ran methods as baselines: DiffPool <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018)</ref>, SAGPool(H) <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref>, TopKPool <ref type=\"bibr\" target=\"#b8\">(Gao b26\">(Vinyals et al., 2016)</ref>, SortPool <ref type=\"bibr\">(Zhang et al., 2018)</ref>, SAGPool(G) <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref>, StructPool <ref type=\"bibr\" target=\"#b31\">(  drop methods, including TopKPool <ref type=\"bibr\" target=\"#b8\">(Gao &amp; Ji, 2019)</ref>, SAGPool <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref> and ASAP <ref type=\"bibr\" target=\"#b20\">(Ran et \u2208 {0, 0.5}. Then we optimize the network with Adam optimizer. For a fair comparison of baselines <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref>, we use the three GCN layers <ref type=\"bibr  2021)</ref>.</p><p>Configurations. Following <ref type=\"bibr\" target=\"#b29\">(Xu et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Lee et al., 2019)</ref>, 10-fold cross-validation is conducted, and w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: belongs to.</p><p>Datasets. Seven benchmarks for graph classification are selected from TU datasets <ref type=\"bibr\" target=\"#b19\">(Morris et al., 2020)</ref>. Specifically, we employ three social net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: belongs to.</p><p>Datasets. Seven benchmarks for graph classification are selected from TU datasets <ref type=\"bibr\" target=\"#b19\">(Morris et al., 2020)</ref>. Specifically, we employ three social net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a set of nodes into a compact representation <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020)</ref>.</p><p>Besides the simplest pooling method s based on node clustering avoid this issue <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020)</ref>, the damage on the graph local structure s ooling layer for baselines as previous works <ref type=\"bibr\" target=\"#b1\">(Baek et al., 2021;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020)</ref>, while our model follows the natural clust s issue, including DiffPool <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018)</ref> and MinCutPool <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>, in which the nodes of original graph are X and output features X r , i.e., min X \u2212 X r 2 . For configuration, we employ the Synthetic graphs <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>, including a ring and a grid that the inp ; Ji, 2019)</ref>, ASAP <ref type=\"bibr\" target=\"#b20\">(Ranjan et al., 2020)</ref>, and Min-CutPool <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>. Besides various hierarchical pooling met  xmlns=\"http://www.tei-c.org/ns/1.0\"><head>SEP-U</head><p>GCN GCN Implementation details. Following <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>, we use the two message passing layers bo methods, including DiffPool <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018)</ref> and MinCutPool <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>. For the node drop methods, we use the un lows the setting in previous pooling studies <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Baek et al.,  is based on the conventionally used training/test splits <ref type=\"bibr\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Baek et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ssification. In addition, the initial feature inputs is in line with the fair comparison setting in <ref type=\"bibr\" target=\"#b7\">(Errica et al., 2020)</ref>. Additional details about experiment setup we use the 10 percent of the training data as a validation data following the fair comparison setup <ref type=\"bibr\" target=\"#b7\">(Errica et al., 2020)</ref>. We use the early stopping criterion, wher. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: belongs to.</p><p>Datasets. Seven benchmarks for graph classification are selected from TU datasets <ref type=\"bibr\" target=\"#b19\">(Morris et al., 2020)</ref>. Specifically, we employ three social net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inear layer after each SEP or SEP-U layer to learn more task-specific node representations. Dropout <ref type=\"bibr\" target=\"#b23\">(Srivastava et al., 2014)</ref> with ReLU on feature matrices is appl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve been applied in medicine <ref type=\"bibr\" target=\"#b15\">(Li et al., 2016b)</ref>, bioinformatics <ref type=\"bibr\" target=\"#b16\">(Li et al., 2018)</ref>, and the security of networks <ref type=\"bibr nding coding tree, in which disturbance derived from noise or stochastic variation can be minimized <ref type=\"bibr\" target=\"#b16\">(Li et al., 2018)</ref>. We believe an effective structural entropy m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plexity of two-and three-level hierarchical structures, respectively, have been applied in medicine <ref type=\"bibr\" target=\"#b15\">(Li et al., 2016b)</ref>, bioinformatics <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a set of nodes into a compact representation <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020)</ref>.</p><p>Besides the simplest pooling method s based on node clustering avoid this issue <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020)</ref>, the damage on the graph local structure s ooling layer for baselines as previous works <ref type=\"bibr\" target=\"#b1\">(Baek et al., 2021;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020)</ref>, while our model follows the natural clust s issue, including DiffPool <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018)</ref> and MinCutPool <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>, in which the nodes of original graph are X and output features X r , i.e., min X \u2212 X r 2 . For configuration, we employ the Synthetic graphs <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>, including a ring and a grid that the inp ; Ji, 2019)</ref>, ASAP <ref type=\"bibr\" target=\"#b20\">(Ranjan et al., 2020)</ref>, and Min-CutPool <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>. Besides various hierarchical pooling met  xmlns=\"http://www.tei-c.org/ns/1.0\"><head>SEP-U</head><p>GCN GCN Implementation details. Following <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>, we use the two message passing layers bo methods, including DiffPool <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018)</ref> and MinCutPool <ref type=\"bibr\" target=\"#b2\">(Bianchi et al., 2020)</ref>. For the node drop methods, we use the un lows the setting in previous pooling studies <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Baek et al.,  is based on the conventionally used training/test splits <ref type=\"bibr\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b2\">Bianchi et al., 2020;</ref><ref type=\"bibr\" target=\"#b1\">Baek et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  layer has been confirmed by previous works <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b17\">Ma et al., 2019)</ref>. Thus, the SEP layer should be invariant with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: belongs to.</p><p>Datasets. Seven benchmarks for graph classification are selected from TU datasets <ref type=\"bibr\" target=\"#b19\">(Morris et al., 2020)</ref>. Specifically, we employ three social net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  layer has been confirmed by previous works <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b17\">Ma et al., 2019)</ref>. Thus, the SEP layer should be invariant with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -N under the transductive learning setting, which includes three datasets Cora, Citeseer and Pubmed <ref type=\"bibr\" target=\"#b21\">(Sen et al., 2008)</ref>. The three benchmarks are constructed on the  datasets. We utilize three standard citation network benchmark datasets: Cora, Citeseer and Pubmed <ref type=\"bibr\" target=\"#b21\">(Sen et al., 2008)</ref>. In all of these datasets, nodes correspond . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: MixHop (Abu-El-Haija et al., 2019), SGC <ref type=\"bibr\" target=\"#b27\">(Wu et al., 2019)</ref>, DGI <ref type=\"bibr\" target=\"#b25\">(Velickovic et al., 2019)</ref>, S 2 GC <ref type=\"bibr\" target=\"#b34. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Gao &amp; Ji, 2019)</ref>, SAGPool <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref> and ASAP <ref type=\"bibr\" target=\"#b20\">(Ranjan et al., 2020)</ref>, unnecessarily cut nodes based on designe Gao &amp; Ji, 2019)</ref>, SAGPool <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref> and ASAP <ref type=\"bibr\" target=\"#b20\">(Ranjan et al., 2020)</ref>. Though efficient, this node-drop design  \">(Lee et al., 2019)</ref>, TopKPool <ref type=\"bibr\" target=\"#b8\">(Gao &amp; Ji, 2019)</ref>, ASAP <ref type=\"bibr\" target=\"#b20\">(Ranjan et al., 2020)</ref>, and Min-CutPool <ref type=\"bibr\" target= Gao &amp; Ji, 2019)</ref>, SAGPool <ref type=\"bibr\" target=\"#b12\">(Lee et al., 2019)</ref> and ASAP <ref type=\"bibr\" target=\"#b20\">(Ranjan et al., 2020)</ref>, and node clustering methods, including D tificially specified node compression quota <ref type=\"bibr\" target=\"#b8\">(Gao &amp; Ji, 2019;</ref><ref type=\"bibr\" target=\"#b20\">Ranjan et al., 2020)</ref>, which also exists in the node drop method. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 29\">(Xu et al., 2019)</ref>. In these works, a key direction is the convolutional mechanism of GNNs <ref type=\"bibr\" target=\"#b27\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Zhu &amp; Koniu ref>, APPNP <ref type=\"bibr\">(Klicpera et al., 2019)</ref>, MixHop (Abu-El-Haija et al., 2019), SGC <ref type=\"bibr\" target=\"#b27\">(Wu et al., 2019)</ref>, DGI <ref type=\"bibr\" target=\"#b25\">(Velickov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he collaboration ability of our pooling method with other GNNs. Specifically, we employ the ChebNet <ref type=\"bibr\" target=\"#b6\">(Defferrard et al., 2016)</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: erlying graph, while their effectiveness is interpreted by several well-accepted isolated concepts: <ref type=\"bibr\" target=\"#b46\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b57\">Zhu et al., 202 ny improvements on GNNs can be unified into the spectral smoothing operations, e.g. low-pass filter <ref type=\"bibr\" target=\"#b46\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b57\">Zhu et al., 202 simple filters, e.g. low-pass filter <ref type=\"bibr\" target=\"#b22\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019)</ref>, or the fixed filter coefficients <ref type=\"b  : R d \u00d1 R d 1 is the feature transformation neural network with the learnable parameters \u0398. In SGC <ref type=\"bibr\" target=\"#b46\">(Wu et al., 2019)</ref>, GDC <ref type=\"bibr\" target=\"#b24\">(Klicpera independent propagation and prediction steps <ref type=\"bibr\" target=\"#b29\">(Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., e=\"bibr\" target=\"#b7\">(Cai et al., 2021;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., on acts as the practical effective choice <ref type=\"bibr\" target=\"#b40\">(Shuman et al., 2013;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">NT &amp; Maehara. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e of simulating low/high pass filters; <ref type=\"bibr\" target=\"#b30\">(Ming Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b48\">Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 2020 et=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\" target=\"#b55\">Zhao &amp; Akoglu, 2020;</ref><ref type=\"bibr\" target=\"#b48\">Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b30\">Ming Chen et al. 2021)</ref>, alleviating oversmoothing (Ming <ref type=\"bibr\" target=\"#b30\">Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b48\">Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 2020 pends on the degree of the corresponding node, provided that the graph is irreducible and aperiodic <ref type=\"bibr\" target=\"#b48\">(Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b57\">Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b1\">Balcilar et al., 2021;</ref><ref type=\"bibr\" target=\"#b3\">Bo et al., 2021;</ref><ref type=\"bibr\" target=\"#b12\">Gao et al., 2021). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b28\">(Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b37\">Oono &amp; Suzuki, 2020;</ref><ref type=\"bibr\" target=\"#b38\">Rong et al., 2019;</ref><ref type=\"bibr\" target=\"#b18\">Huang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pproximation abilities of polynomial filters to better approximate the desired more complex filters <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard  h P R n is a graph signal that corresponds to one dimension of H.</p><p>Spectral Graph Convolution <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard ead><p>The graph filter is approximated by a polynomial in the theory of spectral graph convolution <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard proximated by a truncated expansion in terms of Chebyshev polynomials T k p \u039bq up to the k-th order <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011)</ref>, which is also the polynomials of \u039b,</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and ZINC. For TUDatasets, we follow the standard 10-fold cross-validation protocol and splits from <ref type=\"bibr\" target=\"#b53\">(Zhang et al., 2018)</ref> and report our results following the proto p; Zhang, 2017)</ref>, AWE <ref type=\"bibr\" target=\"#b19\">(Ivanov &amp; Burnaev, 2018)</ref>, DGCNN <ref type=\"bibr\" target=\"#b53\">(Zhang et al., 2018)</ref>, PSCN <ref type=\"bibr\" target=\"#b35\">(Niep . When the results are not given in the original paper, we report the best testing results given in <ref type=\"bibr\" target=\"#b53\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b19\">Ivanov &amp;  The learned representation is invariant to graph isomorphism (also known as permutation invariance <ref type=\"bibr\" target=\"#b53\">(Zaheer et al., 2017;</ref><ref type=\"bibr\" target=\"#b33\">Murphy et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and ZINC. For TUDatasets, we follow the standard 10-fold cross-validation protocol and splits from <ref type=\"bibr\" target=\"#b53\">(Zhang et al., 2018)</ref> and report our results following the proto p; Zhang, 2017)</ref>, AWE <ref type=\"bibr\" target=\"#b19\">(Ivanov &amp; Burnaev, 2018)</ref>, DGCNN <ref type=\"bibr\" target=\"#b53\">(Zhang et al., 2018)</ref>, PSCN <ref type=\"bibr\" target=\"#b35\">(Niep . When the results are not given in the original paper, we report the best testing results given in <ref type=\"bibr\" target=\"#b53\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b19\">Ivanov &amp;  The learned representation is invariant to graph isomorphism (also known as permutation invariance <ref type=\"bibr\" target=\"#b53\">(Zaheer et al., 2017;</ref><ref type=\"bibr\" target=\"#b33\">Murphy et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021;</ref><ref type=\"bibr\" target=\"#b1\">Balcilar et al. arget=\"#b10\">(Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b26\">Levie et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021;</ref><ref type=\"bibr\" target=\"#b16\">He et al., 202 l., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021;</ref><ref type=\"bibr\" target=\"#b1\">Balcilar et al. et=\"#b56\">Zhu &amp; Koniusz, 2020;</ref><ref type=\"bibr\" target=\"#b24\">Klicpera et al., 2019b;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021)</ref>.</p><p>sue. In the well-known oversmoothing  \" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b55\">Zhao &amp; Akoglu, 2020;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021)</ref>. Our analysis generalizes this result. In ou pera et al., 2019a)</ref>, SSGC <ref type=\"bibr\" target=\"#b56\">(Zhu &amp; Koniusz, 2020)</ref>, GPR <ref type=\"bibr\" target=\"#b8\">(Chien et al., 2021)</ref>, BernNet <ref type=\"bibr\" target=\"#b16\">(He. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b48\">Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Li et al., 2018)</ref> interpret it as ways of alleviating oversmooth pe=\"bibr\" target=\"#b48\">Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Li et al., 2018)</ref>, graph normalization <ref type=\"bibr\" target=\" ls over-correlated which is evidence of information loss.</p><p>Compared with oversmoothing studies <ref type=\"bibr\" target=\"#b28\">(Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b37\">Oono &amp; Suzu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b46\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b57\">Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bibr e=\"bibr\" target=\"#b29\">(Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\" target=\"#b56\">Zhu &amp; pe=\"bibr\" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\" target=\"#b55\">Zhao &amp e=\"bibr\" target=\"#b46\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b57\">Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bibr  2017;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019)</ref>, or the fixed filter coefficients <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2019a;</ref><ref type=\"bibr\">b)</ref> serve as the  g the nonlinear layer in GCNII (Ming <ref type=\"bibr\" target=\"#b30\">Chen et al., 2020)</ref>, APPNP <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2019a)</ref> and GCNII share the similar graph conv \"#b10\">(Defferrard et al., 2016</ref><ref type=\"bibr\">), CayleNet (Levie et al., 2019)</ref>, APPNP <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2019a)</ref>, SSGC <ref type=\"bibr\" target=\"#b56\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in the graph.</p><p>We prove Proposition 4.1 in Appendix D. Proposition 4.1 extends the results in <ref type=\"bibr\" target=\"#b42\">(Spielman, 2007)</ref>, showing that the normalization has a scaling . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8)</ref>, GatedGCN-PE <ref type=\"bibr\" target=\"#b5\">(Bresson &amp; Laurent, 2017)</ref>, MPNN (sum) <ref type=\"bibr\" target=\"#b13\">(Gilmer et al., 2017)</ref>, DeeperG <ref type=\"bibr\" target=\"#b27\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b52\">(Ying et al., 2018)</ref>, GIN <ref type=\"bibr\" target=\"#b49\">(Xu et al., 2019)</ref>, k-GNN <ref type=\"bibr\" target=\"#b31\">(Morris et al., 2019)</ref>, GraphSage <ref type=\"bibr\" target=\"#b14\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021;</ref><ref type=\"bibr\" target=\"#b1\">Balcilar et al. arget=\"#b10\">(Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b26\">Levie et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021;</ref><ref type=\"bibr\" target=\"#b16\">He et al., 202 l., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021;</ref><ref type=\"bibr\" target=\"#b1\">Balcilar et al. et=\"#b56\">Zhu &amp; Koniusz, 2020;</ref><ref type=\"bibr\" target=\"#b24\">Klicpera et al., 2019b;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021)</ref>.</p><p>sue. In the well-known oversmoothing  \" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b55\">Zhao &amp; Akoglu, 2020;</ref><ref type=\"bibr\" target=\"#b8\">Chien et al., 2021)</ref>. Our analysis generalizes this result. In ou pera et al., 2019a)</ref>, SSGC <ref type=\"bibr\" target=\"#b56\">(Zhu &amp; Koniusz, 2020)</ref>, GPR <ref type=\"bibr\" target=\"#b8\">(Chien et al., 2021)</ref>, BernNet <ref type=\"bibr\" target=\"#b16\">(He. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: shidze et al., 2009)</ref>, RW <ref type=\"bibr\" target=\"#b45\">(Vishwanathan et al., 2010)</ref>, PK <ref type=\"bibr\" target=\"#b34\">(Neumann et al., 2016)</ref>, FGSD <ref type=\"bibr\" target=\"#b44\">(Ve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>where W is learnable matrix and \u03c3 is nonlinear function.</p><p>Graph Diffusion Convolution (GDC) <ref type=\"bibr\" target=\"#b24\">(Klicpera et al., 2019b)</ref>. A generalized graph diffusion is give with the learnable parameters \u0398. In SGC <ref type=\"bibr\" target=\"#b46\">(Wu et al., 2019)</ref>, GDC <ref type=\"bibr\" target=\"#b24\">(Klicpera et al., 2019b)</ref>, SSGC <ref type=\"bibr\" target=\"#b56\">( ly designed coefficients to explicit modify spectrum, i.e. Personalized PageRank (PPR), heat kernel <ref type=\"bibr\" target=\"#b24\">(Klicpera et al., 2019b)</ref>, etc or the coefficients learned under et=\"#b30\">Ming Chen et al., 2020;</ref><ref type=\"bibr\" target=\"#b56\">Zhu &amp; Koniusz, 2020;</ref><ref type=\"bibr\" target=\"#b24\">Klicpera et al., 2019b;</ref><ref type=\"bibr\" target=\"#b8\">Chien et a rget=\"#b36\">NT &amp; Maehara, 2019;</ref><ref type=\"bibr\" target=\"#b32\">Muhammet et al., 2020;</ref><ref type=\"bibr\" target=\"#b24\">Klicpera et al., 2019b)</ref>. Although there are studies involving h e its spectrum is bounded by r\u00b41, 1s <ref type=\"bibr\" target=\"#b22\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b24\">Klicpera et al., 2019b)</ref> or the (symmetry) normalized L, i.e. I  ] and then rescale it to r0, 1s <ref type=\"bibr\" target=\"#b16\">(He et al., 2021)</ref>. free manner <ref type=\"bibr\" target=\"#b24\">(Klicpera et al., 2019b;</ref><ref type=\"bibr\" target=\"#b16\">He et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oximate the desired more complex filters <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard et al., 2016)</ref>. However, we also find that it cannot  n of H.</p><p>Spectral Graph Convolution <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard et al., 2016)</ref>. The definition of spectral graph conv the theory of spectral graph convolution <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard et al., 2016)</ref>. Although theoretically, one can appro nsive studies on the polynomial filters including the fixed coefficients and learnable coefficients <ref type=\"bibr\" target=\"#b10\">(Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b26\">Levie e form, we reformulate it with Eq. 6 as p \u03b3 pSq \" \u0159 k\u00b41 i\"0 \u03b1p1 \u00b4\u03b1q i S i `p1 \u00b4\u03b1q k S k . In ChebyNet <ref type=\"bibr\" target=\"#b10\">(Defferrard et al., 2016</ref><ref type=\"bibr\">), CayleNet (Levie et   al., 2019b)</ref>, etc or the coefficients learned under the constrained condition, i.e. Chebyshev <ref type=\"bibr\" target=\"#b10\">(Defferrard et al., 2016</ref><ref type=\"bibr\">), Cayley (Levie et al  underlying graph's matrix provides a unified interpretation on their effectiveness.</p><p>ChebyNet <ref type=\"bibr\" target=\"#b10\">(Defferrard et al., 2016</ref><ref type=\"bibr\">), CayleNet (Levie et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8)</ref>, GatedGCN-PE <ref type=\"bibr\" target=\"#b5\">(Bresson &amp; Laurent, 2017)</ref>, MPNN (sum) <ref type=\"bibr\" target=\"#b13\">(Gilmer et al., 2017)</ref>, DeeperG <ref type=\"bibr\" target=\"#b27\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8)</ref>, GatedGCN-PE <ref type=\"bibr\" target=\"#b5\">(Bresson &amp; Laurent, 2017)</ref>, MPNN (sum) <ref type=\"bibr\" target=\"#b13\">(Gilmer et al., 2017)</ref>, DeeperG <ref type=\"bibr\" target=\"#b27\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pproximation abilities of polynomial filters to better approximate the desired more complex filters <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard  h P R n is a graph signal that corresponds to one dimension of H.</p><p>Spectral Graph Convolution <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard ead><p>The graph filter is approximated by a polynomial in the theory of spectral graph convolution <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b10\">Defferrard proximated by a truncated expansion in terms of Chebyshev polynomials T k p \u039bq up to the k-th order <ref type=\"bibr\" target=\"#b15\">(Hammond et al., 2011)</ref>, which is also the polynomials of \u039b,</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in the graph.</p><p>We prove Proposition 4.1 in Appendix D. Proposition 4.1 extends the results in <ref type=\"bibr\" target=\"#b42\">(Spielman, 2007)</ref>, showing that the normalization has a scaling . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aderboard of ZINC, we control the number of parameters around 500K. The baseline models include: GK <ref type=\"bibr\" target=\"#b39\">(Shervashidze et al., 2009)</ref>, RW <ref type=\"bibr\" target=\"#b45\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b48\">Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Li et al., 2018)</ref> interpret it as ways of alleviating oversmooth pe=\"bibr\" target=\"#b48\">Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Li et al., 2018)</ref>, graph normalization <ref type=\"bibr\" target=\" ls over-correlated which is evidence of information loss.</p><p>Compared with oversmoothing studies <ref type=\"bibr\" target=\"#b28\">(Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b37\">Oono &amp; Suzu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2018)</ref> interpret it as ways of alleviating oversmoothing phenomenon in deep architectures; <ref type=\"bibr\" target=\"#b7\">(Cai et al., 2021)</ref> adopts the conception of normalization operat \">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b28\">Li et al., 2018)</ref>, graph normalization <ref type=\"bibr\" target=\"#b7\">(Cai et al., 2021)</ref>, etc, to evaluate the shrinking effects of D  20)</ref>. Most convergence analyses (such as over-smoothing) only study the simplified linear case <ref type=\"bibr\" target=\"#b7\">(Cai et al., 2021;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8)</ref>, GatedGCN-PE <ref type=\"bibr\" target=\"#b5\">(Bresson &amp; Laurent, 2017)</ref>, MPNN (sum) <ref type=\"bibr\" target=\"#b13\">(Gilmer et al., 2017)</ref>, DeeperG <ref type=\"bibr\" target=\"#b27\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b46\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b57\">Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bibr e=\"bibr\" target=\"#b29\">(Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\" target=\"#b56\">Zhu &amp; pe=\"bibr\" target=\"#b29\">Liu et al., 2020;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\" target=\"#b55\">Zhao &amp e=\"bibr\" target=\"#b46\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b57\">Zhu et al., 2021;</ref><ref type=\"bibr\" target=\"#b23\">Klicpera et al., 2019a;</ref><ref type=\"bibr\">b;</ref><ref type=\"bibr  2017;</ref><ref type=\"bibr\" target=\"#b46\">Wu et al., 2019)</ref>, or the fixed filter coefficients <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2019a;</ref><ref type=\"bibr\">b)</ref> serve as the  g the nonlinear layer in GCNII (Ming <ref type=\"bibr\" target=\"#b30\">Chen et al., 2020)</ref>, APPNP <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2019a)</ref> and GCNII share the similar graph conv \"#b10\">(Defferrard et al., 2016</ref><ref type=\"bibr\">), CayleNet (Levie et al., 2019)</ref>, APPNP <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2019a)</ref>, SSGC <ref type=\"bibr\" target=\"#b56\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: counterfactual representation learning <ref type=\"bibr\" target=\"#b24\">(Johansson et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Assaad et al., 2021)</ref>.</p><p>To force the distributions of repres target=\"#b62\">Wager &amp; Athey, 2018;</ref><ref type=\"bibr\" target=\"#b30\">Kuang et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Assaad et al., 2021)</ref>.</p><p>Graph Data Augmentation Graph data a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b54\">(Shalit et al., 2017;</ref><ref type=\"bibr\" target=\"#b32\">Li &amp; Fu, 2017;</ref><ref type=\"bibr\" target=\"#b76\">Yao et al., 2018;</ref><ref type=\"bibr\" target=\"#b77\">Yoon et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  al., 2020)</ref>, health care <ref type=\"bibr\" target=\"#b2\">(Alaa &amp; van der Schaar, 2017;</ref><ref type=\"bibr\" target=\"#b45\">Pawlowski et al., 2020)</ref>, and decision making <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  lookup and use them for link prediction <ref type=\"bibr\" target=\"#b46\">(Perozzi et al., 2014;</ref><ref type=\"bibr\" target=\"#b58\">Tang et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Grover &amp; L. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dered as treatments. Future work can leverage the rich structural information by bundled treatments <ref type=\"bibr\" target=\"#b89\">(Zou et al., 2020)</ref> in the generation of counterfactual links.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dered as treatments. Future work can leverage the rich structural information by bundled treatments <ref type=\"bibr\" target=\"#b89\">(Zou et al., 2020)</ref> in the generation of counterfactual links.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ocial network (FACEBOOK (McAuley &amp; Leskovec, 2012)), and drug-drug interaction network (OGB-DDI <ref type=\"bibr\" target=\"#b70\">(Wishart et al., 2018)</ref>) from the Open Graph Benchmark (OGB) <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elling, 2016a)</ref>, GSAGE <ref type=\"bibr\" target=\"#b18\">(Hamilton et al., 2017)</ref>, and JKNet <ref type=\"bibr\" target=\"#b73\">(Xu et al., 2018)</ref>. We compare the link prediction performance o d hierarchical clustering (Ward) <ref type=\"bibr\" target=\"#b68\">(Ward Jr, 1963)</ref>. We use JKNet <ref type=\"bibr\" target=\"#b73\">(Xu et al., 2018)</ref> as default graph encoder.</p><p>Implementatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t node features with graph structures unchanged. FLAG leverages \"free\" adversarial training methods <ref type=\"bibr\" target=\"#b30\">[31]</ref> to conduct efficient adversarial training so that it is hi l, we leverage the techniques below.</p><p>\"Free\" training. We leverage \"free\" adversarial training <ref type=\"bibr\" target=\"#b30\">[31]</ref> to craft adversarial data augmentations. PGD is a powerful. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b42\">[43]</ref> can all work with FLAG to further boost performance, while Cluster <ref type=\"bibr\" target=\"#b3\">[4]</ref> suffers an accuracy drop.</p><p>Compatibility with dropout. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation, while data augmentation should function regardless of tasks. Besides, the formulation of VAT <ref type=\"bibr\" target=\"#b27\">[28]</ref> utilized by these works involves both supervised clean and  that adversarial training could benefit standard accuracy <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. It is widely obser. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>.</p><p>In the meant. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: natural image datasets.</p><p>Observation 3: To illustrate, we provide a simple example on the Cora <ref type=\"bibr\" target=\"#b11\">[12]</ref> dataset. To simplify the scenario, we choose FGSM to craft. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation, while data augmentation should function regardless of tasks. Besides, the formulation of VAT <ref type=\"bibr\" target=\"#b27\">[28]</ref> utilized by these works involves both supervised clean and  that adversarial training could benefit standard accuracy <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. It is widely obser. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ut-of-distribution samples, and its efficacy has been verified in domains including computer vision <ref type=\"bibr\" target=\"#b39\">[40]</ref>, language understanding <ref type=\"bibr\" target=\"#b18\">[19 atibility with batch norm. Batch norm is appearing more and more frequently in top-performing GNNs. <ref type=\"bibr\" target=\"#b39\">[40]</ref> argued that there was a potential risk, that adversarial e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gradient ascent, it would be unnatural and suboptimal to add noises to discrete input node features <ref type=\"bibr\" target=\"#b44\">[45]</ref>. We firstly project discrete node features into the contin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0]</ref>. <ref type=\"bibr\">GraphAT [8]</ref>, BVAT <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and LAT <ref type=\"bibr\" target=\"#b19\">[20]</ref> are three semi-supervised methods on the node classificati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing/removing edges <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar inly focus on augmenting graph structures by modifying edges <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. <ref type=\"bibr\">G augmentation methods to illustrate<ref type=\"foot\" target=\"#foot_0\">1</ref> : (i) Neighbor sampling <ref type=\"bibr\" target=\"#b15\">[16]</ref> randomly samples neighbors for information aggregation. It s the backbone. From Table <ref type=\"table\" target=\"#tab_6\">8</ref>, we see that neighbor sampling <ref type=\"bibr\" target=\"#b15\">[16]</ref> and GraphSAINT <ref type=\"bibr\" target=\"#b42\">[43]</ref> c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the notion of data augmentation to GNNs. Transformations on images rely heavily on image structures <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and it is challenging to design low-cost transformations th t is optimized. The algorithm above is called PGD.</p><p>Multi-scale Augmentation. On visual tasks, <ref type=\"bibr\" target=\"#b2\">[3]</ref> highlighted the importance of using diverse types of data au. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> and recent work <ref type=\"bibr\" target=\"#b77\">[79,</ref><ref type=\"bibr\" target=\"#b87\">89]</ref> has solved some of er, we consider our previously-proposed native HPT design called Elastic Cuckoo Page Tables (ECPTs) <ref type=\"bibr\" target=\"#b77\">[79]</ref>. If we directly extend the ECPT design in a nested manner  Ts)</head><p>A design that addresses the shortcomings of HPTs is Elastic Cuckoo Page Tables (ECPTs) <ref type=\"bibr\" target=\"#b77\">[79]</ref>. ECPTs resolve hash collisions by using cuckoo hashing <re nment.</p><p>In this section, we build a design that directly incorporates the ECPT structures from <ref type=\"bibr\" target=\"#b77\">[79]</ref> into host and guest HPTs. We call the design Plain Nested  imit the number of parallel memory accesses issued. Then, we augment the design with special caches <ref type=\"bibr\" target=\"#b77\">[79]</ref> to minimize the number of parallel memory accesses. We use 0\"><head n=\"3.2\">Augmenting the Design with Caches</head><p>The ECPT design for native translations <ref type=\"bibr\" target=\"#b77\">[79]</ref> includes Cuckoo Walk Tables (CWTs), which are software str cesses due to subsequent fetches of the needed PTE CWT entries. As a result, the native ECPT design <ref type=\"bibr\" target=\"#b77\">[79]</ref> opted not to use a PTE CWT. Similarly, the Plain Nested EC f>), it may issue from 1 to \ud835\udc5b \u00d7 \ud835\udc51 accesses. The paper that introduced ECPTs for native environments <ref type=\"bibr\" target=\"#b77\">[79]</ref> used a naming convention to refer to the different possibl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. These approaches create translations that map very large c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b70\">72,</ref><ref type=\"bibr\" target=\"#b74\">76]</ref>. To reduce TLB miss. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. With TLB access times already overtaking those of the L2 c get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. These approaches c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> reports the estimated area and power of these structures. For the measurements, we use Cacti <ref type=\"bibr\" target=\"#b11\">[12]</ref> with 22nm technology. From the table, we see that these st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: porting huge pages <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b64\">66,</ref><ref type=\"bibr\" target=\"#b65\">67,</ref><ref type=\"bibr\" target=\"#b82\">84]</ref> and hardware cachin  huge page support <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b64\">66,</ref><ref type=\"bibr\" target=\"#b65\">67]</ref> have been proposed.</p><p>Moreover, other designs have been. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b81\">83,</ref><ref type=\"bibr\" target=\"#b87\">89]</ref>. HPTs fundamentally eliminate the sequential steps of radix f><ref type=\"bibr\" target=\"#b49\">50]</ref> and recent work <ref type=\"bibr\" target=\"#b77\">[79,</ref><ref type=\"bibr\" target=\"#b87\">89]</ref> has solved some of their traditional shortcomings <ref type ional shortcomings <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b87\">89]</ref>.</p><p>In particular, in this paper, we consider our previo nvironment that uses HPTs, only three memory references are needed for a nested address translation <ref type=\"bibr\" target=\"#b87\">[89]</ref>-again unrealistically assuming no hash collisions, no mult o walk a collision chain <ref type=\"bibr\" target=\"#b13\">[14]</ref>, open-addressed hash table slots <ref type=\"bibr\" target=\"#b87\">[89]</ref>, or invoking the OS <ref type=\"bibr\" target=\"#b26\">[27,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of studies have measured the overhead of nested page table walks for virtualized memory translation <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target irtualized memory <ref type=\"bibr\" target=\"#b38\">[39]</ref>, application-managed memory translation <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and optimized huge page support <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. ECPTs resolve hash collisions by using cuckoo hashing <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b63\">65]</ref>. A target entry can be in one of \ud835\udc51 locations in a \ud835\udc51-way (or s until, in practically all cases, all entries find a slot <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b63\">65]</ref>.</p><p>ECPTs use process-private HPTs and, hence, support b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. With TLB access times already overtaking those of the L2 c get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>. These approaches c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of a program's access pattern in memory system optimization <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. For instance, page coloring places virtual pages that may  ent locations of cache <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, heterogeneous memories <ref type=\"bibr\" target=\"#b11\">[12, roaches use OS support <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar ained page granularity <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. However, as shown  tatic program analysis <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar ave different patterns <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b20\">(21)</ref><ref type=\"bibr\" target=\"#b21\">(22)</ref><ref type=\"bibr\" target=\"#b22\">(23)</ref><ref type=\"bibr\" target=\"#b23\">(24)</ref><ref type=\"bibr\" target=\"#b24\">(25)</ref><ref type=\"bibr\" t , we run 19 applications to collect variable-level statistics of SPEC2006 and PARSEC. As defined in <ref type=\"bibr\" target=\"#b23\">[24]</ref>, a variable is the reference symbol in the program for a p  tools as described in <ref type=\"bibr\" target=\"#b47\">[48]</ref> (identified by call-stack matching <ref type=\"bibr\" target=\"#b23\">[24]</ref>). The call-stack matching has two passes. In the first pas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ngle address mapping <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> and apply one globa =\"#b26\">(27)</ref><ref type=\"bibr\" target=\"#b27\">(28)</ref><ref type=\"bibr\" target=\"#b28\">(29)</ref><ref type=\"bibr\" target=\"#b29\">(30)</ref><ref type=\"bibr\" target=\"#b30\">(31)</ref><ref type=\"bibr\" t ex address mapping schemes: bit-shuffle address mapping(BSM) and hashing-based address mapping (HM) <ref type=\"bibr\" target=\"#b29\">[30]</ref>. The bit-shuffle approach rearranges the order of address  dress mapping for all applications but applies a different optimization method, i.e., using hashing <ref type=\"bibr\" target=\"#b29\">[30]</ref> for address mapping selection. The selected hash function   BSM, HM does not rely on profiling. In this configuration, we refer to the method in a recent work <ref type=\"bibr\" target=\"#b29\">[30]</ref> which provides a good balance between implementation compl  our study, we found theoretically perfect hashing function leads to marginal speedup (&lt;3%) over <ref type=\"bibr\" target=\"#b29\">[30]</ref> at the cost of significantly increased overhead. We defer  ent access behaviors, as it takes many address bits as input to cover a majority of access patterns <ref type=\"bibr\" target=\"#b29\">[30]</ref>. We plot the distribution of the CLP utilization of differ LP on CPU <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> and GPU <ref type=\"bibr\" target=\"#b29\">[30]</ref>, which could be extended to improve CLP. However, this one e CPUs <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and GPUs <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Bit-shuffle rearranges the order of the address bits that   XOR of several address bits to increase the entropy of the bank address bits to improve BLP on CPU <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> and GPU <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b17\">(18)</ref><ref type=\"bibr\" target=\"#b18\">(19)</ref><ref type=\"bibr\" target=\"#b19\">(20)</ref><ref type=\"bibr\" target=\"#b20\">(21)</ref><ref type=\"bibr\" target=\"#b21\">(22)</ref><ref type=\"bibr\" t ge-scale graph processing (Breadth-First Search <ref type=\"bibr\" target=\"#b46\">[47]</ref>, PageRank <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Single-Source Shortest Path <ref type=\"bibr\" target=\"#b33. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b15\">(16)</ref><ref type=\"bibr\" target=\"#b16\">(17)</ref><ref type=\"bibr\" target=\"#b17\">(18)</ref><ref type=\"bibr\" target=\"#b18\">(19)</ref><ref type=\"bibr\" target=\"#b19\">(20)</ref><ref type=\"bibr\" t the cache-line size of the RISC-V processor (64B). SPEC2006: We studied all 12 integer applications <ref type=\"bibr\" target=\"#b18\">[19]</ref>. PARSEC: PARSEC <ref type=\"bibr\" target=\"#b5\">[6]</ref> is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b20\">(21)</ref><ref type=\"bibr\" target=\"#b21\">(22)</ref><ref type=\"bibr\" target=\"#b22\">(23)</ref><ref type=\"bibr\" target=\"#b23\">(24)</ref><ref type=\"bibr\" target=\"#b24\">(25)</ref><ref type=\"bibr\" t , we run 19 applications to collect variable-level statistics of SPEC2006 and PARSEC. As defined in <ref type=\"bibr\" target=\"#b23\">[24]</ref>, a variable is the reference symbol in the program for a p  tools as described in <ref type=\"bibr\" target=\"#b47\">[48]</ref> (identified by call-stack matching <ref type=\"bibr\" target=\"#b23\">[24]</ref>). The call-stack matching has two passes. In the first pas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> to more intelligently place data in physical memory by chan target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. However, as shown in Fig. <ref type=\"figure\" target=\"#fig_ get=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> and provide mechanisms to migrate performance critical data get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, dynamic profiling <ref type=\"bibr\" target=\"#b41\">[42,</ref get=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> and NUMA nodes <ref type=\"bibr\" target=\"#b27\">[28,</ref><re ef><ref type=\"bibr\" target=\"#b41\">42]</ref> and NUMA nodes <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> to better exploit MLP and/or to improve the locality. Howev get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Moreover, these works can only control the data-placement  get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. However, as discussed in Section 3, to best exploit CLP in <ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, dynamic profiling <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> or a combination o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l isolation between data belonging to different security domains, following the same methodology in <ref type=\"bibr\" target=\"#b6\">[7]</ref>. More detailed study on extending SDAM to address security c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l parallelism (RLP) to exploit parallelism across channels, banks and within a row. Among them, CLP <ref type=\"bibr\" target=\"#b15\">(16)</ref><ref type=\"bibr\" target=\"#b16\">(17)</ref><ref type=\"bibr\" t echanism keeps separate memory pools (arena in glibc) for each thread to reduce the lock contention <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>As in Fig. <ref type=\"figure\">8</ref>, in the first. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b24\">(25)</ref><ref type=\"bibr\" target=\"#b25\">(26)</ref><ref type=\"bibr\" target=\"#b26\">(27)</ref><ref type=\"bibr\" target=\"#b27\">(28)</ref><ref type=\"bibr\" target=\"#b28\">(29)</ref><ref type=\"bibr\" t ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> and NUMA nodes <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> to better exploit  target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l parallelism (RLP) to exploit parallelism across channels, banks and within a row. Among them, CLP <ref type=\"bibr\" target=\"#b15\">(16)</ref><ref type=\"bibr\" target=\"#b16\">(17)</ref><ref type=\"bibr\" t echanism keeps separate memory pools (arena in glibc) for each thread to reduce the lock contention <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>As in Fig. <ref type=\"figure\">8</ref>, in the first. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o generate complex head motions and poses, related methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref> decouple appearance and motion information, and use a dense  criminator to ensure the smooth transition of consecutive output frames. Additionally, Wiles et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> propose a network X2Face that uses a dense motion field to g the-art baselines <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>. For a fair compa n Fig. <ref type=\"figure\" target=\"#fig_13\">12 (c</ref>)(e), the results generated from Wiles et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Siarohin et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>  nd low resolution facial images. Compared to these methods <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, our method can generate more precise mouth movements and cl  of our generated results with Kumar et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref>, Wiles et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref>, Chen et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>, Siaroh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#foot_1\">1</ref> are firstly generated by force-alignment with Hidden Markov Model Toolkit <ref type=\"bibr\" target=\"#b46\">[46]</ref>. In such labels, we can obtain the fully-context phoneme i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion. Essentially, facial animation synthesis is a temporaldependent problem, whereas existing works <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref> tend to formula. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly detailed features within internal representations of face images. Hence, a self-attention module <ref type=\"bibr\" target=\"#b53\">[53]</ref> is employed to learn highly detailed features across face . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l dubbing <ref type=\"bibr\" target=\"#b8\">[9]</ref>, bandwidth reduction in video coding/transmission <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b12\">[13]</ref> and human-comput. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le-modal analysis of facial animation, which focuses on mapping audio to realistic facial movements <ref type=\"bibr\" target=\"#b14\">[15]</ref>- <ref type=\"bibr\" target=\"#b22\">[22]</ref>, as shown in Fi  is called articulator synergy <ref type=\"bibr\" target=\"#b26\">[26]</ref>. However, existing methods <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bib synchronization <ref type=\"bibr\" target=\"#b40\">[40]</ref>. Given an audio clip, Suwajanakorn et al. <ref type=\"bibr\" target=\"#b14\">[15]</ref> first introduce a time-delayed long short-term memory (LST satisfactory visual quality.</p><p>Inspired by the methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we also utiliz  process these input sequential data. Compared to the canonical recurrent architectures (e.g., LSTM <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>), TCN does not  th that of a single-layer, unidirectional LSTM, which is often adopted to keypoint prediction tasks <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In Table <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[47]</ref>. And then, for each frontalized face, the mouth landmarks are detected by dlib detector <ref type=\"bibr\" target=\"#b48\">[48]</ref> which gives 33 keypoints along the outer and inner contour ed as follows.</p><p>1) Face Sketch Generation.: For face sketch generation, the dlib face detector <ref type=\"bibr\" target=\"#b48\">[48]</ref> is firstly used to detect 68 facial keypoints from each im  mouth part. Note that, during training, these lip landmarks are detected by the dlib face detector <ref type=\"bibr\" target=\"#b48\">[48]</ref> (ground truth), while during test, these lip landmarks are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l.</p><p>1) Data Processing: For acoustic features, 40-dimensional Mel-Cepstral Coefficients (MCEP) <ref type=\"bibr\" target=\"#b43\">[43]</ref> and 1-dimensional logenergy, with delta and delta-delta fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the mouth area), which often have a large motion and only occupy a small portion of a facial image <ref type=\"bibr\" target=\"#b30\">[30]</ref>. Consequently, the generated mouth area tends to be blurry or manipulating or generating images/videos with a high level of realism. For instance, Wang et al. <ref type=\"bibr\" target=\"#b30\">[30]</ref> propose a novel coarse-to-fine network to learn a mapping  photo-realistic video frames conditioned on the predicted mouth landmarks. In VCN, the optical flow <ref type=\"bibr\" target=\"#b30\">[30]</ref> is employed to model the temporal dependency between video neration is a temporal-dependent task. Thus, to model the inter-frame consistency, the optical flow <ref type=\"bibr\" target=\"#b30\">[30]</ref> is adopted in VCN to generate temporally coherent videos a ecture is also adopted to ensure the temporal dynamics between consecutive synthesized video frames <ref type=\"bibr\" target=\"#b30\">[30]</ref>. The D V should output 0 for a fake pair (x t\u22121 t\u2212K , w t\u2212. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iven large quantities of unlabeled videos, related works <ref type=\"bibr\" target=\"#b32\">[32]</ref>- <ref type=\"bibr\" target=\"#b34\">[34]</ref>, <ref type=\"bibr\" target=\"#b37\">[37]</ref>, <ref type=\"bib y appearance-based approaches. Different from <ref type=\"bibr\" target=\"#b37\">[37]</ref>, Gan et al. <ref type=\"bibr\" target=\"#b34\">[34]</ref> propose a keypoint-based structured representation to expl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e production profiles to reorder basic-blocks and functions in a profile-guided manner. Prior works <ref type=\"bibr\" target=\"#b8\">[11,</ref><ref type=\"bibr\" target=\"#b22\">25]</ref> have shown the effe ughput) through traditional compiler-based optimizations. Measuring profile similarity. Prior works <ref type=\"bibr\" target=\"#b8\">[11,</ref><ref type=\"bibr\" target=\"#b11\">14,</ref><ref type=\"bibr\" tar these techniques are widely deployed in today's data centers <ref type=\"bibr\" target=\"#b6\">[9,</ref><ref type=\"bibr\" target=\"#b8\">11]</ref>.</p><p>Despite the widespread adoption of PGO techniques for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: educe a kernel's memory footprint for specific applications <ref type=\"bibr\" target=\"#b7\">[10,</ref><ref type=\"bibr\" target=\"#b15\">18]</ref>. Our work focuses solely on improving performance (i.e., ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: struction footprints due to complex and deep software stacks <ref type=\"bibr\" target=\"#b5\">[8,</ref><ref type=\"bibr\" target=\"#b6\">9,</ref><ref type=\"bibr\" target=\"#b9\">12,</ref><ref type=\"bibr\" target for optimizing these applications in a profile-guided manner <ref type=\"bibr\" target=\"#b5\">[8,</ref><ref type=\"bibr\" target=\"#b6\">9,</ref><ref type=\"bibr\" target=\"#b9\">12,</ref><ref type=\"bibr\" target  the typical instruction footprints for these applications range from tens to hundreds of megabytes <ref type=\"bibr\" target=\"#b6\">[9,</ref><ref type=\"bibr\" target=\"#b23\">26]</ref>. On the other hand,  ata center applications. Consequently, these techniques are widely deployed in today's data centers <ref type=\"bibr\" target=\"#b6\">[9,</ref><ref type=\"bibr\" target=\"#b8\">11]</ref>.</p><p>Despite the wi ef type=\"bibr\" target=\"#b44\">47]</ref> and incur millions of dollars in energy and management costs <ref type=\"bibr\" target=\"#b6\">[9]</ref>. Consequently, even a single-digit speedup for these widely-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: locks and functions in a profile-guided manner. Prior works <ref type=\"bibr\" target=\"#b8\">[11,</ref><ref type=\"bibr\" target=\"#b22\">25]</ref> have shown the effectiveness of these profileguided optimiz. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\">[5,</ref><ref type=\"bibr\" target=\"#b4\">7,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b40\">[43]</ref><ref type=\"bibr\" target=\"#b41\">[44]</ref><ref type=\"bibr\" t  the kernel binary <ref type=\"bibr\" target=\"#b24\">[27,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b40\">[43]</ref><ref type=\"bibr\" target=\"#b41\">[44]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b12\">15,</ref><ref type=\"bibr\" target=\"#b20\">23,</ref><ref type=\"bibr\" target=\"#b23\">26,</ref><ref type=\"bibr\" target=\"#b27\">30]</ref>. Recent studies from Google and Facebook estimate that the  target=\"#b6\">9,</ref><ref type=\"bibr\" target=\"#b9\">12,</ref><ref type=\"bibr\" target=\"#b12\">15,</ref><ref type=\"bibr\" target=\"#b27\">30]</ref>. Consequently, a plethora of recent techniques <ref type=\"b substantially reduces data centers' Total Cost of Ownership (TCO) and planet-scale carbon footprint <ref type=\"bibr\" target=\"#b27\">[30]</ref>.</p><p>The significance of reducing I-cache and I-TLB miss. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b8\">[11,</ref><ref type=\"bibr\" target=\"#b11\">14,</ref><ref type=\"bibr\" target=\"#b28\">31,</ref><ref type=\"bibr\" target=\"#b43\">46]</ref> on measuring profile similarity and diversity primarily inv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b12\">15,</ref><ref type=\"bibr\" target=\"#b20\">23,</ref><ref type=\"bibr\" target=\"#b23\">26,</ref><ref type=\"bibr\" target=\"#b27\">30]</ref>. Recent studies from Google and Facebook estimate that the  target=\"#b6\">9,</ref><ref type=\"bibr\" target=\"#b9\">12,</ref><ref type=\"bibr\" target=\"#b12\">15,</ref><ref type=\"bibr\" target=\"#b27\">30]</ref>. Consequently, a plethora of recent techniques <ref type=\"b substantially reduces data centers' Total Cost of Ownership (TCO) and planet-scale carbon footprint <ref type=\"bibr\" target=\"#b27\">[30]</ref>.</p><p>The significance of reducing I-cache and I-TLB miss. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h academic and industry researchers <ref type=\"bibr\">[5,</ref><ref type=\"bibr\" target=\"#b4\">7,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b40\">[43]</ref><ref type=\"bibr\" ta plicationspecific kernels and re-writing the kernel binary <ref type=\"bibr\" target=\"#b24\">[27,</ref><ref type=\"bibr\" target=\"#b26\">29,</ref><ref type=\"bibr\" target=\"#b40\">[43]</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and Leskovec 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b19\">(Velickovic et al. 2018)</ref>, MixHop <ref type=\"bibr\" target=\"#b0\">(Abu-El-Haija et al. 2019)</ref>, and HIN <ref type=\"bibr\" target=\"#b2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref>.</p><p>Although many GCN-based methods have been proposed in recent years, such as GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ks and traffic networks) are ubiquitous structures that can model relational data. Network analysis <ref type=\"bibr\" target=\"#b22\">(Wang et al. 2016</ref><ref type=\"bibr\" target=\"#b20\">(Wang et al. , . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>.</p><p>Although many GCN-based methods have been proposed in recent years, such as GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>.</p><p>Although many GCN-based methods have been proposed in recent years, such as GraphSage <ref type=\"bibr\" target=\"#b5\">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: combination of intermediate representations, to boost learning from graph with heterophily. GPR-GNN <ref type=\"bibr\" target=\"#b2\">(Chien et al. 2021</ref>) deals with heterophily and oversmoothing by  b26\">(Zhu et al. 2020)</ref>, CPGNN <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2021)</ref>, GPR-GNN <ref type=\"bibr\" target=\"#b2\">(Chien et al. 2021</ref>) and AM-GCN <ref type=\"bibr\" target=\"#b21\">(W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and distant nodes that have a certain similarity with the target node in a continuous space. H2GCN <ref type=\"bibr\" target=\"#b26\">(Zhu et al. 2020)</ref> applies some key designs, such as higher-orde often believed that the 2-hop neighborhoods of a node v is always homophily-dominant in expectation <ref type=\"bibr\" target=\"#b26\">(Zhu et al. 2020)</ref>. So, we also set k = 2 in this work since it  models tackling heterophily: Geom-GCN <ref type=\"bibr\" target=\"#b14\">(Pei et al. 2020</ref>), H2GCN <ref type=\"bibr\" target=\"#b26\">(Zhu et al. 2020)</ref>, CPGNN <ref type=\"bibr\" target=\"#b25\">(Zhu et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Wisconsin <ref type=\"bibr\" target=\"#b14\">(Pei et al. 2020)</ref>, and a film industry dataset Film <ref type=\"bibr\" target=\"#b17\">(Tang et al. 2009</ref>). The homophilic networks include Cora, Cites. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b24\">Yan et al. 2019)</ref>. Recently, graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b11\">(Kipf and Welling 2017)</ref>, which exhibits significant power on pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: combination of intermediate representations, to boost learning from graph with heterophily. GPR-GNN <ref type=\"bibr\" target=\"#b2\">(Chien et al. 2021</ref>) deals with heterophily and oversmoothing by  b26\">(Zhu et al. 2020)</ref>, CPGNN <ref type=\"bibr\" target=\"#b25\">(Zhu et al. 2021)</ref>, GPR-GNN <ref type=\"bibr\" target=\"#b2\">(Chien et al. 2021</ref>) and AM-GCN <ref type=\"bibr\" target=\"#b21\">(W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  et al. 2018)</ref>, MixHop <ref type=\"bibr\" target=\"#b0\">(Abu-El-Haija et al. 2019)</ref>, and HIN <ref type=\"bibr\" target=\"#b23\">(Xu et al. 2019)</ref>, they implicitly assume that most connected no phily and oversmoothing by combining each step of feature propagation with a learnable weight. GGCN <ref type=\"bibr\" target=\"#b23\">(Yan et al. 2021</ref>) al-lows negative message propagation between . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re prefetching incurs overheads due to code-bloating-up to 8.5\u00d7 the instruction count in innerloops <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Prefetching might thrash the L1 too with large blocks or un curs an instruction overhead to calculate the address of the target prefetch and other book-keeping <ref type=\"bibr\" target=\"#b1\">[2]</ref>. To remove that overhead, MAPLE can prefetch Loops of Indire  structures and coherence. Recent automatic compiler techniques already target software prefetching <ref type=\"bibr\" target=\"#b1\">[2]</ref> for (i=0; i&lt;N; i++) res and slice decoupled programs <ref br\" target=\"#b65\">[66]</ref>. Software prefetching has been shown effective for pointer indirection <ref type=\"bibr\" target=\"#b1\">[2]</ref>, aided by compiler techniques to automatically insert prefet  is targeted to an address composed of MAPLE's instance base address, queue ID, and operation code; <ref type=\"bibr\" target=\"#b1\">(2)</ref> The decoder identifies the operation as a pointer-produce, a e is targeted to an address composed of MAPLE's instance base address, queue ID, and operation code;<ref type=\"bibr\" target=\"#b1\">(2)</ref> The decoder identifies the operation as a pointer-produce, a bine with MAPLE. For example, we envision leveraging existing compiler techniques to target its API <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. This paper combines. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>, the need for new ISA instructions <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" ta  target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b56\">57]</ref>, and decouple access-execute (DAE) <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" ta r computation, it can act as a non-speculative perfect prefetcher. Among other decoupling proposals <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta arget=\"#b54\">55]</ref>, or even having both Access and Execute as physical threads in the same core <ref type=\"bibr\" target=\"#b14\">[15]</ref>. DeSC <ref type=\"bibr\" target=\"#b21\">[22]</ref> introduces. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , but traditionally does not work for IMAs. Recent proposals <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b61\">62]</ref> achieve better perfor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: SPMM): Performs a matrix multiplication between two sparse matrices A and B in a layer-wise fashion <ref type=\"bibr\" target=\"#b38\">[39]</ref> to train a sparse deep neural network. This kernel is para. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: an open-source tile-based manycore architecture, which supports multiple ISAs. We use RISC-V Ariane <ref type=\"bibr\" target=\"#b62\">[63]</ref> cores to demonstrate how latency tolerance can be achieved lar queues sharing a 1KB scratchpad represents 1.1% of the area of the single-issue in-order Ariane <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b63\">64]</ref> cores it can suppl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w ISA instructions <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" tar oO to hoist accesses and thus mitigate the latency of IMAs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>. However, we choose cess-execute (DAE) <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. We accommodate the actice.</p><p>Software latency tolerance often uses compiler knowledge to improve performance. DSWP <ref type=\"bibr\" target=\"#b44\">[45]</ref> does automatic software pipelining without speculation by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to include in real chips, but that is often not the case due to complex core or cache modifications <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" targ  provided RTL implementations. Since implementing in RTL the related work that we wanted to compare <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> would take months, e ncluding DeSC decoupling <ref type=\"bibr\" target=\"#b21\">[22]</ref> and DROPLET hardware prefetching <ref type=\"bibr\" target=\"#b8\">[9]</ref>, via system simulation. To do this we leverage MosaicSim <re ormance of MAPLE decoupling, DeSC <ref type=\"bibr\" target=\"#b21\">[22]</ref> decoupling, and DROPLET <ref type=\"bibr\" target=\"#b8\">[9]</ref> hardware prefetching, as well as that of traditional doall p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Prior work has leveraged SMT and beefy OoO to hoist accesses and thus mitigate the latency of IMAs <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" ta tion windows by using a secondary thread of execution to improve the performance of the main thread <ref type=\"bibr\" target=\"#b34\">[35]</ref>. This thread is either programmer <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: HP uses matrices from SuiteSparse <ref type=\"bibr\" target=\"#b17\">[18]</ref> and a Kronecker network <ref type=\"bibr\" target=\"#b31\">[32]</ref>, BFS operates on Wikipedia, YouTube, and LiveJournal graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng <ref type=\"bibr\" target=\"#b8\">[9]</ref>, via system simulation. To do this we leverage MosaicSim <ref type=\"bibr\" target=\"#b37\">[38]</ref>, a simulator for heterogeneous architectures and hardware-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplicability and portability problems, especially in the context of heterogeneous-ISA architectures <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. (4) Hardware-only t cores <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> and hybrid ISAs <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>.</p></div> <div xmln. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  cannot be met by traditional electronics due to their conformality, thinness, or cost requirements <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Flexible electronic d ively flexible microprocessors has been a challenge and very few such processors have been reported <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" targ , most previous efforts in flexible microprocessors have featured fixed or factoryprogrammable ROMs <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b69\">69]</ref>, while FlexiCores ca b38\">[39,</ref><ref type=\"bibr\" target=\"#b97\">97]</ref>. Several prior works on flexible processors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> also use this techno ty of microprocessors make them a critical electronic component even for the applications we target <ref type=\"bibr\" target=\"#b3\">[4]</ref>. There  have been several attempts at integrating thinned si otprint, high yielding, field reprogrammable flexible processors. We observe that while recent work <ref type=\"bibr\" target=\"#b3\">[4]</ref> demonstrated a 32-bit Indium-Gallium-Zinc-Oxide (IGZO)-based ref type=\"bibr\" target=\"#b74\">[74]</ref> report a flexible machine learning classifier. Biggs et al <ref type=\"bibr\" target=\"#b3\">[4]</ref> report a 32-bit natively flexible ARM-v6m microprocessor.</p irst, the devices have poor noise margin, high power consumption, and significant variation in \ud835\udc49 \ud835\udc61\u210e <ref type=\"bibr\" target=\"#b3\">[4]</ref>, all of which lead to yield and energy efficiency concerns.   Second, the technology is several generations behind silicon-based CMOS in terms of size and speed <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which leads to footprint concerns and limits applications t licon-based CMOS, nearly all power consumption (&gt;99%) in 0.8 \u00b5m IGZO is static power consumption <ref type=\"bibr\" target=\"#b3\">[4]</ref>. This requires power reduction to be achieved primarily thro nd a description of their semantics. FlexiCore4 supports a four-bit datapath (vs 32-bit datapath in <ref type=\"bibr\" target=\"#b3\">[4]</ref> and 8-bit datapaths in <ref type=\"bibr\" target=\"#b69\">[69]</ rogram ROM whose contents are determined at tape-out (via mask ROM as in the flexible ARM processor <ref type=\"bibr\" target=\"#b3\">[4]</ref>) or post-manufacturing (via write-once, read many laser prog re listed in Table <ref type=\"table\" target=\"#tab_6\">7</ref>. The fabricated 'PlasticARM' processor <ref type=\"bibr\" target=\"#b3\">[4]</ref> uses a mask ROM for programs and is, therefore, not programm n in device count led to a &gt; 90% reduction in static power for FlexiCore4 compared to PlasticARM <ref type=\"bibr\" target=\"#b3\">[4]</ref>. This ratio should be similar for electronics in traditional. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b8\">[9]</ref>, pharmaceuticals <ref type=\"bibr\" target=\"#b1\">[2]</ref>, agriculture and forestry <ref type=\"bibr\" target=\"#b86\">[86]</ref>, and environment <ref type=\"bibr\" target=\"#b60\">[60]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >There are other works on tiny, low gate count processors with high variation and high static power <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b43\">43,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b1\">[2]</ref>, agriculture and forestry <ref type=\"bibr\" target=\"#b86\">[86]</ref>, and environment <ref type=\"bibr\" target=\"#b60\">[60]</ref> -have not seen much penetration of computing.</p><p>The pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s such as smart bandages <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and disposable sensors for food <ref type=\"bibr\" target=\"#b8\">[9]</ref>, pharmaceuticals <ref type=\"bibr\" target=\"#b1\">[2]</ref>, ag. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lso widely used in flexible sensors, including gas sensors <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b89\">89]</ref>, pressure sensors <ref type=\"bibr\" target=\"#b96\">[96]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s such as smart bandages <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and disposable sensors for food <ref type=\"bibr\" target=\"#b8\">[9]</ref>, pharmaceuticals <ref type=\"bibr\" target=\"#b1\">[2]</ref>, ag. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Prefetching I-cache lines corresponding to these future accesses avoids potential frontend stalls <ref type=\"bibr\" target=\"#b82\">[83,</ref><ref type=\"bibr\" target=\"#b83\">84]</ref>, providing perform \"bibr\" target=\"#b71\">72,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>, and BTBdirected prefetchers <ref type=\"bibr\" target=\"#b82\">[83,</ref><ref type=\"bibr\" target=\"#b83\">84]</ref>. These techniques  e. Prefetching I-cache lines corresponding to these future accesses avoids potential frontend stalls<ref type=\"bibr\" target=\"#b82\">[83,</ref><ref type=\"bibr\" target=\"#b83\">84]</ref>, providing perform et=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b83\">84,</ref><ref type=\"bibr\" tar FDIP's prefetching <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b83\">84]</ref> or cause FDIP to pr . As we and others <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b83\">84,</ref><ref type=\"bibr\" tar  the most efficient manner (as also reported by prior work <ref type=\"bibr\" target=\"#b74\">[75,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b83\">84,</ref><ref type=\"bibr\" tar et=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b83\">84,</ref><ref type=\"bibr\" tar  FDIP's prefetching<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b83\">84]</ref> or cause FDIP to pr >. As we and others<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b83\">84,</ref><ref type=\"bibr\" tar victions. As a result, existing BTB prefetching mechanisms <ref type=\"bibr\" target=\"#b72\">[73,</ref><ref type=\"bibr\" target=\"#b82\">83]</ref> fall short as they bring in unused branch entries into the  hat Confluence <ref type=\"bibr\" target=\"#b72\">[73]</ref>   <ref type=\"bibr\" target=\"#b72\">[73,</ref><ref type=\"bibr\" target=\"#b82\">83]</ref> provide merely 1.4% mean speedup, while a perfect BTB offer egradation induced by these prior BTB prefetching policies <ref type=\"bibr\" target=\"#b72\">[73,</ref><ref type=\"bibr\" target=\"#b82\">83]</ref>. First, like any temporal prefetcher <ref type=\"bibr\" targe  replacement policy that is more optimized than prior work <ref type=\"bibr\" target=\"#b72\">[73,</ref><ref type=\"bibr\" target=\"#b82\">83]</ref> can better close the performance gap between a baseline and  buffer used by state-of-the-art BTB prefetching solutions <ref type=\"bibr\" target=\"#b74\">[75,</ref><ref type=\"bibr\" target=\"#b82\">83]</ref>. return \ud835\udc65 0 \u22b2 Bypass 7: \ud835\udc67 \u2190 the least recently used branch  evictions. As a result, existing BTB prefetching mechanisms<ref type=\"bibr\" target=\"#b72\">[73,</ref><ref type=\"bibr\" target=\"#b82\">83]</ref> fall short as they bring in unused branch entries into the  ion techniques. Prior work such as Confluence <ref type=\"bibr\" target=\"#b72\">[73]</ref> and Shotgun <ref type=\"bibr\" target=\"#b82\">[83]</ref> use BTB prefetching to reduce BTB misses and improve FDIP  61]</ref> significantly reduces this performance gap by providing 10.4% mean speedup.</p><p>Shotgun <ref type=\"bibr\" target=\"#b82\">[83]</ref> faces a slight slowdown as it wastes critical BTB capacity. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ad (only up to 1% <ref type=\"bibr\" target=\"#b68\">[69]</ref><ref type=\"bibr\" target=\"#b69\">[70]</ref><ref type=\"bibr\" target=\"#b70\">[71]</ref><ref type=\"bibr\" target=\"#b150\">151]</ref>) and widespread . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: well only as long as the Branch Target Buffer (BTB) supplies correct targets for all taken branches <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" ta  found that FDIP's performance is significantly limited by BTB misses that stall FDIP's prefetching <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" ta ef type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b131\">132]</ref>. As we and others <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" ta n as it wastes critical BTB capacity to store unused prefetch metadata. We corroborate the findings <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" ta  match the working set sizes of conditional and unconditional branches for data center applications <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75]</ref>. Third, Shotgun al  well only as long as the Branch Target Buffer (BTB) supplies correct targets for all taken branches<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" ta e found that FDIP's performance is significantly limited by BTB misses that stall FDIP's prefetching<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" ta ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b131\">132]</ref>. As we and others<ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ding performance similar to aggressive I-cache prefetchers <ref type=\"bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>.</p><p>However, FDIP performs well only as long as the Bran  resemble a recent state-of-the-art industry FDIP baseline <ref type=\"bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>, as listed in Table <ref type=\"table\" target=\"#tab_0\">1</re et=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref> evaluate their frontend optimizations.</p></div> <div xmlns et=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n t=\"#b126\">127]</ref> without incurring a high storage cost <ref type=\"bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>. Consequently, FDIP and its variants are employed in modern iding performance similar to aggressive I-cache prefetchers<ref type=\"bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b57\">58]</ref>.However, FDIP performs well only as long as the Branch Targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad (only up to 1% <ref type=\"bibr\" target=\"#b68\">[69]</ref><ref type=\"bibr\" target=\"#b69\">[70]</ref><ref type=\"bibr\" target=\"#b70\">[71]</ref><ref type=\"bibr\" target=\"#b150\">151]</ref>) and widespread . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: illions of dollars <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" tar e considerably greater than L2iMPKIs of all 12 other applications and closer to verilator's L2iMPKI <ref type=\"bibr\" target=\"#b41\">(42)</ref>. Therefore, we study verilator's behavior as a proxy <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , introducing performance losses worth millions of dollars <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" tar more than 15% of all pipeline slots due to frontend stalls <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" tar \" target=\"#b132\">133]</ref>. Modern data center applications exhibit multi-megabyte code footprints <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" ta -digit performance gains in data center applications can minimize the Total Cost of Ownership (TCO) <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b66\">67]</ref> and reduce data ce profile quality <ref type=\"bibr\" target=\"#b54\">[55]</ref>, they work exceptionally well in practice <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta e opportunity to adapt to changing application profiles and are widely used in today's data centers <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta zes (both instruction and branch footprint) are the key characteristics of data center applications <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" ta meter's profiling, as data center applications are already routinely profiled with Intel LBR and PT <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta inate a subset of all I-cache misses, in practice it is intractable to find the optimal code layout <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b111\">112]</ref>. Hardware techni ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. Hybrid techniques <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b76\">77]</ref> combine the effect o profile quality<ref type=\"bibr\" target=\"#b54\">[55]</ref>, they work exceptionally well in practice<ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta le opportunity to adapt to changing application profiles and are widely used in today's data centers<ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta  type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b102\">103]</ref>, and kernel modules <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Data center applications' large code footprints do not fi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b66\">67,</ref><ref type=\"bibr\" target=\"#b103\">104,</ref><ref type=\"bibr\" target=\"#b106\">107,</ref><ref type=\"bibr\" target=\"#b132\">133]</ref>. Modern data center applications exhibit multi-megabyte c et=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" target=\"#b132\">133]</ref>. As these applications are proprietary, we use the applic ions in the study. Recent works from data center providers <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b132\">133]</ref> observe that their workloads' L2iMPKIs range from 10-40,  \"#b32\">33,</ref><ref type=\"bibr\" target=\"#b105\">106,</ref><ref type=\"bibr\" target=\"#b106\">107,</ref><ref type=\"bibr\" target=\"#b132\">133]</ref>. Hence, Thermometer can be combined with existing build a t=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b66\">67]</ref> and reduce data center carbon emissions <ref type=\"bibr\" target=\"#b132\">[133]</ref>, there is a critical need to mitigate frontend stalls to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Existing work <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b64\">65,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: replay prefetchers <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b71\">72,</ref><ref type=\"bibr\" target=\"#b72\">73]</ref>, and BTBdirected pr 83\">84]</ref>. These techniques usually have one of two limitations: (1) the design is more complex <ref type=\"bibr\" target=\"#b71\">[72,</ref><ref type=\"bibr\" target=\"#b72\">73,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b147\">[148]</ref>. Existing work <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /ref>. They introduce non-affine and dynamic memory accesses <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>, and require domain-specific caches. Unfortunately, cache c t, and minimize controller occupancy. We create caches for four different DSA families: Sparse GEMM <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, GraphPulse <ref t dynamic accesses would need to implement addresstranslation from meta-tags to these local addresses <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Further, they can  reused across multiple DSA families. We create caches for four different DSA families: Sparse GEMM <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, GraphPulse <ref t indirectly addressed) and irregular non-linear accesses. A cache is necessary to capture the reuse. <ref type=\"bibr\" target=\"#b34\">[35]</ref> iii) Walkers: Since data is stored in non-linear data stru ref type=\"bibr\" target=\"#b17\">[18]</ref>, DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and Gamma <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Both SpArch and Gamma can use the same X-Cache microarchi d this, we consider two DSAs from the same family, sparse GEMM: inner-product and Gustavson product <ref type=\"bibr\" target=\"#b34\">[35]</ref> (Figure <ref type=\"figure\">5</ref>). Both DSAs work with t B requires a walker, and this varies between the DSAs due to the loop order. In the Gustavson DSA , <ref type=\"bibr\" target=\"#b34\">[35]</ref> we perform an outer product between elements of matrix A a aset from MonetDB as Widx.</p><p>SpGEMM: SpArch <ref type=\"bibr\" target=\"#b36\">[37]</ref> and gamma <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Input: p2p-Gnutella31. N = 67K, NNZ = 147K. The input to  e controllers. Some prior DSAs include cache-like structures <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> that are inextricab che uses global addresses only on misses. While prior DSAs <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> include fields that the same type of meta-tags, e.g., sparse GEMM accelerators <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>.</p></div> <div xml. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b36\">[37]</ref> and indirectly-indexed data structures <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. They introduce non-affine and dynamic memory accesses <ref  GEMM <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, GraphPulse <ref type=\"bibr\" target=\"#b29\">[30]</ref>, DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and Widx  nce, they employ sparse data structures <ref type=\"bibr\" target=\"#b36\">[37]</ref>, indirect-indexes <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and hash tables <ref type=\"bibr\" target=\"#b17\">[18]</ref> ecific caches for five different DSAs, SpArch <ref type=\"bibr\" target=\"#b36\">[37]</ref>, GraphPulse <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Widx <ref type=\"bibr\" target=\"#b17\">[18]</ref>, DASX <ref searches and optimized for underlying search patterns. For instance, in the case of the Graph-Pulse <ref type=\"bibr\" target=\"#b29\">[30]</ref>, a direct-mapped cache suffices. The cache is preloaded on ex tables.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>DSA #Active</head><p>GraphPulse <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Workload: PageRank. Inputs: p2p-Gnutella08: N = 6.3K, NNZ  GEMM <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, GraphPulse <ref type=\"bibr\" target=\"#b29\">[30]</ref>, DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and Widx   implicitly locate the data on-chip; X-Cache uses global addresses only on misses. While prior DSAs <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> e.g., Buffets<ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g/ns/1.0\"><p>implicitly fetch the data. Jenga <ref type=\"bibr\" target=\"#b32\">[33]</ref> and Hotpads <ref type=\"bibr\" target=\"#b33\">[34]</ref> organize the hierarchy of caches as a collection of SRAM b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cate limitations) Caches Scratch+DMA Scratch+AE FIFOs X-Cache<ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b33\">[34]</ref> organize the hierarchy of caches as a collection of SRAM banks. GPUs <ref type=\"bibr\" target=\"#b15\">[16]</ref> and many cores use software-managed scratchpads. These app. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iple DSAs in the same family will employ the same type of meta-tags, e.g., sparse GEMM accelerators <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: patterns driven by affine loops and    There have also been works on augmenting scratchpads in GPUs <ref type=\"bibr\" target=\"#b20\">[21]</ref> and FPGAs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type \"9\">Related Work</head><p>There have been multiple proposals rethinking address-based caches. Stash <ref type=\"bibr\" target=\"#b20\">[21]</ref> carves out portions of the address space. Hits to the scra \">[12,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> AE<ref type=\"bibr\" target=\"#b4\">[5]</ref>,Stash<ref type=\"bibr\" target=\"#b20\">[21]</ref> Pipeline<ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\" \">[12,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> AE<ref type=\"bibr\" target=\"#b4\">[5]</ref>,Stash<ref type=\"bibr\" target=\"#b20\">[21]</ref> Pipeline<ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b30\">31]</ref>, memory controllers <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and tiled DMAs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target e pattern on non-zeros in matrix A's rows, e.g., B[0,0]: a brought in first for A[0,0], reused by A <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\">0]</ref>. A programmable cache controller wi and the patch runs ahead in a decoupled manner. It cannot be employed for the DSAs we explore. LEAP <ref type=\"bibr\" target=\"#b0\">[1]</ref> and CoRAM <ref type=\"bibr\" target=\"#b5\">[6]</ref> targeted B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: RAM onto onchip SRAMs. This includes scatter/gather engines <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d orchestrating data movement and computation. Existing DSAs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> predominantly work with dense data and organize them into s  CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., Spatial<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> Stream<ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"b  CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., Spatial<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> Stream<ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b36\">37]</ref>, GraphPulse <ref type=\"bibr\" target=\"#b29\">[30]</ref>, DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and Widx <ref type=\"bibr\" target=\"#b17\">[18]</ref>. X-Cac lse <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Widx <ref type=\"bibr\" target=\"#b17\">[18]</ref>, DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and Gamma <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Both e merge the payload of the incoming event and the existing entry using an add operation.</p><p>DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Workload: MonetDB and TPC-H. DASX <ref type=\"bibr\" target  operation.</p><p>DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Workload: MonetDB and TPC-H. DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref> is a data structure iterator. The DSA references the keys  ref type=\"bibr\" target=\"#b36\">37]</ref>, GraphPulse <ref type=\"bibr\" target=\"#b29\">[30]</ref>, DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and Widx <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We ar es, and regular computation.</p><p>Finally, Widx <ref type=\"bibr\" target=\"#b17\">[18]</ref> and DASX <ref type=\"bibr\" target=\"#b21\">[22]</ref> accelerated the walk over data structures. Widx created an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, and tiled DMAs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. All designs introduce additional address spaces, either lo  designs introduce additional address spaces, either local <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, carved from the physical <ref type=\"bibr\" target=\"#b30\">[3 </ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> e.g., Buffets<ref type=\"bibr\" target=\"#b27\">[28]</ref> e.g., CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., S >engine)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>e.g., Buffets<ref type=\"bibr\" target=\"#b27\">[28]</ref> e.g., CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., S e has been extensive work in combining the benefits of DMAs and scratchpads. Most recently, buffets <ref type=\"bibr\" target=\"#b27\">[28]</ref> and Ax-DAE <ref type=\"bibr\" target=\"#b4\">[5]</ref> develop. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .g., Spatial<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> Stream<ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> AE<ref type=\"bibr\" .g., Spatial<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> Stream<ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> AE<ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b30\">31]</ref>, memory controllers <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and tiled DMAs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target e pattern on non-zeros in matrix A's rows, e.g., B[0,0]: a brought in first for A[0,0], reused by A <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\">0]</ref>. A programmable cache controller wi and the patch runs ahead in a decoupled manner. It cannot be employed for the DSAs we explore. LEAP <ref type=\"bibr\" target=\"#b0\">[1]</ref> and CoRAM <ref type=\"bibr\" target=\"#b5\">[6]</ref> targeted B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cate limitations) Caches Scratch+DMA Scratch+AE FIFOs X-Cache<ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>, and tiled DMAs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. All designs introduce additional address spaces, either lo  designs introduce additional address spaces, either local <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, carved from the physical <ref type=\"bibr\" target=\"#b30\">[3 </ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> e.g., Buffets<ref type=\"bibr\" target=\"#b27\">[28]</ref> e.g., CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., S >engine)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>e.g., Buffets<ref type=\"bibr\" target=\"#b27\">[28]</ref> e.g., CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., S e has been extensive work in combining the benefits of DMAs and scratchpads. Most recently, buffets <ref type=\"bibr\" target=\"#b27\">[28]</ref> and Ax-DAE <ref type=\"bibr\" target=\"#b4\">[5]</ref> develop. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. They introduce non-affine and dynamic memory accesses <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>, and require domain- eve DSA-specialization through the cache controllers. Some prior DSAs include cache-like structures <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" targ adata accesses and multiple address generations are required during the walking phase. Patch memory <ref type=\"bibr\" target=\"#b6\">[7]</ref> targeted image processing pipelines that need tiling. The DS. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a structures, packing them in SRAMs, and orchestrating data movement and computation. Existing DSAs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> predominantly work w events, in contrast to prior work that used blocking threads <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Coroutines conveniently capture the underlying parallelism a  in GPUs <ref type=\"bibr\" target=\"#b20\">[21]</ref> and FPGAs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Scratchpad-AE targets DSAs, which can split into coarse-grai lers <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and tiled DMAs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. All designs introduc  have explored the use of threads for static access patterns <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Each walker is assig pe=\"bibr\" target=\"#b26\">27]</ref> e.g., Buffets<ref type=\"bibr\" target=\"#b27\">[28]</ref> e.g., CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., Spatial<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref t ><cell></cell></row></table><note>e.g., Buffets<ref type=\"bibr\" target=\"#b27\">[28]</ref> e.g., CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., Spatial<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref t  cannot be employed for the DSAs we explore. LEAP <ref type=\"bibr\" target=\"#b0\">[1]</ref> and CoRAM <ref type=\"bibr\" target=\"#b5\">[6]</ref> targeted BRAMs on an FPGA. Leap unified logically separate s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a structures, packing them in SRAMs, and orchestrating data movement and computation. Existing DSAs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> predominantly work w events, in contrast to prior work that used blocking threads <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Coroutines conveniently capture the underlying parallelism a  in GPUs <ref type=\"bibr\" target=\"#b20\">[21]</ref> and FPGAs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Scratchpad-AE targets DSAs, which can split into coarse-grai lers <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and tiled DMAs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. All designs introduc  have explored the use of threads for static access patterns <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Each walker is assig pe=\"bibr\" target=\"#b26\">27]</ref> e.g., Buffets<ref type=\"bibr\" target=\"#b27\">[28]</ref> e.g., CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., Spatial<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref t ><cell></cell></row></table><note>e.g., Buffets<ref type=\"bibr\" target=\"#b27\">[28]</ref> e.g., CoRAM<ref type=\"bibr\" target=\"#b5\">[6]</ref> e.g., Spatial<ref type=\"bibr\" target=\"#b18\">[19,</ref><ref t  cannot be employed for the DSAs we explore. LEAP <ref type=\"bibr\" target=\"#b0\">[1]</ref> and CoRAM <ref type=\"bibr\" target=\"#b5\">[6]</ref> targeted BRAMs on an FPGA. Leap unified logically separate s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e-art storage idioms (shaded cells indicate limitations) Caches Scratch+DMA Scratch+AE FIFOs X-Cache<ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: not exploited by previous prefetching works that focus primarily on mitigating main memory latency. <ref type=\"bibr\" target=\"#b1\">(2)</ref> We hence propose RFP (Register File Prefetch) that prefetche wrong execution and flushes.</p><p>A recent work, Efficient Pipeline Prefetch (EPP) by Alves et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>, attempts to reduce L1 accesses and power of DLVP <ref type= EVES <ref type=\"bibr\" target=\"#b64\">[65]</ref> and DLVP <ref type=\"bibr\" target=\"#b66\">[67]</ref>   <ref type=\"bibr\" target=\"#b1\">[2]</ref> and the state-of-the-art Composite value predictor <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b57\">58,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b64\">65]</ref> speculatively breaks dependencies to unlock higher instruct ds corresponding to a PT entry, similar to prior proposals <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b64\">65]</ref>. It is incremented on load allocation and decremented on lo get=\"#b57\">58,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b64\">65]</ref>. Some recent works have focused on implementation details o s, stride and the inflight counter present in the PT entry <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b64\">65,</ref><ref type=\"bibr\" target=\"#b67\">68]</ref>. Some prior works s type=\"bibr\" target=\"#b67\">[68]</ref>. The Composite VP is essentially an intelligent fusion of EVES <ref type=\"bibr\" target=\"#b64\">[65]</ref> and DLVP <ref type=\"bibr\" target=\"#b66\">[67]</ref>   <ref  dictor <ref type=\"bibr\" target=\"#b67\">[68]</ref>. The Composite VP is an intelligent fusion of EVES <ref type=\"bibr\" target=\"#b64\">[65]</ref> and DLVP <ref type=\"bibr\" target=\"#b66\">[67]</ref> predict oduced by false-positives from SSBF.</p><p>We also evaluated a new configuration that combines EVES <ref type=\"bibr\" target=\"#b64\">[65]</ref> and RFP, essentially demonstrating a fusion of VP and RFP.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b69\">70,</ref><ref type=\"bibr\" target=\"#b71\">[72]</ref><ref type=\"bibr\" target=\"#b72\">[73]</ref><ref type=\"bibr\" target=\"#b73\">[74]</ref> is a very well studied technique to hide the DRAM memory l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: v xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Value Prediction</head><p>Value Prediction (VP) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" targ guration in next sections. For our studies, the flush penalty of a value misprediction is 20 cycles <ref type=\"bibr\" target=\"#b6\">[7]</ref>. We selected 65 diverse, single-threaded applications for ou n important observation given the scarce L1 bandwidth. Past proposals like Focused Value Prediction <ref type=\"bibr\" target=\"#b6\">[7]</ref> have exploited similar criticality phenomena to identify can posed implementation of VP without adding extra read ports to the register file for validation. FVP <ref type=\"bibr\" target=\"#b6\">[7]</ref> focused on only critical loads to manage the value predictor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"2.1\">Value Prediction</head><p>Value Prediction (VP) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" tar c.org/ns/1.0\"><head n=\"6\">OTHER RELATED WORK</head><p>VP have been explored extensively in the past <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f loads, as address patterns are easier to predict than data <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" tar ted that VP and RFP are synergistic. Load address predictors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b66\">67,</ref><ref type=\"bibr\" tar fetch can be launched as soon as the load has been fetched in the front-end, similar to prior works <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b66\">67]</ref>. However, as discu etting scheduled and hide a fraction of the load's latency.</p><p>Bandwidth: Traditional AP schemes <ref type=\"bibr\" target=\"#b28\">[29]</ref> [67] require two cache accesses-first for retrieving predi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  address patterns of cache misses. Works on helper threads <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" target=\"#b59\">60,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b69\">70,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"2.1\">Value Prediction</head><p>Value Prediction (VP) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" tar c.org/ns/1.0\"><head n=\"6\">OTHER RELATED WORK</head><p>VP have been explored extensively in the past <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: head n=\"2.1\">Value Prediction</head><p>Value Prediction (VP) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" tar c.org/ns/1.0\"><head n=\"6\">OTHER RELATED WORK</head><p>VP have been explored extensively in the past <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b57\">58,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rate the lengthy cold boots of serverless instances provided that a function snapshotting technique <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: LC capacity available for each virtual machine, to preserve LLC working set across context switches <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Zhu et al. examine event-driven server-side applications an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r long-running server workloads by recording entire streams of instruction cache accesses or misses <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta g mechanism to find the correct metadata when the actual execution diverges from the recorded trace <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\">17,</ref><ref type=\"bibr\" target=\"#b27\">28 .</p><p>Compared to state-of-the-art instruction prefetchers <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar ead><p>In this section, we compare Jukebox to a state-of-the-art instruction prefetcher, called PIF <ref type=\"bibr\" target=\"#b15\">[16]</ref>. PIF is a stream-based prefetcher, which works by recordin d the most recent recorded stream that starts with that address. By using the same parameters as in <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we configure PIF with a 49KB index, 164KB of stream metad . To store the metadata, existing temporal streaming proposals either use dedicated on-chip storage <ref type=\"bibr\" target=\"#b15\">[16]</ref> or virtualize the metadata into the LLC <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  manager that can move pages in memory (e.g., for memory compaction purposes).</p><p>Jevdjic et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> record spatial footprints of data pages to reduce off-chip. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with many warm functions experiencing invocation inter-arrival rates measured in seconds or minutes <ref type=\"bibr\" target=\"#b42\">[43]</ref> -an invocation rate that is relatively infrequent compared ed functions have a warm instance when a request arrives, according to the study of Azure Functions <ref type=\"bibr\" target=\"#b42\">[43]</ref>. The same study shows that fewer than 5% of all invocation tle memory footprint, allocating less than 300MB of memory <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>.</p><p>Despite the short running time of many function inst. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r long-running server workloads by recording entire streams of instruction cache accesses or misses <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta g mechanism to find the correct metadata when the actual execution diverges from the recorded trace <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\">17,</ref><ref type=\"bibr\" target=\"#b27\">28 .</p><p>Compared to state-of-the-art instruction prefetchers <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar ead><p>In this section, we compare Jukebox to a state-of-the-art instruction prefetcher, called PIF <ref type=\"bibr\" target=\"#b15\">[16]</ref>. PIF is a stream-based prefetcher, which works by recordin d the most recent recorded stream that starts with that address. By using the same parameters as in <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we configure PIF with a 49KB index, 164KB of stream metad . To store the metadata, existing temporal streaming proposals either use dedicated on-chip storage <ref type=\"bibr\" target=\"#b15\">[16]</ref> or virtualize the metadata into the LLC <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tions may be expected to complete in a millisecond or less <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>.</p><p>To avoid the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: invocation to the same instance arrives.</p><p>Compared to state-of-the-art instruction prefetchers <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  manager that can move pages in memory (e.g., for memory compaction purposes).</p><p>Jevdjic et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> record spatial footprints of data pages to reduce off-chip. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e and reproducible results, we invoke each JIT'ed function 20000 times before starting measurements <ref type=\"bibr\" target=\"#b50\">[51]</ref>. We empirically found that for our functions more invocati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oft all keep recentlyinvoked function instances warm for at least several minutes and up to an hour <ref type=\"bibr\" target=\"#b48\">[49]</ref>. The combination of small memory footprints for many funct f small memory footprints for many functions, long keep-alive intervals enforced by cloud providers <ref type=\"bibr\" target=\"#b48\">[49]</ref>, and hundreds of gigabytes of memory in a representative s ting a function is a longlatency operation that can take hundreds of milliseconds in today's clouds <ref type=\"bibr\" target=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. To avoid this lat =\"#b35\">[36]</ref><ref type=\"bibr\" target=\"#b36\">[37]</ref><ref type=\"bibr\" target=\"#b37\">[38]</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. Although keeping function instances warm comes at a cost f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: work to get 2.2\u00d7 speedup and 61% energy savings. \u2022 Commutative scatter-updates: t\u00e4k\u014d implements PHI <ref type=\"bibr\" target=\"#b94\">[95]</ref>, transforming the caches to use push-based semantics to ac d semantics to accelerate commutative scatter-updates in graphs. t\u00e4k\u014d gets 4.2\u00d7 speedup, similar to <ref type=\"bibr\" target=\"#b94\">[95]</ref>. \u2022 Decoupled graph traversals: t\u00e4k\u014d implements HATS <ref t efits are essential to implementing many data movement features and optimizations. For example, PHI <ref type=\"bibr\" target=\"#b94\">[95]</ref> (a) buffers graph updates in-cache, and (b) decides on evi ple of how t\u00e4k\u014d can redefine cache semantics to accelerate data movement. This study implements PHI <ref type=\"bibr\" target=\"#b94\">[95]</ref>, a push-based hierarchy for commutative scatter-updates, e ibr\" target=\"#b69\">70]</ref>, and an ideal dataflow engine. We see similar results as the PHI paper <ref type=\"bibr\" target=\"#b94\">[95]</ref>: UB in software gets 3.2\u00d7 speedup, but t\u00e4k\u014d gets 4.2\u00d7 spee currently only run PHI at a single level. But t\u00e4k\u014d's design allows hierarchical PHI as described in <ref type=\"bibr\" target=\"#b94\">[95]</ref>, which would show even better results.</p><p>there is poor rior designs that accelerate graphs in very different ways <ref type=\"bibr\" target=\"#b91\">[92,</ref><ref type=\"bibr\" target=\"#b94\">95]</ref>. \u2022 t\u00e4k\u014d enables features in software that are impossible wi ><ref type=\"bibr\" target=\"#b49\">50]</ref>, graph analytics <ref type=\"bibr\" target=\"#b91\">[92,</ref><ref type=\"bibr\" target=\"#b94\">95,</ref><ref type=\"bibr\" target=\"#b149\">150]</ref>, data structures . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b104\">105,</ref><ref type=\"bibr\" target=\"#b141\">142,</ref><ref type=\"bibr\" target=\"#b149\">150]</ref>, t\u00e4k\u014d adds prog et=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b104\">105,</ref><ref type=\"bibr\" target=\"#b141\">142,</ref><ref type=\"bibr\" target=\"#b149\">150]</ref>, t\u00e4k\u014d extends a n cores, motivating the need for separate streaming hardware <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b141\">142,</ref><ref type=\"bibr\" target=\"#b149\">150]</ref>. This case stud \"#b98\">99,</ref><ref type=\"bibr\" target=\"#b138\">139,</ref><ref type=\"bibr\" target=\"#b139\">140,</ref><ref type=\"bibr\" target=\"#b141\">142]</ref>, many of which use dedicated engines to stream data to th \"#b82\">83,</ref><ref type=\"bibr\" target=\"#b104\">105,</ref><ref type=\"bibr\" target=\"#b128\">129,</ref><ref type=\"bibr\" target=\"#b141\">142]</ref>. However, there is no mechanism to trigger software when . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b151\">152]</ref>, memory management <ref type=\"bibr\" target=\"#b84\">[85,</ref><ref type=\"bibr\" target=\"#b134\">135]</ref>, and system software <ref type=\"bibr\" target=\"#b66\">[67,<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b152\">153,</ref><ref type=\"bibr\" target=\"#b153\">154]</ref>, or serialize/de-serialize <ref type=\"bibr\" target=\"#b107\">[108]</ref> data. We motivate t\u00e4k\u014d by observing how its onMiss callb gram has two major problems. Cores are inefficient at data transformations, wasting time and energy <ref type=\"bibr\" target=\"#b107\">[108,</ref><ref type=\"bibr\" target=\"#b145\">146]</ref>. And if data a type=\"bibr\" target=\"#b134\">135]</ref>, and system software <ref type=\"bibr\" target=\"#b66\">[67,</ref><ref type=\"bibr\" target=\"#b107\">108,</ref><ref type=\"bibr\" target=\"#b126\">127]</ref>. While highly e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ar to <ref type=\"bibr\" target=\"#b94\">[95]</ref>. \u2022 Decoupled graph traversals: t\u00e4k\u014d implements HATS <ref type=\"bibr\" target=\"#b91\">[92]</ref> as a representative decoupled streaming application. t\u00e4k\u014d  s support for programmable streams by implementing HATS (hardware-accelerated traversal scheduling) <ref type=\"bibr\" target=\"#b91\">[92]</ref>, which computes an efficient graph traversal to improve da 's onMiss keeps a small stack and walks the graph in BDFS order, as described in the original paper <ref type=\"bibr\" target=\"#b91\">[92]</ref>. Our current implementation of HATS sequentializes all onM ata during the edge t\u00e4k\u014d achieves significant speedups on HATS, but somewhat lower than reported in <ref type=\"bibr\" target=\"#b91\">[92]</ref>. This is because we sequentialize the calls to onMiss, whe type=\"bibr\" target=\"#b91\">[92]</ref>. This is because we sequentialize the calls to onMiss, whereas <ref type=\"bibr\" target=\"#b91\">[92]</ref> re-orders the trace to exploit locality by traversing mult zed cache hierarchies. We implement two prior designs that accelerate graphs in very different ways <ref type=\"bibr\" target=\"#b91\">[92,</ref><ref type=\"bibr\" target=\"#b94\">95]</ref>. \u2022 t\u00e4k\u014d enables fe ng <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, graph analytics <ref type=\"bibr\" target=\"#b91\">[92,</ref><ref type=\"bibr\" target=\"#b94\">95,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b72\">73,</ref><ref type=\"bibr\" target=\"#b74\">75,</ref><ref type=\"bibr\" tar rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b72\">73,</ref><ref type=\"bibr\" target=\"#b112\">113,</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b106\">107,</ref><ref type=\"bibr\" target=\"#b117\">118,</ref><ref type=\"bibr\" target=\"#b135\">136,</ref><ref type=\"bibr\" target=\"#b145\">146]</ref>, decrypt <ref type=\"bibr\" target=\"#b46\">[47,</ref><ref ty ficient at data transformations, wasting time and energy <ref type=\"bibr\" target=\"#b107\">[108,</ref><ref type=\"bibr\" target=\"#b145\">146]</ref>. And if data are re-used, then the program re-executes th b106\">107,</ref><ref type=\"bibr\" target=\"#b117\">118,</ref><ref type=\"bibr\" target=\"#b135\">136,</ref><ref type=\"bibr\" target=\"#b145\">146]</ref>, data layout <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref  ng <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b130\">131]</ref> or compression <ref type=\"bibr\" target=\"#b145\">[146]</ref>. By contrast, t\u00e4k\u014d targets a much wider set of features . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  software policies <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b65\">66,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b89\">90,</ref><ref type=\"bibr\" target=\"#b105\">106,</ref><ref type=\"bibr\" target=\"#b106\">107,</ref><ref type=\"bibr\" target=\"#b117\">118,</ref><ref type=\"bibr\" target=\"#b135\">136,</ref><ref type=\"bibr\" \"#b89\">90,</ref><ref type=\"bibr\" target=\"#b105\">106,</ref><ref type=\"bibr\" target=\"#b106\">107,</ref><ref type=\"bibr\" target=\"#b117\">118,</ref><ref type=\"bibr\" target=\"#b135\">136,</ref><ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b89\">90,</ref><ref type=\"bibr\" target=\"#b105\">106,</ref><ref type=\"bibr\" target=\"#b106\">107,</ref><ref type=\"bibr\" target=\"#b117\">118,</ref><ref type=\"bibr\" t=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b89\">90,</ref><ref type=\"bibr\" target=\"#b105\">106,</ref><ref type=\"bibr\" target=\"#b106\">107,</ref><ref type=\"bibr\" target=\"#b117\">118,</ref><ref type=\"bibr\" at is stored in an approximate, compressed format in memory as a base plus offset value, similar to <ref type=\"bibr\" target=\"#b106\">[107]</ref>. Unlike standard compressed caches, this lossy compressi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for sequential data augmentation. Yu et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref>, Zhang et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> and Xia et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref> l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: epresentations from the user-item graph, SGL employs a popular and effective graph encoder LightGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref> as its backbone, whose message passing process is defined as f type=\"bibr\" target=\"#b37\">[38]</ref> (#user 13,024, #item 22,347, #interaction 792,062), Yelp2018 <ref type=\"bibr\" target=\"#b8\">[9]</ref> (#user 31,668 #item 38,048, #interaction 1,561,406), and Ama C <ref type=\"bibr\" target=\"#b1\">[2]</ref>, NGCF <ref type=\"bibr\" target=\"#b24\">[25]</ref>, LightGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and LCF <ref type=\"bibr\" target=\"#b41\">[42]</ref>. Despite  he core ingredients of SGL and LightGCN. More technical details can be found in the original papers <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>.</p><formula xml:id= ts for the performance comparison are conducted on two benchmark datasets: Yelp2018 and Amazon-Book <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. A three-layer setti models, which adopt GNNs as their bases, claim that they have achieved state-of-the-art performance <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ltiple research fields <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. As data annotation ith recommendation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar e in recommender systems <ref type=\"bibr\">[22?</ref> ]. An increasing number of very recent studies <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta recommendation with a particular set of presumed representational invariances to data augmentations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta 44\">45]</ref>. In this paper, we revisit the most commonly used dropoutbased augmentation on graphs <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> which assumes that  CL for improving recommendation performance and have demonstrated significant gains. A typical way <ref type=\"bibr\" target=\"#b27\">[28]</ref> to apply CL to recommendation is first augmenting the user turbations. An investigation is launched into a state-of-the-art CL-based recommendation model, SGL <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which performs node and edge dropout to augment the origi =\"bibr\" target=\"#b8\">[9]</ref> (#user 31,668 #item 38,048, #interaction 1,561,406), and Amazon-Book <ref type=\"bibr\" target=\"#b27\">[28]</ref> (#user 52,463, #item 91,599, #interaction 2,984,108) are u ed efficient and effective, and inspires a lot of follow-up CL-based recommendation models like SGL <ref type=\"bibr\" target=\"#b27\">[28]</ref> and MHCN <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p></d . More technical details can be found in the original papers <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>.</p><formula xml:id=\"formula_3\">E (0) \u2208 R |\ud835\udc41 |\u00d7\ud835\udc51</formula><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  for contrasting augmented item features. SEPT <ref type=\"bibr\" target=\"#b37\">[38]</ref> and COTREC <ref type=\"bibr\" target=\"#b30\">[31]</ref> further propose to mine multiple positive samples with sem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> have sought to harn structures for representation regularization. In addition to the data sparsity problem, Zhou et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> theoretically proved that CL can also mitigate the exposur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ods, LightGCN is the most popular one due to its simple structure and decent performance. Following <ref type=\"bibr\" target=\"#b26\">[27]</ref>, it removes the redundant operations including transformat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ensional normalized vectors on the unit hypersphere S 1 (i.e., circle with radius 1) by using t-SNE <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p><p>All the representations are obtained when the metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resumed representational invariances to data augmentations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar  on the perturbed graph for social/session-based recommendation. In addition to the dropout, CL4Rec <ref type=\"bibr\" target=\"#b32\">[33]</ref> proposes to reorder and crop item segments for sequential . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">[41]</ref>, it is inherently a possible solution to the data sparsity issue <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> in recommender sys. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te information from each other in the graph convolution) and causes the representation degeneration <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>As for the distributions in other columns, by rewri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" tar data augmentations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. In this paper, we  get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Zhou et al. <ref t CL. Because we focus on the Top-N recommendation, following the convention in the previous research <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, we discard rating ndation, following the convention in the previous research <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, we discard ratings less than 4 in Douban-Book, which is wi -the-art performance <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> in different subfields. Particularly, GCN <ref type=\"bibr\"  \">[33]</ref> proposes to reorder and crop item segments for sequential data augmentation. Yu et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref>, Zhang et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resumed representational invariances to data augmentations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar  on the perturbed graph for social/session-based recommendation. In addition to the dropout, CL4Rec <ref type=\"bibr\" target=\"#b32\">[33]</ref> proposes to reorder and crop item segments for sequential . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>Recently, a resurgence of contrastive learning (CL) <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> has been witnessed . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of CL in other fields, there has been a wave of new research that integrates CL with recommendation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">InfoNCE Loss Influences More</head><p>Wang and Isola <ref type=\"bibr\" target=\"#b23\">[24]</ref> have identified that optimizing the contrastive loss inten is a one-class problem, we only investigate the uniformity by following the visualization method in <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>We first map the learned representations (randomly  wards higher uniformity. We present the following experimental analysis to demonstrate it.</p><p>In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, a metric is proposed to measure the uniformity of the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y account for its incompetence. Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> now have become widely acknowledged powerful architectures . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. As data annotation is not required in CL, it is a natural  the most commonly used dropoutbased augmentation on graphs <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> which assumes that the representations are invariant to par. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y removing the redundancy and impurity with the edge/node dropout. Unexpectedly, a few latest works <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta  Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>, and conducts feature masking for CL.</p><p>\u2022 BUIR <ref type=\"bibr\" target=\"#b13\">[14]</ref> has a two-branch architecture which consists of a target n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">[41]</ref>, it is inherently a possible solution to the data sparsity issue <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> in recommender sys. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>Recently, a resurgence of contrastive learning (CL) <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> has been witnessed . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">InfoNCE Loss Influences More</head><p>Wang and Isola <ref type=\"bibr\" target=\"#b23\">[24]</ref> have identified that optimizing the contrastive loss inten is a one-class problem, we only investigate the uniformity by following the visualization method in <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>We first map the learned representations (randomly  wards higher uniformity. We present the following experimental analysis to demonstrate it.</p><p>In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, a metric is proposed to measure the uniformity of the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ensional normalized vectors on the unit hypersphere S 1 (i.e., circle with radius 1) by using t-SNE <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p><p>All the representations are obtained when the metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te information from each other in the graph convolution) and causes the representation degeneration <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>As for the distributions in other columns, by rewri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">InfoNCE Loss Influences More</head><p>Wang and Isola <ref type=\"bibr\" target=\"#b23\">[24]</ref> have identified that optimizing the contrastive loss inten is a one-class problem, we only investigate the uniformity by following the visualization method in <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>We first map the learned representations (randomly  wards higher uniformity. We present the following experimental analysis to demonstrate it.</p><p>In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, a metric is proposed to measure the uniformity of the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: c.org/ns/1.0\"><head n=\"1\">INTRODUCTION</head><p>Recently, a resurgence of contrastive learning (CL) <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce augmentations for sequential model pretraining with mutual information maximization. Wei et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> reformulated the cold-start item representation learning f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e SGL variants, the following recent data augmentation-based methods are compared.</p><p>\u2022 Mult-VAE <ref type=\"bibr\" target=\"#b14\">[15]</ref> is a variational autoencoder-based recommendation model. I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y removing the redundancy and impurity with the edge/node dropout. Unexpectedly, a few latest works <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta  Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>, and conducts feature masking for CL.</p><p>\u2022 BUIR <ref type=\"bibr\" target=\"#b13\">[14]</ref> has a two-branch architecture which consists of a target n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ery recent studies <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar th the edge/node dropout. Unexpectedly, a few latest works <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> have reported that  get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar <head n=\"4.1\">Experimental Settings</head><p>Datasets. Three public benchmark datasets: Douban-Book <ref type=\"bibr\" target=\"#b37\">[38]</ref> (#user 13,024, #item 22,347, #interaction 792,062), Yelp20 ollow-up CL-based recommendation models like SGL <ref type=\"bibr\" target=\"#b27\">[28]</ref> and MHCN <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  ecommendation, in which the item tower is also shared for contrasting augmented item features. SEPT <ref type=\"bibr\" target=\"#b37\">[38]</ref> and COTREC <ref type=\"bibr\" target=\"#b30\">[31]</ref> furth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a special self-supervised recommendation model because it has a reconstruction objective. \u2022 DNN+SSL <ref type=\"bibr\" target=\"#b33\">[34]</ref> is a recent DNN-based recommendation method which adopts t ent and collaborative signals to alleviate the data sparsity issue. Similar ideas are also found in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, where a two-tower DNN architecture is developed for recom. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[22?</ref> ]. An increasing number of very recent studies <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar et=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" tar br\" target=\"#b39\">[40]</ref>, Zhang et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> and Xia et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref> leveraged hypergraph to model recommendation data, and pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e. Then we plot the feature distributions with the nonparametric Gaussian kernel density estimation <ref type=\"bibr\" target=\"#b2\">[3]</ref> in R 2 (shown in Fig. <ref type=\"figure\">2</ref>). For a cle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">InfoNCE Loss Influences More</head><p>Wang and Isola <ref type=\"bibr\" target=\"#b23\">[24]</ref> have identified that optimizing the contrastive loss inten is a one-class problem, we only investigate the uniformity by following the visualization method in <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>We first map the learned representations (randomly  wards higher uniformity. We present the following experimental analysis to demonstrate it.</p><p>In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, a metric is proposed to measure the uniformity of the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onducted on two benchmark datasets: Yelp2018 and Amazon-Book <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. A three-layer setting is adopted and the hyperparameters a t of the graph neural recommendation models like GCMC <ref type=\"bibr\" target=\"#b1\">[2]</ref>, NGCF <ref type=\"bibr\" target=\"#b24\">[25]</ref>, LightGCN <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and LCF. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" tar data augmentations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. In this paper, we  get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Zhou et al. <ref t CL. Because we focus on the Top-N recommendation, following the convention in the previous research <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, we discard rating ndation, following the convention in the previous research <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, we discard ratings less than 4 in Douban-Book, which is wi -the-art performance <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> in different subfields. Particularly, GCN <ref type=\"bibr\"  \">[33]</ref> proposes to reorder and crop item segments for sequential data augmentation. Yu et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref>, Zhang et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: larity bias <ref type=\"bibr\" target=\"#b3\">[4]</ref> in the recommendation data. Recall the BPR loss <ref type=\"bibr\" target=\"#b20\">[21]</ref> used in LightGCN:</p><formula xml:id=\"formula_7\">L \ud835\udc5f\ud835\udc52\ud835\udc50 = \u2212. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">InfoNCE Loss Influences More</head><p>Wang and Isola <ref type=\"bibr\" target=\"#b23\">[24]</ref> have identified that optimizing the contrastive loss inten is a one-class problem, we only investigate the uniformity by following the visualization method in <ref type=\"bibr\" target=\"#b23\">[24]</ref>.</p><p>We first map the learned representations (randomly  wards higher uniformity. We present the following experimental analysis to demonstrate it.</p><p>In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, a metric is proposed to measure the uniformity of the rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" tar data augmentations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. In this paper, we  get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. Zhou et al. <ref t CL. Because we focus on the Top-N recommendation, following the convention in the previous research <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, we discard rating ndation, following the convention in the previous research <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, we discard ratings less than 4 in Douban-Book, which is wi -the-art performance <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> in different subfields. Particularly, GCN <ref type=\"bibr\"  \">[33]</ref> proposes to reorder and crop item segments for sequential data augmentation. Yu et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref>, Zhang et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ave of new research that integrates CL with recommendation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a special self-supervised recommendation model because it has a reconstruction objective. \u2022 DNN+SSL <ref type=\"bibr\" target=\"#b33\">[34]</ref> is a recent DNN-based recommendation method which adopts t ent and collaborative signals to alleviate the data sparsity issue. Similar ideas are also found in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, where a two-tower DNN architecture is developed for recom. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y removing the redundancy and impurity with the edge/node dropout. Unexpectedly, a few latest works <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta  Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>, and conducts feature masking for CL.</p><p>\u2022 BUIR <ref type=\"bibr\" target=\"#b13\">[14]</ref> has a two-branch architecture which consists of a target n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e. Then we plot the feature distributions with the nonparametric Gaussian kernel density estimation <ref type=\"bibr\" target=\"#b2\">[3]</ref> in R 2 (shown in Fig. <ref type=\"figure\">2</ref>). For a cle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ensional normalized vectors on the unit hypersphere S 1 (i.e., circle with radius 1) by using t-SNE <ref type=\"bibr\" target=\"#b22\">[23]</ref>.</p><p>All the representations are obtained when the metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te information from each other in the graph convolution) and causes the representation degeneration <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>As for the distributions in other columns, by rewri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b42\">[39]</ref>: 1) optimization-based methods taking few-shot learning as an optimization problem <ref type=\"bibr\" target=\"#b8\">[5,</ref><ref type=\"bibr\" target=\"#b18\">15,</ref><ref type=\"bibr\" targ some approaches integrate graph neural networks <ref type=\"bibr\" target=\"#b15\">[12]</ref> with MAML <ref type=\"bibr\" target=\"#b8\">[5]</ref> to initialize the graph base learner <ref type=\"bibr\" target d HAN <ref type=\"bibr\" target=\"#b36\">[33]</ref> for HGs; 3) Fewshot learning algorithms, i.e., MAML <ref type=\"bibr\" target=\"#b8\">[5]</ref> and ProtoNet <ref type=\"bibr\" target=\"#b29\">[26]</ref>; 4) G rithms here take examples as i.i.d.</p><p>data with no consideration of graph topology.</p><p>-MAML <ref type=\"bibr\" target=\"#b8\">[5]</ref> initializes network parameters with related tasks upon which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s by preserving rich and hybrid semantic information, such as academic networks and social networks <ref type=\"bibr\" target=\"#b32\">[29]</ref>. Label scarcity problem exists in learning from HGs as wel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b47\">[44]</ref>, subgraph gradients <ref type=\"bibr\" target=\"#b13\">[10]</ref>, node informativeness <ref type=\"bibr\" target=\"#b19\">[16]</ref>, or auxiliary graph knowledge <ref type=\"bibr\" target=\"#b4 ers <ref type=\"bibr\" target=\"#b5\">[2]</ref> or combine selective prototypes with a gating mechanism <ref type=\"bibr\" target=\"#b19\">[16]</ref>.  <ref type=\"bibr\" target=\"#b27\">[24]</ref> or adversarial CN, as well as MAML-Sage and Proto-Sage by replacing GCN with GraphSage; besides, we compare to GPN <ref type=\"bibr\" target=\"#b19\">[16]</ref>, G-Meta <ref type=\"bibr\" target=\"#b13\">[10]</ref>, Meta-SG s GCN in Proto-GCN with GraphSage to test the performance of the graph neural network variant. -GPN <ref type=\"bibr\" target=\"#b19\">[16]</ref> builds dual networks for weighted metric learning by propa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in a variety of applications, such as node classification <ref type=\"bibr\" target=\"#b13\">[10,</ref><ref type=\"bibr\" target=\"#b35\">32,</ref><ref type=\"bibr\" target=\"#b41\">38,</ref><ref type=\"bibr\" tar ype=\"bibr\" target=\"#b13\">[10]</ref>, Meta-SGC <ref type=\"bibr\" target=\"#b47\">[44]</ref> and AMM-GNN <ref type=\"bibr\" target=\"#b35\">[32]</ref>; 5) Domain generalization techniques incorporating Feature </ref> with MAML, thus avoiding adjacency computation with pre-calculated node embeddings. -AMM-GNN <ref type=\"bibr\" target=\"#b35\">[32]</ref> extends Meta-SGC using attention mechanism to transform th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pace. The domain generalization approaches relax the requirement of target domain data for training <ref type=\"bibr\" target=\"#b34\">[31]</ref>. Recently, meta-learning strategies are employed to learn . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b13\">[10,</ref><ref type=\"bibr\" target=\"#b35\">32,</ref><ref type=\"bibr\" target=\"#b41\">38,</ref><ref type=\"bibr\" target=\"#b47\">44]</ref>, link prediction <ref type=\"bibr\" target=\"#b38\">[35,</ref>< arget=\"#b8\">[5]</ref> to initialize the graph base learner <ref type=\"bibr\" target=\"#b45\">[42,</ref><ref type=\"bibr\" target=\"#b47\">44]</ref>, modulate network initialization with transferable graph si ided with few-shot annotations. Assorted meta-knowledge to acquire includes optimization parameters <ref type=\"bibr\" target=\"#b47\">[44]</ref>, subgraph gradients <ref type=\"bibr\" target=\"#b13\">[10]</r ef type=\"bibr\" target=\"#b19\">[16]</ref>, G-Meta <ref type=\"bibr\" target=\"#b13\">[10]</ref>, Meta-SGC <ref type=\"bibr\" target=\"#b47\">[44]</ref> and AMM-GNN <ref type=\"bibr\" target=\"#b35\">[32]</ref>; 5)  Gs to conduct unsupervised learning. (2) Graph neural networks are tested following the workflow in <ref type=\"bibr\" target=\"#b47\">[44]</ref> for the scenario.</p><p>-GCN learns node embeddings here b s integrate graph neural networks with few-shot methods to conquer low-data regime.</p><p>-MAML-GCN <ref type=\"bibr\" target=\"#b47\">[44]</ref> employs GCN as the graph base leaner to be meta-learned by les local subgraph surrounding the target node to transfer subgraph-specific information. -Meta-SGC <ref type=\"bibr\" target=\"#b47\">[44]</ref> trains SGC <ref type=\"bibr\" target=\"#b37\">[34]</ref> with  </row><row><cell>\u2022 CrossHG-</cell></row></table><note>Performance. Performances (in term of accuracy<ref type=\"bibr\" target=\"#b47\">[44]</ref>) of CrossHG-Meta and baseline models are reported in Table. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of extracting meta-knowledge, few-shot learning approaches are mainly categorized into two branches <ref type=\"bibr\" target=\"#b42\">[39]</ref>: 1) optimization-based methods taking few-shot learning as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e domain under 2,3-way 5,10-shot settings. Amazon. We utilize five product domains from Amazon data <ref type=\"bibr\" target=\"#b23\">[20]</ref>, namely Office, Music, Outdoors , Arts and Toy, each conta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[12]</ref> with MAML <ref type=\"bibr\" target=\"#b8\">[5]</ref> to initialize the graph base learner <ref type=\"bibr\" target=\"#b45\">[42,</ref><ref type=\"bibr\" target=\"#b47\">44]</ref>, modulate network . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trategies are employed to learn transferable knowledge with meta-training process on source domains <ref type=\"bibr\" target=\"#b7\">[4,</ref><ref type=\"bibr\" target=\"#b16\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e domain under 2,3-way 5,10-shot settings. Amazon. We utilize five product domains from Amazon data <ref type=\"bibr\" target=\"#b23\">[20]</ref>, namely Office, Music, Outdoors , Arts and Toy, each conta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e two nodes on the right-hand side require a max/min pooling. These two pairs have been used by PNA <ref type=\"bibr\" target=\"#b5\">[3]</ref> to motivate the usage of a mixture of aggregators for a GNN. ts, we prefer to update the controllers by gradient descent unless they are not differentiable. PNA <ref type=\"bibr\" target=\"#b5\">[3]</ref> improves the expressiveness by considering a mixture of diff GAT <ref type=\"bibr\" target=\"#b29\">[27]</ref>, JKNet <ref type=\"bibr\" target=\"#b34\">[32]</ref>, PNA <ref type=\"bibr\" target=\"#b5\">[3]</ref>, and Policy-GNN <ref type=\"bibr\" target=\"#b16\">[14]</ref> wh our case, we emphasize the necessity of selecting the appropriate aggregator in a node-wise manner. <ref type=\"bibr\" target=\"#b5\">(3)</ref> We  propose a novel notion, the resolution of a GNN layer, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  graphs and thus architectures of the applied GNN. In practice, the widely-adopted neighbor sampler <ref type=\"bibr\" target=\"#b10\">[8]</ref> uses the same pre-specified resolutions of the GNN layers f r neighborhood. In this paper, we consider one of the most widely adopted samplers-neighbor sampler <ref type=\"bibr\" target=\"#b10\">[8]</ref>, where a fixed number of nodes are sampled in each hop. We  t <ref type=\"bibr\" target=\"#b6\">[4]</ref>, GCN <ref type=\"bibr\" target=\"#b14\">[12]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[8]</ref>, GIN <ref type=\"bibr\" target=\"#b33\">[31]</ref>, APPNP <ref  omes necessary. Hence, we adopt GraphSAGE as the backbone and train the model with neighbor sampler <ref type=\"bibr\" target=\"#b10\">[8]</ref>. Specifically, we aim to train a three-layer GraphSAGE mode o study the impact of the resolution controller, we adopt a twolayer GraphSAGE and neighbor sampler <ref type=\"bibr\" target=\"#b10\">[8]</ref> as our testbed, where the fixed resolutions {1-5, 2-3, 3-2, a GNN from the labeled nodes to predict the unlabeled ones. GNN. Existing GNN models, spatial-based <ref type=\"bibr\" target=\"#b10\">[8,</ref><ref type=\"bibr\" target=\"#b33\">31]</ref> or spectral-based <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 3]</ref>. Node-wise depth has been studied in recent works <ref type=\"bibr\" target=\"#b16\">[14,</ref><ref type=\"bibr\" target=\"#b32\">30]</ref> to allow nodes with different local structures to have diff onnections to related works. There are several recent works that our framework can express. Ala-GCN <ref type=\"bibr\" target=\"#b32\">[30]</ref> terminates the iteration of message passing when an indica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: la_10\">(\ud835\udc59) \ud835\udc63 , \ud835\udc59 = 1, . . . , \ud835\udc3f}), we can feed (h (1) \ud835\udc63 , . . . , h (\ud835\udc3f)</formula><p>\ud835\udc63 ) into a LSTM <ref type=\"bibr\" target=\"#b12\">[10]</ref> and produce the distribution based on its last hidden stat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GNN to a new graph, Neural Architecture Search (NAS) for GNN <ref type=\"bibr\" target=\"#b7\">[5,</ref><ref type=\"bibr\" target=\"#b9\">7,</ref><ref type=\"bibr\" target=\"#b20\">18,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b34\">[32]</ref>, GAT <ref type=\"bibr\" target=\"#b29\">[27]</ref>, and GeniePath <ref type=\"bibr\" target=\"#b22\">[20]</ref>. When our framework is restricted to only one controller o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctly calculate the gradients of z \ud835\udc63 w.r.t. the sampled resolutions, we adopt policy gradient method <ref type=\"bibr\" target=\"#b27\">[25]</ref> to optimize it. The goal of \ud835\udc54 \ud835\udf53 r (\u2022) is to select a suita. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  DARTS <ref type=\"bibr\" target=\"#b21\">[19]</ref> or making exponentiated gradient descent like GAEA <ref type=\"bibr\" target=\"#b19\">[17]</ref>. On the other hand, we regard the resolution controller \ud835\udc54 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph data, including social networks <ref type=\"bibr\" target=\"#b17\">[15]</ref>, citation networks <ref type=\"bibr\" target=\"#b18\">[16]</ref>, and biological networks <ref type=\"bibr\" target=\"#b40\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 7\">[15]</ref>, citation networks <ref type=\"bibr\" target=\"#b18\">[16]</ref>, and biological networks <ref type=\"bibr\" target=\"#b40\">[38]</ref>. When applying GNN to a new graph, Neural Architecture Sea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : The two one-hot attributes are given. We compare NW-GNN to Message Passing Neural Networks (MPNN) <ref type=\"bibr\" target=\"#b8\">[6]</ref>, GCN, and GPR-GNN on this node regression task, where the me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : The two one-hot attributes are given. We compare NW-GNN to Message Passing Neural Networks (MPNN) <ref type=\"bibr\" target=\"#b8\">[6]</ref>, GCN, and GPR-GNN on this node regression task, where the me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ace, e.g., the Aggr(\u2022) is allowed to take choice from O = {mean, add, max, min}. Differentiable NAS <ref type=\"bibr\" target=\"#b21\">[19,</ref><ref type=\"bibr\" target=\"#b38\">36]</ref>, one of the most w  z \ud835\udc63 and thus can be optimized in a differentiable manner, e.g., making gradient descent like DARTS <ref type=\"bibr\" target=\"#b21\">[19]</ref> or making exponentiated gradient descent like GAEA <ref ty rs play a similar role as the architecture parameter defined in differentiable NAS, we follow DARTS <ref type=\"bibr\" target=\"#b21\">[19]</ref> to alternatively update \ud835\udf3d on the training set D train and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : The two one-hot attributes are given. We compare NW-GNN to Message Passing Neural Networks (MPNN) <ref type=\"bibr\" target=\"#b8\">[6]</ref>, GCN, and GPR-GNN on this node regression task, where the me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : The two one-hot attributes are given. We compare NW-GNN to Message Passing Neural Networks (MPNN) <ref type=\"bibr\" target=\"#b8\">[6]</ref>, GCN, and GPR-GNN on this node regression task, where the me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w.tei-c.org/ns/1.0\"><head n=\"1\">INTRODUCTION</head><p>In recent years, Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[4,</ref><ref type=\"bibr\" target=\"#b31\">29]</ref> have been proposed a  <ref type=\"bibr\" target=\"#b10\">[8,</ref><ref type=\"bibr\" target=\"#b33\">31]</ref> or spectral-based <ref type=\"bibr\" target=\"#b6\">[4,</ref><ref type=\"bibr\" target=\"#b11\">9,</ref><ref type=\"bibr\" targe itectures: Conventionally, we adopt MLP and the widely adopted GNN architectures including ChebyNet <ref type=\"bibr\" target=\"#b6\">[4]</ref>, GCN <ref type=\"bibr\" target=\"#b14\">[12]</ref>, GraphSAGE <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b26\">[24]</ref>, which lead to unsatisfactory performances of several representative GNNs <ref type=\"bibr\" target=\"#b23\">[21]</ref>. Therefore, we argue that GNN with node-wise architecture . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: la_10\">(\ud835\udc59) \ud835\udc63 , \ud835\udc59 = 1, . . . , \ud835\udc3f}), we can feed (h (1) \ud835\udc63 , . . . , h (\ud835\udc3f)</formula><p>\ud835\udc63 ) into a LSTM <ref type=\"bibr\" target=\"#b12\">[10]</ref> and produce the distribution based on its last hidden stat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctly calculate the gradients of z \ud835\udc63 w.r.t. the sampled resolutions, we adopt policy gradient method <ref type=\"bibr\" target=\"#b27\">[25]</ref> to optimize it. The goal of \ud835\udc54 \ud835\udf53 r (\u2022) is to select a suita. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 7\">[15]</ref>, citation networks <ref type=\"bibr\" target=\"#b18\">[16]</ref>, and biological networks <ref type=\"bibr\" target=\"#b40\">[38]</ref>. When applying GNN to a new graph, Neural Architecture Sea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph data, including social networks <ref type=\"bibr\" target=\"#b17\">[15]</ref>, citation networks <ref type=\"bibr\" target=\"#b18\">[16]</ref>, and biological networks <ref type=\"bibr\" target=\"#b40\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  DARTS <ref type=\"bibr\" target=\"#b21\">[19]</ref> or making exponentiated gradient descent like GAEA <ref type=\"bibr\" target=\"#b19\">[17]</ref>. On the other hand, we regard the resolution controller \ud835\udc54 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rded as a non-parametric version of Eq. ( <ref type=\"formula\" target=\"#formula_6\">4</ref>). IterGNN <ref type=\"bibr\" target=\"#b28\">[26]</ref> determines the depth of GNN on-the-fly similarly as Eq. ( . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: : The two one-hot attributes are given. We compare NW-GNN to Message Passing Neural Networks (MPNN) <ref type=\"bibr\" target=\"#b8\">[6]</ref>, GCN, and GPR-GNN on this node regression task, where the me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  graph data, including social networks <ref type=\"bibr\" target=\"#b17\">[15]</ref>, citation networks <ref type=\"bibr\" target=\"#b18\">[16]</ref>, and biological networks <ref type=\"bibr\" target=\"#b40\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b34\">[32]</ref>, GAT <ref type=\"bibr\" target=\"#b29\">[27]</ref>, and GeniePath <ref type=\"bibr\" target=\"#b22\">[20]</ref>. When our framework is restricted to only one controller o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e Search (NAS) for GNN <ref type=\"bibr\" target=\"#b7\">[5,</ref><ref type=\"bibr\" target=\"#b9\">7,</ref><ref type=\"bibr\" target=\"#b20\">18,</ref><ref type=\"bibr\" target=\"#b30\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /p><p>3 -Jackknife resampling is a classic method to estimate the bias and variance of a population <ref type=\"bibr\" target=\"#b40\">[41]</ref>. It often relies on a jackknife estimator which is built b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o estimate the GCN parameters without exhaustively re-training GCN, we leverage influence functions <ref type=\"bibr\" target=\"#b27\">[28]</ref> to quantify the change in GCN parameters by infinitesimall ]</ref>. In order to obtain \u0398 * \ud835\udf16,\ud835\udc56 without re-training the GCN, we leverage the influence function <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which is essentially the Taylor expansion over the model  s a powerful approach to evaluate the dependence of the estimator on the value of the data examples <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. In order to obtai he key idea is to apply Hessian-vector product (Algorithm 1) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, which approximates I(\ud835\udc62) = H \u22121 flat f \ud835\udc62 using the power me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b0\">[1]</ref>. Regarding uncertainty quantification for graph data, Dallachiesa et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  ibr\" target=\"#b0\">[1]</ref>. Regarding uncertainty quantification for graph data, Dallachiesa et al.<ref type=\"bibr\" target=\"#b9\">[10]</ref> classify nodes in consideration of uncertainty in edge exis  \ud835\udc56 &gt; \ud835\udc59 + 1, we get Eq. ( <ref type=\"formula\">15</ref>) by taking derivative on both sides of Eq. <ref type=\"bibr\" target=\"#b9\">(10)</ref>.</p><p>Putting everything (Cases 1 -5) together, we complet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b44\">[45]</ref>, drug discovery <ref type=\"bibr\" target=\"#b18\">[19]</ref> and traffic prediction <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>To date, the vast majority of existing works do not t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: diagonal matrix to simulate the convolution operation in graph signal processing. Defferrard et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> improve the efficieny of convolution operation on graphs w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. More related works on IID data can be     found in recent . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b9\">[10]</ref> classify nodes in consideration of uncertainty in edge existence. Hu et al.<ref type=\"bibr\" target=\"#b22\">[23]</ref> learns node</figDesc></figure> <figure xmlns=\"http://www.t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance on many tasks like classification <ref type=\"bibr\" target=\"#b26\">[27]</ref>, anomaly detection <ref type=\"bibr\" target=\"#b11\">[12]</ref> and recommendation <ref type=\"bibr\" target=\"#b45\">[46,</re of Eq. <ref type=\"bibr\" target=\"#b8\">(9)</ref>. See the proof of Proposition 2 for the proof of Eq. <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p><p>Case 3. When \ud835\udc56 &lt; \ud835\udc59 \u2212 1, we first take derivative. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ationbased approaches. Regarding Bayesian-based approaches <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, they either drops edge(s) with certain sampling strategies ation, which is not available in Bayesian-based approaches <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Second, different from existing works on deterministic unc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n-based approaches <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>, directly quantifies uncertainty by parameterizing a Dirich nty quantification <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>, our method does not introduce any additional parameters or graph neural network without consideration of uncertainty) <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. As such, given a well-trained GNN, it requires epoch(s) of  in existing works <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>.  Vectorize \u2207 W (\ud835\udc59 ) \ud835\udc5f (\ud835\udc62, y \ud835\udc62 , \u0398) and stack it to f \ud835\udc62 as   For semi-supervised node classification, we have two uncertainty-based approaches, including S-GNN <ref type=\"bibr\" target=\"#b48\">[49]</ref> and GPN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, as well node classification, we compare the proposed framework with the following approaches.</p><p>\u2022 S-GNN <ref type=\"bibr\" target=\"#b48\">[49]</ref> is an uncertainty-aware estimation framework which leverag. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. Bruna et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> leverages a learnable diagonal matrix to simulate the convol. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ationbased approaches. Regarding Bayesian-based approaches <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, they either drops edge(s) with certain sampling strategies ation, which is not available in Bayesian-based approaches <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Second, different from existing works on deterministic unc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on of research works in quantifying uncertainty for IID data <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thods, i.e., deterministic quantification-based approaches <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>, directly quantifie existing works on deterministic uncertainty quantification <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>, our method does no ining procedures of a vanilla GNN (i.e., graph neural network without consideration of uncertainty) <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. As such, given a  s consistent with the homophily assumption with respect to uncertainty/confidence in existing works <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" ta two uncertainty-based approaches, including S-GNN <ref type=\"bibr\" target=\"#b48\">[49]</ref> and GPN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, as well as GCN <ref type=\"bibr\" target=\"#b26\">[27]</ref>  utilize the obtained node-level representations to perform classification in the experiments. \u2022 GPN <ref type=\"bibr\" target=\"#b39\">[40]</ref> derives three axioms for characterizing the predictive unc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance on many tasks like classification <ref type=\"bibr\" target=\"#b26\">[27]</ref>, anomaly detection <ref type=\"bibr\" target=\"#b11\">[12]</ref> and recommendation <ref type=\"bibr\" target=\"#b45\">[46,</re of Eq. <ref type=\"bibr\" target=\"#b8\">(9)</ref>. See the proof of Proposition 2 for the proof of Eq. <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p><p>Case 3. When \ud835\udc56 &lt; \ud835\udc59 \u2212 1, we first take derivative. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: egarding post-hoc uncertainty quantification for IID (i.e., nongraph) data, Alaa and van der Schaar <ref type=\"bibr\" target=\"#b1\">[2]</ref> propose a frequentistbased method inspired by jackknife resa ackknife resampling <ref type=\"bibr\" target=\"#b33\">[34]</ref> and the general principle outlined in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we seek to bridge the gap between frequentistbased uncertai >U \u0398 (\ud835\udc62) = C + \u0398 (\ud835\udc62) \u2212 C \u2212 \u0398 (\ud835\udc62)<label>(8)</label></formula><p>We note that Alaa and van Der Schaar <ref type=\"bibr\" target=\"#b1\">[2]</ref> leverage high-order influence functions to quantify the jack nife uncertainty for IID data. Though Eq. ( <ref type=\"formula\">4</ref>) shares the same form as in <ref type=\"bibr\" target=\"#b1\">[2]</ref> when the order is up to 1, our work bears three subtle diffe \" target=\"#b1\">[2]</ref> when the order is up to 1, our work bears three subtle differences. First, <ref type=\"bibr\" target=\"#b1\">[2]</ref> views the model parameters as statistical functionals of dat ependently from Gaussian distribution(s). Thus, our method is able to generalize on graphs. Second, <ref type=\"bibr\" target=\"#b1\">[2]</ref> works for regression or binary classification tasks by defau sion task by replacing the cross-entropy loss for node classification with mean squared error (MSE) <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Besides active learning, reinforcement learning (RL) is ext  Regarding the second challenge (C2), the key idea is to apply Hessian-vector product (Algorithm 1) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, which approximates   sample. There has been a rich collection of research works in quantifying uncertainty for IID data <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e include the following methods for comparison, AGE <ref type=\"bibr\" target=\"#b4\">[5]</ref>, ANRMAB <ref type=\"bibr\" target=\"#b17\">[18]</ref>, Coreset <ref type=\"bibr\" target=\"#b38\">[39]</ref>, Centra t cluster center. At each step of query, nodes with the highest scores are selected.</p><p>\u2022 ANRMAB <ref type=\"bibr\" target=\"#b17\">[18]</ref> leverages the same selection criterion as in AGE. Addition. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. More related works. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aph models, e.g., Erd\u0151s-R\u00e9nyi model<ref type=\"bibr\" target=\"#b13\">[14]</ref>, stochastic block model<ref type=\"bibr\" target=\"#b21\">[22]</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: diagonal matrix to simulate the convolution operation in graph signal processing. Defferrard et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> improve the efficieny of convolution operation on graphs w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trated in Fig. <ref type=\"figure\" target=\"#fig_2\">2</ref>. In DICM, following causal assumptions of <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, we split the late re spuriously correlated, which forms a spurious and even harmful path from \ud835\udc49 to \ud835\udc4c . Different from <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, we introduce an e =\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> further assume tha e under the scenario of attack and defense, we build our SCM with two main contributions: (i) while <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> focus on Out-of-Di that determine the observed variables (\ud835\udc4b , \ud835\udc4c ), which have been similarly assumed in existing works <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. To be specific, \ud835\udc46 nt space <ref type=\"bibr\" target=\"#b23\">[24]</ref>. For latent factors, we follow the assumption of <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, i.e., the mechani tack and defense on latent factors; (iii) propose our domain-attack invariance principle. Moreover, <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> infer latent facto nobserved abstractions constitute inputs and their outputs <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. <ref type=\"bibr\" t target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, or adversarial env  visual data which is the majority of benchmarks, since causality lies in conceptually latent space <ref type=\"bibr\" target=\"#b23\">[24]</ref>. For latent factors, we follow the assumption of <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#fig_2\">2</ref>. In DICM, following causal assumptions of <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, we split the latent factors into output-causative factors  purious and even harmful path from \ud835\udc49 to \ud835\udc4c . Different from <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, we introduce an extra domain variable \ud835\udc37 to explicitly mode  and their outputs <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\"  =\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> further assume that the latent factors can be disentangled  e, we build our SCM with two main contributions: (i) while <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> focus on Out-of-Distribution (OOD) problem and only conside , \ud835\udc4c ), which have been similarly assumed in existing works <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. To be specific, \ud835\udc46 as the Y-causative factors has a direct  arget=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, or adversarial environment inference <ref type=\"bibr\" targ 24]</ref>. For latent factors, we follow the assumption of <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, i.e., the mechanism that maps latent factors to observatio  propose our domain-attack invariance principle. Moreover, <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> infer latent factors with latent generative models while we  model the effect of domain shift on the latent factors, resulted by the attack and defense. Though <ref type=\"bibr\" target=\"#b40\">[41]</ref> also adds a domain variable, it acts as the domain index,  omain shift is incurred by manually injected bias, e.g., malicious noise by attacks. Different from <ref type=\"bibr\" target=\"#b40\">[41]</ref>, we focus on \ud835\udc49 related to manual bias. Recall that the key. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g test error of DNNs <ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, and thus by poiso e attacks have been devised by adopting new techniques and approximations, e.g., gradient alignment <ref type=\"bibr\" target=\"#b10\">[11]</ref>, computation graph unrolling <ref type=\"bibr\" target=\"#b18. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acy and robustness <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>], (ii) robust overfi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b24\">[25]</ref>, C&amp;W attack <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and AutoAttack <ref type=\"bibr\" target=\"#b7\">[8]</ref> become the popular attack baselines to evaluate robustness o  knowledge of target models.  <ref type=\"bibr\" target=\"#b3\">[4]</ref> attacks, and AutoAttack (AA.) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We choose WRN-34-10 (WRN) as the backbone for both CIFAR-10. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lar regularization <ref type=\"bibr\" target=\"#b34\">[35]</ref>, rethinking the misclassified examples <ref type=\"bibr\" target=\"#b49\">[50]</ref>, sample-wise importance reweighting <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nder Replay Buffer. Unlike existing invariant learning works <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> for OOD, which constr  by either partitioning the training data by prior knowledge <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b50\">51]</ref>, while our work focuses on single domain shift incurred by the attack and defense. <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposes an adversarial domain augmentation to achieve dom. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b24\">[25]</ref>, C&amp;W attack <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and AutoAttack <ref type=\"bibr\" target=\"#b7\">[8]</ref> become the popular attack baselines to evaluate robustness o  knowledge of target models.  <ref type=\"bibr\" target=\"#b3\">[4]</ref> attacks, and AutoAttack (AA.) <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We choose WRN-34-10 (WRN) as the backbone for both CIFAR-10. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b12\">[13]</ref>, PGD attack <ref type=\"bibr\" target=\"#b24\">[25]</ref>, C&amp;W attack <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and AutoAttack <ref type=\"bibr\" target=\"#b7\">[8]</ref> beco ge margin, which is comparable to white-box attacks that have the full knowledge of target models.  <ref type=\"bibr\" target=\"#b3\">[4]</ref> attacks, and AutoAttack (AA.) <ref type=\"bibr\" target=\"#b7\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st attacked examples \ud835\udc4b reveals that the attacker may exploit high-frequency components to fool DNNs <ref type=\"bibr\" target=\"#b47\">[48]</ref>, which are not perceivable to humans and spuriously correl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve. To address this challenge, we adopt the Normalized Weighted Geometric Mean (NWGM) approximation <ref type=\"bibr\" target=\"#b51\">[52]</ref> to move the outer sampling over \ud835\udc63 into the feature level, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ormance.</p><p>Attack Models. To fairly compare our method with EM and TAP, we use ResNet-18 (RN18) <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the backbone of all methods to generate poisoned data.  ible, we select VGG16 <ref type=\"bibr\" target=\"#b38\">[39]</ref>, ResNet-18 (RN18), ResNet-50 (RN50) <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and DenseNet-121 (DN121) <ref type=\"bibr\" target=\"#b16\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tly shows its effectiveness on maximizing test error of DNNs <ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta  learned causal invariant features.</p><p>Delusive Attack: For this attack, we adopt the setting of <ref type=\"bibr\" target=\"#b9\">[10]</ref> </p><p>Adversarial Defense: For adversarial defense, our ca ve <ref type=\"bibr\" target=\"#b17\">[18]</ref>, etc. Our work is based on loss maximization objective <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and further mitigates its limitation against adversarial t Evaluation metrics and training details. In Sec. 3.4, we introduce our delusive attack based on TAP <ref type=\"bibr\" target=\"#b9\">[10]</ref>. In line with TAP, we perform delusive attack on the base m t the maximum size of reply buffer as 10000 and the confounder set size is set as 20. We follow TAP <ref type=\"bibr\" target=\"#b9\">[10]</ref> to perform delusive attacks. For the standard trained class g/ns/1.0\"><head>7.2.2</head><p>Hyper-parameters and training details for evaluatioin. We follow TAP <ref type=\"bibr\" target=\"#b9\">[10]</ref> to craft poisoned data on a fixed pretrained model. For pre gainst adversarial attacks. Recent works also show the effectiveness of AT against delusive attacks <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b38\">[39]</ref>, GoogleNet <ref type=\"bibr\" target=\"#b41\">[42]</ref>, DenseNet-121 (DN121) <ref type=\"bibr\" target=\"#b16\">[17]</ref> and MobileNet-V2 (MOB-V2) <ref type=\"bibr\" target=\"#b15\">[ Net-18 (RN18), ResNet-50 (RN50) <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and DenseNet-121 (DN121) <ref type=\"bibr\" target=\"#b16\">[17]</ref> as our target models. We reproduce these models based on a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b45\">46]</ref>; or more unlabeled data <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, both of which extend single domain to multiple domains and eover, for the recent defense methods that require more data <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> or an ensemble of cross-domain models <ref type=\"bibr\" targ br\" target=\"#b59\">[60]</ref>, and adding more unlabeled data <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Based on our causal graph, all of them can be encapsulated. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  variables to the latent factors: those unobserved abstractions constitute inputs and their outputs <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: main variable, e.g., some defense seek robust prediction through an ensemble of cross-domain models <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>; or more unlabeled arget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> or an ensemble of cross-domain models <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, we argue that the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b38\">[39]</ref>, GoogleNet <ref type=\"bibr\" target=\"#b41\">[42]</ref>, DenseNet-121 (DN121) <ref type=\"bibr\" target=\"#b16\">[17]</ref> and MobileNet-V2 (MOB-V2) <ref type=\"bibr\" target=\"#b15\">[ Net-18 (RN18), ResNet-50 (RN50) <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and DenseNet-121 (DN121) <ref type=\"bibr\" target=\"#b16\">[17]</ref> as our target models. We reproduce these models based on a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: purious bias by causal intervention. Specifically, we propose to use the backdoor adjustment method <ref type=\"bibr\" target=\"#b30\">[31]</ref> for intervention, i.e., blocking the spurious path from \ud835\udc49  with all possible views or backgrounds e.g. for images, is impossible, we apply backdoor adjustment <ref type=\"bibr\" target=\"#b30\">[31]</ref> to do \"virtual\" intervention on \ud835\udc4b in Eq. 5.</p><formula xm 40\">41]</ref> infer latent factors with latent generative models while we do by causal intervention <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Another similar work is <ref type=\"bibr\" target=\"#b25\">[2 spurious bias by causal intervention. Specifically, we propose to use the backdoor adjustment method<ref type=\"bibr\" target=\"#b30\">[31]</ref> for intervention, i.e., blocking the spurious path from \ud835\udc49 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: site side, AT itself suffers from poor generalization with relatively low robustness on test domain <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Since we assume t type=\"bibr\" target=\"#b58\">59</ref>], (ii) robust overfitting <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, for which we take a causal view to derive our insight and  g is a common limitation existing in current defense methods <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, which means that model robustness on test data begins decr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acy and robustness <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>], (ii) robust overfi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  development of sampling techniques for synchronizing multi-threaded applications. These techniques <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref> describe one of   FAST AND GENERIC MULTI-THREADED SIMULATION REQUIREMENTS</head><p>Time-based sampling methodologies <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref> present the firs used directly for synchronizing multi-threaded applications. To sample multi-threaded applications, <ref type=\"bibr\" target=\"#b9\">[10]</ref> and <ref type=\"bibr\" target=\"#b10\">[11]</ref> were proposed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nchronizing multi-threaded applications. These techniques <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref> describe one of the first generic sampling solutions for m N REQUIREMENTS</head><p>Time-based sampling methodologies <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref> present the first workable solution to sample generic mult ulation points may be required, and such regions are highly sensitive to warmup and aliasing issues <ref type=\"bibr\" target=\"#b10\">[11]</ref>. At the same time, we also need to make sure that there ar d applications. To sample multi-threaded applications, <ref type=\"bibr\" target=\"#b9\">[10]</ref> and <ref type=\"bibr\" target=\"#b10\">[11]</ref> were proposed which use time as a sampling unit by fast-fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  methodology. Later proposals, in the form of application and synchronizationspecific methodologies <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b13\">[14]</ref>, exceeded the p tunately, these methodologies are tied to specific application characteristics (the use of barriers <ref type=\"bibr\" target=\"#b11\">[12]</ref> or tasks <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref t s errors for the passive wait policy are as high as 20%.</p><p>Previous works like the BarrierPoint <ref type=\"bibr\" target=\"#b11\">[12]</ref> methodology use inter-barrier regions as the unit of work, ibr\" target=\"#b17\">[18]</ref> with 8 threads. BarrierPoint works well for NPB with the A input size <ref type=\"bibr\" target=\"#b11\">[12]</ref>, but as the input sizes grow, for classes C, D and E, inte ify similar runtime behavior. Although BBVs are used in this work, other feature vector information <ref type=\"bibr\" target=\"#b11\">[12]</ref> can be concatenated on a per-thread basis and can be used  -end methodology to identify workload representatives for performance extrapolation. Previous works <ref type=\"bibr\" target=\"#b11\">[12]</ref> have shown that extrapolation in this manner does apply to  passive applications is 2.23%. These error rates are comparable to previous sampling methodologies <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>The looppoints identified are representative of the solute values and a small difference can result in a high percentage error.</p><p>Previous research <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref> has presented d mulation. Another methodology which reduces the simulation time of MT applications significantly is <ref type=\"bibr\" target=\"#b11\">[12]</ref>, a microarchitecture-independent, Simpoint-like approach w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: LoopPoint that (a) uses loop iterations as the main unit of work, (b) utilizes constrained pinballs <ref type=\"bibr\" target=\"#b15\">[16]</ref> (user-level checkpoints that allow for reproducible analys producible execution. We leverage Intel's Pin <ref type=\"bibr\" target=\"#b28\">[29]</ref> and Pinplay <ref type=\"bibr\" target=\"#b15\">[16]</ref> tools to generate reproducible, constrained, multi-threade www.tei-c.org/ns/1.0\"><head>C. Constrained Execution Infrastructure</head><p>We use Intel's PinPlay <ref type=\"bibr\" target=\"#b15\">[16]</ref> infrastructure that provides tools to record and replay ar al with artificial stalls.</p><p>Handling busy-waiting. The problem of busy-waiting is mentioned in <ref type=\"bibr\" target=\"#b15\">[16]</ref> although in the context of multi-process programs using Me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: take advantage of the underlying hardware, while not constraining execution to a deterministic path <ref type=\"bibr\" target=\"#b14\">[15]</ref> that might not exhibit true application behavior.</p><p>We. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  overall amount of work required to simulate applications in detail, including input size reduction <ref type=\"bibr\" target=\"#b2\">[3]</ref> and benchmark synthesis <ref type=\"bibr\" target=\"#b3\">[4]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that there are enough intervals in the application for the clustering algorithm to work efficiently <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>Prior analyses <ref type=\"bibr\" target=\"#b0\">[1]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sult in a high percentage error.</p><p>Previous research <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref> has presented differences in a similar manner.</p></div> <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of application and synchronizationspecific methodologies <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b13\">[14]</ref>, exceeded the performance of time-based sampling and allow riers <ref type=\"bibr\" target=\"#b11\">[12]</ref> or tasks <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>), and therefore do not represent a general sampling soluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pport this type of execution to allow for reliable, reproducible execution. We leverage Intel's Pin <ref type=\"bibr\" target=\"#b28\">[29]</ref> and Pinplay <ref type=\"bibr\" target=\"#b15\">[16]</ref> tool  it can be replayed in both constrained and unconstrained mode later on. We have developed Pintools <ref type=\"bibr\" target=\"#b28\">[29]</ref> to generate BBVs of the regions which are fed to Simpoint . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  CPU2017 is available in two different versions depending on the evaluation purpose: rate and speed <ref type=\"bibr\" target=\"#b31\">[32]</ref>. The rate version is used to estimate the throughput of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: LoopPoint that (a) uses loop iterations as the main unit of work, (b) utilizes constrained pinballs <ref type=\"bibr\" target=\"#b15\">[16]</ref> (user-level checkpoints that allow for reproducible analys producible execution. We leverage Intel's Pin <ref type=\"bibr\" target=\"#b28\">[29]</ref> and Pinplay <ref type=\"bibr\" target=\"#b15\">[16]</ref> tools to generate reproducible, constrained, multi-threade www.tei-c.org/ns/1.0\"><head>C. Constrained Execution Infrastructure</head><p>We use Intel's PinPlay <ref type=\"bibr\" target=\"#b15\">[16]</ref> infrastructure that provides tools to record and replay ar al with artificial stalls.</p><p>Handling busy-waiting. The problem of busy-waiting is mentioned in <ref type=\"bibr\" target=\"#b15\">[16]</ref> although in the context of multi-process programs using Me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g down the computing requirements for the clustering algorithm. We use K-means clustering technique <ref type=\"bibr\" target=\"#b23\">[24]</ref> along with a BIC goodness criteria <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of application and synchronizationspecific methodologies <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b13\">[14]</ref>, exceeded the performance of time-based sampling and allow riers <ref type=\"bibr\" target=\"#b11\">[12]</ref> or tasks <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>), and therefore do not represent a general sampling soluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that rely on instruction counting can perform poorly when dealing with multi-threaded applications <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We demonstrate this by performing a naive adaptation of S  and therefore the use of IPC to evaluate the performance of multi-threaded workloads is infeasible <ref type=\"bibr\" target=\"#b16\">[17]</ref>. LoopPoint proposes a strategy that identifies regions of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  programs. The idea of using loop iterations as slices for single-threaded programs was proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. With loop entries as slice boundaries, the simulation reg e whose executions correlate to a phase change in the application are called software phase markers <ref type=\"bibr\" target=\"#b18\">[19]</ref>. The software phase markers can accurately identify the ph ruction-count target is achieved. Although this can be implemented in several ways (as described in <ref type=\"bibr\" target=\"#b18\">[19]</ref>), we do not currently differentiate between inner and oute e code locations whose executions correlate with a phase change are called a software phase markers <ref type=\"bibr\" target=\"#b18\">[19]</ref>. The software phase markers identify the phase changes tha  interactive simulation making use of in-memory checkpoints. A prior work on software phase markers <ref type=\"bibr\" target=\"#b18\">[19]</ref> uses loops to determine simulation regions, but is limited. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: unconstrained simulation using pinballs is to convert them to executable checkpoints, called ELFies <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> ay. One such method is to convert them to ELF binaries, called ELFies, as discussed in a prior work <ref type=\"bibr\" target=\"#b20\">[21]</ref>. In this paper, however, we are not evaluating ELFies. Ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: LoopPoint that (a) uses loop iterations as the main unit of work, (b) utilizes constrained pinballs <ref type=\"bibr\" target=\"#b15\">[16]</ref> (user-level checkpoints that allow for reproducible analys producible execution. We leverage Intel's Pin <ref type=\"bibr\" target=\"#b28\">[29]</ref> and Pinplay <ref type=\"bibr\" target=\"#b15\">[16]</ref> tools to generate reproducible, constrained, multi-threade www.tei-c.org/ns/1.0\"><head>C. Constrained Execution Infrastructure</head><p>We use Intel's PinPlay <ref type=\"bibr\" target=\"#b15\">[16]</ref> infrastructure that provides tools to record and replay ar al with artificial stalls.</p><p>Handling busy-waiting. The problem of busy-waiting is mentioned in <ref type=\"bibr\" target=\"#b15\">[16]</ref> although in the context of multi-process programs using Me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g down the computing requirements for the clustering algorithm. We use K-means clustering technique <ref type=\"bibr\" target=\"#b23\">[24]</ref> along with a BIC goodness criteria <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of application and synchronizationspecific methodologies <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b13\">[14]</ref>, exceeded the performance of time-based sampling and allow riers <ref type=\"bibr\" target=\"#b11\">[12]</ref> or tasks <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>), and therefore do not represent a general sampling soluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: LoopPoint that (a) uses loop iterations as the main unit of work, (b) utilizes constrained pinballs <ref type=\"bibr\" target=\"#b15\">[16]</ref> (user-level checkpoints that allow for reproducible analys producible execution. We leverage Intel's Pin <ref type=\"bibr\" target=\"#b28\">[29]</ref> and Pinplay <ref type=\"bibr\" target=\"#b15\">[16]</ref> tools to generate reproducible, constrained, multi-threade www.tei-c.org/ns/1.0\"><head>C. Constrained Execution Infrastructure</head><p>We use Intel's PinPlay <ref type=\"bibr\" target=\"#b15\">[16]</ref> infrastructure that provides tools to record and replay ar al with artificial stalls.</p><p>Handling busy-waiting. The problem of busy-waiting is mentioned in <ref type=\"bibr\" target=\"#b15\">[16]</ref> although in the context of multi-process programs using Me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  overall amount of work required to simulate applications in detail, including input size reduction <ref type=\"bibr\" target=\"#b2\">[3]</ref> and benchmark synthesis <ref type=\"bibr\" target=\"#b3\">[4]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplication workload reduction technique that traces its roots back decades. From the earliest works <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, researchers have b urately predict the original workload behavior, and significantly reduce the simulation time needed <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Apart from s e=\"bibr\" target=\"#b16\">[17]</ref>. We demonstrate this by performing a naive adaptation of Simpoint <ref type=\"bibr\" target=\"#b0\">[1]</ref> for multi-threaded applications of SPEC CPU2017 using 8 thre ing the whole-program performance based on the simulation results of the selected regions. SimPoint <ref type=\"bibr\" target=\"#b0\">[1]</ref> is a popular simulation region selection approach. It works  hreaded programs, using a fixed instruction count called the slice size has been shown to work well <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In our work, we keep slices of approximately similar sizes  ering algorithm to work efficiently <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>Prior analyses <ref type=\"bibr\" target=\"#b0\">[1]</ref> on single-threaded applications showed that fixed size (of 1 <ref type=\"bibr\" target=\"#b24\">[25]</ref> to select clustering in a method similar to previous work <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The K-means algorithm requires the selection of the maximum tructure within the code to create a representative sample application was the Simpoint methodology <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The authors show that applications can be broken down into  sentative regions, called looppoints, by taking into account several key factors like understanding <ref type=\"bibr\" target=\"#b0\">(1)</ref> where to simulate which requires (1a) an accurate analysis m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>Simulation solutions alone are insufficient because of the significant slowdown (10,000\u00d7 or more <ref type=\"bibr\" target=\"#b49\">[50]</ref>) seen when simulating applications with industrial-quality. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplication workload reduction technique that traces its roots back decades. From the earliest works <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, researchers have b urately predict the original workload behavior, and significantly reduce the simulation time needed <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Apart from s e=\"bibr\" target=\"#b16\">[17]</ref>. We demonstrate this by performing a naive adaptation of Simpoint <ref type=\"bibr\" target=\"#b0\">[1]</ref> for multi-threaded applications of SPEC CPU2017 using 8 thre ing the whole-program performance based on the simulation results of the selected regions. SimPoint <ref type=\"bibr\" target=\"#b0\">[1]</ref> is a popular simulation region selection approach. It works  hreaded programs, using a fixed instruction count called the slice size has been shown to work well <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In our work, we keep slices of approximately similar sizes  ering algorithm to work efficiently <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>Prior analyses <ref type=\"bibr\" target=\"#b0\">[1]</ref> on single-threaded applications showed that fixed size (of 1 <ref type=\"bibr\" target=\"#b24\">[25]</ref> to select clustering in a method similar to previous work <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The K-means algorithm requires the selection of the maximum tructure within the code to create a representative sample application was the Simpoint methodology <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The authors show that applications can be broken down into  sentative regions, called looppoints, by taking into account several key factors like understanding <ref type=\"bibr\" target=\"#b0\">(1)</ref> where to simulate which requires (1a) an accurate analysis m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ions) intervals of execution can be used to identify phase behavior. Using varying length intervals <ref type=\"bibr\" target=\"#b22\">[23]</ref> corresponding to the application periodicity can help mark. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ail, including input size reduction <ref type=\"bibr\" target=\"#b2\">[3]</ref> and benchmark synthesis <ref type=\"bibr\" target=\"#b3\">[4]</ref>. While each Fig. <ref type=\"figure\">1</ref>: Approximate tim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  programs. The idea of using loop iterations as slices for single-threaded programs was proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. With loop entries as slice boundaries, the simulation reg e whose executions correlate to a phase change in the application are called software phase markers <ref type=\"bibr\" target=\"#b18\">[19]</ref>. The software phase markers can accurately identify the ph ruction-count target is achieved. Although this can be implemented in several ways (as described in <ref type=\"bibr\" target=\"#b18\">[19]</ref>), we do not currently differentiate between inner and oute e code locations whose executions correlate with a phase change are called a software phase markers <ref type=\"bibr\" target=\"#b18\">[19]</ref>. The software phase markers identify the phase changes tha  interactive simulation making use of in-memory checkpoints. A prior work on software phase markers <ref type=\"bibr\" target=\"#b18\">[19]</ref> uses loops to determine simulation regions, but is limited. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  CPU2017 is available in two different versions depending on the evaluation purpose: rate and speed <ref type=\"bibr\" target=\"#b31\">[32]</ref>. The rate version is used to estimate the throughput of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  CPU2017 is available in two different versions depending on the evaluation purpose: rate and speed <ref type=\"bibr\" target=\"#b31\">[32]</ref>. The rate version is used to estimate the throughput of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sult in a high percentage error.</p><p>Previous research <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref> has presented differences in a similar manner.</p></div> <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  higher throughput per watt thanks to GPU's power efficiency advantage.</p><p>Related Work. Ithemal <ref type=\"bibr\" target=\"#b28\">[28]</ref> represents the closest related work to this effort, propos  Transformer <ref type=\"bibr\" target=\"#b52\">[51]</ref>, for instruction latency prediction. Ithemal <ref type=\"bibr\" target=\"#b28\">[28]</ref> follows this strategy and adopts LSTM to predict basic blo  constrained computation budget.</p><p>Comparison with Ithemal. We also compare SimNet with Ithemal <ref type=\"bibr\" target=\"#b28\">[28]</ref>, a state-of-the-art MLbased latency prediction approach. I =\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">RELATED WORK</head><p>ML for Latency Prediction. Ithemal <ref type=\"bibr\" target=\"#b28\">[28]</ref> uses LSTM models to predict the execution latency of stati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to predict a processor's performance/power based on those obtained on different types of processors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e virtualization to fast-forward between samples, so different samples can be simulated in parallel <ref type=\"bibr\" target=\"#b41\">[40]</ref>. These statistical simulation approaches can be used toget. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs are around 10%. For example, ZSim reports an average error of 9.7% against an Intel Westmere CPU <ref type=\"bibr\" target=\"#b39\">[39]</ref>, and <ref type=\"bibr\" target=\"#b12\">[13]</ref> reports a 1 itional Simulation Acceleration. ZSim is an X86 simulator that supports many-core system simulation <ref type=\"bibr\" target=\"#b39\">[39]</ref>. It decouples the simulation of individual cores and resou /threads in parallel <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>. However, they cannot simulate a single program/thread in p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rocess sequences, such as recurrent neural networks <ref type=\"bibr\" target=\"#b38\">[38]</ref>, LSTM <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and Transformer <ref type=\"bibr\" target=\"#b52\">[51]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: software but require a huge amount of effort to develop and validate register-transfer level models <ref type=\"bibr\" target=\"#b21\">[21]</ref>. In comparison, our work accelerates simulation from a dif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctional simulation can be accomplished using fast instruction set simulators/emulators such as QEMU <ref type=\"bibr\" target=\"#b5\">[6]</ref>. History context simulation is also fast because it only req. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: esentative ones with the aim that the selected samples capture the overall execution behaviors well <ref type=\"bibr\" target=\"#b36\">[36,</ref><ref type=\"bibr\" target=\"#b46\">45]</ref>. Similarly, PinPoi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: periments and previous research show such simulation can be done at \u223c 100 MIPS on a single CPU core <ref type=\"bibr\" target=\"#b50\">[49]</ref>, which is much larger than SimNet's simulation throughputs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on the ZINC-2M dataset, which cotains 2 million unlabeled molecule graphs sampled from MoleculeNet <ref type=\"bibr\" target=\"#b40\">(Wu et al., 2018a)</ref>, then evaluate its performance on eight bina ther evaluate GraphCV under semi-supervised setting. Besides, the datasets sampled from MoleculeNet <ref type=\"bibr\" target=\"#b40\">(Wu et al., 2018a)</ref> are employed to evaluate our model under tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: D-GCL(Suresh et al., 2021), GASSL <ref type=\"bibr\" target=\"#b45\">(Yang et al., 2021)</ref>, InfoGCL <ref type=\"bibr\" target=\"#b42\">(Xu et al., 2021a)</ref>, RGCL <ref type=\"bibr\" target=\"#b21\">(Li et  aph contrastive learning by finding more challenge view <ref type=\"bibr\">(Suresh et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Xu et al., 2021a;</ref><ref type=\"bibr\" target=\"#b47\">You et al., 202 ning of graph structure data. Current methods <ref type=\"bibr\" target=\"#b38\">(Wu et al., 2020;</ref><ref type=\"bibr\" target=\"#b42\">Xu et al., 2021a;</ref><ref type=\"bibr\">Suresh et al., 2021;</ref><re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rategy of SSL, contrastive learning follows the mutual information maximization principle (InfoMax) <ref type=\"bibr\" target=\"#b36\">(Veli\u010dkovi\u0107 et al., 2019)</ref> to maximize the agreements of the pos contrastive learning works generally rely on various graph augmentation (transformation) techniques <ref type=\"bibr\" target=\"#b36\">(Veli\u010dkovi\u0107 et al., 2019;</ref><ref type=\"bibr\" target=\"#b28\">Qiu et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b28\">(Qiu et al., 2020;</ref><ref type=\"bibr\" target=\"#b13\">Hassani &amp; Khasahmadi, 2020a;</ref><ref type=\"bibr\" target=\"#b47\">You et al., 2020)</ref> usually rely on augmentor(s) tp\u00a8q (e.g., Iden ks <ref type=\"bibr\">(Suresh et al., 2021;</ref><ref type=\"bibr\" target=\"#b21\">Li et al., 2022;</ref><ref type=\"bibr\" target=\"#b47\">You et al., 2021)</ref> propose to use automated augmentations to ext w <ref type=\"bibr\">(Suresh et al., 2021;</ref><ref type=\"bibr\" target=\"#b42\">Xu et al., 2021a;</ref><ref type=\"bibr\" target=\"#b47\">You et al., 2021)</ref> or adding adversarial perturbation <ref type= scrimination. Recent works(Suresh et al., 2021;<ref type=\"bibr\" target=\"#b21\">Li et al., 2022;</ref><ref type=\"bibr\" target=\"#b47\">You et al., 2021)</ref> propose to use automated augmentations to ext  al., 2019)</ref>, as the backbone network, and the commonly-used graph data augmentation operators <ref type=\"bibr\" target=\"#b47\">(You et al., 2020)</ref>, such as node dropping, edge perturbation, s n learning setting, we compare GraphCV with the eight SOTA self-supervised learning methods GraphCL <ref type=\"bibr\" target=\"#b47\">(You et al., 2020)</ref>, InfoGraph <ref type=\"bibr\" target=\"#b32\">(S  et al., 2020b)</ref>, ContextPred <ref type=\"bibr\" target=\"#b17\">(Hu et al., 2020b)</ref>, GraphCL <ref type=\"bibr\" target=\"#b47\">(You et al., 2020)</ref>, GraphLoG <ref type=\"bibr\" target=\"#b44\">(Xu stream task-specific classification. The graph augmentation operations used in our work are same as <ref type=\"bibr\" target=\"#b47\">(You et al., 2020)</ref>, including node dropping, edge perturbation,  of graph-level property classification. For the transfer learning setting, we follow previous work <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 202 al performance. For transfer learning setting, we follow the finetuning procedures of previous work <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 202 9)</ref> as the backbone graph encoder and the model is optimized through Adam optimizer. We follow <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b45\">Yang et al., 2 tation details of transfer learning, we basically follow the pre-training setting of previous works <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 202 e evaluation protocols in the previous works <ref type=\"bibr\" target=\"#b32\">(Sun et al., 2019;</ref><ref type=\"bibr\" target=\"#b47\">You et al., 2020;</ref><ref type=\"bibr\" target=\"#b20\">Li et al., 2021 =\"#b28\">Qiu et al., 2020;</ref><ref type=\"bibr\" target=\"#b14\">Hassani &amp; Khasahmadi, 2020b;</ref><ref type=\"bibr\" target=\"#b47\">You et al., 2020;</ref><ref type=\"bibr\" target=\"#b32\">Sun et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sical unsupervised representation learning methods, including node2vec ( Leskovec, 2016), graph2vec <ref type=\"bibr\" target=\"#b27\">(Narayanan et al., 2017)</ref>, and GVAE <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  size and node degree), which might be helpful under certain domains but not necessarily for others <ref type=\"bibr\" target=\"#b3\">(Bevilacqua et al., 2021)</ref>, thus fail to guarantee stronger robus h size and node degree), which might be helpful under certain domains but not necessarily for others<ref type=\"bibr\" target=\"#b3\">(Bevilacqua et al., 2021)</ref>, thus fail to guarantee stronger robus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er learning setting, we follow previous work <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 2021b</ref>) to pretrain our model on the ZINC-2M dataset, w the pre-training setting of previous works <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 2021b</ref>).</p></div> <div xmlns=\"http://www.tei-c.org/n (Hu et al., 2020b)</ref>, GraphCL <ref type=\"bibr\" target=\"#b47\">(You et al., 2020)</ref>, GraphLoG <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2021b)</ref>, AD-GCL (Suresh et al., 2021) and RGCL <ref  w the finetuning procedures of previous work <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 2021b)</ref> and report the mean ROC-AUC scores with stand. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er learning setting, we follow previous work <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 2021b</ref>) to pretrain our model on the ZINC-2M dataset, w the pre-training setting of previous works <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 2021b</ref>).</p></div> <div xmlns=\"http://www.tei-c.org/n (Hu et al., 2020b)</ref>, GraphCL <ref type=\"bibr\" target=\"#b47\">(You et al., 2020)</ref>, GraphLoG <ref type=\"bibr\" target=\"#b44\">(Xu et al., 2021b)</ref>, AD-GCL (Suresh et al., 2021) and RGCL <ref  w the finetuning procedures of previous work <ref type=\"bibr\" target=\"#b47\">(You et al., 2020;</ref><ref type=\"bibr\" target=\"#b44\">Xu et al., 2021b)</ref> and report the mean ROC-AUC scores with stand. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt of artificial intelligence, a recent breakthrough on in silico protein folding, namely AlphaFold <ref type=\"bibr\" target=\"#b0\">[1]</ref>, unprecedentedly achieved \"near experimental accuracy\" on a  d directly from <ref type=\"bibr\" target=\"#b18\">[19]</ref>; results of AlphaFold(-Multimer) are from <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. We do not include the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b7\">[8]</ref> + BFD for monomers. For multimers, we additionally used JackHMMER with UniProt <ref type=\"bibr\" target=\"#b8\">[9]</ref> to search for sequences with species annotations. We used th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </head><p>Plenty of efforts have been devoted to reimplementing or improving AlphaFold. RoseTTAFold <ref type=\"bibr\" target=\"#b20\">[21]</ref>, known as the earliest re-implementation of AlphaFold (bef. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndoubtedly shed light on countless new possibilities of life science exploration. Well discussed in <ref type=\"bibr\" target=\"#b2\">[3]</ref>, these possibilities include assistance in solving experimen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>We reused the genetic search protocol in AlphaFold and AlphaFold-Multimer. We used JackHMMER <ref type=\"bibr\" target=\"#b3\">[4]</ref> with MGnify <ref type=\"bibr\" target=\"#b4\">[5]</ref>, JackHMM. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </head><p>Plenty of efforts have been devoted to reimplementing or improving AlphaFold. RoseTTAFold <ref type=\"bibr\" target=\"#b20\">[21]</ref>, known as the earliest re-implementation of AlphaFold (bef. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ormer acceleration <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, we fused the softmax and layer normalization operators. Th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndoubtedly shed light on countless new possibilities of life science exploration. Well discussed in <ref type=\"bibr\" target=\"#b2\">[3]</ref>, these possibilities include assistance in solving experimen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>We reused the genetic search protocol in AlphaFold and AlphaFold-Multimer. We used JackHMMER <ref type=\"bibr\" target=\"#b3\">[4]</ref> with MGnify <ref type=\"bibr\" target=\"#b4\">[5]</ref>, JackHMM. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>We reused the genetic search protocol in AlphaFold and AlphaFold-Multimer. We used JackHMMER <ref type=\"bibr\" target=\"#b3\">[4]</ref> with MGnify <ref type=\"bibr\" target=\"#b4\">[5]</ref>, JackHMM. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead><p>We reused the genetic search protocol in AlphaFold and AlphaFold-Multimer. We used JackHMMER <ref type=\"bibr\" target=\"#b3\">[4]</ref> with MGnify <ref type=\"bibr\" target=\"#b4\">[5]</ref>, JackHMM. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b16\">[17]</ref>, GAT <ref type=\"bibr\" target=\"#b17\">[18]</ref>, GIN-Virtual <ref type=\"bibr\" target=\"#b18\">[19]</ref>, Deeper-GCN <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The  each node. And the Graph Embeddings are set to zeros before the first layer.</p><p>Although GIN-VN <ref type=\"bibr\" target=\"#b18\">[19]</ref> wants to alleviate the lagging message passing problem bet ause the feature of one takes multiple iterations/layers to reach another. Therefore, Gilmer et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> came up with a variant, GIN-VIRTUAL, which chose to add a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ameters. metabolism, excretion, and toxicity (ADMET) are highly related to the acid/base properties <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Moreover, the HUMO-LUMO gap, a quantum chemical property, m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ameworks have been developed and performed well in the application of small molecule drug discovery <ref type=\"bibr\" target=\"#b9\">[10]</ref>, such as transformer-based BERT <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: However, to achieve the better performance, TokenGT <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GRPE <ref type=\"bibr\" target=\"#b24\">[25]</ref>, EGT <ref type=\"bibr\" target=\"#b25\">[26]</ref> and Graphor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  well-verified pre-train language models to predict the molecule property. For example, SMILES-BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref> directly applied the BERT-style training strategy to the S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olecule representation learned by CoAt-GIN using T-distributed Stochastic Neighbor Embedding(t-SNE) <ref type=\"bibr\" target=\"#b27\">[28]</ref> method and the scikit-learn <ref type=\"bibr\" target=\"#b28\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref>. However, small molecule drug research is confronted with ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: However, to achieve the better performance, TokenGT <ref type=\"bibr\" target=\"#b23\">[24]</ref>, GRPE <ref type=\"bibr\" target=\"#b24\">[25]</ref>, EGT <ref type=\"bibr\" target=\"#b25\">[26]</ref> and Graphor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: these two embeddings will be updated for layerwise iteration. Using the embedding block provided by <ref type=\"bibr\" target=\"#b20\">[21]</ref>, we initialize the Node Embedding as the atom type of each  has shown that the method works effectively across a wide range of graph-level prediction datasets <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>However, as shown in Fig <ref type=\"figure\" target= <p>As mentioned before, the CoAtGIN model is evaluated in the Large Scale Challenge (LSC) benchmark <ref type=\"bibr\" target=\"#b20\">[21]</ref> PCQM4Mv2 dataset. There are many fancy models using the be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ameters. metabolism, excretion, and toxicity (ADMET) are highly related to the acid/base properties <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Moreover, the HUMO-LUMO gap, a quantum chemical property, m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref>. However, small molecule drug research is confronted with ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nspired by faithfulness enhanced machine translation works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we introduce a max-margin loss into the summarization task  of maximizing m t . We choose Quintic function (fifth power) here as it is shown to be more stable <ref type=\"bibr\" target=\"#b37\">[38]</ref>. The first factor (1 \u2212 P t ) is for fitting the two possib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uce our proposed Faithfulness Enhanced Summarization model, which is generally built on Transformer <ref type=\"bibr\" target=\"#b31\">[32]</ref>. The faithfulness enhancement is implemented from three as tions: h i = hi + l i . We iteratively use the above GAT layer and position-wise feed-forward layer <ref type=\"bibr\" target=\"#b31\">[32]</ref> to update each node representation. The output entity feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: thus will be prone to extrinsic errors. Inspired by faithfulness enhanced machine translation works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we introduce a ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: take external sentence guidance as input. ( <ref type=\"formula\" target=\"#formula_5\">5</ref>) SimCLS <ref type=\"bibr\" target=\"#b40\">[41]</ref> bridges the gap between the learning objective and evaluat t models on CNN/DM and Xsum. Marked ROUGE results are from <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>. Numbers in bold mean that the improvement to the best base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is a key challenge in the summarization task, and less progress has been made on it. Pioneer works <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> incorporated fact . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: swers from the source document. Then, it uses a finetuned answer-conditional question generation T5 <ref type=\"bibr\" target=\"#b38\">[39]</ref> model to generate questions via beam search. To ensure the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b4\">5]</ref> has largely improved the fluency and salience of generated summaries. However, studies <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref> showed that many summa  BERT embeddings. We also evaluate our approach with the latest factual consistency metrics, FactCC <ref type=\"bibr\" target=\"#b5\">[6]</ref> and QuestEval <ref type=\"bibr\" target=\"#b6\">[7]</ref>. FactC. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n recent years, text generation has made impressive progress <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. The abstractive summar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: de impressive progress <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. The abstractive summarization task, aiming to produce a conc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ucture modeling has also been shown to be effective in summarization tasks. For example, Jin et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> incorporated semantic dependency graphs to help generate s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8\">[9]</ref>.</p><p>Given the constructed graph with node features, we use graph attention networks <ref type=\"bibr\" target=\"#b33\">[34]</ref> to update the representations of our semantic nodes. We re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eference-free evaluation.</p><p>Implementation Details. We implement our experiments in Huggingface <ref type=\"bibr\" target=\"#b41\">[42]</ref> on 4 NVIDIA A100 GPUs. We build our models based on BART (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: active Summarization. In recent years, the research on text generation has made impressive progress <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, which promotes th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: enerated summary is not entailed by the information presented in the source document. Durmus et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> highlighted two notions of the unfaithfulness problem in sum that uses QA in the postgeneration stage for evaluating the faithfulness of the generated summaries <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, as shown in Figure <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the effectiveness of our FES model by conducting extensive experiments on public benchmark CNN/DM <ref type=\"bibr\" target=\"#b11\">[12]</ref> and XSum <ref type=\"bibr\" target=\"#b12\">[13]</ref> dataset. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >. Most works apply an encoder-decoder architecture to implicitly learn the summarization procedure <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. More recently, ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: active Summarization. In recent years, the research on text generation has made impressive progress <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, which promotes th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l words and phrases not featured in the source text to capture the salient ideas of the source text <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Most works apply an encoder-decoder architecture to impli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a, to train summarization systems that are better at distinguishing between them. Aralikatte et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> introduced focus attention mechanism to encourage decoders. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , POS tagging, dependency parsing, and text classification <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ore recently, applying pretrained language models as encoder <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> or pre-training the generation process by leveraging a larg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , POS tagging, dependency parsing, and text classification <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , POS tagging, dependency parsing, and text classification <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e the effectiveness of our FES model by conducting extensive experiments on public benchmark CNN/DM <ref type=\"bibr\" target=\"#b11\">[12]</ref> and XSum <ref type=\"bibr\" target=\"#b12\">[13]</ref> dataset. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , POS tagging, dependency parsing, and text classification <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n better semantic meanings and a graph-aware decoder to utilize the encoded information. Cao et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> used contrastive learning to help the model be aware of the d the facts in the source article with knowledge graphs based on a graph neural network. Cao et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> proposed to leverage reference summaries as positive traini and integrates factual relations into the summary generation process via graph attention. (2) CLIFF <ref type=\"bibr\" target=\"#b9\">[10]</ref> leverages reference summaries as positive data and erroneou on to see whether our generated summaries are faithful to the source document. Following Cao et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we randomly sample 100 cases from CNN/DM and XSum, and the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atches of unigram, bigrams, and the longest common subsequence, respectively. We then use BERTScore <ref type=\"bibr\" target=\"#b43\">[44]</ref> to calculate a similarity score between the summaries base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 8\">19]</ref> or pre-training the generation process by leveraging a large-scale of unlabeled corpus <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> brings significant art abstractive summarization model pretrained with a denoising autoencoding objective. (4) PEGASUS <ref type=\"bibr\" target=\"#b19\">[20]</ref> is a pre-training large Transformer-based encoder-decoder . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is a key challenge in the summarization task, and less progress has been made on it. Pioneer works <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> incorporated fact . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: luency and salience of generated summaries. However, studies <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref> showed that many summarization models suffer from unfaithfuln e for evaluating the faithfulness of the generated summaries <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, as shown in Figure <ref type=\"figure\" target=\"#fig_0\">1</ref ask. Hence, we pre-construct QA pairs for each case using QuestEval tool provided by Scialom et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Concretely, QuestEval first selects a set of the named enti  the quality of the QA pairs, we only select those questions for which the question-answering model <ref type=\"bibr\" target=\"#b6\">[7]</ref> gives the right answers. Finally, we take 38 QA pairs for CN he latest factual consistency metrics, FactCC <ref type=\"bibr\" target=\"#b5\">[6]</ref> and QuestEval <ref type=\"bibr\" target=\"#b6\">[7]</ref>. FactCC is a weakly-supervised, model-based approach for ver. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -task learning, such as word segmentation, POS tagging, dependency parsing, and text classification <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ext classification <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. In this work, we a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is a key challenge in the summarization task, and less progress has been made on it. Pioneer works <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> incorporated fact . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: quality heterogeneous training data motivates us to leverage the idea from curriculum learning (CL) <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 2009)</ref>, which better learns heterogeneous data by rained with the easier subsets or subtasks, and then the training difficulty is gradually increased <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 2009)</ref> to improve model performance in difficult  t=\"#b9\">(Huang and Du, 2019)</ref>. The existing CL methods can be divided into predefined CL (PCL) <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 2009)</ref> and automatic CL (ACL) <ref type=\"bibr\" ta tiveness of CL.</p><p>FT is fine-tuned directly with training data without CL; PCL adopts Baby Step <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 2009)</ref> instead of SPL, which inputs the subsets i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: omain adaption <ref type=\"bibr\" target=\"#b28\">(Wang et al., 2021)</ref> and training generalization <ref type=\"bibr\" target=\"#b9\">(Huang and Du, 2019)</ref>. The existing CL methods can be divided int. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he learning pace. The regularizer g(; ) is the binary self-paced function used to avoid overfitting <ref type=\"bibr\" target=\"#b11\">(Jiang et al., 2014)</ref>. In the training process, \"easy\" samples w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head>B.5 Result Confidence</head><p>We also conduct a statistical significance test <ref type=\"bibr\" target=\"#b7\">(Dror et al., 2018)</ref> to show our experiment results are convincin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head>B.5 Result Confidence</head><p>We also conduct a statistical significance test <ref type=\"bibr\" target=\"#b7\">(Dror et al., 2018)</ref> to show our experiment results are convincin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h uses a neural architecture to learn a distributional semantic representation to classify.</p><p>\u2022 <ref type=\"bibr\" target=\"#b18\">Lin and Ji (2019)</ref>: This approach proposes a two-step mention-aw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h uses a neural architecture to learn a distributional semantic representation to classify.</p><p>\u2022 <ref type=\"bibr\" target=\"#b18\">Lin and Ji (2019)</ref>: This approach proposes a two-step mention-aw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as entity linking <ref type=\"bibr\" target=\"#b32\">(Yang et al., 2019</ref>) and text classification <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2019)</ref>. Traditional entity typing approaches follow org/ns/1.0\"><head>Short Text Classification.</head><p>Existing short text classification approaches <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2019)</ref> directly use KG as external knowledge to imp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as entity linking <ref type=\"bibr\" target=\"#b32\">(Yang et al., 2019</ref>) and text classification <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2019)</ref>. Traditional entity typing approaches follow org/ns/1.0\"><head>Short Text Classification.</head><p>Existing short text classification approaches <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2019)</ref> directly use KG as external knowledge to imp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tive convex search (ACS) to realize SPL, of which the algorithm is shown in Appendix A. We use Adam <ref type=\"bibr\" target=\"#b13\">(Kingma and Ba, 2015)</ref> to update the model parameters.</p></div>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et al., 2009)</ref> to improve model performance in difficult target tasks, such as domain adaption <ref type=\"bibr\" target=\"#b28\">(Wang et al., 2021)</ref> and training generalization <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the accelerator's hardware (compute and memory) resources <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. Specifically, a mapping (aka schedule) includes the comput computation order, parallelization strategy and tile sizes <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, as shown in Fig. <ref type=\"figure\">1</ref>. In order to a et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, as a critical problem for NPU design and/or deployment, cl ce and time. The mapping includes the following components <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, shown in Fig. <ref type=\"figure\">1</ref>:</p><p>(1) Tile s ble spatial arrays <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, sparse accelerators <ref type=\"bibr\" target=\"#b70\">[71]</r m sampling on the search space or apply pruned random search <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, which prunes off the redundant search space to increase th  have demonstrated <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>.</p><p>While several mappers are being actively developed < et=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>.</p><p>4.4.3. Loop Order Sensitivity Analysis. We perform a  input/ output/ row) <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> or inner/ outer product <ref type=\"bibr\" target=\"#b70\">[71] the best of our knowledge, to quantitatively compare three wide categories of mappers: random-based <ref type=\"bibr\" target=\"#b43\">[44]</ref> (i.e., heuristic pruning), feedbackbased <ref type=\"bibr\"  ble mappings (i.e., dataflows + tile-sizes) that an accelerator can support is called its Map-Space <ref type=\"bibr\" target=\"#b43\">[44]</ref>.</p><p>Flexible DNN accelerators <ref type=\"bibr\" target=\" aluation of different designpoints in a matter of ms. Some widely used cost models include Timeloop <ref type=\"bibr\" target=\"#b43\">[44]</ref>, MAESTRO <ref type=\"bibr\" target=\"#b33\">[34]</ref>, dMazeR so on) and capture each accelerator's map space in different formats. In this work, we use Timeloop <ref type=\"bibr\" target=\"#b43\">[44]</ref> as our cost model <ref type=\"foot\" target=\"#foot_1\">4</ref f type=\"table\" target=\"#tab_1\">1</ref>.</p><p>Hardware Accelerator. We model the NPU using Timeloop <ref type=\"bibr\" target=\"#b43\">[44]</ref>. We assume three-levels of buffer hierarchies: DRAM, a 64K figure\">2</ref>). We select state-of-the-art mappers out of each category -Timeloop's Random-Pruned <ref type=\"bibr\" target=\"#b43\">[44]</ref> from random-based, Gamma <ref type=\"bibr\" target=\"#b27\">[2 formance improvement over number of sampled points.   \u2022 Random-Pruned (random-based): Random-Pruned <ref type=\"bibr\" target=\"#b43\">[44]</ref> uses random sampling on a pruned search space. The pruning cs, e.g., permutations do not matter for the innermost tiling level and for tile sizes that are one <ref type=\"bibr\" target=\"#b43\">[44]</ref>.</p><p>\u2022 Gamma (feedback-based): Gamma In the following ev ll-clock time to acquire one sample (10x more costly than random-based mappers, e.g., Random-Pruned <ref type=\"bibr\" target=\"#b43\">[44]</ref>). Neural architecture search is leading to new DNN models  thms) with different algorithmic techniques are proposed to tackle the MSE problem. Timeloop-mapper <ref type=\"bibr\" target=\"#b43\">[44]</ref>, Simba <ref type=\"bibr\" target=\"#b53\">[54]</ref>, dmazeRun timized mapping<ref type=\"foot\" target=\"#foot_0\">3</ref> , <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b74\">75]</ref>, as we discuss next een proposed, which we categorize into random search based <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b60\">[61]</ref>, Mobilenet <ref type=\"bibr\" target=\"#b52\">[53]</ref>, and Bert-large <ref type=\"bibr\" target=\"#b64\">[65]</ref>. Some frequently referenced workloads across different exp formance of inner and outer product style mapping on sparse-dense GEMM workloads in Bert-large model<ref type=\"bibr\" target=\"#b64\">[65]</ref>. The workload density indicates the density of the sparse . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ck-box optimization) <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar \">[50]</ref> uses Bayesian optimization, RELEASE <ref type=\"bibr\" target=\"#b1\">[2]</ref>, ConfuciuX <ref type=\"bibr\" target=\"#b26\">[27]</ref>, and FlexTensor <ref type=\"bibr\" target=\"#b78\">[79]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b60\">61]</ref>. Furthermore, the advent of compressed-sparse DNNs <ref typ t <ref type=\"bibr\" target=\"#b17\">[18]</ref>, VGG <ref type=\"bibr\" target=\"#b55\">[56]</ref>, Mnasnet <ref type=\"bibr\" target=\"#b60\">[61]</ref>, Mobilenet <ref type=\"bibr\" target=\"#b52\">[53]</ref>, and  e evaluate workloads from two DNN models, VGG <ref type=\"bibr\" target=\"#b55\">[56]</ref> and Mnasnet <ref type=\"bibr\" target=\"#b60\">[61]</ref>. Many DNN models are made by human experts, where the shap umber of generation-to-converge, an equivalent index of time-to-converge.). We observe that Mnasnet <ref type=\"bibr\" target=\"#b60\">[61]</ref> enjoys the least speedup. It is because Mnasnet is a resul  We evaluate workloads from two DNN models, VGG<ref type=\"bibr\" target=\"#b55\">[56]</ref> and Mnasnet<ref type=\"bibr\" target=\"#b60\">[61]</ref>. Many DNN models are made by human experts, where the shap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or low sparsity workloads and outer product accelerators perform better at high amounts of sparsity <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. We study this gen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sparse mappings ( \u00a74.5), we use TimeloopV2, aka Sparseloop <ref type=\"bibr\" target=\"#b70\">[71,</ref><ref type=\"bibr\" target=\"#b71\">72]</ref>, as the cost model to explore the map space in a flexible s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #foot_0\">3</ref> , <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b74\">75]</ref>, as we discuss next.</p></div> <div xmlns=\"http://www.tei-c get=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b53\">54,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b74\">75]</ref>, feedback-based (including reinforcement learning and black bibr\" target=\"#b33\">[34]</ref>, dMazeRunner <ref type=\"bibr\" target=\"#b11\">[12]</ref>, Interstellar <ref type=\"bibr\" target=\"#b74\">[75]</ref>, SCALE-sim <ref type=\"bibr\" target=\"#b51\">[52]</ref> and o bibr\" target=\"#b53\">[54]</ref>, dmazeRunner <ref type=\"bibr\" target=\"#b11\">[12]</ref>, Interstellar <ref type=\"bibr\" target=\"#b74\">[75]</ref>, and others <ref type=\"bibr\">[13, 14, 41, 55, 57-60, 63, 6. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b63\">64]</ref> (Fig. <ref type=\"fi get=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" tar 5]</ref>, and others <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b63\">64]</ref>. While there have b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b73\">74,</ref><ref type=\"bibr\" target=\"#b76\">77,</ref><ref type=\"bibr\" target=\"#b77\">78]</ref> for efficiently running sparse workloads, skipping zeros in get=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b73\">74,</ref><ref type=\"bibr\" target=\"#b76\">77,</ref><ref type=\"bibr\" target=\"#b77\">78]</ref>. Next, when executing MSE, we score the mapping by the perf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  n=\"1\">Introduction</head><p>Hypergraphs have raised a surge of interests in the research community <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target f>. Concomitant with the trend, hypergraph neural networks (HyperGNNs) have recently been developed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target -of-the-art approaches to encode such complex structures are hypergraph neural networks (HyperGNNs) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target  would lead to distribution collapse (i.e., two hypergraph generators output the same distribution) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> which results in less   community, hypergraph neural networks are developed for effective hypergraph representations. HGNN <ref type=\"bibr\" target=\"#b0\">[1]</ref> adopt the clique expansion technique and designs the weighte to a conventional graph using the clique expansion technique, and we choose the representative HGNN <ref type=\"bibr\" target=\"#b0\">[1]</ref> as the backbone network for learning on the converted graph. redit <ref type=\"bibr\" target=\"#b49\">[50]</ref>. The hypergraph construction follows the setting in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The top 5 similar objects in each data set are built as a h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, we set out to leverage contrastive self-supervision to add. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, we set out to leve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, we set out to leve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, via contrasting between graphs and hypergraphs which might. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e self-supervision <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> has achieved unprecedented success in computer vision. The . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  hypergraphs remains largely unexplored. Most existing works <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar HyperGNN architecture. For baselines, we compare two existing hypergraph self-supervised approaches <ref type=\"bibr\" target=\"#b35\">[36]</ref> and <ref type=\"bibr\" target=\"#b25\">[26]</ref> in recommend. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" tar et=\"#b15\">16]</ref>, especially the contrastive approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar truction, otherwise it would result in \"negative transfer\" <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. However, it is non-trivial to build hypergraph views due t graphs and graphs close.</p><p>Contributions. Motivated by <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> that appropriate data augmentations suffice for the effecti Main components of our Hy-perGCL, similar to images/graphs <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> include: (i) hypergraph augmentations for contrastive views e adopt three schemes of vertex dropping, attribute masking and subgraph from graph-structured data <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Our finding is that, different from the fact that vertex  eralized version of A1.</p><p>Moreover, we find that vertex augmentations for graph-structured data <ref type=\"bibr\" target=\"#b13\">[14]</ref> are applicable to hypergraphs. Therefore, we adopt three a asking (A4) and subgraph (A5) into our experiments, with similar prior knowledge incorporated as in <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  structures in broad applications, e.g., recommender systems <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, financial analyses <ref type=\"bibr\" target=\"#b6\">[7,</ref><r . Nevertheless, contrastive learning on hypergraphs remains largely unexplored. Most existing works <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  by the emerging self-supervised learning on images/graphs <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" tar get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, especially the contrastive approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" ta rning hinges on the appropriate view construction, otherwise it would result in \"negative transfer\" <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. However, it is no tion via pulling representations of hypergraphs and graphs close.</p><p>Contributions. Motivated by <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> that appropriate d s in a data-driven manner.</p><p>Contrastive self-supervised learning. Contrastive self-supervision <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta GNNs in the low-label regime (HyperGCL). Main components of our Hy-perGCL, similar to images/graphs <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> include: (i) hyper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, financial analyses <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, and bioinformatics <r ich are ubiquitous in real-world applications of hypergraphs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and empirically restric. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mative local nodes for attention. As for global attention, we first use graph coarsening algorithms <ref type=\"bibr\" target=\"#b25\">[26]</ref> to pre-process the input graph and generate a coarse graph e attention matrices and calculate the significance scores: end if 11: end for coarsening algorithm <ref type=\"bibr\" target=\"#b25\">[26]</ref> to generate the coarsened graph G \u2032 . The sampled n s supe up stage followed by a linear decay learning rate scheduler. We adopt the Variational Neighborhoods <ref type=\"bibr\" target=\"#b25\">[26]</ref> with a coarsening rate of 0.01 as the default coarsening m ning rates. Specifically, the considered coarsening algorithms include Variation Neighborhoods (VN) <ref type=\"bibr\" target=\"#b25\">[26]</ref>, Variation Edges (VE) <ref type=\"bibr\" target=\"#b25\">[26]< nclude Variation Neighborhoods (VN) <ref type=\"bibr\" target=\"#b25\">[26]</ref>, Variation Edges (VE) <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and Algebraic (JC) <ref type=\"bibr\" target=\"#b28\">[29]</r .3\">Graph Coarsening</head><p>The goal of Graph Coarsening <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> is to reduce the nu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">[34]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>, JKNet <ref type=\"bibr\" target=\"#b35\">[36]</ref>, APPNP <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Geom-GCN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  coarse graph is a smaller weighted graph  We conduct experiments on the Newman artificial networks <ref type=\"bibr\" target=\"#b9\">[10]</ref> since it enable us to obtain networks with different proper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>In recent years, the Transformer architecture <ref type=\"bibr\" target=\"#b32\">[33]</ref> and its variants (e.g., Bert <ref type=\"bibr\" target=\"#b6\" s=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Transformers for Graph</head><p>Recently, Transformer <ref type=\"bibr\" target=\"#b32\">[33]</ref> has shown its superiority in an increasing number of domai ormer Architecture</head><p>The Transformer architecture consists of a series of Transformer layers <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Each Transformer layer has two parts: a multi-head self-a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> follow a message-pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: A. Here, we consider the property of homophily/heterophily as one example. Following previous works <ref type=\"bibr\" target=\"#b46\">[47]</ref>, the degree of homophily \u03b1 can be defined as the fraction . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: A. Here, we consider the property of homophily/heterophily as one example. Following previous works <ref type=\"bibr\" target=\"#b46\">[47]</ref>, the degree of homophily \u03b1 can be defined as the fraction . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">[34]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>, JKNet <ref type=\"bibr\" target=\"#b35\">[36]</ref>, APPNP <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Geom-GCN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th the number of nodes inhibits the practical application. Although some Sparse Transformer methods <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\">2,</ref><ref type=\"bibr\" target=\"#b18\">19] by using approximate attention computation based on locality-sensitive hashing. Routing Transformer <ref type=\"bibr\" target=\"#b29\">[30]</ref> employs online k-means clustering on the tokens. Linformer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  coarse graph is a smaller weighted graph  We conduct experiments on the Newman artificial networks <ref type=\"bibr\" target=\"#b9\">[10]</ref> since it enable us to obtain networks with different proper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sing number of domains <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, e.g. Bert <ref type=\"bibr\" target=\"#b6\">[7]</ref> in NLP a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b27\">[28]</ref>, H 2 GCN <ref type=\"bibr\" target=\"#b47\">[48]</ref>, and GPRGNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> along with four state-of-the-art Graph Transformers, i.e. GT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b27\">[28]</ref>, H 2 GCN <ref type=\"bibr\" target=\"#b47\">[48]</ref>, and GPRGNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> along with four state-of-the-art Graph Transformers, i.e. GT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b27\">[28]</ref>, H 2 GCN <ref type=\"bibr\" target=\"#b47\">[48]</ref>, and GPRGNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> along with four state-of-the-art Graph Transformers, i.e. GT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b27\">[28]</ref>, H 2 GCN <ref type=\"bibr\" target=\"#b47\">[48]</ref>, and GPRGNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> along with four state-of-the-art Graph Transformers, i.e. GT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">[34]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>, JKNet <ref type=\"bibr\" target=\"#b35\">[36]</ref>, APPNP <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Geom-GCN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> follow a message-pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: der the 1-hop neighboring nodes; Gophormer <ref type=\"bibr\" target=\"#b45\">[46]</ref> uses GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref> sampling to uniformly sample ego-graphs with pre-defined m  sampling distribution, then the total number of nodes in the l-th layer becomes O(k l ). GraphSage <ref type=\"bibr\" target=\"#b10\">[11]</ref> is one of the most well-known node-wise sampling methods w <ref type=\"bibr\" target=\"#b17\">[18]</ref>, GAT <ref type=\"bibr\" target=\"#b33\">[34]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b10\">[11]</ref>, JKNet <ref type=\"bibr\" target=\"#b35\">[36]</ref>, APPNP <r ks and Node Sampling</head><p>Graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" tar  another perspective, these sampling methods can also be categorized into fixed sampling strategies <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\">4,</ref><ref type=\"bibr\" target=\"#b48\">49]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> with the expected emergence of lower latency networks offer get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. In this work, we ask whether the performance optimizations </p><p>2. Huge pages are detrimental. While prior approaches <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> have advocated for using huge pages in tiered memory system get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>Even though these approaches have been investigated  red memory proposals <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, HotBox uses the access bits of pages in local and remote m get=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. Here, we review the most relevant research work in the con get=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> target memory latencies lower than those offered by current w accesses to disaggregated memory.</p><p>3. Batch-migration of pages is inefficient. Prior studies <ref type=\"bibr\" target=\"#b47\">[48]</ref> argued for migrating batches of pages to amortize system o bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, as well as the recent Nimble <ref type=\"bibr\" target=\"#b47\">[48]</ref> system for tiered memory, which relies on huge pages and b -the-art work that utilizes the hybrid model transparently and without dedicated hardware is Nimble <ref type=\"bibr\" target=\"#b47\">[48]</ref> 2 , hence it serves as a baseline in our analysis of curre icy yields the best-case performance estimate, as it works better than the known best online policy <ref type=\"bibr\" target=\"#b47\">[48]</ref> for this workload.</p><p>As we reduce the fraction of the  static (top hot pages pinned in local memory) and online dynamic (which migrates pages using Nimble <ref type=\"bibr\" target=\"#b47\">[48]</ref>). To compute \ud835\udc3b\ud835\udc39 , we periodically (every 1 sec) sample and re <ref type=\"figure\">5b</ref>. Second, the dynamic migration policy in the state-of-the-art system <ref type=\"bibr\" target=\"#b47\">[48]</ref> causes significant \ud835\udc3b\ud835\udc39 . Finally, the memory bloat caused b le pages to perform their migration together, is essential for achieving high migration performance <ref type=\"bibr\" target=\"#b47\">[48]</ref>. This is in contrast to on-demand page migration such as u and benefits of the migration granularity trade-off: Do batch migrations amortize costs? Prior work <ref type=\"bibr\" target=\"#b47\">[48]</ref> observed that migrating a single page via the built-in Lin onstantly updated, such that the desired page may soon leave the LRU cache and its migration would  <ref type=\"bibr\" target=\"#b47\">[48]</ref>). Local memory size is 20% of the working set. Batching ca  is 1.4\u00d7 faster than Linux swap when mounted on BRD <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Nimble <ref type=\"bibr\" target=\"#b47\">[48]</ref>: Nimble migration management is evaluated using the same n target=\"#b34\">[35]</ref> with two kernels: graph construction and BFS. It was used in a prior study <ref type=\"bibr\" target=\"#b47\">[48]</ref>. The access pattern is random. Ligra-BFS: BFS on a popular work. Hybrid access models. The approach most relevant to ours is the Nimble page management system <ref type=\"bibr\" target=\"#b47\">[48]</ref>, which uses huge pages and batch migrations to accelerate  ration rate in the on-demand migration (aka swap) and batched migrations (aka Nimble with base pages<ref type=\"bibr\" target=\"#b47\">[48]</ref>). Local memory size is 20% of the working set. Batching ca rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b47\">48</ref>] combine direct cache-line accesses to tiered memory and pag. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: trum of 200-600 nsec <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar  the tiers at runtime. Recent works on tiered memory systems <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar >with low overheads Similar to prior tiered memory proposals <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>, HotBox uses the ac get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar naging tiered memory <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar s considered new hardware support for disaggregated memory <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmal analysis. Intuition: \ud835\udc3b\ud835\udc39 in memcached. We compare the performance of the memcached-ETC workload <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> using base and hug f pages with data structures used by the Java Virtual Machine. memcached-ETC: An in-memory KV store <ref type=\"bibr\" target=\"#b20\">[21]</ref> measured using the Mutilate client <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar s considered new hardware support for disaggregated memory <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Swapping can be employed in commodity systems today and has been thoroughly studied in prior work <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar ws:</p><p>1. A swap-only approach under higher memory latencies is inefficient. Several prior works <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar ompare HotBox with state-of-the-art mechanisms, including swapping-based systems such as InfiniSwap <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar f>. This is in contrast to on-demand page migration such as used in swapbased tiered memory systems <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar lustrate the performance of systems that do not perform direct access to remote memory (not hybrid) <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" tar e models:</p><p>Swap: Remote memory is used as a swap device <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ pectrum of 750-1500 nsec consider only the swap access model <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ re and software stacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" targ  of specialized hardware support. These and other approaches <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> differ from HotBox in that they do not support direct cache-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed using the Mutilate client <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Facebook's ETC benchmark <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The throughput is measured after loading 30 M records and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the entries in the underlying page table (PTE), as no page inside the page table has been accessed <ref type=\"bibr\" target=\"#b24\">[25]</ref>.</p><p>Remote memory pages' hotness is tracked with a per- esses of each task in the system (using the UOPS_LLC_-MISS_RETIRED.{REMOTE,LOCAL}_DRAM PMU counters <ref type=\"bibr\" target=\"#b24\">[25]</ref> via the in-kernel perf_event API) and consider the system . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: systems today and has been thoroughly studied in prior work <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar cies as low as 750 nsec for optimistic mid-term estimates [2, <ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> (accounting for net higher memory latencies is inefficient. Several prior works <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar anisms, including swapping-based systems such as InfiniSwap <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, as well as the rec used as a swap device <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar the swap access model <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar e migration such as used in swapbased tiered memory systems <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, which migrate a pa  do not perform direct access to remote memory (not hybrid) <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. This is an optimiz nly access models. Gao et al. analyzed the relationship between network and application performance <ref type=\"bibr\" target=\"#b21\">[22]</ref>, using a swap device implementation that injects additiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tly what the properties are inherited by models that were trained using SGD.</p><p>1.2 RELATED WORK <ref type=\"bibr\" target=\"#b11\">Hardt et al. (2016)</ref> give an upper bound on the generalization e dels trained on random labels (high generalization error). This also highlights why the analysis of <ref type=\"bibr\" target=\"#b11\">Hardt et al. (2016)</ref> for non-convex optimization was rather pess. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Delalleau &amp; Bengio, 2011;</ref><ref type=\"bibr\" target=\"#b10\">Eldan &amp; Shamir, 2016;</ref><ref type=\"bibr\" target=\"#b28\">Telgarsky, 2016;</ref><ref type=\"bibr\" target=\"#b6\">Cohen &amp; Shash b18\">Mhaskar &amp; Poggio, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Eldan &amp; Shamir, 2016;</ref><ref type=\"bibr\" target=\"#b28\">Telgarsky, 2016;</ref><ref type=\"bibr\" target=\"#b6\">Cohen &amp; Shash >Mhaskar &amp; Poggio (2016)</ref>; <ref type=\"bibr\" target=\"#b10\">Eldan &amp; Shamir (2016)</ref>; <ref type=\"bibr\" target=\"#b28\">Telgarsky (2016)</ref>; <ref type=\"bibr\" target=\"#b6\">Cohen &amp; Sha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eights to an Euclidean ball, with the radius decided by the amount of weight decay.</p><p>\u2022 Dropout <ref type=\"bibr\" target=\"#b26\">(Srivastava et al., 2014)</ref>: mask out each element of a layer out. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ZATIONS</head><p>Early stopping was shown to implicitly regularize on some convex learning problems <ref type=\"bibr\" target=\"#b30\">(Yao et al., 2007;</ref><ref type=\"bibr\" target=\"#b16\">Lin et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: unction at the solution. But in the linear case, the curvature of all optimal solutions is the same <ref type=\"bibr\" target=\"#b4\">(Choromanska et al., 2015)</ref>. To see this, note that in the case w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  absence of all regularization does not necessarily imply poor generalization error. As reported by <ref type=\"bibr\" target=\"#b15\">Krizhevsky et al. (2012)</ref>, 2 -regularization (weight decay) some \">(Szegedy et al., 2016)</ref> architecture on ImageNet and a smaller version of Inception, Alexnet <ref type=\"bibr\" target=\"#b15\">(Krizhevsky et al., 2012)</ref>, and MLPs on CIFAR10. Please see Sect 0.38% top-5 accuracy without regularization, while the reported number of the winner of ILSVRC 2012 <ref type=\"bibr\" target=\"#b15\">(Krizhevsky et al., 2012)</ref> achieved 83.6%. So while regularizati test a simplified Inception <ref type=\"bibr\" target=\"#b27\">(Szegedy et al., 2016)</ref> and Alexnet <ref type=\"bibr\" target=\"#b15\">(Krizhevsky et al., 2012)</ref> by adapting the architectures to smal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: FAR10 dataset <ref type=\"bibr\" target=\"#b14\">(Krizhevsky &amp; Hinton, 2009)</ref> and the ImageNet <ref type=\"bibr\" target=\"#b24\">(Russakovsky et al., 2015)</ref> ILSVRC 2012 dataset. We test the Inc FAR10 dataset <ref type=\"bibr\" target=\"#b14\">(Krizhevsky &amp; Hinton, 2009)</ref> and the ImageNet <ref type=\"bibr\" target=\"#b24\">(Russakovsky et al., 2015)</ref> ILSVRC 2012 dataset.</p><p>The CIFAR. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (amino acids). Specifically, we augment the autoregressive self-attention of recent sequence models <ref type=\"bibr\" target=\"#b6\">[7]</ref> with graph-based representations of 3D molecular structure.  ncoder develops a sequence-independent representation of 3D structure via multi-head self-attention <ref type=\"bibr\" target=\"#b6\">[7]</ref> on the spatial k-nearest neighbors graph. A decoder then aut  Structured Transformer model that draws inspiration from the selfattention based Transformer model <ref type=\"bibr\" target=\"#b6\">[7]</ref> and is augmented for scalable incorporation of relational in an attend to a separate subspace of the embeddings via learned query, key and value transformations <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>The queries are derived from the current embedding at een these self-attention layers and position-wise feedforward layers as in the original Transformer <ref type=\"bibr\" target=\"#b6\">[7]</ref>. We stack multiple layers atop each other, and thereby obtai rained models using the learning rate schedule and initialization of the original Transformer paper <ref type=\"bibr\" target=\"#b6\">[7]</ref>, a dropout rate of 10% <ref type=\"bibr\" target=\"#b41\">[42]</ \"#foot_6\">6</ref> . Our model augments the traditional sequence-level selfattention of Transformers <ref type=\"bibr\" target=\"#b6\">[7]</ref> with graph-based, 3D structural encodings and is able to lev encoder develops a sequence-independent representation of 3D structure via multi-head self-attention<ref type=\"bibr\" target=\"#b6\">[7]</ref> on the spatial k-nearest neighbors graph. A decoder then aut. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on of the original Transformer paper <ref type=\"bibr\" target=\"#b6\">[7]</ref>, a dropout rate of 10% <ref type=\"bibr\" target=\"#b41\">[42]</ref>, a label smoothing rate of 10%, and early stopping based o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -range in 3D space <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref>. By making the graph and self-attention similarly sparse a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n design. More recently, there have been successes with non-parametric approaches to protein design <ref type=\"bibr\" target=\"#b31\">[32]</ref> which are based on finding substructural homologies betwee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rotations as four-element vectors that can be efficiently and reasonably compared by inner products <ref type=\"bibr\" target=\"#b38\">[39]</ref>. <ref type=\"foot\" target=\"#foot_3\">3</ref>Relative positio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: an 8 Angstroms and hydrogen bonds which are directed and defined by the electrostatic model of DSSP <ref type=\"bibr\" target=\"#b40\">[41]</ref>. For coarse node features, we compute virtual dihedral ang. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  often unreliable, requiring multiple rounds of trial-and-error in which initial designs often fail <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Moreover, diagnosing  xible backbone' design where softer constraints such as blueprints of hydrogen-bonding connectivity <ref type=\"bibr\" target=\"#b4\">[5]</ref> or 1D architectures <ref type=\"bibr\" target=\"#b14\">[15]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[1]</ref>, including the design of novel 3D folds <ref type=\"bibr\" target=\"#b1\">[2]</ref>, enzymes <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and complexes <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Desp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref>). In the second, we also compared to a prior benchmark from members of the Rosetta community <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref> across 40 diverse . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in folds, we collected a dataset based on the CATH hierarchical classification of protein structure <ref type=\"bibr\" target=\"#b39\">[40]</ref>. For all domains in the CATH 4.2 40% non-redundant set of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s for sequences given 3D structure, where the amino acids are modeled independently of one another. <ref type=\"bibr\" target=\"#b14\">[15]</ref> introduced a generative model for protein sequences condit prints of hydrogen-bonding connectivity <ref type=\"bibr\" target=\"#b4\">[5]</ref> or 1D architectures <ref type=\"bibr\" target=\"#b14\">[15]</ref> could define the structure of interest.</p><p>3D considera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  language understanding and generation tasks <ref type=\"bibr\" target=\"#b11\">(Dai et al., 2019;</ref><ref type=\"bibr\" target=\"#b44\">Shaw et al., 2018)</ref>. The proposed disentangled attention mechani se a separate embedding matrix to compute the relative position bias in computing attention weights <ref type=\"bibr\" target=\"#b44\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Huang et al.,  \" \u00c3p\u00d1c r\u03b4rj, is, js For an input sequence of length N , it requires a space complexity of OpN 2 dq <ref type=\"bibr\" target=\"#b44\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Huang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b30\">(Liu et al., 2019b;</ref><ref type=\"bibr\" target=\"#b35\">Minaee et al., 2020;</ref><ref type=\"bibr\" target=\"#b21\">Jiang et al., 2020;</ref><ref type=\"bibr\" target=\"#b18\">He et al., 20  a variant to the algorithm described in <ref type=\"bibr\" target=\"#b36\">Miyato et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b21\">Jiang et al. (2020)</ref>, for fine-tuning.</p><p>Virtual adversarial. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b1\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b16\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b3\">Bentivogli et al., 2009)</ref>, Word Sense Disambiguation <ref type=\"b get=\"#b1\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b16\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b3\">Bentivogli et al., 2009;</ref><ref type=\"bibr\" target=\"#b27\">Levesque  get=\"#b1\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b16\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b3\">Bentivogli et al., 2009;</ref><ref type=\"bibr\" target=\"#b12\">De Marnef. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of NLU tasks, including Question Answering <ref type=\"bibr\" target=\"#b8\">(Clark et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Khashabi et al., 2018;</ref><ref type=\"bibr\" target=\"#b58\">Zhang et a ibr\" target=\"#b58\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Clark et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Khashabi et al., 2018)</ref>, natural language inference <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stic acceptability <ref type=\"bibr\" target=\"#b54\">(Warstadt et al., 2018)</ref>, sentiment analysis <ref type=\"bibr\" target=\"#b47\">(Socher et al., 2013</ref>), text similarity <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b1\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b16\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b3\">Bentivogli et al., 2009)</ref>, Word Sense Disambiguation <ref type=\"b get=\"#b1\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b16\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b3\">Bentivogli et al., 2009;</ref><ref type=\"bibr\" target=\"#b27\">Levesque  get=\"#b1\">Bar-Haim et al., 2006;</ref><ref type=\"bibr\" target=\"#b16\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b3\">Bentivogli et al., 2009;</ref><ref type=\"bibr\" target=\"#b12\">De Marnef. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b16\">Giampiccolo et al., 2007;</ref><ref type=\"bibr\" target=\"#b3\">Bentivogli et al., 2009;</ref><ref type=\"bibr\" target=\"#b12\">De Marneffe et al., 2019)</ref>, coreference resolution <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ost popular NLU benchmarks. SuperGLUE consists of a wide of NLU tasks, including Question Answering <ref type=\"bibr\" target=\"#b8\">(Clark et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Khashabi et a ious of tasks including question answering <ref type=\"bibr\" target=\"#b58\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Clark et al., 2019;</ref><ref type=\"bibr\" target=\"#b23\">Khashabi et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re is extended for long sequence handling <ref type=\"bibr\" target=\"#b2\">(Beltagy et al., 2020;</ref><ref type=\"bibr\" target=\"#b24\">Kitaev et al., 2019;</ref><ref type=\"bibr\" target=\"#b7\">Child et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the rise of a set of large-scale Transformer-based Pre-trained Language Models (PLMs), such as GPT <ref type=\"bibr\" target=\"#b38\">(Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Brown et al mented using absolute position embedding <ref type=\"bibr\" target=\"#b50\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b38\">Radford et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Devlin et a <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref>, except that we use the BPE vocabulary of<ref type=\"bibr\" target=\"#b38\">Radford et al. (2019)</ref>;<ref type=\"bibr\" target=\"#b32\">Liu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5\">(Williams et al., 2018)</ref>; and (3) NER: CoNLL-2003. For comparison, we include ALBERT xxlarge<ref type=\"bibr\" target=\"#b26\">(Lan et al., 2019)</ref> 4 and Megatron<ref type=\"bibr\" target=\"#b45\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctures (e.g., NASNet, AmoebaNet) are not efficient for inference. Recent hardware-aware NAS methods <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Tan et al., 201 ers and skip the last N \u2212 D layers, rather than keeping any D layers as done in current NAS methods <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019;</ref><ref type=\"bibr\" target=\"#b35\">Wu et al., 2019 nd input image size<ref type=\"foot\" target=\"#foot_1\">2</ref> . We also build a latency lookup table <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref> on each target hardware platform to predict t orms (Figure <ref type=\"figure\" target=\"#fig_9\">11</ref>) using the ProxylessNAS architecture space <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref>. OFA consistently improves the trade-off betw e). It is impossible for previous NAS methods<ref type=\"bibr\" target=\"#b32\">(Tan et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Cai et al., 2019)</ref> due to the prohibitive training cost.</figDesc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esign techniques <ref type=\"bibr\" target=\"#b20\">(Jiang et al., 2019b;</ref><ref type=\"bibr\">a;</ref><ref type=\"bibr\" target=\"#b10\">Hao et al., 2019)</ref>   <ref type=\"bibr\">Huang, 2019b)</ref> propos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: deployment scenarios increases, which will result in excessive energy consumption and CO 2 emission <ref type=\"bibr\" target=\"#b31\">(Strubell et al., 2019)</ref>. However, training the once-for-all net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b23\">(Zoph et al., 2018;</ref><ref type=\"bibr\" target=\"#b29\">Real et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">Cai et al., 2018b)</ref> search for highaccuracy architectures without. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b11\">(He et al., 2016;</ref><ref type=\"bibr\" target=\"#b30\">Sandler et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Huang et al., 2017)</ref>, we divide a CNN model into a sequence of u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b29\">Real et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Cai et al., 2018a;</ref><ref type=\"bibr\" target=\"#b24\">Liu et al., 2019)</ref>. Early NAS methods <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b15\">(Howard et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Sandler et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Zhang et al., 2018)</ref> or accelerate the existing models by compre b30\">Sandler et al., 2018)</ref>, ShuffleNets <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Zhang et al., 2018)</ref>, etc. Orthogonal to architecting efficient  >(Howard et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Sandler et al., 2018)</ref>, ShuffleNets <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Zhang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b15\">(Howard et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Sandler et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Zhang et al., 2018)</ref> or accelerate the existing models by compre b30\">Sandler et al., 2018)</ref>, ShuffleNets <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Zhang et al., 2018)</ref>, etc. Orthogonal to architecting efficient  >(Howard et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Sandler et al., 2018)</ref>, ShuffleNets <ref type=\"bibr\" target=\"#b28\">(Ma et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Zhang et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2018)</ref>. However, they focus on integrating thi ep, decoupled from the other steps. This type of prediction smoothing is similar in spirit to APPNP <ref type=\"bibr\" target=\"#b23\">(Klicpera et al., 2018)</ref>, which we compare against later. Howeve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dden channels with learning rate equal to 0.005.</p><p>\u2022 US County <ref type=\"bibr\">(Jia &amp;</ref><ref type=\"bibr\" target=\"#b19\">Benson, 2020) and</ref><ref type=\"bibr\">Rice31 (Traud et al., 2012)</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b1\">(Bojchevski et al., 2020;</ref><ref type=\"bibr\" target=\"#b48\">Zeng et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Frasca et al., 2020)</ref>. The primary focus of our approach, however. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\">(Eliav &amp; Cohen, 2018;</ref><ref type=\"bibr\" target=\"#b18\">Ibrahim &amp; Gleich, 2019;</ref><ref type=\"bibr\" target=\"#b40\">Tudisco et al., 2021)</ref>, but these methods focus on settings of l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n make it faster but less accurate. Our framework also complements the Simplified Graph Convolution <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2019)</ref> and other algorithms designed to increase sca t). Again, these results suggest that smoothed outputs are important, aligning with recent research <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bojchevski et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: where classes are dorm residences and features are attributes such as gender, major, and class year <ref type=\"bibr\" target=\"#b39\">(Traud et al., 2012)</ref>, as well as a geographic dataset of US cou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: three classic citation network benchmarks <ref type=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b30\">Namata et al., 2012 o 0.01.</p><p>\u2022 Cora, Citseer, and Pubmed <ref type=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b30\">Namata et al., 2012. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b1\">(Bojchevski et al., 2020;</ref><ref type=\"bibr\" target=\"#b48\">Zeng et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Frasca et al., 2020)</ref>. The primary focus of our approach, however. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b1\">(Bojchevski et al., 2020;</ref><ref type=\"bibr\" target=\"#b48\">Zeng et al., 2019;</ref><ref type=\"bibr\" target=\"#b8\">Frasca et al., 2020)</ref>. The primary focus of our approach, however. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b30\">Namata et al., 2012)</ref>; and wikiCS is a web graph <ref type=\"bibr e=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b30\">Namata et al., 2012)</ref> and Email <ref type=\"bibr\" target=\"#b25\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: u et al., 2020)</ref>; the Cora, Citeseer, and Pubmed are three classic citation network benchmarks <ref type=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 200  layers and 256 hidden channels with learning rate equal to 0.01.</p><p>\u2022 Cora, Citseer, and Pubmed <ref type=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 200. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b44\">(Wu et al., 2019)</ref> and other algorithms designed to increase scalability <ref type=\"bibr\" target=\"#b1\">(Bojchevski et al., 2020;</ref><ref type=\"bibr\" target=\"#b48\">Zeng et   are important, aligning with recent research <ref type=\"bibr\" target=\"#b44\">(Wu et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bojchevski et al., 2020)</ref>, and that the original motivations for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: three classic citation network benchmarks <ref type=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b30\">Namata et al., 2012 o 0.01.</p><p>\u2022 Cora, Citseer, and Pubmed <ref type=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b30\">Namata et al., 2012. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b26\">(Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b36\">Rong et al., 2019;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2020)</ref>. Many ideas for new GNN architectures are ada  OGB leaderboard, as of October 1, 2020). For Cora, Citeseer and Pubmed, we use the top scores from <ref type=\"bibr\" target=\"#b4\">Chen et al. (2020)</ref>. For Email and US County, we use GCNII <ref t  from <ref type=\"bibr\" target=\"#b4\">Chen et al. (2020)</ref>. For Email and US County, we use GCNII <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2020)</ref>. For Rice31, we use GCN with spectral embedd. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt membership and there are no features <ref type=\"bibr\" target=\"#b25\">(Leskovec et al., 2007;</ref><ref type=\"bibr\" target=\"#b47\">Yin et al., 2017)</ref>.</p><p>Data splits. The training/validation/t 0\">Namata et al., 2012)</ref> and Email <ref type=\"bibr\" target=\"#b25\">(Leskovec et al., 2007;</ref><ref type=\"bibr\" target=\"#b47\">Yin et al., 2017)</ref>: 3 layers and 64 hidden channels with learnin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017a)</ref>; examples include Graph Attention Networks <ref type=\"bibr\" target=\"#b41\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, Graph Isomorphism Networks <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  nodes is at the center of much network analysis and corresponds to homophily or assortative mixing <ref type=\"bibr\" target=\"#b28\">(McPherson et al., 2001;</ref><ref type=\"bibr\" target=\"#b31\">Newman, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h substantially outperformed the other GNN variants.</p><p>All models were implemented with PyTorch <ref type=\"bibr\" target=\"#b32\">(Paszke et al., 2019)</ref> and PyTorch Geometric <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: three classic citation network benchmarks <ref type=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b30\">Namata et al., 2012 o 0.01.</p><p>\u2022 Cora, Citseer, and Pubmed <ref type=\"bibr\" target=\"#b11\">(Getoor et al., 2001;</ref><ref type=\"bibr\" target=\"#b10\">Getoor, 2005;</ref><ref type=\"bibr\" target=\"#b30\">Namata et al., 2012. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b12\">Gleich &amp; Mahoney, 2015;</ref><ref type=\"bibr\" target=\"#b33\">Peel, 2017;</ref><ref type=\"bibr\" target=\"#b5\">Chin et al., 2019)</ref>; however, they have largely been ignored in t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the ROCStories <ref type=\"bibr\" target=\"#b33\">(Mostafazadeh et al., 2016)</ref> and WritingPrompts <ref type=\"bibr\" target=\"#b14\">(Fan et al., 2018)</ref> datasets. ROCStories contains 100k artificia a text. Third, some works focused on the coherence of story generation conditioned on short prompts <ref type=\"bibr\" target=\"#b14\">(Fan et al., 2018)</ref>, titles <ref type=\"bibr\" target=\"#b54\">(Yao  nerated outputs using the greedy decoding algorithm for the text infilling task, and top-k sampling <ref type=\"bibr\" target=\"#b14\">(Fan et al., 2018)</ref> with k = 40 and a softmax temperature of 0.7 ned models including ConvS2S <ref type=\"bibr\" target=\"#b17\">(Gehring et al., 2017)</ref> and Fusion <ref type=\"bibr\" target=\"#b14\">(Fan et al., 2018)</ref> on the generation tasks in LOT. We used GPT2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: asets Previous studies in the field of long text modeling have frequently focused on the ROCStories <ref type=\"bibr\" target=\"#b33\">(Mostafazadeh et al., 2016)</ref> and WritingPrompts <ref type=\"bibr\" roposed various tasks to evaluate story understanding and generation. First, story ending selection <ref type=\"bibr\" target=\"#b33\">(Mostafazadeh et al., 2016)</ref>, story ending generation <ref type= s told in the form of a coherent event sequence involving several specific and related characters'' <ref type=\"bibr\" target=\"#b33\">(Mostafazadeh et al., 2016)</ref>. We provided detailed cases for ann. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\">(Wang et al., 2019)</ref> included more diverse tasks such as natural language inference <ref type=\"bibr\" target=\"#b42\">(Rockt\u00e4schel et al., 2016)</ref>. <ref type=\"bibr\" target=\"#b44\">Sarl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emotional trajectories <ref type=\"bibr\" target=\"#b4\">(Brahman and Chaturvedi, 2020)</ref>, outlines <ref type=\"bibr\" target=\"#b40\">(Rashkin et al., 2020)</ref>, and styles <ref type=\"bibr\" target=\"#b2 nd Huang, 2020)</ref>. To alleviate the issue, we introduce the Outline-conditioned Generation task <ref type=\"bibr\" target=\"#b40\">(Rashkin et al., 2020)</ref>, which requires generating a coherent lo T2 \u2020 base . Moreover, we evaluated two task-specific pretraining models including PlotMachines (PM) <ref type=\"bibr\" target=\"#b40\">(Rashkin et al., 2020)</ref> and Plan&amp;Write (PW) <ref type=\"bibr\" on</head><p>We built the dataset for this task automatically based on filtered stories. We followed <ref type=\"bibr\" target=\"#b40\">Rashkin et al. (2020)</ref> to extract the outline of a story using t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b14\">(Fan et al., 2018)</ref> with k = 40 and a softmax temperature of 0.7 <ref type=\"bibr\" target=\"#b19\">(Goodfellow et al., 2014)</ref> for the conditional continuation task. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ificial and contains innate biases between right and wrong endings in some features such as lengths <ref type=\"bibr\" target=\"#b45\">(Schwartz et al., 2017;</ref><ref type=\"bibr\" target=\"#b46\">Sharma et ><p>(7) BERT w/o Context: We fine-tuned BERT to directly choose without taking the context as input <ref type=\"bibr\" target=\"#b45\">(Schwartz et al., 2017)</ref>. (8) BERT w/o Long:</p><p>It is used to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ved significant advances in various natural language understanding (NLU) and generation (NLG) tasks <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b37\">Radford et  t al., 2017)</ref>.</p><p>(2) BERT: It is implemented based on the bert-base-Chinese register model <ref type=\"bibr\" target=\"#b13\">(Devlin et al., 2019)</ref>.</p><p>(3) RoBERTa: It is implemented bas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and generation tasks, we build LongLM following the original encoder-decoder design of Transformer <ref type=\"bibr\" target=\"#b48\">(Vaswani et al., 2017)</ref> with three different sizes, as shown in  Transformer: It has the same architecture as BERT base except that the number of layers is set to 3 <ref type=\"bibr\" target=\"#b48\">(Vaswani et al., 2017)</ref>.</p><p>(2) BERT: It is implemented based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: English benchmarks, many benchmarks were proposed to evaluate NLU for other languages, such as CLUE <ref type=\"bibr\" target=\"#b51\">(Xu et al., 2020a)</ref> for Chinese. Moreover, GLGE <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: selection <ref type=\"bibr\" target=\"#b33\">(Mostafazadeh et al., 2016)</ref>, story ending generation <ref type=\"bibr\" target=\"#b22\">(Guan et al., 2019)</ref>, and story completion <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .</p><p>Dataset Construction Prior studies <ref type=\"bibr\" target=\"#b50\">(Wang and Wan, 2019;</ref><ref type=\"bibr\" target=\"#b35\">Paul and Frank, 2021)</ref> automatically constructed datasets for th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ndings in some features such as lengths <ref type=\"bibr\" target=\"#b45\">(Schwartz et al., 2017;</ref><ref type=\"bibr\" target=\"#b46\">Sharma et al., 2018)</ref>. Such biases may leak information about th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ny models pretrained on long texts such as GPT3 <ref type=\"bibr\">(Brown et al., 2020)</ref> and CPM <ref type=\"bibr\" target=\"#b55\">(Zhang et al., 2020)</ref>, the lack of benchmark datasets makes it d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ration conditioned on short prompts <ref type=\"bibr\" target=\"#b14\">(Fan et al., 2018)</ref>, titles <ref type=\"bibr\" target=\"#b54\">(Yao et al., 2019)</ref> and beginnings <ref type=\"bibr\">(Guan et al. t generation through story generation conditioned on inputs with limited information such as titles <ref type=\"bibr\" target=\"#b54\">(Yao et al., 2019)</ref>. However, these tasks are extremely open-end otMachines (PM) <ref type=\"bibr\" target=\"#b40\">(Rashkin et al., 2020)</ref> and Plan&amp;Write (PW) <ref type=\"bibr\" target=\"#b54\">(Yao et al., 2019)</ref>, and two typical non-pretrained models inclu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , 2016)</ref>, roleplayerguild <ref type=\"bibr\" target=\"#b31\">(Louis and Sutton, 2018)</ref>, PG-19 <ref type=\"bibr\" target=\"#b38\">(Rae et al., 2020)</ref>, STORIUM <ref type=\"bibr\" target=\"#b1\">(Akou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trollability, that is, the imposing of controllable attributes on story generation such as keywords <ref type=\"bibr\" target=\"#b52\">(Xu et al., 2020b)</ref>, emotional trajectories <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ificial and contains innate biases between right and wrong endings in some features such as lengths <ref type=\"bibr\" target=\"#b45\">(Schwartz et al., 2017;</ref><ref type=\"bibr\" target=\"#b46\">Sharma et ><p>(7) BERT w/o Context: We fine-tuned BERT to directly choose without taking the context as input <ref type=\"bibr\" target=\"#b45\">(Schwartz et al., 2017)</ref>. (8) BERT w/o Long:</p><p>It is used to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: encoder and decoder, we propose to train LongLM with two pretraining tasks including text infilling <ref type=\"bibr\" target=\"#b39\">(Raffel et al., 2020)</ref> and conditional continuation <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oints into gate-level netlists based on the node representations learned by a graph neural network. <ref type=\"bibr\" target=\"#b5\">[6]</ref> develop a graph learning-based solution to extract desired l gate (cell) functions including AND, OR, INV, MAJ, MUX, NAND, NOR and XOR.</p><p>Motivated by ABGNN <ref type=\"bibr\" target=\"#b5\">[6]</ref>, our proposed FGNN follows an asynchronous message passing s on and then sends the newly-built representation to all its successors. Our work differs from ABGNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> since we use independent aggregators for vertices of differe ll-one vector. The red part in the formulation emphasizes the difference between our work and ABGNN <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>The vertex representations learned by FGNN can be dir ddition), whose boundaries are defined as the input/output wires interacting with external circuits <ref type=\"bibr\" target=\"#b5\">[6]</ref>. In general, our task is to recognize the boundaries of targ we focus on identifying the output boundaries of adders, following the same experimental setting as <ref type=\"bibr\" target=\"#b5\">[6]</ref>, where the performance is measured in terms of recall and F1 od <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and (3) customized GNN-based method that adapts to DAGs <ref type=\"bibr\" target=\"#b5\">[6]</ref>. All these baselines are trained end-to-end, and we report t 4% \u223c 12.3% recall gain and 1.2% \u223c 10.5% F1-Score improvement compared with the second-best solution <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Additionally, we can see that GNN-based methods <ref type=\" tions and further construct a global-level representation for each netlist as described in Equation <ref type=\"bibr\" target=\"#b5\">(6)</ref>. The representations are then fed into a classifier (Multila t solution <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Additionally, we can see that GNN-based methods <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> significantly outperfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nalityconstant augmentation, resulting in 2\ud835\udc41 augmented netlists. Following the sampling strategy in <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we define the negative samples for any positive pair (\ud835\udc50 \u2032 onalityconstant augmentation, resulting in 2\ud835\udc41 augmented netlists. Following the sampling strategy in<ref type=\"bibr\" target=\"#b22\">[23]</ref>, we define the negative samples for any positive pair (\ud835\udc50 \u2032. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \ud835\udc41 \u2212 2) netlists within the mini-batch. A normalized temperature-scaled cross-entropy loss (NT-Xent) <ref type=\"bibr\" target=\"#b23\">[24]</ref> is then applied to maximize the consistency between positi 2\ud835\udc41 \u2212 2) netlists within the mini-batch. A normalized temperature-scaled cross-entropy loss (NT-Xent)<ref type=\"bibr\" target=\"#b23\">[24]</ref> is then applied to maximize the consistency between positi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> have emerged as a promising approach for analyzing graph-str. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rning achieves great success in the computer vision domain <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. Theoretical analyses shed light on the reasons behind thei  views without causing semantic changes is relatively natural for images (e.g., translation, scale) <ref type=\"bibr\" target=\"#b15\">[16]</ref>, it is not explicit for graph data. The primary difficulty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct representation termed level-dependent decaying sum (LDDS) existence vector (EV) is introduced in <ref type=\"bibr\" target=\"#b4\">[5]</ref> to embed a circuit node with its neighbors. A fixed number o ad n=\"5\">Experiments</head><p>We implemented the netlist representation learning framework with DGL <ref type=\"bibr\" target=\"#b4\">[5]</ref>, a graph learning library based on PyTorch. FGNN is pre-trai enerating node-level representations, covering the following three categories: (1) CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, (2) general GNN-based method <ref type=\"bibr\" target=\"#b7\"> =\"#b13\">14]</ref> as the baseline methods for comparison. These works have covered CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, general GNN-based method <ref type=\"bibr\" target=\"#b6\">[7]< e use accuracy as the performance metric.</p><p>We reimplemented several representative prior works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target  only 3.1% compared with the best performance. In contrast, the performance of the baseline methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  incorporating several perturbations, e.g., edge dropping, node dropping, feature masking, etc. GCA <ref type=\"bibr\" target=\"#b19\">[20]</ref> is proposed to explore graph data augmentation.</p><p>Howe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  outstanding performance, contrastive learning achieves great success in the computer vision domain <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. Theoretical analy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: de representations by maximizing mutual information between global and local embeddings. You et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> build multiple views of a graph by incorporating several p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge in incorporating ML in electronic design automation (EDA) <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref>. Most existing works . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hniques develop rapidly, there is a surge in incorporating ML in electronic design automation (EDA) <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe re selected to satisfy the fixed-input-size requirement of convolutional neural networks. Ma et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> propose an iterative process to insert observation points in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct representation termed level-dependent decaying sum (LDDS) existence vector (EV) is introduced in <ref type=\"bibr\" target=\"#b4\">[5]</ref> to embed a circuit node with its neighbors. A fixed number o ad n=\"5\">Experiments</head><p>We implemented the netlist representation learning framework with DGL <ref type=\"bibr\" target=\"#b4\">[5]</ref>, a graph learning library based on PyTorch. FGNN is pre-trai enerating node-level representations, covering the following three categories: (1) CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, (2) general GNN-based method <ref type=\"bibr\" target=\"#b7\"> =\"#b13\">14]</ref> as the baseline methods for comparison. These works have covered CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, general GNN-based method <ref type=\"bibr\" target=\"#b6\">[7]< e use accuracy as the performance metric.</p><p>We reimplemented several representative prior works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target  only 3.1% compared with the best performance. In contrast, the performance of the baseline methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erest in a unique graph type, directed acyclic graph (DAG) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. DAGs are widely applied to model many realworld data, incl esentative prior works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> as the baseline methods for comparison. These works have co f the baseline methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> on the same case is decreased by 5.0%, 4.6% and 4.9% respec /ref> construct an impressive GNN architecture driven by the partial order induced by DAG. Besides, <ref type=\"bibr\" target=\"#b13\">[14]</ref> propose an asynchronous message passing scheme to encode c <ref type=\"bibr\" target=\"#b6\">[7]</ref>, as well as customized GNN-based method that adapts to DAGs <ref type=\"bibr\" target=\"#b13\">[14]</ref>. All these baselines are trained end-to-end, and we report es 97.5% target netlists, achieving a performance gain of 6.2% accuracy over the second-best method <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Moreover, our proposed framework suffers from slighter pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tectures to evaluate the generalization ability. Specifically, we use adder/multiplier designs from <ref type=\"bibr\" target=\"#b24\">[25]</ref>, whose architectures are distinguished from training ones.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  many researchers have been focusing on extending the contrastive methods to handle graph data. DGI <ref type=\"bibr\" target=\"#b17\">[18]</ref> embeds high-order global contextual features into node rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentation with its neighborhood message.</p><p>Various GNNs <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref> have been propose. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> have emerged as a promising approach for analyzing graph-str. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct representation termed level-dependent decaying sum (LDDS) existence vector (EV) is introduced in <ref type=\"bibr\" target=\"#b4\">[5]</ref> to embed a circuit node with its neighbors. A fixed number o ad n=\"5\">Experiments</head><p>We implemented the netlist representation learning framework with DGL <ref type=\"bibr\" target=\"#b4\">[5]</ref>, a graph learning library based on PyTorch. FGNN is pre-trai enerating node-level representations, covering the following three categories: (1) CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, (2) general GNN-based method <ref type=\"bibr\" target=\"#b7\"> =\"#b13\">14]</ref> as the baseline methods for comparison. These works have covered CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, general GNN-based method <ref type=\"bibr\" target=\"#b6\">[7]< e use accuracy as the performance metric.</p><p>We reimplemented several representative prior works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target  only 3.1% compared with the best performance. In contrast, the performance of the baseline methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b15\">16]</ref>. Theoretical analyses shed light on the reasons behind their success <ref type=\"bibr\" target=\"#b16\">[17]</ref>: objectives used in contrastive methods can be seen as max. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1 Graph Neural Network</head><p>Graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> have emerged as a pro egories: (1) CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, (2) general GNN-based method <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and (3) customized GNN-based method that adapts to DAGs <re \">[6]</ref>. Additionally, we can see that GNN-based methods <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> significantly outperform previous CNN-based work on all the c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  incorporating several perturbations, e.g., edge dropping, node dropping, feature masking, etc. GCA <ref type=\"bibr\" target=\"#b19\">[20]</ref> is proposed to explore graph data augmentation.</p><p>Howe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hniques develop rapidly, there is a surge in incorporating ML in electronic design automation (EDA) <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe re selected to satisfy the fixed-input-size requirement of convolutional neural networks. Ma et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> propose an iterative process to insert observation points in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentation with its neighborhood message.</p><p>Various GNNs <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref> have been propose. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sign automation (EDA) <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref>. Most existing works follow a representation learning paradi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentation with its neighborhood message.</p><p>Various GNNs <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref> have been propose. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rg/ns/1.0\"><head n=\"2\">Preliminaries 2.1 Graph Neural Network</head><p>Graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe rks have covered CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, general GNN-based method <ref type=\"bibr\" target=\"#b6\">[7]</ref>, as well as customized GNN-based method that adapts to DAGs  c.</p><p>We reimplemented several representative prior works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> as the baseline metho rmance. In contrast, the performance of the baseline methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> on the same case is d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erest in a unique graph type, directed acyclic graph (DAG) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. DAGs are widely applied to model many realworld data, incl esentative prior works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> as the baseline methods for comparison. These works have co f the baseline methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> on the same case is decreased by 5.0%, 4.6% and 4.9% respec /ref> construct an impressive GNN architecture driven by the partial order induced by DAG. Besides, <ref type=\"bibr\" target=\"#b13\">[14]</ref> propose an asynchronous message passing scheme to encode c <ref type=\"bibr\" target=\"#b6\">[7]</ref>, as well as customized GNN-based method that adapts to DAGs <ref type=\"bibr\" target=\"#b13\">[14]</ref>. All these baselines are trained end-to-end, and we report es 97.5% target netlists, achieving a performance gain of 6.2% accuracy over the second-best method <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Moreover, our proposed framework suffers from slighter pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> have emerged as a promising approach for analyzing graph-str. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erest in a unique graph type, directed acyclic graph (DAG) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. DAGs are widely applied to model many realworld data, incl esentative prior works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> as the baseline methods for comparison. These works have co f the baseline methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> on the same case is decreased by 5.0%, 4.6% and 4.9% respec /ref> construct an impressive GNN architecture driven by the partial order induced by DAG. Besides, <ref type=\"bibr\" target=\"#b13\">[14]</ref> propose an asynchronous message passing scheme to encode c <ref type=\"bibr\" target=\"#b6\">[7]</ref>, as well as customized GNN-based method that adapts to DAGs <ref type=\"bibr\" target=\"#b13\">[14]</ref>. All these baselines are trained end-to-end, and we report es 97.5% target netlists, achieving a performance gain of 6.2% accuracy over the second-best method <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Moreover, our proposed framework suffers from slighter pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rg/ns/1.0\"><head n=\"2\">Preliminaries 2.1 Graph Neural Network</head><p>Graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe rks have covered CNN-based method <ref type=\"bibr\" target=\"#b4\">[5]</ref>, general GNN-based method <ref type=\"bibr\" target=\"#b6\">[7]</ref>, as well as customized GNN-based method that adapts to DAGs  c.</p><p>We reimplemented several representative prior works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> as the baseline metho rmance. In contrast, the performance of the baseline methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> on the same case is d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge in incorporating ML in electronic design automation (EDA) <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref>. Most existing works . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentation with its neighborhood message.</p><p>Various GNNs <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref> have been propose. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge in incorporating ML in electronic design automation (EDA) <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref>. Most existing works . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" targ target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref> utilizes virtual mem target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref> achieves memory elas -13, 15, 16]</ref> (or similar fine-grain network interfaces <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target , 5, 11-13, 15,16]</ref> or similar customized DMA protocols <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Figure <ref type=\"fig luding page cache coherence) to the network <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memory nodes <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. However, all these a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t al memory systems <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t gregation systems <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cant efforts into making it practical in large-scale systems <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe : i) pagebased and ii) object-based. The page-based approach <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe -based Memory Pool</head><p>Page-based memory disaggregation <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe  that migrate locally cached data in a finer granular manner <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t al memory systems <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t gregation systems <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref> or reduce the page fault overhead by offloading memory manag ccess (RDMA) <ref type=\"bibr\">[4, 5, 11-13, 15, 16]</ref> (or similar fine-grain network interfaces <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target emory access (RDMA) <ref type=\"bibr\">[4, 5, 11-13, 15,16]</ref> or similar customized DMA protocols <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e memory disaggregation and put significant efforts into making it practical in large-scale systems <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe ata between a host and memory server(s): i) pagebased and ii) object-based. The page-based approach <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe -c.org/ns/1.0\"><head n=\"2.2\">Swap: Page-based Memory Pool</head><p>Page-based memory disaggregation <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe ><p>Note that there are several studies that migrate locally cached data in a finer granular manner <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e memory disaggregation and put significant efforts into making it practical in large-scale systems <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe ata between a host and memory server(s): i) pagebased and ii) object-based. The page-based approach <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe -c.org/ns/1.0\"><head n=\"2.2\">Swap: Page-based Memory Pool</head><p>Page-based memory disaggregation <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe ><p>Note that there are several studies that migrate locally cached data in a finer granular manner <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: management efficiency <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref>. Many studies have explored various software and hardware ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t al memory systems <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t gregation systems <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[12]</ref>), and four graph analysis workloads (MIS <ref type=\"bibr\" target=\"#b31\">[32]</ref>, BFS <ref type=\"bibr\" target=\"#b32\">[33]</ref>, CC <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and BC <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" targe page fault overhead by offloading memory management (including page cache coherence) to the network <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memory nodes <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  solution among a large search space, often with millions of choices. Therefore, advanced compilers <ref type=\"bibr\" target=\"#b12\">[15,</ref><ref type=\"bibr\" target=\"#b30\">33,</ref><ref type=\"bibr\" ta tion. Our own experience shows that tuning an end-to-end DNN model using state-of-the-art compilers <ref type=\"bibr\" target=\"#b12\">[15,</ref><ref type=\"bibr\" target=\"#b30\">33]</ref> often requires day  tensor expression ( \u00a73.1). The expression is generated by users or from a graph-level DNN compiler <ref type=\"bibr\" target=\"#b12\">[15,</ref><ref type=\"bibr\" target=\"#b23\">26,</ref><ref type=\"bibr\" ta LER takes input of a tensor computation as an indexbased lambda expression, i.e., tensor expression <ref type=\"bibr\" target=\"#b12\">[15,</ref><ref type=\"bibr\" target=\"#b24\">27]</ref>. It describes how  er differs from the evaluation process driven by a machine learning algorithm in previous compilers <ref type=\"bibr\" target=\"#b12\">[15,</ref><ref type=\"bibr\" target=\"#b30\">33,</ref><ref type=\"bibr\" ta uct an optimization space overlapped with, but different from, existing DNN compilers (e.g., Ansor) <ref type=\"bibr\" target=\"#b12\">[15,</ref><ref type=\"bibr\" target=\"#b30\">33,</ref><ref type=\"bibr\" ta d, ROLLER can generate efficient kernels in seconds.</p><p>We have implemented ROLLER on top of TVM <ref type=\"bibr\" target=\"#b12\">[15]</ref> and Rammer <ref type=\"bibr\" target=\"#b23\">[26]</ref>, and   the majority of operators in DNN models and is widely used in existing DNN compilers including TVM <ref type=\"bibr\" target=\"#b12\">[15]</ref>, Ansor <ref type=\"bibr\" target=\"#b30\">[33]</ref>, and Flex .tei-c.org/ns/1.0\"><head n=\"4\">Implementation</head><p>Our implementation of ROLLER is based on TVM <ref type=\"bibr\" target=\"#b12\">[15]</ref> and Rammer <ref type=\"bibr\" target=\"#b23\">[26]</ref>, two  e compare ROLLER against other tensor compilers, vendor libraries and DNN frameworks, including TVM <ref type=\"bibr\" target=\"#b12\">[15]</ref> (v0.8) and Ansor <ref type=\"bibr\" target=\"#b30\">[33]</ref> evel loop computation, which essentially defines a large space with a combinatorial complexity. TVM <ref type=\"bibr\" target=\"#b12\">[15]</ref> inherits the insight from Halide <ref type=\"bibr\" target=\" initiatives.</p><p>Graph-level DNN compilers like XLA <ref type=\"bibr\" target=\"#b8\">[11]</ref>, TVM <ref type=\"bibr\" target=\"#b12\">[15]</ref>, and Rammer <ref type=\"bibr\" target=\"#b23\">[26]</ref> focu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ode structure and adopts an evolution algorithm to find performant kernels. Compilers like Tiramisu <ref type=\"bibr\" target=\"#b11\">[14]</ref>, AKG <ref type=\"bibr\" target=\"#b29\">[32]</ref>, and Tensor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ode structure and adopts an evolution algorithm to find performant kernels. Compilers like Tiramisu <ref type=\"bibr\" target=\"#b11\">[14]</ref>, AKG <ref type=\"bibr\" target=\"#b29\">[32]</ref>, and Tensor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e of ROLLER generated kernels by comparing against TVM (i.e., AutoTVM with XGBoost tuning algorithm <ref type=\"bibr\" target=\"#b13\">[16]</ref>), Ansor, cuBLAS (for matrix multiplication operators) and  24\">[27]</ref> and describes DNN operators as loop optimization schedule primitives. Later, AutoTVM <ref type=\"bibr\" target=\"#b13\">[16]</ref> extends TVM to apply an ML-method to search for the best c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s an indexbased lambda expression, i.e., tensor expression <ref type=\"bibr\" target=\"#b12\">[15,</ref><ref type=\"bibr\" target=\"#b24\">27]</ref>. It describes how each element in the output tensor is comp natorial complexity. TVM <ref type=\"bibr\" target=\"#b12\">[15]</ref> inherits the insight from Halide <ref type=\"bibr\" target=\"#b24\">[27]</ref> and describes DNN operators as loop optimization schedule . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s an indexbased lambda expression, i.e., tensor expression <ref type=\"bibr\" target=\"#b12\">[15,</ref><ref type=\"bibr\" target=\"#b24\">27]</ref>. It describes how each element in the output tensor is comp natorial complexity. TVM <ref type=\"bibr\" target=\"#b12\">[15]</ref> inherits the insight from Halide <ref type=\"bibr\" target=\"#b24\">[27]</ref> and describes DNN operators as loop optimization schedule . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ws. Its input is an ONNX graph <ref type=\"bibr\" target=\"#b7\">[9]</ref> or a TensorFlow frozen graph <ref type=\"bibr\" target=\"#b10\">[13]</ref>. ROLLER first leverages Rammer to conduct graph level opti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t loop-level optimization configuration only for convolution operators on multi-core CPUs. And DREW <ref type=\"bibr\" target=\"#b27\">[30]</ref> proposes a new way to optimize Winograd convolution using . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er filters, which are usually implemented with a more efficient numerical algorithm (e.g., Winograd <ref type=\"bibr\" target=\"#b20\">[23]</ref>) in cuDNN and hard to be expressed by the tensor expressio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b5\">[7]</ref> is a template for implementing matrixmultiplication. An analytical model <ref type=\"bibr\" target=\"#b21\">[24]</ref> is proposed to find the best loop-level optimization confi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nst Ansor's. Benchmarks. Our evaluation benchmark uses four typical DNN models, including ResNet-50 <ref type=\"bibr\" target=\"#b16\">[19]</ref> (CNN), LSTM <ref type=\"bibr\" target=\"#b17\">[20]</ref> (RNN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: noisy space such as a classroom. In this paper, we employed the cross-spectrum phase analysis (CSP) <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which can sensitively detect correlations without dependi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bgroups. There are various methods for network clustering, but here we will use spectral clustering <ref type=\"bibr\" target=\"#b18\">[19]</ref>, which allows us to specify the target number of clusters.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b4\">[5]</ref> <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Speech separation is also explored <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alysis, the context is discussed as cohesion and coherence <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. However, it is v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b15\">[16]</ref> that use MFCCs and spectral features, and power-based methods <ref type=\"bibr\" target=\"#b16\">[17]</ref>[18] that use speech power as a feature. In the assumed sce. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b5\">[6]</ref>. Speech separation is also explored <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. However, it cannot w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion and coherence <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. However, it is very difficult to judge the context becaus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nes on the recorders can be estimated. There is a study combining this technology with watermarking <ref type=\"bibr\" target=\"#b9\">[10]</ref>. If the locations of the speakers are determined, we can pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata collected by multiple microphones distributed on a desk <ref type=\"bibr\" target=\"#b4\">[5]</ref> <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Speech separation is also explored <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in the center of the desk, the position and direction of the speaker can be estimated by the MUSIC <ref type=\"bibr\" target=\"#b2\">[3]</ref>[4] method or other methods. Research is also being conducted. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the context of the texts. In discourse analysis, the context is discussed as cohesion and coherence <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion and coherence <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. However, it is very difficult to judge the context becaus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b5\">[6]</ref>. Speech separation is also explored <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. However, it cannot w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion is also explored <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. However, it cannot work in the case where a speaker with a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: one to record what they speak, and subsequently, ASR is used to convert their speech data into text <ref type=\"bibr\" target=\"#b0\">[1]</ref> <ref type=\"bibr\" target=\"#b1\">[2]</ref>. By observing the in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion and coherence <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>. However, it is very difficult to judge the context becaus. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: one to record what they speak, and subsequently, ASR is used to convert their speech data into text <ref type=\"bibr\" target=\"#b0\">[1]</ref> <ref type=\"bibr\" target=\"#b1\">[2]</ref>. By observing the in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion is also explored <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref>. However, it cannot work in the case where a speaker with a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t leaks to other audio tracks.</p><p>There are two main types of VAD techniques: modelbased methods <ref type=\"bibr\" target=\"#b15\">[16]</ref> that use MFCCs and spectral features, and power-based meth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ance on the few-shot data-to-text generation tasks.</p><p>In this paper, we introduce self-training <ref type=\"bibr\" target=\"#b10\">[Scudder, 1965]</ref> as a better few-shot learner for data-to-text g ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Self-Training</head><p>Self-training <ref type=\"bibr\" target=\"#b10\">[Scudder, 1965]</ref> is a teacher-student framework to leverage unla ata better. Thus, researchers devise complex encoder structures based on sequential neural networks <ref type=\"bibr\" target=\"#b10\">[Trisedya et al., 2018]</ref> and graph neural networks <ref type=\"bi n=\"4.1\">Dataset</head><p>WebNLG. This dataset aims to generate textual descriptions for RDF triples <ref type=\"bibr\" target=\"#b10\">[Shimorina and Gardent, 2018]</ref>. The number of instances in train http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Automatic Result</head><p>We followed the existing works <ref type=\"bibr\" target=\"#b10\">[Shimorina and Gardent, 2018;</ref><ref type=\"bibr\" target=\"#b2\">Chen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n the complex relationship between source structured data and target texts via limited labeled data <ref type=\"bibr\" target=\"#b4\">[Gong et al., 2020]</ref>. Existing works commonly continue pre-traini hrough semi-supervised learning. Self-training has been applied to the tasks of text classification <ref type=\"bibr\" target=\"#b4\">[Du et al., 2021]</ref> and generation <ref type=\"bibr\" target=\"#b5\">[ orks also show that self-training is complementary to pre-training for various classification tasks <ref type=\"bibr\" target=\"#b4\">[Du et al., 2021]</ref>.</p><p>For comparison, our work is the first a e generated texts based on coverage and generation probability which reflect the generation quality <ref type=\"bibr\" target=\"#b4\">[Gehrmann et al., 2018]</ref> to construct the pseudo-labeled dataset  generate fluent texts which describe structured data more clearly. We also calculated Fleiss' Kappa <ref type=\"bibr\" target=\"#b4\">[Fleiss, 1971]</ref> for each pairwise comparison to measure the agree. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly restricted by the number of labeled data <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2020b]</ref>. Especially in the task of few-shot data-to- 00 / 72,831. We followed the existing works <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2020b]</ref> to pre-process this dataset and use 50, 100,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing.</p><p>We further constructed the unlabeled dataset for each benchmark dataset based on GenWiki <ref type=\"bibr\" target=\"#b6\">[Jin et al., 2020]</ref>. This dataset consists of general-domain unpa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing.</p><p>We further constructed the unlabeled dataset for each benchmark dataset based on GenWiki <ref type=\"bibr\" target=\"#b6\">[Jin et al., 2020]</ref>. This dataset consists of general-domain unpa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  dataset into different subsets based on some difficulty metric such as the number of input triples <ref type=\"bibr\" target=\"#b9\">[Ribeiro et al., 2020b]</ref>. At each iteration, we first collect the ral networks <ref type=\"bibr\" target=\"#b10\">[Trisedya et al., 2018]</ref> and graph neural networks <ref type=\"bibr\" target=\"#b9\">[Ribeiro et al., 2020b]</ref> to achieve this goal. Recently, since te  of structures in the input data and has a significant impact on the quality of the generated texts <ref type=\"bibr\" target=\"#b9\">[Ribeiro et al., 2020b]</ref>. We also try other metrics in \u00a74.6.</p><  results, we conducted human evaluation in the 1% setting of WebNLG. We followed the existing works <ref type=\"bibr\" target=\"#b9\">[Ribeiro et al., 2020b]</ref> to use two criteria: fluency (whether a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  dataset into different subsets based on some difficulty metric such as the number of input triples <ref type=\"bibr\" target=\"#b9\">[Ribeiro et al., 2020b]</ref>. At each iteration, we first collect the ral networks <ref type=\"bibr\" target=\"#b10\">[Trisedya et al., 2018]</ref> and graph neural networks <ref type=\"bibr\" target=\"#b9\">[Ribeiro et al., 2020b]</ref> to achieve this goal. Recently, since te  of structures in the input data and has a significant impact on the quality of the generated texts <ref type=\"bibr\" target=\"#b9\">[Ribeiro et al., 2020b]</ref>. We also try other metrics in \u00a74.6.</p><  results, we conducted human evaluation in the 1% setting of WebNLG. We followed the existing works <ref type=\"bibr\" target=\"#b9\">[Ribeiro et al., 2020b]</ref> to use two criteria: fluency (whether a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly restricted by the number of labeled data <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2020b]</ref>. Especially in the task of few-shot data-to- 00 / 72,831. We followed the existing works <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2020b]</ref> to pre-process this dataset and use 50, 100,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing.</p><p>We further constructed the unlabeled dataset for each benchmark dataset based on GenWiki <ref type=\"bibr\" target=\"#b6\">[Jin et al., 2020]</ref>. This dataset consists of general-domain unpa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\">T5 [Raffel et al., 2020]</ref> on data-to-text datasets and report impressive results <ref type=\"bibr\" target=\"#b8\">[Ribeiro et al., 2020a;</ref><ref type=\"bibr\">Kale and Rastogi, 2020]< linearize the structured data as a sequence of triples containing subjects, predicates, and objects <ref type=\"bibr\" target=\"#b8\">[Ribeiro et al., 2020a;</ref><ref type=\"bibr\" target=\"#b2\">Chen et al. l., 2002]</ref>, METEOR <ref type=\"bibr\" target=\"#b2\">[Banerjee and Lavie, 2005]</ref>, and ROUGE-L <ref type=\"bibr\" target=\"#b8\">[Lin, 2004]</ref> to evaluate the generated results on WebNLG, and use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd few-shot settings of downstream datasets <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2020a;</ref><ref type=\"bibr\" target=\"#b7\">Ke et al., 2021;</ref><ref type=\"bibr\">Xing and Wan, 2021]</ref>.</p>< io. This dataset aims to generate the first sentence of biography descriptions for Wikipedia tables <ref type=\"bibr\" target=\"#b7\">[Lebret et al., 2016]</ref>. The original split of the training / vali ta when encoding corrupted structured data; 2) Graph-level reconstruction (GraphRecon) from JointGT <ref type=\"bibr\" target=\"#b7\">[Ke et al., 2021]</ref> which predicts the masked tokens at the output , and collected the generated results from CBST and other baselines. We adopted pairwise comparison <ref type=\"bibr\" target=\"#b7\">[Ke et al., 2021]</ref> between CBST and other baselines. For each pai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"> \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Transformers <ref type=\"bibr\" target=\"#b15\">[Vaswani et al., 2017]</ref> have revolutionized many areas of AI wit <p>Self Attention. The majority of recent language models are based on the Transformer architecture <ref type=\"bibr\" target=\"#b15\">[Vaswani et al., 2017]</ref>. One of the most important components in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: RTa <ref type=\"bibr\">[Liu et al., 2019]</ref>, <ref type=\"bibr\">ALBERT [Lan et al., 2019]</ref>, T5 <ref type=\"bibr\" target=\"#b11\">[Raffel et al., 2019]</ref>, ELECTRA <ref type=\"bibr\" target=\"#b3\">[C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he proposed external attention, the accuracy of commonsense reasoning using a DeBERTa-xxlarge model <ref type=\"bibr\" target=\"#b7\">[He et al., 2020]</ref> can be significantly boosted from 83.8% to 90. l et al., 2019]</ref>, ELECTRA <ref type=\"bibr\" target=\"#b3\">[Clark et al., 2020]</ref> and DeBERTa <ref type=\"bibr\" target=\"#b7\">[He et al., 2020]</ref> as the text encoder, achieving state-of-the-ar osen from {1e \u2212 5, 2e \u2212 5, 3e \u2212 6} for all encoders except for DeBERTa; following the DeBERTa paper <ref type=\"bibr\" target=\"#b7\">[He et al., 2020]</ref> we use a smaller learning rate, chosen from {4 the dev set of Common-senseQA. Based on these results, we choose ELECTRA-large and DeBERTa variants <ref type=\"bibr\" target=\"#b7\">[He et al., 2020]</ref> as the encoders for subsequent experimentation adversarial training (VAT) can improve the performance for general NLU and question answering tasks <ref type=\"bibr\" target=\"#b7\">[Jiang et al., 2020]</ref>. In the multiple-choice commonsense reasoni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"foot\" target=\"#foot_2\">2</ref> to retrieve relevant relation triples in the ConceptNet graph <ref type=\"bibr\" target=\"#b13\">[Speer et al., 2017]</ref>. Suppose the question entity is e q and th \"bibr\" target=\"#b2\">[Chang et al., 2021;</ref><ref type=\"bibr\" target=\"#b16\">Yao et al., 2022;</ref><ref type=\"bibr\" target=\"#b13\">Song et al., 2021]</ref>. <ref type=\"bibr\" target=\"#b9\">Lin et al. [2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as Wikipedia and dictionaries for commonsense reasoning <ref type=\"bibr\">[Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b5\">Guu et al., 2020]</ref>. <ref type=\"bibr\" target=\"#b0\">Bhakthavatsalam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as Wikipedia and dictionaries for commonsense reasoning <ref type=\"bibr\">[Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b5\">Guu et al., 2020]</ref>. <ref type=\"bibr\" target=\"#b0\">Bhakthavatsalam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: RTa <ref type=\"bibr\">[Liu et al., 2019]</ref>, <ref type=\"bibr\">ALBERT [Lan et al., 2019]</ref>, T5 <ref type=\"bibr\" target=\"#b11\">[Raffel et al., 2019]</ref>, ELECTRA <ref type=\"bibr\" target=\"#b3\">[C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: RTa <ref type=\"bibr\">[Liu et al., 2019]</ref>, <ref type=\"bibr\">ALBERT [Lan et al., 2019]</ref>, T5 <ref type=\"bibr\" target=\"#b11\">[Raffel et al., 2019]</ref>, ELECTRA <ref type=\"bibr\" target=\"#b3\">[C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers, recent studies show that they cannot perfectly memorize all the details of their training data <ref type=\"bibr\" target=\"#b16\">[Wang et al., 2022]</ref>.</p><p>To tackle this challenge, we propose  question q we filter itself from the retrieved results to avoid data leakage.</p><p>Different from <ref type=\"bibr\" target=\"#b16\">Wang et al. [2022]</ref> where the retrieval questions are only obtai  al., 2020]</ref> as the text encoder, achieving state-of-the-art performance on the GLUE benchmark <ref type=\"bibr\" target=\"#b16\">[Wang et al., 2019]</ref>. Thus, we evaluate these models as encoders most popular choices for external knowledge <ref type=\"bibr\" target=\"#b2\">[Chang et al., 2021;</ref><ref type=\"bibr\" target=\"#b16\">Yao et al., 2022;</ref><ref type=\"bibr\" target=\"#b13\">Song et al., 20  al., 2021]</ref>, conversational <ref type=\"bibr\">QA [Qin et al., 2019]</ref>, and text generation <ref type=\"bibr\" target=\"#b16\">[Yu et al., 2020]</ref>. Compared with prior work that uses extra mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as Wikipedia and dictionaries for commonsense reasoning <ref type=\"bibr\">[Xu et al., 2021;</ref><ref type=\"bibr\" target=\"#b5\">Guu et al., 2020]</ref>. <ref type=\"bibr\" target=\"#b0\">Bhakthavatsalam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"foot\" target=\"#foot_2\">2</ref> to retrieve relevant relation triples in the ConceptNet graph <ref type=\"bibr\" target=\"#b13\">[Speer et al., 2017]</ref>. Suppose the question entity is e q and th \"bibr\" target=\"#b2\">[Chang et al., 2021;</ref><ref type=\"bibr\" target=\"#b16\">Yao et al., 2022;</ref><ref type=\"bibr\" target=\"#b13\">Song et al., 2021]</ref>. <ref type=\"bibr\" target=\"#b9\">Lin et al. [2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tp://www.tei-c.org/ns/1.0\"><head n=\"3.3.3\">Annotation Quality &amp; Remuneration</head><p>Following <ref type=\"bibr\" target=\"#b0\">(Artstein and Poesio, 2008;</ref><ref type=\"bibr\" target=\"#b29\">McHugh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: amp; Remuneration</head><p>Following <ref type=\"bibr\" target=\"#b0\">(Artstein and Poesio, 2008;</ref><ref type=\"bibr\" target=\"#b29\">McHugh, 2012)</ref>, we use Cohen's kappa coefficient to measure the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b33\">Ohta et al., 2011;</ref><ref type=\"bibr\" target=\"#b21\">Kim et al., 2011</ref><ref type=\"bibr\" target=\"#b22\">Kim et al., , 2013;;</ref><ref type=\"bibr\" target=\"#b32\">Ohta et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b21\">Kim et al., 2011</ref><ref type=\"bibr\" target=\"#b22\">Kim et al., , 2013;;</ref><ref type=\"bibr\" target=\"#b32\">Ohta et al., 2013;</ref><ref type=\"bibr\">Van Landeghem et al., 2013)<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: = {start r , end r }. We give \u22121 for these not mentioned event arguments. 5) Ontology_QA. Following <ref type=\"bibr\" target=\"#b40\">Vargas-Vera and Motta (2004)</ref>, we refine the initial query in BE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sists of only 246 documents with very few (22% of total) cross-sentences argument annotations. RAMS <ref type=\"bibr\" target=\"#b12\">(Ebner et al., 2020)</ref> limits the scope of the arguments in a 5-s rrorist attack topic 6 . WikiEvents <ref type=\"bibr\" target=\"#b27\">(Li et al., 2021)</ref> and RAMS <ref type=\"bibr\" target=\"#b12\">(Ebner et al., 2020)</ref> consist of 246/9,124 documents with only 5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es events into hard news and soft news <ref type=\"bibr\" target=\"#b36\">(Reinemann et al., 2012;</ref><ref type=\"bibr\" target=\"#b38\">Tuchman, 1973)</ref>. Hard news is a social emergency that must be re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of hot events, so we focus on extracting events from news. Previous event schemas, such as FrameNet <ref type=\"bibr\" target=\"#b1\">(Baker, 2014)</ref> and HowNet <ref type=\"bibr\" target=\"#b7\">(Dong and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b33\">Ohta et al., 2011;</ref><ref type=\"bibr\" target=\"#b21\">Kim et al., 2011</ref><ref type=\"bibr\" target=\"#b22\">Kim et al., , 2013;;</ref><ref type=\"bibr\" target=\"#b32\">Ohta et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1, GE-NIA2013, Pathway Curation and MLEE <ref type=\"bibr\" target=\"#b35\">(Pyysalo et al., 2013;</ref><ref type=\"bibr\" target=\"#b33\">Ohta et al., 2011;</ref><ref type=\"bibr\" target=\"#b21\">Kim et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nstance, it can monitor political or military crises to generate real-time notifications and alerts <ref type=\"bibr\" target=\"#b8\">(Dragos, 2013)</ref>, and dig the links and connections (e.g., Who Met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is quite limited. Doc2EDAG, TDJEE and GIT <ref type=\"bibr\" target=\"#b45\">(Zheng et al., 2019;</ref><ref type=\"bibr\" target=\"#b41\">Wang et al., 2021;</ref><ref type=\"bibr\" target=\"#b43\">Xu et al., 202 urther refinement. Doc2EDAG, TDJEE and GIT <ref type=\"bibr\" target=\"#b45\">(Zheng et al., 2019;</ref><ref type=\"bibr\" target=\"#b41\">Wang et al., 2021;</ref><ref type=\"bibr\" target=\"#b43\">Xu et al., 202. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sists of only 246 documents with very few (22% of total) cross-sentences argument annotations. RAMS <ref type=\"bibr\" target=\"#b12\">(Ebner et al., 2020)</ref> limits the scope of the arguments in a 5-s rrorist attack topic 6 . WikiEvents <ref type=\"bibr\" target=\"#b27\">(Li et al., 2021)</ref> and RAMS <ref type=\"bibr\" target=\"#b12\">(Ebner et al., 2020)</ref> consist of 246/9,124 documents with only 5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t types in financial domain. Cancer Genetics, EPM, GENIA2011, GE-NIA2013, Pathway Curation and MLEE <ref type=\"bibr\" target=\"#b35\">(Pyysalo et al., 2013;</ref><ref type=\"bibr\" target=\"#b33\">Ohta et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t types in financial domain. Cancer Genetics, EPM, GENIA2011, GE-NIA2013, Pathway Curation and MLEE <ref type=\"bibr\" target=\"#b35\">(Pyysalo et al., 2013;</ref><ref type=\"bibr\" target=\"#b33\">Ohta et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b33\">Ohta et al., 2011;</ref><ref type=\"bibr\" target=\"#b21\">Kim et al., 2011</ref><ref type=\"bibr\" target=\"#b22\">Kim et al., , 2013;;</ref><ref type=\"bibr\" target=\"#b32\">Ohta et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2 articles in 5 categories. MAVEN <ref type=\"bibr\" target=\"#b42\">(Wang et al., 2020)</ref> and LSEE <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2017)</ref> only annotate event triggers, with 168/21 ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nstance, it can monitor political or military crises to generate real-time notifications and alerts <ref type=\"bibr\" target=\"#b8\">(Dragos, 2013)</ref>, and dig the links and connections (e.g., Who Met. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sentence-level to document-level.</p><p>Only a few datasets are curated for documentlevel EE. MUC-4 <ref type=\"bibr\" target=\"#b16\">(Grishman and Sundheim, 1996)</ref> provides 1,700 news articles anno. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re introduced. Inspired by some feature modulation methods <ref type=\"bibr\" target=\"#b79\">[80,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>, we propose a new p ces a learnable module to spatially transform feature maps. In the field of image generation, AdaIN <ref type=\"bibr\" target=\"#b39\">[40]</ref> generates scale and shift factors to characterize specific  input \u03b3 and \u03b2 to be input-independent. As FiLM <ref type=\"bibr\" target=\"#b65\">[66]</ref> and AdaIN <ref type=\"bibr\" target=\"#b39\">[40]</ref> show, we could obtain \u03b3 and \u03b2 by conditioning an image sam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: modulation methods <ref type=\"bibr\" target=\"#b79\">[80,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>, we propose a new parameter-efficient fine-tuning method na erator. In vision-language tasks, Conditional BN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and FiLM <ref type=\"bibr\" target=\"#b65\">[66]</ref> are often utilized to modulate the features of two modalit <p>Discussion. The first question is why we want the input \u03b3 and \u03b2 to be input-independent. As FiLM <ref type=\"bibr\" target=\"#b65\">[66]</ref> and AdaIN <ref type=\"bibr\" target=\"#b39\">[40]</ref> show, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  have been studied <ref type=\"bibr\" target=\"#b69\">[70,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" tar et=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b82\">83,</ref><ref type=\"bibr\" target=\"#b71\">72]</ref> are usually pre-tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b75\">[76]</ref>,</p><p>Oxford Flowers <ref type=\"bibr\" target=\"#b63\">[64]</ref>, Stanford Dogs <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Stanford Cars <ref type=\"bibr\" target=\"#b17\">[18]</ref et=\"#b75\">[76]</ref>,</p><p>Oxford Flowers <ref type=\"bibr\" target=\"#b63\">[64]</ref>, Stanford Dogs <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Stanford Cars <ref type=\"bibr\" target=\"#b17\">[18]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ream tasks. Some other works attempt to explore how to efficiently fine-tune the pre-trained models <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b95\">96]</ref> on the target task ype=\"bibr\" target=\"#b95\">96]</ref> on the target tasks. For instance, given a target task, SpotTune <ref type=\"bibr\" target=\"#b22\">[23]</ref> investigates which layers need to be fine-tuned. Touvron e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ream tasks. Some other works attempt to explore how to efficiently fine-tune the pre-trained models <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b95\">96]</ref> on the target task ype=\"bibr\" target=\"#b95\">96]</ref> on the target tasks. For instance, given a target task, SpotTune <ref type=\"bibr\" target=\"#b22\">[23]</ref> investigates which layers need to be fine-tuned. Touvron e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ameter-efficient fine-tuning strategy with prompt in the field of natural language processing (NLP) <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" ta wers102); ii) VPT <ref type=\"bibr\" target=\"#b43\">[44]</ref>, as well as other Adapter-based methods <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b59\">60]</ref>, introduces additi network to fine-tune in a parameter-efficient way. These adapters can be a small non-linear network <ref type=\"bibr\" target=\"#b35\">[36]</ref>, a hyper-network that generates model weights <ref type=\"b l be fed to a two-layer MLP to extract information in the channel dimension.</p><p>Adapter. Adapter <ref type=\"bibr\" target=\"#b35\">[36]</ref> is inserted into the transformer layer for efficient fine- dated. We also compare our method with recent parameter-efficient fine-tuning methods: iii) Adapter <ref type=\"bibr\" target=\"#b35\">[36]</ref>, where a new adapter structure with up-projection, non-lin  xml:id=\"tab_2\"><head>Table 2 :</head><label>2</label><figDesc>The complexity comparisons of Adapter<ref type=\"bibr\" target=\"#b35\">[36]</ref>, VPT<ref type=\"bibr\" target=\"#b43\">[44]</ref> and our SSF.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n graph-based data <ref type=\"bibr\" target=\"#b85\">[86,</ref><ref type=\"bibr\" target=\"#b84\">85,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. Recently, another architecture family, Transformer, has ga. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to obtain better performance. The most relevant ones to our work are various normalization methods <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ f type=\"bibr\" target=\"#b41\">[42]</ref>. The parameters introduced by the batch normalization layers <ref type=\"bibr\" target=\"#b40\">[41]</ref> are merged into the convolutional layers usually stacked b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  from the fact that the upstream datasets and downstream datasets have different data distributions <ref type=\"bibr\" target=\"#b70\">[71]</ref>. Therefore, it is difficult to apply the model weights tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b78\">[79]</ref>, NABirds <ref type=\"bibr\" target=\"#b75\">[76]</ref>,</p><p>Oxford Flowers <ref type=\"bibr\" target=\"#b63\">[64]</ref>, Stanford Dogs <ref type=\"bibr\" target=\"#b45\">[46]</ref> a \" target=\"#b78\">[79]</ref>, NABirds <ref type=\"bibr\" target=\"#b75\">[76]</ref>,</p><p>Oxford Flowers <ref type=\"bibr\" target=\"#b63\">[64]</ref>, Stanford Dogs <ref type=\"bibr\" target=\"#b45\">[46]</ref> a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b9\">[10]</ref> 2.4G).</p><p>A simple solution for the above problem is linear probing <ref type=\"bibr\" target=\"#b25\">[26]</ref>, where only the last head layer is fine-tuned. However, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b75\">[76]</ref>,</p><p>Oxford Flowers <ref type=\"bibr\" target=\"#b63\">[64]</ref>, Stanford Dogs <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Stanford Cars <ref type=\"bibr\" target=\"#b17\">[18]</ref et=\"#b75\">[76]</ref>,</p><p>Oxford Flowers <ref type=\"bibr\" target=\"#b63\">[64]</ref>, Stanford Dogs <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Stanford Cars <ref type=\"bibr\" target=\"#b17\">[18]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: our proposed SSF, which consists of CUB-200-2011 <ref type=\"bibr\" target=\"#b78\">[79]</ref>, NABirds <ref type=\"bibr\" target=\"#b75\">[76]</ref>,</p><p>Oxford Flowers <ref type=\"bibr\" target=\"#b63\">[64]< our proposed SSF, which consists of CUB-200-2011 <ref type=\"bibr\" target=\"#b78\">[79]</ref>, NABirds <ref type=\"bibr\" target=\"#b75\">[76]</ref>,</p><p>Oxford Flowers <ref type=\"bibr\" target=\"#b63\">[64]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch as object detection, instance segmentation and semantic segmentation. We employ the COCO dataset <ref type=\"bibr\" target=\"#b51\">[52]</ref> for evaluation based on mmdetection <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s for validation, which is one of the most challenging object detection datasets. We use Mask R-CNN <ref type=\"bibr\" target=\"#b27\">[28]</ref> with Swin Transformer backbone to perform our experiments, ect detection and instance segmentation, we perform experiments on the COCO dataset with Mask R-CNN <ref type=\"bibr\" target=\"#b27\">[28]</ref>, where Swin-T pre-trained on ImageNet-1K is adopted as the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: opose a pure MLP-based architecture, and subsequent papers <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref> have interestingly demonstrated that the MLP-based architec arget=\"#b55\">[56]</ref> (Swin-B), ConvNext-B <ref type=\"bibr\" target=\"#b56\">[57]</ref> and AS-MLP-B <ref type=\"bibr\" target=\"#b49\">[50]</ref>. The former builds a hierarchical transformer-based archit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e representative techniques is batch normalization folding used in the model compression algorithms <ref type=\"bibr\" target=\"#b41\">[42]</ref>. The parameters introduced by the batch normalization laye. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: refetcher, offset prefetchers, and the sandbox method for selecting the prefetch offset dynamically <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Offset prefetching is a generalization of next-line prefe </ref> (this list is not exhaustive).</p><p>Recently, Pugsley et al. introduced Sandbox prefetching <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The Sandbox prefetcher prefetches line X + D when line X  dge, the first published full-fledged offset prefetcher is the Sandbox prefetcher by Pugsley et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref>. However, the offset selection mechanism in the Sandbox pr owledge, the SBP prefetcher of Pugsley et al. is the first published full-fledged offset prefetcher <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The SBP prefetcher is cost-effective and was shown to out  with actual prefetches.</p><p>We implemented the SBP prefetcher as described in the original paper <ref type=\"bibr\" target=\"#b25\">[26]</ref>, but with a few modifications to make the comparison with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ins can be obtained by making the L3 replacement policy prefetch-aware (confirming previous studies <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e to exploit row buffer locality and bank parallelism. For read requests, an FR-FCFS policy is used <ref type=\"bibr\" target=\"#b27\">[28]</ref>. A row is left open after it has been accessed until a sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ses and use that history to predict future memory accesses <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: able some history about past memory accesses and use that history to predict future memory accesses <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ins can be obtained by making the L3 replacement policy prefetch-aware (confirming previous studies <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b26\">[27]</ref>.</p><p>\u2022 IP3: MRU insertion if demand miss, otherwise LRU insertion <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: try to identify, among load and store instructions, those that access memory with a constant stride <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target  a stride prefetcher <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref>. It features a 64-entry prefetch table accessed with the PC (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[2]</ref>, phase-based sampling <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Of these techniques, the sampling based approaches typicall 8\">[9]</ref> extended SimPoint to provide statistical confidence measures.</p><p>Wunderlich, et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> developed the SMARTS framework, which applies statistical sa wn techniques for inferring statistics about a population given a sample of that population. SMARTS <ref type=\"bibr\" target=\"#b3\">[4]</ref> demonstrated that systematic sampling can be used to approxi mpared LiveSim with no sampling simulation and with a sampling mode that was very similar to SMARTS <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= f type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >Many techniques have been developed to reduce simulation time including: benchmarks size reduction <ref type=\"bibr\" target=\"#b0\">[1]</ref>, specialized hardware <ref type=\"bibr\" target=\"#b1\">[2]</ref  comparable to those obtained with standard benchmark inputs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref>. Another approach is to accelerate the timing simulation usin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n by varying the level of simulation detail depending on the region of code that is being simulated <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hreaded simulation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and simulation of softerrors in caches <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b27\">28]</ref>, multithreaded simulation <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and simulation of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion-only mode, or even modes that run parts of the simulated benchmark directly on the host system <ref type=\"bibr\" target=\"#b7\">[8]</ref>. As the level of detail decreases, the speed of the simulati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ering more complex scenarios. Other researchers have looked for ways to speed up thermal simulation <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, multithreaded sim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chieve the goals of LiveSim.</p><p>The most closely related work to LiveSim is from Sandberg et al. <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Like us, they use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: The effectiveness of the original SimPoint proposal was purely heuristic based, but Perelman et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> extended SimPoint to provide statistical confidence measures ach cluster based on the cluster size. This technique has some similarities to what Perelman et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> do to statistically bound the error for results obtained usi d some of the seminal work related to profile based sampling <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  they proposed ways to determine when to begin warmup prior to simulating a sample. Eeckhout et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> proposed a similar technique that further reduced the amou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wever, LiveSim is able to take advantage of the correlation between code signatures and performance <ref type=\"bibr\" target=\"#b15\">[16]</ref> and use this information to group the checkpoints into clu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ps a high-precision value into discrete levels. In this work, we study integer uniform quantization <ref type=\"bibr\" target=\"#b13\">(Jacob et al., 2018</ref>) (specifically INT8) for better hardware su implicity; the discussion is similar for asymmetric cases (e.g., after ReLU) by adding a zero-point <ref type=\"bibr\" target=\"#b13\">(Jacob et al., 2018)</ref>.</p><p>Such quantizer uses the maximum abs imate the scale of activations channels using the calibration samples from the pre-training dataset <ref type=\"bibr\" target=\"#b13\">(Jacob et al., 2018)</ref>. However, this formula pushes all the quan or various convolutional neural works (CNNs) <ref type=\"bibr\" target=\"#b11\">(Han et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Jacob et al., 2018;</ref><ref type=\"bibr\" target=\"#b20\">Nagel et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and accelerating inference. It proves to be effective for various convolutional neural works (CNNs) <ref type=\"bibr\" target=\"#b11\">(Han et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Jacob et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntroduction</head><p>Large-scale language models (LLMs) show excellent performance on various tasks <ref type=\"bibr\" target=\"#b4\">(Brown et al., 2020a;</ref><ref type=\"bibr\" target=\"#b38\">Zhang et al.  serving LLMs is budget and energy-consuming due to the gigantic model size. For example, the GPT-3 <ref type=\"bibr\" target=\"#b4\">(Brown et al., 2020a</ref>) model contains 175B parameters, which will. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and accelerating inference. It proves to be effective for various convolutional neural works (CNNs) <ref type=\"bibr\" target=\"#b11\">(Han et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Jacob et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ned language models have achieved remarkable performance on various benchmarks by scaling up. GPT-3 <ref type=\"bibr\" target=\"#b5\">(Brown et al., 2020b)</ref> is the first LLM be-Table <ref type=\"table. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ications compared to FP16.</p><p>However, unlike CNN models or smaller transformer models like BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>, the activations of LLMs are difficult to   deal with the activation outliers. However, it only succeeds on small language models such as BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> and BART <ref type=\"bibr\" target=\"#b15\">(L. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers <ref type=\"bibr\" target=\"#b27\">(Shen et al., 2020;</ref><ref type=\"bibr\">Kim et al., 2021;</ref><ref type=\"bibr\" target=\"#b17\">Liu et al., 2021)</ref>. Weight equalization <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ications compared to FP16.</p><p>However, unlike CNN models or smaller transformer models like BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>, the activations of LLMs are difficult to   deal with the activation outliers. However, it only succeeds on small language models such as BERT <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> and BART <ref type=\"bibr\" target=\"#b15\">(L. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">(Mihaylov et al., 2018)</ref>, RTE <ref type=\"bibr\" target=\"#b31\">(Wang et al., 2018)</ref>, COPA <ref type=\"bibr\" target=\"#b24\">(Roemmele et al., 2011)</ref>, and one language modeling dataset Wiki. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s the multicommodity flow method, ILP, and multistage partition method, the negotiationbased method <ref type=\"bibr\" target=\"#b15\">[16]</ref> is a popular and effective method to solve routing problem he grid edge e can be formulated in the following two forms which were adopted by different routers <ref type=\"bibr\" target=\"#b15\">[16]</ref>- <ref type=\"bibr\" target=\"#b19\">[20]</ref>: cost(e) = b e  of Negotiation-Based Method</head><p>The negotiation-based method is first introduced in PathFinder <ref type=\"bibr\" target=\"#b15\">[16]</ref> to solve the FPGA routing problem, after which it is appli e value is zero) cells to be ripped-up and relocated. In the routing problem, McMurchie and Ebeling <ref type=\"bibr\" target=\"#b15\">[16]</ref> suggested that only nets passing through congested regions. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed to further reduce the displacement.</p><p>Several works <ref type=\"bibr\" target=\"#b6\">[7]</ref>- <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref> formulated the le w model to remove overlaps among the cells while minimizing the cell displacement, and Darav et al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> set a precomputed maximum movement \u03b8 as a hard constraint to tworkflow tasks, which can be solved with mature solutions <ref type=\"bibr\" target=\"#b5\">[6]</ref>- <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>. However, the flo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es considerable time compared with other state-of-the-art approaches. Four state-of-the-art methods <ref type=\"bibr\" target=\"#b10\">[11]</ref>- <ref type=\"bibr\" target=\"#b13\">[14]</ref> formulate the l ce the size of the problem, which may lead to a decline in the solution quality. 3) Several studies <ref type=\"bibr\" target=\"#b10\">[11]</ref>- <ref type=\"bibr\" target=\"#b13\">[14]</ref> reformulated th vergence of the negotiation-based method, we will add a penalty value to the target cost defined in <ref type=\"bibr\" target=\"#b10\">(11)</ref> for grids that cause pin short, pin access, or edge spacin  fence region and technology constraints, we tested the effects of the user-defined constant m f in <ref type=\"bibr\" target=\"#b10\">(11)</ref> on the displacement and runtime. When m f was set to zero,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  subregions, we deal with each fence region in turn and store each rectangular subregion in R-trees <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The cell assigned to the current fence region is then che. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m, after which it is applied for the ASIC routing problem. It has also been adopted in other fields <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. By allocating  on. 4) Negotiation order. 5) Negotiation region.  <ref type=\"bibr\" target=\"#b18\">[19]</ref> AND NVM <ref type=\"bibr\" target=\"#b20\">[21]</ref> The main components of the global router NTHU-Route 2.0 <r bal router NTHU-Route 2.0 <ref type=\"bibr\" target=\"#b18\">[19]</ref> and layer assignment method NVM <ref type=\"bibr\" target=\"#b20\">[21]</ref> are shown in Table <ref type=\"table\" target=\"#tab_0\">I</re egion congestion, and the net assignment order was ignored according to the experimental results in <ref type=\"bibr\" target=\"#b20\">[21]</ref>. The Negotiation Region denotes a local area where the Neg r\" target=\"#b18\">[19]</ref> ripped-up and rerouted nets in an adaptive bounding box, and Liu and Li <ref type=\"bibr\" target=\"#b20\">[21]</ref> found the minimum-cost global tree without changing the tw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m, after which it is applied for the ASIC routing problem. It has also been adopted in other fields <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. By allocating  on. 4) Negotiation order. 5) Negotiation region.  <ref type=\"bibr\" target=\"#b18\">[19]</ref> AND NVM <ref type=\"bibr\" target=\"#b20\">[21]</ref> The main components of the global router NTHU-Route 2.0 <r bal router NTHU-Route 2.0 <ref type=\"bibr\" target=\"#b18\">[19]</ref> and layer assignment method NVM <ref type=\"bibr\" target=\"#b20\">[21]</ref> are shown in Table <ref type=\"table\" target=\"#tab_0\">I</re egion congestion, and the net assignment order was ignored according to the experimental results in <ref type=\"bibr\" target=\"#b20\">[21]</ref>. The Negotiation Region denotes a local area where the Neg r\" target=\"#b18\">[19]</ref> ripped-up and rerouted nets in an adaptive bounding box, and Liu and Li <ref type=\"bibr\" target=\"#b20\">[21]</ref> found the minimum-cost global tree without changing the tw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eight standard cell legalization problem, Tetris <ref type=\"bibr\" target=\"#b1\">[2]</ref> and Abacus <ref type=\"bibr\" target=\"#b2\">[3]</ref> are the two most popular methods. Abacus preserves the relat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gnment constraints. P/G alignment constraints are due to the popularity of mixed-cell-height design <ref type=\"bibr\" target=\"#b11\">[12]</ref>. For cells whose height is an odd multiple of the row heig nd technology constraints with the state-of-theart methods <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>. It should be noted that the results of <ref type=\"bibr\" t b11\">[12]</ref>. It should be noted that the results of <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b11\">[12]</ref> are quoted from the publication directly, and the binary o quoted from the publication directly, and the binary of <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b11\">[12]</ref> was executed with eight threads and single thread, respect , 6% smaller maximum displacement, and 31.8% smaller score. Compared with the state-of-the-art work <ref type=\"bibr\" target=\"#b11\">[12]</ref>, our algorithm achieves a 21.6% smaller average displaceme rithm, we compare the NBLG with two state-of-the-art works <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref> based on a new set of benchmarks. The benchmarks were modi /ref> shows the comparison results among our legalizer, <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The second, third, and fourth columns show the characteri le\" target=\"#tab_6\">VIII</ref>, compared with the works <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the NBLG with four threads achieved 10% and 6% smaller to ll cells. When the number of overflows is greater than zero, the history cost will be updated using <ref type=\"bibr\" target=\"#b11\">(12)</ref>, and cells will be sorted by Negotiation Order; the legali. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  subregions, we deal with each fence region in turn and store each rectangular subregion in R-trees <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The cell assigned to the current fence region is then che. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d it has been adopted in many excellent academic routers <ref type=\"bibr\" target=\"#b16\">[17]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref>. The negotiation-based method combined with a rip-up and r ><p>2) Negotiation cost.</p><p>3) Negotiation action. 4) Negotiation order. 5) Negotiation region.  <ref type=\"bibr\" target=\"#b18\">[19]</ref> AND NVM <ref type=\"bibr\" target=\"#b20\">[21]</ref> The main M <ref type=\"bibr\" target=\"#b20\">[21]</ref> The main components of the global router NTHU-Route 2.0 <ref type=\"bibr\" target=\"#b18\">[19]</ref> and layer assignment method NVM <ref type=\"bibr\" target=\"#  and sink. The Negotiation Cost is composed of the target cost and congestion cost. For example, in <ref type=\"bibr\" target=\"#b18\">[19]</ref>, the target cost consisted of the wirelength cost and via  iation Order, it will affect the algorithm speed and solution quality, to some extent. Chang et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed a sorting method based on net and region congesti ion denotes a local area where the Negotiation Object performs the Negotiation Action. Chang et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> ripped-up and rerouted nets in an adaptive bounding box, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: em is efficiently solved using a modulus-based matrix splitting iteration method (MMSIM). Li et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> considered both the average and the maximum cell movement, nology Constraints</head><p>We compared our legalization algorithm with the state-ofthe-art methods <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>. All three algo placement site width, and \"CPU (s)\" denotes the CPU runtime. It should be noted that the results of <ref type=\"bibr\" target=\"#b12\">[13]</ref> are quoted from the publication directly because the binar ]</ref> are quoted from the publication directly because the binary had been altered. Compared with <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which simultaneously minimized the average and the maximu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rval), and state-of-the-art frameworks \u00a7 Sihao Liu and Jian Weng are co-first authors. like AutoDSE <ref type=\"bibr\" target=\"#b21\">[21]</ref> explore these parameters on behalf of the programmer. Whil s competitive performance across many domains compared to the stateof-the-art HLS framework AutoDSE <ref type=\"bibr\" target=\"#b21\">[21]</ref>. Across workload suites of DSP, Machsuite, and Vitis Visio ads by automatically reasoning about the cross-workload flexibility.   and cutting edge DSE-for-HLS <ref type=\"bibr\" target=\"#b21\">[21]</ref>.</p><p>Paper Organization: Section II gives background on  DSE time and device reprogram time. We compare against the state-of-the-art HLS technology, AutoDSE <ref type=\"bibr\" target=\"#b21\">[21]</ref>, as our baseline by using Merlin Compiler (2020.3) and Xil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref>), and have garner. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: icore with scalable networks (e.g. Heracles <ref type=\"bibr\" target=\"#b23\">[23]</ref>, Kumar et al. <ref type=\"bibr\" target=\"#b91\">[91]</ref>), vector operations (e.g. SIMD-Octavo <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  FPGA overlays map a coarser grain architecture (e.g. CPUs <ref type=\"bibr\" target=\"#b23\">[23]</ref><ref type=\"bibr\" target=\"#b24\">[24]</ref><ref type=\"bibr\" target=\"#b25\">[25]</ref><ref type=\"bibr\" t b88\">[88]</ref>, FPGA-Nehalem <ref type=\"bibr\" target=\"#b26\">[26]</ref>), multi-thread (e.g. Octavo <ref type=\"bibr\" target=\"#b24\">[24]</ref>, CUSTARD <ref type=\"bibr\" target=\"#b89\">[89]</ref>, MT-MB . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: regions of the FPGA, and enable linking through a packetswitched network. The RapidStream framework <ref type=\"bibr\" target=\"#b117\">[117]</ref><ref type=\"bibr\" target=\"#b118\">[118]</ref><ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trol flow to predication based dataflow execution. A more general dataflow control flow model (e.g. <ref type=\"bibr\" target=\"#b74\">[74,</ref><ref type=\"bibr\" target=\"#b75\">75]</ref>) is future work. M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: am engine in the stream dispatch queue. 3 The stream dispatch queue uses a basic Tomasulo algorithm <ref type=\"bibr\" target=\"#b64\">[64]</ref> at stream synchronization to see whether its required reso. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olling to take advantage. This can be improved by integrating prior work on reuse distance analysis <ref type=\"bibr\" target=\"#b76\">[76]</ref>. Also, our reuse analysis relies on strong assumptions on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  FPGA overlays map a coarser grain architecture (e.g. CPUs <ref type=\"bibr\" target=\"#b23\">[23]</ref><ref type=\"bibr\" target=\"#b24\">[24]</ref><ref type=\"bibr\" target=\"#b25\">[25]</ref><ref type=\"bibr\" t b88\">[88]</ref>, FPGA-Nehalem <ref type=\"bibr\" target=\"#b26\">[26]</ref>), multi-thread (e.g. Octavo <ref type=\"bibr\" target=\"#b24\">[24]</ref>, CUSTARD <ref type=\"bibr\" target=\"#b89\">[89]</ref>, MT-MB . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: than in-order scheduling that has limited issue capability <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Albeit superior performance, it has been claimed that ple energy-efficient in-order IQs, expecting some degree of dynamic scheduling effects between them <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t because every ready-at-dispatch instruction should allocate a separate P-IQ. The latter one, CASINO <ref type=\"bibr\" target=\"#b1\">[2]</ref>, proposed a simple and effective filtering mechanism that im ursuing energy-efficient dynamic scheduling: CES <ref type=\"bibr\" target=\"#b2\">[3]</ref> and CASINO <ref type=\"bibr\" target=\"#b1\">[2]</ref>. To this end, we present our key observation that the two sc esign concepts and scheduling performance of CES <ref type=\"bibr\" target=\"#b2\">[3]</ref> and CASINO <ref type=\"bibr\" target=\"#b1\">[2]</ref>, compared to baseline designs matrices <ref type=\"bibr\" targ  no free entries in the target P-IQs in which its producers wait for issue.</p><p>2) CASINO: CASINO <ref type=\"bibr\" target=\"#b1\">[2]</ref> is built on a stall-on-use inorder core by leveraging the ob caused by a large amount of ready-at-dispatch instructions <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. This is an inher g window. However, various shapes and numbers of in-flight DCs would be the sources of inefficiency <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>. A DC longer than ) and an out-of-order core (OoO). \u2022 Prior work: CES <ref type=\"bibr\" target=\"#b2\">[3]</ref>, CASINO <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and a front-end execution architecture (FXA) <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: either pushed back to the out-of-order IQ to be scheduled with minimal wakeup and select operations <ref type=\"bibr\" target=\"#b46\">[46]</ref>, <ref type=\"bibr\" target=\"#b47\">[47]</ref>, or directly is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hich is further exacerbated as the scheduling window grows <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t tion (i.e., random queue), the select logic implemented by prefix-sum circuits, and the payload RAM <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\"  and thus assigning higher issue priority to older instructions usually provides better performance <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" ng that it does not lengthen the clock cycle time. This result infers that, as already discussed in <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the age-based select policy would provide no benefit in som play phases. Some recent work leverages both criticality and repetitiveness of dynamic instructions <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b55\">[55]</ref>. Ando proposed to ritize the issue of instructions in unconfident branch slices to reduce the mis-speculation penalty <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Some of the out-of-order IQ entries are reserved for such s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  considering the growing cost and complexity of circuitry <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>CASINO exhibits remarkably different behavior. Firs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hich is further exacerbated as the scheduling window grows <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t tion (i.e., random queue), the select logic implemented by prefix-sum circuits, and the payload RAM <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\"  and thus assigning higher issue priority to older instructions usually provides better performance <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" ng that it does not lengthen the clock cycle time. This result infers that, as already discussed in <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the age-based select policy would provide no benefit in som play phases. Some recent work leverages both criticality and repetitiveness of dynamic instructions <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b55\">[55]</ref>. Ando proposed to ritize the issue of instructions in unconfident branch slices to reduce the mis-speculation penalty <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Some of the out-of-order IQ entries are reserved for such s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing some degree of dynamic scheduling effects between them <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" t new class of cores aiming at complexity-effective dynamic scheduling on a stall-onuse in-order core <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" t ibr\" target=\"#b9\">[10]</ref>. These cores primarily focus on exploiting MLP by extracting (backward <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref> or forward <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ER processors <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>. On the other h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: up phase of 300 million instructions. Energy consumption is estimated via modified version of McPAT <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref> at the 22 nm pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y consumption is estimated via modified version of McPAT <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b42\">[43]</ref> at the 22 nm process technology. We also incorporate the m d structures (e.g., MDP) and control logic (e.g., steer logic) on top of an MR2 model introduced in <ref type=\"bibr\" target=\"#b42\">[43]</ref>.</p><p>The microarchitectural parameters and scheduling wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ER processors <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>. On the other h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ificant complexity and energy overhead, which is further exacerbated as the scheduling window grows <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" t ing point for reconstructing out-oforder IQ. The former one, complexity-effective superscalar (CES) <ref type=\"bibr\" target=\"#b2\">[3]</ref>, paved the way for concurrently tracking multiple data depen  in-depth analysis on two core microarchitectures pursuing energy-efficient dynamic scheduling: CES <ref type=\"bibr\" target=\"#b2\">[3]</ref> and CASINO <ref type=\"bibr\" target=\"#b1\">[2]</ref>. To this  dependence chain (DC) to refer to a sequence of instructions along the R-dependences (solid arrows) <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Within a DC, each instruction is allowed to have up to one  ef>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> and select logic that consists of a tree of arbiters <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref> -have also been p  path of the IQ and cannot be pipelined to support the back-to-back issue of dependent instructions <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>Note that, eduling window Figure <ref type=\"figure\">3</ref>: Design concepts and scheduling performance of CES <ref type=\"bibr\" target=\"#b2\">[3]</ref> and CASINO <ref type=\"bibr\" target=\"#b1\">[2]</ref>, compared head>B. Energy-Efficient Dynamic Scheduling</head><p>1) Complexity-Effective Superscalar (CES): CES <ref type=\"bibr\" target=\"#b2\">[3]</ref> is a dependence-based microarchitecture that performs dynami t it has a number of P-IQs sufficient to accommodate all the in-flight DCs. A heuristic proposed in <ref type=\"bibr\" target=\"#b2\">[3]</ref> steers an instruction to a new (empty) P-IQ in three cases:  this section, we assume a 8-wide issue machine using eight in-order IQs following the convention in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Section VI-E3 discusses the performance impact of different e operands whose producers are in the P-IQs, one is selected by their relative order (yellow muxes) <ref type=\"bibr\" target=\"#b2\">[3]</ref>. If both M/R-dependences are detected,  An instruction is st  groups:</p><p>\u2022 Baseline: An in-order core (InO) and an out-of-order core (OoO). \u2022 Prior work: CES <ref type=\"bibr\" target=\"#b2\">[3]</ref>, CASINO <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and a front ><label>4</label><figDesc>Figure4: Breakdown of instruction steering results in CES with eight P-IQs<ref type=\"bibr\" target=\"#b2\">[3]</ref>. From left to right, speedup over in-order core degrades due. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e.g., a cacheline, or even fuse non-consecutive and/or asymmetric (different width) memory accesses <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Consequently,  , as we will show in Section IV-B, this is not always the case, which is likely why Kim and Lipasti <ref type=\"bibr\" target=\"#b16\">[17]</ref> considered fusing non consecutive memory accesses but only and contiguous (for memory) instructions.</p><p>The works closest to this paper are Kim and Lipasti <ref type=\"bibr\" target=\"#b16\">[17]</ref> and Thakker et al. <ref type=\"bibr\" target=\"#b28\">[29]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b8\">[9]</ref> and with respect to other stores while respecting sequential semantics <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>5) Memory \u00b5-ops with Different Base Registers: As p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Model</head><p>Intel Icelake Predictors L-TAGE <ref type=\"bibr\" target=\"#b24\">[25]</ref>, Store-set <ref type=\"bibr\">[</ref> Decode stages to ensur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Model</head><p>Intel Icelake Predictors L-TAGE <ref type=\"bibr\" target=\"#b24\">[25]</ref>, Store-set <ref type=\"bibr\">[</ref> Decode stages to ensur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Fusion. Kim and Lipasti introduce macro-op scheduling for the purpose of simplifying the IQ logic <ref type=\"bibr\" target=\"#b15\">[16]</ref>. In this work, candidate pairs of \u00b5-ops are scheduled as a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l predictor bitcount is therefore 72Kbits (9KB). Alternatively, other predictors, such as TAGEbased <ref type=\"bibr\" target=\"#b26\">[27]</ref> or local history based <ref type=\"bibr\" target=\"#b31\">[32] the gap could undoubtedly be bridged by tuning the fusion predictor or considering other algorithms <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Other configurations discussed in the paper achieve 0.8% . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cache bandwidth <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, but previous work has so far considered it to avoid acces  one go by reading the whole cacheline and letting entries of the FIFO match against returning data <ref type=\"bibr\" target=\"#b30\">[31]</ref>. Rivers et al. follow a similar -in spiritpath in the purs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ugmented with a single highly ported line buffer from which multiple loads can be served each cycle <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Similarly, Baoni et al. introduce Fat loads <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dictions and updates in the same cycle if they go to different banks, as described by Seznec et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref>.</p><p>Once a distance is retrieved from the FP at Decode,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l predictor bitcount is therefore 72Kbits (9KB). Alternatively, other predictors, such as TAGEbased <ref type=\"bibr\" target=\"#b26\">[27]</ref> or local history based <ref type=\"bibr\" target=\"#b31\">[32] the gap could undoubtedly be bridged by tuning the fusion predictor or considering other algorithms <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Other configurations discussed in the paper achieve 0.8% . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eate more loadstore and load-load forwarding opportunities <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and coalescing non-consecutive stores after they commit w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b48\">[50]</ref>. Exceptions are the multi-lookahead offset prefetching (MLOP) <ref type=\"bibr\" target=\"#b46\">[48]</ref> and the instruction pointer classifier-based prefetching ( ry hierarchy for state-of-the-art prefetchers (IPCP <ref type=\"bibr\" target=\"#b38\">[40]</ref>, MLOP <ref type=\"bibr\" target=\"#b46\">[48]</ref>, SPP-PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref>, and Bi roach is different from BOP and other offset prefetchers <ref type=\"bibr\" target=\"#b27\">[29]</ref>, <ref type=\"bibr\" target=\"#b46\">[48]</ref>. Our key observation is that the best delta for access is  nt while selecting the best offset per application phase. Multi-lookahead offset prefetching (MLOP) <ref type=\"bibr\" target=\"#b46\">[48]</ref> is an extension on BOP that is motivated by Jain's Ph.D. t \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b38\">[40]</ref>, <ref type=\"bibr\" target=\"#b46\">[48]</ref> are also coded and evaluated on ChampSim. The recently mod irst compare  <ref type=\"bibr\" target=\"#b11\">[13]</ref> 2 KB region, 64/128/4K-entry FT/AT/PHT MLOP <ref type=\"bibr\" target=\"#b46\">[48]</ref> 128-entry AMT, 500-update, 16-degree IPCP <ref type=\"bibr\" ing at the L2), and then with multi-level prefetching combinations. The L1D prefetchers are i) MLOP <ref type=\"bibr\" target=\"#b46\">[48]</ref> (DPC-3, 3rd place), an extension of the BOP (DPC-2 winner) \">[35]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[40]</ref>, <ref type=\"bibr\" target=\"#b46\">[48]</ref>. In this Section we compare other relevant prefetching tec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b46\">[48]</ref> and the instruction pointer classifier-based prefetching (IPCP) <ref type=\"bibr\" target=\"#b38\">[40]</ref>, which are L1D prefetchers. It is well known that an L1D p uracy and dynamic energy consumption of the memory hierarchy for state-of-the-art prefetchers (IPCP <ref type=\"bibr\" target=\"#b38\">[40]</ref>, MLOP <ref type=\"bibr\" target=\"#b46\">[48]</ref>, SPP-PPF < \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b38\">[40]</ref>. However, as shown in Figure <ref type=\"figure\">1</ref>(a) g (IPCP). The winner of DPC-3 is a state-of-the-art L1D data prefetcher that is composite in nature <ref type=\"bibr\" target=\"#b38\">[40]</ref>. It classifies an IP into three classes: constant stride ( ing proposals <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b38\">[40]</ref>, <ref type=\"bibr\" target=\"#b46\">[48]</ref> are also coded   FT/AT/PHT MLOP <ref type=\"bibr\" target=\"#b46\">[48]</ref> 128-entry AMT, 500-update, 16-degree IPCP <ref type=\"bibr\" target=\"#b38\">[40]</ref> 128-entry IP its performance with prefetchers designed for d place), an extension of the BOP (DPC-2 winner), and ii) IPCP (DPC-3 winner published at ISCA 2020 <ref type=\"bibr\" target=\"#b38\">[40]</ref>). For multi-level prefetching, we evaluate two state-of-th \">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[40]</ref>, <ref type=\"bibr\" target=\"#b46\">[48]</ref>. In this Sectio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, <ref type=\"bibr\" target=\"#b48\">[50]</ref>. Exceptions are  should be used for prefetching?\" The best offset prefetcher (BOP) inspires us to ask this question <ref type=\"bibr\" target=\"#b36\">[38]</ref>. However, our approach is different from BOP and other off  prefetcher that finds an offset that provides the maximum likelihood of future use at the L2 cache <ref type=\"bibr\" target=\"#b36\">[38]</ref>. An offset of k means that a cache line is k cache lines a \">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, <ref type=\"bibr\" target=\"#b38\">[40]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: croarchitecture research, and ML techniques for data prefetching have been proposed in recent years <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b23\">[25]</ref>, <ref type=\"bib owever, ML techniques have the potential to learn highly complex memory access patterns, and Pythia <ref type=\"bibr\" target=\"#b13\">[15]</ref> shows that with a high performing L2 prefetcher. Berti is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is one of the first proposals that propose a correlation between an IP and delta sequences. DSPatch <ref type=\"bibr\" target=\"#b14\">[16]</ref> tunes a hardware prefetcher based on available DRAM bandwi ers and throttling mechanisms. Similar to PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref> and DSPatch <ref type=\"bibr\" target=\"#b14\">[16]</ref>, there are proposals that control the aggressiveness of pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th only 2.55 KB of storage overhead, Berti improves performance by 8.5% over IP-stride 0 5 10 15 20 <ref type=\"bibr\" target=\"#b23\">25</ref>  The Berti+SPP-PPF multi-level prefetcher obtains the highes  for data prefetching have been proposed in recent years <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b23\">[25]</ref>, <ref type=\"bibr\" target=\"#b49\">[51]</ref>. In ISCA 2021, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tching are in the pursuit of improving the storage overhead without affecting the prefetch coverage <ref type=\"bibr\" target=\"#b56\">[58]</ref>, <ref type=\"bibr\" target=\"#b57\">[59]</ref>. Berti, on the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed as input for the GAP benchmarks.</p><p>We also report performance for the CloudSuite benchmarks <ref type=\"bibr\" target=\"#b20\">[22]</ref>. All traces are publicly available [4], <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nal contextual information is not easy to propagate to L2 and LLC, such as instruction pointer (IP) <ref type=\"bibr\" target=\"#b34\">[36]</ref>, which is usually available at the L1D (e.g., Intel's IP-s p><p>Kill the program counter (KPC) proposes a holistic cache replacement and prefetching framework <ref type=\"bibr\" target=\"#b34\">[36]</ref>. However, the prefetching technique is similar to SPP, wit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> and stream <ref type=\"bibr\" target=\"#b22\">[24]</ref>, <ref type=\"bibr\" target=\"#b26\">[28]</ref>, <ref type=\"bibr\" target=\"#b51\">[53]</ref> are already deployed on commercial processors. Timely Stri -the-art L1D and L2 prefetching techniques. Spatial prefetchers like Spatial Memory Streaming (SMS) <ref type=\"bibr\" target=\"#b51\">[53]</ref> (similar to Bingo) usually learn single repeating deltas o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el cache (LLC).</p><p>Most of the recently proposed storage-efficient spatial prefetchers target L2 <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bib  type=\"bibr\" target=\"#b46\">[48]</ref>, SPP-PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref>, and Bingo <ref type=\"bibr\" target=\"#b11\">[13]</ref>) averaged across single-threaded traces from memory-intens  prefetchers push the limit of singlethread performance with average performance boosts of 3% to 5% <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bib ng events (such as IP, IP+offset, and memory region) and selecting the best pattern for prefetching <ref type=\"bibr\" target=\"#b11\">[13]</ref>. A key point of Bingo is the use of only one hardware tabl et=\"#b1\">[2]</ref> and DPC-3 <ref type=\"bibr\" target=\"#b5\">[7]</ref>). Recent prefetching proposals <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bib i with high performing L1D and L1D+L2 prefetchers. As Berti is an L1D prefetcher, we first compare  <ref type=\"bibr\" target=\"#b11\">[13]</ref> 2 KB region, 64/128/4K-entry FT/AT/PHT MLOP <ref type=\"bib -PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref> and Bingo <ref type=\"bibr\" target=\"#b11\">[13]</ref>. We also compare with a multi-level IPCP that uses IPCP bo ection IV we presented quantitative comparison of Berti with recent hardware prefetching techniques <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: proposed storage-efficient spatial prefetchers target L2 <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bib  <ref type=\"bibr\" target=\"#b38\">[40]</ref>, MLOP <ref type=\"bibr\" target=\"#b46\">[48]</ref>, SPP-PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref>, and Bingo <ref type=\"bibr\" target=\"#b11\">[13]</ref>) aver  performance with average performance boosts of 3% to 5% <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bib ilter that further improves the effectiveness of SPP by deciding whether to prefetch into L2 or not <ref type=\"bibr\" target=\"#b15\">[17]</ref>. In general, SPP combined with PPF (SPP-PPF) provides bett r\" target=\"#b5\">[7]</ref>). Recent prefetching proposals <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b38\">[40]</ref>, <ref type=\"bib hing, we evaluate two state-of-the-art L2 prefetchers along with MLOP and Berti at the L1D: SPP-PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref> and Bingo <ref  son of Berti with recent hardware prefetching techniques <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bib vement with Pythia (less than 1%).</p><p>Prefetch filters and throttling mechanisms. Similar to PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref> and DSPatch <ref type=\"bibr\" target=\"#b14\">[16]</ref>, the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ide <ref type=\"bibr\" target=\"#b18\">[20]</ref> and stream <ref type=\"bibr\" target=\"#b22\">[24]</ref>, <ref type=\"bibr\" target=\"#b26\">[28]</ref>, <ref type=\"bibr\" target=\"#b51\">[53]</ref> are already dep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ers target L2 <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, <ref type=\"bib s of 3% to 5% <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b38\">[40]</ref>. However, as sh refetching (SPP). This state-of-the-art delta prefetcher predicts irregular strides at the L2 cache <ref type=\"bibr\" target=\"#b33\">[35]</ref>. SPP works by relying on the signatures (hashes of consecu refetchers along with MLOP and Berti at the L1D: SPP-PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref> and Bingo <ref type=\"bibr\" target=\"#b11\">[13]</ref>. We al ng techniques <ref type=\"bibr\" target=\"#b11\">[13]</ref>, <ref type=\"bibr\" target=\"#b15\">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r GAP workloads, which results in sub-optimal performance and additional dynamic energy consumption <ref type=\"bibr\" target=\"#b32\">[34]</ref>. <ref type=\"bibr\">Figure 1(b)</ref> shows that state-ofthe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmance improvements as SPP. Multi-level adaptive prefetching based on performance gradient tracking <ref type=\"bibr\" target=\"#b42\">[44]</ref> (3rd place in DPC-1 <ref type=\"bibr\" target=\"#b0\">[1]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fer (MISB) prefetcher <ref type=\"bibr\" target=\"#b57\">[59]</ref>, a storage efficient version of ISB <ref type=\"bibr\" target=\"#b28\">[30]</ref> at L2 with MLOP, IPCP, and Berti at L1D, as shown in Figur t the deltas) <ref type=\"bibr\" target=\"#b10\">[12]</ref>, <ref type=\"bibr\" target=\"#b25\">[27]</ref>, <ref type=\"bibr\" target=\"#b28\">[30]</ref>, <ref type=\"bibr\" target=\"#b31\">[33]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e argue that prefetching based on global (context-agnostic) deltas results in missing opportunities <ref type=\"bibr\" target=\"#b37\">[39]</ref>.</p><p>We propose Berti, a cost-effective, per-IP best req. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as two read ports and one write port. The latency of this structure is two cycles, based on CACTI-P <ref type=\"bibr\" target=\"#b35\">[37]</ref>. Since prefetching training is out of the critical path of gy consumption of reads and writes to tag and data arrays at each cache level and DRAM with CACTI-P <ref type=\"bibr\" target=\"#b35\">[37]</ref> and Micron DRAM power calculator <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed as input for the GAP benchmarks.</p><p>We also report performance for the CloudSuite benchmarks <ref type=\"bibr\" target=\"#b20\">[22]</ref>. All traces are publicly available [4], <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . The recently modified ChampSim extends the one provided with the DPC-3 with a decoupled front-end <ref type=\"bibr\" target=\"#b43\">[45]</ref> and a detailed memory hierarchy support for address transl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed as input for the GAP benchmarks.</p><p>We also report performance for the CloudSuite benchmarks <ref type=\"bibr\" target=\"#b20\">[22]</ref>. All traces are publicly available [4], <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trolling its prefetch degree and distance, or decides whether to prefetch into the L2 or to the LLC <ref type=\"bibr\" target=\"#b9\">[11]</ref>, <ref type=\"bibr\" target=\"#b19\">[21]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n with a temporal prefetcher</head><p>We simulate managed irregular stream buffer (MISB) prefetcher <ref type=\"bibr\" target=\"#b57\">[59]</ref>, a storage efficient version of ISB <ref type=\"bibr\" targe storage overhead without affecting the prefetch coverage <ref type=\"bibr\" target=\"#b56\">[58]</ref>, <ref type=\"bibr\" target=\"#b57\">[59]</ref>. Berti, on the other hand, incurs a storage overhead of ju. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is one of the first proposals that propose a correlation between an IP and delta sequences. DSPatch <ref type=\"bibr\" target=\"#b14\">[16]</ref> tunes a hardware prefetcher based on available DRAM bandwi ers and throttling mechanisms. Similar to PPF <ref type=\"bibr\" target=\"#b15\">[17]</ref> and DSPatch <ref type=\"bibr\" target=\"#b14\">[16]</ref>, there are proposals that control the aggressiveness of pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[17]</ref>, <ref type=\"bibr\" target=\"#b33\">[35]</ref>, <ref type=\"bibr\" target=\"#b36\">[38]</ref>, <ref type=\"bibr\" target=\"#b48\">[50]</ref>. Exceptions are the multi-lookahead offset prefetching (ML served within an operating system (OS) page to predict the future memory accesses in other OS pages <ref type=\"bibr\" target=\"#b48\">[50]</ref>. One of the key features of VLDP is that it uses multiple . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nal contextual information is not easy to propagate to L2 and LLC, such as instruction pointer (IP) <ref type=\"bibr\" target=\"#b34\">[36]</ref>, which is usually available at the L1D (e.g., Intel's IP-s p><p>Kill the program counter (KPC) proposes a holistic cache replacement and prefetching framework <ref type=\"bibr\" target=\"#b34\">[36]</ref>. However, the prefetching technique is similar to SPP, wit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n with a temporal prefetcher</head><p>We simulate managed irregular stream buffer (MISB) prefetcher <ref type=\"bibr\" target=\"#b57\">[59]</ref>, a storage efficient version of ISB <ref type=\"bibr\" targe storage overhead without affecting the prefetch coverage <ref type=\"bibr\" target=\"#b56\">[58]</ref>, <ref type=\"bibr\" target=\"#b57\">[59]</ref>. Berti, on the other hand, incurs a storage overhead of ju. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  recent years <ref type=\"bibr\" target=\"#b13\">[15]</ref>, <ref type=\"bibr\" target=\"#b23\">[25]</ref>, <ref type=\"bibr\" target=\"#b49\">[51]</ref>. In ISCA 2021, a prefetching competition with ML technique. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the prefetch requests to the memory hierarchy. Our Berti prefetcher is inspired by Berti from DPC-3 <ref type=\"bibr\" target=\"#b44\">[46]</ref>.</p><p>Accurate and timely local deltas. We define local d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[11]</ref>, <ref type=\"bibr\" target=\"#b19\">[21]</ref>, <ref type=\"bibr\" target=\"#b24\">[26]</ref>, <ref type=\"bibr\" target=\"#b39\">[41]</ref>, <ref type=\"bibr\" target=\"#b40\">[42]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[27]</ref>, <ref type=\"bibr\" target=\"#b28\">[30]</ref>, <ref type=\"bibr\" target=\"#b31\">[33]</ref>, <ref type=\"bibr\" target=\"#b50\">[52]</ref>, <ref type=\"bibr\" target=\"#b55\">[57]</ref>. Temporal prefe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Fortunately, we notice that the Eqn. 3 can be modified to incorporate the reparametrization trick <ref type=\"bibr\" target=\"#b15\">[16]</ref> to allow differentiable learning:</p><formula xml:id=\"form  introduce two basic technical lemmas. While such results are already mentioned in previous studies <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, their proofs will. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng the pairwise similarity. The kernel function can be further approximated by random features (RF) <ref type=\"bibr\" target=\"#b25\">[26]</ref>which serves as an unbiased</p><formula xml:id=\"formula_5\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: random transformation. There are many potential choices for \u03d5, e.g., Positive Random Features (PRF) <ref type=\"bibr\" target=\"#b5\">[6]</ref> \u03d5</p><formula xml:id=\"formula_6\">(x) = exp ( \u2212\u2225x\u2225 2 2 2 ) \u221a  rem 1</head><p>To prove our theorem, we first introduce the following lemma given by the Lemma 2 in <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Proposition 1. Denote a softmax kernel as SM(x, y) = exp(x . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ture extrapolation <ref type=\"bibr\" target=\"#b39\">[40]</ref> and cold-start users in recommendation <ref type=\"bibr\" target=\"#b40\">[41]</ref>. NODEFORMER can serve as a plug-in scalable structure lear. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ture extrapolation <ref type=\"bibr\" target=\"#b39\">[40]</ref> and cold-start users in recommendation <ref type=\"bibr\" target=\"#b40\">[41]</ref>. NODEFORMER can serve as a plug-in scalable structure lear. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aph for enhancing the expressiveness. Other studies, e.g., <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> focus on sparsifying input structures to promote robust rep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nexpensive. While this latter scenario has been explored in the context of graph structure learning <ref type=\"bibr\" target=\"#b37\">[38]</ref> and all-pair message passing design, e.g., graph Transform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e source, except Mini-ImageNet, whose features are extracted by ourselves. Following the setting of <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we compute node embeddings via a CNN model with 4 convolu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b10\">[11]</ref>, etc. Moreover, in graph-enhanced applications, e.g., text classification <ref type=\"bibr\" target=\"#b44\">[45]</ref>, vision navigation <ref type=\"bibr\" target=\"#b11\">[12]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng community, like explainability <ref type=\"bibr\" target=\"#b45\">[46]</ref>, adversarial robustness <ref type=\"bibr\" target=\"#b48\">[49]</ref>, training acceleration <ref type=\"bibr\" target=\"#b32\">[33]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: random transformation. There are many potential choices for \u03d5, e.g., Positive Random Features (PRF) <ref type=\"bibr\" target=\"#b5\">[6]</ref> \u03d5</p><formula xml:id=\"formula_6\">(x) = exp ( \u2212\u2225x\u2225 2 2 2 ) \u221a  rem 1</head><p>To prove our theorem, we first introduce the following lemma given by the Lemma 2 in <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Proposition 1. Denote a softmax kernel as SM(x, y) = exp(x . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n on Mini-ImageNet and 20News-Groups datasets, without input graphs. The instances of Mini-ImageNet <ref type=\"bibr\" target=\"#b36\">[37]</ref>  Impact of Temperature and Feature Map Dimension. We study  without graph structure: 20News-Groups <ref type=\"bibr\" target=\"#b23\">[24]</ref> and Mini-ImageNet <ref type=\"bibr\" target=\"#b36\">[37]</ref>. The 20News dataset is a collection of approximately 20,00. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b35\">[36]</ref> as an early attempt, there are many follow-up works, e.g., <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, considering weigh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> have shown promising power for leveraging such data depende sive GNNs is a fundamental problem in learning over graph data. With Graph Attention Networks (GAT) <ref type=\"bibr\" target=\"#b35\">[36]</ref> as an early attempt, there are many follow-up works, e.g., ><p>As baseline models, we basically consider GCN <ref type=\"bibr\" target=\"#b18\">[19]</ref> and GAT <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Besides, we compare with some advanced GNN models, includ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . There are also quite a few approaches that propose scalable GNNs through, e.g., subgraph sampling <ref type=\"bibr\" target=\"#b46\">[47]</ref>, linear feature mapping <ref type=\"bibr\" target=\"#b38\">[39  a linear model SGC <ref type=\"bibr\" target=\"#b38\">[39]</ref> and a graph-sampling model GraphSAINT <ref type=\"bibr\" target=\"#b46\">[47]</ref>. More detailed information about these models are presente. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een node pairs, e.g., Gaussian kernels <ref type=\"bibr\" target=\"#b42\">[43]</ref>, cosine similarity <ref type=\"bibr\" target=\"#b3\">[4]</ref>, attention networks <ref type=\"bibr\" target=\"#b16\">[17]</ref  GLCN <ref type=\"bibr\" target=\"#b16\">[17]</ref> Function Fixed Not necessary Yes O(N 2 ) 0.02M IDGL <ref type=\"bibr\" target=\"#b3\">[4]</ref> Function approaches <ref type=\"bibr\" target=\"#b9\">[10,</ref> o SOTA graph structure learning methods, LDS-GNN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and IDGL <ref type=\"bibr\" target=\"#b3\">[4]</ref> for comparison. For large-scale datasets, we additionally co he original papers <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Concretely, we use GCN as the backbone for them.</p><p>C.1 D. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hile Amazon2M is extracted from the Amazon Co-Purchasing network that entails long-range dependence <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For OGB-Proteins, we use the protocol of <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cy as relational bias for better expressiveness at some expense of efficiency, as similarly done by <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We summarize the feed-forward computation of NODEFORMER in  with some advanced GNN models, including JKNet <ref type=\"bibr\" target=\"#b43\">[44]</ref> and MixHop <ref type=\"bibr\" target=\"#b0\">[1]</ref>. These GNN models all rely on input graphs. We further consi  1, 2, . . . , K do 4 Gk = {e g ku /\u2327 } N u=1</formula><p>, gku \u21e0 Gumbel(0, 1); 5 Gk = Gk.unsqueeze <ref type=\"bibr\" target=\"#b0\">(1)</ref>.repeat(1, m); l+1) ; % add relational bias</p><formula xml:i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2]</ref>, long-range dependencies <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and graph incompleteness <ref type=\"bibr\" target=\"#b10\">[11]</ref>, etc. Moreover, in graph-enhanced applications, e.g., text ion difficulties, some sophisticated training methods are introduced, such as bi-level optimization <ref type=\"bibr\" target=\"#b10\">[11]</ref>, variational Table <ref type=\"table\">1</ref>: Comparison o odels</head><p>Parameterization Expressivity Input Graphs Inductive Complexity Largest Demo LDS-GNN <ref type=\"bibr\" target=\"#b10\">[11]</ref> Adjacency Fixed Required No O(N 2 ) 0.01M ProGNN <ref type ge <ref type=\"bibr\" target=\"#b26\">[27]</ref> and two SOTA graph structure learning methods, LDS-GNN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and IDGL <ref type=\"bibr\" target=\"#b3\">[4]</ref> for compa er to their implementation provided by the original papers <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Concretely, we use G. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b42\">[43]</ref>, cosine similarity <ref type=\"bibr\" target=\"#b3\">[4]</ref>, attention networks <ref type=\"bibr\" target=\"#b16\">[17]</ref>, non-linear MLP <ref type=\"bibr\" target=\"#b6\">[7]</ref> et 0.02M BGCN <ref type=\"bibr\" target=\"#b49\">[50]</ref> Adjacency Fixed Required No O(N 2 ) 0.02M GLCN <ref type=\"bibr\" target=\"#b16\">[17]</ref> Function Fixed Not necessary Yes O(N 2 ) 0.02M IDGL <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n on Mini-ImageNet and 20News-Groups datasets, without input graphs. The instances of Mini-ImageNet <ref type=\"bibr\" target=\"#b36\">[37]</ref>  Impact of Temperature and Feature Map Dimension. We study  without graph structure: 20News-Groups <ref type=\"bibr\" target=\"#b23\">[24]</ref> and Mini-ImageNet <ref type=\"bibr\" target=\"#b36\">[37]</ref>. The 20News dataset is a collection of approximately 20,00. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as an early attempt, there are many follow-up works, e.g., <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, considering weighting the edges in input graph for enhanci. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntral topic in graph machine learning in the last decade. For example, graph neural networks (GNNs) <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe Recently, graph neural networks (GNNs) have shown enormous success in graph representation learning <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: raphs using different GNNs, including Attention <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Top-k Pool <ref type=\"bibr\" target=\"#b20\">[21]</ref>, SAGPool <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and AS. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Besides, learning M for each graph G separately hinders the method from handling unseen test graphs <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Therefore, we adopt a shared learnable GNN (denoted as GN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref>, demonstrating their strength in various tasks <ref type=\"bibr\" target=\"#b29\">[30]</ref><ref type=\"bibr\" target=\"#b30\">[31]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ethods inapplicable. A few works study OOD generalization on latent environments in computer vision <ref type=\"bibr\" target=\"#b54\">[55,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> or raw feature dat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b15\">[16]</ref>. We also consider a recent interpretable graph learning method GSAT <ref type=\"bibr\" target=\"#b25\">[26]</ref>. For a fair comparison, we use the same GNN backbone as GI ld be violated and expensive to satisfy in practice <ref type=\"bibr\" target=\"#b53\">[54]</ref>. GSAT <ref type=\"bibr\" target=\"#b25\">[26]</ref> applies graph information bottleneck criteria for generali. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns/1.0\"><head>Generalization of GNNs.</head><p>Early works <ref type=\"bibr\" target=\"#b37\">[38]</ref><ref type=\"bibr\" target=\"#b38\">[39]</ref><ref type=\"bibr\" target=\"#b39\">[40]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raphs using different GNNs, including Attention <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Top-k Pool <ref type=\"bibr\" target=\"#b20\">[21]</ref>, SAGPool <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and AS. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: D generalization on latent environments in computer vision <ref type=\"bibr\" target=\"#b54\">[55,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> or raw feature data <ref type=\"bibr\" target=\"#b56\">[57]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fer the latent environments. To verify that GIL can infer the environments accurately, we use t-SNE <ref type=\"bibr\" target=\"#b28\">[29]</ref> to plot the discovered environments on a 2D-plane when the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Such studies are particularly critical for high-stake graph applications such as medical diagnosis <ref type=\"bibr\" target=\"#b4\">[5]</ref>, financial analysis <ref type=\"bibr\" target=\"#b5\">[6]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
