{"content": "The context is: notations, but lack the ability to generalize to other domains. In contrast, embedding-based models <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b;</ref><ref type=\"bibr\" target=\"#b13\">Hao et al., en executing logical queries on incomplete KBs. Our work follows the line of Embedding-based models <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b;</ref><ref type=\"bibr\" target=\"#b10\">Dong et al. ated to these three OOV relations during the test.</p><p>Several baselines are included here: Embed <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b)</ref> deals with factoid QA over KB by matching. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> considering edge attributes <ref type=\"bibr\" target=\"#b14\"> target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uestion answering <ref type=\"bibr\" target=\"#b22\">(Yu et al., 2017)</ref>, knowledge base population <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, and biomedical knowledge discovery <ref t  our group compared (1) and ( <ref type=\"formula\" target=\"#formula_1\">2</ref>) with sequence models <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, and we report these results; for (3) we r TM), and showed that it outperforms several CNN and dependency-based models by a substantial margin <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>. We compare with this strong baseline, and etup</head><p>We conduct experiments on two relation extraction datasets: (1) TACRED: Introduced in <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, TACRED contains over 106k mention pairs d f the two entities.</p><p>More recently, <ref type=\"bibr\" target=\"#b0\">Adel et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b25\">Zhang et al. (2017)</ref> have shown that relatively simple neural mo la_1\">2</ref> For fair comparisons on the TACRED dataset, we follow the evaluation protocol used in <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017</ref>) by selecting the model with the median dev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: order to improve lie detection in criminalsuspect interrogations, Sumriddetchkajorn and Somboonkaew <ref type=\"bibr\" target=\"#b36\">[37]</ref> developed an infrared system to detect lies by using therm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2015) )</ref> and recurrent neural network <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Zhou et al., 2016)</ref>. To automatically obtain a large training da ying relations, we apply an attention mechanism over a BiLSTM Encoder, which is first introduced in <ref type=\"bibr\" target=\"#b23\">(Zhou et al., 2016)</ref> for RC. The model architecture is illustrat g et al., 2015)</ref> is also commonly used for RE with the help of position embeddings. BiLSTM+ATT <ref type=\"bibr\" target=\"#b23\">(Zhou et al., 2016)</ref> adds an attention mechanism into BiLSTM to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, social networks analysis <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, and visual unders. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"2.6.1\">Implementation</head><p>We implement the neural network using the torch7 library <ref type=\"bibr\" target=\"#b7\">(Collobert et al., 2011a)</ref>. Training and inference are done on a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t the US Navy Research and Development Laboratories (NRaD) and the University of Rhode Island (URI) <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The system supports expression and enforcement of dynamic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. This approach made sen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (FDP) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Address Map Pattern Matching Prefetching (AMPM) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. We now describe both of these techniques in some detail.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  large-scale ST dataset, multitask learning <ref type=\"bibr\" target=\"#b32\">(Weiss et al. 2017;</ref><ref type=\"bibr\" target=\"#b6\">B\u00e9rard et al. 2018</ref>) and pretraining techniques <ref type=\"bibr\"  ficantly increases the learning difficulty.</p><p>\u2022 Non-pre-trained Attention Module: Previous work <ref type=\"bibr\" target=\"#b6\">(B\u00e9rard et al. 2018)</ref> trains attention modules for ASR, MT and ST. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 59.5 LSTM <ref type=\"bibr\" target=\"#b42\">(Yang et al., 2018)</ref> VD, WT, 2, AWD, MoC 22 57.6 LSTM <ref type=\"bibr\" target=\"#b32\">(Merity et al., 2017)</ref> VD, WT, 2, AWD 24 57.3 LSTM <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecommendation. Prior efforts, such as ItemRank <ref type=\"bibr\" target=\"#b10\">[11]</ref> and BiRank <ref type=\"bibr\" target=\"#b15\">[16]</ref>, use the label propagation mechanism to directly propagate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation that the control-flow path leading to a branch is correlated with the direction of the branch <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -cache miss sequences directly. Our key observation, inspired by recent studies of data prefetching <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target  prefetching approaches that only retrieve a constant number of blocks in response to a miss (e.g., <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" targ sign is based on recent proposals for addresscorrelated prefetch of recurring temporal data streams <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Bayesian active learning algorithm for deep learning in image data is proposed based on the idea in <ref type=\"bibr\" target=\"#b41\">[42]</ref>.</p><p>Most works that apply active learning to recommende  basic equation of Variational Inference <ref type=\"bibr\" target=\"#b51\">[52]</ref>.</p><p>Following <ref type=\"bibr\" target=\"#b41\">[42]</ref>, we use the distribution of the network parameter with dro nsidered as the smoothed version of hinge loss.</p><p>As for the second term in (2), it's proved in <ref type=\"bibr\" target=\"#b41\">[42]</ref> that it can be approximated by L2 regularization term</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">Rosas et al., 2014)</ref>, which was then used to develop a multimodal deception detection system <ref type=\"bibr\" target=\"#b1\">(Abouelenien et al., 2014)</ref>. An extensive review of approaches fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: able place to look is human cognition <ref type=\"bibr\" target=\"#b7\">(Davis &amp; Marcus, 2015;</ref><ref type=\"bibr\" target=\"#b29\">Lake et al., 2016;</ref><ref type=\"bibr\" target=\"#b42\">Marcus, 2001;<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -sampling and down-sampling layers to provide an error feedback mechanism for each stage. Jo et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> introduced the dynamic upsampling filters for video super- cally generated from the arbitrary customized prior boxes. In the video super resolution, Jo et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a dynamic upsampling filters. The dynamic upsampl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tial Locality.</p><p>Irregular memory accesses can also be prefetched by detecting spatial locality <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta >3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Variations of the Spatial Locality Detection Table <ref type=\"bibr\" target=\"#b21\">[22]</ref> track accesses to different regions of memory so that spat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b13\">[14]</ref> of machine translation and the neural aggregation networks <ref type=\"bibr\" target=\"#b14\">[15]</ref> of video face recognition, we propose the Frame Attention . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ormally, given the reference frame I t and each support frame I t+\u03c4 , Feature Pyramid Network (FPN) <ref type=\"bibr\" target=\"#b55\">[56]</ref> is leveraged to extract multi-scale pyramidal feature maps ad><p>Feature Pyramid Network. FPN is built at the top of ResNet-101 pre-trained on ImageNet. As in <ref type=\"bibr\" target=\"#b55\">[56]</ref>, P3, P4, Algorithm 2 Inference Algorithm of our SSVD </p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef>, attacks of this form have proven difficult to prevent <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as follows.</p><p>\u2022 Flickr dataset. From Flickr, we first downloaded a set of image IDs provided by <ref type=\"bibr\" target=\"#b35\">[28]</ref>. Some images were unavailable, and limiting the number of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion so far have been linear, based on various methods of factorizing the third-order binary tensor <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011;</ref><ref type=\"bibr\" target=\"#b28\">Yang et al.  expressiveness.</p><p>Finally, we show that several previous stateof-the-art linear models, RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref>, DistMult <ref type=\"bibr\" target=\"#b28\"> lated Work</head><p>Several linear models for link prediction have previously been proposed: RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref>  HypER <ref type=\"bibr\" target=\"#b0\">(Bal d><p>Several previous tensor factorization models can be viewed as a special case of TuckER: RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref> Following the notation introduced in Sect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  search and indexing <ref type=\"bibr\" target=\"#b44\">[43]</ref>, personalized content recommendation <ref type=\"bibr\" target=\"#b47\">[46]</ref>, and question answering <ref type=\"bibr\" target=\"#b43\">[42. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, show substantially better effectiveness at the cost of orders of magnitude longer inference time <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta ><p>Recently, the issue of efficiency gained traction in the neural IR community. Hofst\u00e4tter et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> establish efficiency baselines for common neural IR models. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: refetcher, offset prefetchers, and the sandbox method for selecting the prefetch offset dynamically <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Offset prefetching is a generalization of next-line prefe </ref> (this list is not exhaustive).</p><p>Recently, Pugsley et al. introduced Sandbox prefetching <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The Sandbox prefetcher prefetches line X + D when line X  dge, the first published full-fledged offset prefetcher is the Sandbox prefetcher by Pugsley et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref>. However, the offset selection mechanism in the Sandbox pr owledge, the SBP prefetcher of Pugsley et al. is the first published full-fledged offset prefetcher <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The SBP prefetcher is cost-effective and was shown to out  with actual prefetches.</p><p>We implemented the SBP prefetcher as described in the original paper <ref type=\"bibr\" target=\"#b25\">[26]</ref>, but with a few modifications to make the comparison with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b4\">[5]</ref>, however, a spectral relaxation of the problem can be solved efficiently <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Let C \u2208 0, 1 n\u00d7k be the cluster assignment matrix and d b r iteration or Lanczos algorithm.</p><p>One can then obtain clusters by means of spectral bisection <ref type=\"bibr\" target=\"#b36\">[37]</ref> with iterative refinement akin to Kernighan-Lin algorithm . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016)</ref>. However, most existing methods require la \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016)</ref> to define the score of the predicted seque \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Finkel et al. \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Ratinov and  cannot deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTM-CRF <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> into a Fuzzy CRF layer, which allows each kens, such as \"Thus\" and \"by\", are labeled as O.</p><p>Fuzzy-LSTM-CRF. We revise the LSTM-CRF model <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> to the Fuzzy-LSTM-CRF model to support th -Disease datasets, LM-LSTM-CRF <ref type=\"bibr\" target=\"#b13\">(Liu et al., 2018)</ref> and LSTM-CRF <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> achieve the state-of-the-art F 1 scores w cent advances in neural models have freed do-main experts from handcrafting features for NER tasks. <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: allenge to process massive remote sensing images. In our work, two lightweight attention mechanisms <ref type=\"bibr\" target=\"#b16\">[17]</ref> which contains spatial attention and channel attention are ts a convolution operation with 7 \u00d7 7 kernel size.</p><p>In this study, we follow the method of Woo <ref type=\"bibr\" target=\"#b16\">[17]</ref>to integrate the two attention mechanisms. First, we use ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  nodes in GCN are inclined to converge to a certain value and thus become indistinguishable. ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> solves a similar problem in computer vision w he -th weight matrix W ( ) . Initial residual connection. To simulate the skip connection in ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref>, <ref type=\"bibr\" target=\"#b16\">(Kipf &amp; W ations for introducing identity mapping into our model.</p><p>\u2022 Similar to the motivation of ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref>, identity mapping ensures that a deep GCNII m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: exed by nodes of an arbitrary directed or undirected graph <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>. This choice is satisfying in the sense that, when the sig  \u2022 1 is norm 1, and A norm = 1 \u03bbmax A. Other norms could be used to define the total variation, see <ref type=\"bibr\" target=\"#b50\">[51]</ref> <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Using this, graph ustified theoretically that the frequency bases obtained from the shift operator tend to be ordered <ref type=\"bibr\" target=\"#b50\">[51]</ref>.</p><p>Up to this point, we have focused primarily on freq odel makes it possible to detect outliers or abnormal values by highpass filtering and thresholding <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b154\">[155]</ref>, or to build  f type=\"bibr\" target=\"#b4\">[5]</ref>, optimizing the prediction of unknown labels in classification <ref type=\"bibr\" target=\"#b50\">[51]</ref> or semisupervised learning problems <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ied to many ASR tasks <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Unlike Deep Neural Networks (DNNs) <ref type=\"bibr\" target nal fully-connected deep neural networks on many ASR tasks <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, we investigate the effect of convolutional layers in seq2s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \u6ee1\u6c5f\u7ea2), Shuidiaogetou(\u6c34\u8c03\u6b4c\u5934), etc .</p><p>Various methods e.g., <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> have been proposed to generate classical Chinese poetry. Howe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vidually score all slot-value combinations <ref type=\"bibr\" target=\"#b12\">(Mrk\u0161i\u0107 et al. 2017;</ref><ref type=\"bibr\" target=\"#b21\">Zhong, Xiong, and Socher 2018)</ref>. Such approaches are not practic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , and deep contextual language models from BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a vanilla and Siamese architecture <ref type=\"bibr\" tar contextual embeddings as the ones used in BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XL-Net <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The Transformer architecture allowed the efficient unsupe ype=\"bibr\" target=\"#b36\">[37]</ref>, named BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The two Transformer models are originally designed to sol  <ref type=\"bibr\" target=\"#b42\">[43]</ref> alone, XLNet uses additional Web corpora for pretraining <ref type=\"bibr\" target=\"#b40\">[41]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  pected is that BERT generally achieves slightly better results than XLNet. According to Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>, XLNet surpasses BERT on the related GLUE benchmark <ref t  may be attributed to two reasons, pretraining on different corpora, and smaller models compared to <ref type=\"bibr\" target=\"#b40\">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained 0\">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained models used by Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>. Furthermore, the published XLNet BASE model we considered ublished XLNet BASE model we considered is pretrained on different data than the one in Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>  <ref type=\"foot\" target=\"#foot_13\">15</ref> . In contrast. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arthquakes increase the intensity more than small earthquakes. For more on the ETAS model, see e.g. <ref type=\"bibr\" target=\"#b11\">Ogata (1988</ref><ref type=\"bibr\" target=\"#b12\">Ogata ( , 1998))</ref <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Residual analysis</head><p>Residual analysis <ref type=\"bibr\" target=\"#b11\">(Ogata, 1988)</ref> is a type of model checking for point processes s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y information for character-based model. To integrate words information into character-based model, <ref type=\"bibr\" target=\"#b40\">Zhang and Yang (2018)</ref> propose a lattice-structured LSTM model t characterbased model. The character baseline denotes the original character-based BiLSTM-CRF model. <ref type=\"bibr\" target=\"#b40\">Zhang and Yang (2018)</ref> propose a lattice LSTM to exploit word in y information into Chinese NER task. Another way to obtain word boundary information is proposed by <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, using a lattice LSTM to integrate word   where b &lt; i and c b,i matches a word in lexicon D. The lexicon D is the same as the one used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, which is built by using automatically s x c i with x \u2212 \u2192 ws i to utilize word information. And this is quite different from the way used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, since they use extra shortcut paths to  j , y j )}| N j=1 , we minimize the sentence-level negative loglikelihood loss to train the model:  <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>.</p><formula xml:id=\"formula_16\">L = \u2212 j rget=\"#tab_0\">1</ref>. Implementation Details. We utilize the character and word embeddings used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, both of which are pre-trained on Chines ng, 2018)</ref>, both of which are pre-trained on Chinese Giga-Word using word2vec model. Following <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, we use the word embedding dictionary as with other parameters.</p><p>For hyper-parameter configurations, we mostly refer to the settings in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>. We set both character embedding size an > 91.28 90.62 90.95 <ref type=\"bibr\" target=\"#b0\">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>   approach to integrating word informati n Chinese Resume dataset. Consistent with the previous results, our models outperform lattice model <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>. The above experimental results strongly  some comparative experiments on training time and convergence speed. The lattice model proposed in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref> is our principal comparison object, sinc ns/1.0\" type=\"table\" xml:id=\"tab_5\"><head></head><label></label><figDesc>are the most common methods<ref type=\"bibr\" target=\"#b40\">Zhang and Yang, 2018)</ref> 94.81 94.11 94.46 Character baseline 93.2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  as exponential service times, infinite buffers, and so on <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Likewise, analys et=\"#b6\">[7]</ref> and hypercubes <ref type=\"bibr\" target=\"#b22\">[24]</ref>. The study presented in <ref type=\"bibr\" target=\"#b8\">[9]</ref> is not restricted to a particular topology, but it assumes a where T Bus is the service time of the Bus architecture and C Bus is the contention matrix given in <ref type=\"bibr\" target=\"#b8\">(9)</ref>. Finally, we note that when each buffer shown in Fig. <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [2]</ref>). All of the models mentioned above are shallow networks (less than 5 layers). Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> first introduced the residual architecture for training mu ly concatenate together, which leads to the underutilization of local features. In 2016, Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed a residual learning framework (Fig. <ref type=\"fi r module, we design a set of comparative experiments to compare the performance with residual block <ref type=\"bibr\" target=\"#b11\">[12]</ref>, dense block <ref type=\"bibr\" target=\"#b23\">[24]</ref>   < figDesc>Fig. 5. Quantitative comparison of three different feature extraction blocks (residual block<ref type=\"bibr\" target=\"#b11\">[12]</ref>, dense block<ref type=\"bibr\" target=\"#b23\">[24]</ref>, and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: se approach <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2018)</ref> and its layer-dependent variant <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2018)</ref>. Specifically, GAT <ref type=\"bibr\" target on et al., 2017)</ref>, FastGCN <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2018)</ref>, and AS-GCN <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2018)</ref>. We name this category of approaches as Dr e <ref type=\"table\" target=\"#tab_1\">2</ref>; for the SOTA methods, we reuse the results reported in <ref type=\"bibr\" target=\"#b13\">Huang et al. (2018)</ref>.</p><p>We have these findings: (1) Clearly, ing the testing nodes are unseen for training. We apply the full-supervised training fashion used in<ref type=\"bibr\" target=\"#b13\">Huang et al. (2018)</ref> and<ref type=\"bibr\" target=\"#b2\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: two-layer fully connected network, and a two-layer LeNet convoluational neural network architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Both networks are trained with SGD with momentum using the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r, a pointwise non-linearity, and either an invariant or equivariant linear output layer. Recently, <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> showed that by allowing higherorder tensor =\"bibr\" target=\"#b13\">Hornik et al., 1989;</ref><ref type=\"bibr\" target=\"#b20\">Pinkus, 1999)</ref>. <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> recently proved that certain invariant GNN aces of continuous invariant (resp. equivariant) functions.</p><p>2 The case of invariant functions <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> recently proved that invariant GNNs simila n the whole set G inv. , that is, for all numbers of nodes n n max simultaneously. On the contrary, <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> work with a fixed n, and it does not seem   on the order of tensorization k s . Indeed, through Noether's theorem on polynomials, the proof of <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> shows that k s n d (n d \u2212 1)/2 is sufficie  previous invariant case could be easily extended to invariance to subgroups of O n , as is done by <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref>, for the equivariant case our theorem only ions in the rest of the introduction, in Section 2 we provide an alternative proof of the result of <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref> for invariant GNNs (Theorem 1), which wil t. Theorem 1. For any \u03c1 \u2208 F MLP , N inv. (\u03c1) is dense in C(G inv. , d edit ).</p><p>Comparison with <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <re th <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. The two proofs are however different: th See the next subsection for details.</p><p>One improvement of our result with respect to the one of <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref> is that it can handle graphs of varying s bibr\" target=\"#b0\">(Battaglia et al., 2016)</ref>. Another outstanding open question, formulated in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>, is the characterization of the approxima ing a single hidden layer of such equivariant operators followed by an invariant layer is proved in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b</ref>) (see also <ref type=\"bibr\">(Kondor et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d was \"set deliberately low to account for inaccuracies in bounding boxes in the ground truth data\" <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Does COCO have better labelling than VOC? This is definitel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  superresolution of arbitrary scale factor. As for the state-of-theart SISR methods, such as ESPCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref>, EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>, RDN <ref  the same size as the final highresolution image, these methods are time-consuming.</p><p>Shi et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> firstly proposed a real-time superresolution algorithm ESP real-time superresolution algorithm ESPCNN by proposing the sub-pixel convolution layer. The ESPCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> upscaled the image at the end of the network to reduce the applied to these networks by simply replacing the traditional upscale module (sub-pixel convolution <ref type=\"bibr\" target=\"#b21\">[22]</ref>). We choose the state-of-the-art SISR network, called resi etwork, the Feature Learning Module, and the Meta-Upscale Module. Most the state-of-the-art methods <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  based merely on implicit feedbacks, i.e., user clicks, in the current session.</p><p>Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> apply recurrent neural networks (RNN) with Gated Recurrent al Networks (RNN) have been devised to model variable-length sequence data. Recently, Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> apply RNN to sessionbased recommendation and achieve signi ARM. We use a RNN with Gated Recurrent Units (GRU) rather than a standard RNN because Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> demonstrate that GRU can outperform the Long Short-Term Me re S i , To learn the parameters of the model, we do not utilize the proposed training procedure in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, where the model is trained in a session-parallel, sequenc sessions of subsequent week for testing. Because we did not train NARM in a session-parallel manner <ref type=\"bibr\" target=\"#b11\">[12]</ref>, a </p><formula xml:id=\"formula_15\">],V (x 2 ), ([x 1 , x  nt representations when computing recommendation scores. \u2022 GRU-Rec: We denote the model proposed in <ref type=\"bibr\" target=\"#b11\">[12]</ref> as GRU-Rec, which utilizes session-parallel mini-batch tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ious human faces, we apply Laplacian mesh editing to morph a canonical mesh into the predicted mesh <ref type=\"bibr\" target=\"#b2\">[3]</ref>. This lets us use the blend shape coefficients for different. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and task dependency <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Some works hav. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  conditioned on the categorical priors. Both DBPN <ref type=\"bibr\" target=\"#b9\">[10]</ref> and DSRN <ref type=\"bibr\" target=\"#b8\">[9]</ref> made use of the mutual dependencies of low-and high-resoluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed in the multi-modal domain, on multimodal representations <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b35\">[35]</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b20\">[21]</ref>. Recently, with the great impact of neural networks on computer vision <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and natural langua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ificant problems such as the growing impact of wire delays <ref type=\"bibr\" target=\"#b1\">[2]</ref>  <ref type=\"bibr\" target=\"#b12\">[13]</ref> and increasing complexity of some parts such as the issue . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b7\">6,</ref><ref type=\"bibr\" target=\"#b16\">15,</ref><ref type=\"bibr\" target=\"#b21\">20,</ref><ref type=\"bibr\" target=\"#b29\">28,</ref><ref type=\"bibr\" target=\"#b48\">47]</ref>. Such patterns have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: timization <ref type=\"bibr\" target=\"#b3\">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type=\"bibr\" target=\"#b11\">(Finn et al., 2017)</ref>. In essence, we turn the gradient-based opt </ref> or initial weights that enable rapid adaptation to new tasks or domains in few-shot learning <ref type=\"bibr\" target=\"#b11\">(Finn et al., 2017)</ref>.</p><p>Meta-gradients (e.g., gradients w.r. s is expensive both from a computational and a memory point-of-view.</p><p>To alleviate this issue, <ref type=\"bibr\" target=\"#b11\">Finn et al. (2017)</ref> propose a first-order approximation, leading thus more knowledge than all competing methods. First-order refers to the approximation proposed by <ref type=\"bibr\" target=\"#b11\">Finn et al. (2017)</ref>, i.e. ignoring all second-order derivatives.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b30\">[31]</ref> to capture all these information over a long period. Yao et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> at-tempt to dynamically select multiple visual representat ssification. The extracted features {v i } n i=1 form the original video representation. Similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref>, temporal soft-attention Attend is used to select visual i tion Attend is used to select visual information most related to each word. But very different from <ref type=\"bibr\" target=\"#b36\">[37]</ref> using the hidden states from a LSTM decoder, we guide the  ce generator and a paragraph generator. To emphasize the mapping from video to sentence, Yao et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> propose a temporal attention model to align the most relev l></formula><p>where W r , U \u03b1 , b \u03b1 , and w are the parameters to be learned.</p><p>Different from <ref type=\"bibr\" target=\"#b36\">[37]</ref>, here we incorporate the content read from multimodal memo o add masks to both sentences and visual features for the convenience of batch training. Similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref>, the sentences with length larger than 30 in MSVD and the  #b25\">[26]</ref>, respectively. Table <ref type=\"table\">3</ref>. The performance comparison with SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> using different visual features on MSR-VTT. Here V and C d t approaches( <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib 13]</ref> and GoogleNet <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Among these compared methods, SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> is the most similar method to ours, which also has an atte ltiple visual feature fusion, we compare our model with the other five state-of-the-art approaches( <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib rget=\"#b1\">[2]</ref>). The comparison results are shown in Table <ref type=\"table\">2</ref>. SA-G-3C <ref type=\"bibr\" target=\"#b36\">[37]</ref> uses the combination of GoogleNet feature and 3D-CNN featu  pairs. Considering that there are few methods tested on this dataset, we compare our model with SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> which is the most similar work to ours. Similarly, we perf <ref type=\"figure\">2</ref> illustrates several descriptions generated by our M 3 -google, SA-google <ref type=\"bibr\" target=\"#b36\">[37]</ref> and human-annotated ground truth on the test set of MSVD.   method. Fig. <ref type=\"figure\">3</ref> shows the attention shift of our M 3 -google and SA-google <ref type=\"bibr\" target=\"#b36\">[37]</ref> across multiple frames when generating the sentence. There about 40 sentences. So there are about 80,000 video-description pairs. Following the standard split <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, we divide the ori. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Hornik et al. states one hidden layer is sufficient to represent arbitrarily accurately a function <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Thus, one can intuitively conceive that improving the tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: :id=\"formula_7\">\u03c3 (b) i = exp (b i ) J j=1 exp (b j )</formula><p>, for b \u2208 R J .</p><p>Recent work <ref type=\"bibr\" target=\"#b26\">(Raffel et al., 2019</ref>) has adopted a simplified form where value arn separate embeddings for each layer as word representations from different layers can vary a lot.<ref type=\"bibr\" target=\"#b26\">Raffel et al. (2019)</ref> learn separate S matrices for each attenti  (2019)</ref> learn separate S matrices for each attention head but share them across layers. We use<ref type=\"bibr\" target=\"#b26\">Raffel et al. (2019)</ref>'s form of relative position encoding for t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b62\">[63]</ref> , and online recommendation systems <ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" target=\"#b64\">65</ref> .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: construction is yet performed. This corresponds to the initialization step in Algorithm 1.</p><p>In <ref type=\"bibr\" target=\"#b22\">[23]</ref>, a time-domain reconstruction approach is proposed for spe ef>, proposed in the same conference. A follow-up work <ref type=\"bibr\" target=\"#b18\">[19]</ref> of <ref type=\"bibr\" target=\"#b22\">[23]</ref> supplies clean phase during training. However, this makes . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uch as bagof-words or n-grams <ref type=\"bibr\" target=\"#b19\">(Wang and Manning, 2012)</ref> or SVMs <ref type=\"bibr\" target=\"#b16\">(Tang et al., 2015)</ref>. The neural network based methods like <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vides tolerance against incorrect labels.</p><p>The recently introduced transform/stability loss of <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> is based on the same principle as our wo rements is \u223c0.5 percentage points better than independent flips.</p><p>A principled comparison with <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> is difficult due to several reasons. The  paths, and comparing the outputs of the network instead of pre-activation data of the final layer. <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> recently introduced a new loss function . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b2\">Baydin &amp; Pearlmutter (2014)</ref>, and applied to small problems by <ref type=\"bibr\" target=\"#b12\">Domke (2012)</ref>. However, the na\u00efve approach fails for real-sized  s=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.\">Related work</head><p>The most closely-related work is <ref type=\"bibr\" target=\"#b12\">Domke (2012)</ref>, who derived algorithms to compute reverse-mode de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ity mentions into pre-defined entity types. For this task, we evaluate all the models on OpenEntity <ref type=\"bibr\" target=\"#b5\">(Choi et al., 2018)</ref> following the setting from <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  cannot generalize to unseen users/items. A recent inductive graphbased recommender system, PinSage <ref type=\"bibr\" target=\"#b39\">(Ying et al., 2018a)</ref>, uses node content as initial node feature t al., 2017)</ref>, F-EAE <ref type=\"bibr\" target=\"#b13\">(Hartford et al., 2018)</ref>, and PinSage <ref type=\"bibr\" target=\"#b39\">(Ying et al., 2018a)</ref>. Among them, GRALS is a graph regularized  y use only one or two message passing layers <ref type=\"bibr\" target=\"#b1\">(Berg et al., 2017;</ref><ref type=\"bibr\" target=\"#b39\">Ying et al., 2018a)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: can be summarized as the paths connecting the target user and item based on historical interactions <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. For example, give al behaviors of users reflect personal interests; meanwhile, the user groups can also profile items <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Hence, in each mo e d \u2032 m is the transformation size; and we select LeakyReLU(\u2022) as the nonlinear activation function <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Such aggregation  y based on CF models <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b38\">[38]</ref><ref type=\"bibr\" target=\"#b39\">[39]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , and those have been very promising <ref type=\"bibr\" target=\"#b17\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b6\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Gilmer et al b28\">(Shervashidze et al., 2011;</ref><ref type=\"bibr\" target=\"#b17\">Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b6\">Hamilton et al., 2017)</ref>.</p><p>Yet, such aggregation schemes some \u2022 u\u2208 N (v) (deg(v)deg(u)) \u22121/2 h (l\u22121) u (2)</formula><p>where deg(v) is the degree of node v in G. <ref type=\"bibr\" target=\"#b6\">Hamilton et al. (2017)</ref> derived a variant of GCN that also works   and can be viewed as a form of a \"skip connection\" between different layers.For COMBINE, GraphSAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> uses concatenation after a feature trans o select the important neighbors via an attention mechanism. The max-pooling operation in GraphSAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> implicitly selects the important nodes.  ords features for each document (node) and citation links (edges) between documents. (II) On Reddit <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>, the task is to predict the community to ataset contains word vectors as node features. (III) For protein-protein interaction networks (PPI) <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> We compare against three baselines: Grap lutional Networks (GCN) <ref type=\"bibr\" target=\"#b17\">(Kipf &amp; Welling, 2017)</ref>, Graph-SAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> and Graph Attention Networks (GAT) <ref  r gives 6 JK-Net variants. We follow exactly the same setting of GraphSAGE as in the original paper <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>, where the model consists of 2 hidden la  the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>. As a result, this graph is more regular. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Data stream strides. The data stream is characterized with respect to local and global data strides <ref type=\"bibr\" target=\"#b9\">[10]</ref>. A global stride is defined as the difference in the data m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cs. The proposal stage (e.g., Selective Search <ref type=\"bibr\" target=\"#b33\">[34]</ref>, EdgeBoxes <ref type=\"bibr\" target=\"#b36\">[37]</ref>, DeepMask <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" targ periments:</p><p>\u2022 AMiner-AND<ref type=\"foot\" target=\"#foot_0\">1</ref> . The dataset is released by <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, which contains 500 author names for traini  e.g., paper network <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" target=\"#b13\">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature  (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: odels with numerous parameters are shown to have better effectiveness and good representation power <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Recently, with the great impact of neural networks on com. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: such resources.</p><p>Finally, we discuss how to combine Cherry and Speculative Multithreading (SM) <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target que where several tasks are extracted from a sequential code and executed speculatively in parallel <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: anguage processing <ref type=\"bibr\" target=\"#b11\">[13,</ref><ref type=\"bibr\" target=\"#b14\">16,</ref><ref type=\"bibr\" target=\"#b19\">21]</ref>. The teacher and student model in conventional knowledge di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: UCTION</head><p>Traditional convolutional neural networks for image classification, such as AlexNet <ref type=\"bibr\" target=\"#b12\">(Krizhevsky et al. (2012)</ref>), are parameterized in such a way tha ile no longer state-of-the-art, this performance is significantly better than the 40.7% reported by <ref type=\"bibr\" target=\"#b12\">Krizhevsky et al. (2012)</ref>, as well as the best all-convolutional. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that pulls representations of noisy samples away from clean ones. Finally, mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> has recently demonstrated outstanding robu ushing the state-of-the-art one step forward by combining our approach with mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref>.</p><p>4. Guiding mixup data augmentation   bootstrapping <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> to deal with the closed-set label noise sc i-c.org/ns/1.0\"><head n=\"3.3.\">Joint label correction and mixup data augmentation</head><p>Recently <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> proposed a data augmentation technique nam ref type=\"bibr\" target=\"#b9\">(Hendrycks et al., 2018)</ref>), and use the configuration reported in <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> for mixup. We outperform the related work  tab_8\">6</ref> shows the results of the proposed approaches M-DYR-H and MD-DYR-SH compared to mixup <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> on TinyImageNet to demonstrate that our ap onstrate that our approach is useful far from CIFAR data. The proposed approach clearly outperforms <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> for different levels of label noise, obtai ref type=\"bibr\" target=\"#b11\">Jiang et al., 2018b;</ref><ref type=\"bibr\">Patrini et al., 2017;</ref><ref type=\"bibr\" target=\"#b34\">Zhang et al., 2018)</ref> modify either the loss directly, or the pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ectures, e.g., Siamese networks <ref type=\"bibr\" target=\"#b9\">(He et al., 2016)</ref> and attention <ref type=\"bibr\" target=\"#b26\">(Seo et al., 2017;</ref><ref type=\"bibr\" target=\"#b31\">Tay et al., 20 her than specific term matches. Context-aware representation learning, such as co-attention methods <ref type=\"bibr\" target=\"#b26\">(Seo et al., 2017)</ref>, has been proved effective in many benchmark ng elements in the missing dimen-sions. Softmax col is the column-wise softmax operator. Similar to <ref type=\"bibr\" target=\"#b26\">Seo et al. (2017)</ref>, we perform co-attention from two directions: ; (2) interaction and attention mecha-nisms <ref type=\"bibr\" target=\"#b31\">(Tay et al., 2019b;</ref><ref type=\"bibr\" target=\"#b26\">Seo et al., 2017;</ref><ref type=\"bibr\" target=\"#b21\">Parikh et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ox optimization, i.e., autotuning. This method is used to tune high performance computing libraries <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. However, auto-tun .</p><p>High-performance libraries such as ATLAS <ref type=\"bibr\" target=\"#b45\">[46]</ref> and FFTW <ref type=\"bibr\" target=\"#b14\">[15]</ref> use auto-tuning to get the best performance. Tensor compre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>.</p><p>This fetch  div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Fetch Models</head><p>The stream fetch engine <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> model is shown in  for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. However, having h ww.tei-c.org/ns/1.0\"><head n=\"5.1\">The Multiple Stream Predictor</head><p>The next stream predictor <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, which is shown in zed code layout. In addition, data are shown for the original single-stream predictor, described in <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, and a 2-stream mu p>To avoid this increase in the fetch engine complexity, we propose to use long instruction streams <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> as basic predictio /p><p>Our instruction cache setup uses wide cache lines, that is, 4 times the processor fetch width <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and 64KB total hardware budget. The trace fetch architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of a single device.</p><p>Recently, pipeline parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> has been proposed as a promising approach for training lar een those nodes during backpropagation. The other category is asynchronous(async) pipeline training <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This manner inserts mini-batches into pipeline continuous lism, and hybrid approaches combining both. Current state-of-theart pipeline partitioning algorithm <ref type=\"bibr\" target=\"#b11\">[12]</ref> is not able to be applied to synchronous training effectiv evice assignment affects communication efficiency and computing resource utilization. Previous work <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses hierarchical planning and works well for asynchronous D. Contributions over previous work</head><p>Previous works on pipeline planning includes PipeDream <ref type=\"bibr\" target=\"#b11\">[12]</ref> (for asynchronous training) and torchgpipe <ref type=\"bibr it the micro-batch further into 2 even slices, and assign each to a device. An alternative approach <ref type=\"bibr\" target=\"#b11\">[12]</ref> (Fig. <ref type=\"figure\" target=\"#fig_6\">8(b)</ref>) is no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: revious work on CycleGANs <ref type=\"bibr\" target=\"#b29\">(Zhu et al., 2017)</ref> and dual learning <ref type=\"bibr\" target=\"#b11\">(He et al., 2016)</ref>, our method takes two initial models in oppos ition to that, we would like to incorporate a language modeling loss during NMT training similar to <ref type=\"bibr\" target=\"#b11\">He et al. (2016)</ref>. Finally, we would like to adapt our approach . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: researchers have found convolutional networks (ConvNets) <ref type=\"bibr\" target=\"#b16\">[17]</ref>  <ref type=\"bibr\" target=\"#b17\">[18]</ref> are useful in extracting information from raw signals, ran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[2]</ref>, phase-based sampling <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Of these techniques, the sampling based approaches typicall 8\">[9]</ref> extended SimPoint to provide statistical confidence measures.</p><p>Wunderlich, et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> developed the SMARTS framework, which applies statistical sa wn techniques for inferring statistics about a population given a sample of that population. SMARTS <ref type=\"bibr\" target=\"#b3\">[4]</ref> demonstrated that systematic sampling can be used to approxi mpared LiveSim with no sampling simulation and with a sampling mode that was very similar to SMARTS <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= f type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: abulous semantic segmentation models such as <ref type=\"bibr\">FCN [Long et al., 2015]</ref>, SegNet <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref>, <ref type=\"bibr\">DeepLab-v3 [Chen work is based on the classic encoderdecoder network architecture without the fully-connected layers <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref> and we improve it by adding the re semantic segmentation networks -FCN <ref type=\"bibr\" target=\"#b7\">[Long et al., 2015]</ref>, SegNet <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref>, and DeepLab-V3 <ref type=\"bibr\" t r network with that of <ref type=\"bibr\">FCN-32s, FCN-16s, FCN-8s [Long et al., 2015]</ref>, Seg-Net <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017], and</ref><ref type=\"bibr\">DeepLab-v3 [C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ures (e.g. landmarks, visemes) and using computer graphics (CG) methods to generate realistic faces <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Some methods avoid the use of CG by selecting frames from  or re-targeting steps to adapt to new faces.</p><p>Convolutional neural networks (CNN) are used in <ref type=\"bibr\" target=\"#b11\">[12]</ref> to transform audio features to 3D meshes of a specific per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing worst case execution times include <ref type=\"bibr\" target=\"#b12\">(Ferdinand et al., 2001;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2007)</ref>.</p><p>All of these models require detailed pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: or example, by emphasizing frequent and meaningful concepts <ref type=\"bibr\" target=\"#b12\">[3,</ref><ref type=\"bibr\" target=\"#b42\">32]</ref>. A more recent trend is to use entities to introduce explic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e nodes into a K-dimensional vector space, which preserves certain properties among nodes. Deepwalk <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref>, LINE <ref type=\"bibr\" target=\"#b6\">[Tang et tain properties among nodes. Deepwalk <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref>, LINE <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref> and Node2vec <ref type=\"bibr\" target=\"#b2\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ased document embeddings from GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> (as Doc2vec implementation <ref type=\"bibr\" target=\"#b32\"> they encode the segments with GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> and compute their similarity to determine whether papers a ref type=\"bibr\" target=\"#b34\">35]</ref> but unable to represent entire documents. Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> (also known as Doc2vec), extends word2vec to learn embeddi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  graph, including auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, and knowledge distill  related tasks simultaneously so that knowledge obtained from each task can be reused by the others <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target ef type=\"figure\" target=\"#fig_1\">1 (c</ref>). This structure is very similar to multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, in which different su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: neural networks to non-Euclidean domains (such as graphs) for robust feature learning. Bruna et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> define the convolution in Fourier domain and calculate the e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: btained negative samples are richer in features and semantically more meaningful for the task. (See <ref type=\"bibr\" target=\"#b6\">Lee et al. (2018)</ref> for an incarnation of this idea in the context asets carefully to obtain robust OOD detection.</p><p>Within the context of uncertainty estimation, <ref type=\"bibr\" target=\"#b6\">Lee et al. (2018)</ref> demonstrate that adversarially generated sampl  samples of GANs is closest to our approach of using generated data points as negative samples, but <ref type=\"bibr\" target=\"#b6\">Lee et al. (2018)</ref> work within a classification setting. <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: consists of hardware and software components. Jenga hardware requires small changes over prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target  most of the heavy lifting to build virtual cache hierarchies. Specifically, Jenga builds on Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, which constructs sing systems with non-uniform SRAM banks, single-lookup NUCAs generally outperform directory-based NUCAs <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target ng just enough capacity to fit the working set at minimum latency and energy. In particular, Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> achieves this by letti rdware needs to be flexible and reconfigurable at low cost. We thus base Jenga's hardware on Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, which supports applic  hardware components, emphasizing differences from Jigsaw at the end of the section. See prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> for details of Jigsaw' g page reclassifications: Like Jigsaw and R-NUCA, Jenga uses a simple technique to map pages to VHs <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ vel hierarchy in a single curve, and thus can use the same partitioning algorithms as in prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> to allocate capacity ref type=\"bibr\" target=\"#b2\">[3]</ref>, R-NUCA <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Jigsaw <ref type=\"bibr\" target=\"#b6\">[7]</ref> do away with hierarchy entirely, adopting a single-lookup de type=\"bibr\" target=\"#b27\">28]</ref> and more aggressive than <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>). Table <ref type=\"ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or example in emergency calls, where agents have to respond emergent requests in a foreign language <ref type=\"bibr\" target=\"#b24\">(Munro 2010)</ref>; or in online courses, where audiences and speaker. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rs. For example, Gaussian-process-based optimization methods could incorporate gradient information <ref type=\"bibr\" target=\"#b35\">(Solak et al., 2003)</ref>. Such methods could make use of parallel e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as designed to be representative of next-generation sharedmemory programs for chip-multiprocessors\" <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Our experiments show that for those programs, no matter how </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Benchmarks</head><p>We use PARSEC <ref type=\"bibr\" target=\"#b2\">[3]</ref> as the benchmark suite. It is a recently released suite desi  Because there is no close-form expression for the equation, the program uses numerical computation <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>The input data file of this benchmark includes an arr 6MB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : see<ref type=\"bibr\" target=\"#b2\">[3]</ref> for detail.</note></figure> <figure xmlns=\"http://www.tei-c. , as well as systems applications that mimic large-scale multithreaded commercial programs. Studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> have shown that the su  0 {0 2 4 6},{1 3 5 7},{0 2 4 6},{1 3 5 7} 1 161.6 0 {0 2 1 3},{4 5 6 7},{0 2 1 3},{4 5 6 7} 4 161. <ref type=\"bibr\" target=\"#b2\">3</ref> No binding 165.7</p><p>In PARSEC, ferret and dedup are two suc urement are relevant to this current work. Bienia and others <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> have shown a detailed exploration of the characterization of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ure the representation quality. All of these works sample negative examples from p(x). Arora et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> theoretically analyze the effect of contrastive representati erparameter. Without loss of generality, we set t = 1 for all theoretical results.</p><p>Similar to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we assume an underlying set of discrete latent classes C th f ) = inf W\u2208R K\u00d7d L Softmax (T , W f ).<label>(10)</label></formula><p>In line with the approach of <ref type=\"bibr\" target=\"#b0\">[1]</ref> we analyze the supervised loss of a mean classifier <ref typ commonly the case. The dependence on on N and T in Theorem 5 is roughly equivalent to the result in <ref type=\"bibr\" target=\"#b0\">[1]</ref>, but the two bounds are not directly comparable since the pr .</formula><p>In order to derive our bound we will exploit a concentration of measure result due to <ref type=\"bibr\" target=\"#b0\">[1]</ref>. They consider an objective of the form</p><formula xml:id=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: model with fewer parameters. The recently-introduced factorised time-delay neural networks (TDNN-F) <ref type=\"bibr\" target=\"#b12\">[13]</ref> utilise half the number of parameters than the hybrid netw to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Consequently, a TDNN-F acoustic model (10 time-delay laye. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <p>Identifying compound-protein interaction (CPI) plays an import role in discovering hit compounds <ref type=\"bibr\" target=\"#b39\">(Vamathevan et al., 2019)</ref>. Conventional methods, such as struct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tional counters/logic. We have pointed to more drawbacks in previous sections.</p><p>More recently, <ref type=\"bibr\" target=\"#b12\">[13]</ref> and <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed ins d instrumentationbased tools to analyze data-locality and scalability bottlenecks, respectively. In <ref type=\"bibr\" target=\"#b12\">[13]</ref>, average memory latency is sampled with a PMU and coupled  better metrics based on our MemStalls.L3Miss event e.g. can be used instead of raw latency value in <ref type=\"bibr\" target=\"#b12\">[13]</ref> to quantify when speedup may apply. Examining metrics at h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dictions and is commonly used in the semisupervised learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> and domain adaptati ata and labeled query data. We use the entropy minimization to define the loss on unlabeled data as <ref type=\"bibr\" target=\"#b14\">[15]</ref> and use the cross-entropy to define the loss on labeled da. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ETS</head><p>Dilated convolutions were originally proposed for the computation of wavelet transform <ref type=\"bibr\" target=\"#b37\">[38]</ref> and employed in the deep learning context (as an alternati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: periments are conducted using the same training data as in <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, which is from multiple domains such as Voice Search, YouTu p>For training, we use the same multidomain datasets as in <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> which include anonymized and hand-transcribed English utter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet <ref type=\"bibr\" target=\"#b37\">(Oord et al., 2016a;</ref><ref type=\"bibr\">b)</ref>. Discretization m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g Solaris 8. We employ a wait-free implementation of the total store order memory consistency model <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. We perform speculati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures across languages which can be mapped to the same space <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Authors in <ref type=\"bibr\" target=\"#b11\">[12]</ref> looke. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion problem.</p><p>Learning upscaling filters was briefly suggested in the footnote of Dong et.al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>. However, the importance of integrating it into the CNN as p eration was not fully recognised and the option not explored. Additionally, as noted by Dong et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are no efficient implementations of a convolution lay uate the power of the sub-pixel convolution layer by comparing against SRCNN's standard 9-1-5 model <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Here, we follow the approach in <ref type=\"bibr\" target=\"#b CNN's standard 9-1-5 model <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Here, we follow the approach in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, using relu as the activation function for our models in thi ard comparison with results from previous published results<ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>.</note> \t\t</body> \t\t<back> \t\t\t<div type=\"references\">  \t\t\t\t<l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: se multi-scale testing on featurized image pyramids (e.g., <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>). The principle advantage of featurizing each level of an i et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>, which creates an inconsistency between train/test-time inf vements, such as iterative regression <ref type=\"bibr\" target=\"#b8\">[9]</ref>, hard negative mining <ref type=\"bibr\" target=\"#b34\">[35]</ref>, context modeling <ref type=\"bibr\" target=\"#b15\">[16]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re-based sequence labeling models with bilingual constraints/inferences for Chinese and English NER <ref type=\"bibr\" target=\"#b1\">[Che et al., 2013;</ref><ref type=\"bibr\">Wang et al., 2013]</ref>. Our ty types (Person, Location, Organization, None), which are commonly adopted in previous NER studies <ref type=\"bibr\" target=\"#b1\">[Che et al., 2013;</ref><ref type=\"bibr\">Wang et al., 2013]</ref>.</p> s drawback of this approach is the requirement of manually annotate bilingual NER data. There-fore, <ref type=\"bibr\" target=\"#b1\">[Chen et al., 2010]</ref> proposed approaches to extract bilingual nam od of labeling bilingual corpora with named entity labels automatically based on Wikipedia.</p><p>[ <ref type=\"bibr\" target=\"#b1\">Che et al., 2013;</ref><ref type=\"bibr\">Wang et al., 2013]</ref> tackl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b7\">[Mottaghi et al., 2014;</ref><ref type=\"bibr\" target=\"#b2\">Cordts et al., 2016;</ref><ref type=\"bibr\" target=\"#b1\">Caesar et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Ros et al., 20 owards furthering botanical taxonomy, as illustrated by the wealth of research regarding this topic <ref type=\"bibr\" target=\"#b1\">[Cerutti et al., 2011;</ref><ref type=\"bibr\" target=\"#b5\">Kebapci et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent and separation <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. However, this usuall. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d><p>Finding L * can be reduced to the problem All-Pairs Bottleneck Paths in vertex weighted graphs <ref type=\"bibr\" target=\"#b35\">[36]</ref> . Unfortunately, the state-of-the-art method for it has a  lem can be reduced to the problem of All-Pairs Bottleneck Paths in edge weighted graphs (edge-APBP) <ref type=\"bibr\" target=\"#b35\">[36]</ref> . For edge-APBP, the following property holds: Theorem 2.  rmula_26\">\u2200 u, v \u2208 V (G ) = V (T ) , u = v , c * G (u, v ) = c * T (u, v ) .</formula><p>Proof. See <ref type=\"bibr\" target=\"#b35\">[36]</ref> .</p><p>c * G (u, v ) and c * T (u, v ) denote the edge ca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  have a longer access time, and this may increase the critical path length and penalize performance <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>In this paper we propose a novel register renaming ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: procedures such as generating forced alignments and decision trees. Meanwhile, another line of work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" target ction to infer speech-label alignments automatically. This CTC technique is further investigated in <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" target incorporate lexicons and language models into decoding. When decoding CTC-trained models, past work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target enchmark show that Eesen results in superior performance than the existing end-to-end ASR pipelines <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>. The WERs of Eesen are ained using frame-level labels with respect to the cross-entropy (CE) criterion. Instead, following <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target /1.0\"><head n=\"3.1.\">Decoding with WFSTs</head><p>Previous work has introduced a variety of methods <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target  ARPA format (which we will consistently refer to as standard). To be consistent with previous work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>, we report our results e\">3</ref> lists the results of end-to-end ASR systems that have been reported in the previous work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref> and on the same datase ature differ not only in their model architectures but also in their decoding methods. For example, <ref type=\"bibr\" target=\"#b6\">[6]</ref> and <ref type=\"bibr\" target=\"#b8\">[8]</ref> adopt two distin \">[10]</ref> or achieve the integration under constrained conditions (e.g., nbest list rescoring in <ref type=\"bibr\" target=\"#b6\">[6]</ref>). In this work, we propose a generalized decoding approach b ed in decoding. When only the lexicon is used, our decoding behaves similarly as the beam search in <ref type=\"bibr\" target=\"#b6\">[6]</ref>. In this case, the WER rises quickly to 26.92%. This obvious dard language model, the character-based system gets the WER of 9.07%. CTC experiments in past work <ref type=\"bibr\" target=\"#b6\">[6]</ref> have adopted an expanded vocabulary, and re-trained the lang ref type=\"bibr\" target=\"#b8\">8]</ref> and on the same dataset. Our Eesen framework outperforms both <ref type=\"bibr\" target=\"#b6\">[6]</ref> and <ref type=\"bibr\" target=\"#b8\">[8]</ref> in terms of WERs ]</ref> in terms of WERs on the testing set. It is worth pointing out that the 8.7% WER reported in <ref type=\"bibr\" target=\"#b6\">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the  bibr\" target=\"#b6\">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the authors of <ref type=\"bibr\" target=\"#b6\">[6]</ref> generate a nbest list of hypotheses from a hybrid DNN model, e WERs of Eesen systems via more advanced learning techniques (e.g., expected transcription loss in <ref type=\"bibr\" target=\"#b6\">[6]</ref>) and alternative decoding approach (e.g., dynamic decoders <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tand the meaning and then point out the entity-relation pairs sequentially.</p><p>Currently, CopyRE <ref type=\"bibr\" target=\"#b19\">(Zeng et al. 2018)</ref> is the most powerful Seq2Seq based joint ext ww.tei-c.org/ns/1.0\"><head>Baselines and Evaluation Metrics</head><p>We compare CopyMTL with CopyRE <ref type=\"bibr\" target=\"#b19\">(Zeng et al. 2018)</ref>, NovelTagging <ref type=\"bibr\" target=\"#b20\" >(Mintz et al. 2009</ref>). To make joint extraction more challenging than DSRE experiment setting, <ref type=\"bibr\" target=\"#b19\">Zeng et al. (2018)</ref> additionally modified the data to include mo ng relation problem, their nature and complexities are akin to table filling.</p><p>Seq2Seq: CopyRE <ref type=\"bibr\" target=\"#b19\">(Zeng et al. 2018</ref>) is another method for solving the overlappin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=\"bibr\" target=\"#b11\">[14]</ref> . It could obtain transformation rules automatically durin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  future process generations. In this section, we review a technique called pipeline reconfiguration <ref type=\"bibr\" target=\"#b20\">[20]</ref>, that allows a large logical design to be implemented on a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  al <ref type=\"bibr\" target=\"#b35\">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref type=\"bibr\" target=\"#b15\">[16]</ref>. As described in Algorithm 1, for the r -th document \u03c0 r ,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: earity tends to degrade the performance of these models. Such a phenomenon is called over-smoothing <ref type=\"bibr\" target=\"#b24\">(Li et al., 2018b)</ref>, which suggests that as the number of layers. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sification (e.g., <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref>). As far as we are aware, these types of defenses have all. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Simonyan &amp; Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b44\">Srivastava et al., 2015;</ref><ref type=\"bibr\" target=\"#b18\">He et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Huang et al., 20 et (left) <ref type=\"bibr\" target=\"#b34\">(LeCun et al., 1998)</ref> with a 110-layer ResNet (right) <ref type=\"bibr\" target=\"#b18\">(He et al., 2016)</ref> on the CIFAR-100 dataset. The top row shows t e normalization techniques have enabled the development of very deep architectures, such as ResNets <ref type=\"bibr\" target=\"#b18\">(He et al., 2016)</ref> and DenseNets <ref type=\"bibr\" target=\"#b22\"> he past few years.</p><p>It is now common to see networks with hundreds, if not thousands of layers <ref type=\"bibr\" target=\"#b18\">(He et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Huang et al., 2 ageNet models of 2015 all use an order of magnitude less weight decay than models of previous years <ref type=\"bibr\" target=\"#b18\">(He et al., 2016;</ref><ref type=\"bibr\" target=\"#b41\">Simonyan &amp; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ence model is an encoder-decoder architecture with attention <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Let X = [X 1 , . . . , P (y u | X, y &lt;u ) = h(S u , Q u ).<label>(4)</label></formula><p>The function g(\u2022) is a GRU RNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> which encodes the previous token and query vector Q u\u22121 to p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , AutoMine).</p><p>To address these challenges, general-purpose graph mining systems like Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref>, Fracta e 16-core machine outperforms state-of-the-art distributed graph mining systems including Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref> and G-M </formula><p>Step 1</p><p>Step 2</p><p>Step 3  <ref type=\"bibr\" target=\"#b70\">[57]</ref>, Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> and Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref>. Num </ref> (a realworld graph dataset), RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref> and Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> generate over a billion partial matches for clique countin neral purpose graph mining systems 3 : Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref>, Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref> and G-M  is because its breadth-first exploration generates large amounts of partial matches which must be  <ref type=\"bibr\" target=\"#b65\">[52]</ref> and RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref>. '\u00d7' 4,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>. Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> is a distributed graph mining system that follows a filter n memory or on disk) so that they can be extended. While systems based on breadth-first exploration <ref type=\"bibr\" target=\"#b65\">[52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref> demand high memory oesn't need to separately define the exploration strategy, as done in other pattern-unaware systems <ref type=\"bibr\" target=\"#b65\">[52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>.</p></div> <div xm get=\"#b21\">[8,</ref><ref type=\"bibr\" target=\"#b25\">12,</ref><ref type=\"bibr\" target=\"#b47\">34,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>, they are not patte ico and labeled Patents have been used by previous systems <ref type=\"bibr\" target=\"#b25\">[12,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref> to evaluate FSM whi get=\"#b25\">12,</ref><ref type=\"bibr\" target=\"#b35\">22,</ref><ref type=\"bibr\" target=\"#b47\">34,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>. Arabesque <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: slation, recently proposed by <ref type=\"bibr\" target=\"#b17\">Kalchbrenner and Blunsom (2013)</ref>, <ref type=\"bibr\" target=\"#b27\">Sutskever et al. (2014)</ref> and <ref type=\"bibr\" target=\"#b7\">Cho e Despite being a quite new approach, neural machine translation has already shown promising results. <ref type=\"bibr\" target=\"#b27\">Sutskever et al. (2014)</ref> reported that the neural machine transl  called RNN Encoder-Decoder, proposed by <ref type=\"bibr\" target=\"#b6\">Cho et al. (2014a)</ref> and <ref type=\"bibr\" target=\"#b27\">Sutskever et al. (2014)</ref> upon which we build a novel architectur is a vector generated from the sequence of the hidden states. f and q are some nonlinear functions. <ref type=\"bibr\" target=\"#b27\">Sutskever et al. (2014)</ref> used an LSTM as f and</p><formula xml:i =\"#b11\">Graves, 2012;</ref><ref type=\"bibr\" target=\"#b5\">Boulanger-Lewandowski et al., 2013)</ref>. <ref type=\"bibr\" target=\"#b27\">Sutskever et al. (2014)</ref> used this approach to generate translat use LSTM units instead of the gated hidden unit described here, as was done in a similar context by <ref type=\"bibr\" target=\"#b27\">Sutskever et al. (2014)</ref>.</p><p>The new state s i of the RNN emp </p><p>Most of the proposed neural machine translation models belong to a family of encoderdecoders <ref type=\"bibr\" target=\"#b27\">(Sutskever et al., 2014;</ref><ref type=\"bibr\" target=\"#b6\">Cho et al t=\"#b17\">Kalchbrenner and Blunsom, 2013;</ref><ref type=\"bibr\" target=\"#b6\">Cho et al., 2014a;</ref><ref type=\"bibr\" target=\"#b27\">Sutskever et al., 2014;</ref><ref type=\"bibr\" target=\"#b7\">Cho et al. rent neural networks (RNN) were used by <ref type=\"bibr\" target=\"#b6\">(Cho et al., 2014a)</ref> and <ref type=\"bibr\" target=\"#b27\">(Sutskever et al., 2014)</ref> to encode a variable-length source sen  table <ref type=\"bibr\" target=\"#b6\">(Cho et al., 2014a)</ref> or to re-rank candidate translations <ref type=\"bibr\" target=\"#b27\">(Sutskever et al., 2014)</ref>, has allowed to surpass the previous s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g may arguably be the most straightforward to incorporate into a neural network pipeline. In Torch7 <ref type=\"bibr\" target=\"#b10\">(Collobert et al., 2011)</ref>, for example, we implement temperature. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type=\"bibr\" target=\"#b2\">(Grover and Leskovec 2016)</ref> to represent these features by v i \u2208 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b33\">34]</ref> or harvest knowledge with specific linguistic patterns <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. Taxonomy can be vie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep GCNs towards node classification <ref type=\"bibr\" target=\"#b14\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b18\">Li et al., 2018a;</ref><ref type=\"bibr\" target=\"#b32\">Xu et al., 2018 othing, towards the other extreme, makes training a very deep GCN difficult. As first introduced by <ref type=\"bibr\" target=\"#b18\">Li et al. (2018a)</ref> and further explained in <ref type=\"bibr\" tar their experiments, residual GCNs still perform worse when the depth is 3 and beyond. The authors in <ref type=\"bibr\" target=\"#b18\">Li et al. (2018a)</ref> first point out the main difficulty in constr  deep GCNs will converge to a subspace and incur information loss. It generalizes the conclusion in <ref type=\"bibr\" target=\"#b18\">Li et al. (2018a)</ref> by further considering the ReLu function and  .org/ns/1.0\"><head n=\"4.2\">TOWARDS PREVENTING OVER-SMOOTHING</head><p>By its original definition in <ref type=\"bibr\" target=\"#b18\">Li et al. (2018a)</ref>, the over-smoothing phenomenon implies that t  of GCNs. <ref type=\"bibr\" target=\"#b24\">Oono &amp; Suzuki (2019)</ref> has generalized the idea in <ref type=\"bibr\" target=\"#b18\">Li et al. (2018a)</ref> by taking both the non-linearity (i.e. the Re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atasets and continuously set new stateof-the-art performance <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar  \u2212 L)X is understood as features averaging and propagation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. In graph signal pr rast to the recent design principle of graph neural networks <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, our results sugges 9]</ref>. Started with the early success of ChebNet <ref type=\"bibr\" target=\"#b5\">[6]</ref> and GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> at vertex classification, many variants of GNN have been p e observe that the parameters of a graph convolutional layer in a Graph Convolutional Network (GCN) <ref type=\"bibr\" target=\"#b15\">[16]</ref> only contribute to overfitting. Similar observations have   problem and provide insights to the mechanism underlying the most commonly used baseline model GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and its simplified variant SGC <ref type=\"bibr\" target=\"# onding NNs using true features.</p><p>Theorem 7 implies that, under Assumption 1, both gfNN and GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> have similar high performance. Since gfNN does not require Network model by removing nonlinearity in the neural network and only averaging features.</p><p>GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> Graph Convolutional Neural Network ($) is the most commonl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een challenged by processors that do not block on loads <ref type=\"bibr\" target=\"#b4\">[ER94]</ref>  <ref type=\"bibr\" target=\"#b1\">[CS95]</ref>. Rather than stalling until a cache miss is satisfied, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: containing perfectly registered images from the same scene. The recently proposed benchmark dataset <ref type=\"bibr\" target=\"#b22\">[23]</ref> is an example. Due to the high cost in capturing the real- ms of both quantitative scores and visual quality. Due to the regional properties of the reflection <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we also adopt SSIMr <ref type=\"bibr\" target=\"#b20\">[21]</ ref>, where the numbers displayed are the mean values over all 100 sets of wild images in the SIR 2 <ref type=\"bibr\" target=\"#b22\">[23]</ref> dataset. In particular, Ours + Eq. ( <ref type=\"formula\" t se a novel approach to jointly generate and separate reflections. Based on the public dataset SIR 2 <ref type=\"bibr\" target=\"#b22\">[23]</ref> and the proposed real-world dataset, our method outperform  testing the models across datasets with different collecting protocols (e.g., the dataset of SIR 2 <ref type=\"bibr\" target=\"#b22\">[23]</ref> and the dataset of Zhang18 <ref type=\"bibr\" target=\"#b28\"> ture image, background image and reflection image) can be captured in a \"remove-and-occlude\" manner <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>: 1) Taking a photo o we capture 4000+ images, which allows for a much larger scale than those used in existing methods <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Finally, we build  the high-end devices (e.g., the DSLR camera with fully manual control model) like previous methods <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, we also use the c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>To obtain typical workload pairs among these benchmarks, we refer to the balanced random method <ref type=\"bibr\" target=\"#b29\">[32]</ref> to select 48 pairs of benchmarks for experiments. Then we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed Bloom filters <ref type=\"bibr\" target=\"#b20\">(Michael, 2002)</ref>, the space-code Bloom filters <ref type=\"bibr\" target=\"#b13\">(Kumar et al., 2005)</ref>, the spectral Bloom filters <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t-dependent sentiment classification. The approach is an extension on long short-term memory (LSTM) <ref type=\"bibr\" target=\"#b6\">(Hochreiter and Schmidhuber, 1997)</ref> by incorporating target infor e. These gates adaptively remember input vector, forget previous history and generate output vector <ref type=\"bibr\" target=\"#b6\">(Hochreiter and Schmidhuber, 1997)</ref>. LSTM cell is calculated as f problem of gradient vanishing or exploding <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 1994;</ref><ref type=\"bibr\" target=\"#b6\">Hochreiter and Schmidhuber, 1997)</ref>, where gradients may grow or d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: xml:id=\"formula_0\">[McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94]</formula><p>Balanced scheduling <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> is an algorithm that can generate schedules that adapt m iss ratios, assuming a workstation-like memory model in which cache misses are normally distributed <ref type=\"bibr\" target=\"#b12\">[KE93]</ref>.</p><p>Since its success depends on the amount of instru no reuse information), which are balanced scheduled.</p><p>The original work on balanced scheduling <ref type=\"bibr\" target=\"#b12\">[KE93]</ref>, which compared it to the traditional approach and witho  produces schedules that are independent of the memory system implementation.</p><p>Initial results <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> indicated that balanced scheduling produced speedups ave results from the original comparison of balanced and traditional scheduling (without optimizations) <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> illustrates both the limitations of simple architecture . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b34\">34,</ref><ref type=\"bibr\" target=\"#b35\">35,</ref><ref type=\"bibr\" target=\"#b37\">37]</ref>. However, all of them only work well for the single-pattern. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: great success, deep neural networks have been shown to be highly vulnerable to adversarial examples <ref type=\"bibr\" target=\"#b4\">[3,</ref><ref type=\"bibr\" target=\"#b33\">32,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b23\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b24\">Radford et al., 2018;</ref><ref type=\"bibr\" target=\"#b5\">Devlin et al., 2019)</ref>, which has improved performances on various ngth of n by the same WordPiece tokenizer <ref type=\"bibr\" target=\"#b35\">(Wu et al., 2016)</ref> in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>. Next, as shown in Fig. <ref type=\"figure\"> s generated by the cross-modality encoder. For the cross-modality output, following the practice in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>, we append a special token [CLS] (denoted a  and each of them only focuses on a single modality (i.e., language or vision). Different from BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, which applies the transformer encoder onl om branch of Fig. <ref type=\"figure\" target=\"#fig_0\">2</ref>, the task setup is almost same to BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>: words are randomly masked with a probabil n image and a sentence match each other. This task is similar to 'Next Sentence Prediction' in BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>.</p><p>Image Question Answering (QA) In or  by the WordPiece tokenizer <ref type=\"bibr\" target=\"#b35\">(Wu et al., 2016)</ref> provided in BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> image to maximize the pre-training compute s. We take Adam (Kingma and Ba, 2014) as the optimizer with a linear-decayed learning-rate schedule <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> and a peak learning rate at 1e \u2212 4. We tra .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">BERT versus LXMERT</head><p>BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019</ref>) is a pre-trained language encoder which im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  increasing of depth brings benefits to representation power <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d new items. State-of-art reinforcement learning methods usually apply the simple \u03f5-greedy strategy <ref type=\"bibr\" target=\"#b29\">[31]</ref> or Upper Confidence Bound (UCB) <ref type=\"bibr\" target=\"# dynamic nature of news characteristics and user preference, we propose to use Deep Q-Learning (DQN) <ref type=\"bibr\" target=\"#b29\">[31]</ref> framework. This framework can consider current reward and  avoid the harm to recommendation accuracy induced by classical exploration strategies like \u03f5-greedy <ref type=\"bibr\" target=\"#b29\">[31]</ref> and Upper Confidence Bound <ref type=\"bibr\" target=\"#b21\"> ss stored in the memory to update the network Q.</p><p>Here, we use the experience replay technique <ref type=\"bibr\" target=\"#b29\">[31]</ref> to update the network. Specifically, agent G maintains a m ture of news recommendation and the need to estimate future reward, we apply a Deep Q-Network (DQN) <ref type=\"bibr\" target=\"#b29\">[31]</ref> to model the probability that one user may click on one sp ead><p>The most straightforward strategies to do exploration in reinforcement learning are \u03f5-greedy <ref type=\"bibr\" target=\"#b29\">[31]</ref> and UCB <ref type=\"bibr\" target=\"#b21\">[23]</ref>. \u03f5-greed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s guaranteed to converge to a local optimum of it.</p><p>Inspired by the previous work on CycleGANs <ref type=\"bibr\" target=\"#b29\">(Zhu et al., 2017)</ref> and dual learning <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b21\">[22]</ref>) and region-based convolutional neural networks (R-CNNs) <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Although region-based CNNs were computationally expensive a b5\">[6]</ref>. Although region-based CNNs were computationally expensive as originally developed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, their cost has been drastically reduced thanks to sharing c rk whose last fc layer simultaneously predicts multiple (e.g., 800) boxes, which are used for R-CNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> object detection. Their proposal network is applied on a sin rget=\"#foot_3\">3</ref>For regression, we adopt the parameterizations of the 4 coordinates following <ref type=\"bibr\" target=\"#b5\">[6]</ref>:</p><formula xml:id=\"formula_2\">t x = (x \u2212 x a )/w a , t y = odel for ImageNet classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, as is standard practice <ref type=\"bibr\" target=\"#b5\">[6]</ref>. We tune all layers of the ZF net, and conv3 1 and up for th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the user's clicked news as the query and the candidate news as the documents.</p><p>Wide &amp; Deep <ref type=\"bibr\" target=\"#b2\">(Cheng et al., 2016)</ref>, a deep model for recommendation which comb. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: also argue that training objectives of these algorithms (either reconstructing the adjacency matrix <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or feature matrix  CN as the encoder, then decode by inner product with cross-entropy loss. As variants of GAE (VGAE), <ref type=\"bibr\" target=\"#b22\">[23]</ref> exploits adversarially regularized method to learn more ro ional networks with the (variational) autoencoder for representation learning.</p><p>ARGA and ARVGA <ref type=\"bibr\" target=\"#b22\">[23]</ref> add adversarial constraints to GAE and VGAE respectively, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lution streams. This paper represents a very substantial extension of our previous conference paper <ref type=\"bibr\" target=\"#b105\">[105]</ref> with an additional material added from our unpublished t t object detection and instance segmentation frameworks. The main technical novelties compared with <ref type=\"bibr\" target=\"#b105\">[105]</ref> lie in threefold. <ref type=\"bibr\" target=\"#b0\">(1)</ref efold. <ref type=\"bibr\" target=\"#b0\">(1)</ref> We extend the network (named as HRNetV1) proposed in <ref type=\"bibr\" target=\"#b105\">[105]</ref>, to two versions: HRNetV2 and HRNetV2p, which explore al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target task has been explored in the context of other IR scenarios, including query classification <ref type=\"bibr\" target=\"#b20\">[21]</ref>, query auto-completion <ref type=\"bibr\" target=\"#b25\">[26]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ity and its performance drops dramatically for similarity based tasks, e.g. nearest neighbor search <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" ta s usually built on classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memorized features <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which has limited efficiency and discriminability. We pro  images and predefined noise signals, which constrains the distribution between raw data and noises <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Bolztmann Machines (RBMs) <ref type=\"bibr\" target=\"#b23\"> discriminability. Softmax Embedding with Memory Bank. To improve the inferior efficiency, Wu et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> propose to set up a memory bank to store the instance feat tance feature rather than classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memory bank <ref type=\"bibr\" target=\"#b45\">[46]</ref>. To achieve the goal that features of the same instance un =\"bibr\" target=\"#b2\">[3]</ref> 67.6 Exemplar <ref type=\"bibr\" target=\"#b7\">[8]</ref> 74.5 NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80.8 NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80. </p ype=\"bibr\" target=\"#b7\">[8]</ref> 74.5 NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80.8 NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80. </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><h ead n=\"4.1.\">Experiments on Seen Testing Categories</head><p>We follow the experimental settings in <ref type=\"bibr\" target=\"#b45\">[46]</ref> to conduct the experiments on CIFAR-10 <ref type=\"bibr\" ta  ColorJitter, RandomHorizontalFlip) in PyTorch with default parameters are adopted.</p><p>Following <ref type=\"bibr\" target=\"#b45\">[46]</ref>, we adopt weighted kNN classifier to evaluate the performa  200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label <ref type=\"bibr\" target=\"#b45\">[46]</ref>   plar CNN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoft  type=\"bibr\" target=\"#b45\">[46]</ref>   plar CNN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Triplet N <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Triplet loss with and without hard mining. Triplet (ha and the margin parameter is set to 0.5. DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> represent the state-of-the-art unsupervised feature learni  classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which u hod outperforms it by 9.1%. Compared to NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which use memorized feature for optimizing, the proposed  target=\"#fig_4\">3</ref>. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while <ref type=\"bibr\" target=\"#b45\">[46]</ref> takes 25 epochs and <ref type=\"bibr\" target=\"#b7\">[8]</ref ance features rather than classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memory bank <ref type=\"bibr\" target=\"#b45\">[46]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  d images in ten The classifier is used to predict the label of test samples. We implement NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepClu ct the label of test samples. We implement NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> (c the best accuracy with both classifiers (kNN: 74.1%, Linear: 69.5%), which are much better than NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> un  three state-of-the-art unsupervised methods (Exemplar <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref>) o n Table <ref type=\"table\">3</ref>.</p><p>Generally, the instance-wise feature learning methods (NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, Examplar <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Ours) o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  MI alone, and the choice of encoder and MI estimators have a significant impact on the performance <ref type=\"bibr\" target=\"#b52\">(Tschannen et al., 2020)</ref>.</p><p>Figure <ref type=\"figure\">1</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b21\">[22]</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref> have gone a step   and extracts meta-path features to represent the connectivity between users and items.</p><p>\u2022 CKE <ref type=\"bibr\" target=\"#b22\">[23]</ref> combines CF with structural, textual, and visual knowledge presentation vectors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. However, commonly-used KGE methods focus on modeling rigor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b4\">[5]</ref>, texture features <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, shape features <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Wic  <ref type=\"bibr\" target=\"#b4\">[5]</ref>, texture features <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, shape features <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Wic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d by the number of stacked layers (depth). Recent evidence <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> reveals that network depth is of crucial importance, and th rk depth is of crucial importance, and the leading results <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" tar to the output <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. In <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, a few intermediat entering layer responses, gradients, and propagated errors, implemented by shortcut connections. In <ref type=\"bibr\" target=\"#b42\">[43]</ref>, an \"inception\" layer is composed of a shortcut branch and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the effect of affirmative action (see e.g., <ref type=\"bibr\" target=\"#b12\">Keith et al., 1985;</ref><ref type=\"bibr\" target=\"#b11\">Kalev et al., 2006)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pogation on just single hidden-layer feedforward neural networks. Recent attempts in this direction <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> propose efficient  r the search space through tunable parameters, in contrast to rigid search procedures in prior work <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Consequently, our of nodes. We contrast the performance of node2vec with state-of-the-art feature learning algorithms <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. We experiment wit odel, recent research established an analogy for networks by representing a network as a \"document\" <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. The same way as a r shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Our algorithm nod roceed by extending the Skip-gram architecture to networks <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. We seek to optimize the following objective function, whic  normalized Laplacian matrix of graph G as the feature vector representations for nodes. \u2022 DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>: This approach learns d-dimensional feature representation lude other matrix factorization approaches which have already been shown to be inferior to DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>. We also exclude a recent approach, GraRep <ref type=\"bibr the sampling procedure computationally efficient. We showed how random walks, also used in DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>, allow the sampled nodes to be reused as neighborhoods for SION</head><p>Both DeepWalk and LINE can be seen as rigid search strategies over networks. DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes search using uniform random walks. The obvious li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b5\">5]</ref>,and trusted measurement is a key problem of this technology <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7]</ref>. Trusted computing tre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: i-c.org/ns/1.0\"><head n=\"2\">Related Work</head><p>CNN Compression and Acceleration. Extensive works <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" ta time. We observe a correlation between the pre-fine-tune accuracy and the post fine-tuning accuracy <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b22\">22]</ref>. As shown in Table  For channel pruning, we use max response selection (pruning the weights according to the magnitude <ref type=\"bibr\" target=\"#b20\">[20]</ref>), and preserve Batch Normalization <ref type=\"bibr\" target /ref>. However, it requires iterative prune &amp; fine-tune procedure to achieve decent performance <ref type=\"bibr\" target=\"#b20\">[20]</ref>, and single-shot pruning without retraining will greatly h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -c.org/ns/1.0\"><head n=\"3.2.2.\">Attention</head><p>ESPnet uses a location-aware attention mechanism <ref type=\"bibr\" target=\"#b34\">[35]</ref>, as a default attention. A dot-product attention <ref type conditions (e.g., <ref type=\"bibr\" target=\"#b32\">[33]</ref> does not use any language models, while <ref type=\"bibr\" target=\"#b34\">[35]</ref> and <ref type=\"bibr\" target=\"#b10\">[11]</ref> use a word-b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: re architectures, IDL compiler features <ref type=\"bibr\" target=\"#b15\">[16]</ref> and optimizations <ref type=\"bibr\" target=\"#b16\">[17]</ref>, systematic benchmarking of multiple ORBs <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: edding-based methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, and hybrid methods edding-based methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> pre-process a KG wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ification dataset -FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> propose a hybrid attention-based prototypical   and useless at the same time.</p><p>So we apply a CNN-based feature attention mechanism similar to <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> proposed as a class feature extractor. It depe 17)</ref> which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> which includes Proto and PHATT. For a fair com ><table /><note>* reported by<ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> and \u25c7 reported by<ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref>.</note></figure> \t\t\t<note xmlns=\"http://www.te assification and few-shot text classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Gao et al., 2019)</ref> tasks respectively, so our model is based on p shra et al., 2018)</ref>, Proto <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> and PHATT <ref type=\"bibr\" target=\"#b3\">(Gao et al., 2019)</ref> respectively.</p></div> <div xmlns=\"http://ww. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rns of prompts that successfully extract knowledge from LMs. In open information extraction systems <ref type=\"bibr\" target=\"#b2\">(Banko et al., 2007)</ref>, manually defined patterns are often levera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vanced methods for comparing the quality of images, such as feature matching based on SIFT analysis <ref type=\"bibr\" target=\"#b15\">[16]</ref>. This technique, however, shows the same trends as the sim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: number of clusters reaches the specified K.</p><p>When K is unknown, we adopt an optimal modularity <ref type=\"bibr\" target=\"#b21\">[22]</ref> partitioning mechanism to determine the partition of publi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ll a challenge for training diversification models.</p><p>To tackle this problem, inspired by IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref ty e Carlo search. It is also used in the traditional information retrieval area, Wang proposed IR-GAN <ref type=\"bibr\" target=\"#b18\">[19]</ref> which consists of two information retrieval models in it.  \ud835\udc5e, \ud835\udc46)) 1 + exp(\ud835\udc53 \ud835\udf19 (\ud835\udc51 |\ud835\udc5e, \ud835\udc46)) .<label>(7)</label></formula><p>Please note that different from IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DVGAN-doc has an additional component \ud835\udc46 to represent the  , it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we generate negative document set \ud835\udc37 \u2032 by selecting the do. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" t static pre-trained models <ref type=\"bibr\" target=\"#b16\">[17]</ref>. In recent work, Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed Boundary Attack, which generates adversarial exam ttacks</head><p>Most related to our work is the Boundary Attack method introduced by Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Boundary Attack is an iterative algorithm based on reject s: We compare HopSkipJumpAttack with three state-of-the-art decision-based attacks: Boundary Attack <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Limited Attack <ref type=\"bibr\" target=\"#b8\">[9]</ref> an ibr\" target=\"#b5\">[6]</ref>. A version normalized by image dimension was employed by Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> for evaluating Boundary Attack. The As an alternative metr <formula xml:id=\"formula_38\">|E[\u03c6 x (x t + \u03b4 t u)]| &gt; 0,</formula><p>as we can see from Equation <ref type=\"bibr\" target=\"#b13\">(14)</ref>. To attempt to control the variance, we introduce a baseli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  works have been done as the defense for adversarial images on convolutional neural networks (e.g., <ref type=\"bibr\" target=\"#b9\">[Xu et al., 2018;</ref><ref type=\"bibr\">Papernot and McDaniel, 2018]</ target=\"#b8\">[Xie et al., 2018]</ref>. Other forms of input pre-processing, such as local smoothing <ref type=\"bibr\" target=\"#b9\">[Xu et al., 2018]</ref> and image compression <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bility than RNNs. However, in the NER task, Transformer encoder has been reported to perform poorly <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>, our experiments also confirm this result. T =\"#tab_3\">3</ref>. The poor performance of the Transformer in the NER datasets was also reported by <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>. Although performance of the Transformer is  ibr\" target=\"#b13\">(Guo et al., 2019)</ref>. Although performance of the Transformer is higher than <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>, it still lags behind the BiLSTM-based model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  networking APIs, by either i) modifying the internal implementation but leaving the APIs untouched <ref type=\"bibr\" target=\"#b16\">[20,</ref><ref type=\"bibr\" target=\"#b29\">33,</ref><ref type=\"bibr\" ta ion in our discussion.</p><p>Contention on Accept Queue (multi-core): As explained in previous work <ref type=\"bibr\" target=\"#b16\">[20,</ref><ref type=\"bibr\" target=\"#b29\">33]</ref>, a single listenin ickly become overloaded since those globally visible objects cause system-wide synchronization cost <ref type=\"bibr\" target=\"#b16\">[20]</ref>. In our microbenchmark, the VFS overhead for socket alloca ti-core systems since the kernel maintains the inode and dentry as globally visible data structures <ref type=\"bibr\" target=\"#b16\">[20]</ref>.</p><p>To address the above issues, we propose lightweight  and differences between Affinity-Accept <ref type=\"bibr\" target=\"#b29\">[33]</ref> and MegaPipe. In <ref type=\"bibr\" target=\"#b16\">[20]</ref>, the authors address the scalability issues in VFS, namely or linear scalability of network I/O on multi-core systems <ref type=\"bibr\" target=\"#b15\">[19,</ref><ref type=\"bibr\" target=\"#b16\">20,</ref><ref type=\"bibr\" target=\"#b29\">33,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y small-scale desktop applications.</p><p>Previously proposed indirect branch prediction techniques <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr equires only 2,048 bits but a 1,024-entry gsharelike indirect branch predictor (tagged target cache <ref type=\"bibr\" target=\"#b9\">[10]</ref>) needs at least 2,048 bytes along with additional tag stora hat the indirect branch will jump to the same target address it jumped to in its previous execution <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b33\">[33]</ref>. 3 To our knowle rget addresses of many indirect branches alternate rather than stay stable for long periods of time <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b33\">[33]</ref>.</p><p>4. Since  ow path leading to an indirect branch is strongly correlated with the target of the indirect branch <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This is very similar to modern conditional branch predicto predictor has low (about 50 percent) prediction accuracy <ref type=\"bibr\" target=\"#b37\">[37]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b33\">[33]</ref>.</p><p>Chang et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> first proposed to use branch history information to disting 5\">15</ref> compares the performance of VPC prediction with the tagged target cache (TTC) predictor <ref type=\"bibr\" target=\"#b9\">[10]</ref>. On average, VPC prediction provides performance improvemen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns=\"http://www.tei-c.org/ns/1.0\"><head>Class</head><p>Ontological Rule</p><p>Uncertain Extractions  <ref type=\"bibr\" target=\"#b0\">[Blum and Mitchell, 1998]</ref>, to combine the strengths of PSL-KGI a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he learned node embeddings, we visualize the node representations in 2D space using t-SNE algorithm <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The figures are shown in Figure <ref type=\"figure\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  on social media given users' posts, connections among users, and a small number of labelled users. <ref type=\"bibr\" target=\"#b34\">Rahimi et al. (2018)</ref> apply GCNs with highway connections on thi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b32\">[33]</ref> bypass signal from one layer to the next via identity connections. Stochastic depth <ref type=\"bibr\" target=\"#b12\">[13]</ref> shortens ResNets by randomly dropping layers during traini ation preservation explicit through additive identity transformations. Recent variations of ResNets <ref type=\"bibr\" target=\"#b12\">[13]</ref> show that many layers contribute very little and can in fa ]</ref>. Recently, stochastic depth was proposed as a way to successfully train a 1202-layer ResNet <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Stochastic depth improves the training of deep residual n tion between dense convolutional net-works and stochastic depth regularization of residual networks <ref type=\"bibr\" target=\"#b12\">[13]</ref>. In stochastic depth, layers in residual networks are rand oring/shifting) that is widely used for these two datasets <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar 31 images for additional training. Following common practice <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar st time. Following <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, we report classification errors on the validation set.</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b6\">7]</ref>. This important finding attracts great interests in edge partitioning recently <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target xisting partitioners, METIS gives the lowest replication factor which is consistent with literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. However, METIS runs titioning algorithm METIS <ref type=\"bibr\" target=\"#b7\">[8]</ref> is extended for edge partitioning <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which makes full access to the graph structure by partition alanced if max i \u2208[p] {|E i |} \u2264 \u2308 \u03b1 |E | /p\u2309.<label>(1)</label></formula><p>The replication factor <ref type=\"bibr\" target=\"#b2\">[3]</ref> of a partitioning is defined as</p><formula xml:id=\"formula_ <head n=\"2.3\">NP-Hardness</head><p>The p-edge partitioning problem has been proved to be NP-hard in <ref type=\"bibr\" target=\"#b2\">[3]</ref> when p grows with n = |V |. To our best knowledge, it has no weight. One can turn a vertex-partitioner into an edge-partitioner while preserving its performance <ref type=\"bibr\" target=\"#b2\">[3]</ref>. To transform METIS to an edge-partitioner, we first call ME </table></figure> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">In<ref type=\"bibr\" target=\"#b2\">[3]</ref>, the NP-hardness is proved by a reduction from 3-partition p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oped for classical ASR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target= nd decoder models using recurrent models with LSTMs <ref type=\"bibr\" target=\"#b5\">[6]</ref> or GRUs <ref type=\"bibr\" target=\"#b3\">[4]</ref>. However, their use of hierarchy in the encoders demonstrate eep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Our best model achieves a WER of 10.53% where our baseline  oder depth of the baseline model without using any convolutional layers. Our baseline model follows <ref type=\"bibr\" target=\"#b3\">[4]</ref> using the skip connection technique in its time reduction. T btained 10.5% WER without a language model, an 8.5% absolute improvement over published best result <ref type=\"bibr\" target=\"#b3\">[4]</ref>. While we demonstrated our results only on the seq2seq task,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[5]</ref> 70.01 \u00b1 0.63% 67.26 \u00b1 1.12% Low&amp;SentiBank 70.54 \u00b1 1.00% 68.03 \u00b1 1.36% SentiStrength <ref type=\"bibr\" target=\"#b36\">[29]</ref> 59.30 \u00b1 0.87% 62.78 \u00b1 0.91% USEA <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hing that often occurs in direct-mapped buffers. A 3-bit performance counter based on Heil's design <ref type=\"bibr\" target=\"#b16\">[17]</ref> tracks the effectiveness of each entry and is used to sele pendence chain. If the generating values are present then ARVI's predictions are near perfect. Heil <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposed another approach that correlates on the differenc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 9]</ref>, <ref type=\"bibr\" target=\"#b39\">[40]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref>, see <ref type=\"bibr\" target=\"#b41\">[42]</ref> for illustrative applications in several domains. Recent w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the uncertainty inherent in user behavior and the limited information provided by browser sessions <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>Based on existing literature, almost all the RNN-ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: imilar to discriminative reranking approaches used in the parsing and machine translation community <ref type=\"bibr\" target=\"#b29\">(Shen et al., 2004)</ref>. However, our approach provides a generativ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b10\">(Graves et al., 2013)</ref> with a Connectionist Temporal Classification (CTC) output layer <ref type=\"bibr\" target=\"#b9\">(Graves et al., 2006;</ref><ref type=\"bibr\">Graves, 2012, Chapter 7)</ _8\">15</ref>) can be efficiently evaluated and differentiated using a dynamic programming algorithm <ref type=\"bibr\" target=\"#b9\">(Graves et al., 2006)</ref>. Given a target transcription y * , the ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aints. As a result, researchers have explored software-based techniques to tolerate hardware faults <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Softwarebased techniques do not require any modification in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rning techniques such as support vector machine and artificial neural networks have been studied in <ref type=\"bibr\" target=\"#b7\">[8]</ref> and <ref type=\"bibr\" target=\"#b8\">[9]</ref>. The automatic m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on-based models <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b12\">[12]</ref>, <ref type=\"bibr\" target=\"#b52\">[43]</ref> widely used in information retrieval. The interaction-base d by each person to 100.</p><p>The hyper-parameters of the RBF kernel functions are set the same as <ref type=\"bibr\" target=\"#b52\">[43]</ref>. We use 11 RBF kernels, with the hyper-parameters m = f1; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e 165 workers involved in these tasks. The final estimations are discretized into 7 bins. Bluebirds <ref type=\"bibr\" target=\"#b18\">[19]</ref>: It consists of 108 bluebird pictures. There are 2 breeds . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nnections) is to increase the network width. The GoogLeNet <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> uses an \"Inception module\" which concatenates feature-maps  ween DenseNets and ResNets. Compared to Inception networks <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, which also concatenate features from different layers, Den  layer only produces k output feature-maps, it typically has many more inputs. It has been noted in <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> that a 1\u00d71 convolu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d language modeling <ref type=\"bibr\" target=\"#b25\">(Ling et al., 2015b)</ref> or dependency parsing <ref type=\"bibr\" target=\"#b2\">(Ballesteros et al., 2015)</ref>. Figure <ref type=\"figure\" target=\"#f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Deep Supervision</head><p>We propose to use deep supervision <ref type=\"bibr\" target=\"#b5\">[6]</ref> in UNet++, enabling the model to operate in two modes: (1) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to guide the pixel denoiser; in contrast, our denoising is applied directly on features. Guo et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> transform the images via non-differentiable image preprocess ts of their non-differentiable computations <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In contrast to <ref type=\"bibr\" target=\"#b7\">[8]</ref>, our feature denoising models are differentiable, but are st y increases as the image is propagated through the network <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, and non-existing activations in the feature maps are halluci. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: usion are personalized PageRank (PPR) <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b35\">[36]</ref>. PPR corresponds to choosing T = T rw and \u03b8 PPR k = \u03b1(1 \u2212  cal graph learning <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, especially for clu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b11\">[8]</ref>, along with its representative updated descendants, e.g. Fast R-CNN <ref type=\"bibr\" target=\"#b10\">[7]</ref> and Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[26]</ref>, l methods, which opens the deep learning era in object detection. Its descendants (e.g., Fast R-CNN <ref type=\"bibr\" target=\"#b10\">[7]</ref>, Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[26]</ref>) up. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: unknown features of the language <ref type=\"bibr\" target=\"#b10\">(Daum\u00e9 III and Campbell, 2007;</ref><ref type=\"bibr\" target=\"#b36\">Takamura et al., 2016;</ref><ref type=\"bibr\" target=\"#b5\">Coke et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g future lightning occurrences. However, although extrapolationbased methods for weather nowcasting <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref> can be migrated to  re sensitive to different dimensions are assembled to predict mobile events in the city. Shi et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposed convolutional LSTM (ConvLSTM) for precipitation now t\u22121 .</formula><p>The ConvLSTM in this paper does not include peephole connections, as mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The data first enters the CNN modules, where sequentially a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  \uf8f9 \uf8fb . (<label>6</label></formula><formula xml:id=\"formula_10\">)</formula><p>The limiting objective <ref type=\"bibr\" target=\"#b5\">(6)</ref>, which we denote by L Q Debiased , still samples examples x  \">CIFAR10 and STL10</head><p>First, for CIFAR10 <ref type=\"bibr\" target=\"#b22\">[23]</ref> and STL10 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: information which may be head pose, expression, or landmarks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref ty N <ref type=\"bibr\" target=\"#b6\">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016</ref>) is unable to distinguish edges with different. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b13\">13,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref> and absorbs their advantages. CFCBS embed measurement instr ranode CFEs occur if the program control before and after the illegal jump resides in the same node <ref type=\"bibr\" target=\"#b14\">[14]</ref>.</p><p>Considering a Program Flow Graph { P ,V}, we define not constant and is determined by the length of the first two fields and consists of a series of 0s <ref type=\"bibr\" target=\"#b14\">[14]</ref>.(see Fig. <ref type=\"figure\" target=\"#fig_1\">1</ref>).</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> outbound absence yes yes Traceroute loops <ref type=\"bibr\" target=\"#b17\">[18]</ref> outbound absence yes yes Passive detection <ref type=\"bibr ed from 2 692 autonomous systems. We refer to this technique as forwarders-based.</p><p>Lone et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> proposed another method that does not require a vantage po iance for a part of it <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>.</p><p>\u2022 Longest ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  t f , of dimension 300 for an input text (transcript), t.</p><p>Audio Feature Extraction openSMILE <ref type=\"bibr\" target=\"#b21\">[22]</ref> is an open-source toolkit used to extract high dimensional. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, disentangling node representations <ref type=\"bibr\" target=\"#b19\">[20]</ref> and automatically selecting hyper-parameters <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b6\">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation methods of <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b4\">Jozefowicz et  pled per training example):</p><p>\u2022 For any K 1, a binary classification variant of NCE, as used by <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b8\">Mikolov et al. n square error as the MLE) as K ! 1.</p><p>\u2022 We discuss application of our results to approaches of <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b8\">Mikolov et al. history x. This is the most straightforward extension of NCE to the conditional case; it is used by <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012)</ref>. It has the clear drawback however of intro o motivate the importance of the two algorithms, we now discuss their application in previous work. <ref type=\"bibr\" target=\"#b9\">Mnih and Teh (2012)</ref> consider language modeling, where x = w 1 w  n of the parameters c</p><p>x corresponding to normalization terms for each history. Interestingly, <ref type=\"bibr\" target=\"#b9\">Mnih and Teh (2012)</ref> acknowledge the difficulties in maintaining . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, GNNGUARD is a general approach and can be effortlessly com ximation of adjacency matrix that drops noisy information through an SVD decomposition. Tang et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> improve the robustness of GNNs against poisoning attack th tion and rewiring. In doing so, the attacker aims to fool the GNN into making incorrect predictions <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The attacker finds optimal perturbation A through optimiz ettack attacker <ref type=\"bibr\" target=\"#b7\">[8]</ref> and so is less versatile. Another technique <ref type=\"bibr\" target=\"#b17\">[18]</ref> uses transfer learning to detect fake edges. While that is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b16\">[Nickel et al., 2012</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016</ref><ref type=\"bibr\" target=\"#b3\">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, wi porate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> and ComplEx <ref type=\"bibr\" target=\"#b2 tion). We work with ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> embeddings which have shown state of the #b23\">[Trouillon et al., 2016]</ref>, <ref type=\"bibr\">SimplE [Kazemi and Poole, 2018]</ref>, ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a dif Then all facts upto length 3 in the hierarchy of taxonomy were included.</p><p>FB15K-237: FB15K-237 <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref>, another popular benchmark does not have  compare them with Com-plEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref>, two state-of-the-art KG embeddings meth g the same class balance, and use them as our validation and test split.</p><p>YAGO3-10: YAGO3-10 [ <ref type=\"bibr\" target=\"#b3\">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s of these two datasets.</p><p>The PPI dataset was collected from the molecular signatures database <ref type=\"bibr\" target=\"#b25\">(Subramanian et al., 2005)</ref>. Each node represents a protein and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2.\">MODEL</head><p>Our sequence-to-sequence model is an encoder-decoder architecture with attention <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed extensively on unsupervised features <ref type=\"bibr\" target=\"#b7\">(Collobert et al., 2011;</ref><ref type=\"bibr\" target=\"#b36\">Turian et al., 2010;</ref><ref type=\"bibr\" target=\"#b23\">Lin and Wu, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, neural networks <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and Latent Dirichlet Allocation <ref type=\"bibr\" target=\"#b4\">[5]</ref>. We will focus on models that are induced by Singular Value . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: figuration of human body structure. This representation is derived from the principle, published in <ref type=\"bibr\" target=\"#b8\">(Johansson, 1973)</ref>, explaining how humans observe actions. This w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: beddings. However, XLNet integrates the relative positional encoding, as proposed in Transformer-XL <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Therefore, XLNet's architecture is, in theory, not bound . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rious heuristic approaches have been proposed to improve the the robustness to adversarial examples <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, such heuristics are often broken by new attack m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i-c.org/ns/1.0\"><head n=\"1.\">Introduction</head><p>Transformer models were originally introduced by <ref type=\"bibr\" target=\"#b36\">Vaswani et al. (2017)</ref> in the context of neural machine translat p><p>Initially, in \u00a7 3.1, we introduce a formulation for the transformer architecture introduced in <ref type=\"bibr\" target=\"#b36\">(Vaswani et al., 2017)</ref>. Subsequently, in \u00a7 3.2 and \u00a7 3.3 we pre the memory consumption with respect to the self attention layer. In all experiments, we use softmax <ref type=\"bibr\" target=\"#b36\">(Vaswani et al., 2017)</ref> to refer to the standard transformer arc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y are unfortunately not the only weak spot in machine learning systems.</p><p>Recently, Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> have demonstrated that data preprocessing used in machine  ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Image-Scaling Attacks</head><p>Recently, Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> have shown that scaling algorithms are vulnerable to attac aling algorithm. Both matrices can be computed in advance and are reusable. We refer to Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> for a description how to calculate L and R.</p><p>Based on  assignment.</p><p>We implement image-scaling attacks in the strong variant proposed by Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref>. We make a slight improvement to the original attacks: Ins  on rectangular blocks instead of columns and rows. As a result, the original attack by Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> is not applicable to this scaling algorithm. To attack are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ine learning tasks involve graph structured datasets, such as classifying posts in a social network <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, predicting interfaces between protein o aggregate a local set of lower-level features. We refer to such an operator as a graph aggregator <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> and the set of local nodes as the rece ref type=\"bibr\" target=\"#b15\">Kipf and Welling, 2017</ref>) can be interpreted as graph aggregators <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>.</p><p>Graph aggregators are the basic t to the inductive node classification problem. We also improve the sampling strategy introduced in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> to reduce the memory cost and increase oral forecasting problem. Extensive experiments on two node classification datasets, PPI and Reddit <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, and one traffic speed forecasting dat rtional to the total number of nodes, which could be hundreds of thousands of nodes in large graphs <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computatio \"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computational cost, <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> proposed the GraphSAGE framework that  ble and the goal is to predict the labels of the unseen testing nodes. Our approach follows that of <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, where a mini-batch of nodes are sampl dels in our framework and a two-layer fully connected neural network on the PPI and Reddit datasets <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. The five baseline aggregators include  the effectiveness of incorporating graph structures, we also evaluate a two-layer fully-connected  <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> (61.2)<ref type=\"foot\" target=\"#foot_0  hyperparameters for training. The training, validation, and testing splits are the same as that in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. The micro-averaged F1 score is used t ith the previous state-of-the-art methods on inductive node classification. This includes GraphSAGE <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, GAT <ref type=\"bibr\" target=\"#b27\">(V  can see steady improvement with larger sampling sizes, which is consistent with the observation in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>.</p><p>Effect of output dimensions in  r\" target=\"#b15\">Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b8\">Fout et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b27\">Veli\u010dkovi d on either pooling over neighborhoods <ref type=\"bibr\" target=\"#b15\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a)</ref> or computing a weighted sum of the neig rget=\"#b7\">(Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a)</ref>, while others integrated edge features . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: : we benchmark Cassandra 0.7.3 database with 30 million records. The request is generated by a YCSB <ref type=\"bibr\" target=\"#b14\">[15]</ref> client with a 50:50 ratio of read to update.</p><p>Media S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ) to a new application <ref type=\"bibr\" target=\"#b1\">[4,</ref><ref type=\"bibr\" target=\"#b2\">5,</ref><ref type=\"bibr\" target=\"#b29\">33]</ref>. As an example, the drugs developed for the treatment of in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: isplays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref>. The gesture annotation is performed usin es is motivated by previous research that has suggested that deceivers' speech has lower complexity <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref>. We use the tool described in <ref type=\" or gaze) and nod (Side-Turn-R) more frequently than truth-tellers. This agrees with the findings in <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref> that liars who are more motivated to get . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e. Local sentiment-related features from each region are incorporated for visual sentiment analysis <ref type=\"bibr\" target=\"#b16\">[17]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref>. However, the w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  2017)</ref> to graphstructured inputs, building on the recent Graph Attention Network architecture <ref type=\"bibr\" target=\"#b31\">(Veli\u010dkovi\u0107 et al., 2018)</ref>. The result is a powerful, general mo trong baselines. In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of <ref type=\"bibr\" target=\"#b31\">(Veli\u010dkovi\u0107 et al., 2018)</ref>. This encoder consists of PReLU activ ibr\" target=\"#b9\">(Kipf and Welling, 2017)</ref>. Our model extends the graph attention networks of <ref type=\"bibr\" target=\"#b31\">Veli\u010dkovi\u0107 et al. (2018)</ref>, a direct descendant of the convolutio  loss of infor- Graph Transformer Our model is most similar to the Graph Attention Network (GAT) of <ref type=\"bibr\" target=\"#b31\">Veli\u010dkovi\u0107 et al. (2018)</ref>, which computes the hidden representat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: iv xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.3\">Software Based Polices</head><p>Eyerman et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a job scheduler based on sampling mechanism along. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: micro-video. In terms of acoustic modality, we separate audio tracks with FFmpeg 6 and adopt VGGish <ref type=\"bibr\" target=\"#b19\">[20]</ref> to learn the acoustic deep learning features. For textual . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xt to speech, TTS) <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and speech recognit et=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and ASR <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"b , we re-sample it to 16kHZ and convert the raw waveform into mel-spectrograms following Shen et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> with 50ms frame size, 12.5ms hop size. For the text, we us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  challenged by their vulnerability to adversarial examples <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, which are crafted by adding small, human-imperceptible noise icted labels and probabilities of these images given by the Inception v3. more varied training data <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p><p>With the knowl >, adversarial training is the most extensively investigated way to increase the robustness of DNNs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ef type=\"bibr\" target=\"#b22\">[23]</ref>, one-step gradient-based methods such as fast gradient sign <ref type=\"bibr\" target=\"#b4\">[5]</ref> and iterative variants of gradient-based methods <ref type=\" t-based methods that iteratively perturb the input with the gradients to maximize the loss function <ref type=\"bibr\" target=\"#b4\">[5]</ref>, momentum-based methods accumulate a velocity vector in the  e-box attacks and the transferability, and act as a stronger attack algorithm than one-step methods <ref type=\"bibr\" target=\"#b4\">[5]</ref> and vanilla iterative methods <ref type=\"bibr\" target=\"#b8\"> ply derived.</p><p>One-step gradient-based approaches, such as the fast gradient sign method (FGSM) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, find an adversarial example x * by maximizing the loss func. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  compared to DNNs <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Recently, very deep CNNs architectures <ref type=\"bibr\" target=\"#b13\">[14]</ref> have also been shown to be successful in ASR <ref type=\"bi R, recently there have been several advancements in the computer vision community on very deep CNNs <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that have not been TMs.</p><p>We are driven by same motivation that led to the success of very deep networks in vision <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tant measure in detecting anomalous edges.</p><p>The works <ref type=\"bibr\">[Sun et al., 2006;</ref><ref type=\"bibr\" target=\"#b5\">Shin et al., 2016;</ref><ref type=\"bibr\" target=\"#b5\">Shin et al., 201 orks <ref type=\"bibr\">[Sun et al., 2006;</ref><ref type=\"bibr\" target=\"#b5\">Shin et al., 2016;</ref><ref type=\"bibr\" target=\"#b5\">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref t </ref><ref type=\"bibr\" target=\"#b5\">Shin et al., 2017]</ref> view the anomaly as a dense sub-graph. <ref type=\"bibr\" target=\"#b5\">Shin [2016;</ref><ref type=\"bibr\">2017]</ref> define a density functio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sting effects <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and the non-local similarity in the images <ref type=\"bibr\" target=\"#b24\">[25]</ref>, have been explored. However, these low-level image priors. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>Many recent sequence labeling frameworks <ref type=\"bibr\" target=\"#b25\">(Ma and Hovy, 2016b;</ref><ref type=\"bibr\" target=\"#b27\">Misawa et al., 2017)</ref> share a very basic structure: a bidirectio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualization <ref type=\"bibr\" target=\"#b28\">[28]</ref>. This optimization statically converts an indirect branch  ly a subset of indirect branches with a limited number of targets that can be determined statically <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Our proposed VPC prediction mechanism provides the benefi 24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> classify the devirtualization techniques into guarded devi on can overcome this limitation, but it requires an expensive mechanism called on-stack replacement <ref type=\"bibr\" target=\"#b28\">[28]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  tion based on static analysis requires type analysis, which in turn requires whole program analysis <ref type=\"bibr\" target=\"#b28\">[28]</ref>, and unsafe languages like C\u00fe\u00fe also require pointer alias  or large applications. Due to the limited applicability of static devirtualization, Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> report only an average 40 percent reduction in the number  et=\"#b23\">[23]</ref>, and type feedback/devirtualization <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. As we show in Section 6, the benefit of devirtualization . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">15]</ref>, large Transformer based models <ref type=\"bibr\" target=\"#b31\">[32]</ref>, such as BERT <ref type=\"bibr\" target=\"#b5\">[6]</ref>, show substantially better effectiveness at the cost of orde hese Transformer layers are the building blocks of versatile multi-task architectures, such as BERT <ref type=\"bibr\" target=\"#b5\">[6]</ref> and XLNet <ref type=\"bibr\" target=\"#b38\">[39]</ref>. These m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: academic communities <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" tar /head><p>GCNs are showing great potential in various tasks <ref type=\"bibr\" target=\"#b16\">[16,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" tar ted by recursively aggregating and transforming the representation vectors of its neighbor vertices <ref type=\"bibr\" target=\"#b18\">[18,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" ta to sample a subset from the neighbor vertices of each vertex <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b18\">18]</ref> as the new neighbors, specifically,</p><formula xml:id=\"for ling to alleviate receptive field expansion that effectively trades off accuracy and execution time <ref type=\"bibr\" target=\"#b18\">[18]</ref>. It is formulated as</p><formula xml:id=\"formula_4\">a k v  ing preprocessing <ref type=\"bibr\" target=\"#b20\">[20]</ref> or with random selection during runtime <ref type=\"bibr\" target=\"#b18\">[18]</ref>. Aggregation aggregates the features from its 1-hop neighb he execution time breakdown of GCN (GCN) <ref type=\"bibr\" target=\"#b25\">[25]</ref>, GraphSage (GSC) <ref type=\"bibr\" target=\"#b18\">[18]</ref>, and GINConv (GIN) <ref type=\"bibr\" target=\"#b39\">[39]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t in time. This is especially true for Long Short Term Memory (LSTM) networks-a popular type of RNN <ref type=\"bibr\" target=\"#b14\">[16]</ref>.</p><p>Recurrent neural networks are competitive or state- =\"formula_3\">h 0 h 1 h 2 h 3 h T x 1 x 2 x 3 x T</formula><p>Long Short Term Memory (LSTM) networks <ref type=\"bibr\" target=\"#b14\">[16]</ref> are a more complex variant of RNNs that often prove more p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ESPnet also uses Kaldi feature extraction for most of recipes, although multichannel end-to-end ASR <ref type=\"bibr\" target=\"#b30\">[31]</ref> includes speech enhancement and feature extraction with it. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that deep neural networks can be used to learn cross-media or share representations in speech video <ref type=\"bibr\" target=\"#b12\">[13]</ref> or images with text tag <ref type=\"bibr\" target=\"#b13\">[14 arn feature within each single modality with few correlation connection between multiple modalities <ref type=\"bibr\" target=\"#b12\">[13]</ref>. We train the CAE with a cropped set of data that input fr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar ef type=\"bibr\" target=\"#b14\">[15]</ref> and style transfer <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>, however these meth . Alternative perceptual losses have been proposed for CNNs <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> where the idea is to shift the loss from the image-space to  target=\"#b28\">[29]</ref> developed an approach that is similar to ours: inspired by Johnson et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref>, they train feed-forward CNNs using a perceptual loss in c beit not pixel-perfect reproductions.  <ref type=\"bibr\" target=\"#b31\">[32]</ref> and Johnson et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> since feed-forward fully convolutional neural networks exh ce</head><p>Dosovitskiy and Brox <ref type=\"bibr\" target=\"#b9\">[10]</ref> as well as Johnson et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> propose a perceptual similarity measure. Rather than compu ts exhibit the same characteristics as previous approaches. The perceptual loss from Johnson et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> produces only a slightly sharper image than ENet-E. On the tly sharper images with realistic textures. Comparisons with further works including Johnson et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Bruna et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> and  d with VGG, but likely a result of sharper images. Best results shown in bold.</p><p>Johnson et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> ENet-PAT IHR Figure <ref type=\"figure\">6</ref>. Comparing  et-PAT IHR Figure <ref type=\"figure\">6</ref>. Comparing our model with a result from Johnson et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> on an image from BSD100 at 4x super-resolution. ENet-PAT's etwork that is able to synthesize a global texture (e.g., a given painting style) onto other images <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b52\">53]</ref>, however a single . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ble research attention <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. State-of-the-art GCNs usually follow a \"message-passing\" f ion of the Laplacian matrix is avoided, thus reducing the overall time complexity. Kipf and Welling <ref type=\"bibr\" target=\"#b17\">[18]</ref> further propose to simplify the graph convolution using on  GCN methods have been proposed, here we focus on a representative one proposed by Kipf and Welling <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Here, the (l + 1) t h convolutional layer is defined as:< istributions by using our Gaussian-based Graph Convolutions.</p><p>Following the original GCN model <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we also impose L 2 regularization on parameters of the fi To evaluate the robustness of RGCN, we compare it with two state-of-the-art GCN models:</p><p>\u2022 GCN <ref type=\"bibr\" target=\"#b17\">[18]</ref>: As introduced in Section 3.2 , this is the original GCN m  methods is evaluated on a separate test set of 1000 labels. We adopt the same dataset splits as in <ref type=\"bibr\" target=\"#b17\">[18]</ref> and report the average results of 10 runs. In experiments, ectiveness of our proposed method, we adopt three citation networks commonly used in previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>: Cora, Citeseer an \"table\" target=\"#tab_0\">1</ref>.</p><p>We closely follow the experimental setting in previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Specifically, we  . In experiments, we set the number of layers as two for all methods as suggested by previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For GCN and RGCN,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed somewhat implicit until residual networks <ref type=\"bibr\" target=\"#b5\">(He et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>) explicitly introduced a reparameterization of  ead><p>Since the advent of residual networks <ref type=\"bibr\" target=\"#b5\">(He et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>), most state-of-the-art networks for image clas d of a sequence of such residual blocks. In comparison with the full pre-activation architecture in <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>, we remove two batch normalization layers and o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b16\">(Kipf and Welling, 2016;</ref><ref type=\"bibr\" target=\"#b6\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Veli\u010dkovi\u0107 et al., 2017)</ref> have received growing attentions in gr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nteraction-focused <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or representation-focused <ref type=\"bibr\" target=\"#b14\">[1 h positions. It is also similar to the indicator matching matrix proposed previously by Pang et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref>. While the interaction matrix X perfectly captures every q <ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>.</p><p>Pang et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref> propose the use of matching matrices to represent the simi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type=\"bibr\" target=\"#b21\">Shi et al. (2016)</ref> replace convolution filters with deep LSTM, w of recurrent units. We find that using GRU as recurrent units outperforms LSTM which is utilized by <ref type=\"bibr\" target=\"#b21\">Shi et al. (2016)</ref>.</p><formula xml:id=\"formula_0\">w 1 w 2 w 3 w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b18\">[22,</ref><ref type=\"bibr\" target=\"#b20\">24,</ref><ref type=\"bibr\" target=\"#b21\">25,</ref><ref type=\"bibr\" target=\"#b38\">42]</ref>. These are mainly (1) to ease the customizing and debugging. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> and speech generation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> tasks. VAE has man ws the good performance of this method.</p><p>We have become aware of recent work by Akuzawa et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> which combines an autoregressive speech synthesis model wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  unknown true labels and some behavior assumptions, with examples of the Dawid-Skene (DS) estimator <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the minimax entropy (Entropy) estimator<ref type=\"foot\" tar ://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Dawid-Skene Estimator</head><p>The method of Dawid and Skene <ref type=\"bibr\" target=\"#b4\">[5]</ref> is a generative approach by considering worker confusability ed majority voting (IWMV) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the Dawid-Skene (DS) estimator <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and the minimax entropy (Entropy) estimator <ref type=\"bibr m c = 2\u02c6[\u22128 : 0] and = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> by the method in Sec. 6.2. As for Gibbs-CrowdSVM, we generate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">[41]</ref>, and graph pattern mining <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. Although many techniques such as search order optimization raphs from data graphs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. Graph mining is essential for several problems involving t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  popular recently with the boom of recurrent neural networks <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target t al. <ref type=\"bibr\" target=\"#b8\">[9]</ref> proposed the GRU4REC, which applies a multi layer GRU <ref type=\"bibr\" target=\"#b1\">[2]</ref> to simply treat the data as time series. Based on the RNN mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rich and rule-based approaches rely on corpus based gender statistics mined from external resources <ref type=\"bibr\" target=\"#b0\">(Bergsma and Lin, 2006)</ref>. Such lists were generated from large un. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, computer vision <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, natural language . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: CRF) <ref type=\"bibr\" target=\"#b5\">[6]</ref> , Decision Tree <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> , Support Vector Machine (SVM) <ref type=\"bibr\">[9~12]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar  explicit approaches <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" tar diversity of results <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Most explicit appr explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> and PM2 <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the relationship between the preference factors and the disentangled embeddings.</p><p>According to <ref type=\"bibr\" target=\"#b30\">(Yang et al., 2018)</ref>, the mutual information maximization can be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: foot_3\">4</ref> constructed by <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Foursquare obtained from <ref type=\"bibr\" target=\"#b32\">[33]</ref> contain implicit feedback through user-venue check-ins. Tm  are much weaker than the above data sets.</p><p>Following <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>, we hold the rst 70. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ut the sequence at every step in the input. For brevity, the details of LSTM equations are given in <ref type=\"bibr\" target=\"#b2\">[Gers et al., 1999]</ref>. The conditional random field (CRF) <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: neural IR models unsuccessfully tried to match single vector representations per query and document <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Then, interaction-focused models moved to a more fine-gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stic depth for the Transformer inspired by the Stochastic Residual Network for image classification <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>We discovered that its ability to regularize is the  r\" target=\"#b15\">[16]</ref>, and thus there are redundant layers. Motivated by the previous work of <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we propose to apply stochastic residual layers into our Tr  sub-layers inside). This way we have one hyper-parameter p for each layer.</p><p>\u2022 As suggested by <ref type=\"bibr\" target=\"#b9\">[10]</ref>, the lower layers of the networks handle raw-level acoustic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ure is based on recent attention-based end-to-end ASR models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and TTS models such as Tacotron <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to overcome the sensitivity to cluster shapes and scales of Euclidean distance in the feature space <ref type=\"bibr\" target=\"#b23\">[24]</ref> . FCM_S algorithm introduces a constraint in image domain,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mimic large-scale multithreaded commercial programs. Studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> have shown that the suite covers a wide range of working set  d characterization and performance measurement are relevant to this current work. Bienia and others <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> have shown a detailed . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nty sampling methods <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> query by committee me. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the coverage analysis to automatically modify the directives to the test generator. For example, in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a genetic algorithm is used to select and modify test-cases. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 018)</ref>, datato-document generation <ref type=\"bibr\" target=\"#b21\">(Moryossef et al., 2019;</ref><ref type=\"bibr\" target=\"#b15\">Koncel-Kedziorski et al., 2019)</ref> and interpretability of KGs in  mer.</p><p>Following previous work, we evaluate Graformer on two benchmarks: (i) the AGENDA dataset <ref type=\"bibr\" target=\"#b15\">(Koncel-Kedziorski et al., 2019)</ref>, i.e., the generation of scien ead><p>We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA <ref type=\"bibr\" target=\"#b15\">(Koncel-Kedziorski et al., 2019)</ref> and WebNLG <ref type=\"bibr\" ta ecoder architectures <ref type=\"bibr\" target=\"#b20\">(Marcheggiani and Perez-Beltrachini, 2018;</ref><ref type=\"bibr\" target=\"#b15\">Koncel-Kedziorski et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">R rms previous Transformerbased models that only consider first-order neighborhoods per encoder layer <ref type=\"bibr\" target=\"#b15\">(Koncel-Kedziorski et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s have been achieved dramatic improvement in SR. Dong et al. <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> first exploit a three-layer convolutional neural network, nam posed method with other SR methods, including bicubic, SRCNN <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRC-N <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: graph).</p><p>Our method also connects to PinSage <ref type=\"bibr\" target=\"#b20\">[21]</ref> and GAT <ref type=\"bibr\" target=\"#b14\">[15]</ref>. But note that both PinSage and GAT are designed for homog ula><p>(5) 2 The knowledge graph G is treated undirected. 3 Technically, S(v) \u2022 Neighbor aggregator <ref type=\"bibr\" target=\"#b14\">[15]</ref> directly takes the neighborhood representation of entity v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: icient message passing across distant nodes. Current methods <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> resort to random walks to generate neighborhoods of various  hoods for the GCN update in a similar way as the random walk <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. However, the random walk requires extra sampling to obtain  cient to deliver promising performance in our experiments.</p><p>To minimize the hybrid loss in Eq. <ref type=\"bibr\" target=\"#b9\">(10)</ref>, it requires to perform gradient calculations. For the netw e reduction, we implement a variant of our model by setting the trade-off parameter as \u03bb = 0 in Eq. <ref type=\"bibr\" target=\"#b9\">(10)</ref>. By this, the parameters of the self-dependent function are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to a conventional design due to an increase in the access time on a filter cache miss. The L-Cache <ref type=\"bibr\" target=\"#b5\">[6]</ref> similarly reduces switching activity by holding loop-nested . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: )</ref> proposed FloWorM system that includes tracker, analyzer and reporter based on NetFlow data. <ref type=\"bibr\" target=\"#b5\">Abdulla et al. (2011)</ref> presented a support vector machine (SVM) m nti and Rossi, 2011)</ref>, or data fusion with other log files such as Snort, DNS related requests <ref type=\"bibr\" target=\"#b5\">(Abdulla et al., 2011)</ref> (number of DNS requests, response, normal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: raining dataset is small, or because the quality of the dataset should be improved?</p><p>Recently, <ref type=\"bibr\" target=\"#b31\">Ning et al. (2018c)</ref> introduced a new dataset called Multi-Axis  lity or neural methods inherently do not work well for this task.</p><p>A recent annotation scheme, <ref type=\"bibr\" target=\"#b31\">Ning et al. (2018c)</ref>, introduced the notion of multi-axis to rep raction prop-3 Between experts: Kohen's \uf8ff \u21e1 0.84. Among crowdsourcers: accuracy 88%. More details in<ref type=\"bibr\" target=\"#b31\">Ning et al. (2018c)</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.or. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent label prediction <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Adversarial examples have been shown to be ubiquitous beyo \" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>. Among them, adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> is one of the most side in a large, contiguous region and a significant portion of the adversarial subspaces is shared <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta e sense of robustness against adversarial attacks due to gradient masking, and adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta se method against adversarial attacks. It improves model robustness by solving a minimax problem as <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>:</p><formula xml:i he proposed formulation deviates from the conventional minimax formulation for adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. More specifically  \u2261 i L \u03b8 (x i , y i ), the proposed approach reduces to the conventional adversarial training setup <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. The overall proce gn method (FGSM) for adversarial attack generation is developed and used in adversarial training in <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Many variants of attacks have been developed later <ref t  inner maximization can be solved approximately, using for example a one-step approach such as FGSM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, or a multi-step projected gradient descent (PGD) method < y measuring the accuracy of the model under different adversarial attacks, including white-box FGSM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, PGD <ref type=\"bibr\" target=\"#b35\">[36]</ref>, CW <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. In the pioneering work, Glasner et al. <ref type=\"bibr\" ta  localized region and thus can greatly reduce computation time. Following this fashion, Yang et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref> proposed a very fast regression model that focused on only. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: an knowledge about the extraction. For a general introduction of first-order logic, please refer to <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Complete consistency describes the fact that the va. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We adopt larger input data sets varying from 147 to 187 G  both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in the memory system. And for each workload, we collect t  performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type=\"bibr\" target=\"#b16\">[17]</ref>).</p><p>We find that big data analytics applications share chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type=\"bibr\" target=\"#b16\">[17]</ref>) workloads. Meanwhile the service workloads in data center s, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>\u2022 Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and service workloa the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type=\"bibr\" target=\"#b16\">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall b vel cache), respectively. For the service workloads, our observations corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><p>\u2022 For the big data ana rk of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, CloudSuite paper is biased towards online servic . The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, our approach are more pragmatic. We adopt a larger data i  in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in memory. The number of instructions retired of the big  , HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and compared them with big data analytics workloads.</p>< as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the o n fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The front end inefficiency may caused by high-level langu  Figure <ref type=\"figure\" target=\"#fig_3\">5</ref>.</p><p>Implications: Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and the service wor ns, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and large binary size complicated by high-level language, sor and save the die area. For the service workloads, our observation corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id=\"formula_ s in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type=\"bibr\" target=\"#b16\">[17]</ref>. According to our correlation analysis in this section, ar ten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Moreover, for mod s prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and etc. Narayanan et al. <ref type=\"bibr\" target=\"#b32\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2017;</ref><ref type=\"bibr\" target=\"#b34\">Veli\u010dkovi\u0107 et al., 2018)</ref> or variants of Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref> that apply self-attention on all nodes t ype=\"bibr\" target=\"#b7\">Cai and Lam, 2020)</ref> base their encoder on the Transformer architecture <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref> and thus, in each layer, compute self-at raformer follows the general multi-layer encoderdecoder pattern known from the original Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>. In the following, we first describe our  computations for one head. The output of multiple heads is combined as in the original Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>.</p><p>Text self-attention. <ref type=\"b ead n=\"3.4\">Graformer decoder</head><p>Our decoder follows closely the standard Transformer decoder <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>, except for the modifications suggested  of the ith node's label.</p><p>To compute the node representation H (L) in the Lth layer, we follow <ref type=\"bibr\" target=\"#b33\">Vaswani et al. (2017)</ref>, i.e., we first normalize the input from . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: as the training progresses <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Madry, Makelov, Schmidt, et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> used adversarial training on the cifar dataset, which stil gest known attack for this metric. PGD has been conjectured to be a near-optimal first-order attack <ref type=\"bibr\" target=\"#b24\">[25]</ref>. We use the Fo olBox library for the implementation of the we also wish to address ac o n c e r nr a i s e db yM a d r y ,M a k e l o v ,S c h m i d t ,et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which is the computational cost of a threat model. They a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b27\">28]</ref> or multi-atlas techniques <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. In particular, atlas approaches benefit from implicit shap DSC) for atlas-based frameworks ranges from 69.6% to 73.9% <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. In <ref type=\"bibr\" target=\"#b38\">[39]</ref> a classificat le being an order of magnitude faster than, e.g., graph-cut and multi-atlas segmentation techniques <ref type=\"bibr\" target=\"#b33\">[34]</ref>. This is mainly attributed to the fact that (I) domain spe ble <ref type=\"table\" target=\"#tab_2\">3</ref>. U-Net model outperforms traditional atlas techniques <ref type=\"bibr\" target=\"#b33\">[34]</ref> although it was trained on a disjoint dataset. Moreover, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -range in 3D space <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref>. By making the graph and self-attention similarly sparse a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b3\">[4]</ref> and Neural Network structure with Connectionist temporal classification (CTC) loss <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The hybrid hidden   by the acoustic model, and finally get better results. y * = arg max y log p(y|x) + \u03bb log P LM (y) <ref type=\"bibr\" target=\"#b4\">(5)</ref> where P LM (y) is provided by the LM, y * denotes the final . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ication citation MiCo <ref type=\"bibr\" target=\"#b13\">[14]</ref> 96638 1080156 Co-authorship Patents <ref type=\"bibr\" target=\"#b25\">[26]</ref> 3.8M 16.5M US Patents LiveJournal-1 <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ge R-CNN framework <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, where detection is et=\"#b12\">[13]</ref> introduced the idea of region-wise feature extraction. Later, the Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[30]</ref> achieved further speeds-up by introducing a Region Proposa  network. SSD <ref type=\"bibr\" target=\"#b24\">[25]</ref> detects objects in a way similar to the RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, but uses multiple feature maps at different resolutions t We focus on modeling a multistage detection sub-network, and adopt, but are not limited to, the RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref> for proposal detection.</p></div> <div xmlns=\"http://www.t evels. At inference, since the majority of the hypotheses produced by a proposal detector, e.g. RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref> or selective search <ref type=\"bibr\" target=\"#b32\">[33]</r \">Object Detection</head><p>In this paper, we extend the two-stage architecture of the Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, shown in Figure < zed by its mean and variance, i.e. is replaced by \u2032 =( \u2212 )/ . This is widely used in the literature <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ PN+.</p><p>Detection Performance: Again, our implementations are better than the original detectors <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ  was further experimented on PAS-CAL VOC dataset <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, the models were t e noted. The sampling of the first detection stage follows <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. In the following stages, resampling is implemented by simp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch, their technique follows the general pseudo-ensemble agreement (PEA) regularization framework of <ref type=\"bibr\" target=\"#b0\">Bachman et al. (2014)</ref>. In addition, they employ a mutual exclusi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: label field is one of the most effective way to construct the connection between neighboring pixels <ref type=\"bibr\" target=\"#b4\">[5]</ref> . It decreases with the number of pixels having the same lab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dio fidelity using a much simplified voice building pipeline <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. However, such models t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed representation learning, has been proven to be successful techniques contributing to the success <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In essence, embedding is a way to represent a sparse vector. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uccessful use of CNNs in image tasks, a newly proposed sequential recommender, referred to as Caser <ref type=\"bibr\" target=\"#b28\">[29]</ref>, abandoned RNN structures, proposing instead a convolution la\">3</ref>) ( see <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>).</p><formula xml:i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum <ref type=\"bibr\" target=\"#b19\">(Sutskever et al., 2013)</ref> and Adagrad <ref type=\"bibr\" target=\"# </ref>, using 5 concurrent steps on each of 10 model replicas, using asynchronous SGD with momentum <ref type=\"bibr\" target=\"#b19\">(Sutskever et al., 2013)</ref>, with the mini-batch size of 32. All n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr rkable progresses have been witnessed for object detection <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: that this might not be the best choice. Instead, we use a recently released sentence-level test set <ref type=\"bibr\" target=\"#b14\">(Ren et al., 2017)</ref> for evaluation. However, there also exist se. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vals must wait until the next core visit. This policy is usually known as \"gated M -limited\" policy <ref type=\"bibr\" target=\"#b13\">[13]</ref>. To complete the processing of a packet, a core needs at l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: of high inter-class similarities. Most recently, an additional center loss was introduced into CNNs <ref type=\"bibr\" target=\"#b43\">[44]</ref> to reduce the intra-class variations of the learned featur suffers from drastic data expansion when constructing image pairs from the training set. Wen et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> introduced a center loss for face recognition, which targe eview of Center Loss</head><p>As illustrated in Fig. <ref type=\"figure\">1</ref>(b), the center loss <ref type=\"bibr\" target=\"#b43\">[44]</ref> explicitly reduces the intra-class variations by pushing s  the CNN training.</p><p>1) Forward propagation: The center loss denoted as L C is defined in Eq. 1 <ref type=\"bibr\" target=\"#b43\">[44]</ref> as the summation of squared distances between samples and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /p><p>One recent work has proposed a matrix-factorizationbased approach to embed uncertain networks <ref type=\"bibr\" target=\"#b9\">(Hu et al. 2017</ref>). However, it cannot be generalized to embed unc pe=\"bibr\" target=\"#b20\">(Trouillon et al. 2016)</ref>, (ii) an uncertain graph embedding model URGE <ref type=\"bibr\" target=\"#b9\">(Hu et al. 2017)</ref> Here linear stands for linear gain, and exp. st. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  give a brief introduction to GNNs, and one can refer to <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> for a more detailed information. GNNs deal with learning p able.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Gra here u \u2208 N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>: It uses the MLP and sum pooling as the aggregation and co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r training neural networks in the reinforcement learning setting, such as neural fitted Q-iteration <ref type=\"bibr\" target=\"#b18\">24</ref> , these methods involve the repeated training of networks de ters on each game, privy only to the inputs a human player would have. In contrast to previous work <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> , our approach incor e history and the action have been used as inputs to the neural network by some previous approaches <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> . The main drawback . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: erial images and LiDAR data. The developed network is based on a modified residual learning network <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> that extracts robust low/mid/high-level featu res in the input images <ref type=\"bibr\" target=\"#b50\">(Zhang et al., 2016)</ref>. Previous studies <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> have found that increasing the depth of neura e=\"figure\" target=\"#fig_2\">1</ref>).</p><p>A more detailed description of ResNet-50 can be found in <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> and here ResNet-50 is modified as follows to  cy may degrade after a saturation. This phenomenon is often referred to as the degradation problem. <ref type=\"bibr\" target=\"#b13\">He et al. (2016)</ref> recently proposed a Residual Network (ResNet) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l language understanding and generation tasks <ref type=\"bibr\" target=\"#b9\">(Dai et al., 2019;</ref><ref type=\"bibr\" target=\"#b38\">Shaw et al., 2018)</ref>. The proposed Disentangled Attention mechani tent, and position-to-position<ref type=\"foot\" target=\"#foot_0\">1</ref> .</p><p>Existing approaches <ref type=\"bibr\" target=\"#b38\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Huang et al., LEMENTATION</head><p>For an input sequence of length N , it requires a space complexity of OpN 2 dq <ref type=\"bibr\" target=\"#b38\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Huang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b23\">[24]</ref> and Schulter et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref> use the BSD500 dataset <ref type=\"bibr\" target=\"#b24\">[25]</ref>. However, images in the BSD500 are in JPEG format, which a <ref type=\"bibr\" target=\"#b14\">[15]</ref>, Set14 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and BSD200 <ref type=\"bibr\" target=\"#b24\">[25]</ref> dataset for testing. Another 20 images from the validation <ref type=\"bibr\" target=\"#b29\">[30]</ref>, Set14 <ref type=\"bibr\" target=\"#b8\">[9]</ref> and BSD200 <ref type=\"bibr\" target=\"#b24\">[25]</ref>   </p></div><figure xmlns=\"http://www.tei-c.org/ns/1.0\" xm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erformance of an end-to-end ST model. Synthetic data has also been used to improve ASR performance. <ref type=\"bibr\" target=\"#b16\">[17]</ref> builds a cycle chain between TTS and ASR models, in which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: meaningful knowledge, and reason with it <ref type=\"bibr\">(Etzioni, Banko, and Cafarella 2006;</ref><ref type=\"bibr\" target=\"#b16\">Hermann et al. 2015;</ref><ref type=\"bibr\" target=\"#b44\">Weston et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hniques in tasks like gene name recognition <ref type=\"bibr\" target=\"#b9\">(Kuksa and Qi, 2010;</ref><ref type=\"bibr\" target=\"#b23\">Tang et al., 2014;</ref><ref type=\"bibr\" target=\"#b24\">Vlachos and Ga. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arn both long-term interests and short-term interests of such implicit feedbacks. As Jannach et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref> noted that both the users' short-term and long-term interest. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b13\">[14]</ref>, which was shown to be useful in large-scale conditional generation tasks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b32\">34]</ref>.</p><p>The model-agn \"#b17\">[19]</ref>, but replace downsampling and upsampling layers with residual blocks similarly to <ref type=\"bibr\" target=\"#b5\">[6]</ref> (with batch normalization [15] replaced by instance normaliz tional and fully connected layers in all the networks. We also use self-attention blocks, following <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b40\">[42]</ref>. They are inse. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b17\">[18]</ref>, which makes the algorithm not robust.</p><p>We note that in previous researches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta .tei-c.org/ns/1.0\"><head n=\"4\">Convergence Analysis</head><p>We first introduce a lemma proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref>:</p><p>Lemma 1. For any arbitrary vector m and n there is<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead n=\"3.1\">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation <ref type=\"bibr\" target=\"#b49\">(Sennrich et al., 2016)</ref> in NMT, referring to training a backwar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  complete KGs, extensive research efforts <ref type=\"bibr\" target=\"#b21\">(Nickel et al., 2011;</ref><ref type=\"bibr\" target=\"#b2\">Bordes et al., 2013</ref>  et <ref type=\"bibr\">al., 2014;</ref><ref ty t al., 2011)</ref> is one of the earlier work that models the relationship using tensor operations. <ref type=\"bibr\" target=\"#b2\">Bordes et al. (2013)</ref> proposed to model relationships in the 1-D  local connections in knowledge graph.</p><p>Although the entity embeddings from KG embedding models <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al.,  embedding-based methods: RESCAL <ref type=\"bibr\" target=\"#b21\">(Nickel et al., 2011)</ref>, TransE <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013)</ref>, Dist-Mult <ref type=\"bibr\" target=\"#b38\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nt training examples, thus results are much worse for elements with a few instances during training <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2019c)</ref>. However, few-shot problem widely exists . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Towards video recommendation, Hamilton et al. <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ments to processor utilization -long latencies and limited per-thread parallelism. Tullsen, et al., <ref type=\"bibr\" target=\"#b26\">[27]</ref> showed the potential of Proceedings of the 23rd Annual Int head><p>The methodology in this paper closely follows the simulation and measurement methodology of <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Our simulator uses emulationbased, instruction-level simu  compiler <ref type=\"bibr\" target=\"#b16\">[17]</ref>, modified to produce Alpha code. In contrast to <ref type=\"bibr\" target=\"#b26\">[27]</ref>, we turn off trace scheduling in the compiler for this stu sor utilization, at less than 50% of the 8-issue processor, is well short of the potential shown in <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>We make several conclusions about the potential bot shion here) that simultaneous multithreading uses to improve the throughput of the functional units <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  tectures have been proposed that exhibit simultaneous multithreading in some form. Tullsen, et al., <ref type=\"bibr\" target=\"#b26\">[27]</ref> demonstrated the potential for simultaneous multithreading. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e for the 'happy' category while the others not. In this paper, inspired by the attention mechanism <ref type=\"bibr\" target=\"#b13\">[14]</ref> of machine translation and the neural aggregation networks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cally detect psychological stress from social networks. <ref type=\"bibr\" target=\"#b3\">[4]</ref> and <ref type=\"bibr\" target=\"#b4\">[5]</ref> presented methods to detect psychological stress from forum . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ase of it. The \u03a0-model can also be seen as a simplification of the \u0393-model of the ladder network by <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref>, a previously presented network architectu he data is obtained.</p><p>Our approach is somewhat similar to the \u0393-model of the ladder network by <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref>, but conceptually simpler. In the \u03a0-model, ed the issue by shuffling the input sequences in such a way that stratification is guaranteed, e.g. <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref> (confirmed from the authors). This kind of he ones that are most directly connected to our work.</p><p>\u0393-model is a subset of a ladder network <ref type=\"bibr\" target=\"#b17\">(Rasmus et al., 2015)</ref> that introduces lateral connections into . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b2\">[3]</ref>, protein interfaces <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and 3D meshes <ref type=\"bibr\" target=\"#b4\">[5]</ref>. How to define convolutional operations on graphs is still a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b6\">(Graves et al., 2014;</ref><ref type=\"bibr\" target=\"#b0\">Bahdanau et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015)</ref>. Our approach is data-driven, computat s of memory network in question answering <ref type=\"bibr\" target=\"#b35\">(Weston et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015)</ref>. We describe the background on memory  ple hops is that more abstractive evidences could be found based on previously extracted evidences. <ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al. (2015)</ref> demonstrate that multiple hops could u ion information in the attention model. The details are described below.</p><p>\u2022 Model 1. Following <ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al. (2015)</ref>, we calculate the memory vector m i wi gure\" target=\"#fig_0\">1</ref>, which is inspired by the use of memory network in question answering <ref type=\"bibr\" target=\"#b27\">(Sukhbaatar et al., 2015)</ref>. Our approach consists of multiple co r\" target=\"#b6\">(Graves et al., 2014;</ref><ref type=\"bibr\" target=\"#b35\">Weston et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b0\">Bahdanau . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: is nondifferentiable. Therefore, for S with model parameters \u03a6, we use the policy gradient based RL <ref type=\"bibr\" target=\"#b13\">[Sutton et al., 2000]</ref> to derive its gradient:</p><formula xml:i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. PGDtype methods are considered the strongest attacks based b7\">[8]</ref>). We choose to use the least-likely  class projected gradient descent (LL-PGD) method <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>. LL-PGD is an iter. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s <ref type=\"bibr\" target=\"#b14\">[15]</ref>, dynamic predication based on frequently executed paths <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and predicate prediction <ref type=\"bibr\" target=\"#b25\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  methods train image embeddings through the local relationships between images in the form of pairs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> or triplets <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b20\">[21]</ref>. Recently, with the great impact of neural networks on computer vision <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and natural langua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: loited to convert the classification clues (such as sequences and parse trees) into feature vectors <ref type=\"bibr\" target=\"#b9\">(Kambhatla, 2004;</ref><ref type=\"bibr\" target=\"#b15\">Suchanek et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >1</ref> depicts an example of 3-shot link prediction in KGs.</p><p>To do few-shot link prediction, <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref> made the first trial and proposed GMatching ed parameters, it's like \"a gradient through a gradient\".</p><p>As far as we know, work proposed by <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref> is the first research on few-shot learning  and Evaluation Metrics</head><p>We use two datasets, NELL-One and Wiki-One which are constructed by <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref>. NELL-One and Wiki-One are derived from NEL  simple TransE embedding model, denoted as -g -r. The result under the third setting is copied from <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref>. It uses the triples from background graph, e transferring relation meta to incomplete triples during prediction.</p><p>Compared with GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018</ref>) which relies on a background knowledge gra e heavily rely on rich training instances <ref type=\"bibr\" target=\"#b26\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b22\">Xiong et al., 2018)</ref>, thus are limited to do few-shot link predi r\" target=\"#b17\">Vinyals et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Snell et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Xiong et al., 2018)</ref>, which tries to learn a matching metric bet f> is a typical method using symmetric twin networks to compute the metric of two inputs. GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018)</ref>, the first trial on one-shot link predicti own in Table <ref type=\"table\" target=\"#tab_6\">4</ref>. The baseline in our experiment is GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018)</ref>, which made the first trial on few-shot li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[28]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref>, have made sign o existing dimensions of depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>, width 2 , and cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref>. We state in Sec. 4.4 that increasing scale is more effect rformance of state-of-the-art CNNs, e.g., ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref>.</p><p> \">[28]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref>, achieving stat modern backbone CNNs architectures, e.g., ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref>. Instea odules have been proposed in recent years, including cardinality dimension introduced by Xie et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, as well as squeeze and excitation (SE) block presented by nts. As shown in Fig. <ref type=\"figure\">3</ref>, we can easily integrate the cardinality dimension <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the SE block <ref type=\"bibr\" target=\"#b24\">[25]</ref> sion cardinality.</head><p>The dimension cardinality indicates the number of groups within a filter <ref type=\"bibr\" target=\"#b55\">[56]</ref>. This dimension changes filters from single-branch to mult ig. <ref type=\"figure\">3</ref>: The Res2Net module can be integrated with the dimension cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> (replace conv with group conv) and SE <ref type=\"bibr\" tar  into the state-ofthe-art models, such as ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref> and Big-Lit and bLRes2Net-50, respectively.</p><p>The proposed scale dimension is orthogonal to the cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> dimension and width <ref type=\"bibr\" target=\"#b22\">[23]</r 4]</ref> dataset, we mainly use the ResNet-50 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt-50 <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA-60 <ref type=\"bibr\" target=\"#b59\">[60]</ref>, and bLR ments on the CIFAR <ref type=\"bibr\" target=\"#b26\">[27]</ref> dataset, we use the ResNeXt-29, 8c\u00d764w <ref type=\"bibr\" target=\"#b55\">[56]</ref> as our baseline model. Empirical evaluations and discussio ons, we use the Pytorch implementation of ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref> as well as  type=\"bibr\" target=\"#b22\">[23]</ref>. On the CIFAR dataset, we use the implementation of ResNeXt-29 <ref type=\"bibr\" target=\"#b55\">[56]</ref>. For all tasks, we use the original implementations of bas ement of 0.73% in terms of top-1 error over the  <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, SE-Net <ref type=\"bibr\" target=\"#b24\">[25]</ref>, bLResNe ve been shown to have stronger representation capability <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref> for vision tasks. To validate our model with greater depth  which contains 50k training images and 10k testing images from 100 classes. The ResNeXt-29, 8c\u00d764w <ref type=\"bibr\" target=\"#b55\">[56]</ref> is used as the baseline model. We only replace the origina iv xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Scale Variation</head><p>Similar to Xie et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, we evaluate the test performance of the baseline model by ng different CNN dimensions, including scale (Equation ( <ref type=\"formula\">1</ref>)), cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>. Whil fix all other dimensions. A series of networks are trained and evaluated under these changes. Since <ref type=\"bibr\" target=\"#b55\">[56]</ref> has already shown that increasing cardinality is more effe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esteps the complicated machinery developed for classical ASR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= w.tei-c.org/ns/1.0\"><head n=\"2.1.\">Listen, Attend and Spell</head><p>Listen, Attend and Spell (LAS) <ref type=\"bibr\" target=\"#b2\">[3]</ref> is an attention-based seq2seq model which learns to transcri TM) <ref type=\"bibr\" target=\"#b24\">[25]</ref> network with hierarchical subsampling as described in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. In our work, we replace Listen with a network of very deep   lower dimension and apply BN and ReLU non-linearity to replace the skip subsampling connections in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Moreover, we further increase the depth of the network by a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Neural Machine Translation (NMT; <ref type=\"bibr\" target=\"#b12\">Cheng et al., 2018)</ref>, and ASR <ref type=\"bibr\" target=\"#b45\">(Sperber et al., 2017)</ref>, we propose two Noise-Aware Training (NA mputer vision <ref type=\"bibr\" target=\"#b27\">(Krizhevsky et al., 2012)</ref> and speech recognition <ref type=\"bibr\" target=\"#b45\">(Sperber et al., 2017)</ref>.</p><p>During training, we artificially . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e their remarkable performance, recent studies show that GCNs are vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, i.e. carefully desi bibr\" target=\"#b37\">38]</ref> try to attack the model by changing training data and evasion attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> try to generate fake after (evasion attacks) the training phase of GCNs. \u2022 Targeted or Non-targeted. In targeted attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, the attacker focus  eted attacks can be further divided into two categories based on attack settings. In direct attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, the attacker can di r example, the attacker tends to connect nodes from different communities to confuse the classifier <ref type=\"bibr\" target=\"#b6\">[7]</ref>. While plain vectors cannot adapt to such changes, Gaussian   into the graph. We regard this method as an illustrating example of non-targeted attacks. \u2022 RL-S2V <ref type=\"bibr\" target=\"#b6\">[7]</ref> <ref type=\"foot\" target=\"#foot_2\">3</ref> : This method gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing instructions out of order. We believe that a solution similar to those proposed by Stark et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> and Cher et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> ca .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Related Work</head><p>Stark et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed a limited form of out-of-order instruction fetch . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b3\">(Han et al. 2004;</ref><ref type=\"bibr\" target=\"#b4\">Huang, Ertekin, and Giles 2006;</ref><ref type=\"bibr\" target=\"#b12\">Yoshida et al. 2010</ref>) usually leverage supervised learning algor t=\"#b4\">Huang, Ertekin, and Giles 2006;</ref><ref type=\"bibr\" target=\"#b8\">Louppe et al. 2016;</ref><ref type=\"bibr\" target=\"#b12\">Yoshida et al. 2010)</ref>, which usually solve the problem in a disc name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=\"bibr\" target=\"#b12\">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es with few contributions in a prediction. Inspired by the theory of hierarchical abstract machines <ref type=\"bibr\" target=\"#b15\">(Parr and Russell 1998)</ref>, we cast the task of profile reviser as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b10\">Das et al., 2018;</ref><ref type=\"bibr\">2017)</ref>, Pixel Deflection <ref type=\"bibr\" target=\"#b28\">(Prakash et al., 2018)</ref>, total variance minimization <ref type=\" ir et al., 2018;</ref><ref type=\"bibr\" target=\"#b10\">Das et al., 2018;</ref> 2017), Pixel Deflection<ref type=\"bibr\" target=\"#b28\">(Prakash et al., 2018)</ref>, total variance minimization<ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  returned by the first stage model. We shared the same spirit as the cascaded embedding training in <ref type=\"bibr\" target=\"#b17\">[18]</ref>, which ensembled a set of models trained with different le. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: been proposed.</p><p>A three-layers convolutional neural network is firstly proposed by Dong et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> called SRCNN. The SRCNN upscaled the low-resolution image wi DN on the DIV2K training images with random scale factor r \u2208 <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. We compare the Meta-RDN with the corresponding baseline RDN  /p><p>Suppose we want to zoom in the LR image with scale r \u2208 <ref type=\"bibr\" target=\"#b0\">(1,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Before we feed it into the network, we could upscale it with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .org/ns/1.0\"><head n=\"2.2\">MAML</head><p>We give an overview of Model-Agnostic Meta-Learning method <ref type=\"bibr\" target=\"#b11\">[12]</ref> which is a representative algorithm of optimization-based   problem, we propose to encode the information from support set into our parameter inspired by MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> and further we can obtain a category-specific model to acc n testing query data.</p><p>\u2022Meta-Learning We select two state-of-the-art meta learning models MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> and Meta-SGD <ref type=\"bibr\" target=\"#b20\">[21]</ref> as  ent based learning procedure for new task quick adaptation. In the optimization-based methods, MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> is a recent promising model which learns a set of model pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ges during training <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. An alternative to approximate the loss is to approximate t set as its own class <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. Dosovitskiy et al. <ref type=\"bibr\" target=\"#b15\">[16]</re ata. In addition, SwAV works with small and large batch sizes and does not need a large memory bank <ref type=\"bibr\" target=\"#b55\">[56]</ref> or a momentum encoder <ref type=\"bibr\" target=\"#b23\">[24]< h as many classes as images in the dataset. As this approach becomes quickly intractable, Wu et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref> mitigate this issue by replacing the classifier with a mem fferent augmentations of the same image. This solution is inspired by contrastive instance learning <ref type=\"bibr\" target=\"#b55\">[56]</ref> as we do not consider the codes as a target, but only enfo  feature. A similar comparison appears in contrastive learning where features are compared directly <ref type=\"bibr\" target=\"#b55\">[56]</ref>. In Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>, we il bel>2</label></formula><formula xml:id=\"formula_3\">)</formula><p>where \u03c4 is a temperature parameter <ref type=\"bibr\" target=\"#b55\">[56]</ref>. Taking this loss over all the images and pairs of data au cating pass forwards to the assignments. This is similar to the memory bank introduced by Wu et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, without momentum.</p><p>Assignment phase in DeepCluster-v .6 Image classification with KNN classifiers on ImageNet</head><p>Following previous work protocols <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>, we evaluate the q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b29\">30]</ref>. Our straw-man file system somewhat resembles timeline entanglement <ref type=\"bibr\" target=\"#b14\">[15]</ref>, which reasons about the temporal ordering of system state. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eature interactions from raw data automatically. A popular approach is factorization machines (FMs) <ref type=\"bibr\" target=\"#b26\">[27]</ref>, which embeds features into a latent space and models the  underlying structure, FM may not be expressive enough. Although higher-order FMs have been proposed <ref type=\"bibr\" target=\"#b26\">[27]</ref>, they still belong to the family of linear models and are  ing the second-order factorized interactions between features. By specifying input features, Rendle <ref type=\"bibr\" target=\"#b26\">[27]</ref> showed that FM can mimic many speci c factorization models  value, rather than simply an embedding table lookup, so as to account for the real valued features <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>Bi-Interaction Layer. We then feed the embedding se nsorFlow implementation<ref type=\"foot\" target=\"#foot_7\">7</ref> of higherorder FM, as described in <ref type=\"bibr\" target=\"#b26\">[27]</ref>. We experimented with order size 3, since the MovieLens da n Machines</head><p>Factorization machines are originally proposed for collaborative recommendation <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Given a real valu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: blockahead predictor <ref type=\"bibr\" target=\"#b34\">[35]</ref>, or the tree-like subgraph predictor <ref type=\"bibr\" target=\"#b7\">[8]</ref> can also be used to implement a fetch engine capable of prov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b9\">Daum\u00e9 III, 2009;</ref><ref type=\"bibr\" target=\"#b5\">Coke et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">Littell et al., 2017)</ref>.</p><p>In this study, we examine whether  h feature vectors from previous work based on the genetic and geographic distance between languages <ref type=\"bibr\" target=\"#b22\">(Littell et al., 2017)</ref>. Results show that the extracted represe up</head><p>Typology Database: To perform our analysis, we use the URIEL language typology database <ref type=\"bibr\" target=\"#b22\">(Littell et al., 2017)</ref>, which is a collection of binary feature not necessarily require pre-existing knowledge of the typological features in the language at hand, <ref type=\"bibr\" target=\"#b22\">Littell et al. (2017)</ref> have proposed a method for inferring typo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ry is not assumed; for a much more thorough treatment with all the measure theoretical details, see <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> and <ref type=\"bibr\" target=\"#b1\">Da fies the mean number of events in a region conditional on the past. Here we use the notation * from <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> to remind ourselves that this densit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  systems. A more detailed description of the protocol and a proof of its security were presented in <ref type=\"bibr\" target=\"#b12\">[14]</ref>, though the published version of that paper did not presen s modifications to the group in the PVL.</p><p>A proof of the consistency protocol was presented in <ref type=\"bibr\" target=\"#b12\">[14]</ref>, but is beyond the scope of this paper. At a high level, h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n the MDP state, which consists of the query, the preceding documents, and the remaining candidates <ref type=\"bibr\" target=\"#b32\">[33]</ref>.</p><p>The greedy sequential document selection simpli es  28]</ref> for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking <ref type=\"bibr\" target=\"#b32\">[33]</ref> with the Monte Carlo tree search (MCTS), for alleviating t anism <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Xia et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> proposed to model the dynamics of the document utility wit <ref type=\"bibr\" target=\"#b37\">[38]</ref>, the log-based document re-ranking is modeled as a POMDP. <ref type=\"bibr\" target=\"#b32\">[33]</ref> and <ref type=\"bibr\" target=\"#b29\">[30]</ref> propose to m <p>In our experiments, for e ective training of the model parameters and following the practices in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, we combined four TREC datasets and constructed a new data approach which automatically learns novelty features based on neural tensor networks.</p><p>MDP-DIV <ref type=\"bibr\" target=\"#b32\">[33]</ref>: a state-of-the-art learning approach which uses an MDP fo ning approach which uses an MDP for modeling the diverse ranking process. Following the practice in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, we con gured the reward function in MDP-DIV as \u03b1-DCG and  minary representations of the queries and the documents as their inputs. Following the practices in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, in the experiments we used the query vector and document . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on <ref type=\"bibr\" target=\"#b14\">(Ioffe &amp; Szegedy, 2015)</ref>, and the information bottleneck <ref type=\"bibr\" target=\"#b0\">(Alemi et al., 2017)</ref>. Notably, <ref type=\"bibr\" target=\"#b13\">(H. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: finition of black-box access as query access <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al., 2017;</ref><ref type=\"bibr\" target=\"#b11\">Hayes &amp; Dan rediction API with small datasets like MNIST and successfully demonstrated an untargeted attack. As <ref type=\"bibr\" target=\"#b15\">Liu et al. (2017)</ref> demonstrated, it is more difficult to transfe ularly when attacking models trained on large datasets like ImageNet. Using ensemble-based methods, <ref type=\"bibr\" target=\"#b15\">Liu et al. (2017)</ref> overcame these limitations to attack the Clar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and pre-trained components <ref type=\"bibr\" target=\"#b10\">[11]</ref> in order to utilize weakly supervised data, i.e. speech-to ing so, both of them achieved better performance with the end-to-end model than the cascaded model. <ref type=\"bibr\" target=\"#b10\">[11]</ref> conducts experiments on a larger 236 hour English-to-Frenc n previous literature <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> in order to improve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: its better capability of capturing the local textual information compared to other GNNs such as GCN <ref type=\"bibr\" target=\"#b6\">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce-based filtering <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and agreementbased selection <ref type=\"bibr\" target=\"#b29\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5</ref>) as robust cross entropy loss. One common issue with deep learning models is overconfidence <ref type=\"bibr\" target=\"#b13\">[14]</ref>, i.e. the models predicting effectively a probability of 1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the data are fed into the following tasks <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type=\"bibr\" target=\"#b7\">[Zhao and Yu, 2013;</ref><ref type=\"bibr\" target=\"#b4\">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type=\"bibr\" target=\"#b4\">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. \u2022 CM-Sketch <ref type=\"bibr\" target=\"#b4\">[Ranshous et al., 2016]</ref>. It uses the local structural feature an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uage understanding tasks for the existing VLP models is VQA. The SoTA result for VQA is from UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large model. Table <ref type=\"table\" target=\"#tab_3\">6</ref> Oscar B is the best among the models with equivalent size, even slightly better (0.04%) than UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large. And the Oscar L improves the SoTA overall accuracy wi other major task for the existing VLP models is NLVR2. Similarly, the SoTA model on NLVR2 is UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large. As reported in Table <ref type=\"table\" target=\"#tab_4 =\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b10\">10]</ref> employ BERT-like objectives <ref type=\"bibr\" target=\"#b6\">[6]</ref> to learn crossmodal representations from a concatenated-sequ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rchy which have shown an advantageous performance over paradigms using user-item interactions alone <ref type=\"bibr\" target=\"#b18\">[19]</ref>- <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>Generall representation involves extensive and unscalable computation with the adjacent matrix of the graph. <ref type=\"bibr\" target=\"#b18\">[19]</ref> learns a hierarchical representation of graphs by decompos nd becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. There are some of our proposed method, which fixes the number of user levels to 2. The parameter of CGNN refers to <ref type=\"bibr\" target=\"#b18\">[19]</ref>. \u2022 DIN: A popular deep neural network method without graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >(Peters et al., 2018)</ref>, GPT <ref type=\"bibr\" target=\"#b39\">(Radford et al., 2018)</ref>, BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, XLM <ref type=\"bibr\" target=\"#b25\">(Lamp  with changes in data size or composition.</p><p>We present a replication study of BERT pretraining <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, which includes a careful evaluation of t cently published methods. We release our model, pretraining and fine-tuning code.</p><p>Setup: BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> takes as input a concatenation of two seg ive training formats:</p><p>\u2022 SEGMENT-PAIR+NSP: This follows the original input format used in BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, with the NSP loss. Each input has a pair d diverse corpora, such as the ones considered in this work.</p><p>The original BERT implementation <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>  Early experiments revealed only minor di SQUAD RESULTS</head><p>We adopt a much simpler approach for SQuAD compared to past work. While BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> and XLNet <ref type=\"bibr\" target=\"#b56\">  submit RoBERTa to the public SQuAD 2.0 leaderboard. Most of the top systems build upon either BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> or XLNet <ref type=\"bibr\" target=\"#b56\">( )</ref>. This formulation significantly simplifies the task, but is not directly comparable to BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>. Following recent work, we adopt the rank training with large batch sizes.</p><p>We pretrain with sequences of at most T = 512 tokens. Unlike <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>, we do not randomly inject short sequences b4\">(Bowman et al., 2015)</ref>, which require predicting relationships between pairs of sentences. <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> observe that removing NSP hurts performanc sults for the four different settings. We first compare the original SEGMENT-PAIR input format from <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> to the SENTENCE-PAIR format; both formats  that removing the NSP loss matches or slightly improves downstream task performance, in contrast to <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. It is possible that the original BERT imp  and end-task accuracy for BERT BASE as we increase the batch size, while tuning the learning rate. <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> originally trained BERT BASE for 1M steps  alf as many optimization steps, thus seeing four times as many sequences in pretraining compared to <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>.</p><p>To help disentangle the importance  ers). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. We pretrain our model using 1024 V100 GPU et=\"#b56\">Yang et al. (2019)</ref>.</p><p>For SQuAD v1.1 we follow the same finetuning procedure as <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. For SQuAD v2.0, we additionally classify  ead>Table 7 :</head><label>7</label><figDesc>Comparison between the published BERT BASE results from<ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> to our reimplementation with either static ranslation <ref type=\"bibr\" target=\"#b30\">(McCann et al., 2017)</ref>, and masked language modeling <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lample &amp  et al., 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Baevski et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) <ref type=\"bibr\" target=\"#b17\">(Sutskever et al., 2014)</ref> with attention paradigm <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" tar st inference times <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. Thus far, realistic textures in the context of high-magnif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b32\">33]</ref>, satellite imaging <ref type=\"bibr\" target=\"#b37\">[38]</ref>, face recognition <ref type=\"bibr\" target=\"#b16\">[17]</ref> and surveillance <ref type=\"bibr\" target=\"#b52\">[53]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Inspired by the success of augmentation methods in ASR <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, as a remedy to avoid overfitting while using lowresource t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b33\">[34]</ref>, semi-supervised classification <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, and recommendation systems <ref type=\"bibr\" target=\"#b42\"> ew of existing methods see Masuda et al. <ref type=\"bibr\" target=\"#b44\">[45]</ref> and Fouss et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>The first models similar in structure to current Gr clude other diffusion coefficients \u03b8 k such as those given by the methods presented in Fouss et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> and more advanced random walks and operators that are not . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: erspective, our work can be seen as an extension of the sensitivity analyses of centrality measures <ref type=\"bibr\" target=\"#b6\">[8]</ref> and community detection algorithms <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uniform distribution. The magnitude of the maximum or minimum equals the square root of the \"fanin\" <ref type=\"bibr\" target=\"#b21\">(Plaut and Hinton 1987</ref>). The number is the network node of the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b0\">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type=\"bibr\" target=\"#b7\">[8]</ref>, CIFAR-100 <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and LFW . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive and abstractive approaches, is similar to <ref type=\"bibr\">Gu et al.'s (2016)</ref> CopyNet and <ref type=\"bibr\" target=\"#b15\">Miao and Blunsom's (2016)</ref> Forced-Attention Sentence Compression al networks <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type=\"bibr\" target=\"#b15\">(Miao and Blunsom, 2016)</ref>, and direct optimization of the perfor bibr\" target=\"#b6\">(Gu et al., 2016;</ref><ref type=\"bibr\" target=\"#b7\">Gulcehre et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Miao and Blunsom, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati al., 2016)</ref>.</p><p>Our approach is close to the Forced-Attention Sentence Compression model of <ref type=\"bibr\" target=\"#b15\">Miao and Blunsom (2016)</ref> and the CopyNet model of <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , the widespread use of such tools raises legitimate privacy concerns. For instance, Mislove et al. <ref type=\"bibr\" target=\"#b22\">[24]</ref> demonstrated how, by analysing Facebook's social network s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: quence Models for Peptide Binding Prediction</head><p>The approach builds on the UDSMProt-framework <ref type=\"bibr\" target=\"#b11\">[12]</ref> and related work in natural language processing <ref type=  with a concat pooling layer and two fully connected layers. The setup closely follows that used in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, where protein properties were predicted. The smaller data  of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50. Similar to <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the training procedure included 1-cycle learning rate sch e potential of unlabeled peptide data in order to observe similar improvements as seen for proteins <ref type=\"bibr\" target=\"#b11\">[12]</ref> in particular for small datasets.  Turning to MHC Class II uarcy of 0.137, which is is considerably lower than the accuracy of 0.41 reported in the literature <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This effect is a direct consequence of the considerably s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and natural language processing <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>The novelty of Caser is to represent the previous L where d is the number of latent dimensions and the rows preserve the order of the items. Similar to <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we regard this embedding matrix as the \"image\" of the L i r\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and natural language processing <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref  sing <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, our approach regards the L \u00d7 d matrix E as the \"image\" of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  spans and linking these nodes with confidenceweighted relation types and coreferences. Other works <ref type=\"bibr\" target=\"#b29\">(Muis and Lu, 2017;</ref><ref type=\"bibr\" target=\"#b43\">Sohrab and Mi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>.</p><p>The most common paradigm for CF is to learn latent f et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> that typically aggregate extended neighbors and need to han edding learning.</p><p>To deepen the use of subgraph structure with high-hop neighbors, Wang et al. <ref type=\"bibr\" target=\"#b38\">[39]</ref> recently proposes NGCF and achieves state-of-the-art perfo <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">PRELIMINARIES</head><p>We first introduce NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, a representative and state-of-the-art GCN model for recom  the scores of NGCF are directly copied from the Table <ref type=\"table\" target=\"#tab_4\">3</ref> of <ref type=\"bibr\" target=\"#b38\">[39]</ref>. As can be seen, removing feature transformation (i.e., NG >transformation and nonlinear activation. The graph convolution operation (a.k.a., propagation rule <ref type=\"bibr\" target=\"#b38\">[39]</ref>) in LightGCN is defined as:</p><formula xml:id=\"formula_5\" ms) that have overlap on interacted items (users), and higher-layers capture higher-order proximity <ref type=\"bibr\" target=\"#b38\">[39]</ref>. Thus combining them will make the representation more com TS</head><p>We first describe experimental settings, and then conduct detailed comparison with NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, the method that is most relevant with LightGCN but more c e experiment workload and keep the comparison fair, we closely follow the settings of the NGCF work <ref type=\"bibr\" target=\"#b38\">[39]</ref>. We request the experimented datasets (including train/tes ation is that increasing the layer number from 0 (i.e., the matrix factorization model, results see <ref type=\"bibr\" target=\"#b38\">[39]</ref>) to 1 leads to the largest performance gain, and  using a  \" target=\"#b26\">27]</ref>. Motivated by the strength of graph convolution, recent efforts like NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GC-MC <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and PinS ifferent layers. The scores of NGCF on Gowalla and Amazon-Book are directly copied from the Table3of<ref type=\"bibr\" target=\"#b38\">[39]</ref>; the scores of NGCF on Yelp2018 are re-run by us.</figDesc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: udy of the distinguishing power of some GNN variants has been initiated. In two independent studies <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., ee-aware MPNNs that do use degree information. The former class of MPNNs covers the GNNs studied in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., s bounded by the WL algorithm. This result can be seen as a slight generalisation of the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al.,  and degree-aware MPNNs matches that of the WL algorithm.</p><p>For anonymous MPNNs related to GNNs <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., or short) is well understood. Indeed, as we will shortly see, it follows from two independent works <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., L , as is indicated in Figure <ref type=\"figure\" target=\"#fig_1\">1</ref>. Proposition 5.2 (Based on <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., ymous MPNN by using an injection h : A s \u2192 Q. What follows is in fact an adaptation of Lemma 5 from <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> itself based on <ref type=\"bibr\">[Zaheer et a gue that M anon is weaker than M WL . The proof is a trivial adaptation of the proofs of Lemma 2 in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=\"bibr\" target=\"#b1 t) w = (\u2113 \u2113 \u2113 (t) M ) w ,</formula><p>as desired.</p><p>We remark that we cannot use the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et x because the class M anon is more general than the class considered in those papers. The proofs in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et can be written in the form g (t) u\u2208NG(v) h (t) (\u2113 \u2113 \u2113 (t\u22121) u</formula><p>) , based on Lemma 5 from <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref>.</p><p>Suppose that \u03bd \u03bd \u03bd : V \u2192 A s0 . It now. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: les <ref type=\"bibr\" target=\"#b50\">[51]</ref>. The related study is presented in our previous paper <ref type=\"bibr\" target=\"#b49\">[50]</ref>.</p><formula xml:id=\"formula_15\">Hmatch 2 = 1/(norm(|P U \u2212 6\"><head></head><label></label><figDesc>) T U describes learning styles. Referring to other research<ref type=\"bibr\" target=\"#b49\">[50]</ref>, we design the elements of learning styles as: T U = {CL, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o-rank (LTR) is challenging due to its biased nature. To address this bias problem, Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref> proposed a counterfactual inference approach, providing an f type=\"bibr\" target=\"#b18\">[18]</ref>.</p><p>To handle biases in a principled way, Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref> introduced an unbiased learning-to-rank framework, which i ecting the examination bias in learning to rank from implicit feedback. As shown by Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref>, the parameters of the PBM can serve as propensity estimat ent d for query q.</p><p>While Pr(E = 1|k) can be used as an estimate of the examination propensity <ref type=\"bibr\" target=\"#b19\">[19]</ref>, it is a rather simplistic model since it assumes that exa max ]. In this case, randomly swapping results at positions k and k \u2032 before presenting the ranking <ref type=\"bibr\" target=\"#b19\">[19]</ref> makes the expected relevance of results at the two positio ck-through rates is a consistent estimator of the relative propensities p k and p k \u2032 under the PBM <ref type=\"bibr\" target=\"#b19\">[19]</ref>. Note that knowing the relative propensities with respect   sufficient, since the counterfactual ERM learning objective is invariant to multiplicative scaling <ref type=\"bibr\" target=\"#b19\">[19]</ref>.</p><p>While this ratio estimator is a sensible approach f interventions were then used to get a gold-standard estimate of the propensities via the methods in <ref type=\"bibr\" target=\"#b19\">[19]</ref>. To avoid any confounding due to changes in the query dist  <ref type=\"table\" target=\"#tab_0\">1</ref>. We then use the gold-standard propensity estimator from <ref type=\"bibr\" target=\"#b19\">[19]</ref> to learn two PBM models from the swap intervention data, o be expected, given that AllPairs makes more efficient use of the data than the ratio-estimates from <ref type=\"bibr\" target=\"#b19\">[19]</ref>.</p><p>Can AllPairs learn CPBM models with many context fe nce compared to using the propensities from the PBM.</p><p>We trained a Clipped Propensity SVM-Rank <ref type=\"bibr\" target=\"#b19\">[19]</ref> for each of the following three propensity models: PBM est ed via cross-validation. For rank r &gt; 21, we impute the propensity p r (x) = p 21 (x). Following <ref type=\"bibr\" target=\"#b19\">[19]</ref>, we measure test-set ranking performance via the average s imitations of existing propensity estimation methods for LTR <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref>. First, existing me or LTR algorithms like <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b19\">19]</ref>. We evaluate the fidelity of the CPBM model and the effecti  <ref type=\"bibr\" target=\"#b23\">[23]</ref>. The most effective methods use randomized interventions <ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref>, which unfortunate first review how explicit interventions have been used for estimating p k := Pr(E = 1|k) in the PBM <ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref>. The PBM requires . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  decades. Most of these measurements <ref type=\"bibr\" target=\"#b9\">[Rowlison and Felner, 1988;</ref><ref type=\"bibr\" target=\"#b1\">Brantley and Jones, 1993]</ref> are based on questionnaires and interv fier that is found to effective in several classification problems.</p><p>\u2022 Softmax Regression (SR) <ref type=\"bibr\" target=\"#b1\">[B\u00f6hning, 1992]</ref>: It is a model that is used to predict the proba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eful tool for dealing with those aspects of compiler and program development. Models of performance <ref type=\"bibr\" target=\"#b5\">[6]</ref> and energy <ref type=\"bibr\" target=\"#b6\">[7]</ref> can also . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref> try to improve the static injection framework. CriticalFau structions using symbolic execution, which enumerates all potential hardware errors.</p><p>The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> presents a selective protection technique that allows user tion technique that allows users to selectively protect these SDC-prone data. The main idea of work <ref type=\"bibr\" target=\"#b15\">[16]</ref> is predicting the SDC proneness of a program's data firstl put program are selected by GA.</p><p>Step 3 strengthens the identified vulnerable blocks. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> introduces a prediction model named SDCAuto to predict the causing errors. Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> illustrates the diagram of the work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> first  _1\">2</ref> illustrates the diagram of the work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> first compiles the source code into LLVM IR, and extracts  sidered, since it always causes illegal opcode exception rather than SDC.</p><p>Finally, as in work <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we assume that at most one fault occurs during a program' e our results with the work <ref type=\"bibr\" target=\"#b14\">[15]</ref> and SDCAuto presented in work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. We use our approach to maximize SDC coverage under the us /1.0\" xml:id=\"fig_1\"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of the work<ref type=\"bibr\" target=\"#b15\">[16]</ref> </figDesc><graphic url=\"image-2.png\" coords=\"4,99.47,451.0. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g is typically conducted on the entire document collection <ref type=\"bibr\" target=\"#b18\">[17,</ref><ref type=\"bibr\" target=\"#b23\">22]</ref>. However, such learning paradigm faces a major drawback in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ional networks, have gained much attention and improved the state of the art in node classification <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: parameters using 600\u21e5600 images for training. In this paper, we focus on the ResNet-50 architecture <ref type=\"bibr\" target=\"#b10\">[11]</ref> due to its good accuracy/cost tradeoff (25.6M parameters)  -of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 <ref type=\"bibr\" target=\"#b10\">[11]</ref>. For larger experiments, we use PNASNet-5-Large <ref type=  although this means that sev- eral forward passes are required to classify one image. For example, <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta  Another performanceboosting strategy is to classify an image by feeding it at multiple resolutions <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s.</p><p>e vertices of MMM CDAG may be arranged in an [m \u00d7 n \u00d7 k] 3D grid called an iteration space <ref type=\"bibr\" target=\"#b59\">[59]</ref>. e orthonormal vectors i, j, k correspond to the loops in  ns</head><p>I/O optimization for linear algebra includes such techniques as loop tiling and skewing <ref type=\"bibr\" target=\"#b59\">[59]</ref>, interchanging and reversal <ref type=\"bibr\" target=\"#b58\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w York Times (NYT) <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010)</ref> and WebNLG <ref type=\"bibr\" target=\"#b6\">(Gardent et al. 2017)</ref>. NYT comes from the distant supervised rel ramework <ref type=\"bibr\" target=\"#b15\">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type=\"bibr\" target=\"#b6\">(Gu et al. 2016)</ref>. But it cannot predict the entire entities. In . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> to address the lack of highquality, large- eters et al., 2018)</ref>, GPT <ref type=\"bibr\" target=\"#b25\">(Radford et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, unsupervised pretraining of language mode s=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Methods</head><p>Background The BERT model architecture <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> is based on a multilayer bidirectional Tra ead n=\"3.3\">Pretrained BERT Variants</head><p>BERT-Base We use the pretrained weights for BERT-Base <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> released with the original BERT code. <ref 9\">(Howard and Ruder, 2018)</ref> which is equivalent to the linear warmup followed by linear decay <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>. For each dataset and BERT variant, we pic h using AllenNLP <ref type=\"bibr\" target=\"#b8\">(Gardner et al., 2017)</ref>.</p><p>Casing We follow <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref> in using the cased models for NER and the u T</head><p>We mostly follow the same architecture, optimization, and hyperparameter choices used in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>. For text classification (i.e. CLS and REL). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: atch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule <ref type=\"bibr\" target=\"#b9\">(Howard and Ruder, 2018)</ref> which is equivalent to the linear warmu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Hidasi et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> use d each item a fixed weight based on the relative distance with response to the target item. Li et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> propose an RNN based encoder-decoder model (NARM), which ta transaction data is used in this study.</p><p>Following <ref type=\"bibr\" target=\"#b4\">[5]</ref> and <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we filter out sessions of length 1 and items that appear l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  optimizing pipeline parallelism for synchronous training <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. This approach requires necessary gradients synchronizatio llelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e distribution of publications(46% authers have written only one publication in their entire career <ref type=\"bibr\" target=\"#b25\">[26]</ref>, whereas some may have written more than 100 ones). The Ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b4\">[5]</ref>, Superthreaded <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Trace Processors <ref type=\"bibr\" target=\"#b16\">[17]</ref> [21], Speculative Multithreaded <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ingDB dataset contains 39 747 positive examples and 31 218 negative examples from a public database <ref type=\"bibr\" target=\"#b10\">(Gilson et al., 2016)</ref>. The training, valid and test sets of Bin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vertex classification, many variants of GNN have been proposed to solve problems in social networks <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, biology <ref type ith the feature matrix X . The product (I \u2212 L)X is understood as features averaging and propagation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he entire training set).</p><p>To further verify the planning diversity, we also computed self-BLEU <ref type=\"bibr\" target=\"#b46\">(Zhu et al., 2018)</ref> to evaluate how different planning results (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th different weights. However, these methods <ref type=\"bibr\" target=\"#b27\">(Wu et al., 2019b;</ref><ref type=\"bibr\" target=\"#b31\">Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">An et al., 2019)  set of documents given a query. Some works <ref type=\"bibr\" target=\"#b24\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Zhu et al., 2019)</ref> propose to improve news representations via e eddings. These embeddings can be pre-trained from a large corpus or randomly initialized. Following <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we define the profile embedding</p><formula ed news representations would be taken as initial input embeddings of our model GNUD. Following DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we use two parallel convolutional neural ne sa-10week, which respectively collect news click logs as long as 1 week and 10 weeks. Following DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we just select user id, news id, time-stamp ws title and profile as semantic-level and knowledge-level representations, respectively.</p><p>DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, a deep attention neural network for news re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ead n=\"3.2\">Tucker Decomposition</head><p>Tucker decomposition, named after Ledyard R.</p><p>Tucker <ref type=\"bibr\" target=\"#b26\">(Tucker, 1964)</ref>, decomposes a tensor into a set of matrices and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Several variants of auto-encoders have also been investigated for representing and generating text <ref type=\"bibr\" target=\"#b3\">(Bowman et al., 2016;</ref><ref type=\"bibr\">Zhao et al., 2018)</ref>, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to handle the variety of models of parallelism that appear in HPC programs. Toward this end, PEBIL <ref type=\"bibr\" target=\"#b0\">[1]</ref> has recently added support for handling multithreaded x86 64 afety</head><p>PEBIL generates and inserts code into the program which has two principle functions: <ref type=\"bibr\" target=\"#b0\">(1)</ref> to add functionality to a program, functionality which is us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ <ref type=\"bibr\" target=\"#b1\">(Budzianowski et al. 2018)</ref>, M2M <ref type=\"bibr\" target=\"#b15\">(  Asri et al. 2017)</ref>, M2M <ref type=\"bibr\" target=\"#b15\">(Shah et al. 2018</ref>) and Multi-WOZ <ref type=\"bibr\" target=\"#b1\">(Budzianowski et al. 2018</ref>). These datasets have utilized a varie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p>Existing literature on training with noisy labels focuses primarily on loss correction approaches <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015;</ref><ref type=\"bibr\" target=\"#b9\">Hendrycks et a ling using the network predictions to predict hard or soft labels.</p><p>Loss correction approaches <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015;</ref><ref type=\"bibr\" target=\"#b11\">Jiang et al., pe=\"bibr\" target=\"#b11\">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrapping loss <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>, which introduces a perceptual consistency  ilities used to compute it, to compensate for the incorrect guidance provided by the noisy samples. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> extend the loss with a perceptual term that is unsupervised model to implement a loss correction approach that benefits both from bootstrapping <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=\"bibr ibr\" target=\"#b33\">(Zhang et al., 2017)</ref>.</p><p>The static hard bootstrapping loss proposed in <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> provides a mechanism to deal with label noi ) ,<label>(10)</label></formula><p>where w i weights the model prediction z i in the loss function. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> use w i = 0.2, \u2200i. We refer to this approac Reed et al., 2015)</ref> use w i = 0.2, \u2200i. We refer to this approach as static hard bootstrapping. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> also proposed a static soft bootstrapping l beling. We also run our proposed approach under these conditions in Subsection 4.5 for comparison.  <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>. The overall results demonstrate that apply sed dynamic hard bootstrapping exhibits better performance than the state-of-the-art static version <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>. It is, however, not better than the perfor d the 300 epochs training scheme (see Subsection 4.1) . We introduce bootstrapping in epoch 105 for <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> for the proposed methods, estimate the T ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d><p>In this study, we use two representative backbones for feature extraction: SegNet and ResNet50 <ref type=\"bibr\" target=\"#b17\">[18]</ref>. On this basis, we proposed two networks: SCAttNet V1 with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2]</ref> have focused on adapting the B + -tree structure to obtain better cache behaviour. Work in <ref type=\"bibr\" target=\"#b1\">[1]</ref> presented buffer-trees, which like the CCB + -tree, use the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f DNNs' performance, researchers and industry practitioners have turned to search-based compilation <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe e performance of programs during the search. We adopt a learned cost model similar to related works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> with newly designed  ion and automatic search. Halide has three versions of auto-scheduler based on different techniques <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b26\">28,</ref><ref type=\"bibr\" targ >) or aggressive pruning by inaccurately evaluating incomplete programs (e.g. Halide auto-scheduler <ref type=\"bibr\" target=\"#b1\">[2]</ref>), which prevents them from covering a large enough search sp <ref type=\"bibr\" target=\"#b29\">[31]</ref> to search for good decisions (e.g., Halide auto-scheduler <ref type=\"bibr\" target=\"#b1\">[2]</ref>). In this approach, the compiler constructs a tensor program limited search space heuristically.</p><p>(2) Aggressive early pruning (e.g., Halide auto-scheduler <ref type=\"bibr\" target=\"#b1\">[2]</ref>). Aggressive early pruning based on evaluating incomplete pr ntel CPU.</p><p>We include PyTorch <ref type=\"bibr\" target=\"#b34\">[36]</ref>, Halide auto-scheduler <ref type=\"bibr\" target=\"#b1\">[2]</ref>, Flex-Tensor <ref type=\"bibr\" target=\"#b51\">[53]</ref> and A for complete programs but fails to accurately predict the final performance of incomplete programs. <ref type=\"bibr\" target=\"#b1\">(2)</ref> The fixed order of sequential decisions limits the design of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he relational graph and apply it to both tasks.</p><p>Our entity classification model, similarly to <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref>, uses softmax classifiers at each node  nction or simply a linear transformation g m (h i , h j ) = W h j with a weight matrix W such as in <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref>. This type of transformation has been s tion. While we only consider such a featureless approach in this work, we note that it was shown in <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref> that it is possible for this class of m that operate on local graph neighborhoods <ref type=\"bibr\" target=\"#b8\">(Duvenaud et al. 2015;</ref><ref type=\"bibr\" target=\"#b16\">Kipf and Welling 2017)</ref> to large-scale relational data. These an d et al. 2015;</ref><ref type=\"bibr\" target=\"#b6\">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" target=\"#b16\">Kipf and Welling 2017)</ref> for large-scale and highly multi-relatio <ref type=\"bibr\" target=\"#b8\">(Duvenaud et al. 2015)</ref> and graph-based semi-supervised learning <ref type=\"bibr\" target=\"#b16\">(Kipf and Welling 2017)</ref>.</p><p>Motivated by these architectures. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b1\">2]</ref> and MT <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= ns/1.0\"><head>Fine-tuning set</head><p>In-domain Out-of-domain Real + one-speaker TTS synthetic 59. <ref type=\"bibr\" target=\"#b4\">5</ref> 19.5 Only one-speaker TTS synthetic 38.5 13.8</p><p>Table <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Where the TMO model creates new ORB services to provide its time-based invocation capabilities <ref type=\"bibr\" target=\"#b26\">[27]</ref>, TAO provides a subset of these capabilities by extending . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sociations. Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets  gion embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> for improving accuracy.</p></div> <di ing each word in the text to a word vector (word embedding). We take a more general viewpoint as in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> and consider text region embedding -e w-1 as input, serves as an unsupervised embedding function in the model for text categorization. In <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> unsupervised embeddings obtained this ings. Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ar how to properly train deep GCN architectures, where several works have studied their limitations <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" ta ref type=\"bibr\" target=\"#b52\">53]</ref> is an open problem in the graph learning space. Recent work <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" ta causes oversmoothing, eventually leading to features of graph vertices converging to the same value <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Due to these limitations, most state-of-the-art GCNs are  is limited to a small number of layers <ref type=\"bibr\" target=\"#b5\">(6)</ref>. Recently, Li et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> studied the depth limitations of GCNs and showed that deep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  k with d \u2032 input channels and d \u2032\u2032 output channels, the total number of multiply add required is h <ref type=\"bibr\" target=\"#b0\">(1)</ref> this expression has an extra term, as indeed we have an extr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly work mostly deals with simple graphs with unlabeled edges, recently proposed relation-aware GNNs <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> consider multi-rel pe=\"bibr\" target=\"#b38\">[39]</ref> consider direction and relation types, respectively. Also, R-GCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> considers direction and relation types simultaneously. Rec e=\"bibr\" target=\"#b44\">45]</ref>. 5) R-GCN. This is a GNN-based method for modeling relational data <ref type=\"bibr\" target=\"#b37\">[38]</ref>. 6) MEAN. 7) LAN. These are GNN models for a out-of-knowle get=\"#b14\">[15]</ref>. 3) R-GCN. The same model used in the entity prediction on KG completion task <ref type=\"bibr\" target=\"#b37\">[38]</ref>. 4) I-GEN. Inductive GEN, which only uses feature represen nds the graph convolutional network to consider multi-relational structure, by Schlichtkrull et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p><p>6) MEAN. This model computes the embedding of entit s W r and W r to prevent the excessive increase in the model size, proposed in Schlichtkrull et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref>: W r = B b=1 a r b V b , where B is the number of basis, a twork based methods <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. While they require ggested by several recent works on multi-relational graphs <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, where directed rel ne in previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we measure the ranks in a filtered setting where we do not. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: viewed as a sequence-to-sequence learning problem. We exploit deep recurrent neural networks (RNNs) <ref type=\"bibr\" target=\"#b16\">[15,</ref><ref type=\"bibr\" target=\"#b17\">16]</ref> as the acoustic mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >, modeling of proteins <ref type=\"bibr\" target=\"#b14\">Gainza et al. (2019)</ref> and nucleic acids <ref type=\"bibr\" target=\"#b37\">Rossi et al. (2019)</ref>, and fake news detection on social media <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al neighborhood <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p><p>More interestingly, Z \u00fcgner et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> focused on the node classification using GCN, and proposed ations based on the obtained embedding vectors could be affected correspondingly.</p><p>Inspired by <ref type=\"bibr\" target=\"#b40\">[41]</ref>, in this paper, we propose a new fast gradient attack (FGA et node, then randomly connect the target node to K \u2212 b nodes of different classes.</p><p>\u2022 NETTACK <ref type=\"bibr\" target=\"#b40\">[41]</ref>. NETTACK generates adversarial network iteratively. In eac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e presence of noise from the automatic extraction methods used to populate them. For instance, NELL <ref type=\"bibr\" target=\"#b1\">[Carlson et al., 2010]</ref> is known to contain various kinds of erro e part of the original benchmark test collection.   The NELL subset taken from its 165 th iteration <ref type=\"bibr\" target=\"#b1\">[Carlson et al., 2010]</ref>) has been used for the KG refinement task. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, and variants of Craig, Landin, and Hagersten (CLH) locks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. Historically, CLH lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, are on multiprogramming environments, attempting to allevi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 18\">[19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and transductive experimental design methods <ref type=\"bibr\" target=\"#b26\">[27]</ref>. These kinds of active learning algorithms are referred to ion matrix A. The selected samples are therefore considered to be the most representative.</p><p>In <ref type=\"bibr\" target=\"#b26\">[27]</ref>, an early active learning via a Transduction Experimental   problem to solve, thus an approximate solution by a sequential optimization problem is proposed in <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  nformative and representative examples for labeling using the min-max margin-based approach. 4. TED <ref type=\"bibr\" target=\"#b26\">[27]</ref> Active learning via Transduction Experimental Design is an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  noisy labeling, previous studies adopt multi-instance learning to consider the noises of instances <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010;</ref><ref type=\"bibr\" target=\"#b5\">H te this issue, many studies formulated relation classification as a multi-instance learning problem <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010;</ref><ref type=\"bibr\" target=\"#b5\">H the framework of reinforcement learning <ref type=\"bibr\" target=\"#b16\">(Sutton and Barto 1998;</ref><ref type=\"bibr\" target=\"#b13\">Narasimhan, Yala, and Barzilay 2016)</ref> and then predicts relation 4</ref> generated by the sentences in NYT<ref type=\"foot\" target=\"#foot_2\">5</ref> and developed by <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010)</ref>. There are 522,611 sentences, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: techniques have also been applied to improving intelligibility for speakers with vocal disabilities <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, and hearing-impai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  inputs, recent work <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> shows that an adversary is often able to manipulate the inp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b11\">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple extractors <ref type=\"bibr\" target=\"#b4\">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type=\"bi tion <ref type=\"bibr\" target=\"#b11\">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type=\"bibr\" target=\"#b4\">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type=\"bibr\"  ent or replace the set of rules we use. Further, evidence from diverse extractors as in the case of <ref type=\"bibr\" target=\"#b4\">[Dong et al., 2014]</ref> can be incorporated into the PSL-KGI framewo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on image) can be captured in a \"remove-and-occlude\" manner <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>: 1) Taking a photo of the mixture image through the glass;  or a much larger scale than those used in existing methods <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Finally, we build a dataset containing 4027 images under v era with fully manual control model) like previous methods <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, we also use the cameras on different types of mobile phone. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, is that repetitive control flow graph traversals lead to r temporal streaming <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> from prefetching approaches that only retrieve a constant n y counting successful prefetches. Hence, like past designs <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, TIFS uses the next-best option, the Recent heuristic, as i target data accesses <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. TIFS adds three logical structures to the chip: a set of S cts the anatomy of the SVB. Our SVB design is adapted from <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. The SVB contains a small fully-associative buffer for temp arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. These prefetchers target primarily off-chip data reference  prefetcher to retrieve instruction-cache blocks ahead of the fetch unit for the rest of the stream <ref type=\"bibr\" target=\"#b36\">[37]</ref>.</p><p>Figure <ref type=\"figure\">5</ref> shows the cumulat  the L2 cache (see <ref type=\"bibr\">Section 5)</ref>.</p><p>The term temporal stream, introduced in <ref type=\"bibr\" target=\"#b36\">[37]</ref>, refers to extended sequences of data references that recu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ref>, which decomposes termdocument matrix and yields latent semantic representations. Lund et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> put forward Hyperspace Analogue to Language (HAL), factori. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: NLP modeled the task as supervised binary classification, using architectures such as Enhanced LSTM <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2016)</ref>, Decomposable Attention <ref type=\"bibr\" tar mes. While many different approaches were used for sentence pair classification, e.g. Enhanced LSTM <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2016)</ref>, Decomposable Attention <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly, a lot of works <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" t  reduced, the statistical fault injection (SFI) experiments are still time-consuming. SmartInjector <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposes an intelligent fault injection framework to ident nly one representative fault is selected to implement fault injection for each group. SmartInjector <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposes an intelligent fault injection framework to ident. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: While there is research work that has used court trial transcripts to identify deceptive statements <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we are not aware of any previous work that took into cons cusing on real-life high-stake data. The work closest to ours is presented by Fornaciari and Poesio <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which targets the identification of deception in statemen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: den state of the last moment, respectively. The detailed calculation steps of (5) were presented in <ref type=\"bibr\" target=\"#b1\">(2)</ref>. Observation Encoder. The observation encoder is in charge o r loading the initial values, the prediction decoder will run following the calculation as shown in <ref type=\"bibr\" target=\"#b1\">(2)</ref>. In particular, the input and the prediction at a moment t & nfiguration of the prediction decoder. In addition, inspired by the method from machine translation <ref type=\"bibr\" target=\"#b1\">[2]</ref>, the prediction for the moment t will be sent as the input o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the effect of affirmative action (see e.g., <ref type=\"bibr\" target=\"#b12\">Keith et al., 1985;</ref><ref type=\"bibr\" target=\"#b11\">Kalev et al., 2006)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ural Networks (GNNs) were introduced in <ref type=\"bibr\" target=\"#b15\">Gori et al. (2005)</ref> and <ref type=\"bibr\" target=\"#b33\">Scarselli et al. (2009)</ref> as a generalization of recursive neural. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" t b0\">[1]</ref> or a supervised measure such as R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref>, PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref>, and NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>. The ex \"#b25\">[26]</ref> only use the top documents in the ideal rankings while other methods such as PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> sample the training rankings by judging it through diversi ll to train the model which may lead to underfit. The quantity of the training dataset used by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> is large enough but the quality of it depends on some hype get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">[23]</ref><ref type=\"bibr\" target=\"#b23\">[24]</ref><ref type=\"bibr\" t robability of optimal rankings. Based on the same score function of R-LTR, Xia et al. proposed PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> in which loss function is designed to directly maximize th uch as \ud835\udefc-NDCG and ERR-IA <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The form of \u2111 is inspired by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> method which aims to maximize the margin between positive  |\ud835\udc5e, \ud835\udc36)] .<label>(11)</label></formula><p>The loss function of the discriminator is inspired by PAMM <ref type=\"bibr\" target=\"#b22\">[23]</ref> method which is aiming to maximize the margin between the   approaches and implicit approaches. The implicit approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar /ref>.</p><p>Studies have shown that supervised approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rns in our framework to extract useful features from the heterogeneous textrich network. Meta-paths <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref> phs, can offer more flexibility and capture richer network semantics than the widely used meta-path <ref type=\"bibr\" target=\"#b32\">[31]</ref> patterns. Recent studies have shown that incorporating mot of two authors (i.e., \"Jure Leskovec\" and \"Jon Kleinberg\").</p><p>It is worth noting that meta-path <ref type=\"bibr\" target=\"#b32\">[31]</ref> can be viewed as a special case of motif patterns when the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: er, recent research showed that an attacker could generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ b0\">(1)</ref> Training the target classifier with adversarial examples, called adversarial training <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>; <ref type=\"bibr\" ta cally, researchers showed that it was possible to generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ  one may use a mixture of normal and adversarial examples in the training set for data augmentation <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, or mix the advers =\"2.3\">Existing attacks</head><p>Since the discovery of adversarial examples for neural networks in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, researchers have found adversarial examples on various ne h leveraged gradient based optimization from normal examples <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Moosavi et al. showe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: used to learn models was only considered in the context of binary SVMs whose training data is known <ref type=\"bibr\" target=\"#b6\">[7]</ref> or anomaly detection systems whose underlying model is known. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is passed into a bidirectional convolutional LSTM (CLSTM) <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> layer using a 1 \u00d7 3 filter, i.e. convolving only across the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: inimal group miss ratio) and required the assumption that the individual miss ratio curve be convex <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Our algorithm uses dynamic programming to examine the entir he cache between instructions and data, and for multiprogramming by giving each process a partition <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>In the way of optimizing their algorithm, they proved ss-rate derivative. The allocation is optimal if the miss-rate derivatives are as equal as possible <ref type=\"bibr\" target=\"#b8\">[9]</ref>. The optimality depends on several assumptions. One is that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualization <ref type=\"bibr\" target=\"#b28\">[28]</ref \">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al.   direct calls <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, as shown in Fig. <ref type=\"figure\" target=\"#fig_0\">11b</re tion call and its devirtualized form. usually has higher accuracy than an indirect branch predictor <ref type=\"bibr\" target=\"#b5\">[6]</ref>. However, not all indirect calls can be converted to multipl form RCPO, the following conditions need to be fulfilled <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>:</p><p>1. The number of frequent target addresses from a cal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  predict execution time of programs by using a set of hand-crafted features of high level programs. <ref type=\"bibr\" target=\"#b9\">Dubach et al. (2007)</ref> uses neural networks with hand-crafted feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f a premise and hypothesis through tree-LSTM <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Le and Zuidema,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d L2 cache energy dissipations using a modified version of the analytical model of Kamble and Ghose <ref type=\"bibr\" target=\"#b14\">[15]</ref>. This model calculates cache energy dissipation using tech. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e x i t s h a 1 _ b a s e 6 4 = \" t A  We use primitives from an existing code generation framework <ref type=\"bibr\" target=\"#b8\">[9]</ref> to form S e . Our search space includes multi-level tiling o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ke other deep learning models, GNNs have also been shown to be vulnerable under adversarial attacks <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which has recently attracted increasing research interest sting (evasion); the attacker may aim to mislead the prediction on specific nodes (targeted attack) <ref type=\"bibr\" target=\"#b27\">[28]</ref> or damage the overall task performance (untargeted attack) model parameters, input data, and labels; grey-box attacks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> have partial inform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f><ref type=\"bibr\" target=\"#b16\">17]</ref> and position bias <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Previous studies have shown that models and evaluation met position bias estimation methods for ranking are proposed in <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. IPS is one of the most popular counterfactual approaches f ly handling of the biases may help improve the performance <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Most of the previo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  vision <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref type=\"bibr\" target=\"#b12\">Cheng et al., 2018)</ref>, and ASR <ref type=\"bibr\" target=\"#b45\">(Sp 6)</ref> presented a general method to stabilize model predictions against small input distortions. <ref type=\"bibr\" target=\"#b12\">Cheng et al. (2018)</ref> continued their work and developed the adve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: fective, but combined with an LSTM brought marginal improvement and greater interpretability, while <ref type=\"bibr\" target=\"#b8\">[9]</ref> did not find any notable improvement using the Transformer i e Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Though self-attention has provided various benefits such as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ral Networks (GNN). We refer the reader to the review papers <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref> for more details.</p><p>Many recent results have improved t f> which are assimilable to message-passing networks <ref type=\"bibr\" target=\"#b18\">[19]</ref>, see <ref type=\"bibr\" target=\"#b44\">[45,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> for reviews. For mes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utions can automatically absorb the effects of such unexpected adversarial changes in the variances <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. As a result, using . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: period under study, which was just over 14% and we were long 653 out of 1077 days.)</p><p>As Sharpe <ref type=\"bibr\" target=\"#b5\">(6)</ref> points out, instead of buying and selling short the DJIA, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hts into global model behavior.</p><p>Triggers are a new form of universal adversarial perturbation <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017)</ref> adapted to discrete textual inp or anyone to fool machine learning models. Moreover, universal attacks often transfer across models <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017)</ref>, which further decreases attack e adversarial threat is higher if an attack is universal: using the exact same attack for any input <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Br. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: leased our implementation of ApproxNDCG in Tensorflow in the open-source Tensorflow Ranking library <ref type=\"bibr\" target=\"#b11\">[12]</ref>. <ref type=\"foot\" target=\"#foot_0\">1</ref></p></div> <div . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b40\">42]</ref>.</p><p>Single image target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" tar tep. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images <ref type=\"bibr\" target=\"#b22\">[24]</ref>. (d) Iterative up and downsampling approach is proposed by can preserve HR components better.</p><p>(c) Progressive upsampling was recently proposed in LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>. It progressively reconstructs the multiple SR images with N <ref type=\"bibr\" target=\"#b20\">[22]</ref>, DRRN <ref type=\"bibr\" target=\"#b40\">[42]</ref>, LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>) on Set5 dataset for 4\u00d7 enlargement.</p><p>the-art methods N <ref type=\"bibr\" target=\"#b20\">[22]</ref>, DRRN <ref type=\"bibr\" target=\"#b40\">[42]</ref>, LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>, and EDSR <ref type=\"bibr\" target=\"#b28\">[30]</ref>. We ca step. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images<ref type=\"bibr\" target=\"#b22\">[24]</ref>. (d) Iterative up and downsampling approach is proposed by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arning can be separated into two categories, depending on whether they are executed during training <ref type=\"bibr\" target=\"#b8\">[9]</ref> or at test time <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he case in SR. Inspired by the success of very deep networks <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">27,</ref><ref type=\"bibr\" target=\"#b26\">28]</ref> on ImageNet <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n single sentence relation extraction with an exception of <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, which focus on general documents while not targeting on a  e extracted dependency parse tree, where the tree roots of different sentences are linked together. <ref type=\"bibr\" target=\"#b36\">[37]</ref> proposes a method using self-attention <ref type=\"bibr\" ta  the self-attention of the words, and use a convolutional layer in self-attention blocks similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref> to alleviate the burden on the model to attend to local fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to successfully inpaint large regions. Despite using no learning, the results may be comparable to <ref type=\"bibr\" target=\"#b14\">[15]</ref> which does. The choice of hyper-parameters is important (f ng). Yet, it works surprisingly well for other situations. We compare to a learning-based method of <ref type=\"bibr\" target=\"#b14\">[15]</ref> in fig. <ref type=\"figure\" target=\"#fig_5\">6</ref>. The de t to successfully inpaint large regions. Despite using no learning, the results may be comparable to<ref type=\"bibr\" target=\"#b14\">[15]</ref> which does. The choice of hyper-parameters is important (f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eCNN <ref type=\"bibr\" target=\"#b8\">(Fey et al. 2018)</ref>, and the spectral approaches proposed in <ref type=\"bibr\" target=\"#b3\">(Bruna et al. 2014;</ref><ref type=\"bibr\" target=\"#b6\">Defferrard, X.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> on Chinese poetry generation have been mostly rulebased or te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /expressing emotions and emotionally interacting with the interlocutors. In literature, Zhou et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> successfully build an emotional chat machine (ECM) that is enerate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Hence, the response generation problem faces a significan tional factors, which are most related to our proposed conversation generation problem. Zhou et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> develop an Emotional Chat Machine (ECM) model using three  \" target=\"#b32\">[33]</ref>, to evaluate our experimental results. In particular, we follow the work <ref type=\"bibr\" target=\"#b45\">[46]</ref> to train an emotion classifier for assigning emotional lab ifferent datasets, i.e., NLPCC 2013 2 and NLPCC 2014 3 emotion classification datasets by following <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which contain 29, 417 manually annotated data in total, a o any emotion information, and rare emotion categories like fear are removed. In particular, unlike <ref type=\"bibr\" target=\"#b45\">[46]</ref> using solely one label for classification, we consider bot rget=\"#b35\">[36]</ref>, the traditional Seq2seq model is adopted as one of our baselines.</p><p>ECM <ref type=\"bibr\" target=\"#b45\">[46]</ref>, as mentioned, ECM model is improper to directly be as the onal matrix (if used). The parameters of imemory and ememory in ECM are the same as the settings in <ref type=\"bibr\" target=\"#b45\">[46]</ref>. We use stochastic gradient descent (SGD) with mini-batch  <note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">Here we follow the work<ref type=\"bibr\" target=\"#b45\">[46]</ref>, where the emotion categories are {Angry, Disgust, Happy,  o the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, Seq2seq with emotion embedding (Seq2seqemb) is also adopte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to thirty <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Many other nontrivial visual recognition tasks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ven methods. 1) Question generation focuses on generating natural language questions. Seyler et al. <ref type=\"bibr\" target=\"#b177\">[178]</ref> studied quiz-style knowledge question generation by gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: upported on GPU platform, thus leading to inefficient training. To solve this issue, as explored in <ref type=\"bibr\" target=\"#b18\">[18]</ref>, we also apply a fast matrix normalization method based on eleration</head><p>To date, fast implementation of EIG on GPU is still an open problem. Inspired by <ref type=\"bibr\" target=\"#b18\">[18]</ref>, we utilize Newton-Schulz iteration to speed up the comput e root as \u03a3 1/2 = Y = Udiag(\u03bb 1/2 i )U T .G i v e nY 0 = \u03a3, Z 0 = I, for n =1 , \u2022\u2022\u2022 ,N, as shown in <ref type=\"bibr\" target=\"#b18\">[18]</ref>, the Newton-Schulz iteration is then updated alternately a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rations are used to model the sequential document selection process in search result diversi cation <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and multi-page sea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>[8] <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta d 100003FA, which is often observed and utilized for prediction in the Markov prefetching algorithm <ref type=\"bibr\" target=\"#b9\">[10]</ref> . The following section discusses data prefetching methodol strides are recognizable. To capture repetitiveness in data reference addresses, Markov prefetching <ref type=\"bibr\" target=\"#b9\">[10]</ref> was proposed. This strategy assumes the history might repea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 6\">(Yao et al. 2015)</ref> and extracting the last hidden state of recurrent visual feature encoder <ref type=\"bibr\" target=\"#b20\">(Venugopalan et al. 2015)</ref>.</p><p>Those feature encoding methods Our basic video caption framework is extended from S2VT (sequence to sequence: video to text) model <ref type=\"bibr\" target=\"#b20\">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ginally introduced for text compression <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and it was used in <ref type=\"bibr\" target=\"#b1\">[2]</ref> for branch prediction. Figure <ref type=\"figure\" target=\"#fi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Virtual Hierarchies rely on a logical two-level directory to partition a cache at bank granularity <ref type=\"bibr\" target=\"#b28\">[29]</ref>, but this comes at the cost of doubling directory overhead. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e most widely used one, <ref type=\"bibr\">BERT [Devlin et al., 2018]</ref> builds on the Transformer <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> architecture and improves the pre-trainin -c.org/ns/1.0\"><head n=\"4.1\">Background of BERT</head><p>Based on a multi-layer Transformer encoder <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> (The transformer architecture has been ub /1.0\"><head>Visit Embedding</head><p>Similar to BERT, we use a multi-layer Transformer architecture <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> as our visit encoder. The model takes the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: chers' noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, which tightens the privacy bound when the t effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, which represent the state-of-the-art in dif e a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>.</p><p>Section 5 further discusses the relat g the need for supervision. \u2022 We present a new application of the moments accountant technique from <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> for improving the differential-privacy analy (8.19, 10 \u22126 ) for SVHN, respectively with accuracy of 98.00% and 90.66%. In comparison, for MNIST, <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> obtain a looser (8, 10 \u22125 ) privacy bound an y cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, building on previous work <ref type=\"bibr\"  rivacy loss random variable.</p><p>The following properties of the moments accountant are proved in <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>.</p><p>Theorem 1. 1. [Composability] Suppose 00% and 90.66%. These results improve the differential privacy state-of-the-art for these datasets. <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> previously obtained 97% accuracy with a (8,  he large number of parameters prevents the technique from providing a meaningful privacy guarantee. <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> provided stricter bounds on the privacy loss  we keep track of the privacy budget throughout the student's training using the moments accountant <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>. When teachers reach a strong quorum, this  e cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b33\">Shokri &amp; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  black blob). <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bib bibr\" target=\"#b55\">[56]</ref>, as well as squeeze and excitation (SE) block presented by Hu et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The proposed Res2Net module introduces the scale dimensio sily integrate the cardinality dimension <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the SE block <ref type=\"bibr\" target=\"#b24\">[25]</ref> with the proposed Res2Net module.</p></div> <div xmlns=\"ht mension cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> (replace conv with group conv) and SE <ref type=\"bibr\" target=\"#b24\">[25]</ref> blocks.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\" calibrates channel-wise feature responses by explicitly modelling inter-dependencies among channels <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Similar to <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we  y modelling inter-dependencies among channels <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Similar to <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we add the SE block right before the residual connections ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, SE-Net <ref type=\"bibr\" target=\"#b24\">[25]</ref>, bLResNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and DLA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tasets, but even with very different training objectives, including supervised image classification <ref type=\"bibr\" target=\"#b9\">(Krizhevsky et al., 2012)</ref>, unsupervised density learning <ref ty rmula></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">Experimental Setup</head><p>Since <ref type=\"bibr\" target=\"#b9\">Krizhevsky et al. (2012)</ref> won the ImageNet 2012 competition, ther. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rticulatory features <ref type=\"bibr\" target=\"#b7\">[8]</ref> to train HMM based systems. Authors in <ref type=\"bibr\" target=\"#b8\">[9]</ref> used subspace Gaussian mixture model to map phonemes of diff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  obtained from meta-training tasks for a newly seen few-shot task such as intention classification, <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> present a relation classification dataset -Few  one layer convolutional neural networks (CNN). For ease of comparison, its details are the same as <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> proposed. Hierarchical Attention In order to g  with the CNN encoder. For the neural networks based baselines, we use the same hyper parameters as <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> proposed.</p><p>For our hierarchical attention or 5 way 5 shot and 10 way 5 shot settings on FewRel test set.</figDesc><table /><note>* reported by<ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> and \u25c7 reported by<ref type=\"bibr\" target=\"#b3\" as achieved excellent performance in few-shot image classification and few-shot text classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Gao et al., 2019 ttp://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Datasets</head><p>FewRel Few-Shot Relation Classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018)</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y anomalous edges detection, is then highly needed before the data are fed into the following tasks <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Ranshous et a d model which is inspired by <ref type=\"bibr\" target=\"#b3\">[Liu et al., 2017]</ref> and proposed by <ref type=\"bibr\" target=\"#b1\">[Cui et al., 2017]</ref>. In our framework, we construct short state o l></formula><p>GRU is a variant of LSTM network. It is simpler and more effective than LSTM network <ref type=\"bibr\" target=\"#b1\">[Chung et al., 2014]</ref>. GRU can record long-term information, and  oss entropy to distinguish the existing edges and the generated ones. We then take the same idea in <ref type=\"bibr\" target=\"#b1\">[Bordes et al., 2013]</ref> and use marginbased pairwise loss in train ally build the required datasets because the ground-truth for the test phase is difficult to obtain <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e a fixed-work methodology and equalize sample lengths to avoid sample imbalance, similar to FIESTA <ref type=\"bibr\" target=\"#b28\">[29]</ref>. We first find how many instructions each app executes in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecommendations based on collaborative filtering principles <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, they have not been. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: re-ranking model's effectiveness and its efficiency. While IR-specific networks are reasonably fast <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ rms in a single interaction match matrix, followed by softhistogram scoring based on kernel-pooling <ref type=\"bibr\" target=\"#b35\">[36]</ref>. This allows us to explain scoring reasons by probing the   of a hard histogram method and the resulting lack of fine-tuned word representations. Xiong et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> improve on the idea and propose the kernel-pooling techniq  qi, dj)<label>(4)</label></formula><p>Then, we transform each entry in M with a set of RBF-kernels <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Each kernel focuses on a specific similarity range with c  similarity range with center \u00b5 k . The size of all ranges is set by \u03c3. In contrast to Xiong et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> we do not employ an exact match kernel -as contextualized  alysis unfeasible.</p><p>The differences of TK to previous kernel-pooling methods are:</p><p>\u2022 KNRM <ref type=\"bibr\" target=\"#b35\">[36]</ref> uses only word embeddings, therefore a match does not have  improves the robustness of PACRR's pooling strategy with randomization during training.</p><p>KNRM <ref type=\"bibr\" target=\"#b35\">[36]</ref> uses a soft-histogram (differentiable Gaussian kernel func. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sion reduction occurs in the recurrent convolutional neural networks used for semantic segmentation <ref type=\"bibr\" target=\"#b21\">[22]</ref>. As SR methods predict full-sized images, dimension reduct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ovide comparisons with LAPGAN in the supplementary material. Adversarial training. The SRGAN method <ref type=\"bibr\" target=\"#b22\">[20]</ref> optimizes the network using the perceptual loss <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ere predicted conditioned on the memory of the support set. In the object detection task, Hu et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposed to predict the mask weights from box weights. And. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b33\">35,</ref><ref type=\"bibr\" target=\"#b34\">36,</ref><ref type=\"bibr\" target=\"#b41\">43]</ref> usually only consider the click / no click labels or rating \" target=\"#b29\">[31]</ref> or Upper Confidence Bound (UCB) <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b41\">43]</ref> (mainly for Multi-Armed Bandit methods). However, both stra do not model the future reward explicitly (MAB-based works <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b41\">43]</ref>), or use discrete user log to represent state and hence can get=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b30\">32,</ref><ref type=\"bibr\" target=\"#b40\">42,</ref><ref type=\"bibr\" target=\"#b41\">43,</ref><ref type=\"bibr\" target=\"#b49\">51]</ref>, in order to model  age. Some state-of-art methods can not be applied due to their inapplicability to our problem, like <ref type=\"bibr\" target=\"#b41\">[43]</ref> (user graph and item graph is oversized and can not be upd. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b5\">Kario et al., 2003)</ref>, alterations of the brain causing differences in memory and cognition <ref type=\"bibr\" target=\"#b14\">(SJ et al., 2009)</ref>, suppression of the immune system <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oss on labeled data, thus it will face the same vulnerability issue as the standard neural networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and 2) the additional smoothness constraint will exacerba mic regularization technique that proactively simulates the perturbations during the training phase <ref type=\"bibr\" target=\"#b13\">[14]</ref>. It has been empirically shown to be able to stabilize neu ive, and then learn over these adversarial examples by minimizing an additional regularization term <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bib rding the target of the training objective. In supervised learning tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, supervised loss <ref type=\"bibr\" target=\"#b13\">[14]</ref> earning tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, supervised loss <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bib  against perturbations for a wide range of standard classification tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bib o obtain the closedform solution of r g i . Inspired by the linear approximation method proposed in <ref type=\"bibr\" target=\"#b13\">[14]</ref> for standard adversarial training, we also design a linear um players Approximation. For labeled nodes, r \u2032 i can be easily evaluated via linear approximation <ref type=\"bibr\" target=\"#b13\">[14]</ref>, i.e., calculating the gradient of D(f (x i , G| \u0398), \u1ef9i )  aphSGAN) in the training phase. Moreover, the results are consistent with findings in previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ectedness, or determining the relevance of specific agents <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"3\" xml:id=\"foot_2\">Note that the graph defined by Ac in<ref type=\"bibr\" target=\"#b6\">(7)</ref> is directed in order to match exactly the behavior of shifts. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sed on three neural-net models: one to infer prosody, one to infer acoustic features, and an LPCNet <ref type=\"bibr\" target=\"#b25\">[26]</ref> vocoder. The main difference between the single and multi-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"><head n=\"1\">Introduction</head><p>The advent of big data <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: umber of effective extensions are proposed to further improve the detection accuracy, such as R-FCN <ref type=\"bibr\" target=\"#b20\">[17]</ref>, FPN <ref type=\"bibr\" target=\"#b22\">[19]</ref>, Mask R-CNN in some reputable object detectors, such as SSD <ref type=\"bibr\" target=\"#b25\">[22]</ref> and R-FCN <ref type=\"bibr\" target=\"#b20\">[17]</ref>, to elevate speed or/and accuracy.</p><p>In this paper, we of 80.5%, while keeping the real-time speed as SSD300. It even reaches the same accuracy with R-FCN <ref type=\"bibr\" target=\"#b20\">[17]</ref>, the advanced model under the two-stage framework. RFB Net ef> VGG 07+12 73.2 7 Faster <ref type=\"bibr\" target=\"#b14\">[11]</ref> ResNet-101 07+12 76.4 5 R-FCN <ref type=\"bibr\" target=\"#b20\">[17]</ref> ResNet-101 07+12 80.5 9 YOLOv2 544 <ref type=\"bibr\" target which surpasses the baseline score of SSD300* with a large margin, and even equals to that of R-FCN <ref type=\"bibr\" target=\"#b20\">[17]</ref> which employs ResNet-101 as the base net with a larger inp (denoted as RFB Net512-E), while the computational cost only marginally ascends.  21.6 25 [B] R-FCN <ref type=\"bibr\" target=\"#b20\">[17]</ref> 29.9 110 [C] SSD512* <ref type=\"bibr\" target=\"#b25\">[22]</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b19\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b9\">Gehring et al., 2017;</ref><ref type=\"bibr\" target=\"#b36\">Vaswani et al., 2017b;</ref><ref type=\"bibr\" target=\"#b32\">Shen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b1\">[2]</ref> based controller to update the memory, while Working Memory Network (W-MemNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses a multi-head attention <ref type=\"bibr\" target=\"#b25\"  <ref type=\"bibr\" target=\"#b25\">[26]</ref>, which is similar to that used in Working Memory Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Multi-head attention allows the model to jointly attend t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed with the supply and demand for cloud resources. Thus, unlike prior works on bidding optimization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, our model not only explicitly accounts for the interplay b old on 2017 spot data (see Figure <ref type=\"figure\" target=\"#fig_4\">3</ref>).</p><p>The authors of <ref type=\"bibr\" target=\"#b9\">[10]</ref> used a profit-maximization model to understand spot price d y explicitly considering job deadlines <ref type=\"bibr\" target=\"#b15\">[16]</ref>, cost minimization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and task depend n-demand instances and a set B t \u2282 R + of bids from B t = |B t | spot instance requests. Many works <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr f this weak assumption has basis in both previous analyses of spot markets and other auctions (e.g. <ref type=\"bibr\" target=\"#b9\">[10]</ref> assumes bids are drawn from U[\u03c0, \u03c0]) as well in the simple  ese variables by the total number of instances, i.e. define n t = N b t instead of B t ), we follow <ref type=\"bibr\" target=\"#b9\">[10]</ref> and assume that all bids b \u2208 B t are drawn independently fr family. A better strategy would be to consider the collective behavior of the spot prices over time <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which we do in this section by accounting for their tempor bly ill-posed (it tends to \u2212\u221e as b t \u2192 0), and the fact that we have constraints on the state space <ref type=\"bibr\" target=\"#b9\">(10)</ref>. Therefore, we need to resort to algorithms that support mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nalysis to prove that our transductive model is a more general form than existing models (e.g., MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>). \u2022 Efficient and scalable learning algorithms for GATNE h beds networks with multiple views in a single collaborated embedding using attention mechanism. MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref> uses one common embedding and several additional embedding  r \u2208 R s\u00d7d is a trainable transformation matrix.</p><p>Connection with Previous Work. We choose MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>, a recent representative work for MHEN, as the base model   PMNE <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MVE <ref type=\"bibr\" target=\"#b29\">[30]</ref>, MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>. We denote the three methods of PMNE as PMNE(n), PMNE(r) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \u6ee1\u6c5f\u7ea2), Shuidiaogetou(\u6c34\u8c03\u6b4c\u5934), etc .</p><p>Various methods e.g., <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> have been proposed to generate classical Chinese poetry. Howe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: its effectiveness in solving such performance deterioration problems in image analysis applications <ref type=\"bibr\" target=\"#b15\">[16]</ref>, but it is not effective for wireless power control. Speci  images, the geometric property means that adjacent pixels are meaningful to be considered together <ref type=\"bibr\" target=\"#b15\">[16]</ref>. In CNN, a 2D convolution kernel is applied to each patch . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head>B. FIFO Queue Benchmark</head><p>A simple FIFO queue implementation was written based upon <ref type=\"bibr\" target=\"#b7\">[8]</ref>. The queue is based around a ring buffer, with read and writ re operations are needed (i.e. we do not need CAS). For MIPS64 this has been implemented exactly as <ref type=\"bibr\" target=\"#b7\">[8]</ref>, for Mamba a simple modification was made. The FIFO ring buf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d instructions in a special purpose cache.</p><p>The next line and set predictor (NLS) architecture <ref type=\"bibr\" target=\"#b4\">[5]</ref>, implemented in the Alpha 21264 <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mining algorithms or evaluate clusters using data analytics workloads in different aspects, such as <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta pe=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and etc. Narayanan et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> characterize traditional data analytics workloads on singl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vectors into \"bitserial\" versions that apply bitwise operations on packed bit vectors. For instance <ref type=\"bibr\" target=\"#b19\">(Rastegari et al., 2016)</ref> report convolution layers that use 58\u00d7 space, with most papers introducing modifications to the core algorithm or new training techniques. <ref type=\"bibr\" target=\"#b19\">Rastegari et al. (2016)</ref> introduced XNOR-Net, which improved the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction on transformed elements in the point set to approximate a general function defined on the set <ref type=\"bibr\" target=\"#b20\">[21]</ref>:</p><formula xml:id=\"formula_12\">AGGREGATE({x 1 , \u2022 \u2022 \u2022 , . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: zation (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualizati ted languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" ircle class at runtime, the compiler can convert the indirect call to multiple guarded direct calls <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , 1993;</ref><ref type=\"bibr\" target=\"#b13\">Hohenberg &amp; Kohn, 1964)</ref>, the GW approximation <ref type=\"bibr\" target=\"#b11\">(Hedin, 1965)</ref>, and Quantum Monte-Carlo <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se performances than a single convolution due to overfitting. To overcome overfitting, Liang and Hu <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses a recurrent layer that takes feed-forward inputs into is in accordance with the limited success of previous methods using at most three recursions so far <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Among many reasons, two severe problems are vanishing and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ortant corollary of this formulation is that any GNN with neighborhood sampling, such as Graph-SAGE <ref type=\"bibr\" target=\"#b16\">(Hamilton et al., 2017)</ref>, could be considered as its correspondi nd over-smoothing in GNNs. Sampling-based stochastic reduction by random walk neighborhood sampling <ref type=\"bibr\" target=\"#b16\">(Hamilton et al., 2017)</ref> and node sampling <ref type=\"bibr\" targ entation learning literature to reduce the size of input graphs. In GNNs, specifically in GraphSAGE <ref type=\"bibr\" target=\"#b16\">(Hamilton et al., 2017)</ref>, random walk sampling has been deployed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: for some benchmark suites to achieve high performance. The baseline CPR uses a store sets predictor <ref type=\"bibr\" target=\"#b6\">[7]</ref> to predict load-store memory dependences and to issue loads . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uts a human player would have. In contrast to previous work <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> , our approach incorporates 'end-to-end' reinforcement learn as inputs to the neural network by some previous approaches <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> . The main drawback of this type of architecture is that a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b14\">[6]</ref>, and identifying users across multiple online social networks <ref type=\"bibr\" target=\"#b42\">[34]</ref>. However, despite much work that has been done, the proble. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad coverage of the protein universe, as found in the 17929 families of the recent Pfam 32.0 release <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Recent work that applies deep learning is either restrict database is carefully curated, at least 25% of sequences have no experimentally validation function <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and additional experimental functional characterization o of the art models including profile HMMs we use the highly curated Protein families (Pfam) database <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. The 17929 familie otKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Many domains have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: side information) of users and items <ref type=\"bibr\" target=\"#b14\">(Jain &amp; Dhillon, 2013;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2013)</ref>. In IMC, a rating is decomposed by r ij = x i . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target= get=\"#b1\">2]</ref>, image captioning <ref type=\"bibr\" target=\"#b2\">[3]</ref> and speech recognition <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: exhibit value locality, meaning that the same static instruction often produces a predictable value <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In the case of load instructions, it is also possible to pr  type=\"table\" target=\"#tab_0\">I</ref>, to determine how they complement Last Value Prediction (LVP) <ref type=\"bibr\" target=\"#b0\">[1]</ref> Stride Address Prediction (SAP) <ref type=\"bibr\" target=\"#b5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b29\">[30]</ref> and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>In Section 3 we describe our proposed stream fetch  f> shows a block diagram of the trace cache mechanism as proposed by Rotenberg, Benett and Smith in <ref type=\"bibr\" target=\"#b31\">[32]</ref>. The trace cache captures the dynamic instruction stream,  <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and the trace cache architecture using a trace predictor <ref type=\"bibr\" target=\"#b31\">[32]</ref> and selective trace storage <ref type=\"bibr\" target=\"#b28\" rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> is one such high fetch width mechanism, recently implemente. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: preliminary work of a neural influence Diff usion Network (i.e., DiffNet) for social recommendation <ref type=\"bibr\" target=\"#b36\">[37]</ref>. DiffNet models the recursive social diffusion process for \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>.</p><p>In fact, as users play a central role in social pla \">[45]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. On one hand, given the useritem interest graph, NGCF is p  that the higher-order social structure is directly modeled in the recursive user embedding process <ref type=\"bibr\" target=\"#b36\">[37]</ref>. These graph based models showed superior performance comp ary, our main contributions are listed as follows:</p><p>\u2022 Compared to our previous work of DiffNet <ref type=\"bibr\" target=\"#b36\">[37]</ref>, we revisit the social recommendation problem as predictin d social recommendation model, DiffNet, for modeling the social diffusion process in recommendation <ref type=\"bibr\" target=\"#b36\">[37]</ref>. DiffNet advances classical embedding based models with ca at the up to K-th order social network structure is injected into the social recommendation process <ref type=\"bibr\" target=\"#b36\">[37]</ref>. In this part, we propose DiffNet++, an enhanced model of  t, in order to transform this model for the recommendation task. For our proposed models of DiffNet <ref type=\"bibr\" target=\"#b36\">[37]</ref> and DiffNet++, since both models are flexible and could be 8]</ref> and Normalized Discounted Cummulative Gain (NDCG) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Specifically, HR measures the percentage of hit items in  formance with large itemset, similar as many other works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, to evaluate the performance, for each user, we randomly s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: m . On March 7 , 2012 , he was named one of five finalists for the Naismith Award , which is 0.064  <ref type=\"bibr\" target=\"#b0\">(Baevski and Auli, 2019;</ref><ref type=\"bibr\" target=\"#b19\">Radford e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  b \u2208 R d\u00d72d is a shared representation of various relations. For all the experiment, we use PyTorch <ref type=\"bibr\" target=\"#b34\">[35]</ref> and PyTorch geometric <ref type=\"bibr\" target=\"#b10\">[11]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  machine learning approaches, a number of differential privacy based methods have been proposed. In <ref type=\"bibr\" target=\"#b21\">[22]</ref>, a differentially-private stochastic gradient descent algo ective deep learning model that can benefit from the data of all users. Our work is most related to <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bib \"bibr\" target=\"#b29\">[30]</ref>, but is quite different in several aspects. The proposed schemes in <ref type=\"bibr\" target=\"#b21\">[22]</ref> and <ref type=\"bibr\" target=\"#b22\">[23]</ref> were not des l privacy which can hide the existence of participants, and uses the moment accountant technique in <ref type=\"bibr\" target=\"#b21\">[22]</ref> to track the privacy loss. However, both methods did not c gression task.</p><p>Since the approaches proposed in <ref type=\"bibr\" target=\"#b22\">[23]</ref> and <ref type=\"bibr\" target=\"#b21\">[22]</ref> are not specially designed for collaborative learning, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: (GGNN) <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref type=\"bibr\" target=\"#b7\">(Lample et al., 2016)</ref> (BiLSTM-CRF), our model learns a weighted  are respec-tively 39.70%, 44.75%, 36.10% and 46.05%.</p><p>Models for Comparison. We use BiLSTM-CRF <ref type=\"bibr\" target=\"#b7\">(Lample et al., 2016)</ref> with character+bigram embedding without us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pretrain the word embeddings -they are learned from scratch during training. We train using Adagrad <ref type=\"bibr\" target=\"#b5\">(Duchi et al., 2011)</ref> with learning rate 0.15 and an initial accu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sers' click behaviors that provide valuable relevance signal <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>; in another application of recommender systems, users and ite. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ct and have high intersubject variability. To learn personalized models for each student, we follow <ref type=\"bibr\" target=\"#b3\">(Jaques et al., 2017)</ref> and use a Multitask approach which compris. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and it's detections are publicly available.</p><p>We use the methodology and tools of Hoiem et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> For each category at test time we look at the top N predic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ised graph clustering is often an extremely useful end-goal in itself -whether for data exploration <ref type=\"bibr\" target=\"#b44\">[45]</ref>, visualization <ref type=\"bibr\" target=\"#b10\">[11,</ref><r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly discussed in one-class collaborative filtering (OCCF) <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Given user u, I + u = {i \u2208 I|r ui = 1} is the set of item imilarity matrices between users and items to predict drug-target interaction. Moreover, Yao et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed dual regularization by combining the weighted-and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 0\"><head n=\"1\">Introduction</head><p>Bidirectional Encoder Representations from Transformers (BERT) <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> has become enormously popular and proven t RT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>.</p><p>\u2022 As there are so many possibilitie  settings and data statistics in different task. \u2020 represents the dataset was also evaluated by BERT<ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>. \u2021 represents the dataset was also evaluat , which is beneficial for the researcher to design more powerful models based on them.</p><p>Before <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> releasing BERT with whole word masking, <re <ref type=\"foot\" target=\"#foot_1\">3</ref> , and pre-processed with WikiExtractor.py as suggested by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>  <ref type=\"bibr\" target=\"#b8\">Devlin et al sed with WikiExtractor.py as suggested by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>  <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, for computation efficiency and learning lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sed for assessment in the educational contexts of games, to predict outcomes based on game activity <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and to predict responses to questions of various skills giv isite courses for other courses in the same department as the target course as a filter before step <ref type=\"bibr\" target=\"#b1\">(2)</ref>. For example, assuming that Table <ref type=\"table\" target=\"  rigorousness of evaluation, we applied several filters to the enumerated input courses before step <ref type=\"bibr\" target=\"#b1\">(2)</ref>, which are listed as follows, with the first two filters bei. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (Ghanea et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Huang and Zhang, 2012)</ref>, LiDAR-based <ref type=\"bibr\" target=\"#b6\">(Du et al., 2017;</ref><ref type=\"bibr\" target=\"#b29\">Mongus et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  larger model size.</p><p>Balancing effectiveness and efficiency has been a line of recent research <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" ta us on database-related methods, such as pruning and indexing to speed-up retrieval of related items <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, using fast models. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b20\">Yang et al. 2016)</ref> including convolutional neural networks, recursive neural network <ref type=\"bibr\" target=\"#b3\">(Ebrahimi and Dou 2015;</ref><ref type=\"bibr\" target=\"#b10\">Liu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and LSTMbased approaches <ref type=\"bibr\" target=\"#b30\">[31]</ref>. These approaches all consider relations lying in a single. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ently flat and do not learn hierarchical representations of graphs. On one hand, it demonstrates in <ref type=\"bibr\" target=\"#b19\">[20]</ref> that hierarchical representations of graphs can be combine ng GNNs with different clustering processes. In particular, the recently proposed approach DIFFPOOL <ref type=\"bibr\" target=\"#b19\">[20]</ref>, a differentiable graph pooling module that can generate h ing to be effective in graph classification tasks, in addition to a user's individual embedding. In <ref type=\"bibr\" target=\"#b19\">[20]</ref>, authors make some efforts in effectively co-training two  world e-commerce tasks of such large scale, including <ref type=\"bibr\" target=\"#b29\">[30]</ref> and <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Our baseline algorithms are as follows:</p><p>\u2022 CGNN: A g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nformation retrieval to capture the exact and soft matches between a query and a candidate document <ref type=\"bibr\" target=\"#b10\">[Xiong et al., 2017]</ref>. Specifically, we apply the basic BERT uni ors are disordered and independent from each other. Thus we adopt a RBF kernel aggregation function <ref type=\"bibr\" target=\"#b10\">[Xiong et al., 2017]</ref> to extract features about the accumulation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ence factor k instead of all the neighbors. In this work, we apply a neighborhood routing algorithm <ref type=\"bibr\" target=\"#b18\">(Ma et al., 2019a)</ref> to identify the subset of neighboring news t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing performance for clustering, SSL, active learning, etc. <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. A simple yet effective way to overcome the limitations is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Usage-based pricing can affect overall demand levels, but does not even out short-term fluctuations <ref type=\"bibr\" target=\"#b11\">[13]</ref>. To manage these fluctuations in demand for a fixed amount re have a shorter expected running time. Job interruptibility. We can use the expected running time <ref type=\"bibr\" target=\"#b11\">(13)</ref> to observe the effect of the recovery time parameter, t r  s feasible at any price.</p><p>The optimal bid price. We can now multiply the expected running time <ref type=\"bibr\" target=\"#b11\">(13)</ref> with the expected spot price <ref type=\"bibr\" target=\"#b7\" o <ref type=\"bibr\" target=\"#b13\">(15)</ref>.</p><p>We now observe that the expected running time in <ref type=\"bibr\" target=\"#b11\">(13)</ref> decreases with the bid price, while the expected spot pric very, execution, and overhead times. Hence, we can extend the result for a single persistent bid in <ref type=\"bibr\" target=\"#b11\">(13)</ref> as</p><formula xml:id=\"formula_32\">M i=1 T i F \u03c0 (p) = t s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed to systems with mono-lingual features. Other approaches <ref type=\"bibr\" target=\"#b17\">[17,</ref><ref type=\"bibr\" target=\"#b18\">18]</ref> constructed a shared language independent phone set, which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the ghosting effects <ref type=\"bibr\" target=\"#b21\">[22]</ref> and the Laplacian data fidelity term <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Other methods in this area remove reflections by virtue of  >[17]</ref> 0.801 0.829 21.77 WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=\"bibr\" target=\"#b4\">[5]</re leGAN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and LB14 <r rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> with fixed coefficients.</p><p>Separator (S). We perform a di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b4\">5]</ref> and universal (i.e. image-agnostic) attacks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  reduce the memory size and enhance efficiency. Second, the pruning and sharing method presented in <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> removes or bind. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b5\">[6]</ref> adopts a random vertex-cut method and two greedy variants for GP.</p><p>GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref> provides some heuristics, such as the grid-based constrained sting vertex-cut methods, such as random method in PowerGraph and grid-based method in GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref>, cannot make effective use of the powerlaw distribution to a \"bibr\" target=\"#b5\">[6]</ref> and the grid-based constrained solution (called Grid) of GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref> are adopted as baselines. Our analysis is based on randomiza. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eters, and (3) knowledge distillation (KD).</p><p>First, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref> proposed the binary encoding of model parameters. Under th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b17\">[18]</ref> such as U-Net <ref type=\"bibr\" target=\"#b23\">[24]</ref>, DeepMedic <ref type=\"bibr\" target=\"#b12\">[13]</ref> and holistically nested networks <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nowledge can be captured for recovering the high-frequency details in HR images.</p><p>Recent works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> have successfully  tional neural network <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Among them, the CNN-based approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> have recently set  ork, making it easy to train. In addition, in previous works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only high-level features at top layers were used in the re formance. Instead of using interpolation for upscaling as in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, recent studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref ere studied and compared in our work. As in previous methods <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only the feature maps at the top layer are used as input f  to learn an end-to-end mapping for SR. Subsequently, a deep network with 20 layers was proposed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> to improve the reconstruction accuracy of CNN. The residua on accuracy of CNN. The residuals between the HR images and the interpolated LR images were used in <ref type=\"bibr\" target=\"#b10\">[11]</ref> to speedup the converging speed in training and also to im yers</head><p>In previous SR methods such as SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> and VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, bicubic interpolation is used to upscale LR images to the plus <ref type=\"bibr\" target=\"#b23\">[24]</ref>, SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> and DRCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The im  <ref type=\"bibr\" target=\"#b1\">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> with 20-layer CNN. It should be mentioned that the most si  Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref> SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> DRCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>   In additi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. The basic idea behind node embedding approaches is to use  get=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. These methods also bear close relationships to more classi et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>) the objective function is invariant to orthogonal transfor get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion. Such attention based approaches have achieved promising performances on a variety of NLP tasks <ref type=\"bibr\" target=\"#b18\">(Luong et al., 2015;</ref><ref type=\"bibr\" target=\"#b11\">Kumar et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, large Transformer based models <ref type=\"bibr\" target=\"#b31\">[32]</ref>, such as BERT <ref type=\"bibr\" target=\"#b5\">[6]</ref>, sho il: markus.zlabinger@tuwien.ac.at 3 TU Wien, Austria, email: hanbury@ifs.tuwien.ac.at former layers <ref type=\"bibr\" target=\"#b31\">[32]</ref> (we evaluate up to three) can effectively contextualize qu  learning -a local contextualization, fixed by the n-gram size hyperparameter.</p><p>Vaswani et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref> proposed the Transformer architecture in the context of la intensity of the contextualization. We calculate the context(t1:n) with a set of Transformer layers <ref type=\"bibr\" target=\"#b31\">[32]</ref>. First, the input sequence is fused with a positional enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ley 2016)</ref>, the powers of a transition matrix is employed to define the neighborhood of nodes. <ref type=\"bibr\" target=\"#b13\">(Monti et al. 2017)</ref> uses the local path operators in the form o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b25\">(Lin et al., 2017;</ref><ref type=\"bibr\" target=\"#b13\">Jiang et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bengio et al., 2009)</ref>. From the perspective of loss functions, ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d on the results recommended users with similar interests on the Twitter network to the target user <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Ramage et al. improved the accuracy of LDA based user recom. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type=\"bibr\" target=\"#b25\">(Peters et al., 2018)</ref>, GPT-2 <ref type=\"bibr\" target=\"#b26\">(Ra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lection can be modeled as a hierarchical graph, in which a document is regarded as a graph-of-words <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and then a set of documents are interconnected via the ci. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: representation to implement optimizations, e.g., auto differentiation and dynamic memory management <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target tional Graphs</head><p>Computational graphs are a common way to represent programs in DL frameworks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">Related Work</head><p>Deep learning frameworks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target on graph DSLs are a typical way to represent and perform high-level optimizations. Tensorflow's XLA <ref type=\"bibr\" target=\"#b2\">[3]</ref> and the recently introduced DLVM <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: kolov et al., 2013)</ref>, ELMo <ref type=\"bibr\" target=\"#b21\">(Peters et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b3\">(Devlin et al., 2019)</ref> are trained and tested mainly on datasets  )</ref> uses machine translation to embed context information into word representations.</p><p>BERT <ref type=\"bibr\" target=\"#b3\">(Devlin et al., 2019)</ref> is a contextualized word representation mo r\" target=\"#b10\">(Krallinger et al., 2017)</ref>. Due to the space limitations, we refer readers to <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref> for a more detailed description of BERT.</p pora were used for pre-training, we initialized BioBERT with the pre-trained BERT model provided by <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref>. We define BioBERT as a language representa han an hour as the size of the training data is much smaller than that of the training data used by <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref>. On the other hand, it takes more than 20 e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b0\">[1]</ref>. Deep recurrent neural networks with long short-term memory (LSTM) cells <ref type=\"bibr\" target=\"#b1\">[2]</ref> have recently been shown to be ideal for this task <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref>.</p><p>In this paper, we propose a novel Bayesian graph con et=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref>.In this paper, we propose a novel Bayesian graph convolutio derived from noisy data. Addressing the uncertainty on the underlying graph was first considered in <ref type=\"bibr\" target=\"#b37\">[39]</ref> for the problem of node classification. In this work, the  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Bayesian Graph Convolutional Networks</head><p>In <ref type=\"bibr\" target=\"#b37\">[39]</ref>, to alleviate the effect of the potential noise in the obs G \ud835\udc5c\ud835\udc4f\ud835\udc60 ) \ud835\udc5d (G|D, G \ud835\udc5c\ud835\udc4f\ud835\udc60 ) \ud835\udc51 G \ud835\udc51\ud835\udf40 .<label>(1)</label></formula><p>Zhang et al. presented this model in <ref type=\"bibr\" target=\"#b37\">[39]</ref> for the node classification task. In their case, the value. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: roposed solutions for different aspects of the problem <ref type=\"bibr\" target=\"#b119\">[120]</ref>, <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref>. In particu arget=\"#b121\">[122]</ref>. In particular, sampling results have been generalized to directed graphs <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref> and to othe ng set selection from an experiment design perspective <ref type=\"bibr\" target=\"#b123\">[124]</ref>, <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref> setting as  ge-scale graphs. Some techniques require computing and storing the first K basis vectors of the GFT <ref type=\"bibr\" target=\"#b120\">[121]</ref>. For larger graph sizes, where this may not be practical  always lead to performance comparable to those of more complex greedy optimization methods such as <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref>.</p><p>Give. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: comes unfeasible. A recent relaxation in language modeling <ref type=\"bibr\" target=\"#b28\">[27,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref> turns the prediction problem on its head. First, instead of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: trated its effectiveness on folding a single protein chain <ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t l information and pairwise potential between any two columns in a paired MSA. See our previous work <ref type=\"bibr\" target=\"#b19\">[20]</ref> for details.</p></div> <div xmlns=\"http://www.tei-c.org/ns atenated sequences and our training proteins. Please see our previous work on training our DL model <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> 42]</ref>.</p><p>We have developed a deep learning (DL) method for intra-protein contact prediction <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, which greatly out  which predicts the probability of any two residues forming a contact. Please see our previous work <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> for a detailed des. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tures in recent papers <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Such deep networks reach top competitive results in the hi riginally used for a face verification task. We also evaluate DCN network architecture described in <ref type=\"bibr\" target=\"#b18\">[19]</ref> without any pre-training. The faces are resized to a fixed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  using cross-entropy <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. This architecture can also be used as a \"bottleneck\" featu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: //www.tei-c.org/ns/1.0\"><head>Introduction</head><p>Large-scale knowledge graphs (KGs) such as YAGO <ref type=\"bibr\" target=\"#b10\">(Suchanek, Kasneci, and Weikum 2007)</ref>, NELL <ref type=\"bibr\" tar e=\"bibr\" target=\"#b0\">(Carlson et al. 2010), and</ref><ref type=\"bibr\">Wikidata (Vrande\u010di\u0107 and</ref><ref type=\"bibr\" target=\"#b10\">Kr\u00f6tzsch 2014)</ref> usually represent facts in the form of relations =\"bibr\" target=\"#b9\">Socher et al. 2013;</ref><ref type=\"bibr\" target=\"#b11\">Yang et al. 2015;</ref><ref type=\"bibr\" target=\"#b10\">Trouillon et al. 2016;</ref><ref type=\"bibr\" target=\"#b8\">Schlichtkru sume available, sufficient training instances for all relations.</p><p>In light of the above issue, <ref type=\"bibr\" target=\"#b10\">Xiong et al. (2018)</ref> proposed GMatching which introduces a local )</ref> have been proposed to learn entity embeddings by using relational information, Xiong et al. <ref type=\"bibr\" target=\"#b10\">(Xiong et al. 2018</ref>) demonstrated that explicitly encoding graph spectively. In order to measure the similarity between two vectors, we employ a recurrent processor <ref type=\"bibr\" target=\"#b10\">(Vinyals et al. 2016</ref>) f \u00b5 to perform multiple steps matching. T. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: The field that gathers all these questions under a common umbrella is graph signal processing (GSP) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>While the pr of the prior work that is more directly connected and in the spirit of signal processing on graphs, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We organize the di /ref>. We organize the discussion along two main lines; some parts of the exposition follow closely <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>.</p><p>1) From al te the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>. This choice is s /ref> studies time signals. Graph signal processing (GSP)<ref type=\"foot\" target=\"#foot_0\">1</ref>  <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t erpretation of DSP can be extended to develop a linear time shift invariant Graph Signal Processing <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Consider now a graph signal s \u2208 C N , where the entries of  lized Laplacian L = D \u22121/2 LD \u22121/2 .</formula><p>The adjacency matrix A can be adopted as the shift <ref type=\"bibr\" target=\"#b1\">[2]</ref> for this general graph. Other choices have been proposed, in nt if it commutes with the shift,</p><formula xml:id=\"formula_18\">AH = HA.</formula><p>As proven in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, if the characteristic polynomial p A (z) and the minimum po aph filtering to two graph Fourier transforms and a pointwise multiplication in the spectral domain <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With a notion of frequency we can now consider the GSP equi Section II-C). If these conditions do not hold, the Jordan canonical form is used to obtain the GFT <ref type=\"bibr\" target=\"#b1\">[2]</ref>, but this is well known to be a numerically unstable procedu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rk approaches have shown promising results on many sentence/document-level sentiment classification <ref type=\"bibr\" target=\"#b21\">(Socher et al., 2013b;</ref><ref type=\"bibr\" target=\"#b25\">Tang et al ntative compositional approaches to learn sentence representation include recursive neural networks <ref type=\"bibr\" target=\"#b21\">(Socher et al., 2013b;</ref><ref type=\"bibr\" target=\"#b7\">Irsoy and C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Conventional studies concentrate on the area of multilingual r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> for a long time, these researches are commonly limited to mak uage dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM <ref type=\"bibr\" target=\"#b4\">[5]</ref> further explores long short-term memory (LSTM) <ref type=\"bi nder the condition of language information being known during training. A comparison with SHL-MLSTM <ref type=\"bibr\" target=\"#b4\">[5]</ref> with residual learning is investigated on CALL-HOME datasets .tei-c.org/ns/1.0\"><head n=\"4.4.\">Results</head><p>The baseline systems come from our previous work <ref type=\"bibr\" target=\"#b4\">[5]</ref> and all results are summarized in Table <ref type=\"table\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: micro-video. In terms of acoustic modality, we separate audio tracks with FFmpeg 6 and adopt VGGish <ref type=\"bibr\" target=\"#b19\">[20]</ref> to learn the acoustic deep learning features. For textual . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b45\">(Shen et al., 2018)</ref>, and the Semantic Scholar literature graph <ref type=\"bibr\" target=\"#b7\">(Ammar et al., 2018)</ref>, have had widespread application in bibliom Constructing the corpus S2ORC is constructed using data from the Semantic Scholar literature corpus <ref type=\"bibr\" target=\"#b7\">(Ammar et al., 2018)</ref>. Papers in Semantic Scholar are derived fro papers included in S2ORC are a curated subset of the papers in the Semantic Scholar literature graph<ref type=\"bibr\" target=\"#b7\">(Ammar et al., 2018)</ref> that focuses only on English-language paper e=\"bibr\" target=\"#b54\">(Yu et al., 2012;</ref><ref type=\"bibr\" target=\"#b29\">Liu et al., 2015;</ref><ref type=\"bibr\" target=\"#b7\">Bhagavatula et al., 2018)</ref> citation recommendation. Among documen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[36]</ref> (e) EDSR <ref type=\"bibr\" target=\"#b36\">[36]</ref> (f) DBPN <ref type=\"bibr\" target=\"#b20\">[20]</ref> (g) RDN <ref type=\"bibr\" target=\"#b6\">[6]</ref> (h) Ours</ with more than 16 layers based on residual learning. To further improve the performance, Lim et al. <ref type=\"bibr\" target=\"#b20\">[20]</ref> proposed a very deep and wide network EDSR by stacking mod  <ref type=\"bibr\" target=\"#b14\">[14]</ref>, Mem-Net <ref type=\"bibr\" target=\"#b30\">[30]</ref>, EDSR <ref type=\"bibr\" target=\"#b20\">[20]</ref>, SRMD <ref type=\"bibr\" target=\"#b36\">[36]</ref>, NLRN <ref module, and reconstruction part. Given I LR and I SR as the input and output of SAN. As explored in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>, we apply only one ndencies.</p><p>It has been verified that stacking residual blocks is helpful to form a deep CNN in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>. However, very dee filter are set as 3 \u00d7 3 and C =6 4 , respectively. For upscale part H \u2191 (\u2022), we follow the works in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref> and apply ESPCNN < ments</head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">Setup</head><p>Following <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" targ <ref type=\"bibr\" target=\"#b39\">[39]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" ta 30\">30]</ref>, L 1 <ref type=\"bibr\" target=\"#b14\">[14,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>, perceptual losses . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  two major end-to-end ASR implementations based on both connectionist temporal classification (CTC) <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: abels from 460 hours of clean speech.</p><p>We also evaluate two methods for pseudo-label filtering <ref type=\"bibr\" target=\"#b3\">[4]</ref> tailored to the mistakes often encountered with sequence-to-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ut regard to their distance in the input or output sequences <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In all but a few cases <ref type=\"bibr\" target=\"#b21\">[22]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  has not interacted before. We use the widely-used protocols <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>: Precision@K, Recall@K, and NDCG@K to evaluate the performa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ule</head><p>This module is a bi-direction recurrent neural network with self-attention as shown in <ref type=\"bibr\" target=\"#b10\">Lin et al. (2017)</ref>. Given an input text x = (w 1 , w 2 , ..., w . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  image as points in it. Pixels representing the same object naturally cluster in the spectral space <ref type=\"bibr\" target=\"#b3\">[4]</ref> . This property provides us an opportunity to segment pixels. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d there is a sizable literature on their construction (e.g., <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>).</p><p>However, the main focus of such work has been on mini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st programs for manufacturing testing of microprocessors ( <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>). These technique. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Normalization (GN) as a simple alternative to BN. We notice that many classical features like SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref> and HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref> are grou ><p>The channels of visual representations are not entirely independent. Classical features of SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and GIST <  more abstract and their behaviors are not as intuitive. However, in addition to orientations (SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref>, or <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t encoder state is simply added by z and then is consumed by a location-sensitive attention network <ref type=\"bibr\" target=\"#b17\">[18]</ref> which converts encoded sequence to a fixed-length context . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s h a 1 _ b a s e 6 4 = \" e A Z 8 7  This paper presents progress in diffusion probabilistic models <ref type=\"bibr\" target=\"#b49\">[50]</ref>. A diffusion probabilistic model (which we will call a \"di onals in p \u03b8 (x t\u22121 |x t ), because both processes have the same functional form when \u03b2 t are small <ref type=\"bibr\" target=\"#b49\">[50]</ref>. A notable property of the forward process is that it admi ing to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance <ref type=\"bibr\" target=\"#b49\">[50]</ref>.</p><p>Second, to represent the mean \u00b5 \u03b8 (x t , t), we pro orithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. <ref type=\"bibr\" target=\"#b49\">[50]</ref>, not yet as a practical compression system. </p></div> <di educed variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. <ref type=\"bibr\" target=\"#b49\">[50]</ref>; we include it here only for completeness.</p><formula xml ments so that the number of neural network evaluations needed during sampling matches previous work <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. We set the forwar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b36\">[37]</ref> to tackle the speaker variability issue. As shown in our recent paper <ref type=\"bibr\" target=\"#b0\">[1]</ref>, VTLP is especially useful when the speaker variability in t  robustness against speaker variability, we apply an onthe-fly VTLP algorithm on the input waveform <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The warping factor is generated randomly for each input utt e frequency, and K is the DFT size. More details about our VTLP algorithm is described in detail in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The acoustic simulator in Fig. <ref type=\"figure\">1</ref> i <ref type=\"bibr\" target=\"#b49\">[50]</ref>. In the example server, we ran the VTLP data augmentation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, acoustic simulator <ref type=\"bibr\" target=\"#b1\">[2]</ref>  c). NBF, VTLP, and AS stand for Neural Beam Former (NBF) [59], Vocal Tract LengthPerturbation (VTLP)<ref type=\"bibr\" target=\"#b0\">[1]</ref> , and Acoustics Simulator (AS)<ref type=\"bibr\" target=\"#b1\"> MFCC features, we use the power mel filterbank energies, since it shows slightly better performance <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Motivated by our pr own scoring and Inverse Text Normalization (ITN) modules, support for power mel filterbank features <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, etc. We have tried . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ss this, some recent studies have proposed greedy methods <ref type=\"bibr\">[Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Z\u00fcgner et al., 2018]</ref> to attack the graph-based deep learning sy tures in pixel color space. However, recent explorations in the graph adversarial attack techniques <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Dai et al.,  ce, this process can be trivial as many statistics can be pre-computed or re-computed incrementally <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>.</p><p>Algorithm 1: IG-JSMA -Integrated G prediction score for its ground-truth class. The adversarial graph was constructed by using nettack <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>. Without any defense, the target node is  ifficult to attack than those with less neighbors. This is also consistent with the observations in <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref> that nodes with higher degrees have highe ber of neighbors which have low similarity scores to the target nodes. This also stands for nettack <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>. For example, we enable both feature and   contribute much to the predictive capabilities of GCN models but introduce unnecessary complexity. <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref> uses a simplified surrogate model to achi y, IG-JSMA is quite stable as the classification margins have much less variance. Just as stated in <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>, the vanilla gradient-based methods, such. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  art theory using relatively small and controlled datasets <ref type=\"bibr\" target=\"#b21\">[14,</ref><ref type=\"bibr\" target=\"#b22\">15]</ref>, while recent works have started to analyze the sentiments . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ian dropout <ref type=\"bibr\" target=\"#b24\">(Wang &amp; Manning, 2013)</ref> and variational dropout <ref type=\"bibr\" target=\"#b11\">(Kingma et al., 2015)</ref> use other random masks to improve dropout. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctly from characters have made rapid progress in recent years, and achieved very high voice quality <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe s the latent variable does in VAE. Therefore, in this paper we intend to introduce VAE to Tacotron2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, a state-of-the-art end-to-end speech synthesis model, to le ate before add operation. The attention module and decoder have the same architecture as Tacotron 2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Then, WaveNet <ref type=\"bibr\" target=\"#b18\">[19]</ref> voc  usually neutral speaking style, is approaching the extreme quality close to human expert recording <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, the interests in expr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b14\">Dosovitskiy et al., 2014;</ref><ref type=\"bibr\" target=\"#b41\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>.</p><p>In this work, we introduce a simple rget=\"#b33\">(Krizhevsky et al., 2012;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>, it has not been considered as a systemati br\" target=\"#b22\">H\u00e9naff et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>. However, it is not clear if the success o tation learning methods:</p><p>\u2022 DIM/AMDIM <ref type=\"bibr\" target=\"#b24\">(Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref> achieve global-to-local/local-to-neighbor  gure\" target=\"#fig_0\">1</ref>), but it is also simpler, requiring neither specialized architectures <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et a ibr\" target=\"#b20\">(Zhang et al., 2016;</ref><ref type=\"bibr\" target=\"#b41\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Kolesnikov e n is useful for self-supervised learning <ref type=\"bibr\" target=\"#b11\">(Doersch et al., 2015;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et al the default nonlinear projection with one additional hidden layer (and ReLU activation), similar to <ref type=\"bibr\" target=\"#b1\">Bachman et al. (2019)</ref>. We observe that a nonlinear projection is tch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2019)</ref>, which achieves 91.2% with a model 25\u00d7 la. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b24\">[26]</ref>, Walktrap <ref type=\"bibr\" target=\"#b27\">[29]</ref>, Louvain <ref type=\"bibr\" target=\"#b4\">[6]</ref>, Greedy <ref type=\"bibr\" target=\"#b5\">[7]</ref>, Infomap <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd efficiently segmenting large point clouds <ref type=\"bibr\" target=\"#b45\">[Wang et al., 2018</ref><ref type=\"bibr\" target=\"#b29\">, Li et al., 2019b]</ref>. Recent works have looked at frameworks to  20\">, Huang et al., 2017</ref><ref type=\"bibr\" target=\"#b53\">, Yu and Koltun, 2016]</ref>, DeepGCNs <ref type=\"bibr\" target=\"#b29\">[Li et al., 2019b]</ref> propose to train very deep GCNs (56 layers)   to be either SoftMax_Agg \u03b2 (\u2022) or PowerMean_Agg p (\u2022).</p><p>Better Residual Connections. DeepGCNs <ref type=\"bibr\" target=\"#b29\">[Li et al., 2019b]</ref> show residual connections <ref type=\"bibr\" t ., 2016]</ref> is used in every layer before the activation function ReLU.</p><p>ResGCN. Similar to <ref type=\"bibr\" target=\"#b29\">Li et al. [2019b]</ref>, we construct ResGCN by adding residual conne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  a bi-level optimization framework. Relatively few works consider meta-learning with GNNs. Meta-GNN <ref type=\"bibr\" target=\"#b58\">[59]</ref> uses meta-learning for few-shot node classification, and M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce the L1 hybrid is used to filter easily predicted highly biased branches, a confidence estimator <ref type=\"bibr\" target=\"#b13\">[14]</ref> indicates whether the branch is more difficult to predict  ch prediction process involve correlating the actual branch register values with the branch outcome <ref type=\"bibr\" target=\"#b13\">[14]</ref> using a conventional value predictor. The authors of the s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, Graph-SAGE <ref type=\"bibr\" target=\"#b5\">(Hamilton, Ying, and Leskovec, 2017)</ref>, and AS-GCN <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2018)</ref>. For the evaluation, we apply a large-scale br\" target=\"#b5\">(Hamilton, Ying, and Leskovec, 2017)</ref> and the ayerwise sampling method AS-GCN <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2018)</ref>. Differ from these methods, our BGNN has be Leskovec, 2017)</ref> directly samples neighbor nodes, while layer-wise sampling method like AS-GCN <ref type=\"bibr\" target=\"#b7\">(Huang et al., 2018)</ref> uses adaptive sampling to fix the number of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: variety of industries including land inventory, vegetation monitoring, and environmental assessment <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In particular, extraction of manufactured features such as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arge in order to express the huge number of interest profiles at Tmall. Deep Interest Network (DIN) <ref type=\"bibr\" target=\"#b29\">[30]</ref> makes the user representation vary over different items wi \">[3]</ref>. Besides the industrial applications proposed by <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, various types of deep models have gained significant atten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  give a brief introduction to GNNs, and one can refer to <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> for a more detailed information. GNNs deal with learning p able.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Gra here u \u2208 N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>: It uses the MLP and sum pooling as the aggregation and co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng to a single point, triplet loss enables documents with the same identity to reside on a manifold <ref type=\"bibr\" target=\"#b28\">[20]</ref>, and at the same time maintain a distance from other docum. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: side language model has been used to rescore or rerank a list of candidate translations (see, e.g., <ref type=\"bibr\" target=\"#b26\">Schwenk et al., 2006)</ref>.</p><p>Although the above approaches were. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: In the context of maintaining multiple versions of the data in a B + -tree, it has been proposed in <ref type=\"bibr\" target=\"#b3\">[3]</ref> to use, where possible, the free space in nodes. Other studi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2017;</ref><ref type=\"bibr\" target=\"#b39\">Veli\u010dkovi\u0107 et al., 2018;</ref><ref type=\"bibr\">2019;</ref><ref type=\"bibr\" target=\"#b33\">Qu et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Gao &amp; Ji, 20 rs and the edge weights between them correspond to the degree of trust between the users. Following <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>, we treat edges with weights greater than 3 a t state-of-the-art methods GAT <ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref> and Graph U-Net <ref type=\"bibr\" target=\"#b13 the results of GraphMix(GCN) are comparable with the recently proposed state-of-the-art method GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>. Since GraphMix consists of various component  \u00b1 0.3% GraphScan <ref type=\"bibr\" target=\"#b11\">(Ding et al., 2018)</ref> 83.3 \u00b11.3 73.1\u00b11.8 -GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref> 83.7% 73.1% 81.8% DisenGCN <ref type=\"bibr\" t ; Welling, 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>, among others. This architecture has one hidd hence much of the recent attention is dedicated to proposing architectural changes to these methods <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Gao &amp; Ji, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  give a brief introduction to GNNs, and one can refer to <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> for a more detailed information. GNNs deal with learning p able.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Gra here u \u2208 N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>: It uses the MLP and sum pooling as the aggregation and co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Two alternative sampling implementations are commonly used nts in the program <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Several techniques have been proposed to warmup cache stat target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. The MTR adds multiprocessor and directory support to these itectural state is updated while fast-forwarding and then used to initialize the detailed simulator <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Microarchitectural state is less amenable to checkpointin ed simulation needed to achieve small error rates with the desired confidence, the SMARTS framework <ref type=\"bibr\" target=\"#b26\">[27]</ref> recently proposed functional warming, which simulates larg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dustry practitioners have turned to search-based compilation <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b27\">29,</ref><ref type=\"bibr\" targ nually-written assembly code on large matrix multiplications <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b43\">45]</ref>, as the code has bee tation definition of matrix multiplication. ple compiler techniques have been introduced (e.g., TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, Halide <ref type=\"bibr\" target=\"#b36\">[38]</ref>, Tensor C ically, the compiler partitions the large computational graph of a DNN into several small subgraphs <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This partition has a negligible effect on the performance  arch and learned cost model performs the best among them, which is also used in our evaluation. TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref> utilizes a similar scheduling language and includes a templ  search for GPU code automatically, but it is not yet meant to be used for compute-bounded problems <ref type=\"bibr\" target=\"#b9\">[10]</ref>. It cannot outperform TVM on operators like conv2d and matm f type=\"bibr\" target=\"#b9\">[10]</ref>. It cannot outperform TVM on operators like conv2d and matmul <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>. This is because of graph level include layout optimizations <ref type=\"bibr\" target=\"#b27\">[29]</ref>, operator fusion <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b33\">35]</ref>, constant folding <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  attack-agnostic manner, except a few touches on denoising <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and obfuscating gradients <ref type=\"bibr\" target=\"#b10\">[1 le FPD can circumvent the structure-replaced white-box attack. Our proposal is partially related to <ref type=\"bibr\" target=\"#b28\">[29]</ref>, as the denoising layers in our FPD are inspired by their  ng layers in our FPD are inspired by their feature denoising approach. Nevertheless, different from <ref type=\"bibr\" target=\"#b28\">[29]</ref>, the principle behind our FPD is to improve the intrinsic  antic information. We will compare the performance between FPD-enhanced CNN and the CNN enhanced by <ref type=\"bibr\" target=\"#b28\">[29]</ref> in Section 4.1.</p></div> <div xmlns=\"http://www.tei-c.org  the Gaussian filtering operator, the dot product operator helps improve the adversarial robustness <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Meanwhile, as the dot product operator does not involve e  framework structure through the exploration study. Moreover, we compare with the most related work <ref type=\"bibr\" target=\"#b28\">[29]</ref> as well. In the comparison experiments, we focus on compar a><p>Comparison with the Related Work As mentioned in Section 2, the denoising approach proposed in <ref type=\"bibr\" target=\"#b28\">[29]</ref> is similar to our denoising layers in FPD. Therefore, we c /ref> is similar to our denoising layers in FPD. Therefore, we conduct a comparison experiment with <ref type=\"bibr\" target=\"#b28\">[29]</ref> as well. In Table <ref type=\"table\" target=\"#tab_5\">1</ref /ref> as well. In Table <ref type=\"table\" target=\"#tab_5\">1</ref>, X represents the enhanced CNN by <ref type=\"bibr\" target=\"#b28\">[29]</ref>. We observe that our F 2I\u2212Mid outperforms X . Especially, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: end ASR. Sub-word methods have a long history of application in a number of language related tasks. <ref type=\"bibr\" target=\"#b10\">[11]</ref> used sub-words units in particular for detecting unseen wo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are done during internship at Microsoft   <ref type=\"bibr\" target=\"#b7\">(Berard et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Bansal et al., 2019)</ref>, where they leverage the available ASR and  bibr\" target=\"#b38\">Weiss et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Bansal et al., 2018</ref><ref type=\"bibr\" target=\"#b4\">Bansal et al., , 2019;;</ref><ref type=\"bibr\" target=\"#b33\">Sperber et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . 2017b) by 37.0%, the Deep3 system (Raychev et al., 2016a) by 29.7%, and an adaptation of Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> for code prediction by 30.0%. These are sig \"bibr\" target=\"#b45\">, Yang and Xiang, 2019)</ref>. We include an adaptation of path-based Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> in our evaluations and show that our models Devanbu, 2017b)</ref>, Deep3 <ref type=\"bibr\" target=\"#b35\">(Raychev et al., 2016a)</ref>, Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref>).</p><p>Fig 3 puts these models in perspect #b35\">(Raychev et al., 2016a</ref>)) vs. TravTrans+ ; \u2022 from 43.6% to 73.6% when comparing Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref>  Thus, we argue that our proposal of using  \">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequences. We also include Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> to compare our efforts against a popular co ven a method body, how well can Code2Seq generate the correct method name? The training proposed in <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> is not well suited for next token predictio tsis et al., 2020</ref><ref type=\"bibr\" target=\"#b28\">, Li et al., 2018)</ref>), to paths in an AST <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a</ref><ref type=\"bibr\">(Alon et al., ,b, 2020))</re iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Code2Seq</head><p>Code2Seq is a model by <ref type=\"bibr\" target=\"#b7\">Alon et al. 2019a</ref> that embeds code snippets by embedding AST pat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  some similarity with traditional locality optimization on NUMA architectures for main memory usage <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Although both try to redistribute computation and data, o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utions.</formula><p>as a means for, e.g., model debugging or architecture selection. A recent paper <ref type=\"bibr\" target=\"#b3\">(Jain and Wallace, 2019)</ref> points to possible pitfalls that may ca ntion Might be Explanation</head><p>In this section, we briefly describe the experimental design of <ref type=\"bibr\" target=\"#b3\">Jain and Wallace (2019)</ref> and look at the results they provide to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or feature matrix <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>) are not compatibl denoising autoencoder to disturb the structure information. To build a symmetric graph autoencoder, <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes Laplacian sharpening as the counterpart of Laplac ing the latent representations to match a prior distribution for robust node embeddings.</p><p>GALA <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes a symmetric graph convolutional autoencoder recov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b3\">Hawkes (1971b</ref><ref type=\"bibr\" target=\"#b4\">Hawkes ( ,a, 1972))</ref>; <ref type=\"bibr\" target=\"#b5\">Hawkes and Oakes (1974)</ref>.</p><p>qq q q q q q qq q q q q qq q qq q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on operation is set to 2, then all the potential words can easily fuse into corresponding positions <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_ tional efficiency. In general, end-to-end CNNs in NLP have mainly been used for text classification <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref>. For sequence labeling tasks, CNNs have been mainly  state \u2190 \u2212 h w i , which are concatenated for the NER prediction.</p><p>CNN. We apply a standard CNN <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref> structure on the character or word sequence to obtai 0\"><head n=\"4.3\">Hyper-Parameter Settings</head><p>For all four of the datasets, we used the Adamax <ref type=\"bibr\" target=\"#b4\">[Kingma and Ba, 2014]</ref> optimization to train our networks. The in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  were made in terms of data annotation <ref type=\"bibr\" target=\"#b37\">(Styler IV et al., 2014;</ref><ref type=\"bibr\" target=\"#b6\">Cassidy et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">Mostafazadeh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref> to sample negative nodes to increase the optimization effi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  even if they are explicitly mentioned in the text. An analysis of the inner Transformer components <ref type=\"bibr\" target=\"#b10\">[11]</ref> is a subject for future work.</p></div> <div xmlns=\"http:/. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\">(PBMT, Wubben et al., 2012;</ref><ref type=\"bibr\" target=\"#b21\">Narayan and Gardent, 2014;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2016)</ref> or neural machine translation (NMT, <ref type= olingual phrase-based machine translation <ref type=\"bibr\" target=\"#b36\">(Wubben et al., 2012;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2016)</ref>. Further, syntactic information was also consi al., 2012)</ref>, which re-ranks sentences generated by PBMT for diverse simplifications; SBMT-SARI <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref>, which uses an external paraphrasing database omatic evaluation on the Newsela and WikiLarge datasets, respectively.</p><p>We use the SARI metric <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref> to measure the simplicity of the generated se s not improve performance, as it is known that WikiLarge does not focus on syntactic simplification <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref>. The best performance for this experiment is  =\"#b24\">(Papineni et al., 2002)</ref> to measure the closeness between a candidate and a reference. <ref type=\"bibr\" target=\"#b38\">Xu et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b32\">Sulem et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion. The leftmost bank on Figure <ref type=\"figure\" target=\"#fig_0\">1</ref> is a bimodal predictor <ref type=\"bibr\" target=\"#b4\">[5]</ref>. We refer to this bank as bank 0. It has 4k entries, and is  of groups of consecutive history bits, then it is XORed with the branch PC as in a gshare predictor <ref type=\"bibr\" target=\"#b4\">[5]</ref>. For example, bank 3 is indexed with 40 history bits, and th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b91\">94,</ref><ref type=\"bibr\" target=\"#b55\">58,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b60\">63]</ref>. Here we make an attempt to actually find this structure. W arget=\"#b91\">94,</ref><ref type=\"bibr\" target=\"#b55\">58,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b60\">63]</ref>. Unlike multi-task learning, we explicitly model the relati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref> and SARI <ref type=\"bibr\">(Xu et al., 2016)</ref>.</p><p>\u2022 Lexical and Syntactic complexity: <ref type=\"bibr\" target=\"#b28\">(Shardlow, 2014)</ref> identified lexical simplification and syntacti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e method cache in Smalltalk-80 <ref type=\"bibr\" target=\"#b10\">[11]</ref>, polymorphic inline caches <ref type=\"bibr\" target=\"#b23\">[23]</ref>, and type feedback/devirtualization <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  knowledge distillation and recommender systems has also attracted the attention of the researchers <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) <ref type=\"bibr\" target=\"#b24\">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) <ref type=\"bibr\" target=\"#b30\">(Park et al., 2018)</ref> is a most similar model to ours, which also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng marginalized graph autoencoder. Its training objective is reconstructing the feature matrix. AGC <ref type=\"bibr\" target=\"#b37\">[38]</ref> exploits high-order graph convolution to filter node featu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ted head sequences by warping a single or multiple static frames. Both classical warping algorithms <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b26\">28]</ref> and warping fields s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hitecture of TransformerCPI</head><p>The model we proposed is based on the transformer architecture <ref type=\"bibr\" target=\"#b41\">(Vaswani et al., 2017)</ref>, which was originally devised for neural. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. One main reason is that recommendation systems' outputs (i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b58\">[59,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> and the possibility of poor margins <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, leading to reduce  type=\"bibr\" target=\"#b58\">[59,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>, adversarial examples <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, and poor margins . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 18</ref><ref type=\"bibr\">), CJRC (Duan et al., 2019</ref>) \u2022 Natural Language Inference (NLI): XNLI <ref type=\"bibr\" target=\"#b4\">(Conneau et al., 2018)</ref> \u2022 Sentiment Classification (SC): ChnSenti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  are highly related, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=\"bibr\" target=\"#b1\">(Caruana 1997</ref>) by sharing some meta-knowledge.</p><p>We propose   from existing work, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=\"bibr\" target=\"#b1\">(Caruana 1997</ref>) by sharing some metaknowledge. Intuitively, answe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: evidence to be retrieved from Wikipedia.</p><p>We constructed a purpose-built dataset for this task <ref type=\"bibr\" target=\"#b15\">(Thorne et al., 2018)</ref> that contains 185,445 human-generated cla nce when constructing the dataset was the trade-off between annotation velocity and evidence recall <ref type=\"bibr\" target=\"#b15\">(Thorne et al., 2018)</ref>. Evidence selected by annotators was ofte ata was released through the FEVER website. 1 We used the reserved portion of the data presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>   </p></div> <div xmlns=\"http://www.tei-c. www.tei-c.org/ns/1.0\"><head n=\"2.2\">Scoring Metric</head><p>We used the scoring metric described in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref> to evaluate the submissions. The FEVER sha pe=\"table\" target=\"#tab_2\">2</ref>). 19 of these teams scored higher than the baseline presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>. All participating teams were invited to s her individually or as a group, can be used as evidence. We retained the annotation guidelines from <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref> (see Sections A.7.1, A.7.3 and A.8 from th rom 86 submissions from 23 teams. 19 of these teams exceeded the score of the baseline presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>. For the teams which provided a system des. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref> are implemented. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e this cosine was negative was in these situations of instability. We therefore switched to RMSProp <ref type=\"bibr\" target=\"#b20\">[21]</ref> which is known to perform well even on very nonstationary . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: measure of how evenly data is distributed. The percent imbalance metric equation 1 is commonly used <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Where Lmax is maximum load for any load unit and \u00b5L is th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, are performed to maintain a manageable balance between fo ves, focusing all attention on the hard negative examples.</p><p>Online Hard Example Mining (OHEM): <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposed to improve training of two-stage detectors by con  hard example mining <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>In this paper, we propose a new loss function that a rous extensions to this framework have been proposed, e.g. <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar rget=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> that samples hard e nt performance saturates. (d) FL outperforms the best variants of online hard example mining (OHEM) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> by over 3 points A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" targe ting <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, sketch2image <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, and other image-to-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow et al., 2015;</ref><ref type=\"bibr\" target=\"#b14\">Moosavi-Dezfooli et al., 2016)</ref>. Adversarial examples are genera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b16\">Szegedy et al. 2015;</ref><ref type=\"bibr\">2016)</ref>, recurrent neural networks (RNN) <ref type=\"bibr\" target=\"#b5\">(Hochreiter and Schmidhuber 1997)</ref> and large paired video languag. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b14\">[15]</ref>, ISSIR <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and consistent Wiener filtering <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which can recover the clean phase to some extent starting. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  given entities from text. We evaluate our model and baselines on two commonlyused datasets: TACRED <ref type=\"bibr\" target=\"#b48\">(Zhang et al., 2017)</ref> and FewRel <ref type=\"bibr\" target=\"#b13\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: culty of detection. The similar anomaly pattern appears in the network attack against IP-IP network <ref type=\"bibr\" target=\"#b2\">[Eswaran et al., 2018]</ref>, where there are sudden large number of c tional Network) is a representative model to combine the content and structural features in a graph <ref type=\"bibr\" target=\"#b2\">[Kipf and Welling, 2017]</ref>. Compared with traditional graph method Tang et al., 2015]</ref>, LINE <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref> and Node2vec <ref type=\"bibr\" target=\"#b2\">[Grover and Leskovec, 2016]</ref> are the methods to yield node embedd xtends the idea of convolution model over regular graphs (i.e., image) to general graphs. The works <ref type=\"bibr\" target=\"#b2\">[Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Kipf and  ., image) to general graphs. The works <ref type=\"bibr\" target=\"#b2\">[Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Kipf and Welling, 2017]</ref>improve the performance of basic GCN from. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resent in natural images, video, and speech. These properties are exploited efficiently by ConvNets <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, which are designed to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e test generators and to better hit areas or specific tasks in the design that are not covered well <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p><p>The analysis of coverage reports, and their translati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olutional kernels such as a fixed window <ref type=\"bibr\" target=\"#b5\">(Collobert et al. 2011;</ref><ref type=\"bibr\" target=\"#b12\">Kalchbrenner and Blunsom 2013)</ref>. When using such kernels, it is  e=\"bibr\" target=\"#b17\">Mikolov (2012)</ref> uses recurrent neural network to build language models. <ref type=\"bibr\" target=\"#b12\">Kalchbrenner and Blunsom (2013)</ref> proposed a novel recurrent netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s (VAEs) have synthesized striking image and audio samples <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  ego-network (i.e., r ) which GCC conducts data augmentation on. In this work, we follow Qiu et al. <ref type=\"bibr\" target=\"#b41\">[42]</ref> to use 0.8 as the restart probability. The proposed GCC fr vertex degrees <ref type=\"bibr\" target=\"#b59\">[60]</ref> and the binary indicator of the ego vertex <ref type=\"bibr\" target=\"#b41\">[42]</ref> as vertex features. After encoded by the graph encoder, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a vanilla and Siamese architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Each system is evaluated under specific configurations rega vych <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed to combine BERT with a Siamese architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref> for semantic representations of sentences and their similari ese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. In Siamese networks, two inputs are fed through identical s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed learning for computing meaningful and interpretable clusters on input graphs. On the other hand, <ref type=\"bibr\" target=\"#b21\">[22]</ref> proposes an approach that automatically constructs an easy such as link prediction, e-commerce recommendation, etc, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. There are some recent works that learn hierarchical graph raph representation is e-commerce taxonomy for offering a personalized dynamic shopping navigation. <ref type=\"bibr\" target=\"#b21\">[22]</ref> illstrates a topic-driven hierarchical taxonomy based on u </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D. Experiments and Results</head><p>SHOAL <ref type=\"bibr\" target=\"#b21\">[22]</ref> is Alibaba's current topic-driven taxonomy solution deploy iveness, we compare our proposed method with Alibaba's current topic-driven taxonomy solution SHOAL <ref type=\"bibr\" target=\"#b21\">[22]</ref>. In the parameter setting, we set the level number of the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rk based methods have achieved great success in relation extraction, including CNN-based approaches <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and LSTMbased appr ions in a paragraph. To this end, our model consists of a single-sentence module with Piecewise CNN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and a cross-sentence module which leverages self-attentio based methods <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and supervised relation extraction methods <ref type=\"bibr\" target=\"#b39\">[40]</ref>. The pattern-based method <ref type=\"bibr\" target=\"#b13\">[ . In the following we introduce evaluated methods in detail.</p><p>PCNN_single: Piecewise CNN model <ref type=\"bibr\" target=\"#b39\">[40]</ref>, which is one of the state of art single-sentence relation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b2\">(Caruana, 1995;</ref><ref type=\"bibr\" target=\"#b1\">Bengio et al., 2011;</ref><ref type=\"bibr\" target=\"#b0\">Bengio, 2011)</ref>. In transfer learning, we first train a base netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and it was used in <ref type=\"bibr\" target=\"#b1\">[2]</ref>  nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=\"bibr\" target=\"#b0\">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=\"bibr\" target=\"#b0\">(1)</ref>, and so now we generate results by performing a second pass . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: chine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  assification, these perturbations cause the legitimate sample to be misclassified at inference time <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  , adversarial training which augments the training data of the classifier with adversarial examples <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  xamples designed to fool the substitute often end up being misclassified by the targeted classifier <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b19\">Papernot e ch to defend against adversarial noise is to augment the training dataset with adversarial examples <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n distributions to obtain the coverage vector -suffices. In this respect our approach is similar to <ref type=\"bibr\" target=\"#b27\">Xu et al. (2015)</ref>, who apply a coverage-like method to image cap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: information retrieval community and search engine industry as the next generation search technology <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>In general, a search engine comprises a recall laye l in Facebook search is not a text embedding problem, as is actively researched in the IR community <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Instead it is a more complex problem that requires unders. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: taset.</p><p>We conduct experiments on seven large-scale text classification datasets introduced by <ref type=\"bibr\" target=\"#b36\">Zhang et al. (2015)</ref>. The experimental results show that our pro get=\"#b13\">Kalchbrenner et al. (2014)</ref> propose a novel CNN model with a dynamic k-max pooling. <ref type=\"bibr\" target=\"#b36\">Zhang et al. (2015)</ref> introduce an empirical exploration on the u hidden states. The Figure <ref type=\"figure\">3</ref> shows the difference to apply dropout between  <ref type=\"bibr\" target=\"#b36\">Zhang et al. (2015)</ref>. We summarize the datasets in Table <ref ty ditional methods and some other neural networks which are not based on RNN or CNN. The linear model <ref type=\"bibr\" target=\"#b36\">(Zhang et al., 2015)</ref> achieves a strong baseline in small datase. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ocks. And of these, there are two main candidates: variants of Mellor-Crummey and Scott (MCS) locks <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and variants of Craig, Landin, and Hagersten (CLH) locks . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ive clothing (Safety smocks and blankets) has been designed to be worn by actively suicidal inmates <ref type=\"bibr\" target=\"#b7\">(Hayes, 2013)</ref>. A top door alarm <ref type=\"bibr\" target=\"#b4\">(C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gnising Textual Entailment <ref type=\"bibr\" target=\"#b10\">(Fyodorov, Winter, and Francez 2000;</ref><ref type=\"bibr\" target=\"#b6\">Bowman et al. 2015)</ref>, and Question Answering <ref type=\"bibr\">(He. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ld datasets, WebQuestions <ref type=\"bibr\" target=\"#b3\">(Berant et al., 2013)</ref> and WikiAnswers <ref type=\"bibr\" target=\"#b11\">(Fader et al., 2013)</ref>. In this way, the syntactic structure and  ts including WebQuestions <ref type=\"bibr\" target=\"#b3\">(Berant et al., 2013)</ref> and WikiAnswers <ref type=\"bibr\" target=\"#b11\">(Fader et al., 2013)</ref> as well as on the Internet. In this manner. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: isters and memory) along with (ii) checkpointed cache warmup <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= ber of papers have proposed checkpointed sampling techniques <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> in which the architectu memory hierarchy state (MHS) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, memory timestamp record (MRT) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, etc.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><he use checkpointed microarchitecture warming for warming cache state, such as memory timestamp record <ref type=\"bibr\" target=\"#b6\">[7]</ref>, live-points <ref type=\"bibr\" target=\"#b8\">[9]</ref> and mem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rks (CNNs) <ref type=\"bibr\" target=\"#b0\">[1]</ref> have achieved great success in acoustic modeling <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target \" target=\"#b6\">6]</ref>, like regular Deep Neural Networks (DNNs), which results in a hybrid system <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target the required non-linear modeling capabilities.</p><p>Unlike the time windows applied in DNN systems <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target ://www.tei-c.org/ns/1.0\"><head n=\"2.\">Convolutional Neural Networks</head><p>Most of the CNN models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target n be very slow due to the iterative multiplications over time when the input sequence is very long; <ref type=\"bibr\" target=\"#b1\">(2)</ref> The training process is sometimes tricky due to the well-kno. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n or essential characteristics. However, there is no such wide-accepted formal definition. Paulheim <ref type=\"bibr\" target=\"#b5\">[6]</ref> defined four criteria for knowledge graphs. Ehrlinger and W  statistical relational learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>, knowledge graph refinement <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Chinese knowledge graph construction <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and their extensions <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> optimize an objecti s have been proposed <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. These methods suff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rediction tasks have achieved major advances recently. Encoder-decoder architectures like the U-Net <ref type=\"bibr\" target=\"#b31\">(Ronneberger et al., 2015)</ref> are state-of-the-art methods for the tion. Similar to pixelwise prediction tasks <ref type=\"bibr\" target=\"#b15\">(Gong et al., 2014;</ref><ref type=\"bibr\" target=\"#b31\">Ronneberger et al., 2015)</ref>, node classification tasks aim to mak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: igh quality for most languages, which can potentially improve the performance of end-to-end systems <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>Sub-word representations have recently seen their suc d proposed a worddependent silence model to improve ASR accuracy; for use in end-to-end ASR models, <ref type=\"bibr\" target=\"#b6\">[7]</ref> investigated the value of a lexicon in end-to-end ASR. Sub-w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  cases. In recent years, unsupervised learning has received increasing attention from the community <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Our novel appro  metric in an unsupervised fashion, without any human annotations.</p><p>Exemplar CNN. Exemplar CNN <ref type=\"bibr\" target=\"#b4\">[5]</ref> appears similar to our work. The fundamental difference is t tecture <ref type=\"bibr\" target=\"#b17\">[18]</ref> in their original papers, except for exemplar CNN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, whose results are reported with ResNet-101 <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cements in the computer vision community on very deep CNNs <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that have not been * Work done as Google Brain interns. exp  build such deeper models. NiN has seen great success in computer vision, building very deep models <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We show how to apply NiN principles in hierarchical Recur on that led to the success of very deep networks in vision <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Algorithms that do consider object articulations <ref type ct's pose and scale relative to a category-specific canonical representation. Recently, Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> extended the object coordinate based approach to perform c re normalized and the orientations are aligned for objects in a given category. Whereas the work by <ref type=\"bibr\" target=\"#b27\">[28]</ref> focuses on pose and size estimation for rigid objects, the NCSH representation is inspired by and closely related to Normalized Object Coordinate Space (NOCS) <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which we briefly review here. NOCS is defined as a 3D spa iefly review here. NOCS is defined as a 3D space contained within a unit cube and was introduced in <ref type=\"bibr\" target=\"#b27\">[28]</ref> to estimate the category-level 6D pose and size of rigid o  closed. In addition to normalizing the articulations, NAOCS applies the same normalization used in <ref type=\"bibr\" target=\"#b27\">[28]</ref> to the objects, including zero-centering, aligning orienta head><p>For each part, NPCS further zero-centers its position and uniformly scales it as is done in <ref type=\"bibr\" target=\"#b27\">[28]</ref>, while at the same time keeps its orientation unchanged as ime keeps its orientation unchanged as in NAOCS. In this respect, NPCS is defined similarly to NOCS <ref type=\"bibr\" target=\"#b27\">[28]</ref> but for individual parts instead of whole objects. NPCS pr s {p i \u2208 S (j) }, we have their corresponding NPCS predictions {c i |p i \u2208 S (j) }. We could follow <ref type=\"bibr\" target=\"#b27\">[28]</ref> to perform pose fitting, where the Umeyama algorithm <ref  j) }, as is commonly done for bundle adjustment <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we also use RANSAC for outlier removal.</p><p>Finally, fo  , t (j) , s (j) and the NPCS {c i |p i \u2208 S (j) } to compute an amodal bounding box, the same as in <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: obabilities. But complex and tricky training methods make it hard to implement. Triggered attention <ref type=\"bibr\" target=\"#b8\">[9]</ref> utilizes the spikes produced by connectionist temporal class. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecommender systems can be classified into path-based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, embedding-based me y lack an end-to-end way of training. (2) Path-based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> explore various pat \"#b1\">[2]</ref> to each user-item pair. The dimension of TransE is 32 for all datasets.</p><p>\u2022 PER <ref type=\"bibr\" target=\"#b32\">[33]</ref> is a representative of path-based methods, which treats th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[12]</ref>.</p><p>On the other hand, many researchers have found convolutional networks (ConvNets) <ref type=\"bibr\" target=\"#b16\">[17]</ref>  <ref type=\"bibr\" target=\"#b17\">[18]</ref> are useful in e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g open information extraction and event extraction, and also overlapping relation extraction models <ref type=\"bibr\" target=\"#b0\">(Dai et al., 2019)</ref>.</p></div><figure xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is sur-prisingly difficult.\" <ref type=\"bibr\" target=\"#b18\">[18]</ref> If humans have a hard time telling the difference, how muc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ER score, significantly outperforming previous BERT and Graph Neural Network (GNN) based approaches <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>. Our experiments demonstrate KGAT's strong   2018)</ref> further incorporates evidence identification to improve claim verification.</p><p>GEAR <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> formulates claim verification as a graph re tei-c.org/ns/1.0\"><head n=\"3.1\">Reasoning with Evidence Graph</head><p>Similar to previous research <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, KGAT constructs the evidence graph G by us narios and produces a probability P (y|c, D) to predict claim label y. Different from previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, we follow the standard graph label predict sentation v p . The aggregation is done by a graph attention mechanism, the same with previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>.</p><p>It first calculate the attention wei n</head><p>The per-node predictions are combined by the \"readout\" function in graph neural networks <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, where KGAT uses node kernels to learn the  ds without pre-training. BERT-pair, BERT-concat and GEAR are three baselines from the previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>. BERT-pair and BERTconcat regard claim-evid eriments are all based on ESIM sentence retrieval, which is the one used by GEAR, our main baseline <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org head n=\"6\">Case Study</head><p>Table <ref type=\"table\">5</ref> shows the example claim used in GEAR <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> and the evidence sentences retrieved by ESI htweight backpacker, inventor, author and global adventurer. Label: SUPPORT Table5: An example claim<ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> whose verification requires multiple pieces \"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Soleimani et a d is kept the same with previous work <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Soleimani et a l keeps the same as the previous work <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019)</ref>. The base version of BERT is used to impleme  KGAT is the best on all testing scenarios. With ESIM sentence retrieval, same as the previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Hanselowski et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a question. Most competitive models <ref type=\"bibr\" target=\"#b24\">(Shen, Yang, and Deng 2017;</ref><ref type=\"bibr\" target=\"#b0\">Bian et al. 2017;</ref><ref type=\"bibr\" target=\"#b29\">Wang, Hamza, and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  embeddings during training.</p><p>We use the Averaged Stochastic Gradient Descent (ASGD) algorithm <ref type=\"bibr\" target=\"#b27\">(Polyak and Juditsky, 1992)</ref> to train the LM, with 0.4 as the dr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: feedback as a composition of user result examination and relevance judgment. Examination hypothesis <ref type=\"bibr\" target=\"#b7\">[8]</ref>, which is a fundamental assumption in click modeling, postul ns; and among them result examination plays a central role <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Unfortunately, most applications of bandit algorithms simply the selected arm a t . Based on the examination hypothesis <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, when C at = 1, the chosen a t must be relevant to the user's. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chanisms have been at the focal point of recent literature on cloud resource allocation and pricing <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Spot Instance <ref the CPU, RAM, and Disk resource pools into typed VM instances, are no longer made randomly a priori <ref type=\"bibr\" target=\"#b2\">[3]</ref>, but made dynamically upon receiving user bids. Dynamic reso  Instead of the actual valuation of each bundle, is used in the one-round resource allocation as in <ref type=\"bibr\" target=\"#b2\">(3)</ref>, such that the bid from a user with a smaller remaining budg llocation: Algorithm 2 is our primal-dual approximation algorithm to the NP-hard allocation problem <ref type=\"bibr\" target=\"#b2\">(3)</ref>. is the maximum amount of type-resource at datacenter requir et=\"#b1\">(2)</ref>. Proof of (3): Constraints (1a), (1c), (1d) are guaranteed by the constraints in <ref type=\"bibr\" target=\"#b2\">(3)</ref>. In order to analyze the property about constraint (1b), we   we prove <ref type=\"bibr\" target=\"#b7\">(8)</ref>. Now we utilize the inequality (8) to prove claim <ref type=\"bibr\" target=\"#b2\">(3)</ref>. For some user , suppose is the first time . Then by <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: no extra</head><p>queueing is introduced in the server. When combined with real-time I/O subsystems <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the Leader/Followers thread pool implementation can reduc /ref> operation scheduling, event processing <ref type=\"bibr\" target=\"#b6\">[7]</ref>, I/O subsystem <ref type=\"bibr\" target=\"#b11\">[12]</ref> and pluggable protocol <ref type=\"bibr\" target=\"#b13\">[14]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: do not use gazetteers or any external labeled resources. The best score reported on this task is by <ref type=\"bibr\" target=\"#b26\">Luo et al. (2015)</ref>. They obtained a F 1 of 91.2 by jointly model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: RNA) <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and the recurrent neural network transducer (RNN-T) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. In particular, th nd-to-end models including attention-based models <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RNN-T <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> trained on \u223c12,500 \"http://www.tei-c.org/ns/1.0\"><head n=\"2.\">RNN-TRANSDUCER</head><p>The RNN-T was proposed by Graves <ref type=\"bibr\" target=\"#b12\">[13]</ref> as an extension to the connectionist temporal classificati ure <ref type=\"figure\">1</ref>, consists of an encoder (referred to as the transcription network in <ref type=\"bibr\" target=\"#b12\">[13]</ref>), a prediction network and a joint network; as described i e=\"bibr\" target=\"#b13\">[14]</ref>. The entire network is trained jointly to optimize the RNN-T loss <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which marginalizes over all alignments of target labels w p><p>During inference, the most likely label sequence is computed using beam search as described in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, with a minor alteration which was found to make the algor nsive without degrading performance: we skip summation over prefixes in pref(y) (see Algorithm 1 in <ref type=\"bibr\" target=\"#b12\">[13]</ref>), unless multiple hypotheses are identical.</p><p>Note tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e target problem <ref type=\"bibr\" target=\"#b14\">(Qi et al., 2018)</ref>. The approaches proposed by <ref type=\"bibr\" target=\"#b18\">Snell et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b20\">Sung et  ication</head><p>Few-shot classification <ref type=\"bibr\" target=\"#b21\">(Vinyals et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Snell et al., 2017)</ref> is a task in which a classifier must be ada ng Networks <ref type=\"bibr\" target=\"#b21\">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017)</ref> 68.17 Graph Network <ref type=\"bibr\" targe </p><p>\u2022 Prototypical Networks: a deep metric-based method using sample average as class prototypes <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017)</ref>.</p><p>\u2022 Graph Network: a graph-based few- the performance by few-shot classification accuracy following previous studies in few-shot learning <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017;</ref><ref type=\"bibr\" target=\"#b20\">Sung et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: vily dependent on the end-to-end data.</p><p>As our second contribution, we apply a two-stage model <ref type=\"bibr\" target=\"#b23\">(Tu et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Kano et al., 20  intermediate representation closely tied to the source text. The architecture has been proposed by <ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref> to realize a reconstruction objective, and a s iliary ASR and MT training data ( \u00a73). This model is similar to the architecture first described by <ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref>. It combines two encoder-decoder models in a c , we apply beam search only for the second stage decoder. We do not use the two-phase beam search of<ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref> because of its prohibitive memory requirements. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ason for the existence of adversarial examples, but there is a lack of consensus on the explanation <ref type=\"bibr\" target=\"#b0\">[1]</ref>. While the working mecha- nism of DNNs is not fully understo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l earthquakes. For more on the ETAS model, see e.g. <ref type=\"bibr\" target=\"#b11\">Ogata (1988</ref><ref type=\"bibr\" target=\"#b12\">Ogata ( , 1998))</ref>.</p><p>We sometimes make simplifying independe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2.1.\">Multiscale segmentation used</head><p>A region based multiscale image segmentation named MSEG <ref type=\"bibr\" target=\"#b36\">(Tzotsos and Argialas, 2006</ref>) is used to generate the initial se D have a size of \u00d7 500 500 pixels. The multiscale segmentation used in this study is MSEG algorithm <ref type=\"bibr\" target=\"#b36\">(Tzotsos and Argialas, 2006)</ref>, which is implemented under C++ en ed on multi-resolution segmentation (MSEG) <ref type=\"bibr\" target=\"#b0\">(Benz et al. (2004)</ref>; <ref type=\"bibr\" target=\"#b36\">Tzotsos and Argialas, 2006)</ref>, the proposed method is implemented. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: data sources. We use natural language processing methods, such as Latent Dirichlet Allocation (LDA) <ref type=\"bibr\" target=\"#b5\">[8]</ref> and topical phrase mining <ref type=\"bibr\" target=\"#b12\">[16 as their topical similarity using a process they call Phrase LDA.</p><p>Latent Dirichlet Allocation <ref type=\"bibr\" target=\"#b5\">[8]</ref> is the most common topic modeling process and PLDA+ is a sca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: th the same text encoder module.</p><p>Implementation Details We use 300-dimension Glove embeddings <ref type=\"bibr\" target=\"#b13\">(Pennington et al., 2014)</ref> for ARSC dataset and 300-dimension Ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arded as belief based on indirect experience, while trust is belief derived from direct experiences <ref type=\"bibr\" target=\"#b23\">[24]</ref> . In accordance with real situations of service-oriented c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng works has paid special attention to embed bipartite networks. While a recent work by Dong et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed metapath2vec++ for embedding heterogeneous networ ght be suboptimal for learning vertex representations for a bipartite network.</p><p>Metapath2vec++ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HNE <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EOE <re  We assign a probability to stop a random walk in each step. In contrast to DeepWalk and other work <ref type=\"bibr\" target=\"#b13\">[14]</ref> that apply a fixed length on the random walk, we allow the  hyper-parameters p and q are set to 0.5 which has empirically shown good results. \u2022 Metapath2vec++ <ref type=\"bibr\" target=\"#b13\">[14]</ref>: This is the state-of-the-art method for embedding heterog. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN <ref type=\"bibr\" target=\"#b1\">(Dong et al., 2014)</ref>. It consists of more than one composition fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b21\">22]</ref>, there are several attempts to adopt GNNs to learn with heterogeneous networks <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta ntly, studies have attempted to extend GNNs for modeling heterogeneous graphs. Schlichtkrull et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> propose the relational graph convolutional networks (RGCN)  heterogeneous GNNs as baselines, including:</p><p>\u2022 Relational Graph Convolutional Networks (RGCN) <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which keeps a different weight for each relationship, i.e  refers to HGT +RT E +H e t e r .</p><p>GNN Models GCN <ref type=\"bibr\" target=\"#b8\">[9]</ref> RGCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> GAT <ref type=\"bibr\" target=\"#b21\">[22]</ref> HetGNN <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ilt before prefetching can be useful.</p><p>The closest prior work to ours is that of Nesbit et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> where a Global History Buffer (GHB) is proposed for holding  mlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_3\"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type=\"bibr\" target=\"#b3\">4</ref>. Performance with and without GHL-prefetching. Four cases are . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n pro/anti stereotypical conditions is significant (p &lt; .05) under an approximate randomized test<ref type=\"bibr\" target=\"#b8\">(Graham et al., 2014)</ref>. Our methods eliminate the difference betw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b23\">[24]</ref>. According to an investigation conducted by Curran et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref>, lightning is near the top of the list of all types of weath. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Networks (GNNs) have been widely adopted in various tasks over graphs, such as graph classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for word embedding has been shown to be an implicit factorization of a certain word-context matrix <ref type=\"bibr\" target=\"#b25\">[24]</ref>, and there is recent effort to theoretically explaining th ol(G) log \u0434 \u2212x \u22a4 i y j .</formula><p>Let us define z i, j = x \u22a4 i y j . Following Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref>, where the authors suggested that for a sufficient large e w i\u2212T , \u2022 \u2022 \u2022 , w i\u22121 , w i+1 , \u2022 \u2022 \u2022 , w i+T .</formula><p>Following the work by Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref>, SGNS is implicitly factorizing</p><formula xml:id=\"formul x is not only ill-defined (since log 0 = \u2212\u221e), but also dense. Inspired by the Shifted PPMI approach <ref type=\"bibr\" target=\"#b25\">[24]</ref>, we define M \u2032 such that M \u2032 i, j = max(M i, j , 1) (Line  </ref>. Recently, there has been effort in understanding this model. For example, Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref> prove that SGNS is actually conducting an implicit matrix   target=\"#b19\">[18]</ref> frame word embedding as a metric learning problem. Built upon the work in <ref type=\"bibr\" target=\"#b25\">[24]</ref>, we theoretically analyze popular skip-gram based network . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene <ref type=\"bibr\" target=\"#b12\">[13]</ref>. These complex pipelines are slow and hard to optimize bec  Then, classifiers <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> or localizers <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  When the input distribution to a learning system changes, it is said to experience covariate shift <ref type=\"bibr\" target=\"#b17\">(Shimodaira, 2000)</ref>. This is typically handled via domain adapta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><p>Subject independent approaches have been proposed that transform audio features to video frames <ref type=\"bibr\" target=\"#b4\">[5]</ref> but there is still no method to directly transform raw audio rating natural facial expressions. Some methods generate frames based solely on present information <ref type=\"bibr\" target=\"#b4\">[5]</ref>, without taking into account the facial dynamics. This makes for capturing articulation dynamics and estimating the 3D points of the mesh. Finally, Chung et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> proposed a CNN applied on Mel-frequency cepstral coefficient  works that are closest to ours are those proposed in <ref type=\"bibr\" target=\"#b21\">[22]</ref> and <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The former method is subject dependent and requires a large  static method that produces video frames using a sliding window of audio samples like that used in <ref type=\"bibr\" target=\"#b4\">[5]</ref>. This is a GAN-based method that uses a combination of an L  rator but also due to the use of the conditional Sequence Discriminator. Unlike previous approaches <ref type=\"bibr\" target=\"#b4\">[5]</ref> that prohibit the generation of facial expressions, the adve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or the pre-trained word embedding will recognize it. Inspired by the character level language model <ref type=\"bibr\" target=\"#b4\">[Kim et al., 2016]</ref>, we combine the input wordconcept embedding w pe=\"bibr\" target=\"#b1\">[Hill et al., 2016]</ref> focus on learning the embedding of a phrase, while <ref type=\"bibr\" target=\"#b4\">[Palangi et al., 2016]</ref>, <ref type=\"bibr\">[Kalchbrenner et al., 2 \" target=\"#b4\">[Palangi et al., 2016]</ref>, <ref type=\"bibr\">[Kalchbrenner et al., 2014]</ref> and <ref type=\"bibr\" target=\"#b4\">[Le and Mikolov, 2014]</ref> focuses on learning the embedding of sent ollobert et al., 2011]</ref> first use CNN with pre-trained word embedding for text classification. <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref> further improves the performance by using multi-chan requency of each unigram. CNN. This method uses a one-layer CNN for text classification proposed by <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref>. It uses a multi-channel architecture for text embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  according to the statistical characters of objects in each grid. Graph-based methods, such as SCAN <ref type=\"bibr\" target=\"#b43\">[44]</ref> and spectral clustering <ref type=\"bibr\" target=\"#b36\">[37. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <ref type=\"bibr\" target=\"#b20\">(Ranganath et al., 2014)</ref>, to include BERT representations. Our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ntations of sentences and their similarity <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In prior work <ref type=\"bibr\" target=\"#b31\">[32]</ref>, we also utilized a Siamese BERT model to determine the di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: > decomposes weights into light-weight pieces, for example <ref type=\"bibr\" target=\"#b51\">[51,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> proposed to acceler. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: has been initiated. In two independent studies <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> the distinguishing power of GNNs is link rmer class of MPNNs covers the GNNs studied in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref t thm.</p><p>For anonymous MPNNs related to GNNs <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs < er the graph neural network architectures <ref type=\"bibr\" target=\"#b4\">[Hamilton et al., 2017</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> defined by:</p><formula xml:id=\"formula_ tly see, it follows from two independent works <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> that the distinguishing power of aMPNNs  et=\"#fig_1\">1</ref>. Proposition 5.2 (Based on <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref>). The classes M anon and M WL are equall h between anonymous graph neural networks <ref type=\"bibr\" target=\"#b4\">[Hamilton et al., 2017</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> and degree-aware graph neural networks < n as a slight generalisation of the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019</ref>]. (ii) The distinguishing power of degree ep-by-step, by GNNs that use ReLU or sign as activation function. This result refines the result in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> in that their simulation using the ReLU f of aMPNNs which are of special interest: those arising from the graph neural networks considered in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. In Example 3.1 we established that such  of the proofs of Lemma 2 in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. We show, by induction on the number of r remark that we cannot use the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> as a black box because the class M anon i onsidered in those papers. The proofs in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> relate to graph neural networks which, in nd M WL , and thus also M anon , are equally strong. The following results are known. Theorem 5.5 ( <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>). (i) The classes M sign GNN and M WL are side effect, we obtain a simpler aMPNN M in M GNN , satisfying M WL M , than the one constructed in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. The proof strategy is inspired by that o ef type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. The proof strategy is inspired by that of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. Crucial in the proof is the notion of ro  of two, at the cost of introducing an extra parameter p \u2208 A. Furthermore, the aMPNN constructed in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> uses two distinct weight matrices in A (s By the induction hypothesis, these rows are linearly independent. Following the same argument as in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>, this implies that there exists an (s t\u22121 ure the labelling \"refines\" \u2113 \u2113 \u2113 (t) MWL . To do so, we again follow closely the proof strategy of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. More specifically, we will need an analo  entries having value 1 and whose size will be determined from the context. Lemma 5.9 (Lemma 9 from <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>). Let C \u2208 A m\u00d7w be a matrix in which all  ns of the non-zero entries in \u00b5 \u00b5 \u00b5 Regarding future work, we point out that, following the work of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>, we fix the input graph in our analysis. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type \"place of birth\" for \"Bill Lockyer\" and \"California\". Such indicating words is called patterns <ref type=\"bibr\" target=\"#b5\">(Hearst, 1992;</ref><ref type=\"bibr\" target=\"#b3\">Hamon and Nazarenko, thod relies on relation patterns. Pattern-based extraction is widely used in information extraction <ref type=\"bibr\" target=\"#b5\">(Hearst, 1992;</ref><ref type=\"bibr\" target=\"#b3\">Hamon and Nazarenko, \">Han et al., 2018)</ref> to select trustable instances. The third research line relies on patterns <ref type=\"bibr\" target=\"#b5\">(Hearst, 1992;</ref><ref type=\"bibr\" target=\"#b3\">Hamon and Nazarenko,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )</ref>. There also exists some studies focusing on learning continuous representation of documents <ref type=\"bibr\" target=\"#b11\">(Le and Mikolov, 2014;</ref><ref type=\"bibr\" target=\"#b25\">Tang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: models can improve model predictions on all tasks by utilizing regularization and transfer learning <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, in practice, multi-task learning models do not alw <p>The backbone of MMoE is built upon the most commonly used Shared-Bottom multi-task DNN structure <ref type=\"bibr\" target=\"#b7\">[8]</ref>. The Shared-Bottom model structure is shown in Figure <ref t igure <ref type=\"figure\" target=\"#fig_0\">1</ref> (a), which is a framework proposed by Rich Caruana <ref type=\"bibr\" target=\"#b7\">[8]</ref> and widely adopted in many multi-task learning applications  sks.</p><p>Prior works <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> investigated task di erences in multi-task learning by assumi target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>Instead of sharing hidden layers and same model parame lt in both improved e ciency and model quality for each task <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. One of the widely us \" target=\"#b29\">30]</ref>. One of the widely used multi-task learning models is proposed by Caruana <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, which has a shared-bo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ngual transfer learning, which is a popular approach to address the limited resource problem in ASR <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amily of coverage events that share common properties are grouped together to form a coverage model <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Members of the coverage model are called coverage tasks and l are called coverage tasks and are considered part of the test plan. Cross-product coverage models <ref type=\"bibr\" target=\"#b6\">[7]</ref> are of special interest. These models are defined by a basic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in RDB has access to all the subsequent layers and passes on information that needs to be preserved <ref type=\"bibr\" target=\"#b10\">[7]</ref>. Concatenating the states of preceding RDB and all the prec oposed DenseNet, which allows direct connections between any two layers within the same dense block <ref type=\"bibr\" target=\"#b10\">[7]</ref>. With the local dense connections, each layer reads informa e the bias term is omitted for simplicity. We assume F d,c consists of G (also known as growth rate <ref type=\"bibr\" target=\"#b10\">[7]</ref>) feature-maps.</p><formula xml:id=\"formula_6\">[F d\u22121 ,F d,1 k architecture as residual dense block (RDB). More differences between RDB and original dense block <ref type=\"bibr\" target=\"#b10\">[7]</ref> would be summarized in Section 4.</p></div> <div xmlns=\"htt .tei-c.org/ns/1.0\"><head n=\"4.\">Discussions</head><p>Difference to DenseNet. Inspired from DenseNet <ref type=\"bibr\" target=\"#b10\">[7]</ref>, we adopt the local dense connections into our proposed res ne is the design of basic building block. SRDenseNet introduces the basic dense block from DenseNet <ref type=\"bibr\" target=\"#b10\">[7]</ref>. Our residual dense block (RDB) improves it in three ways:  <ref type=\"bibr\" target=\"#b6\">[3]</ref> and also demonstrates that stacking many basic dense blocks <ref type=\"bibr\" target=\"#b10\">[7]</ref> in a very deep network would not result in better performan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t information for making matching decisions. The last technique, data augmentation, is adapted from <ref type=\"bibr\" target=\"#b30\">[31]</ref> for EM to help D learn \"harder\" to understand the data inv  address this issue, D applies MixDA, a recently proposed data augmentation technique for NLP tasks <ref type=\"bibr\" target=\"#b30\">[31]</ref> illustrated in Figure <ref type=\"figure\" target=\"#fig_3\">3 h the entry_swap operator. We compare the different combinations and report the best one. Following <ref type=\"bibr\" target=\"#b30\">[31]</ref>, we apply MixDA with the interpolation parameter \u03bb sampled arget=\"#b58\">59</ref>]. We designed a set of DA operators suitable for EM and apply them with MixDA <ref type=\"bibr\" target=\"#b30\">[31]</ref>, a recently proposed DA strategy based on convex interpola nd span_shuffle. These two operators are used in NLP tasks <ref type=\"bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> and shown to be effective for text classification. For span DA) has been extensively studied in computer vision and has recently received more attention in NLP <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: endation can also be addressed as an online learning problem solved with spectral bandit algorithms <ref type=\"bibr\" target=\"#b197\">[198]</ref>. The key idea is to represent the reward function in an  ring smoothness on the graph, which has been shown to be effective in video recommendation examples <ref type=\"bibr\" target=\"#b197\">[198]</ref>.</p><p>Data clustering or community detection can also b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, are on multiprogramming environments, attempting to allevi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  OF ADVERSARIES</head><p>Since the initial work by <ref type=\"bibr\">Szegedy et al. (2013)</ref> and <ref type=\"bibr\" target=\"#b31\">Goodfellow et al. (2014)</ref>, many different adversaries have been  mlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_0\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b31\">Goodfellow et al. (2014)</ref> and the L 2 optimization method propos versarial example. As originally described, FGS generates untargeted adversarial examples. On MNIST,<ref type=\"bibr\" target=\"#b31\">Goodfellow et al. (2014)</ref> reported that FGS could generate adver  a minimal perturbation that changes the model's classification; single-step gradient-based attacks <ref type=\"bibr\" target=\"#b31\">(Goodfellow et al., 2014;</ref><ref type=\"bibr\">Kurakin et al., 2016; the perturbation using L 0 , L 1 , L 2 , and L \u221e norms <ref type=\"bibr\">(Szegedy et al., 2013;</ref><ref type=\"bibr\" target=\"#b31\">Goodfellow et al., 2014;</ref><ref type=\"bibr\" target=\"#b7\">Carlini &. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b39\">Zhang and Lapata, 2017;</ref><ref type=\"bibr\" target=\"#b7\">Guo et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Kriz et al., 2019)</ref>. Recently, sequence-to-sequence (Seq2Seq)-ba ><p>A few recent text simplification studies <ref type=\"bibr\" target=\"#b5\">(Dong et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Kriz et al., 2019)</ref> did not use BLEU for evaluation, noticing th f> integrated the transformer architecture and paraphrasing rules to guide simplification learning. <ref type=\"bibr\" target=\"#b12\">Kriz et al. (2019)</ref> produced diverse simplifications by generati  et al., 2018)</ref>; S2S-All-FA, which a reranking based model focussing on lexical simplification <ref type=\"bibr\" target=\"#b12\">(Kriz et al., 2019)</ref>; and Access, which is based on the transfor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, with adversarial training <ref type=\"bibr\" target=\"#b20\">[21]</ref> being one of the most effective methods. It formulates tra  Typically, using more attack iterations (higher value of k) produces stronger adversarial examples <ref type=\"bibr\" target=\"#b20\">[21]</ref>. However, each attack iteration needs to compute the gradi ct to traditional PGD-40.</p><p>We apply our technique on Madry's Adversarial Training method (MAT) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref> and e arget=\"#b38\">39]</ref> focuse on analyzing and improving adversarial machine learning. Madry et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> first formulate adversarial training as a min-max optimiza higher value of k (more attack iterations), PGDk can generate adversarial examples with higher loss <ref type=\"bibr\" target=\"#b20\">[21]</ref> els. This property is named as transferability. This prope rbations from previous epochs. To compare the attack strength of two attacks, we use Madry's method <ref type=\"bibr\" target=\"#b20\">[21]</ref> to adversarially train two models on MNIST and CIFAR10 and we integrate ATTA with two popular adversarial training methods: Madry's Adversarial Training (MAT) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref>. By e  efficiency</head><p>We select four state-of-the-art adversarial training methods as baselines: MAT <ref type=\"bibr\" target=\"#b20\">[21]</ref>, TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref>, YOPO <r ed in <ref type=\"bibr\" target=\"#b15\">[16]</ref> and is formulated as a min-max optimization problem <ref type=\"bibr\" target=\"#b20\">[21]</ref>. As one of the most effective defense methods, lots of wor , are widely adopted in various adversarial training methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar arget=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" tar jected back to S, k-step projected gradient descent method <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> (PGDk) has been widely adopted to generate adversarial exam  gradient descent <ref type=\"bibr\" target=\"#b15\">[16]</ref>) is adopted to conduct iterative attack <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Setup</head><p>Following the literature <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta  convolutional layers followed by three full-connected layers which is same architecture as used in <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. The adversarial p  with size = 0.3.</p><p>For the CIFAR10 dataset, we use the wide residual network  which is same as <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. The perturbation   a new perspective (e.g., improving transferability between epochs). which is same with other works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta re, and hyper-parameters used in this work.</p><p>MNIST. We use the same model architecture used in <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta  step size and set decay factor as 1 for M-PGD (momentum PGD).</p><p>CIFAR10. Following other works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mentation <ref type=\"bibr\" target=\"#b32\">[33]</ref>), and deep contextual language models from BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a v  GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref>, to contextual embeddings as the ones used in BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XL-Net <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The  tations based on the Transformer architecture <ref type=\"bibr\" target=\"#b36\">[37]</ref>, named BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The t r Siamese</figDesc><table /><note>XLNet-512 (most complex Transformer architecture). As suggested in<ref type=\"bibr\" target=\"#b14\">[15]</ref>, the Transformer training is performed with batch size b =. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sadness, fear, anger, disgust, and anxiety) are considered <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Traditional studies on visual sentiment analysis are mainly. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eep learning entirely infeasible.</p><p>Applying RMD to hyperparameter optimization was proposed by <ref type=\"bibr\" target=\"#b3\">Bengio (2000)</ref> and <ref type=\"bibr\" target=\"#b2\">Baydin &amp; Pea , Eigenmann &amp; Nossek (1999)</ref>, <ref type=\"bibr\" target=\"#b9\">Chen &amp; Hagan (1999)</ref>, <ref type=\"bibr\" target=\"#b3\">Bengio (2000)</ref>, <ref type=\"bibr\" target=\"#b0\">Abdel-Gawad &amp; R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b46\">[47]</ref> in videos, or clustering features <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p><p>Contrastive learning vs. pretext tasks. Various pretex ssification.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"4\" xml:id=\"foot_3\"><ref type=\"bibr\" target=\"#b3\">4</ref> Our w2\u00d7 and w4\u00d7 models correspond to the \"\u00d78\" and \"\u00d716\" cases . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eful tool for dealing with those aspects of compiler and program development. Models of performance <ref type=\"bibr\" target=\"#b5\">[6]</ref> and energy <ref type=\"bibr\" target=\"#b6\">[7]</ref> can also . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> has demonstrated superior performance to the previous hand-cr m, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> has drawn considerable attention due to its simple network st Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. Motivated by SRCNN, some problems such as face hallucination ><p>We first briefly describe the network structure of SRCNN <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, and then we detail how we reformulate the network layer by l Different Upscaling Factors</head><p>Unlike existing methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> that need to train a network from scratch for a different sca lgorithms are mostly learning-based (or patch-based) methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target= space, then followed by a complex mapping to another high-dimensional HR feature space. Dong et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> show that the mapping accuracy can be substantially improved a wider mapping layer, but at the cost of the running time. For example, the large SRCNN (SRCNN-Ex) <ref type=\"bibr\" target=\"#b1\">[2]</ref> has 57,184 parameters, which are six times larger than that  with no pre-processing. 2) The proposed model achieves a speed up of at least 40\u00d7 than the SRCNN-Ex <ref type=\"bibr\" target=\"#b1\">[2]</ref> while still keeping its exceptional performance. One of its  ters in a layer) and depth (i.e., the number of layers) of the mapping layer. As indicated in SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a 5 \u00d7 5 layer achieves much better results than a 1 \u00d7 1 lay s an average PSNR of 32.87 dB, which is already higher than that of SRCNN-Ex (32.75 dB) reported in <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The FSRCNN (48,12,2) contains only 8,832 parameters, then t F) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SRCNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>, SRCNN-Ex <ref type=\"bibr\" target=\"#b1\">[2]</ref> and the sparse coding based network (SCN) <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . And we find inspirations from the integrate-and-fire model <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>.</p><p>Integrate-and-fire is one of the earliest models in sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  obtained noisy labels have previously been demonstrated useful for learning visual representations <ref type=\"bibr\" target=\"#b18\">(Pathak et al., 2017;</ref><ref type=\"bibr\" target=\"#b6\">Gidaris et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to perform Chinese NER is to first perform word segmentation and then apply word sequence labeling <ref type=\"bibr\" target=\"#b7\">[Yang et al., 2016;</ref><ref type=\"bibr\">He and Sun, 2017]</ref>.</p> rd information for NER has attracted research attention <ref type=\"bibr\">[Passos et al., 2014;</ref><ref type=\"bibr\" target=\"#b7\">Zhang and Yang, 2018]</ref>. In particular, to exploit explicit word i ational efficiency <ref type=\"bibr\">[Strubell et al., 2017]</ref>.</p><p>Specifically, lattice LSTM <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref> employs double recurrent transition compu hinese word segmentation, character-based name taggers can outperform their word-based counterparts <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. <ref type=\"bibr\" target=\"#b7\">Zhang and  ilation. This method achieves great performance in the English NER task. Lattice LSTM. Lattice LSTM <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref> can model the characters in sequence and  ></head><label></label><figDesc>, Weibo NER[Peng and Dredze, 2015; He and Sun, 2016], and Resume NER<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<re NER<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>.The OntoNotes and MSRA are the newswire d utperform their word-based counterparts <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. <ref type=\"bibr\" target=\"#b7\">Zhang and Yang [2018]</ref> exploit an RNNbased lattice structure to s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the precision and the recall of the NEs which occur in the training corpus rarely would be degraded <ref type=\"bibr\" target=\"#b12\">[15]</ref> . Second, if the training corpus is limited, it is feasibl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ures the relationship between two items and show the statistical relationships involving dependence <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Correlations are useful because they can indicate a predi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b11\">[12]</ref>, which also proposed a combinatorial algorithm to compute embeddings. Ganea et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> and Gulcehre et al. <ref type=\"bibr\" target=\"#b20\">[21]</r  taking its outputs. An alternative to prevent such collapse would be to introduce bias terms as in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Importantly, when applying the non-linearity directly on   )y 1 + 2 x, y + x 2 y 2<label>(8)</label></formula><p>Similar to the Euclidean case, and following <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we use x = x 0 . On the Poincar\u00e9 ball, we employ pointwis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b2\">[3]</ref>. On the other hand, prior work has proposed a variety of partitioning techniques <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" targ y on restricting the locations where a line can reside depending on its partition. Way-partitioning <ref type=\"bibr\" target=\"#b8\">[9]</ref> restricts insertions from each partition to its assigned sub. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roduction</head><p>Recent approaches to sequence to sequence learning typically leverage recurrence <ref type=\"bibr\" target=\"#b8\">(Sutskever et al., 2014)</ref>, convolution <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . (2015)</ref> apply a character-level CNN for text classification and achieve competitive results. <ref type=\"bibr\" target=\"#b22\">Socher et al. (2013)</ref>   explore the structure of a sentence and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: out the learning process would be beneficial to enhance the OOD discriminative power of the system. <ref type=\"bibr\" target=\"#b3\">Hendrycks et al. (2019)</ref> demonstrate that utilizing auxiliary dat ing higher likelihood estimates on unseen OOD samples. The ominous observation is presented also by <ref type=\"bibr\" target=\"#b3\">Hendrycks et al. (2019)</ref>, but they concentrate on improving the O r\" target=\"#b8\">Nalisnick et al., 2019a;</ref><ref type=\"bibr\" target=\"#b0\">Choi et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Hendrycks et al., 2019)</ref>. These works report that despite intuiti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: presentation learning for generic graphs (GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>). In general, GNNs recursively update each node's feature by icult. Although sampling methods, such as GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and FastGCN <ref type=\"bibr\" target=\"#b0\">[1]</ref>), have b  type=\"bibr\" target=\"#b9\">[10]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We use a large-scale bipartite graph dataset from the Tence igh memory cost. Sampling methods like GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref> and AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> have been proposed to deal with this issue by reducing the n ons: GCN and MEAN aggregator. Node-wise sampling is used to address the scalability issue. \u2022 AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>: This method uses adaptive sampling between each layer to de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying <ref type=\"bibr\" target=\"#b22\">[24]</ref>. The node-copying model can be used to produce sample grap type=\"bibr\" target=\"#b21\">[23]</ref> uses a non-parametric model for the graph generative model and <ref type=\"bibr\" target=\"#b22\">[24]</ref> proposes a node copying model to achieve flexibility in th rnative, we use a more general generative model for graphs based on copying nodes, as introduced in <ref type=\"bibr\" target=\"#b22\">[24]</ref>. We demonstrate in the following sections that this model   setting.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Node Copying</head><p>In <ref type=\"bibr\" target=\"#b22\">[24]</ref>, Pal et al. introduce the node copying model for \ud835\udc5d (G). Sa etworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying<ref type=\"bibr\" target=\"#b22\">[24]</ref>. The node-copying model can be used to produce sample grap ode classification when there are very few training labels <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" tar node classification when there are very few training labels<ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" tar s. These limitations were addressed in the follow-up works <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24]</ref>, where <ref type=\"bibr\" target=\"#b21\">[23]</ref> uses a non. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, satellite imaging <ref type=\"bibr\" target=\"#b37\">[38]</ref>, face recognition <ref type=\"bibr\" target=\"#b16\">[17]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 10; 11]</ref> and we discuss in more detail how USMPep stands out from these approaches. MHCnuggets <ref type=\"bibr\" target=\"#b9\">[10]</ref> is rather similar to the proposed approach (apart from the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t will cause selection bias, making the model under-trained and resulting in suboptimal performance <ref type=\"bibr\" target=\"#b10\">[Lian et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Ding et al., 2 m mean discrepancy (MMD) between the empirical distribution of feature vectors in these two subsets <ref type=\"bibr\" target=\"#b10\">[Li et al., 2015;</ref><ref type=\"bibr\">Zhang et al., 2017]</ref>. Co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ngs in such a way as to maximise the plausibility of the triples that are already present in the KG <ref type=\"bibr\" target=\"#b15\">[Nickel et al., 2011</ref><ref type=\"bibr\" target=\"#b21\">, Socher et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: entrality <ref type=\"bibr\" target=\"#b30\">[32]</ref> is denoted by c degr , the closeness centrality <ref type=\"bibr\" target=\"#b3\">[5]</ref> is denoted by c clos , and the betweeness centrality <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >2007), most Natural Language Processing (NLP) systems fail when processing corrupted or noisy text <ref type=\"bibr\" target=\"#b6\">(Belinkov and Bisk, 2018)</ref>. Although this problem is not new to N bibr\" target=\"#b44\">(Smith, 2007)</ref>. Moreover, we employed two sets of misspellings released by <ref type=\"bibr\" target=\"#b6\">Belinkov and Bisk (2018)</ref> and <ref type=\"bibr\" target=\"#b38\">Pikt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly related to our approach are the Defense-GAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> and MagNet <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which first estimate the manifold of clean data to detect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: based loss functions <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> adopt the \"Negative Sampling\" strategy, in which k negative ther embedding methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> can automatically extract better features that produce high esc>Negative Sampling<ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> Require: number of positive links in a mini-batch b, number edding (e.g., Word2Vec <ref type=\"bibr\" target=\"#b21\">[22]</ref>) and network embedding (e.g., LINE <ref type=\"bibr\" target=\"#b28\">[29]</ref>) tasks, negative sampling is utilized. Recent efforts have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: usal model which could measure this causal relationship i.e. Linear Structual Equation Models (SEM) <ref type=\"bibr\" target=\"#b20\">(Shimizu et al., 2006)</ref>. Existing methods for disentangled repre et=\"#b6\">(Hoyer et al., 2009;</ref><ref type=\"bibr\" target=\"#b24\">Zhang &amp; Hyvarinen, 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shimizu et al., 2006)</ref>. <ref type=\"bibr\" target=\"#b19\">Pearl (20 2009)</ref> introduce a probabilistic graphical model based framework to learn causality from data. <ref type=\"bibr\" target=\"#b20\">Shimizu et al. (2006)</ref> proposed an effective method called LiNGA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: wise approach.</p><p>\u2022 Combining the proposed model with densityweighted Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we introduce active learning into POLAR <ref type=\"bibr\"  ediction the parameters under the posterior disagree about are selected. Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref> selects the instance that maximizes the expected loss base ected Loss Optimization</head><p>The active learning metric we choose is Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The basic idea is to choose the instance that maximizes t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tly. On one hand, prior non-uniform cache access (NUCA) work <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= NUCA by adaptively placing data close to the requesting core <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= istance. However, these best-effort techniques often result in hotspots and additional interference <ref type=\"bibr\" target=\"#b2\">[3]</ref>. On the other hand, prior work has proposed a variety of par at D-NUCA often causes significant bank contention and uneven distribution of accesses across banks <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We also see this effect in Sec. VI -R-NUCA has the highest . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" tar e a novel face-focused cross-stream network (FFCSN). Different from the popular two-stream networks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targ d for action recognition in videos and has been popular for many human-centric video analysis tasks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Various improvement. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  model. Recent works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> also show that adversarial examples can be physically reali. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ://www.tei-c.org/ns/1.0\"><p>We present a variational approximation to the information bottleneck of <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>. This variational approach allows us to pa oot_2\">3</ref> This approach is known as the information bottleneck (IB), and was first proposed in <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>. Intuitively, the first term in R IB encou challenging. There are two notable exceptions: the first is when X, Y and Z are all discrete, as in <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>; this can be used to cluster discrete data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b19\">[20]</ref> and sparse representation <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. While these approaches are effective, the extracted featur  target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we use 91 images from Yang et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> and 200 images from Berkeley Segmentation Dataset (BSD) <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. The recognition process has become a necessary part of man aved first, then the recognition tasks are conducted on text <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Given the plain text of an academic homepage, we aim to re ding, i.e., S \u2208 R n\u00d7d e . Following state-of-the-art methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, we use GloVe <ref type=\"bibr\" target=\"#b18\">[19]</ref> to  ods have been developed to address these problems. The state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> uses a Bi-LSTM-CRF based model to learn the page-level and l language processing methods. For example, state-of-the-art techniques for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> and for person names recognition <ref type=\"bibr\" target=\" ifferent methods to capture the position patterns. The state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> trains webpage-level and line-level models together to cap nd Preprocessing. We use the same datasets used by the state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> and person name recognition <ref type=\"bibr\" target=\"#b0\"> ref type=\"table\" target=\"#tab_0\">2</ref> summarises the dataset statistics.</p><p>\u2022 HomePub dataset <ref type=\"bibr\" target=\"#b30\">[31]</ref> contains the plain text of 2,087 homepages from different  formation about the position patterns and person names. PAM also outperforms the hierarchical PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> model, which can capture the positional diversity, by 3.64 a manual inspection of recognition results of state-ofthe-art models for the two tasks (i.e., PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> and CogNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>) and our ublications and 5,542 person names.</p><p>For publication string recognition, we observe that PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> misrecognises strings about patents, grants, and research . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: alized with restricted Boltzmann machines (RBMs) that are pre-trained in a greedy layerwise fashion <ref type=\"bibr\" target=\"#b31\">[30]</ref>. The DNN is fine-tuned to optimize the CE objective with r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: it feedback, implicit feedback is more difficult to utilize because of the lack of negative samples <ref type=\"bibr\" target=\"#b10\">[Pan et al., 2008]</ref>. Secondly, generating top-k preferred items  it operation. Finding approximate top-K items can be even finished in sublinear or logarithmic time <ref type=\"bibr\" target=\"#b10\">[Wang et al., 2012;</ref><ref type=\"bibr\" target=\"#b10\">Muja and Lowe n finished in sublinear or logarithmic time <ref type=\"bibr\" target=\"#b10\">[Wang et al., 2012;</ref><ref type=\"bibr\" target=\"#b10\">Muja and Lowe, 2009]</ref> by making use of index technique.</p><p>Se bibr\">[Zhou and Zha, 2012]</ref>, PPH <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2014]</ref>, CH <ref type=\"bibr\" target=\"#b10\">[Liu et al., 2014]</ref> incur large quantization loss <ref type=\"bib rm for f (x) and \u03b2 is its penalty coefficient. <ref type=\"bibr\">[Giannessi and Tardella, 1998;</ref><ref type=\"bibr\" target=\"#b10\">Lucidi and Rinaldi, 2010]</ref> show that the above two problems are  13)</label></formula><p>In terms of the loss function, we employ the popular and effective BPR loss <ref type=\"bibr\" target=\"#b10\">[Rendle et al., 2009]</ref>. In particular, given a user matrix U and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: private-attribute inference attack can be naturally formulated as a problem of adversarial learning <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In our proposed RAP, there are two components: a Bayesian. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS <ref type=\"bibr\" target=\"#b8\">[8]</ref>). We take this idea further by proposing a stochastic multi- p>We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis <ref type=\"bibr\" target=\"#b8\">[8]</ref> and Graclus <ref type=\"bibr\" target=\"#b4\">[4]</ref> aim to c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [40,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> and QM9 <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>.</p><p>ZINC is a large dataset of commercially available dr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b51\">[52,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. A popular learning paradigm is graphbased / hypergraph-bas ral network f (G, X) <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> (X contains the initial features on the vertices for exampl learning problem on the approximation. While the state-of-the-art hypergraph neural networks (HGNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> approximates each hyperedge by a clique and hence requires  detailed experimentation, we demonstrate their effectiveness compared to the state-of-the art HGNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> and other baselines (Sections 5, and 7). \u2022 We thoroughly d =\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Hypergraph neural networks <ref type=\"bibr\" target=\"#b16\">[17]</ref> and their variants <ref type=\"bibr\" target=\"#b22\">[23,</re 2 . Our approach requires at most a linear number of edges (1 and 2|e| \u2212 3 respectively) while HGNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> requires a quadratic number of edges for each hyperedge. < yperGCN and FastHyperGCN against the following baselines:</p><p>\u2022 Hypergraph neural networks (HGNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses the clique expansion <ref type=\"bibr\" target=\"#b51\">[  [7].Our approach requires at most a linear number of edges (1 and 2|e| \u2212 3 respectively) while HGNN<ref type=\"bibr\" target=\"#b16\">[17]</ref> requires a quadratic number of edges for each hyperedge.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ric learning methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, showing that dimensionality does not significantly affect . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: emic paper analysis.</p><p>The pipeline for creating S2ORC was used to construct the CORD-19 corpus <ref type=\"bibr\" target=\"#b2\">(Wang et al., 2020)</ref>, which saw fervent adoption as the canonical. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  a single image has recently received a huge boost in performance using Deep-Learning based methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe  CNN-based SR methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, we only learn the residual between the interpolated LR and i haustively trained for these conditions. In fact, ZSSR is significantly better than the older SRCNN <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and in some cases achieves comparable or better results tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b16\">Phang et al., 2018)</ref>.</p><p>When finetuning a big, pretrained language model, dropout <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014)</ref> has been used as a regularization tec er of that neuron as w during training, then we use (1 \u2212 p)w for that weight parameter at test time <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014)</ref>. This ensures that the expected outpu ght decay of \u03bb is equivalent to wdecay(0, \u03bb).</p><p>Probability for Dropout and Dropconnect Dropout <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014</ref>) is a regularization technique selecti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the counts of each word as the features. For the TFIDF (term-frequency inverse-document-frequency) <ref type=\"bibr\" target=\"#b13\">[14]</ref> version, we use the counts as the term-frequency. The inve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff and Jones, 1999)</ref>, which uses the prediction of models wi br\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff and Jones, 1999)</ref>. However, in order to prevent the deep . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: measure of how evenly data is distributed. The percent imbalance metric equation 1 is commonly used <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Where Lmax is maximum load for any load unit and \u00b5L is th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e minded users. Later, an analogous item-oriented approach <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> became popular. In those methods, a rating is estimated usi  make the itemoriented approach more favorable in many cases <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. In addition, item-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: roaches for representation learning of graphs <ref type=\"bibr\" target=\"#b23\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b21\">Kipf &amp arget=\"#b6\">Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b8\">Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b13\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b19\">Kearnes e erage via attention <ref type=\"bibr\" target=\"#b34\">(Velickovic et al., 2018)</ref> and LSTM pooling <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b24\">Murphy e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: train. Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion <ref type=\"bibr\" target=\"#b21\">[22]</ref>. No parameter tuning was performed during neural network t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing comprehension models such as <ref type=\"bibr\" target=\"#b40\">Weissenborn et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b3\">Chen et al. (2017)</ref>. We use C and Q to denote the encoded context bibr\" target=\"#b43\">Xiong et al., 2016;</ref><ref type=\"bibr\" target=\"#b38\">Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2017)</ref>, the validation score is well correlated with t <ref type=\"bibr\" target=\"#b33\">(Shen et al., 2017b)</ref> 69.1 / 78.9 70.6 / 79.4 Document Reader <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2017)</ref> 70.0 / 79.0 70.7 / 79.4 Ruminating Reader <r s respectively, as such layer numbers fall into the usual range of the reading comprehension models <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2017)</ref>. All of these LSTMs have hidden size 128. Th ., 2016)</ref>, ReasoNet <ref type=\"bibr\" target=\"#b33\">(Shen et al., 2017b)</ref>, Document Reader <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2017)</ref>, Interactive AoA Reader <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mal preprocessing scheme.</p><p>The spectrograms are processed by a deep bidirectional LSTM network <ref type=\"bibr\" target=\"#b10\">(Graves et al., 2013)</ref> with a Connectionist Temporal Classificat M is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM <ref type=\"bibr\" target=\"#b10\">(Graves et al., 2013)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion in detail and then extrapolate to the entire execution <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. A major challenge in sampled evaluation however is to quic  is transferable across both hardware and software changes <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Functional warming  warms up the microarchitecture state b es not allow for software changes. Functional warming (FW) <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> does not incur any storage overhead, allows for software ch ect number of representative detailed regions that are evaluated in detail to then extrapolate from <ref type=\"bibr\" target=\"#b33\">[34]</ref>. The key challenge in sampling is to get (i) the correct a unctional fast-forwarding, checkpointing and virtualized fastforwarding. Functional fast-forwarding <ref type=\"bibr\" target=\"#b33\">[34]</ref> leverages functional simulation to get to the next represe a detailed warm-up using a small number of instructions (e.g., 30,000) prior to the detailed region <ref type=\"bibr\" target=\"#b33\">[34]</ref>. With this small amount of warming, only a small part of t instructions. Prior research shows that the highest accuracy is achieved for small detailed regions <ref type=\"bibr\" target=\"#b33\">[34]</ref>; larger detailed regions will likely make DeLorean even mo d to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS <ref type=\"bibr\" target=\"#b33\">[34]</ref>. \u2022 CoolSim: Randomized Statistical Warming (RSW) is employ ture state using all memory references between two consecutive detailed regions, which is very slow <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Various approaches have been proposed to reduce the warm-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Related to our work, for example, <ref type=\"bibr\" target=\"# ibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Related to our work, for example, <ref type=\"bibr\" target=\"#b6\">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model rget=\"#b6\">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS <ref type=\"bibr\" target=\"#b6\">[7]</ref>. These studies consider learning methods within the traditio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 36]</ref>, and random forest <ref type=\"bibr\" target=\"#b28\">[26]</ref>.</p><p>Recently, Dong et al. <ref type=\"bibr\" target=\"#b9\">[7]</ref> propose a Super-Resolution Convolutional Neural Network (SRC s demonstrate that our method is faster than several CNN based super-resolution models, e.g., SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref>, SCN <ref type=\"bibr\" target=\"#b35\">[33]</ref>, VDSR <ref ty onal neural networks based SR. In contrast to modeling the LR-HR mapping in the patch space, SR-CNN <ref type=\"bibr\" target=\"#b9\">[7]</ref> jointly optimize all the steps and learn the nonlinear mappi R network <ref type=\"bibr\" target=\"#b19\">[17]</ref> demonstrates significant improvement over SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref> by increasing the network depth from 3 to 20 convolutional l er model with a fast Table <ref type=\"table\">1</ref>: Comparisons of CNN based SR algorithms: SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref>, FSRCNN <ref type=\"bibr\" target=\"#b10\">[8]</ref>, SCN <ref t ed LapSRN with 8 state-of-theart SR algorithms: A+ <ref type=\"bibr\" target=\"#b32\">[30]</ref>, SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref>, FSRCNN <ref type=\"bibr\" target=\"#b10\">[8]</ref>, SelfExSR < ge, our method reconstructs the rails without the ringing artifacts.</p><p>Ground-truth HR HR SRCNN <ref type=\"bibr\" target=\"#b9\">[7]</ref> VDSR <ref type=\"bibr\" target=\"#b19\">[17]</ref> LapSRN (ours) l:id=\"fig_0\"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Network architectures of SRCNN<ref type=\"bibr\" target=\"#b9\">[7]</ref>, FSRCNN<ref type=\"bibr\" target=\"#b10\">[8]</ref>, VDSR<ref ty arget=\"#b19\">[17,</ref><ref type=\"bibr\" target=\"#b28\">26]</ref>    the protocol of existing methods <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\" target=\"#b19\">17]</ref>, we generate the LR   methods using the bicubic upsampling for pre-processing generate results with noticeable artifacts <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\" target=\"#b19\">17,</ref><ref type=\"bibr\" targ pe=\"figure\">5</ref>. For 8\u00d7 SR, it is challenging to predict HR images from bicubicupsampled images <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\" target=\"#b19\">17,</ref><ref type=\"bibr\" targ y to hallucinate the regular structure. This is a common limitation shared by parametric SR methods <ref type=\"bibr\" target=\"#b9\">[7,</ref><ref type=\"bibr\" target=\"#b10\">8,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: While previous learning simulation approaches <ref type=\"bibr\" target=\"#b17\">(Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Ummenhofer et al., 2020)</ref> have been highly specialized for parti lution 3D water scenario with randomized water position, initial velocity and volume, comparable to <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>'s containers of water. We used SPlisHS d boundary particles, a loss function that weights slow particles with few neighbors more heavily). <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> reported CConv outperformed DPI, so we  is to, during training, provide the model with its own predictions by rolling out short sequences. <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>, for example, train with two-step pred e=\"bibr\" target=\"#b14\">(Kipf &amp; Welling, 2016)</ref> work. The full CConv update as described in <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> is,</p><formula xml:id=\"formula_19\">f  e comparisons.</head><p>We implemented the CConv model, loss and training procedure as described by <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>. For simplicity, we only tested the CC  appended a particle type learned embedding to the input node features.</p><p>To be consistent with <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>, we used their batch size of 16, learn \"><head>D. Supplementary baseline comparisons D.1. Continuous convolution (CConv)</head><p>Recently <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> presented Continuous Convolution (CCon eral tasks.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Interpretation.</head><p>While <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> state that \"Unlike previous approaches. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b4\">5]</ref>, and trusted measurement is a key problem of this technology <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Trusted computing tre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y predicted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy ibr\" target=\"#b15\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Lample et al., 2016)</ref>, using LSTM-CRF as the main network struct epresentations Both character CNN <ref type=\"bibr\" target=\"#b27\">(Ma and Hovy, 2016)</ref> and LSTM <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016)</ref> have been used for representing the chara. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  performance on various applications of machine learning to graph data, such as recommender systems <ref type=\"bibr\" target=\"#b24\">[25]</ref>, social network analysis <ref type=\"bibr\" target=\"#b10\">[1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ence (seq2seq) models. Sub-word units were used in seq2seq <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref> and RNNT <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nal GNNs can operate on the transformed homogeneous graphs <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. This is a two-stage approach and requires hand-crafted met <head n=\"3.2\">Meta-Path Generation</head><p>Previous works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> require manually defined meta-paths and perform Graph Neura. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"bibr\" target=\"#b40\">Zoph et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Real et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Cai et al., 2018a;</ref><ref type=\"bibr\" target=\"#b21\">Liu et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ve also been demonstrated to misled DNN based classification systems in physical world applications <ref type=\"bibr\" target=\"#b5\">(Sharif et al., 2016;</ref><ref type=\"bibr\" target=\"#b6\">Brown et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6)</ref> partially motivate the ranking-based variant throught the importance sampling viewpoint of <ref type=\"bibr\" target=\"#b0\">Bengio and Sen\u00e9cal (2008)</ref>. However there are two critical differ 0\">Bengio and Sen\u00e9cal (2008)</ref>. However there are two critical differences: 1) the algorithm of <ref type=\"bibr\" target=\"#b0\">Bengio and Sen\u00e9cal (2008)</ref> does not lead to the same objective L . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rrent connections, this incurs significant overhead due to lock contention between multiple threads <ref type=\"bibr\" target=\"#b16\">[20]</ref>. The use of file descriptors for sockets, in turn, creates ore resources. For efficient inter-core communication, they use asynchronous message passing. Corey <ref type=\"bibr\" target=\"#b16\">[20]</ref> attempts to address the resource sharing problem on multic enhance operating system scalability for multicore systems <ref type=\"bibr\" target=\"#b15\">[19,</ref><ref type=\"bibr\" target=\"#b16\">20,</ref><ref type=\"bibr\" target=\"#b40\">44]</ref>. Bar-relfish <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , and several branches, regardless of them being taken or not taken.</p><p>The next trace predictor <ref type=\"bibr\" target=\"#b16\">[17]</ref> provides trace level sequencing. That is, the fetch engine. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g techniques such as fully convolutional networks (FCNs) <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and U-Net <ref type=\"bibr\" target=\"#b17\">[18]</ref> algor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eled through iterative sparse matrix vector multiplication <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. In each iteration, the system traverses all active vertice stems that explicitly maintain states for subgraph patterns <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" tar 6,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>. Arabesque <ref type=\"bibr\" target=\"#b45\">[46]</ref> is the first distributed system that proposes the \"think l .tei-c.org/ns/1.0\"><head n=\"9\">Related Work</head><p>Graph mining systems and algorithms. Arabesque <ref type=\"bibr\" target=\"#b45\">[46]</ref> built on Giraph <ref type=\"bibr\" target=\"#b0\">[1]</ref> is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: re mainly three kinds of methods for dealing with such noise problem. First, multiinstance learning <ref type=\"bibr\" target=\"#b15\">(Riedel et al., 2010;</ref><ref type=\"bibr\" target=\"#b8\">Lin et al.,  ds to solve the noise problem. The first widely studied method is based on multi-instance  learning <ref type=\"bibr\" target=\"#b15\">(Riedel et al., 2010;</ref><ref type=\"bibr\" target=\"#b8\">Lin et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s is a central task for news recommendation. Traditional collaborative filtering (CF) based methods <ref type=\"bibr\" target=\"#b23\">(Wang and Blei, 2011)</ref> often utilize historical interactions bet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  capitalize on FlowNet-s <ref type=\"bibr\" target=\"#b58\">[59]</ref> to produce optical flow, PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> is particularly remould in our motion stream. Compared to  4 2 , and 7 2 , respectively. Two-stream Feature Aggregation. For motion stream, we utilize PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> pre-trained on Flying Chairs dataset for optical flow esti t that the receptive field in sampling stream for offset prediction is smaller than that in PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> for optical flow generation. As such, the range of estimat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by the recent success of graph convolution networks (GCNs) <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, we use the information-propagation mechanism to encode hig wn feature and the interaction among different modalities. However, existing GNN efforts (e.g., GCN <ref type=\"bibr\" target=\"#b21\">[22]</ref>, GraphSage <ref type=\"bibr\" target=\"#b13\">[14]</ref>, GAT  icro-videos, which is widespread in recommendation systems <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that the model manifold and the true distribution's support have a non-negligible intersection (see <ref type=\"bibr\" target=\"#b0\">[1]</ref>), and this means that the KL distance is not defined (or sim ining GANs is well known for being delicate and unstable, for reasons theoretically investigated in <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>In this paper, we direct our attention on the various  zero. This happens to be the case when two low dimensional manifolds intersect in general position <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>Since the Wasserstein distance is much weaker than th ing gradients, as can be seen in Figure <ref type=\"figure\">1</ref> of this paper and Theorem 2.4 of <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In Figure <ref type=\"figure\" target=\"#fig_1\">2</ref> we sho e <ref type=\"bibr\" target=\"#b3\">[4]</ref>. This last phenomenon has been theoretically explained in <ref type=\"bibr\" target=\"#b0\">[1]</ref> and highlighted in <ref type=\"bibr\" target=\"#b10\">[11]</ref> nerated image, when the pixels were already normalized to be in the range <ref type=\"bibr\">[0,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref>. This is a very high amount of noise, so much that when paper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: xtraction of the middle-level representations is based on psychological principles and art theories <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Finally, a deep neura ions from tweets' low level features based on previous psychological principles and art theories in <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. The definitions are a /1.0\"><head>2) Visual Attributes:</head><p>Based on previous work on affective image classification <ref type=\"bibr\" target=\"#b5\">[6]</ref> and color psychology theories <ref type=\"bibr\" target=\"#b9\"> eels dull, otherwise clear. To extract the five-color theme feature, we use the method described in <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Saturation, brightness and warm or cool color features of a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ven their complementary strengths and weaknesses <ref type=\"bibr\">(d'Avila Garcez et al. 2015;</ref><ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\" target=\"#b47\">Yang nowledge is compiled into a neural network architecture <ref type=\"bibr\">(Bo\u0161njak et al. 2017;</ref><ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\">Evans and Grefenst retability and generalisation, thereby inheriting the best of both worlds. Among such systems, NTPs <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\" target=\"#b32\">Min div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>End-to-end Differentiable Proving</head><p>NTPs <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017)</ref> recursively build a neural networ l atoms in the body need to be proven, and because Z is a free variable with many possible bindings <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017)</ref>. We consider two problems -given  (F) log[1 \u2212 ntp K \u03b8 ( F, d)]<label>(4)</label></formula><p>NTPs can also learn interpretable rules. <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> show that it is possible to learn tes. Although NTPs can be used for learning interpretable rules from data, the solution proposed by <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> can be quite inefficient, as the  <ref type=\"bibr\" target=\"#b21\">(Kemp et al. 2006</ref>) -following the same evaluation protocols as <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>. Furthermore, since GNTPs allows   Prediction Results. We compare GNTPs and NTPs on a set of link prediction benchmarks, also used in <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>. Results, presented in Table 1, s  predicates, 111 unary predicates, 14 constants and 2565 true facts. We follow the protocol used by <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> and split every dataset into trai ww.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_0\">For consistency, we use the same notation as<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>.</note> \t\t\t<note xmlns=\"http://ww s parallel inference to be implemented very efficiently on GPU. This optimisation is also present in<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>.</note> \t\t\t<note xmlns=\"http://ww  \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"7\" xml:id=\"foot_5\">Results reported in<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> were calculated with an incorrect 158 facts about the neighbourhood of countries, and the location of countries and subregions. As in <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>, we randomly split countries into. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e locations per image but only a few locations contain objects. This imbalance causes two problems: <ref type=\"bibr\" target=\"#b0\">(1)</ref> training is inefficient as most locations are easy negatives  benchmark <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>For training, we follow common practice <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> and use the COCO tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amounts of supervision. However, even systems that have relied extensively on unsupervised features <ref type=\"bibr\" target=\"#b7\">(Collobert et al., 2011;</ref><ref type=\"bibr\" target=\"#b36\">Turian et s, and present a hybrid tagging architecture. This architecture is similar to the ones presented by <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref> and <ref type=\"bibr\" target=\"#b19\">Huang ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">Pretrained embeddings</head><p>As in <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref>, we use pretrained word embeddings to in g of our    Several other neural architectures have previously been proposed for NER. For instance, <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref> uses a CNN over a sequence of word embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tput mappings, potentially stochastic, with learnable parameters using directed acyclic graphs (see <ref type=\"bibr\" target=\"#b38\">Schulman et al. (2015)</ref> for a review). The state of each non-inp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: understanding started attracting more attentions in some close related fields such as image caption <ref type=\"bibr\" target=\"#b4\">[Li et al., 2019b]</ref>. However, due to the complexity of video unde ibr\" target=\"#b2\">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation classifier <ref type=\"bibr\" target=\"#b4\">[Li et al., 2019b]</ref> on it. We adopt almost the same operation exc \" target=\"#b5\">[Venugopalan et al., 2015;</ref><ref type=\"bibr\" target=\"#b5\">Xu et al., 2018b;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2016]</ref>. These methods are effective but they overlook patial-attention in image caption domain <ref type=\"bibr\" target=\"#b0\">[Anderson et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Li et al., 2019a] {h 1 , h 2 , ..., h m } where v \u2208 R n\u00d7d is the global feature extracted by a pre-trained 3D-ConvNet <ref type=\"bibr\" target=\"#b4\">[Tran et al., 2015]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ eo segment in the dataset, we uniformly sample 10 frames. And for each frame, we use a Faster R-CNN <ref type=\"bibr\" target=\"#b4\">[Ren et al., 2015]</ref> detector with ResNeXt-101 backbone to detect . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nitial design space and the output is a refined design space of simpler or better models. Following <ref type=\"bibr\" target=\"#b20\">[21]</ref>, we characterize the quality of a design space by sampling gn, elevated to the population level and guided via distribution estimates of network design spaces <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>As a testbed for this paradigm, our focus is on exp essential to use a reliable comparison metric to guide our design process. Recently, the authors of <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposed a methodology for comparing and analyzing populat c scenario).</p><p>We rely on the concept of network design spaces introduced by Radosavovic et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref>. A design space is a large, possibly infinite, population  esign space is a large, possibly infinite, population of model architectures. The core insight from <ref type=\"bibr\" target=\"#b20\">[21]</ref> is that we can sample models from a design space, giving r ce design. To evaluate and compare design spaces, we use the tools introduced by Radosavovic et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref>, who propose to quantify the quality of a design space by  a single ResNet-50 <ref type=\"bibr\" target=\"#b7\">[8]</ref> model at 4GF for 100 epochs.</p><p>As in <ref type=\"bibr\" target=\"#b20\">[21]</ref>, our primary tool for analyzing design space quality is th i-c.org/ns/1.0\"><head>Appendix C: Optimization Settings</head><p>Our basic training settings follow <ref type=\"bibr\" target=\"#b20\">[21]</ref> as discussed in \u00a73. To tune the learning rate lr and weigh initial design space and the output is a refined design space of simpler or better models. Following<ref type=\"bibr\" target=\"#b20\">[21]</ref>, we characterize the quality of a design space by sampling tp://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We use the term design space following<ref type=\"bibr\" target=\"#b20\">[21]</ref>, rather than search space, to emphasize that we are not se ://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"5\" xml:id=\"foot_4\">Our training setup in \u00a73 exactly follows<ref type=\"bibr\" target=\"#b20\">[21]</ref>. We use SGD with momentum of 0.9, mini-batch size of 128 o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: enerating the entire sequence at once <ref type=\"bibr\" target=\"#b24\">[25]</ref> or in small batches <ref type=\"bibr\" target=\"#b19\">[20]</ref>. However, this introduces a lag in the generation process,  to capture temporal dependencies but requires fixed length videos. This limitation was overcome in <ref type=\"bibr\" target=\"#b19\">[20]</ref> but constraints need to be imposed in the latent space to  ght-forward adaptations of GANs for videos are proposed in <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, replacing the 2D convolutional layers with 3D convolutiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch contain knowledge representative of the path as a whole. PLDA+, a scalable implementation of LDA <ref type=\"bibr\" target=\"#b24\">[28]</ref>, allows us to quickly nd topic models in these clouds. Unl truction process, all share common topics. We perform topic modeling on these documents using PLDA+ <ref type=\"bibr\" target=\"#b24\">[28]</ref>. e result is a set of plain text topics which represent di s and PLDA+ is a scalable implementation of this algorithm <ref type=\"bibr\" target=\"#b16\">[20,</ref><ref type=\"bibr\" target=\"#b24\">28]</ref>. Developed by Zhiyuan Liu et al., PLDA+ quickly identi es g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ariability in behavioural and environmental patterns have stymied predictive modeling of this kind. <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref> have tried to predict stress of student tp://www.tei-c.org/ns/1.0\"><head n=\"3.2.1.\">LOCATION FEATURE BASED MLP</head><p>In the work done by <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>, a Multilayer Perceptron (MLP) with 4 f ><head n=\"4.\">Result</head><p>Due to a heavy imbalance of class labels on a scale of 1-5, we follow <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>, converting the five stress label scale ses of 23 students, totaling to 1183 data points achieving roughly equal amount of training data in <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>. These 1183 data points have the follow. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hallenges above, we propose to model the attributed networks with graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b15\">[16]</ref>. GCN, which takes the topological structure and nodal attr se a new type of attributed network encoder inspired by the graph convolutional network (GCN) model <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Specifically, GCN considers the high-order node proximity a particular layer, the convolution operation is D \u2212 1 2 A D \u2212 1 2 XW, and its complexity is O(mdh) <ref type=\"bibr\" target=\"#b15\">[16]</ref> as AX can be efficiently implemented using sparse-dense ma rning performance by considering neighbors of nodes that are multiple hops away. In particular, GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> takes the structure and attribute information as input, an  autoencoder architecture. Meanwhile, recent research advances on graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s. Checkpointed Warming (CW) takes a checkpoint of the microarchitecture state prior to each region <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Unfortunately, ch erages functional simulation to get to the next representative region, which is slow. Checkpointing <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> takes a snapshot o s an accuracy/speed/overhead trade-off. As mentioned in the introduction, checkpointed warming (CW) <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> is fast, requires  ons to make checkpoints transferable across cache structures <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>. Functional Warming ich takes a checkpoint of the microarchitecture state prior to each detailed region. Wenisch et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref> store the state of the caches and other micro-architectura. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ave been considered for HMMbased recognisers as a way of dealing with out of vocabulary (OOV) words <ref type=\"bibr\" target=\"#b6\">(Galescu, 2003;</ref><ref type=\"bibr\" target=\"#b1\">Bisani &amp; Ney, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rom root node to the leaf nodes. We calculate the derivatives to update the parameters. The AdaGrad <ref type=\"bibr\" target=\"#b2\">(Duchi et al., 2011)</ref> is employed to solve this optimization prob. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f its neighbor pairs <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. For example, SiGMa <ref type=\"bibr\" target=\"#b8\">[9]</ref> enerate candidate user pairs from all the pairs. Following <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, we only keep the user pairs if their names are similar to  gate the matching scores from a confidential seed set of user pairs to their neighbor pairs. COSNET <ref type=\"bibr\" target=\"#b27\">[28]</ref> proposed a supervised method to infer the marginal probabi ls of the unlabeled pairs and update the model based on the inferred labels and the user attributes <ref type=\"bibr\" target=\"#b27\">[28]</ref>. However, error propagations may be introduced in above me fore resorting to the model, we can easily select the most useful neighbor pairs by heuristic rules <ref type=\"bibr\" target=\"#b27\">[28]</ref>. This paper simply selects the neighbor pairs if their nam  by propagating the matching scores (predicted by SVM) through the two input networks.</p><p>COSNET <ref type=\"bibr\" target=\"#b27\">[28]</ref>: is a factor graph model that incorporates the attributes . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ections, background, and reflection images, respectively. Here, \u03b1 and \u03b2 are the mixing coefficients <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" targ  generation results and clearer separation results. Moreover, we introduce the gradient constraints <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> to make the model le ><head n=\"4.1.\">Framework of the Proposed Scheme</head><p>In contrast to the conventional pipelines <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" targ ird columns in Figure <ref type=\"figure\" target=\"#fig_3\">4</ref> 1 ) than previous linear functions <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ n, and the background edge map (E) concurrently. Instead of one-toone framework in previous methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, our separator learn . Recently, deep learning based reflection removal methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> with better generalization ability have been proposed to addr h previous methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, that heavily rely on the simplified model in Equation 1 and  learn the edge features of the reflections with the light field camera. The framework introduced in <ref type=\"bibr\" target=\"#b4\">[5]</ref> exploited the edge information when training the whole netwo and reflection, and three discriminator networks to produce the adversarial losses. Existing method <ref type=\"bibr\" target=\"#b4\">[5]</ref> can be treated as a special instance of our method when the   type=\"bibr\" target=\"#b28\">[29]</ref>, CycleGAN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The yellow boxes highlight some noticeable differences.</p> 5\">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type=\"bibr\" target=\"#b29\">[3 <ref type=\"bibr\" target=\"#b28\">[29]</ref>, CycleGAN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, WS16 <ref typ able\" xml:id=\"tab_3\"><head>Table 3 .</head><label>3</label><figDesc>Efficiency comparisons with FY17<ref type=\"bibr\" target=\"#b4\">[5]</ref>, Zhang18<ref type=\"bibr\" target=\"#b28\">[29]</ref> and Wan18<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  two-layer LSTMs and the parameters are estimated by MLE (note that the current state-of-the-art is <ref type=\"bibr\" target=\"#b10\">(Yang et al., 2018)</ref>). <ref type=\"bibr\" target=\"#b11\">Zaremba et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cision. D 's architecture is much simpler when compared to many state-of-the-art EM solutions today <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. Even though the b  <ref type=\"bibr\" target=\"#b13\">[14]</ref> or DeepMatcher<ref type=\"foot\" target=\"#foot_1\">3</ref>  <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Furthermore, D can also ingest and match hierarchically s head><p>We experimented with all the 13 publicly available datasets used for evaluating DeepMatcher <ref type=\"bibr\" target=\"#b33\">[34]</ref>. These datasets are from the ER Benchmark datasets <ref ty pany datasets are textheavy meaning that at least one attributes contain long text. Also, following <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we use the dirty version of the DBLP-ACM, DBLP-Scholar, i . We report the average F1 of 5 repeated runs in all the settings.</p><p>\u2022 DeepMatcher: DeepMatcher <ref type=\"bibr\" target=\"#b33\">[34]</ref> is the SOTA matching solution. Compared to D , DeepMatcher ]</ref> to train the word embeddings. When reporting DeepMatcher's F1 scores, we use the numbers in <ref type=\"bibr\" target=\"#b33\">[34]</ref> for the ER-Magellan datasets and numbers in <ref type=\"bib \"#b16\">[17]</ref> achieves better F1 in the Walmart-Amazon and Amazon-Google datasets. According to <ref type=\"bibr\" target=\"#b33\">[34]</ref>, the Magellan system ( <ref type=\"bibr\" target=\"#b24\">[25] pe=\"bibr\" target=\"#b13\">[14]</ref>, Magellan <ref type=\"bibr\" target=\"#b24\">[25]</ref>, DeepMatcher <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and DeepMatcher's follow-up work <ref type=\"bibr\" target= rent (e.g., they used k-fold cross-validation while we use the train/valid/test splits according to <ref type=\"bibr\" target=\"#b33\">[34]</ref>). In our experiments, we implemented DeepER with LSTM as t have summarized DM in Section 4.2. In addition to simply taking the numbers from the original paper <ref type=\"bibr\" target=\"#b33\">[34]</ref>, we also ran their open-source version (DM (reproduced)) w 2 and 15 epochs). The reproduced results are in general lower than the original reported numbers in <ref type=\"bibr\" target=\"#b33\">[34]</ref> (the 3rd column) because we did not try the other model va e Abt-Buy dataset. Others: We obtained the results for Magellan by taking the reported results from <ref type=\"bibr\" target=\"#b33\">[34]</ref> and the two follow-up works <ref type=\"bibr\" target=\"#b22\" er 4.6.5 of <ref type=\"bibr\" target=\"#b32\">[33]</ref>) and is also used by DeepMatcher and Magellan <ref type=\"bibr\" target=\"#b33\">[34]</ref>. It is not difficult to see that over the same set of mode other EM solutions <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. We list the size of each dataset in Table <ref type=\"table available found in <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>     The use of a pre-trained LM contributes to a large port et=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b63\">64]</ref>. DeepER <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: struction performance. <ref type=\"bibr\">Kim et al.</ref> propose a 20-layer CNN model known as VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which adopts residual learning and adaptive gradient clip eover, the traditional convolutional networks usually adopt cascaded network topologies, e.g., VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref> and DRC-N <ref type=\"bibr\" target=\"#b12\">[13]</ref>. In th cise structure of the proposed IDN, it is much faster than several CNN-based SR methods, e.g., VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRCN <ref type=\"bibr\" target=\"#b12\">[13]</ref>, LapSRN <r o accelerate SRCNN in combination with smaller filter sizes and more convolution layers. Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> propose a very deep CNN model with global residual archite nerate the residual image. The bias term of this transposed convolution can auto-Dataset Scale VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref> DRCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> LapSRN <ref  bicubic, SRCNN <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRC-N <ref type=\"bibr\" target=\"#b12\">[13]</ref>, LapSRN < v> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.1\">Training datasets</head><p>By following <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ate a variable number of features to a fixed-length vector. Fisher Vectors were first introduced in <ref type=\"bibr\" target=\"#b15\">(Jaakkola and Haussler 1999)</ref> to combine the advantages of gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t al. 2017;</ref><ref type=\"bibr\" target=\"#b6\">B\u00e9rard et al. 2018</ref>) and pretraining techniques <ref type=\"bibr\" target=\"#b3\">(Bansal et al. 2019)</ref> have been applied to end-to-end ST model to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: )</ref>. In this paper we use Checkpoint Processing and Recovery (CPR) as the baseline architecture <ref type=\"bibr\" target=\"#b1\">[2]</ref> since it has been shown to outperform conventional ROB-based rview</head><p>CPR is a ROB-free proposal for building scalable large instruction window processors <ref type=\"bibr\" target=\"#b1\">[2]</ref>. CPR addresses the scalability and performance limitations o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b9\">[10]</ref>, and Gaussian mixture models <ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref>. Recent work has . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b2\">(Bowman et al., 2016;</ref><ref type=\"bibr\" target=\"#b31\">Vendrov et al., 2015;</ref><ref type=\"bibr\" target=\"#b19\">Mou et al., 2016;</ref><ref type=\"bibr\" target=\"#b16\">Liu et al., 201 ent-wise product are then concatenated with the original vectors, \u0101 and \u00e3, or b and b, respectively <ref type=\"bibr\" target=\"#b19\">(Mou et al., 2016;</ref><ref type=\"bibr\">Zhang et al., 2017)</ref>. T 015)</ref> uses unsupervised \"skip-thoughts\" pre-training in GRU encoders. The approach proposed by <ref type=\"bibr\" target=\"#b19\">Mou et al. (2016)</ref> considers tree-based CNN to capture sentence- pe=\"bibr\" target=\"#b31\">(Vendrov et al., 2015)</ref> 15M 98.8 81.4 (4) 300D tree-based CNN encoders <ref type=\"bibr\" target=\"#b19\">(Mou et al., 2016)</ref> 3.5M 83.3 82.1 (5) 300D SPINN-PI encoders <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ody of literature <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bib implementations <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bib ting through a particular routing edge given its current congestion.</p><p>Congestion Amplification <ref type=\"bibr\" target=\"#b14\">[15]</ref> was recently introduced as an improvement to pricing of ro uncongested regions, and then drastically increase cost once a routing edge is full. The authors of <ref type=\"bibr\" target=\"#b14\">[15]</ref> propose to use a more gradual linear cost function for edg \" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> and the Chi dispersion router <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>In continuous optimization, dynamic pricing of cons. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ercent of the branch mispredictions are due to indirect branches. In two programs, Virtutech Simics <ref type=\"bibr\" target=\"#b39\">[39]</ref> and Microsoft Excel 2003, almost half of the branch mispre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b0\">2,</ref><ref type=\"bibr\" target=\"#b1\">3,</ref><ref type=\"bibr\" target=\"#b2\">4,</ref><ref type=\"bibr\" target=\"#b3\">5]</ref>. This pipeline system usually suffers from time delay, parame nd generates target words from left to right at each step [1, <ref type=\"bibr\" target=\"#b1\">3,</ref><ref type=\"bibr\" target=\"#b3\">5]</ref>. This model has also achieved promising results in ASR fields chitecture for all three tasks (ASR, ST and MT). The model architecture is similar with Transformer <ref type=\"bibr\" target=\"#b3\">[5]</ref>, which is the state-of-art model in MT task. Recently, this . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 1.0\"><head>Functional Unit Count Latency</head><p>Simple The processor has a lookup-free data cache <ref type=\"bibr\" target=\"#b6\">[7]</ref> that allows up to 8 pending misses to different cache lines.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b18\">Nie et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b31\">Yoneda et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> have achieved the top three results a laimevidence pair individually and then aggregate all NLI predictions for final verification. Then, <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b31\">Yoned the task. In the document retrieval and sentence selection stages, we simply follow the method from <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> since their method has the highest sc ose noisy evidence.</p><p>In the document retrieval step, we adopt the entity linking approach from <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>. Given a claim, the method first util ent selects the most relevant evidence for the claim from all sentences in the retrieved documents. <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> modify the ESIM 2 https://www.mediawi he OFEVER scores of our model and models from other teams. After running the same model proposed by <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>, we find our OFEVER score is slightly cted to form the final evidence set in the original method.</p><p>In addition to the original model <ref type=\"bibr\" target=\"#b10\">(Hanselowski et al., 2018)</ref>, we add a relevance score filter wit models from the FEVER shared task as our baselines.</p><p>The Athene UKP TU Darmstadt team (Athene) <ref type=\"bibr\" target=\"#b10\">(Hanselowski et al., 2018)</ref> combines five inference vectors from. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the target user and item based on historical interactions <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. For example, given two paths p 1 u 1 \u2192 i 1 \u2192 u 2 \u2192 i 2 and terests; meanwhile, the user groups can also profile items <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Hence, in each modality (e.g., visual), we aggregate signa e select LeakyReLU(\u2022) as the nonlinear activation function <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Such aggregation method assumes that different neighbors w ntegrate multi-modal features as the node features to learn the representation of each node. \u2022 NGCF <ref type=\"bibr\" target=\"#b41\">[41]</ref>. This method represent a novel recommendation framework to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et al., 2019)</ref>, QuAC <ref type=\"bibr\" target=\"#b3\">(Choi et al., 2018)</ref>, NaturalQuestions <ref type=\"bibr\" target=\"#b10\">(Kwiatkowski et al., 2019)</ref>, RACE <ref type=\"bibr\" target=\"#b11\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b5\">[5]</ref>, kernel functions <ref type=\"bibr\" target=\"#b29\">[28]</ref> <ref type=\"bibr\" target=\"#b25\">[24]</ref> or other hand-crafted features which measure local neighbo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph signal processing literature to measure the smoothness of a signal defined over nodes of a graph <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2015)</ref>. More specifically, given a graph with the a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and verification flow was adopted by the ACL2 theorem prover. Another GeNoC was formalized by Broek <ref type=\"bibr\" target=\"#b28\">[29]</ref> and is verified for both packet and circuit-switched based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: measures the relevance of a term to the subject of the document. For example, in the following text <ref type=\"bibr\" target=\"#b58\">[59]</ref>:</p><p>Example 1. We propose a low-complexity audio-visual. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lso are sensitive to small, worst-case perturbations of the input, so-called \"adversarial examples\" <ref type=\"bibr\" target=\"#b32\">(Szegedy et al., 2014)</ref>. This latter phenomenon has struck many   et al., 2004;</ref><ref type=\"bibr\" target=\"#b3\">Biggio &amp; Roli, 2018)</ref>. Since the work of <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref>, a subfield has focused specifically on t ly on the phenomenon of small adversarial perturbations of the input, or \"adversarial examples.\" In <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref> it was proposed these adversarial example mproved robustness to small perturbations.</p><p>In the introduction we referred to a question from <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref> about why we find errors so close to our  lem for every point in the test set <ref type=\"bibr\" target=\"#b23\">(Katz et al., 2017)</ref>. Since <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref>, hundreds of adversarial defense papers h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  attention to decide when to stop and then performing soft attention to calculate, also rather than <ref type=\"bibr\" target=\"#b12\">[13]</ref> which needs a CTC trained model to conduct pre-partition b  shows a clear performance advantage than other soft and monotonic models (e.g. triggered attention <ref type=\"bibr\" target=\"#b12\">[13]</ref>), but also matches or surpasses most of the published resu int CTC-attention model / ESPNet <ref type=\"bibr\" target=\"#b15\">[16]</ref> 27.4 Triggered Attention <ref type=\"bibr\" target=\"#b12\">[13]</ref> 30 where the membrane potential Um is constantly simulated. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t harm their representational capacity for object recognition.</p><p>The Single Shot Detector (SSD) <ref type=\"bibr\" target=\"#b21\">[22]</ref> is one of the first attempts at using a ConvNet's pyramida tiple layers before computing predictions, which is equivalent to summing transformed features. SSD <ref type=\"bibr\" target=\"#b21\">[22]</ref> and MS-CNN <ref type=\"bibr\" target=\"#b2\">[3]</ref> predict >[35]</ref>, context modeling <ref type=\"bibr\" target=\"#b15\">[16]</ref>, stronger data augmentation <ref type=\"bibr\" target=\"#b21\">[22]</ref>, etc. These improvements are complementary to FPNs and sho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b8\">[9]</ref>. Thanks to these advances, voice assistant devices such as Google Home <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> , Amazon Alexa or Sam ype=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35</ref>]. An \"acoustic simulator\" <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> is used to generate  /ref>. The acoustic simulator in Fig. <ref type=\"figure\">1</ref> is similar to what we described in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. One difference comp ract LengthPerturbation (VTLP)<ref type=\"bibr\" target=\"#b0\">[1]</ref> , and Acoustics Simulator (AS)<ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, respectively.</figD \">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. One difference compared to our previous one in <ref type=\"bibr\" target=\"#b1\">[2]</ref> is that we do not pre-calculate room impulse responses, but  rver, we ran the VTLP data augmentation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, acoustic simulator <ref type=\"bibr\" target=\"#b1\">[2]</ref> and feature extraction modules shown in Fig. <ref type=\"figu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nese poetry generation have been mostly rulebased or template-based. Recurrent Neural Network (RNN) <ref type=\"bibr\" target=\"#b10\">[11]</ref> was recently introduced as it has been proved to be effect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref> processes speech inpu re modular and each component can be optimized separately or jointly (also with end-to-end criteria <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ommendation algorithms <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and propose a more general framework that combines both co stateof-the-art models <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> are special cases of our framework (e.g. using MSE-loss/Log rve that the current recommendation models based on MSE-loss <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> can be improved by others such as SG-loss and pairwise loss bsumes existing models <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Stochastic Gradient Descent <ref type=\"bibr\" target= id=\"formula_1\">-(u,v )\u2208D log \u03c3 (f T u g v ) + \u03bbE v \u2032 \u223cPn log \u03c3 (\u2212f T u g v \u2032 )</formula><p>MSE-loss <ref type=\"bibr\" target=\"#b29\">[30]</ref>:</p><formula xml:id=\"formula_2\">(u,v )\u2208D ( r + uv \u2212 f T u  have been applied to such problems in many existing work. In <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, mean square loss ( ng and neural networks <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: two representative real-world images, \"chip\" (with 244\u00d7200 pixels) and \"hatc\" (with 133\u00d7174 pixels) <ref type=\"bibr\" target=\"#b44\">[41]</ref>. In this case, the original HR images are not available an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  i to see how well the model performs on that task. The goal of Model-Agnostic Meta-Learning (MAML) <ref type=\"bibr\" target=\"#b9\">[9]</ref> is to obtain a parameter initialization \u03b8 * that can adapt t nowledge across meta-training tasks and is the optimal parameter to adapt to unseen tasks quickly.  <ref type=\"bibr\" target=\"#b9\">[9]</ref> switches ProtoNet to MAML as the meta-learner. All experimen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff an ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dy> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Attention mechanisms <ref type=\"bibr\" target=\"#b0\">(Bahdanau et al., 2014)</ref> are nowadays ubiquitous in NLP, and thei use a single-layer bidirectional LSTM with tanh activation, followed by an additive attention layer <ref type=\"bibr\" target=\"#b0\">(Bahdanau et al., 2014)</ref> and softmax prediction, which is equival. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fline model lack fine-grained estimation and customized models are not general as desired. Timeloop <ref type=\"bibr\" target=\"#b21\">[21]</ref> and Eyeriss <ref type=\"bibr\" target=\"#b22\">[22]</ref> use . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch cannot be simply integrated like other auxiliary information in multiple feedback recommendation <ref type=\"bibr\" target=\"#b5\">[Ding et al., 2018b;</ref><ref type=\"bibr\" target=\"#b6\">Gao et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [2]</ref> have recently been shown to be ideal for this task <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. The pronunciation mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e been considered in numerous papers (e.g. <ref type=\"bibr\" target=\"#b1\">Calders et al., 2009;</ref><ref type=\"bibr\" target=\"#b18\">Zafar et al., 2017)</ref>. <ref type=\"bibr\" target=\"#b8\">Hardt et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  N have a different distribution from that with a larger N <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. Identity mapping approximation. Another possible attack is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecific sequence generation. A coverage driven test generation technique is presented by Fine et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Shen et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> have us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the data are fed into the following tasks <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Ranshous et al., 2015]</ref>.</p><p>It is not trivial to detect the an inly us-ing structural features. Other works <ref type=\"bibr\" target=\"#b7\">[Zhao and Yu, 2013;</ref><ref type=\"bibr\" target=\"#b4\">McConville et al., 2015]</ref> consider content feature or even tempor sides the structural features, the temporal ones are considered in the anomaly detection. CM-Sketch <ref type=\"bibr\" target=\"#b4\">[Ranshous et al., 2016]</ref> is a sketch-based method, which uses the de cluster, and the model can also be used to produce anomalous score for a given edge. \u2022 CM-Sketch <ref type=\"bibr\" target=\"#b4\">[Ranshous et al., 2016]</ref>. It uses the local structural feature an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ith kernel machines, one typically observes an increase in performance when using soft-DTW over DTW <ref type=\"bibr\" target=\"#b6\">(Cuturi, 2011)</ref> for classification.</p><p>Our contributions. We e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ew other studies recognise person names and publications from research papers and digital libraries <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd other conventional semantic models.</p><p>In this study, based on a convolutional neural network <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we present a new Convolutional Deep Structured Semantic Mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ructures with only 6 convolutional layers, where the activation functions, batch normalization (BN) <ref type=\"bibr\" target=\"#b10\">[11]</ref> and ReLU <ref type=\"bibr\" target=\"#b18\">[19]</ref>, are om et=\"#b7\">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the activation functions (BN <ref type=\"bibr\" target=\"#b10\">[11]</ref> and ReLU <ref type=\"bibr\" target=\"#b18\">[19]</ref>) are pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in that represents the DUT. The Markov Chain is then used to generate test-cases for the design. In <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the coverage analysis results trigger a set of generation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: well as training time (poisoning attacks) <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b10\">Dai et al., 2018)</ref>. A core strength of models using graph convol <p>Only recently researchers have started to study adversarial attacks on deep learning for graphs. <ref type=\"bibr\" target=\"#b10\">Dai et al. (2018)</ref> consider test-time (i.e., evasion) attacks on ating their impact by training a classifier on the data modified by their algorithm. In contrast to <ref type=\"bibr\" target=\"#b10\">Dai et al. (2018)</ref>, their attacks can both insert and remove edg fied by our algorithm. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b10\">Dai et al. (2018)</ref>, our algorithm is designed for global attacks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rsonalized content recommendation <ref type=\"bibr\" target=\"#b47\">[46]</ref>, and question answering <ref type=\"bibr\" target=\"#b43\">[42]</ref>. For example, organizing copious scientific papers into a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >43]</ref>. Hypergraph neural networks <ref type=\"bibr\" target=\"#b16\">[17]</ref> and their variants <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> use the clique exp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and labeling the individual body parts. Numerous type of features have been used such as silhouette <ref type=\"bibr\" target=\"#b6\">(Gouiaa and Meunier, 2015)</ref>, contour <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: suffering too much from the vanishing effect <ref type=\"bibr\" target=\"#b15\">(Hochreiter, 1991;</ref><ref type=\"bibr\" target=\"#b2\">Bengio et al., 1994;</ref><ref type=\"bibr\" target=\"#b20\">Pascanu et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: only provide few partitions and often degrade performance, D-NUCA schemes seldom use them. ASP-NUCA <ref type=\"bibr\" target=\"#b11\">[12]</ref>, ESP-NUCA <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and E tency of private caches. These schemes often size partitions using hill-climbing (e.g., shadow tags <ref type=\"bibr\" target=\"#b11\">[12]</ref> or LRU way hit counters <ref type=\"bibr\" target=\"#b15\">[16. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing (AT) procedure <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> shows promising res al training (e.g., <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar unreliable for generating adversarial samples during single-step adversarial training. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that models trained using adversarial samples =\"bibr\" target=\"#b8\">9]</ref> accepted to ICLR 2018. In this direction, adversarial training method <ref type=\"bibr\" target=\"#b21\">[22]</ref>, shows promising results for learning robust deep learning ls trained using EAT are still susceptible to multi-step attacks in white-box setting. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that adversarially trained model can be made  show that over-fitting effect is the reason for failure to satisfy the criteria.</p><p>Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that it is possible to learn robust models us rameters (\u03b8) should be updated so as to decrease the loss on such adversarial samples. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> solves the maximization step by generating adversarial sam raining method <ref type=\"bibr\" target=\"#b12\">[13]</ref> and multi-step adversarial training method <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Column-1 of Fig. <ref type=\"figure\" target=\"#fig_0\">1</re  for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by <ref type=\"bibr\" target=\"#b21\">[22]</ref>. At each iteration all the clean samples in the mini-batch d for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by<ref type=\"bibr\" target=\"#b21\">[22]</ref>. At each iteration all the clean samples in the mini-batch s added to the image. In our experiments, we set \u03b1 = /steps.</p><p>Projected Gradient Descent (PGD) <ref type=\"bibr\" target=\"#b21\">[22]</ref>: Initially, a small random noise sampled from Uniform dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d><p>Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The network predicts 4 coordinates for each bounding box, ocation of filter application using a sigmoid function. This figure blatantly self-plagiarized from <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>is not the best but does overlap a ground truth obj location of filter application using a sigmoid function. This figure blatantly self-plagiarized from<ref type=\"bibr\" target=\"#b14\">[15]</ref>.</figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n applied to a variety of tasks, like image classification <ref type=\"bibr\" target=\"#b10\">[12,</ref><ref type=\"bibr\" target=\"#b16\">18,</ref><ref type=\"bibr\" target=\"#b17\">19,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We adopt larger input data sets varying from 147 to 187 G  both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in the memory system. And for each workload, we collect t  performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type=\"bibr\" target=\"#b16\">[17]</ref>).</p><p>We find that big data analytics applications share chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type=\"bibr\" target=\"#b16\">[17]</ref>) workloads. Meanwhile the service workloads in data center s, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>\u2022 Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and service workloa the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type=\"bibr\" target=\"#b16\">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall b vel cache), respectively. For the service workloads, our observations corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><p>\u2022 For the big data ana rk of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, CloudSuite paper is biased towards online servic . The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, our approach are more pragmatic. We adopt a larger data i  in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in memory. The number of instructions retired of the big  , HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and compared them with big data analytics workloads.</p>< as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the o n fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The front end inefficiency may caused by high-level langu  Figure <ref type=\"figure\" target=\"#fig_3\">5</ref>.</p><p>Implications: Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and the service wor ns, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and large binary size complicated by high-level language, sor and save the die area. For the service workloads, our observation corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id=\"formula_ s in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type=\"bibr\" target=\"#b16\">[17]</ref>. According to our correlation analysis in this section, ar ten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Moreover, for mod s prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and etc. Narayanan et al. <ref type=\"bibr\" target=\"#b32\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  has made abstractive summarization viable <ref type=\"bibr\" target=\"#b3\">(Chopra et al., 2016;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016;</ref><ref type=\"bibr\" target=\"#b20\">Rush et a arget=\"#b7\">Gulcehre et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Miao and Blunsom, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016;</ref><ref type=\"bibr\" target=\"#b28\">Zeng et a cently-introduced CNN/ Daily Mail dataset <ref type=\"bibr\" target=\"#b8\">(Hermann et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016)</ref>, which contains news articles (39 sente head><p>We use the CNN/Daily Mail dataset <ref type=\"bibr\" target=\"#b8\">(Hermann et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016)</ref>, which contains online news articles (7 ad n=\"2.1\">Sequence-to-sequence attentional model</head><p>Our baseline model is similar to that of <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>, and is depicted in Figure <ref type=\"f e on those datasets.</p><p>However, large-scale datasets for summarization of longer text are rare. <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> adapted the DeepMind question-answering  considerably different from that of <ref type=\"bibr\" target=\"#b7\">Gulcehre et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>. Those works train their pointer compon with multi-sentence summaries (3.75 sentences or 56 tokens on average). We used scripts supplied by <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> to obtain the same version of the the d and b ptr in equation 8), and coverage adds 512 extra parameters (w c in equation 11).</p><p>Unlike <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>, we do not pretrain the word embeddings is (+1.1 ROUGE-1, +2.0 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> by (+4.07 ROUGE-1, +3.98 ROUGE-2, +3.73  scene. (...) Summary: more questions than answers emerge in controversial s.c. police shooting. of <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> by several ROUGE points. Despite the br g Representations <ref type=\"bibr\" target=\"#b24\">(Takase et al., 2016)</ref>, hierarchical networks <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type=\"b nique that has been applied to <ref type=\"bibr\">NMT (Sankaran et al., 2016)</ref> and summarization <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>. In this approach, each attention dist rget=\"#tab_2\">1</ref>), compared to the smaller boost given by temporal attention for the same task <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>.</p></div> <div xmlns=\"http://www.tei- he first three sentences of the article as a summary), and compare to the only existing abstractive <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref> and extractive <ref type=\"bibr\" target training pairs, 13,368 validation pairs and 11,490 test pairs. Both the dataset's published results <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016</ref><ref type=\"bibr\" target=\"#b16\">(Nallapat le online. <ref type=\"foot\" target=\"#foot_5\">6</ref>Given that we generate plain-text summaries but <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016;</ref><ref type=\"bibr\" target=\"#b16\">2017)</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 7]</ref>). Other systems HLLs are less compatible with existing kernel designs. For example, Erlang <ref type=\"bibr\" target=\"#b2\">[2]</ref> is a \"shared-nothing\" language with immutable objects, which. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ethods aim to model the temporal or motion information in videos. The Long Short-Term Memory (LSTM) <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and C3D <ref type=\"bibr\" target=\"#b6\">[7]</ref> are two wid. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of localized spectral filters on graphs <ref type=\"bibr\" target=\"#b11\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b5\">Defferrard et al., 2016)</ref>.</p></div> <div xmlns=\"http://www.tei-c er neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. <ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> use this K-localized convolution to def pectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by<ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> with fast localized convolutions. In co  introduced to the original frameworks of<ref type=\"bibr\" target=\"#b3\">Bruna et al. (2014)</ref> and<ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> that improve scalability and classifica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  and reinforcement <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> learning. Specifically, recent work has shown that deep neu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orms the sentiment view. Then, using a framework of multi-view canonical correlation analysis (CCA) <ref type=\"bibr\" target=\"#b18\">[11]</ref>, we calculate a latent embedding space in which correlatio s the linear relationship between random variables. Several nonlinear extensions such as kernel CCA <ref type=\"bibr\" target=\"#b18\">[11]</ref> and Deep CCA <ref type=\"bibr\" target=\"#b29\">[22]</ref> hav ions among multiple views using a framework of the generalization of canonical correlation analysis <ref type=\"bibr\" target=\"#b18\">[11]</ref>. Let X i (i \u2208 {v, t, s}) denote the feature matrix of the  at the distances in the resulting space between each pair of views for the same image are minimized <ref type=\"bibr\" target=\"#b18\">[11]</ref>. The objective function to learn the latent space is as fo  \u03d5 j (X j ), and w ik represents the k-th column of the matrix W i . In the conventional kernel CCA <ref type=\"bibr\" target=\"#b18\">[11]</ref>, kernel trick is used in Eq. (1). To reduce the computatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  few data but achieve good performance. A typical example of this approach is prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref>, which averages the vector of few support  http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">Prototypical Networks</head><p>The prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> has achieved excellent performance in few- cia and Bruna, 2018)</ref>, SNAIL <ref type=\"bibr\" target=\"#b10\">(Mishra et al., 2018)</ref>, Proto <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> and PHATT <ref type=\"bibr\" target=\"#b3\">(G  its label, and obviate the need for fine-tuning to adapt to new class types. Prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017</ref>) learns a metric space in which the model c  (y = l i q) = exp(\u2212d(g \u03b8 (q), c i ) \u03a3 L l=1 exp(\u2212d(g \u03b8 (q), c l )<label>(9)</label></formula><p>As <ref type=\"bibr\" target=\"#b13\">Snell et al. (2017)</ref> mentioned, squared Euclidean distance is a  he implementation details are as follows.</p><p>For FewRel dataset, we cite the results reported by <ref type=\"bibr\" target=\"#b13\">Snell et al. (2017)</ref> which includes Finetune, kNN, MetaN, GNN, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ding block for many efficient neural network architectures <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> and we use them in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by powerful baseline systems, such as the Fast/Faster R-CNN <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29]</ref> and Fully Convolutional Network (FCN) <ref type=\"bibr\" targ target=\"#b14\">[9]</ref>. N is 64 for the C4 backbone (as in <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29]</ref>) and 512 for FPN (as in <ref type=\"bibr\" target=\"#b27\">[22] state-of-the-art instance segmentation results. Our method, called Mask R-CNN, extends Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref> by adding a branch for predicting segmentation masks on ea ding to RoIs on feature maps using RoIPool, leading to fast speed and better accuracy. Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref> advanced this stream by learning the attention mechanism w e of Fast/Faster R-CNN.</p><p>Faster R-CNN: We begin by briefly reviewing the Faster R-CNN detector <ref type=\"bibr\" target=\"#b34\">[29]</ref>. Faster R-CNN consists of two stages. The first stage, cal are shareable.</p><p>Inference: At test time, the proposal number is 300 for the C4 backbone (as in <ref type=\"bibr\" target=\"#b34\">[29]</ref>) and 1000 for FPN (as in <ref type=\"bibr\" target=\"#b27\">[2 hares features between the RPN and Mask R-CNN stages, following the 4-step training of Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref>. This model runs at 195ms per image on an Nvidia Tesla M40  hyper-parameters following existing Fast/Faster R-CNN work <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29,</ref><ref type=\"bibr\" target=\"#b27\">22]</ref>. Although these dec decisions were made for object detection in original papers <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29,</ref><ref type=\"bibr\" target=\"#b27\">22]</ref>, we found our insta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b18\">[17,</ref><ref type=\"bibr\" target=\"#b22\">21,</ref><ref type=\"bibr\" target=\"#b45\">44,</ref><ref type=\"bibr\" target=\"#b49\">48]</ref>, network embedding with rich vertex attributes <ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b4\">Fan et al., 2017;</ref><ref type=\"bibr\" target=\"#b25\">Scarton and Specia, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Nishihara et al., 2019)</ref> where a Sequence-to-Sequence (Seq2Seq)  =\"#b4\">(Fan et al., 2017)</ref>. <ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref> and <ref type=\"bibr\" target=\"#b17\">Nishihara et al. (2019)</ref> similarly showed that adding control to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b12\">13]</ref>, Inception V3 <ref type=\"bibr\" target=\"#b34\">[35]</ref>, Inception V4 <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and Inception-ResNet V2 <ref type=\"bibr\" target=\"#b33\">[3 =\"#b34\">[35]</ref>, Inception V4 <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and Inception-ResNet V2 <ref type=\"bibr\" target=\"#b33\">[34]</ref>  We also attack the corresponding ensemble model (referred. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[37]</ref> uses semantic segmentation for video deblurring. Zhu et al. <ref type=\"bibr\" target=\"#b51\">[52]</ref> propose an approach to generate new clothing on a wearer.  nal bias at the input layer.</p><p>2) Compositional mapping -This method is identical to Zhu et al. <ref type=\"bibr\" target=\"#b51\">[52]</ref>. It decomposes an LR image based on the predicted semantic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e human and model word distributions. This does not appear to be rectified by training on more data <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref>. Recent fixes involve modifying the deco emedied by simply increasing the amount of the training data; e.g. largescale GPT-2 language models <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref> display the same issues.</p><p>Improved  story generation <ref type=\"bibr\" target=\"#b7\">(Fan et al., 2018)</ref>, contextual text completion <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref>, language modeling (for k = 0), and dial  type=\"table\">4</ref>, which</p><p>shows completions from the state-of-the-art GPT-2 language model <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref>. Greedy decoding as well as top-k and nu mprove existing pre-trained language models. We demonstrate this by fine-tuning a pre-trained GPT-2 <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref> language model with sequence-level unlik /ref>: Top: Degenerate repetition in completions from a state-of-the-art large-scale language model <ref type=\"bibr\" target=\"#b19\">(Radford et al., 2019)</ref>. The examples contain single-word repeti ecture agnostic; we choose this one as a representative of recent large-scale language models, e.g. <ref type=\"bibr\" target=\"#b19\">Radford et al. (2019)</ref>. LUL-token+seq , and German . In the firs or the Naismith Award , which is 0.064  <ref type=\"bibr\" target=\"#b0\">(Baevski and Auli, 2019;</ref><ref type=\"bibr\" target=\"#b19\">Radford et al., 2019)</ref>. We perform experiments at the word level. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Indeed, some efforts <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> have been dedicated to eliminating the effects of false-pos Figure <ref type=\"figure\" target=\"#fig_1\">1(b)</ref>); and 2) the incorporation of various feedback <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> (shown in Figure < rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref> also consider incor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: targeted network, which corresponds to a more restrictive black box threat model.</p><p>Recent work <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b1\">Bhagoji et al., =\"bibr\" target=\"#b10\">Ilyas et al., 2017)</ref> provides a number of attacks for this threat model. <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> show how to use a basic primitive of zeroth o (x, ) (x l\u22121 + \u03b7 s l ) with s l = \u03a0 \u2202Bp(0,1) \u2207 x L(x l\u22121 , y)<label>(4)</label></formula><p>Indeed, <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> were the first to use finite differences meth ty d=268,203 and thus this method would require 268,204 queries. (It is worth noting, however, that <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> developed additional methods to, at least par. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><p>Speech animation for rigged models. Several related methods produce animation curves for speech <ref type=\"bibr\" target=\"#b12\">[Edwards et al. 2016;</ref><ref type=\"bibr\" target=\"#b59\">Taylor et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recurs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: raged to launch Distributed Denial-of-Service (DDoS) attacks, in particular, the reflection attacks <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Due to the absence of a method to block packet header modif eans conclude on the filtering policies of the whole AS-they reveal SAV compliance for a part of it <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target ysis of the SAV deployment at the longest matching prefix is another commonly used unit of analysis <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>.</p><p>\u2022 /24 IPv4 ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: not visible at the node-level.</p><p>Graph kernels based on the k-WL have been proposed in the past <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref>. However, a key advantage o ref type=\"bibr\" target=\"#b35\">(Shervashidze et al. 2011)</ref> as well as its higher-order variants <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref>. Graphlet and Weisfeiler-Le  <ref type=\"bibr\" target=\"#b22\">(Kriege, Giscard, and Wilson 2016)</ref>, and the global-local k-WL <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref> with k in {2, 3} as kernel  \"foot_0\">Note that the definition of the local neighborhood is different from the the one defined in<ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref> which is a superset of our  e that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017;</ref><ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: educes the effectiveness of prefetching.</p><p>Recent solutions use the Global History Buffer (GHB) <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which organizes correlation information by storing recent uce the GHB as a general structure for prefetching streams of temporally correlated memory requests <ref type=\"bibr\" target=\"#b27\">[28]</ref>. However, when used to record address correlation <ref typ /DC prefetcher, which which learns the deltas, or differences, between consecutive memory addresses <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Delta correlation allows PC/DC to store all meta-data on  http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\">Using Nesbit and Smith's terminology<ref type=\"bibr\" target=\"#b27\">[28]</ref>, in which the name before the slash describes the referenc streams based on the PC of the loading instruction, which is known to improve coverage and accuracy <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta fetchers, SMS <ref type=\"bibr\" target=\"#b38\">[39]</ref>, which exploits spatial locality, and PC/DC <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, which uses delta  long streams. Rather than use address correlation, other GHBbased prefetchers use delta correlation <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, whose space requi -based prefetchers <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar ype=\"bibr\" target=\"#b42\">[43]</ref> or address correlation <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, sacrificing signif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to produce natural prosody with high audio fidelity using a much simplified voice building pipeline <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target ct. Specifically, we propose a simple yet effective semi-supervised framework for training Tacotron <ref type=\"bibr\" target=\"#b0\">[1]</ref>, a recently proposed end-to-end TTS model. We propose to tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te networks do not always transfer to the target model, especially when conducting targeted attacks <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">Narodytska &am :</p><p>Black-box setting. In this paper, we use the definition of black-box access as query access <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al., 20 orders of magnitude more query-efficient than previous methods based on gradient estimation such as <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>. We show that our approach reliably produces  imated by querying the classifier rather than computed by autodifferentiation. This idea is used in <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>, where the gradient is estimated via pixel-by =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.2.\">BLACK-BOX ATTACKS WITH GRADIENT</head><p>ESTIMATION <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref> explore black-box gradient estimation methods mpability of the 2 and \u221e metric as well as the fixed-budget nature of the optimization algorithm in <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>, our method takes far fewer queries to genera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: vely more abstract representations by mapping the input through a series of parameterized functions <ref type=\"bibr\" target=\"#b23\">(LeCun et al., 2015)</ref>. In the current generation of neural netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d does not incur any storage overhead and is transferable across both hardware and software changes <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Functional warmin t, requires huge storage overhead, and does not allow for software changes. Functional warming (FW) <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> does not incur any on, which is fast but does not allow for changes to the software. Virtualized Fast-Forwarding (VFF) <ref type=\"bibr\" target=\"#b25\">[26]</ref> leverages hardware virtualization to quickly get to the ne 31]</ref> extend on the concept of BLRL using a form of hardware state checkpoints. Sandberg et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> propose a method that uses two parallel simulations, pessi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uire specialized architectures and training scheme modifications to achieve competitive performance <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target =\"bibr\" target=\"#b13\">[14]</ref>. Based on past experience <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> we favor this formulation over other variants <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b19\">Maclaurin et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b0\">Abadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b23\">Paszke et al., ribution, and code generation (see, e.g., <ref type=\"bibr\" target=\"#b2\">Bergstra et al., 2010;</ref><ref type=\"bibr\" target=\"#b0\">Abadi et al., 2016)</ref>. But, because declarative DSLs prevent users bed in \u00a74.6. TensorFlow graphs come with their own set of design principles, which are presented in <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>. The following terminology will be used in  devices and parallelizes operations when possible. Readers interested in the runtime should consult <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>.</p><p>The function decorator supports code. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esults. Our work is partly inspired by the works on generating and refining score maps. Yang et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> adopts pyramid features as inputs of the network in the pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tism <ref type=\"bibr\" target=\"#b3\">(Evans et al., 2014)</ref> but also for second language learners <ref type=\"bibr\" target=\"#b36\">(Xia et al., 2016)</ref> and people with low literacy <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b21\">22]</ref> and TTS models such as Tacotron <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n f 256 ReLU units, which was found to help to learn attention <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. The pre-net output and attention context vector are concat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce and is not structured for human communication (e.g., unlike words).</p><p>Several recent studies <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" ta  be used with various pretext tasks. In this paper, we follow a simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" ta 8\">[29]</ref>. Contrastive learning is at the core of several recent works on unsupervised learning <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" ta </ref>. Overall, all three mechanisms benefit from a larger K. A similar trend has been observed in <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> under the memory b t tasks can be based on some form of contrastive loss functions. The instance discrimination method <ref type=\"bibr\" target=\"#b60\">[61]</ref> is related to the exemplar-based task <ref type=\"bibr\" tar +/\u03c4 ) K i=0 exp(q\u2022ki/\u03c4 )<label>(1)</label></formula><p>where \u03c4 is a temperature hyper-parameter per <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The sum is over one positive and K negative samples. Intu these networks to downstream tasks.</p><p>Another mechanism is the memory bank approach proposed by <ref type=\"bibr\" target=\"#b60\">[61]</ref> (Figure <ref type=\"figure\">2b</ref>). A memory bank consis ver the past epoch and thus are less consistent. A momentum update is adopted on the memory bank in <ref type=\"bibr\" target=\"#b60\">[61]</ref>. Its momentum update is on the representations of the same igning a new pretext task, we use a simple one mainly following the instance discrimination task in <ref type=\"bibr\" target=\"#b60\">[61]</ref>, to which some recent works <ref type=\"bibr\" target=\"#b62\" =\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> are related.</p><p>Following <ref type=\"bibr\" target=\"#b60\">[61]</ref>, we consider a query and a key as a positive pair if they  ose last fully-connected layer (after global average pooling) has a fixed-dimensional output (128-D <ref type=\"bibr\" target=\"#b60\">[61]</ref>). This output vector is normalized by its L2-norm <ref typ  (128-D <ref type=\"bibr\" target=\"#b60\">[61]</ref>). This output vector is normalized by its L2-norm <ref type=\"bibr\" target=\"#b60\">[61]</ref>. This is the representation of the query or key. The tempe  or key. The temperature \u03c4 in Eqn.( <ref type=\"formula\" target=\"#formula_0\">1</ref>) is set as 0.07 <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The data augmentation setting follows <ref type=\"bibr\" ta f>) is set as 0.07 <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The data augmentation setting follows <ref type=\"bibr\" target=\"#b60\">[61]</ref>: a 224\u00d7224-pixel crop is taken from a randomly resized ima ate of 0.03. We train for 200 epochs with the learning rate multiplied by 0.1 at 120 and 160 epochs <ref type=\"bibr\" target=\"#b60\">[61]</ref>, taking \u223c53 hours training ResNet-50. For IG-1B, we use a  ecause the positive key is in the same mini-batch). The network is ResNet-50.</p><p>The memory bank <ref type=\"bibr\" target=\"#b60\">[61]</ref> mechanism can support a larger dictionary size. But it is   We hope an advanced pretext task will improve this. Beyond the simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61]</ref>, it is possible to adopt MoCo for pretext tasks like maske 1\">Here 58.0% is with InfoNCE and K=65536. We reproduce 54.3% when using NCE and K=4096 (the same as<ref type=\"bibr\" target=\"#b60\">[61]</ref>), close to 54.0% in<ref type=\"bibr\" target=\"#b60\">[61]</re  when using NCE and K=4096 (the same as<ref type=\"bibr\" target=\"#b60\">[61]</ref>), close to 54.0% in<ref type=\"bibr\" target=\"#b60\">[61]</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place sed on other forms <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, such as margin-bas specific pretext task. The input x q and x k can be images <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, patches <ref type= backbone, by default used in existing ResNet-based results <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f Ithemal is a hierarchical multiscale RNN <ref type=\"bibr\" target=\"#b37\">(Shuai et al., 2015;</ref><ref type=\"bibr\" target=\"#b44\">Zhu et al., 2016)</ref> that sequentially processes all instructions  <ref type=\"figure\">4</ref> shows a DAG-RNN <ref type=\"bibr\" target=\"#b37\">(Shuai et al., 2015;</ref><ref type=\"bibr\" target=\"#b44\">Zhu et al., 2016)</ref>. Instructions are embedded identically as to  ent execution platforms. In with a DAG-RNN <ref type=\"bibr\" target=\"#b37\">(Shuai et al., 2015;</ref><ref type=\"bibr\" target=\"#b44\">Zhu et al., 2016)</ref>, we hoped to be able to take advantage of the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rnatively refer to this CaffeNet as model S, for \"small.\" The second network is VGG CNN M 1024 from <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which has the same depth as S, but is wider. We call this n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: corruption process, denoted by corrupt(\u2022), by modifying the subject and object of triples in the KB <ref type=\"bibr\" target=\"#b33\">(Nickel et al. 2016)</ref>:</p><formula xml:id=\"formula_5\">L K (\u03b8) =  contains over 637 \u00d7 10 6 facts, while the Google Knowledge Graph contains more than 18 \u00d7 10 9 facts <ref type=\"bibr\" target=\"#b33\">(Nickel et al. 2016)</ref>. Identifying the facts F \u2208 K that yield th  in real-world use cases, where there is an abundance of text but the KBs are sparse and incomplete <ref type=\"bibr\" target=\"#b33\">(Nickel et al. 2016)</ref>. GNTPs are extremely efficient at learning. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eural methods, abstractive summarization received less attention than extractive summarization, but <ref type=\"bibr\" target=\"#b9\">Jing (2000)</ref> explored cutting unimportant parts of sentences to c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing to produce only a single texture and have so far not been applied to SISR. Adversarial networks <ref type=\"bibr\" target=\"#b17\">[18]</ref> have recently been shown to produce sharp results in a num ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.4\">Adversarial training</head><p>Adversarial training <ref type=\"bibr\" target=\"#b17\">[18]</ref> is a recent technique that has proven to be a useful mecha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b12\">(Maal\u00f8e et al., 2016;</ref><ref type=\"bibr\" target=\"#b25\">Springenberg, 2016;</ref><ref type=\"bibr\" target=\"#b15\">Odena, 2016;</ref><ref type=\"bibr\">Salimans et al., 2016)</ref>. It A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: c.org/ns/1.0\"><head n=\"4.2.2\">Predicting Performance in Solo-Mode.</head><p>Referring to prior work <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we design shadow solo-cycle accounting (SSCA) approach to e prediction method of QoSMT is inspired by PTA. PTA uses MLP correction to achieve higher accuracy <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, we can not get an application's MLP without offl SMT throughput and fairness, but they did not take performance control into account. Eyerman et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed the per-thread cycle accounting (PTA) mechanism t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ons of deep clustering <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, deep attractor networks <ref type=\"bibr\" target=\"#b3\">[4,</r et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. However, this usually only leads to small improvements, even proach for supervised speech separation is via T-F masking <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. The proposed approach is expected to produce even better sep  reconstruction, it is necessary to first obtain a good enough magnitude estimate. Our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a novel multi-task learning approach combining the  gs:</p><formula xml:id=\"formula_0\">LDC,classic = V V T \u2212 Y Y T 2 F (1)</formula><p>Our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> suggests that an alternative loss function, which whitens th  be discussed in Section 3.4. Following <ref type=\"bibr\" target=\"#b21\">[22]</ref>, our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a chimera++ network combining the two approaches vi \" target=\"#b13\">[14]</ref> only performs iterative reconstruction for each source independently. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref ty es remain fixed during iterations, while the phase of each source are iteratively reconstructed. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the phase reconstruction was only added as a post-processin tained when applying five iterations of Griffin-Lim on each source independently, as is reported in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Performing end-to-end optimization using LWA improves the r ates directly in the time domain. Our result is 1.1 dB better than the previous state-of-the-art by <ref type=\"bibr\" target=\"#b2\">[3]</ref> in terms of both SI-SDR and SDR.</p></div> <div xmlns=\"http:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s. Plenty of researches have been conducted to solve the name ambiguity problem. Supervised methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>,</p><p>The research is supported by the National Key Researc representations play a critical role to quantify distinctions and similarities between publications <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The majority of existing solutions utilize biographical fea lustering (HAC) method works well for skewed data and is widely in many name disambiguation methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t ected components to generate the clustering results for each name to be disambiguated. Zhang et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>: This method uses a global metric learning and local linkage For example, <ref type=\"bibr\" target=\"#b4\">[5]</ref> need to specify the number of distinct author, <ref type=\"bibr\" target=\"#b0\">[1]</ref> need labeled data to estimate the number. For a fair compari d co-occurrence information of text and loss a certain amount of semantic information. Zhang et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> also use a graph convolutional network based encoder-decoder based framework to extract multiple types of characteristics and relations in publication database. <ref type=\"bibr\" target=\"#b0\">[1]</ref> use a global metric learning and local linkage graph auto-en. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 38]</ref> is proved effective for machine translation, which can yield large gains in terms of BLEU <ref type=\"bibr\" target=\"#b23\">[24]</ref> as compared to the state-of-the-art methods. To cope with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gh neural networks, these networks have made advancements in diverse fields such as computer vision <ref type=\"bibr\" target=\"#b23\">[22]</ref>, speech recognition <ref type=\"bibr\" target=\"#b8\">[8]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref> . Kernel function-based FCM defined the dissimilarity between pixels by RBF kernel function <ref type=\"bibr\" target=\"#b26\">[27]</ref> . Riemannian manifold space-based Manifold Projection (MP). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s) and directly harness the resultant example to fool the remote target model (i.e., victim models) <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>Among these t \" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, and the other one is transfer-based <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta or fair comparisons, we adopt default parameters as recommended in benchmark approaches and Foolbox <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. The random noise   of the source model <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Specifically, althou ork is the regularization-based approach: transferable adversarial perturbation (TAP) introduced by <ref type=\"bibr\" target=\"#b40\">[41]</ref>. TAP injects two regularization terms into the vanilla tra 19\">[20,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. We follow the protocol of the baseline method <ref type=\"bibr\" target=\"#b40\">[41]</ref> to curate experimental datasets and target models for fair cannot strictly meet the l \u221e budget, we employ the modified l \u221e version of C&amp;W as introduced by <ref type=\"bibr\" target=\"#b40\">[41]</ref>, which can explicitly satisfy the l \u221e norm constraint. Sim b40\">[41]</ref>, which can explicitly satisfy the l \u221e norm constraint. Similar to our strategy, TAP <ref type=\"bibr\" target=\"#b40\">[41]</ref> boosts adversarial transferability through two regularizat e random noise is sampled from a clipped normal distribution with mean 0 and variance 1.  Following <ref type=\"bibr\" target=\"#b40\">[41]</ref>, we fix the perturbation budget \u01eb to 16 for all methods. W ext attack models defended by adversarial training. For fair comparisons with the baseline approach <ref type=\"bibr\" target=\"#b40\">[41]</ref>, we stick to employing undefended models as local source m d the other one is the regularization-based transferable adversarial perturbation (TAP) proposed by <ref type=\"bibr\" target=\"#b40\">[41]</ref>. With the integrated attacks, we conduct experiments simil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: igner. We call this method pronunciation-assisted sub-word modeling (PASM), which adopts fast align <ref type=\"bibr\" target=\"#b8\">[9]</ref> to align a pronunciation lexicon arXiv:1811.04284v2 [cs.CL] . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: variety of relevance match signals and shows strong performance in various ad-hoc retrieval dataset <ref type=\"bibr\" target=\"#b3\">(Dai and Callan, 2019)</ref>. Recent research also has shown kernels c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref> processes speech input directly into intent without going th tly the most promising results under-perform or just barely outperform traditional cascaded systems <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. One reason is that d to-end speech-to-intent model, we need intent-labeled speech data, and such data is usually scarce. <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> address this problem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: emand for secure communication and secure operation due to rising online fraud and software attacks <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Some of these vulnerabilities are due to the complexity and  a binary positive number to each BB, and the length of a signature Len can be obtained by equation <ref type=\"bibr\" target=\"#b0\">(1)</ref>, where N is the number of total BBs. </p></div> <div xmlns=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b5\">6]</ref>, previous model bias <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and position bias < niform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. In this paper, we  f a recommender system, and that explicitly handling of the biases may help improve the performance <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta f>.</p><p>A recent work has shown that a uniform data can alleviate the previous model bias problem <ref type=\"bibr\" target=\"#b15\">[16]</ref>. But the uniform data is always few and expensive to colle us on how to solve the bias problems in a recommender system with a uniform data. Along the line of <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we conduct empirical studies on a real advertising system /p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">MOTIVATION</head><p>In a recent work <ref type=\"bibr\" target=\"#b15\">[16]</ref>, it is shown that a uniform (i.e., unbiased) data can alle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  that the cost benefits brought by spot offerings can be realized with intuitive bidding strategies <ref type=\"bibr\" target=\"#b14\">[15]</ref>. However, choosing between spot instances and bid levels a e on-demand price since the user only pays the spot price anyway, as is commonly advocated (e.g. in <ref type=\"bibr\" target=\"#b14\">[15]</ref>) and used in practice. Under the given model, the WSD assu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b16\">[17]</ref>.</p><p>A more direct approach to LTR metric optimization was proposed by Qin et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref>, where the rank variable in the definition of metrics like Recent hardware and software advances in the training of neural networks, however, make the work in <ref type=\"bibr\" target=\"#b13\">[14]</ref> relevant again and potentially allow us to harvest the eff ng state-of-the-art LTR algorithms such as LambdaMART. We give an overview of LTR and in particular <ref type=\"bibr\" target=\"#b13\">[14]</ref> in Section 2. We discuss experimental results in Section 3 \"#b14\">[15]</ref>, boosting <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and approximating the metric <ref type=\"bibr\" target=\"#b13\">[14]</ref>. It is the latter that can tightly bound any ranking metri \" target=\"#b13\">[14]</ref>. It is the latter that can tightly bound any ranking metric such as NDCG <ref type=\"bibr\" target=\"#b13\">[14]</ref> and can be easily optimized with gradient descent.</p><p>S adient descent.</p><p>Surprisingly, despite its attractive theoretical properties, the framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> has received little attention in LTR studies in the decade ts to optimize NDCG-referred to as ApproxNDCG. Our results show that  the theoretical guarantees in <ref type=\"bibr\" target=\"#b13\">[14]</ref> materialize in practice. Before we go any further, we give  the indicator which is 1 if s &lt; t and 0 otherwise. <ref type=\"bibr\">Qin et al.</ref> propose in <ref type=\"bibr\" target=\"#b13\">[14]</ref> a smooth approximation of Equation <ref type=\"formula\" tar ture of ranking utility functions.</p><p>In this work, we set out to revisit the work of Qin et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> which formulates a smooth approximation to any ranking met  any ranking metric such as NDCG. Unlike many other existing surrogate LTR losses, the framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> offers a way to directly optimize ranking metrics. Because g metrics rather than loosely related surrogate losses; and (b) that the approximation framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> could lay out the foundation of deep neural networks in LT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: irectly. Two popular neural network sequence models are Connectionist Temporal Classification (CTC) <ref type=\"bibr\" target=\"#b10\">[10]</ref> and recurrent models for sequence generation <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Dynamic Memory Network (DMN) <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses a gated recurrent unit <ref type=\"bibr\" target=\"#b1\">[2]</ref> based controller to update the memory, while Working Memory . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ughput estimations, as do learningbased techniques like genetic algorithm based register allocation <ref type=\"bibr\" target=\"#b40\">(Stephenson et al., 2003)</ref> and reinforcement learning based inst. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  sampled sizes for each layer (S 1 = 25, S 2 = 10) in GraphSAGE. We implement our method in PyTorch <ref type=\"bibr\" target=\"#b13\">[13]</ref>. For the other methods, we use all the original papers' co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .org/ns/1.0\"><head n=\"1.\">Introduction</head><p>Adversarial attacks to image classification systems <ref type=\"bibr\" target=\"#b19\">[20]</ref> add small perturbations to images that lead these systems . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: diagnoses. Longitudinal-based methods leverage the temporal dependencies among clinical events, see <ref type=\"bibr\" target=\"#b1\">[Choi et al., 2016;</ref><ref type=\"bibr\" target=\"#b6\">Xiao et al., 20 ref type=\"bibr\" target=\"#b2\">[Peters et al., 2018;</ref><ref type=\"bibr\">Radford et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Devlin et al., 2018]</ref> have shown to largely improve the performan (GCN) <ref type=\"bibr\" target=\"#b2\">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref type=\"bibr\" target=\"#b1\">[Gilmer et al., 2017]</ref>, graph attention networks (GAT) <ref type= =\"bibr\">[Velickovic et al., 2017]</ref>. GNNs have already been demonstrated useful on EHR modeling <ref type=\"bibr\" target=\"#b1\">[Choi et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Shang et al., 2 whether one sentence is the next sentence of the other.</p><p>A typical input to BERT is as follows <ref type=\"bibr\" target=\"#b1\">( [Devlin et al., 2018]</ref>):</p><formula xml:id=\"formula_4\">Input = p>It is worth mentioning that our graph embedding method on medical ontology is different from GRAM <ref type=\"bibr\" target=\"#b1\">[Choi et al., 2017]</ref> from the following two aspects: 1. Initializ with weight as 1.1. For deep learning models, we implemented RNN using a gated recurrent unit (GRU) <ref type=\"bibr\" target=\"#b1\">[Cho et al., 2014]</ref> and utilize dropout with a probability of 0.4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /1.0\"><head n=\"3.1.\">Language Model with Penn Treebank</head><p>Dataset and Settings. Penn Treebank <ref type=\"bibr\" target=\"#b30\">(Marcus et al., 1994</ref>) is a well-studied benchmark for language . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el>(15)</label></formula><p>For OntoNotes, we use the same training, development and test splits as <ref type=\"bibr\" target=\"#b1\">(Che et al., 2013)</ref>. For other datasets which have already been s 2</ref> are the results of wordbased models <ref type=\"bibr\" target=\"#b31\">(Wang et al., 2013;</ref><ref type=\"bibr\" target=\"#b1\">Che et al., 2013;</ref><ref type=\"bibr\" target=\"#b34\">Yang et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is critical to realism.</p><p>In contrast to methods that use a parametric model of the human face <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we directly predict the positions of face mesh vertices in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e ensemble approaches <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref> fused different text features and achieved promising result. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uch as bagof-words or n-grams <ref type=\"bibr\" target=\"#b19\">(Wang and Manning, 2012)</ref> or SVMs <ref type=\"bibr\" target=\"#b16\">(Tang et al., 2015)</ref>. The neural network based methods like <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eration for paraphrasing <ref type=\"bibr\" target=\"#b15\">(Liu et al., 2020)</ref> and summa-rization <ref type=\"bibr\" target=\"#b30\">(Schumann et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: wer the expected stochastic gradient variance. As shown in <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, the reduction of variance can lead to faster convergence.  ques, such as stratified sampling <ref type=\"bibr\" target=\"#b34\">[35]</ref> and importance sampling <ref type=\"bibr\" target=\"#b35\">[36]</ref> are proposed to achieve the variance reduction. Different . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. Such a recognition problem is simpler since the text in re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tle as possible. We will elaborate.</p><p>There are previous works that achieve the first condition <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017;</ref><ref type=\"bibr\" target=\"#b17\">Hein &amp; A ef><ref type=\"bibr\" target=\"#b16\">Haber &amp; Ruthotto, 2017)</ref>. For example, Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> bound the Lipschitz constant by requiring   leads to degradation in nominal accuracy, average confidence gap and robustness. Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> can be viewed as models without L c term,   weight matrices and shows its effect in reducing generalization gap. The work on Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> shows that it is possible to control Lipsc ing the first condition, allows greater degrees of freedom in parameter training than the scheme in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>; a new loss function is specially designed   |M i,j | (2)</formula><p>The above is where our linear and convolution layers differ from those in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>: they require W W T to be an identity matri s to be 1; they also propose to restrict aggregation operations. The reported robustness results of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>, however, are much weaker than those by adv  common parameter with (9).</p><p>ResNet-like reconvergence is referred to as aggregation layers in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> and a different formula was used:</p><formu abel></formula><p>) where \u03b1 \u2208 [0, 1] is a trainable parameter. Because splitting is not modified in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>, their scheme may seem approximately equiva  preserve distances. In contrast, because splitting is not modified, at reconvergence the scheme of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> must apply the shrinking factor of 1 \u2212 \u03b1 on ents. We can also have a different t per channel or even per entry.</p><p>To be fair, the scheme of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> has an advantage of being nonexpansive with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Most importantly, there are fast approximations for both PPR <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b75\">76]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b33\">[34]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rotect the proprietary data. However, recent work by Zhu et al., \"Deep Leakage from Gradient\" (DLG) <ref type=\"bibr\" target=\"#b0\">[1]</ref> showed the possibility to steal the private training data fr nables us to always extract the ground-truth labels and significantly simplify the objective of DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> in order to extract good-quality data. Hence, we name our ap extraction with better fidelity.</p><p>\u2022 We empirically demonstrate the advantages of iDLG over DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> via comparing the accuracy of extracted labels and the fidel <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Methodology</head><p>Recent work by Zhu et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> presents an approach (DLG) to steal the proprietary data pro s</head><p>In this section, we empirically demonstrate the advantages of our (iDLG) method over DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We perform experiments on the classification task over thre \" target=\"#b9\">[10]</ref> with 10, 100, and 5749 categories respectively. Following the settings in <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we use the randomly initialized LeNet for all experiments.   is used as the optimizer. For fast training, we resize all images in LFW to 32 \u00d7 32.</p><p>For DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, as described by the authors, we start the procedure with th % 100.0% LFW 79.1% 100.0% Table <ref type=\"table\">1</ref>: Accuracy of the extracted labels for DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> and iDLG. Note that iDLG always extracts the correct label a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> show that GNNs are vulnerable to poisoning attacks which ad get=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. As already noted, such attacks can be (i) node specific, a ) node specific, as in the case of a target evasion attack <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> that is designed to ensure that the GNNs are fooled into mi graph. As shown by <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, both node specific and non-target attacks can be executed  n adversary can attack the GNNs by poisoning the graph data used for training. For example, Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> shows that by adding the adversarial perturbations on the  des in the graph so as to reduce the accuracy of the resulting graph neural networks.</p><p>Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> is one of the first methods that perturbs the graph data t  Baseline Methods. Though there are several adversarial attack algorithms on graphs such as Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> and RL-S2v <ref type=\"bibr\" target=\"#b11\">[12]</ref>, most //www.tei-c.org/ns/1.0\"><head n=\"5.2.1\">Node Classification on Clean</head><p>Graph. As the Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> points out that \"poisoning attacks are in general harder a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by m orthonormal matrices. We refer the readers to <ref type=\"bibr\" target=\"#b24\">(Wong, 1967;</ref><ref type=\"bibr\" target=\"#b0\">Absil et al., 2004)</ref> for details on the Riemannian geometry of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: denoted by source A.</p><p>\u2022 White-box attacks with PGD using the Carlini-Wagner (CW) loss function <ref type=\"bibr\" target=\"#b5\">[6]</ref> (directly optimizing the difference between correct and inco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ;</ref><ref type=\"bibr\">Kitaev et al., 2019, i.a.)</ref>.</p><p>In this context, the GLUE benchmark <ref type=\"bibr\" target=\"#b62\">(Wang et al., 2019a)</ref> has become a prominent evaluation framewor s, we use an average of the metrics. More information on the tasks included in GLUE can be found in <ref type=\"bibr\" target=\"#b62\">Wang et al. (2019a)</ref> and in <ref type=\"bibr\">Warstadt et al. (20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b1\">[2]</ref>, DyninstAPI <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Valgrind <ref type=\"bibr\" target=\"#b7\">[8]</ref> being the most popular. This section focuses on Pin and Dyni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hes have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar ype=\"bibr\" target=\"#b23\">[24]</ref>. The explicit approaches <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar  to solve this problem <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar tly leverage subtopics to determine the diversity of results <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar  target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and supervised approaches such as DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref  xplicit method calculating the distribution by counting the relevant document of the subtopic. DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduces the machine learning method into explicit appro ent \ud835\udc36 to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> score function for the generator. We introduce the score f <p>In the training process, we first train R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref> and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> respectively using MLE loss in both ways. It is because ou br\" target=\"#b5\">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> as supervised baseline methods. Top 20 results of Lemur ar b7\">[8]</ref> as the RNN cell for comparison. In our experiments, we conduct the list-pairwise loss <ref type=\"bibr\" target=\"#b11\">[12]</ref> to train DSSA method. The feature vector 1 http://playbigd DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . QuO's QDLs are based on the separation of concerns advocated by Aspect-Oriented Programming (AoP) <ref type=\"bibr\" target=\"#b22\">[23]</ref>. The QuO middleware adds significant value to adaptive rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: graphs <ref type=\"bibr\" target=\"#b8\">[9]</ref>, prediction of customer types in e-commerce networks <ref type=\"bibr\" target=\"#b5\">[6]</ref>, or the assignment of scientific papers from a citation netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [2]</ref> have recently been shown to be ideal for this task <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. The pronunciation mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as been achieved by using LSTM-CRF models <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b5\">Chiu and Nichol ad><p>We follow the best English NER model <ref type=\"bibr\" target=\"#b15\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Lample et al.,  concatenated as its representation:</p><p>Integrating character representations Both character CNN <ref type=\"bibr\" target=\"#b27\">(Ma and Hovy, 2016)</ref> and LSTM <ref type=\"bibr\" target=\"#b17\">(La. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  presented an intelligent video-based system for automated detection of suicide by hanging attempts <ref type=\"bibr\" target=\"#b1\">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ures</head><p>There have been a large number of works and debates on NIC offloading of TCP features <ref type=\"bibr\" target=\"#b33\">[35,</ref><ref type=\"bibr\" target=\"#b45\">47,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p>How to better leverage word information for Chinese NER has received continued research attention <ref type=\"bibr\" target=\"#b9\">(Gao et al., 2005)</ref>, where segmentation information has been used. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b7\">[8]</ref>.</p><p>Stream buffers <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> introduce small associative structures which track data acc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h node features and graph topological structural information to make predictions. Velickovic et al. <ref type=\"bibr\" target=\"#b28\">[27]</ref> adopt attention mechanism into graph learning, and propose ctions of the contents of a neighborhood or sequence. Therefore, they are adaptive to the contents. <ref type=\"bibr\" target=\"#b28\">[27]</ref> adapts an attention mechanism to graph learning and propos  thus can help stabilize the process, compared with the previously used row normalization as in GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>:</p><formula xml:id=\"formula_4\">E ijp = \u00caijp N j=1 \u00caijp<la ention based EGNN layer</head><p>We describe the attention based EGNN layer. The original GAT model <ref type=\"bibr\" target=\"#b28\">[27]</ref> is only able to handle one dimensional binary edge feature ula xml:id=\"formula_9\">X l\u22121 i\u2022 , X l\u22121 j\u2022</formula><p>and E ijp . In existing attention mechanisms <ref type=\"bibr\" target=\"#b28\">[27]</ref>, the attention coefficient depends on X i\u2022 and X j\u2022 only.  yer. Indeed, the essential difference between GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref> is whether we use the attention coefficients (i.e., matrix The three citation network datasets are also used in <ref type=\"bibr\" target=\"#b33\">[32]</ref> [18] <ref type=\"bibr\" target=\"#b28\">[27]</ref>. However, they all use a pre-processed version which disca class.</p><p>The baseline methods we used are GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>. To investigate the effectivenesses of each components, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#b3\">Collobert et al., 2011)</ref>. They have been subsequently applied to sentence classification <ref type=\"bibr\" target=\"#b11\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b10\">Kalchbrenner et al.,  hich are projected into a high-dimensional space.</p><p>A rather shallow neural net was proposed in <ref type=\"bibr\" target=\"#b11\">(Kim, 2014)</ref>: one convolutional layer (using multiple widths and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  been proposed such as maxpooling <ref type=\"bibr\" target=\"#b34\">[35]</ref> and selective attention <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We follow at-least-one assumption, where a positive examp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in a relatively small proportion of programs' data variables <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, and by selectively protecting these SDC-prone variables, one ref type=\"bibr\" target=\"#b18\">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and allows fault injections to be performed at specific pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, neural networks are increasingly used in the multi-modal domain, on multimodal representations <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b13\">Yang et al. (2017)</ref>  <ref type=\"bibr\" target=\"#b13\">[14]</ref> and <ref type=\"bibr\" target=\"#b12\">Wang et al. (2016)</ref>  <ref type=\"bibr\" target=\"#b12\">[13]</ref> e f type=\"bibr\" target=\"#b13\">[14]</ref> and <ref type=\"bibr\" target=\"#b12\">Wang et al. (2016)</ref>  <ref type=\"bibr\" target=\"#b12\">[13]</ref> employ a two-stage approach,</p><formula xml:id=\"formula_3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ending against the adversarial samples have been analyzed in <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. As stated in our following Theorem 1, the network could be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -order and 2nd-order proximities between vertices to embed homogeneous networks. Specifically, LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref> learns two separated embeddings for 1st-order and 2nd-orde ignal on constructing the bipartite network. Similar to the modeling of 1st-order proximity in LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we model explicit relations by considering the local prox  of vertex sequences. Then the word2vec is applied on the corpus to learn vertex embeddings. \u2022 LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref>: This approach optimizes both the 1st-order and 2nd-order  vec inspire many works <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> to use inner product to model the interaction between two e ural embedding methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, we parameterize the conditional probability P(u c |u i ) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: us with two intermediate minibatches (g k (x), y) and (g k (x ), y ). Third, we perform Input Mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> on these intermediate minibatches. This pr <p>Here, (y, y ) are one-hot labels, and the mixing coefficient \u03bb \u223c Beta(\u03b1, \u03b1) as proposed in mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>. For instance, \u03b1 = 1.0 is equivalent to sa  to substantial improvements, achieving 2.45% test error on CIFAR-10 when combined with Input Mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>. As AgrLearn is complimentary to Input Mix rs: no regularization, AdaMix, Input Mixup, and Manifold Mixup. We follow the training procedure of <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>, which is to use SGD with momentum, a weig e=\"table\">1</ref>: Classification errors on (a) CIFAR-10 and (b) CIFAR-100. We include results from <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> \u2020 and <ref type=\"bibr\" target=\"#b9\">(Guo e  more detail). In the case where S = {0}, Manifold Mixup reduces to the original mixup algorithm of <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>. While one could try to reduce the variance random interpolations between training examples and perform the same interpolation for their labels <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b24\">Tokozume et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[28]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib r model with the other five state-of-the-art approaches( <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib arget=\"#b36\">[37]</ref> uses the combination of GoogleNet feature and 3D-CNN feature. S2VT-rgb-flow <ref type=\"bibr\" target=\"#b29\">[30]</ref> uses the two-stream features consisting of RGB feature ext. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: gment every physical register with a Superseded bit and a Pending count. This support is similar to <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The Superseded bit marks whether the instruction that sup  techniques.</p><p>The second category includes work related to register recycling. Moudgill et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> discuss performing early register recycling in out-of-orde er processors that support precise exceptions. However, the implementation of precise exceptions in <ref type=\"bibr\" target=\"#b16\">[17]</ref> relies on either checkpoint/rollback for every replay even. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: CRF) <ref type=\"bibr\" target=\"#b5\">[6]</ref> , Decision Tree <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> , Support Vector Machine (SVM) <ref type=\"bibr\">[9~12]</ref> . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Ns) <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and U-Net <ref type=\"bibr\" target=\"#b17\">[18]</ref> algorithms have been used.</p><p>Although it is possible f er for upsampling.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. U-Net</head><p>U-Net <ref type=\"bibr\" target=\"#b17\">[18]</ref> is a modified FCN for yielding more precise segmentation.  formation such as boundaries of objects. To overcome this problem, skip connection methods are used <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The features e U-Net. It is important to note that the compared U-Net is not the original architecture proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref> but a highly calibrated model for the enhanced capability . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Some following literature modifies the framework for the purpose of the adversarial training. IRGAN <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2017)</ref>  </p></div> <div xmlns=\"http://www.tei-c.org ive samples iteratively. And to make the generative module aware of relation information, following <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2018a)</ref>, we design a random walk based generating s name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r comparison, which claims to have the highest correlation with perceptual scores for SR evaluation <ref type=\"bibr\" target=\"#b32\">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Sc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  bounding box regression, where a R-CNN is applied several times, to produce better bounding boxes. <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b30\">[31]</ref>. Even a small perturbation on the image can fool a well-trained model. Recent works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the same train/validation/test splits of the same three datasets (CORA, CiteSeer and PubMed) from <ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref>. Such experimental setup favors the model th  consider the problem of semi-supervised transductive node classification in a graph, as defined in <ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref>. In this paper we compare the four following raged over all datasets. See text for the definition. (b) Model accuracy on the Planetoid split from<ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref> and another split on the same datasets. Diff ute the following simple experiment. We run the 4 models on the datasets and respective splits from <ref type=\"bibr\" target=\"#b15\">[Yang et al., 2016]</ref>. As shown in Table <ref type=\"table\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b42\">[43]</ref>, attention prediction <ref type=\"bibr\" target=\"#b44\">[45]</ref>, target tracking <ref type=\"bibr\" target=\"#b62\">[63]</ref>, action recognition <ref type=\"bibr\" target=\"#b45\">[46]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmula></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Related Work</head><p>Early works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [23]</ref> consider network load when allocating VMs in a distributed cloud system. Maguluri et al. <ref type=\"bibr\" target=\"#b23\">[24]</ref> tackle the randomness of arriving workloads and solve opti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: phs. To address this issue, different sampling-based methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amples unique to the input. Inspired by the learning iterative shrinkage and thresholding algorithm <ref type=\"bibr\" target=\"#b4\">[5]</ref>, Cascaded Sparse Coding Network (CSCN) <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: not need any external data. Further, recent studies (e.g., <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b43\">44]</ref>) focus on theoretical certificates for GNN robustness inste. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to query for concepts intersecting two keywords, is a notable contribution to hypothesis generation <ref type=\"bibr\" target=\"#b23\">[27]</ref>. is system, proposed by Liu et al., is similar to both our. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tation models (PLMs) such as ELMo <ref type=\"bibr\" target=\"#b29\">(Peters et al., 2018a)</ref>, BERT <ref type=\"bibr\" target=\"#b9\">(Devlin et al., 2019a)</ref> and XLNet <ref type=\"bibr\" target=\"#b46\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: //www.tei-c.org/ns/1.0\" place=\"foot\" n=\"4\" xml:id=\"foot_1\">PointGLR is a recently published preprint<ref type=\"bibr\" target=\"#b19\">[20]</ref>. The performance of the following models was taken from th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction based models <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b31\">21,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>. Interaction based models thrive with encoding word-word tr to ranking signals <ref type=\"bibr\" target=\"#b20\">[11,</ref><ref type=\"bibr\" target=\"#b22\">13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>. Learned end-to-end from user feedbacks <ref type=\"bibr\" ta t=\"#b39\">29]</ref>. Learned end-to-end from user feedbacks <ref type=\"bibr\" target=\"#b33\">[23,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>, the word embeddings can encode so matches tailored for rel nce than score-based ones like mean-pooling or max-pooling <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>.</p><p>Kernel-pooling is applied to each M h q ,h d matrix  hes are more e ective than weight-summing the similarities <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>-\"similarity does not necessarily mean relevance\" <ref type= lored for relevance ranking, which has signi cant advantages over traditional feature-based methods <ref type=\"bibr\" target=\"#b39\">[29,</ref><ref type=\"bibr\" target=\"#b40\">30]</ref>. ese initial succe d learningto-rank techniques are then used to combine the n-gram somatches to the nal ranking score <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>e CNN is the key to modeling n-grams. Typical IR ap [30]</ref>.</p><p>K-NRM uni ed the progress of IR customized embeddings and interaction based model <ref type=\"bibr\" target=\"#b39\">[29]</ref>. It rst embeds words and builds the translation matrix usi  log, K-NRM outperforms both neural IR methods and feature-based learning-to-rank by a large margin <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>ough the so matching of n-grams in information retr -torank layer to calculate the ranking score using the n-gram translations M. is part extends K-NRM <ref type=\"bibr\" target=\"#b39\">[29]</ref> to n-grams. Kernel-pooling is a pooling technique that use ead><p>Conv-KNRM adds the ability of so matching n-grams to the recent state-of-the-art K-NRM model <ref type=\"bibr\" target=\"#b39\">[29]</ref> with convolutional neural networks (CNNs). Without CNNs, C g Conv-KNRM requires large-scale training data, for example, user clicks in a commercial search log <ref type=\"bibr\" target=\"#b39\">[29]</ref> or industryscale annotations <ref type=\"bibr\" target=\"#b31 raining data. ey are then used in the target domain to generate so -TF features \u03a6(M). Xiong, et al. <ref type=\"bibr\" target=\"#b39\">[29]</ref> showed that kernel-pooled so -TF features reveal di erent  rnel is of low importance in search logs as all candidate documents already contain the query words <ref type=\"bibr\" target=\"#b39\">[29]</ref>; however, synonyms can be a strong signal in a recall-orie Log: Sogou.com is a major Chinese commercial search engine.</p><p>e same se ings as K-NRM were used <ref type=\"bibr\" target=\"#b39\">[29]</ref>. e same sample of Sogou log and training-testing splits ar IR baselines for stronger baseline performance. Body texts of training documents were not available <ref type=\"bibr\" target=\"#b39\">[29]</ref>. e Chinese text was segmented by ICTCLASS <ref type=\"bibr\" ad><p>Training and testing labels on Sogou-Log and Bing-Log were generated following prior research <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> ref type=\"bibr\" target=\"#b33\">[23]</ref>, DRMM <ref type=\"bibr\" target=\"#b22\">[13]</ref>, and K-NRM <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>CDSSM <ref type=\"bibr\" target=\"#b36\">[26]</ref> is  ry and document representations on their words' le er-tri-grams (or Chinese characters in Sogou-Log <ref type=\"bibr\" target=\"#b39\">[29]</ref>). e ranking scores are calculated by the similarity betwee  erwards.</p><p>K-NRM is a state-of-the-art neural model previously tested on the Sogou-Log dataset <ref type=\"bibr\" target=\"#b39\">[29]</ref>. It uses kernel-pooling instead of DRMM's histogram poolin earch logs, 5-fold cross validation were used to be consistent with the previous study on Sogou-Log <ref type=\"bibr\" target=\"#b39\">[29]</ref>. On ClueWeb09-B, the 10-fold cross validation splits from  n Sogou-Log, traditional IR methods used both title and body, and neural IR methods only used title <ref type=\"bibr\" target=\"#b39\">[29]</ref>, as discussed in section 5.1. On Bing-Log, all methods use were all learned end-to-end using the query logs. For Sogou-log, we set embedding dimension L = 300 <ref type=\"bibr\" target=\"#b39\">[29]</ref> . For Bing-Log, we set L = 100 because our pilot study sho ers were: \u00b5 1 = 0.9, \u00b5 2 = 0.7, ..., \u00b5 10 = \u22120.9.</p><p>e \u03c3 of the so match bins were set to be 0.1 <ref type=\"bibr\" target=\"#b39\">[29]</ref>. Model Implementation and E ciency: e model was implemente bout 12 hours on an AWS GPU machine. e training time is similar with prior work using only unigrams <ref type=\"bibr\" target=\"#b39\">[29]</ref>. Most computation time was spent on the embedding layer; t rrent neural IR methods to provide additional improvements <ref type=\"bibr\" target=\"#b32\">[22,</ref><ref type=\"bibr\" target=\"#b39\">29,</ref><ref type=\"bibr\" target=\"#b40\">30]</ref> Comparing the two s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s/1.0\"><head n=\"2.\">PROPOSED APPROACH</head><p>We use a baseline Tacotron architecture specified in <ref type=\"bibr\" target=\"#b7\">[8]</ref>, where we use a GMM attention <ref type=\"bibr\" target=\"#b8\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Radford et al. (2018)</ref> demonstrate a pre-trained generative model (GPT) and its effects, while <ref type=\"bibr\" target=\"#b10\">Devlin et al. (2019b)</ref> release a pre-trained deep Bidirectional  entation from Transformers (BERT), achieving state-of-the-arts on dozens of benchmarks.</p><p>After <ref type=\"bibr\" target=\"#b10\">Devlin et al. (2019b)</ref>, similar pre-trained encoders spring up r  many alternatives for pre-trained language representation can be used, e.g., masked language model <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref>. Note that those two tasks only share th  output at [CLS] is often used as the sentence representation.</p><p>PLM Objective Inspired by BERT <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref>, MLM randomly selects 15% of input token LS] output for sentence-level prediction and the outputs of all tokens for sequence labelling tasks <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref> </p></div> <div xmlns=\"http://www.tei-c.  use the transformer architecture <ref type=\"bibr\" target=\"#b41\">(Vaswani et al., 2017)</ref> as in <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b;</ref><ref type=\"bibr\" target=\"#b20\">Liu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: learn node representations by utilizing information from distant neighbors. GCNs and their variants <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b14\">Veli\u010dkovi sed node classification <ref type=\"bibr\">(Kipf &amp; Welling, 2017)</ref>, inductive node embedding <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>, link prediction <ref type=\"bibr\" targe >Implementation Details</head><p>Training with the CV estimator is similar as with the NS estimator <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>. Particularly, each iteration of the al r all the other multi-class datasets. The model is GCN for the former 4 datasets and GraphSAGE-mean <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref> for the latter 2 datasets, see Appendix orted by <ref type=\"bibr\" target=\"#b2\">Chen et al. (2018)</ref>, while their NS baseline, GraphSAGE <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>, does not implement the preprocessing t sets because of their slow convergence and the requirement to fit the entire dataset in GPU memory. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> make an initial attempt to develop stoch www.tei-c.org/ns/1.0\"><head n=\"2.3.\">Neighbor Sampling</head><p>To reduce the receptive field size, <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> propose a neighbor sampling (NS) algorit id=\"formula_8\">P (l) uv = n(u) D (l) P uv if v \u2208 n(l) (u)</formula><p>, and P (l) uv = 0 otherwise. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> propose to perform an approximate forwar D (l) needs to be large for NS, to keep comparable predictive performance with the exact algorithm. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> choose D (1) = 10 and D (2) = 25, and th r, Cora, PubMed and NELL from <ref type=\"bibr\">Kipf &amp; Welling (2017)</ref> and Reddit, PPI from <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref>, with the same train / validation / test ted to the task nor the model. Our algorithm is applicable to other models including GraphSAGE-mean <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a</ref>) and graph attention networks (GAT) <ref e most GCNs only have two graph convolution layers <ref type=\"bibr\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b4\">Hamilton et al., 2017a)</ref>, this gives a significant reduction of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oustic model with a \"bottleneck\" layer using a frame based criterion on a large multilingual corpus <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target ilingual models can be adapted to the specific language to improve performance further. The work by <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> presented bottleneck ific softmax layers, which are trained using cross-entropy <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. This architecture ca ) if X \u2208 XL1 softmax(WL2e + bL2) if X \u2208 XL2 . . . softmax(WLne + bLn) if X \u2208 XLn</formula><p>Unlike <ref type=\"bibr\" target=\"#b4\">[5]</ref>, we do not have any bottleneck layer, and the whole model is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" targ for experiments: \u2022 AMiner-AND<ref type=\"foot\" target=\"#foot_0\">1</ref> . The dataset is released by <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, which contains 500 author names for traini (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s messages. In contrast to attention mechanisms (e.g., GAT <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>), GNNGUARD determines importance weights using theory of ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, Blind-Dehazing <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and more. While such unsupervised methods can exploit image. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>] in the image. However, little work on adversarial examples  ance image regions <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>]. All of these approaches share a common challenge: They hav get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, has focused on hiding perturbations in image regions with  lowing recent work <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b55\">56,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>, we conduct our experiments on the development set (1000 RG. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b9\">Gong et al., 2018;</ref><ref type=\"bibr\" target=\"#b24\">Tay et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Ghaeini et al., 2018)</ref>. It is intuitive to transfer NLI models in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b19\">18]</ref> have been widely adopted to extract useful structural infor oss various domains, such as neuroscience <ref type=\"bibr\" target=\"#b31\">[30]</ref>, bioinformatics <ref type=\"bibr\" target=\"#b19\">[18]</ref>, and information networks <ref type=\"bibr\" target=\"#b6\">[5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: independence <ref type=\"bibr\" target=\"#b22\">[23]</ref> or by approximating interaction similarities <ref type=\"bibr\" target=\"#b16\">[17]</ref>. When a large number of pre-trained Transformer layers is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref> and the trace cache architecture as proposed by Rotenberg, h prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref>. The branch prediction mechanism is a fully autonomous eng ion cache is then driven by the requests stored in the FTQ.</p><p>Another important contribution of <ref type=\"bibr\" target=\"#b29\">[30]</ref> is the Fetch Target Buffer (FTB). It extends the BTB by al ssibly containing multiple basic blocks.</p><p>The use of an FTQ is not novel, it was introduced in <ref type=\"bibr\" target=\"#b29\">[30]</ref>. It decouples the branch prediction from the memory access .0\"><head n=\"3.3.\">Fetch target queue</head><p>Following the proposal of Reinman, Austin and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref> we have decoupled the branch prediction stage from the ins ream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture <ref type=\"bibr\" target=\"#b29\">[30]</ref> using a perceptron branch predictor <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: models by fine-tuning pre-trained LMs in a simpler architecture.</p><p>Pre-trained LMs such as BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref> and GPT-2 <ref type=\"bibr\" target=\"#b40\">[41]</ref> have d task. See Appendix A for the model architecture. In D , we fine-tune the popular 12layer BERT model <ref type=\"bibr\" target=\"#b12\">[13]</ref>, RoBERTa <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and a  currently support 4 pre-trained models: Distil-BERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, RoBERTa <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and XL Figure <ref type=\"figure\">6</ref> shows the model architecture of D 's language models such as BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DistilBERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, and  best performing pre-trained model among DistilBERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, XLNet <ref type=\"bibr\" target=\"#b60\">[61]</ref>, and RoBE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  consider the degrees of importance of items based on their rankings.</p><p>Recently, Tang and Wang <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposed a KD model to address the ranking problem, called ledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> and present rank distillation (RD) <ref type=\"bibr\" target=\"#b10\">[11]</ref> that applies knowledge distillation to recommender models. ommendation problem because Fig. <ref type=\"figure\">1</ref>. Illustration of rank distillation (RD) <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The teacher model transfers manipulated top-k items as th t may have worse performance than the original student model. Rank distillation (RD). Tang and Wang <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposed ranking distillation (RD) that applies KD for ran e rated by less than 5 users. Table I reports the detailed statistics of these datasets.</p><p>\u2022 RD <ref type=\"bibr\" target=\"#b10\">[11]</ref>: To define the KD loss in equation ( <ref type=\"formula\" t een \u03bb that appears in RD and CD. Specifically, we used the following parameter settings.</p><p>\u2022 RD <ref type=\"bibr\" target=\"#b10\">[11]</ref> and RD-Rank: We set \u03c1 to be 0.5. For CDAE, the number of i er, we used the public PyTorch implementation <ref type=\"foot\" target=\"#foot_5\">6</ref> provided in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. All experiments were conducted on a desktop with 128 GB m KD. Also, the gain indicates how additional accuracy achieved by the proposed model over that of RD <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>Based on this evaluation, we found several interest rform RD over all datasets. Note that the improvement gap for RD is somewhat different from that in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. It is because we used leave-one-out evaluation while <ref  in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. It is because we used leave-one-out evaluation while <ref type=\"bibr\" target=\"#b10\">[11]</ref> used cross-validation evaluation. Our models are consisten el size and efficiency. The model size is proportional to the accuracy of our model, as observed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> as well. The same tendency consistently holds in different oot\" n=\"4\" xml:id=\"foot_3\">http://dawenl.github.io/data/gowalla pro.zip Competitive models. Since RD<ref type=\"bibr\" target=\"#b10\">[11]</ref> is the state-of-the-art KD model for top-N recommendation,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l correlation similar to prior studies of spatial footprints <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. We define a spatial region as a fixed-size portion of the  with the code and/or data address that initiates the pattern <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Whereas existing spatial pattern prefetching designs are e ow that the cache-coupled structures used in previous work ( <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>) are suboptimal for observing spatial correlation. Accesses rate predictions when correlation table storage is unbounded <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. By combining both quantities, which we call PC+address ind pproximated by combining the PC with a spatial region offset <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. The spatial region offset of a data address is the distanc \"4.2.\">Indexing</head><p>Prior studies of spatial predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> advocate predictor indices that include address information tries, consequently polluting the PHT.</p><p>Past predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> couple the predictor training structure to a sectored (i.e. y practical implementation evaluated on server workloads provides less than 20% miss rate reduction <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>In this paper, we reconsider prediction and streami is work demonstrates: \u2022 Effective spatial correlation and prediction. Contrary to previous findings <ref type=\"bibr\" target=\"#b16\">[17]</ref>, address-based correlation is not needed to predict the ac region generation is defined can significantly impact the accuracy and coverage of spatial patterns <ref type=\"bibr\" target=\"#b16\">[17]</ref>. A generation must be defined to ensure that, when SMS str t distinguish among distinct access patterns to different data structures by the same code (e.g.,   <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which indicated that PC+address provides superior coverag  experience worse conflict behavior. To mitigate this disadvantage, the spatial footprint predictor <ref type=\"bibr\" target=\"#b16\">[17]</ref> employed a decoupled sectored cache <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -of-the-art adversarially trained models as remote targets <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, since adversarial training is arguably the most promising . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s on Graph Convolutional Networks (GCNs) <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b8\">Gao et al., 201  large graphs, layer sampling techniques <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b26\">Ying et al., 2 ly a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b14\">Huang et  he above edge sampler to perform layer sampling. Under the independent layer sampling assumption of <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref>, one would sample a connection u ( ) , v ( + b3\">Chen et al. (2018a)</ref>. Point (2) is due to the better interlayer connectivity compared with <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref>, and unbiased minibatch estimator compared w s to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b)</ref> performs sampling from another perspective. , 2016)</ref>, 2. GraphSAGE <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017)</ref>, 3. FastGCN <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b)</ref>, 4. S-GCN <ref type=\"bibr\" target=\"#b3\">(Ch of each layer independently. This is similar to the treatment of layers independently by prior work <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., ip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mentioning branch predictor warmup is by Haskins and Conte <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> in which they propose memory reference reuse latency (MRRL) mup length per sampling unit. (Note that the MRRL approach <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> corresponds to a zero BHM history length.) We further obser. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: CLS]\" hidden state to ranking score. Pairwise loss is used to optimize the ranking model. Some work <ref type=\"bibr\" target=\"#b32\">(Zhao et al., 2020;</ref><ref type=\"bibr\" target=\"#b29\">Ye et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  limitations of deep SISR. SISR performance was boosted right after the non-local attention modules <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targ   <ref type=\"bibr\" target=\"#b19\">[20]</ref>, RNAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> and SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, incorporate non-local operation into their networks in orde o the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from <ref type=\"bibr\" target=\"#b1\">[2]</ref> and a deconvolution layer for upscaling the module outputs.  ution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we divide the feature maps into region grids, where the int N <ref type=\"bibr\" target=\"#b17\">[18]</ref>, OISR <ref type=\"bibr\" target=\"#b11\">[12]</ref> and SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Quantitative Evaluations In Table <ref type=\"table\" t f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours Urban100 (4\u00d7): img 078 HR Bicubic LapSRN <ref type=\"bib f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours Urban100 (4\u00d7): img 047 HR Bicubic LapSRN <ref type=\"bib f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours which only needs 20% parameters of RCAN and SAN, but ac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the 2.5D technique for sparse matrices, both for square and rectangular grids. Koanantakool et al. <ref type=\"bibr\" target=\"#b37\">[37]</ref> observed that for sparse-dense MMM, 1.5D decomposition per. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ut <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>When the graph structure of the input is known, <ref type=\"bibr\" target=\"#b1\">[2]</ref> introduced a model to generalize ConvNets using low learning r parameters. Our main contributions can be summarized as follows:</p><p>\u2022 We extend the ideas from <ref type=\"bibr\" target=\"#b1\">[2]</ref> to large-scale classification problems, specifically Imagene vised fashion. However, it does not attempt to exploit any weight-sharing strategy.</p><p>Recently, <ref type=\"bibr\" target=\"#b1\">[2]</ref> proposed a generalization of convolutions to graphs via the  v xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Spectral Networks</head><p>Our work builds upon <ref type=\"bibr\" target=\"#b1\">[2]</ref> which introduced spectral networks. We recall the definition mula_4\">\u2202 k x(\u03be) \u2202\u03be k \u2264 C |u| k |x(u)|du ,</formula><p>where x(\u03be) is the Fourier transform of x. In <ref type=\"bibr\" target=\"#b1\">[2]</ref> it was suggested to use the same principle in a general grap ction</head><p>Whereas some recognition tasks in non-Euclidean domains, such as those considered in <ref type=\"bibr\" target=\"#b1\">[2]</ref> or <ref type=\"bibr\" target=\"#b11\">[12]</ref>, might have a p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: variants of the architecture have been applied to parsing and other tasks requiring tree structures <ref type=\"bibr\" target=\"#b2\">(Blunsom et al., 2014)</ref>. However, the effectiveness of character-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion as suggested by Velickovic et al. <ref type=\"bibr\" target=\"#b48\">[49]</ref> and Vaswani et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref>. The multi-head attention mechanism performs K independent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bination of bidirectional LSTM and CTC has been applied to characterlevel speech recognition before <ref type=\"bibr\" target=\"#b5\">(Eyben et al., 2009)</ref>, however the relatively shallow architectur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: vskaya et al., 2014)</ref> uses a classifier-based approach, which is improved by the latter system <ref type=\"bibr\" target=\"#b44\">(Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based \"bibr\" target=\"#b50\">(Susanto et al., 2014;</ref><ref type=\"bibr\">Chollampatt et al., 2016b,a;</ref><ref type=\"bibr\" target=\"#b44\">Rozovskaya and Roth, 2016;</ref><ref type=\"bibr\" target=\"#b26\">Junczy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  trained GCN-CF model into a binarized model (DGCN-BinCF) with knowledge distillation technique (KD <ref type=\"bibr\" target=\"#b7\">[Hinton et al., 2015]</ref>). To be more specific, we introduce a nove e distance one. So it is easy to fall into local optima.</p><p>2.3 Distilling Knowledge for Ranking <ref type=\"bibr\" target=\"#b7\">[Hinton et al., 2015]</ref> was the first one that proposed method \"Kn utions via softmax function, and utilize cross entropy for penalizing the discrepancy. According to <ref type=\"bibr\" target=\"#b7\">[Hinton et al., 2015]</ref>, combining L Bin\u2212CF with L rank as a multi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is gradient-free model-based optimization <ref type=\"bibr\" target=\"#b34\">(Snoek et al., 2012;</ref><ref type=\"bibr\" target=\"#b6\">Bergstra et al., 2011;</ref><ref type=\"bibr\">2013;</ref><ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead n=\"4.\">Experiments</head><p>For the experiments, we follow the protocol used in previous papers <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta  (2) lifted structured embedding <ref type=\"bibr\" target=\"#b20\">[21]</ref>, (3) N-pairs metric loss <ref type=\"bibr\" target=\"#b19\">[20]</ref>, (4) clustering <ref type=\"bibr\" target=\"#b14\">[15]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he transferability <ref type=\"bibr\" target=\"#b11\">[10,</ref><ref type=\"bibr\" target=\"#b20\">19,</ref><ref type=\"bibr\" target=\"#b8\">7]</ref>, an adversarial example is usually generated for a single inp box attacks than FGSM at the cost of worse transferability <ref type=\"bibr\" target=\"#b17\">[16,</ref><ref type=\"bibr\" target=\"#b8\">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) =\"#b20\">[19]</ref> of adversarial examples can be used to attack a black-box model. Several methods <ref type=\"bibr\" target=\"#b8\">[7,</ref><ref type=\"bibr\" target=\"#b38\">37]</ref> have been proposed t rg/ns/1.0\"><head n=\"3.2.\">Translation-Invariant Attack Method</head><p>Although many attack methods <ref type=\"bibr\" target=\"#b8\">[7,</ref><ref type=\"bibr\" target=\"#b38\">37]</ref> can generate adversa ref type=\"bibr\" target=\"#b8\">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type=\"bibr\" target=\"#b8\">[7]</ref> proposes to improve the transferability of adversarial examp formula_13\">)</formula><p>The translation-invariant method can be similarly integrated into MI-FGSM <ref type=\"bibr\" target=\"#b8\">[7]</ref> and DIM <ref type=\"bibr\" target=\"#b38\">[37]</ref> as TI-MI-F ) <ref type=\"bibr\" target=\"#b11\">[10]</ref>, momentum iterative fast gradient sign method (MI-FGSM) <ref type=\"bibr\" target=\"#b8\">[7]</ref>, and diverse inputs method (DIM) <ref type=\"bibr\" target=\"#b target=\"#b6\">[5]</ref> since that they are not good at generating transferable adversarial examples <ref type=\"bibr\" target=\"#b8\">[7]</ref>. We denote the attacks combined with our translation-invaria  more likely to transfer to another black-box model.</p><p>We adopt the ensemble method proposed in <ref type=\"bibr\" target=\"#b8\">[7]</ref>, which fuses the logit activations of different models. We a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rent MegaPipe design naturally fits event-driven servers based on callback or event-loop mechanisms <ref type=\"bibr\" target=\"#b28\">[32,</ref><ref type=\"bibr\" target=\"#b36\">40]</ref>. We mostly focus o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e with maximum-likelihood training, motivating objectives that operate on model-generated sequences <ref type=\"bibr\" target=\"#b3\">(Daum\u00e9 et al., 2009;</ref><ref type=\"bibr\" target=\"#b21\">Ross et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: history in the literature. Recent research focuses on refining the two-level scheme of Yeh and Patt <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In this scheme, a pattern history table (PHT) of two-bit . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, computer vision <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and weakly-supervised learning <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ustness for classical neural networks/robust training (e.g. <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>), we tackle various ertifiable robustness <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> providing guarantee ginal sample measured by, e.g., the infinity-norm or L2-norm <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, often e.g. \u03f5 &lt;  /p><p>For this work, specifically the class of methods based on convex relaxations are of relevance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. They construct a   the remaining layers, since the input to them is no longer binary, we adapt the bounds proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Generalized to the GNN we therefore obtain:</p><formula x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et al. (2016)</ref>; Minmax robust training <ref type=\"bibr\" target=\"#b4\">(Madry et al., 2018;</ref><ref type=\"bibr\" target=\"#b10\">Sinha et al., 2018)</ref>; Input transformation <ref type=\"bibr\" targ  certifiable adversarial-robust classifiers <ref type=\"bibr\" target=\"#b4\">(Madry et al., 2018;</ref><ref type=\"bibr\" target=\"#b10\">Sinha et al., 2018)</ref>, the robustness is achieved only for the pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tation tasks before the DL revolution, including amplitude segmentation based on histogram features <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the region based segmentation method <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: read. Helper threads <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> also utilize parallel hardware for speedup, without actuall e similar structures in the capture stage of our migration system.</p><p>Speculative Precomputation <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> targets memory ins. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ehring et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Kalchbrenner et al., 2016)</ref>, attention <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref>, or a combination of recurrence and atte ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Transformer</head><p>The Transformer <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref> employs an encoder-decoder structure, co  for all sequences, heads, and positions in a batch using parallel matrix multiplication operations <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref>. Without relative position representatio d><p>We compared our model using only relative position representations to the baseline Transformer <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref> with sinusoidal position encodings. We g e a deterministic function of position <ref type=\"bibr\" target=\"#b7\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b10\">Vaswani et al., 2017)</ref> or learned representations. Convolutional se in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as <ref type=\"bibr\" target=\"#b10\">Vaswani et al. (2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o  1 = 0.9, \u03b2 2 = 0.98, and = 10 \u22129 . We used the same warmup and decay strategy for learning rate as <ref type=\"bibr\" target=\"#b10\">Vaswani et al. (2017)</ref>, with 4,000 warmup steps. During training. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b49\">(Rogers et al., 2020)</ref>, which specifically studies the BERT model <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>.</p><p>In this work, we adapt and extend   In NLP, transformers are the backbone of state-of-the-art pre-trained language models such as BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>. BERTology focuses on interpreting what t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on technique.</p><p>Increasing model capacity. Prior works <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref> have experimentally confirmed the resistance improvement agai >Analysis</head><p>Effect of Network Width. In prior works <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, enhance the capacity of target DNN via increasing the layer  sion, similar to the observations from adversarial example <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, increasing the network capacity by a large amount will enhan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -to-video generation <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> to text-to-video generation <ref type=\"bibr\" target=\"#b22\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: asy yet accurate way of taking into account batches in the packet processing in two existing models <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref>. Note that i ef type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref>. Note that in <ref type=\"bibr\" target=\"#b11\">[11]</ref> we came up with a first modeling framework that does not a ODELS WITHOUT BATCH SERVICE</head><p>We start this section by reviewing two of our previous models, <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref> that work wi \"http://www.tei-c.org/ns/1.0\"><head>B. Model for a negligible switch-over time: Model 1</head><p>In <ref type=\"bibr\" target=\"#b11\">[11]</ref> it is assumed that the switch-over time is negligible, whi te with each queue i of the decomposed model, the continuous-time Markov chain depicted in Figure 5 <ref type=\"bibr\" target=\"#b11\">[11]</ref>. A state (k, P ) of this chain, k = 1..., K, corresponds t lution of all Markov chains, the model relies on a fixed-point iterative technique, as developed in <ref type=\"bibr\" target=\"#b11\">[11]</ref>.</p><p>3) Performance parameters: Once all Markov chains ( d><p>The model presented in <ref type=\"bibr\" target=\"#b12\">[12]</ref> is an extension of the one in <ref type=\"bibr\" target=\"#b11\">[11]</ref> that takes into account the switch-over times, and thus co 1 in the case without batches (M = 1), i.e., T M + T R . Let us consider an example (extracted from <ref type=\"bibr\" target=\"#b11\">[11]</ref>) of a virtual switch comprising N = 4 input ports served b ng to standard operational modes. This is exactly what we propose to do in this paper: using models <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref> (presented i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ntiable doubly-linked lists and stacks <ref type=\"bibr\" target=\"#b14\">[15]</ref>, queues and deques <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Different from exploring various forms of dynamic storage. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: plemented in various ways: lightweight userlevel threading <ref type=\"bibr\" target=\"#b19\">[23,</ref><ref type=\"bibr\" target=\"#b35\">39]</ref>, closures or coroutines <ref type=\"bibr\" target=\"#b1\">[4,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e unnormalized arrival-departure random variables are likely best captured by Poisson distributions <ref type=\"bibr\" target=\"#b24\">[25]</ref>- <ref type=\"bibr\" target=\"#b26\">[27]</ref>, we find the cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dapts to changing program requirements in order to achieve better power/performance characteristics <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Similarly, on th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b0\">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type=\"bibr\" target=\"#b7\">[8]</ref>, CIFAR-100 <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and LFW . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ne for person name recognition and the other for publication recognition. Specifically, we use LSTM <ref type=\"bibr\" target=\"#b6\">[7]</ref> as the RNN unit:</p><formula xml:id=\"formula_0\">N = LST M (S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  . , h T \u2032 ) (T \u2032 \u2264 T ), interleaved with subsampling layers to reduce the computational complexity <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The decoder network generates a probability distribution . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  be developed based on an encoder-attentiondecoder framework <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>, where the encoder . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: alistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27]  and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p></div> \t\t\t</abstract> \t\t</profileDesc> \t</teiHeader> \t e instead of pixel space. Ledig et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Sajjadi et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> further propose adversarial loss to encourage the network  7]</ref> introduce an adversarial loss, generating images with more natural details. Sajjadi et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> develop a similar approach and further explore the local t pe=\"figure\">5</ref>. GAN-based methods (SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref>, EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref> and ours) clearly outperform PSNR-oriented approaches in t ef>, and GAN-based methods, such as SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and En-hanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. More results are provided in the supplementary material.  -GAN and the counterpart generated by SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> or EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. The users were asked to pick the image with more natural  on, our method is ranked higher than SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>, especially in building, animal, and grass categories. Com r studies, comparing our method with SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Second row: our methods produce visual results that are r ser studies, comparing our method with SRGAN<ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet<ref type=\"bibr\" target=\"#b37\">[38]</ref>. Second row: our methods produce visual results that are r ref type=\"bibr\" target=\"#b2\">3]</ref> and adversarial loss <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> are introduced to solve the regression-to-the-mean problem  ur framework is based on adversarial learning, inspired by <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Specifically, it consists of one generator G \u03b8 and one dis d n=\"3.3.\">Loss Function</head><p>We draw inspiration from <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and apply perceptual loss and adversarial loss in our model leasing textures, outperforming previous GAN-based methods <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>.</p><p>Our work currently focuses on SR of outdoor scenes. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: echniques to mitigate this issue include multi-task learning <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and pre-trained components <ref type=\"bibr\" target=\"#b10\">[1  both fully supervised data and also weakly supervised data, <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> use multi-task learning to train the ST model jointly with t g and multi-task learning as proposed in previous literature <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sses, respectively. We conduct empirical studies on the CIFAR-10 dataset with ResNet-32, ResNet-110 <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and DenseNet-40-12 <ref type=\"bibr\" target=\"#b10\">[11]</ref ve learning helps improve the performance of ResNet-50 network.</p><p>As following the notations in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we consider two heads sharing ILRs up to \"conv3_x\" block fo ><figDesc>Figure 4: Per-layer weight distribution in trained ResNet-50.As following the notations in<ref type=\"bibr\" target=\"#b8\">[9]</ref>, the two split points in the hierarchical sharing with four . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmance degradation as we will show later. Schlichtkrull et al. also propose using GCNs to model KGs <ref type=\"bibr\" target=\"#b16\">[17]</ref>, but not for the purpose of recommendations.</p></div> <di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  sampled sizes for each layer (S 1 = 25, S 2 = 10) in GraphSAGE. We implement our method in PyTorch <ref type=\"bibr\" target=\"#b13\">[13]</ref>. For the other methods, we use all the original papers' co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ate to handle multi-relation QA due to the lack of reasoning ability.</p><p>Recent reasoning models <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016;</ref><ref type=\"bibr\" target=\"#b26\">Wang et al. manner during reasoning. MemNN <ref type=\"bibr\" target=\"#b27\">(Weston et al., 2015)</ref>, KVMemN2N <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016)</ref> and EviNet <ref type=\"bibr\" target=\"#b21\" re the settings are the same as <ref type=\"bibr\" target=\"#b8\">(Bordes et al., 2015)</ref>. KVMemN2N <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016)</ref> improves the MemN2N for KBQA as it divide. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b15\">[16]</ref> introduced the residual block and the adversarial learning <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> to make the generated . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b33\">[25]</ref>. The system extracts researchers' pro les automatically from the Web <ref type=\"bibr\" target=\"#b32\">[24]</ref> and integrates them with published papers after name disam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on. The frame sharpness is evaluated using the cumulative probability blur detection (CPBD) measure <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which determines blur based on the presence of edges in t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rogeneous textrich network. Meta-paths <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b19\">18]</ref> have been widely ado b31\">[30]</ref>, bioinformatics <ref type=\"bibr\" target=\"#b19\">[18]</ref>, and information networks <ref type=\"bibr\" target=\"#b6\">[5]</ref>. In the context of heterogeneous information networks, netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, taking into account additional images <ref type=\"bibr\" target=\"#b32\">[33]</ref> or 3D scans <ref type=\"bibr\" target=\"#b3\">[4]</ref>, or by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w-resource settings <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p></div> <div x  can improve ASR performance in an under-resourced scenario<ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. As we only have less than two hours of transcribed Somali . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the short-term pattern of nodes, we apply the contextual attention-based model which is inspired by <ref type=\"bibr\" target=\"#b3\">[Liu et al., 2017]</ref> and proposed by <ref type=\"bibr\" target=\"#b1\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: form single image super-resolution (SISR), and significant improvements over shallow CNN structures <ref type=\"bibr\" target=\"#b1\">[2]</ref> have been observed. One benefit from using deeper networks i [4]</ref>, random forest <ref type=\"bibr\" target=\"#b19\">[20]</ref> and convolutional neural network <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Among them, the CNN-based approaches <ref type=\"bibr\" targe ef> have recently set state of the art for SISR. A network with three layers was first developed in <ref type=\"bibr\" target=\"#b1\">[2]</ref> to learn an end-to-end mapping for SR. Subsequently, a deep  tei-c.org/ns/1.0\"><head n=\"3.2.\">Deconvolution layers</head><p>In previous SR methods such as SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> and VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, bicubic  using other SISR methods, including bicubic, Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref>, SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> and DRCN <re datasets. On average, an increase of about 1.0 dB using the proposed method was achieved over SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR < llowing us to train very deep Dataset Bicubic Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref> SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> DRCN <ref typ formation and gradient through the network, making it easy to train. In addition, in previous works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only high-level fea  also to improve the reconstruction performance. Instead of using interpolation for upscaling as in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, recent studies <ref different types of network structures were studied and compared in our work. As in previous methods <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only the feature ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: system through iterative backtranslation <ref type=\"bibr\" target=\"#b15\">(Lample et al., 2018b;</ref><ref type=\"bibr\" target=\"#b2\">Artetxe et al., 2018b)</ref>.</p><p>In this paper, we develop a more p  more suitable for this problem, and <ref type=\"bibr\" target=\"#b15\">Lample et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> adapted the same principles discussed abo back-translation <ref type=\"bibr\" target=\"#b25\">(Sennrich et al., 2016)</ref> which, in the case of <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>, is preceded by an unsupervised tuning st 1.0\"><head n=\"3.1\">Initial phrase-table</head><p>So as to build our initial phrase-table, we follow <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> and learn n-gram embeddings for each lang it, and taking the product of their respective translation probabilities. The reader is referred to <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> for more details.</p></div> <div xmlns=\"h  well with test performance. Unfortunately, neither of the existing unsupervised SMT systems do so: <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> use a heuristic that builds two initial m 4</ref> shows some translation examples from our proposed system in comparison to those reported by <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>. We choose the exact same sentences repor pe=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>. We choose the exact same sentences reported by <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>, which were randomly taken from newstest2 supervised machine translation can be a usable alternative in practical settings.</p><p>Compared to <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref>, our translations are generally more flue at they are produced by an NMT system rather than an SMT system. In addition to that, the system of <ref type=\"bibr\" target=\"#b2\">Artetxe et al. (2018b)</ref> has some adequacy issues when translating l information alone, yielding to translation errors like \"Sunday Telegraph\" \u2192 \"The Times of London\" <ref type=\"bibr\" target=\"#b2\">(Artetxe et al., 2018b)</ref>. So as to overcome this issue, we propos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (category id, brand id and seller id etc.) that have been proved to be useful for cold-start items <ref type=\"bibr\" target=\"#b23\">[24]</ref> from F i , corresponding embeddings are further passed thr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hod for a special type of graphs (i.e., knowledge graph).</p><p>Our method also connects to PinSage <ref type=\"bibr\" target=\"#b20\">[21]</ref> and GAT <ref type=\"bibr\" target=\"#b14\">[15]</ref>. But not. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es a relational graph convolutional network to link prediction task and entity classification task. <ref type=\"bibr\" target=\"#b35\">[36]</ref> propose a heterogeneous graph neural network model which c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [6]</ref>. We omit recently proposed extensions such as for example the use of residual connections <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, dense connections <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ei-c.org/ns/1.0\"><head n=\"4.1\">Stressor Event and Subject Dictionaries</head><p>The word embeddings <ref type=\"bibr\" target=\"#b8\">[Mikolov et al., 2013]</ref> have been found effective in estimating t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e linked together. <ref type=\"bibr\" target=\"#b36\">[37]</ref> proposes a method using self-attention <ref type=\"bibr\" target=\"#b35\">[36]</ref> and bi-affine scoring algorithm to predict biological rela 14\">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Attention. We adapt Transformer <ref type=\"bibr\" target=\"#b35\">[36]</ref> to encode word sequences in a paragraph, where we calculat across sentences. We base on recent Transformer architecture <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> to build this module, due to its better performance in enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  for other network analysis tasks, such as link prediction <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and graph classification <ref type=\"bibr\" target=\"#b16\">[17. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ponding trailers instead of the full-length videos from Youtube 5 . We use the pre-trained ResNet50 <ref type=\"bibr\" target=\"#b15\">[16]</ref> models to extract the visual features from key frames extr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ual acoustic modeling by the contextdependent deep neural network hidden Markov models (CD-DNN-HMM) <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The hidden layers of DNN in CD-DNN-HMM can be thought of co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Hassani, 2019)</ref>. For an overview see <ref type=\"bibr\" target=\"#b69\">(Zhang et al., 2020;</ref><ref type=\"bibr\" target=\"#b61\">Wu et al., 2020)</ref>.</p><p>GNNs mostly require task-dependent labe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re network-depth-features. We evaluate ResNet <ref type=\"bibr\" target=\"#b20\">[15]</ref> and ResNeXt <ref type=\"bibr\" target=\"#b40\">[35]</ref> networks of depth 50 or 101 layers. The original implement \"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"3\" xml:id=\"foot_2\">We use the 64\u00d74d variant of ResNeXt<ref type=\"bibr\" target=\"#b40\">[35]</ref>.</note> \t\t</body> \t\t<back> \t\t\t<div type=\"annex\"> <div xmln. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar algorithm design, our work is most closely related to Hamilton et al. (2017a)'s GraphSAGE algorithm <ref type=\"bibr\" target=\"#b17\">[18]</ref> and the closely related follow-up work of <ref type=\"bibr\"  nodes to aggregate from allows us to control the memory footprint of the algorithm during training <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Second, it allows Algorithm 1 to take into account the im ean); \u2022 mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. \u2022 mean-pooling-hard is the same as mean-pooling, except t ing and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref>-other variants (e.g., based on Kipf et al. <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: measured in terms of Word Error Rate (WER) and translation performance is measured in terms of BLEU <ref type=\"bibr\" target=\"#b29\">[30]</ref> scores, both on case and punctuation sensitive reference t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: been applied in the transductive setting with fixed graphs <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this work we both extend GCNs to the task of inductive  aph convolutional network (GCN), introduced by Kipf et al. <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN algorithm <ref type=\"bibr\" target=\"#b16\"> n O(|V|), so this requirement is not entirely unreasonable <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>.</p><p>Following Theorem 1, we let x v \u2208 U, \u2200v \u2208 V denote t  trained on a single, fixed graph. (That said, Kipf et al <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref> found that GCN-based approach consistently outperformed De. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uto-encoder. Most recently, a Deep Structured Semantic Models (DSSM) for Web search was proposed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, which is reported to outperform significantly semantic hash in an input word sequence into a feature vector using the technique called word hashing proposed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>. For example, the word is represented by a count vector of i hastic gradient ascent. Learning of the C-DSSM is similar to that of learning the DSSM described in <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t taken by a student on a particular learning task, predict aspects of their next interaction x t+1 <ref type=\"bibr\" target=\"#b4\">[6]</ref>.</p><p>In the most ubiquitous instantiation of knowledge tra f binary variables, each of which represents understanding or non-understanding of a single concept <ref type=\"bibr\" target=\"#b4\">[6]</ref>. A Hidden Markov Model (HMM) is used to update the probabili. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ble to DT-ORS-Net applications. To this end, the two-player Markov stopping game (MSG) developed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> can serve as a good starting point, which extends the clas e theoretic setting so as to handle the potential conflicts between the two players.</p><p>However, <ref type=\"bibr\" target=\"#b16\">[17]</ref> does not provide a systematic method to deal with a genera  the optimal strategy for each player in such situations, the two-player MSG framework developed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> may serve as a basis. Particularly, in the two-player MSG, yer. However, a systematic method for handling a general number of players in a MSG is missing from <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Considering this, a general M-MSG is proposed in the next  needed. Particularly, when two players coexist in the MSG, the randomized stopping time is used in <ref type=\"bibr\" target=\"#b16\">[17]</ref> to deal with the potential competition from the other play ynamism in the number of players as the game evolving, the concept of selection time is proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, constructing the selection time essentially requ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a vanilla and Siamese architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Each system is evaluated under specific configurations rega vych <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed to combine BERT with a Siamese architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref> for semantic representations of sentences and their similari ese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture <ref type=\"bibr\" target=\"#b8\">[9]</ref>. In Siamese networks, two inputs are fed through identical s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: promoted the performance of object detection on major challenges and benchmarks, such as Pascal VOC <ref type=\"bibr\" target=\"#b8\">[5]</ref>, MS COCO <ref type=\"bibr\" target=\"#b24\">[21]</ref>, and ILSV ww.tei-c.org/ns/1.0\"><head n=\"4\">Experiments</head><p>We conduct experiments on the Pascal VOC 2007 <ref type=\"bibr\" target=\"#b8\">[5]</ref> and MS COCO <ref type=\"bibr\" target=\"#b24\">[21]</ref> datase. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata</head><p>All experiments are performed on the publicly available Lib-riSpeech audio book corpus <ref type=\"bibr\" target=\"#b10\">[11]</ref>. We use the \"train-clean-100\" set as the paired data set,  blic domain books. The books were selected such that there is no overlap with the dev and test sets <ref type=\"bibr\" target=\"#b10\">[11]</ref>. On the other hand, the training data set transcriptions a for the apostrophe in contractions (we replace hyphens with a space). Unlike the original LM corpus <ref type=\"bibr\" target=\"#b10\">[11]</ref> we take no steps to replace non-standard words with a cano. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ses of end-to-end models in the MT field <ref type=\"bibr\" target=\"#b2\">(Bahdanau et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Luong et al., 2015;</ref><ref type=\"bibr\" target=\"#b34\">Vaswani et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b17\">17</ref> , face recognition <ref type=\"bibr\" target=\"#b18\">18</ref> , and playing Atari games <ref type=\"bibr\" target=\"#b19\">19</ref> . They use many layers of neurons, each arranged in overlapp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: olves these problems using the following techniques:</p><p>\u2022 Reversible layers, first introduced in <ref type=\"bibr\" target=\"#b7\">Gomez et al. (2017)</ref>, enable storing only a single copy of activa  term: the b \u2022 n h \u2022 l \u2022 d k ,</formula><p>RevNets. Reversible residual networks were introduced by <ref type=\"bibr\" target=\"#b7\">Gomez et al. (2017)</ref> where it was shown that they can replace Res mer model's self-attention mechanism <ref type=\"bibr\" target=\"#b21\">(Sukhbaatar et al., 2019a;</ref><ref type=\"bibr\" target=\"#b7\">b)</ref> have also recently been explored.</p><p>In particular, levera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">34]</ref>, and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these app hs, which are hard to tune in practice. (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above t ht for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.</p><p>\u2022 RippleNet <ref type=\"bibr\" target=\"#b23\">[24]</ref> is a representative of hybrid methods, which is a memory-n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ectively. For VGG19, we use SGD with an initial learning rate of 0.1. For AmoebaNet, we use RMSProp <ref type=\"bibr\" target=\"#b36\">[38]</ref> optimizer with an initial learning rate of 2.56. We use fp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ignatures have similar architectural behavior, and this has been shown to be the case by Lau et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Therefore, only one interval from each phase needs to be . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oach of <ref type=\"bibr\" target=\"#b0\">[1]</ref> we analyze the supervised loss of a mean classifier <ref type=\"bibr\" target=\"#b29\">[30]</ref>, where for each class c, the rows of W are set to the mean. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hich consist of a large number of heterogeneous components <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" ute C 2 with C 1 +\u03b4, in lines (2) and (3). Next, we observe that (T C 2 ) n \u2212(T C 1 ) n = n in line <ref type=\"bibr\" target=\"#b3\">(4)</ref>. Finally, we note that all the matrices in line ( <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: co-occurrence based methods <ref type=\"bibr\" target=\"#b9\">[10]</ref>, word-similarity based methods <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and supervised relation extraction methods <ref type=\"bib core based on word embedding similarity, where the embedding is pretrained with the Skip-Gram model <ref type=\"bibr\" target=\"#b19\">[20]</ref>    <ref type=\"table\" target=\"#tab_4\">4</ref> shows the man. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad><p>We formalize our notion of spatial correlation similar to prior studies of spatial footprints <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. We define a spatial ed in hardware by correlating patterns with the code and/or data address that initiates the pattern <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Whereas existing sp  tracking of spatial correlation. We show that the cache-coupled structures used in previous work ( <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>) are suboptimal for  dex consistently provides the most accurate predictions when correlation table storage is unbounded <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. By combining both q lications, PC+address indexing can be approximated by combining the PC with a spatial region offset <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. The spatial region  =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.\">Indexing</head><p>Prior studies of spatial predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> advocate predictor i  coverage and/or fragment prediction entries, consequently polluting the PHT.</p><p>Past predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> couple the predictor eas existing spatial pattern prefetching designs are effective for desktop/engineering applications <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the only practical implementation evaluated on server workl e highest coverage.</p><p>For scientific applications, we corroborate the conclusions of prior work <ref type=\"bibr\" target=\"#b3\">[4]</ref> that indicate PC+offset indexing generally approaches the pe led sectored cache <ref type=\"bibr\" target=\"#b21\">[22]</ref>, whereas the spatial pattern predictor <ref type=\"bibr\" target=\"#b3\">[4]</ref> provided a logical sectored-cache tag array alongside a trad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet <ref type=\"bibr\" target=\"#b17\">[10]</ref>, which forms the sentiment view. Then, using a framework o ment aspect of the associate text. For this, we use an external knowledge base, called SentiWordNet <ref type=\"bibr\" target=\"#b17\">[10]</ref>. It is based on the well-known English lexical dictionary  ures with the mid-level features (denoted as Low&amp;SentiBank), and a textual feature-based method <ref type=\"bibr\" target=\"#b17\">[10]</ref> (denoted as SentiStrength<ref type=\"foot\" target=\"#foot_3\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: any studies <ref type=\"bibr\" target=\"#b6\">[7]</ref>[30] <ref type=\"bibr\" target=\"#b8\">[9]</ref>[10] <ref type=\"bibr\" target=\"#b24\">[27]</ref> providing solutions on improving overall SMT throughput an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to improve the performance of CNNs for various tasks, such as image and video classification tasks <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b33\">33]</ref>. Wang et al. <ref ty ks. The second one is the way of enhancing discriminative ability of the network. Channel attention <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> has been shown to be rate non-local operations for spatial attention in video classification. On the contrary, Hu et al. <ref type=\"bibr\" target=\"#b9\">[ 9]</ref> proposed SENet to exploit channel-wise relationships to ach N-based SR models do not consider the feature interdependencies. To utilize such information, SENet <ref type=\"bibr\" target=\"#b9\">[9]</ref> was introduced in CNNs to rescale the channelwise features f he aggregated information by global covariance pooling, we apply a gating mechanism. As explored in <ref type=\"bibr\" target=\"#b9\">[9]</ref>, the simple sigmoid function can serve as a proper gating fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d items. Later, the NCF framework was extended to model a ribute interactions for a ribute-aware CF <ref type=\"bibr\" target=\"#b36\">[37]</ref>. However, their methods are only applicable to learn inter g task, we can optimize pairwise personalized ranking loss <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> or contrastive max-margin loss <ref type=\"bibr\" target=\"#b4. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oric traffic speeds and the underlying road graph. DCRNN replaces the fully-connected layers in GRU <ref type=\"bibr\" target=\"#b5\">(Chung et al., 2014)</ref> with the diffusion convolution operator <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g/ns/1.0\"><head n=\"4.1\">The Base Model: BLSTM-CRF</head><p>Many recent sequence labeling frameworks <ref type=\"bibr\" target=\"#b25\">(Ma and Hovy, 2016b;</ref><ref type=\"bibr\" target=\"#b27\">Misawa et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resolution networks. Previous works including EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, BTSRN <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RDN <ref type=\"bibr\" target=\"#b41\">[42]</ref> found that br\" target=\"#b30\">31]</ref>. It is also experimentally proved in single image super-resolution task <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ \"#b11\">[12]</ref> hinders the accuracy of image super-resolution. Thus, in recent image SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ small image patches (e.g. 48 \u00d7 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ ny kinds of regularizers, for examples, weight decaying and dropout, are not adopted in SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ a is augmented with random horizontal flips and rotations following common data augmentation methods<ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. During training, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b43\">(Xiong et al., 2016;</ref><ref type=\"bibr\" target=\"#b31\">Seo et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Bahdanau et al., 2015)</ref>. The resulting representation is encoded  l language.</p><p>In this work, we consider attention-based neural machine translation (NMT) models <ref type=\"bibr\" target=\"#b2\">Bahdanau et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b24\">Luong et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: for graph-structured data.</p><p>The notion of neural networks for graph data was first outlined in <ref type=\"bibr\" target=\"#b14\">Gori et al. (2005)</ref>  <ref type=\"bibr\" target=\"#b14\">[15]</ref> a works for graph data was first outlined in <ref type=\"bibr\" target=\"#b14\">Gori et al. (2005)</ref>  <ref type=\"bibr\" target=\"#b14\">[15]</ref> and further elaborated on in <ref type=\"bibr\" target=\"#b26. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igure\" target=\"#fig_0\">1</ref>). For the training data scarcity problem, we introduce meta learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta used as a data augmentation strategy to deal with the lack of training data. However, meta-learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta number of model parameters. To deal with the data scarcity problem, we propose to use meta learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b15\">Malon (2018)</ref> fine-tunes the generative pretraining transformer (GPT) <ref type=\"bibr\" target=\"#b22\">(Radford et al., 2018)</ref> for FV. Based on the methods mentioned a ation models such as ELMo <ref type=\"bibr\" target=\"#b21\">(Peters et al., 2018)</ref> and OpenAI GPT <ref type=\"bibr\" target=\"#b22\">(Radford et al., 2018)</ref> are proven to be effective on many NLP t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ort, baseball is false since Matt Flynn is an NFL player), incompatible entity types, and many more <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. It has also been observed that such nois fectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>.</p></div> <div xmlns=\"http://www.tei-c.o exclusive (MUT and RMUT); and inverse relations (INV). We reproduce the list of information used in <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> in tabular form in Table <ref type=\"table mal distributions: N (0.7, 0.2) for facts in the original KG and N (0.3, 0.2) for added noisy facts <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. The SAMEENT facts between entities are g hyper-parameter threshold as the cutoff for classifying a test triple based on the prediction score <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. Our experiments were run on Intel(R) Xeo the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx e the probabilistic sources of information such as the confidence scores obtained during extraction <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b8\">, Jiang et al pe=\"bibr\" target=\"#b8\">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b19\">[Pujara et a ref type=\"bibr\" target=\"#b1\">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b8\">, Jiang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: .</p><p>However, existing works on multimedia recommendation <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> mainly treat multi-modal information as a whole and incorpo derived from CF framework, such as MF <ref type=\"bibr\" target=\"#b29\">[30]</ref>. For instance, VBPR <ref type=\"bibr\" target=\"#b16\">[17]</ref> leverages visual features to enrich ID embeddings of items to two categories: CF-based (VBPR and ACF) and GCN-based (NGCF and GraphSAGE) methods.</p><p>\u2022 VBPR <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Such model integrates the content features and ID embeddi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on. The frame sharpness is evaluated using the cumulative probability blur detection (CPBD) measure <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which determines blur based on the presence of edges in t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ccess, to generate a speculative value that does not necessarily exhibit value locality (e.g., DLVP <ref type=\"bibr\" target=\"#b2\">[3]</ref>). While value predictors can generate speculative results fo rk has shown that load-only predictors are most efficient with a modest hardware budget (e.g., 8KB) <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>In this stud r\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref> Context Address Prediction (CAP) <ref type=\"bibr\" target=\"#b2\">[3]</ref> one another. We found that no individual predictor is strict e focus only on predicting load values since that is most effective with limited hardware resources <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xml sm needed to communicate the predicted values from the value-predicted producers to their consumers <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Consumers of the load can use the prediction by reading the s practical implementations of value prediction, we encourage the readers to visit prior art papers <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t about the baseline ISA, microarchitecture, and storage constraints (Sheikh reports similar findings <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>).</p><p>In all of \"#b7\">[8]</ref>, and subsequent work confirmed the same is true for load instructions in particular <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>Our implemen an directly generating values from We use the state-of-the-art DLVP predictor as a reference design <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The predictor consists of one tagged table indexed by a has. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s for PPI (0.995) and Reddit (0.970). * Equal contribution * The skip-connection design proposed by <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref> does not have such \"subset\" requirement, an ve the softmax step which normalizes attention values within the same neighborhood, as suggested by <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref>. See Appendix C.3.</p></div> \t\t\t</abstract> selected by one node in the next layer. <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref> further propose samplers to restrict the ne ize in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref> improves FastGCN by an additional sampling   (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref> and clustering of <ref type=\"bibr\" target=\" its neighbors in the training graph. The removal of softmax is also seen in the attention design of <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref>. Note that during the minibatch training, G e=\"bibr\" target=\"#b4\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b8\">Gao et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2 e=\"bibr\" target=\"#b3\">Chen et al., 2018a;</ref><ref type=\"bibr\" target=\"#b8\">Gao et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2018)</ref> have been proposed for efficient minibatch  tment of layers independently by prior work <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2018)</ref>. Consider a layer-( + 1) node v and a layer other hand, for some layer sampling methods <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2018)</ref>, extra modification to their samplers is re  et al., 2018b)</ref>, 4. S-GCN <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018a)</ref>, 5. AS-GCN <ref type=\"bibr\" target=\"#b14\">(Huang et al., 2018)</ref>, and 6. ClusterGCN <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: through which they occur, are important and challenging problems that have attracted much attention <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This paper focuses on predicting protein interfaces. Despi ppears to be saturated. This calls for new methodologies or sources of information to be exploited\" <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Most machine learning methods for interface prediction use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ww.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>The task of Grounded video description (GVD) <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> aims to generate more grounded and accurate  existing works either encode region proposals independently or using selfattention-based mechanisms <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>. Therefore, it either fails to consider impl a]</ref>, many works model the video in both global video features and regional object features. In <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>, they encode the objects with transformer <r /head><p>We model the video's global level feature by a Bi-directional LSTM network like most works <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ... ose a novel visual representation method from the perspective of regions. First of all, inspired by <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>, we enhance the proposal features by adding  <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Feature Enhancement</head><p>In this part, we follow <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>'s work, which fusing the spatial-temporal an  feature aggregation on the enhanced feature R.</p><p>We adopt the same classification loss just as <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> do denoted as L cls .</p></div> <div xmlns=\" > t , h f rame + h attention ). h t is used to generate descriptions. We adopt the same MLE loss as <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> which denoted by L sent .</p><p>Finally, the ad n=\"4.1\">Dataset</head><p>We conduct our experiments on the Grounded ActivityNet-Entities Dataset <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> for evaluation. It contains 15k video with 1 ph2Seq method. Data processing. For a fair comparison, the data processing procedure is the same to <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we u  2018]</ref>, BiM-STM+TempoAtnn <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> on Grounded ActivityNet Captions Dataset to  emove the hierarchical attention and replace it with the coarsegrain proposal attention proposed by <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>.</p><p>Table <ref type=\"table\" target=\"#tab_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >12]</ref>.</p><p>Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, <ref type=\"bibr\" target=\"#b48\">[49]</ref>), a selfsupervised representation learning technique for r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b1\">2]</ref> and MT <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= ns/1.0\"><head>Fine-tuning set</head><p>In-domain Out-of-domain Real + one-speaker TTS synthetic 59. <ref type=\"bibr\" target=\"#b4\">5</ref> 19.5 Only one-speaker TTS synthetic 38.5 13.8</p><p>Table <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fense strategies into consideration <ref type=\"bibr\">(Athalye et al., 2018b)</ref>.</p><p>Recently, <ref type=\"bibr\" target=\"#b13\">Shafahi et al. (2019)</ref> showed that, for two classes of data dist for general classifiers, and their relationship to some recent works in literature.</p><p>Recently, <ref type=\"bibr\" target=\"#b13\">Shafahi et al. (2019)</ref> shows that no classifier can achieve low . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ty. For the discriminator networks, we use 70 \u00d7 70 PatchGANs <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> which can be applied to arbitrarily-sized images in a fully. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is mostly focusing on the classical collaborative filtering user-item setting. Salakhutdinov et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref> first propose to use Restricted Boltzmann Machines (RBM) f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: id levels.</p><p>Featurized image pyramids were heavily used in the era of hand-engineered features <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>. They were so critic  these methods adopt architectures with pyramidal shapes, they are unlike featurized image pyramids <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target  were originally extracted at scale-space extrema and used for feature point matching. HOG features <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and later SIFT features as well, were computed densely over. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the uncertainty inherent in user behavior and the limited information provided by browser sessions <ref type=\"bibr\" target=\"#b17\">[18]</ref>.</p><p>Based on existing literature, almost all the RNN-ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e proposed method naturally extends our previous work of unsupervised information network embedding <ref type=\"bibr\" target=\"#b26\">[27]</ref> and first learns a low dimensional embedding for words thr e network is embedded into a low dimensional vector space that preserves the second-order proximity <ref type=\"bibr\" target=\"#b26\">[27]</ref> between the vertices in the network. The representation of is suitable for arbitrary types of information networks: undirected or directed, binary or weighted <ref type=\"bibr\" target=\"#b26\">[27]</ref>. The LINE model optimizes an objective function which aims vious work, we introduced the LINE model to learn the embedding of large-scale information networks <ref type=\"bibr\" target=\"#b26\">[27]</ref>. LINE is mainly designed for homogeneous networks, i.e., n l for embedding bipartite networks. The essential idea is to make use of the second-order proximity <ref type=\"bibr\" target=\"#b26\">[27]</ref> between vertices, which assumes vertices with similar neig jective (3) can be optimized with stochastic gradient descent using the techniques of edge sampling <ref type=\"bibr\" target=\"#b26\">[27]</ref> and negative sampling <ref type=\"bibr\" target=\"#b17\">[18]< descent in learning network embeddings. For the detailed optimization process, readers can refer to <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>The embeddings of the word-word, word-document, and  (4) is to merge the all the edges in the three sets Eww, E wd , E wl and then deploy edge sampling <ref type=\"bibr\" target=\"#b26\">[27]</ref>, which samples an edge for model updating in each step, wi 0]</ref>.</p><p>\u2022 LINE: the large-scale information network embedding model proposed by Tang et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref>. We use the LINE model to learn unsupervised embeddings wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b3\">Hawkes (1971b</ref><ref type=\"bibr\" target=\"#b4\">Hawkes ( ,a, 1972))</ref>; <ref type=\"bibr\" target=\"#b5\">Hawkes and Oakes (1974)</ref>.</p><p>qq q q q q q qq q q q q qq q qq q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and it was used in <ref type=\"bibr\" target=\"#b1\">[2]</ref>  nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=\"bibr\" target=\"#b0\">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=\"bibr\" target=\"#b0\">(1)</ref>, and so now we generate results by performing a second pass . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b34\">31]</ref>, ASPP <ref type=\"bibr\" target=\"#b6\">[3]</ref>, and Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref>.</p><p>The Inception block adopts multiple branches with dif all the positions equally, probably leading to confusion between object and context. Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref> learns distinctive resolutions of individual objects, unfort pe=\"bibr\" target=\"#b36\">[33]</ref>, ASPP <ref type=\"bibr\" target=\"#b6\">[3]</ref> and Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref>. For Inception, besides the original version, we change its . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aints that help generate more content rich responses that are based on a model of syntax and topics <ref type=\"bibr\" target=\"#b12\">(Griffiths et al., 2005)</ref> and semantic similarity <ref type=\"bib o estimate these distributions, we leverage the unsupervised model of topics and syntax proposed by <ref type=\"bibr\" target=\"#b12\">Griffiths and Steyvers (2005)</ref>. The second constraint encourages. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: search of graph embedding ( <ref type=\"bibr\" target=\"#b15\">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Grover and Leskovec, 2016)</ref>, where graph topology and node relati he performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref>, VGAE <ref type=\"bibr\" target=\"#b9\"> ds. DeepWalk <ref type=\"bibr\" target=\"#b15\">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref> are representative random walk-based ns from nodes' raw features, without using any graph structure information incorporated. \u2022 Node2Vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref>: This approach is an extension of Wo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s and construct publication networks, then use graph-based <ref type=\"bibr\" target=\"#b2\">[3]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref> or heuristic methods <ref type=\"bibr\" target=\"#b5\">[6]</ref> ns when we do not exactly know the number of distinct person for an ambiguous name. Some researches <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref> assume that the c that may contain potentially ambiguous names. Many methods <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" t wed data and is widely in many name disambiguation methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" t  is determined adaptively based on the results of HDBSCAN and AP clustering algorithm. Zhang et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref>: This method constructs three different networks on relation  in their own paper. These comparison methods use different kinds of cluster strategy. For example, <ref type=\"bibr\" target=\"#b4\">[5]</ref> need to specify the number of distinct author, <ref type=\"bi our HGCN based heterogenous network embedding method. Among the compared methods, both Zhang et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Xu et al. construct several graphs based on various rela use network embedding based methods to learn representations of publications. However, Zhang et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> ignore the text information, and Xu et al. just use the word use network embedding methods to learn publication embeddings by constructing publication networks. <ref type=\"bibr\" target=\"#b4\">[5]</ref> utilize a network representation learning based approach on . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i-c.org/ns/1.0\"><head n=\"2.2\">GAN-Based Video Synthesis</head><p>The recent introduction of GANs in <ref type=\"bibr\" target=\"#b9\">[10]</ref> has shifted the focus of the machine learning community to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e. Naively augmenting on different corruptions often will not transfer well to held out corruptions <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, the impressive robustness of AutoAugment gives u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the goal of humancomputer interaction popular, and it has been a research hotspot in recent decades <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Automatic Speech Recognition (ASR) refers to the task of an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ).</p><p>To effect our study, we use a collection of abstracts from a corpus of scientific articles <ref type=\"bibr\" target=\"#b0\">(Ammar et al., 2018)</ref>. We extract entity, coreference, and relati  and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences <ref type=\"bibr\" target=\"#b0\">(Ammar et al., 2018)</ref>.</p><p>For each abstract, we create a knowl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: olution operations <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. GC-MC <ref type=\"bibr\" target=\"#b28\">[29]</ref> applies th t is captured on the level of item relations, rather than the collective user behaviors. SpectralCF <ref type=\"bibr\" target=\"#b41\">[42]</ref> proposes a spectral convolution operation to discover all  , is used as suggested in <ref type=\"bibr\" target=\"#b28\">[29]</ref>.</p><p>We also tried SpectralCF <ref type=\"bibr\" target=\"#b41\">[42]</ref> but found that the eigen-decomposition leads to high time . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  they have recently been shown to be particularly unstable to adversarial perturbations of the data <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, very small and often imperceptible perturbations e classifier, it seems that the adversarial perturbations are generalizable across different models <ref type=\"bibr\" target=\"#b17\">[18]</ref>. This can actually become a real concern from a security p of the relevant work. The phenomenon of adversarial instability was first introduced and studied in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The authors estimated adversarial examples by solving pen explaining the presence of adversarial examples. Unfortunately, the optimization method employed in <ref type=\"bibr\" target=\"#b17\">[18]</ref> is time-consuming and therefore does not scale to large da he training procedure that allows to boost the robustness of the classifier. Notably, the method in <ref type=\"bibr\" target=\"#b17\">[18]</ref> was applied in order to generate adversarial perturbations he proposed DeepFool approach to stateof-the-art techniques to compute adversarial perturbations in <ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The method in ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The method in <ref type=\"bibr\" target=\"#b17\">[18]</ref> solves a series of penalized optimization problems to find ver that the proposed approach also yields slightly smaller perturbation vectors than the method in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The proposed approach is hence more accurate in detecting mplexity aspect, the proposed approach is substantially faster than the standard method proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, while the approach <ref type=\"bibr\" target=\"#b17  standard method proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, while the approach <ref type=\"bibr\" target=\"#b17\">[18]</ref> involves a costly minimization of a series of objective fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Peters et al., 2018)</ref>, GPT-2 <ref type=\"bibr\" target=\"#b26\">(Radford et al., 2019)</ref>, BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, <ref type=\"bibr\">XLNet (Yang et al., 2019 as reestablished the new state-ofthe-art baselines across various tasks, such as question answering <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, coreference resolution <ref type=\"bibr\" t in the pre-training stage, such as updating the model using only short sequences in the early stage <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>.</p><p>Common strategies for reducing memo ction. Following the paradigm of language model pre-training and down-stream task fine-tuning, BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref> consists of multiple layers of bidirection lti-head self-attention layer and a position-wise feed-forward layer. Using the same notation as in <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, we denote the number of Transformer layer ource-intensive process. For instance, the training of BERT-family models is notoriously expensive. <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref> report that it takes four days for pre-trai e compare BlockBERT with the following baselines:</p><p>Google BERT The pre-trained base model from <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref>.</p><p>RoBERTa-2seq and RoBERTa-1seq We com \"figure\">6</ref>.</p><p>For all the pre-trained models, we adopt the same fine-tuning QA setup from <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref>.</p><p>The tokenized paragraph (p 1 , \u2022 \u2022 \u2022 n units H leads to significant performance degradation <ref type=\"bibr\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Devlin et al., 2019)</ref> and does not address the long sequence issu  target=\"#b3\">Dai et al., 2019)</ref> and its successful application on language model pre-training <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Radford et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e two tasks or simply concatenating the representation of publication and person name when training <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. However, our experi  minimising the total loss of the two tasks, or using simple concatenation procedures when training <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ ation procedure similar to Ma et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Hashimoto et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>. This model has a pipeline architecture for two jointly trai. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b20\">(Pasupat and Liang, 2015;</ref><ref type=\"bibr\" target=\"#b31\">Yih et al., 2016;</ref><ref type=\"bibr\" target=\"#b0\">Abujabal et al., 2017)</ref>. These systems are effective but at the c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lar to recent work on sequence-tosequence voice conversion <ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref>. <ref type=\"bibr\" r identities, to transform word segments from multiple speakers into multiple target voices. Unlike <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which trained separate models for each source-target spea a pretrained speech recognizer to more explicitly capture phonemic information in the source speech <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, we do find it helpful to multitask train the mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rained language models (e.g., ELMo <ref type=\"bibr\" target=\"#b29\">(Peters et al., 2018)</ref>, BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, XLnet <ref type=\"bibr\" target=\"#b45\">(Yan e achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al.,  entations of the fully supervised NER methods attain very close to the state-of-the-art performance <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Limsopatham . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> or designing specific CNN-RNN networks <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Such classifier that takes video sequences as input is beco erm Memory (LSTM) recurrent neural network is widely adopted <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. LSTM has memory abilit ion recognition using CNN or RNN structures in recent papers <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Such deep networks r 18\">19]</ref>. Such deep networks reach top competitive results in the history of EmotiW challenges <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Therefore we take s ork shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b0\">[1]</ref>, Giffin, et al., <ref type=\"bibr\" target=\"#b22\">[23]</ref>, Spivey <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Bond and McKinley <ref type=\"bibr\" target=\"#b24\">[25]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref> and dynamic <ref type=\"bibr\" target=\"#b12\">[13]</ref> operation scheduling, event processing <ref type=\"bibr\" target=\"#b6\">[7]</ref>, I/O subsystem <ref type=\"bibr\" target=\"#b11\">[12]</ref> and  extensions. Timer-based invocation capabilities are provided through TAO's Real-Time Event Service <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Where the TMO model creates new ORB services to provide its. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and it's nontrivial to change the system to train in an end-to-end fashion.</p><p>To our knowledge, <ref type=\"bibr\" target=\"#b22\">Wang et al. (2016)</ref> is the earliest work touching end-to-end TTS. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: HDR) compression <ref type=\"bibr\" target=\"#b7\">[Fattal et al. 2002]</ref>, intrinsic image recovery <ref type=\"bibr\" target=\"#b30\">[Weiss 2001</ref>], shadow removal <ref type=\"bibr\" target=\"#b8\">[Fin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Gui et al., 2019b)</ref>.</p><p>Recently, Transformer <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, l \"#b34\">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, like machine translation <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>, language modeling <ref type=\"bibr\" targ mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Transformer</head><p>Transformer was introduced by <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>, which was mainly based on self-attentio =\"bibr\" target=\"#b7\">Devlin et al., 2018)</ref>. Instead of using the sinusoidal position embedding <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref> and learned absolute position embedding, 1\">Transformer Encoder Architecture</head><p>We first introduce the Transformer encoder proposed in <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>. The Transformer encoder takes in an mat e Transformer encoder includes layer normalization and Residual connection, we use them the same as <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c. ng it unable to capture the sequential characteristic of languages. In order to solve this problem, <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref> suggested to use position embeddings gen  two tokens. For any fixed offset k, P E t+k can be represented by a linear transformation of P E t <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>. In TENER, Transformer encoder is used n d in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Devlin et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rable to different noise distributions at test time? Inspired by recent research in computer vision <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref typ  model using a mixture of noisy and clean samples.</p><p>\u2022 We implement a stability training method <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, adapted to the sequence labeling scenario r method to improve robustness is to design a representation that is less sensitive to noisy input. <ref type=\"bibr\" target=\"#b51\">Zheng et al. (2016)</ref> presented a general method to stabilize mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  huge boost in performance using Deep-Learning based methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target yer. The network input is interpolated to the output size. As done in previous CNN-based SR methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targe orted numerical were produced using the evaluation script of <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t al., 2018)</ref>, OpenAI GPT <ref type=\"bibr\" target=\"#b15\">(Radford et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2018)</ref>. It has been shown that language modeling   target=\"#b2\">(Castro et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">de Araujo et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Fernandes et al., 2018)</ref>. The model is composed of two bidirectio ained word embeddings were explored by <ref type=\"bibr\" target=\"#b2\">Castro et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b4\">Fernandes et al. (2018)</ref> compared it to 3 other architectures. Th mming. During evaluation, the most likely sequence is obtained by Viterbi decoding. As described in <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>, WordPiece tokenization requires prediction ead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>. The resulting architecture resembles the L , we use document context for input examples instead of sentence context. Following the approach of <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref> on the SQuAD dataset, examples larger than  pment set comprised of 10% of the First HAREM training set. We use the customized Adam optimizer of <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>.</p><p>For the feature-based approach, we u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">INTRODUCTION</head><p>Score matching <ref type=\"bibr\" target=\"#b12\">(Hyv\u00e4rinen, 2005)</ref> is particularly suitable for learning unnorma MLE) can be difficult due to the intractable partition function Z \u03b8 . To avoid this, score matching <ref type=\"bibr\" target=\"#b12\">(Hyv\u00e4rinen, 2005)</ref> minimizes the Fisher divergence between p d a  not have access to the score function of the data s d (x).</p><p>By applying integration by parts, <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref> shows that L(\u03b8) can be written as L(\u03b8) = J(\u03b8)  </p><p>Other than our requirements on p v , the assumptions are exactly the same as in Theorem 1 of <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>. We advise the interested readers to read Appe he consistency of <ref type=\"bibr\">MLE (van der Vaart, 1998)</ref>. We also adopt the assumption in <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref> that all densities are strictly positive (Assu M . These two facts lead to consistency. For a complete proof, see Appendix B.3.</p><p>Remark 1. In <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>, the authors only showed that J(\u03b8) = 0 \u21d4 \u03b8 = \u03b8 s a constant w.r.t. \u03b8.</p><p>Proof. The basic idea of this proof is similar to that of Theorem 1 in <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>. First, note that L(\u03b8, p v ) can be expanded t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: odels for each semantic category on exemplar-based methods <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and show that SR results can be improved by semantic priors ately for each semantic category on exemplar-based methods <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. In contrast to these studies, we explore categorical prior et=\"#b48\">[49,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> and random forest <ref type=\"bibr\" target=\"#b38\">[39]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vanced methods for comparing the quality of images, such as feature matching based on SIFT analysis <ref type=\"bibr\" target=\"#b15\">[16]</ref>. This technique, however, shows the same trends as the sim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  significant overhead in terms of the number of queries needed. For instance, attacking an ImageNet <ref type=\"bibr\" target=\"#b21\">(Russakovsky et al., 2015)</ref> classifier requires hundreds of thou erating untargeted adversarial examples. We consider both the 2 and \u221e threat models on the ImageNet <ref type=\"bibr\" target=\"#b21\">(Russakovsky et al., 2015)</ref> dataset, in terms of success rate an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (amino acids). Specifically, we augment the autoregressive self-attention of recent sequence models <ref type=\"bibr\" target=\"#b6\">[7]</ref> with graph-based descriptions of the 3D structure. By compos dependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention <ref type=\"bibr\" target=\"#b6\">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d  Structured Transformer model that draws inspiration from the selfattention based Transformer model <ref type=\"bibr\" target=\"#b6\">[7]</ref> and is augmented for scalable incorporation of relational in an attend to a separate subspace of the embeddings via learned query, key and value transformations <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>The queries are derived from the current embedding at een these self-attention layers and position-wise feedforward layers as in the original Transformer <ref type=\"bibr\" target=\"#b6\">[7]</ref>. We stack multiple layers atop each other, and thereby obtai rained models using the learning rate schedule and initialization of the original Transformer paper <ref type=\"bibr\" target=\"#b6\">[7]</ref>, a dropout rate of 10% <ref type=\"bibr\" target=\"#b41\">[42]</ f their structure. Our model augments the traditional sequence-level self-attention of Transformers <ref type=\"bibr\" target=\"#b6\">[7]</ref> with relational 3D structural encodings and is able to lever ndependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention<ref type=\"bibr\" target=\"#b6\">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b45\">[43,</ref><ref type=\"bibr\" target=\"#b56\">54]</ref>. Based on Cycle-GAN <ref type=\"bibr\" target=\"#b58\">[56]</ref>, Yuan et al. <ref type=\"bibr\" target=\"#b45\">[43]</ref> pro scheme has also been used to perform image translation without paired training data, e.g., CycleGAN <ref type=\"bibr\" target=\"#b58\">[56]</ref> and DualGAN <ref type=\"bibr\" target=\"#b44\">[42]</ref>. Spe avoid the possible mode collapse issue when solving the under-constrained image translation problem <ref type=\"bibr\" target=\"#b58\">[56]</ref>. Unlike these methods, we seek to improve the performance  es collected from YouTube. Thus, there are 3 DRN-adapt models in total. And We also train a CinCGAN <ref type=\"bibr\" target=\"#b58\">[56]</ref> model for each kind of unpaired data for comparison. Based  Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods <ref type=\"bibr\" target=\"#b58\">[56,</ref><ref type=\"bibr\" target=\"#b6\">4,</ref><ref type=\"bibr\" targ  CycleGAN based SR methods. First, Cycle-GAN based methods <ref type=\"bibr\" target=\"#b45\">[43,</ref><ref type=\"bibr\" target=\"#b58\">56]</ref> use a cycle consistency loss to avoid the possible mode col. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f stable feature distillation, which consists of a deep global balancing regression (DGBR) algorithm<ref type=\"bibr\" target=\"#b12\">[13]</ref>, a teacher network and a student network. The DGBR algorit ner is another promising direction.</p><p>Feature-Based Module. The current stable feature approach <ref type=\"bibr\" target=\"#b12\">[13]</ref> needs much time and computing resources. For implementing . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: anned vehicles <ref type=\"bibr\" target=\"#b7\">[Menze and Geiger, 2015]</ref>, medical image analysis <ref type=\"bibr\" target=\"#b11\">[Zhang et al., 2017]</ref>, robots and etc. Endowed with the power of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  predict execution time of programs by using a set of hand-crafted features of high level programs. <ref type=\"bibr\" target=\"#b9\">Dubach et al. (2007)</ref> uses neural networks with hand-crafted feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: While correlations are typically sufficient to reveal the intrinsic geometrical structure of images <ref type=\"bibr\" target=\"#b15\">[16]</ref>, the effects of higher-order statistics might be non-negli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a trainable and personalized relation scoring func  where GCNs can be used directly, while here we investigate GCNs for heterogeneous KGs. Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC o a user-personalized weighted graph that characterizes user's preferences. To this end, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero \u03bb is better than \u03bb = 0 (the case of Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref>), which justifies our claim that LS regularization can ass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b12\">Dahlmeier and Ng, 2012c;</ref><ref type=\"bibr\" target=\"#b33\">Napoles et al., 2015;</ref><ref type=\"bibr\" target=\"#b46\">Sakaguchi et al., 2016;</ref><ref type=\"bibr\" target=\"#b34\">Napoles e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b34\">[35]</ref>, which was then used to develop a multimodal deception detection system <ref type=\"bibr\" target=\"#b1\">[2]</ref>. An extensive review of approaches for evaluating human cred sent useful clues for deception, their performance is often similar to that of the n-grams features <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Since in our current work we are not focusing on the insigh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o-Encoder (CAE) to learn the joint representation using Denoising Auto-Encoder (DAE) style learning <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Fig. <ref type=\"figure\" target=\"#fig_4\">5</ref> shows a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b8\">[9]</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which is a standard multimodal annotation scheme for interp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tation is based on Z-MERT <ref type=\"bibr\" target=\"#b28\">(Zaidan, 2009)</ref>, and we use FastAlign <ref type=\"bibr\" target=\"#b8\">(Dyer et al., 2013)</ref> for word alignment within the joint refineme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ods (GCN <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[13]</ref>), etc.</p><p>Graph-level embedding. The most intuitive way. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s. Although the time cost was reduced, it brought a large number of false positives.</p><p>The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a configurable protection technique for SDC-causi s, machine learning based methods are introduced to identify the SDC-causing instructions. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a machine learning algorithm based model, namely  pproach in detail. We first define some terms used in this paper, some of which are drawn from work <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>Dynamic Dependency Graph: A Dynamic Dependency Grap ased on prior work <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" t  are based on duplicating the backward slices of the instructions to protect, similar to prior work <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>We insert a check immediately after the instruction tion efficiency and SDC impact are imperative parameters for evaluating our approach. In literature <ref type=\"bibr\" target=\"#b15\">[16]</ref>, the SDC detection efficiency (DE) is defined as the ratio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: LR/HR patches, such as nearest neighbor <ref type=\"bibr\" target=\"#b6\">[7]</ref>, manifold embedding <ref type=\"bibr\" target=\"#b1\">[2]</ref>, random forest <ref type=\"bibr\" target=\"#b19\">[20]</ref> and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=\"bibr\" target=\"#b13\">[14]</ref> . It could obtain transformation rules automatically durin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o this end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 20 \"#b20\">(Munkhdalai and Yu, 2016b)</ref>.</p><p>We ensemble our ESIM model with syntactic tree-LSTMs <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015)</ref> based on syntactic parse trees and achieve s here are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015)</ref> same as in model ( <ref type=\"formula\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: imensions. Label smoothed cross entropy is used as the objective function with an uncertainty = 0.1 <ref type=\"bibr\" target=\"#b27\">(Szegedy et al., 2016)</ref>. We use linear learning rate decay start. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g existing drugs (or drug target genes) to a new application <ref type=\"bibr\" target=\"#b1\">[4,</ref><ref type=\"bibr\" target=\"#b2\">5,</ref><ref type=\"bibr\" target=\"#b29\">33]</ref>. As an example, the d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tering transforms or certain CNN architectures have been shown to be stable to spatial deformations <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ ze the stability of GCNs to small deformation of the underlying random graph model. Similar to CNNs <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, studying GCNs in th es of stability are often balanced by discussions on how the representation preserves signal (e.g., <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ ine intuitive notions of deformations and stability in the continuous world like the Euclidean case <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ p><p>Related work on stability. The study of stability to deformations has been pioneered by Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref> in the context of the scattering transform for signals on  ph models and to obtain deformation stability bounds that are similar to those on Euclidean domains <ref type=\"bibr\" target=\"#b31\">[32]</ref>. We note that <ref type=\"bibr\" target=\"#b28\">[29]</ref> al eformations is an essential feature for the generalization properties of deep architectures. Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref> studied the stability to small deformations of the wavelet for small enough \u2207\u03c4 \u221e , we obtain N P (\u03c4 ) d \u2207\u03c4 \u221e , recovering the more standard quantity of Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref>. In this case, we also have the bound</p><formula xml:id=\"  ). Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>Proposition 4 (Signal deformation). Consider a GCN  \u03c4 \u221e , the GCN is invariant to translations and stable to deformations, similar to Euclidean domains <ref type=\"bibr\" target=\"#b31\">[32]</ref>. We note that studies of stability are often balanced by d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tences contribute to the classification decision which can be of value in applications and analysis <ref type=\"bibr\" target=\"#b21\">(Shen et al., 2014;</ref><ref type=\"bibr\">Gao et al., 2014)</ref>.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Active learning is widely studied to solve this kind of sample selection problem. As discussed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>, active learning methods can be divided into two categorie ertain number of labeled samples to evaluate the uncertainty of the unlabeled data or sampling bias <ref type=\"bibr\" target=\"#b17\">[18]</ref> will result. It is therefore recommended that such methods ive learning algorithms are referred to as early active learning or early stage experimental design <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We illustrate the procedures of and example of the tradit a><p>Finding the optimal subset V \u2282 X in Eq. ( <ref type=\"formula\">7</ref>) is NP-hard. Inspired by <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we relax the problem to the following problem by introduc ime, the least squared loss used in Eq. ( <ref type=\"formula\">8</ref>) is sensitive to the outliers <ref type=\"bibr\" target=\"#b17\">[18]</ref>, which makes the algorithm not robust.</p><p>We note that  type=\"bibr\" target=\"#b25\">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is shown in <ref type=\"bibr\" target=\"#b17\">[18]</ref> that the 2,1 -norm is the minimum convex hull of the 2,0 - 1.0\"><head n=\"2.\">K-means</head><p>We use the K-means algorithm as another baseline algorithm as in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In each experiment, samples are ranked by their distances It formulates a regularized linear regression problem which minimizes reconstruction error. 5. RRSS <ref type=\"bibr\" target=\"#b17\">[18]</ref> Early active learning via Robust Representation and Struct ance of the linear methods with our algorithm. This is consistent with the mathematical analysis in <ref type=\"bibr\" target=\"#b17\">[18]</ref> that kernelization produces more discriminative representa ithm not robust.</p><p>We note that in previous researches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, the 2,1 -norm is u s, minimization of A 2,1 will achieve the same result as A 2,0 when A is row-sparse. As analyzed in <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, the 2,1 -norm can. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s topic has attracted considerable attention in recent years <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target  to identify the same person in different camera views among a potentially huge number of imposters <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. At the same time, p  Laplacian matrix and D is the degree matrix with each element D ii = j S V (i, j). As discussed in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, minimizing the pairwise constraint will force the similar r  images of a person have a high probability of sharing the similar representation features in re-id <ref type=\"bibr\" target=\"#b8\">[9]</ref>, this will make early active learning schema more suitable f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or each kernel, we computed the normalized Gram matrix. We used the C-SVM im-plementation of LIBSVM <ref type=\"bibr\" target=\"#b5\">(Chang and Lin 2011)</ref> to compute the classification accuracies us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b18\">[19]</ref>, Ur-ban100 <ref type=\"bibr\" target=\"#b19\">[20]</ref> and Manga109 <ref type=\"bibr\" target=\"#b20\">[21]</ref>. These datasets contain a wide variety of images that can . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  way that GNNs exchange that information between nodes makes them vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Adversarial attacks on graphs, which carefully rewire the g ning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks (e.g., Nettack <ref type=\"bibr\" target=\"#b7\">[8]</ref>) that perturb the graph in training-time and evasion attacks \"#b28\">[29]</ref>. The former deceives the model to misclassify a specific node (i.e., target node) <ref type=\"bibr\" target=\"#b7\">[8]</ref> while the latter degrades the overall performance of the tra e targeted attack where the attacker only manipulates edges of the target node's neighbors. Nettack <ref type=\"bibr\" target=\"#b7\">[8]</ref> generates perturbations by modifying graph structure (i.e.,  . Also, <ref type=\"bibr\" target=\"#b15\">[16]</ref> is designed specifically for the Nettack attacker <ref type=\"bibr\" target=\"#b7\">[8]</ref> and so is less versatile. Another technique <ref type=\"bibr\" our model to baselines under three kinds of adversarial attacks: direct targeted attack (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>), influence targeted attack (Nettack-In <ref type=\"bibr\" tar  attack (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>), influence targeted attack (Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>), and non-targeted attack (Mettack <ref type=\"bibr\" target=\" or all neighbors. In the targeted attack, we select 40 correctly classified target nodes (following <ref type=\"bibr\" target=\"#b7\">[8]</ref>): 10 nodes with the largest classification margin, 20 random taset into training (10%), validation (10%), and test set (80%) following the experimental setup in <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Ap ef type=\"bibr\" target=\"#b15\">[16]</ref>), and models for generating adversarial attacks (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Met  for generating adversarial attacks (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Mettack <ref type=\"bibr\" target=\"#b25\">[26]</ref>).</p> y loss using Adam optimizer and learning rate of 0.01. For other parameters, we follow the setup in <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p></div>\t\t\t</div> \t\t\t<div type=\"references\">  \t\t\t\t<listBib attacker finds optimal perturbation A through optimization <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>:</p><formula xml:id=\"formula_3\">argmin A \u2208P G \u2206 L attack (f(A s. The attacker aims to destroy prediction for target node u by manipulating the incident edges of u<ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Here, T = A = {u}. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: trained word embeddings over randomly initialized ones. Embeddings are pretrained using skip-n-gram <ref type=\"bibr\" target=\"#b24\">(Ling et al., 2015a)</ref>, a variation of word2vec <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: method of changing speed has the lowest implementation cost and achieve stateof-the-art performance <ref type=\"bibr\" target=\"#b22\">[23]</ref>. In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, A new metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b3\">[4]</ref> and Neural Network structure with Connectionist temporal classification (CTC) loss <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The hybrid hidden   by the acoustic model, and finally get better results. y * = arg max y log p(y|x) + \u03bb log P LM (y) <ref type=\"bibr\" target=\"#b4\">(5)</ref> where P LM (y) is provided by the LM, y * denotes the final . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tes the feature representation in each position by weighted sum the features of all other positions <ref type=\"bibr\" target=\"#b14\">[15]</ref> <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Thus, it can mo , it can model the longrange context information for semantic segmentation task. For example, DANet <ref type=\"bibr\" target=\"#b14\">[15]</ref> uses two self-attention mechanisms to model long-range con. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lized as performance indices. The experiments were implemented using the public platform Tensorflow <ref type=\"bibr\" target=\"#b31\">[32]</ref> and run on an Intel core 6 i7-7820X CPU at 3.6 GHz with 2 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2014</ref><ref type=\"bibr\" target=\"#b20\">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Zhou et al., ef> is a revision of CNN which uses piecewise max-pooling to extract more relation features. BiLSTM <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015)</ref> is also commonly used for RE with the help. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -Lim algorithm <ref type=\"bibr\" target=\"#b13\">[14]</ref>, multiple input spectrogram inverse (MISI) <ref type=\"bibr\" target=\"#b14\">[15]</ref>, ISSIR <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and cons ly. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref type=\"bibr\" target=\"#b14\">[15]</ref> (see Algorithm 1) to reconstruct the clean phase of each s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ome video generation methods have dealt with this problem by generating the entire sequence at once <ref type=\"bibr\" target=\"#b24\">[25]</ref> or in small batches <ref type=\"bibr\" target=\"#b19\">[20]</r d to handle videos <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p><p>Straight-for ibr\" target=\"#b23\">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed in <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, replacing the 2D . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <p>We employ three types of regularization during training:</p><p>Residual Dropout We apply dropout <ref type=\"bibr\" target=\"#b26\">[27]</ref> to the output of each sub-layer, before it is added to the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the context of deep neural networks, and there is now a quickly growing body of work on this topic <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ e, the \u221e -ball around x has recently been studied as a natural notion for adversarial perturbations <ref type=\"bibr\" target=\"#b10\">[11]</ref>. While we focus on robustness against \u221e -bounded attacks i s. On the attack side, prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) <ref type=\"bibr\" target=\"#b10\">[11]</ref> and multiple variations of it <ref type=\"bibr\" target=\"#b1 rget=\"#b2\">[3]</ref> for an overview of earlier work).</p><p>Adversarial training was introduced in <ref type=\"bibr\" target=\"#b10\">[11]</ref>, however the adversary utilized was quite weak-it relied o arial training discusses the phenomenon of transferability <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>-adversarial example. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rk (IncepGCN) <ref type=\"bibr\" target=\"#b28\">(Szegedy et al., 2016)</ref> and dense network (JKNet) <ref type=\"bibr\" target=\"#b12\">(Huang et al., 2017;</ref><ref type=\"bibr\" target=\"#b33\">Xu et al., 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b19\">Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b7\">Chiu and Nichols, 2016;</ref><ref type=\"bibr\" target=\"#b22\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b46\">Yang et al., model we use to perform NER. We will first describe the basic hierarchical neural CRF tagging model <ref type=\"bibr\" target=\"#b22\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy  labels and performs inference.</p><p>In this paper, we closely follow the architecture proposed by <ref type=\"bibr\" target=\"#b22\">Lample et al. (2016)</ref>, and use bidirectional LSTMs for both the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: .7 points in AP dp 75 , in this highly localization-sensitive task. LVIS v0.5 instance segmentation <ref type=\"bibr\" target=\"#b26\">[27]</ref>: this task has \u223c1000 long-tailed distributed categories. S lementation: LVIS instance segmentation</head><p>We use Mask R-CNN with R50-FPN, fine-tuned in LVIS <ref type=\"bibr\" target=\"#b26\">[27]</ref> train v0.5 and evaluated in val v0.5. We follow the baseli ype=\"bibr\" target=\"#b26\">[27]</ref> train v0.5 and evaluated in val v0.5. We follow the baseline in <ref type=\"bibr\" target=\"#b26\">[27]</ref> (arXiv v3 Appendix B).</p><p>LVIS is a new dataset and mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m for the chest database creation or expansion, performing NER training and modeling using NeuroNER <ref type=\"bibr\" target=\"#b9\">[12]</ref> and then generating the current or the new model, and hence. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ich are \"unnoticeable\" to humans but can cause the learning models to misclassify some target nodes <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., >Limitations of Current Approaches. There are two common issues of existing works on attacking GCNs <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., ch puts a high demand on the scalability of the underlying attack models. However, existing efforts <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., e directly and then treat the newly added nodes as existing nodes and apply the attacks proposed in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Z\u00fcgner and  e attackers can benefit more by additionally manipulating features of nodes (shown in Section 5.4). <ref type=\"bibr\" target=\"#b34\">Z\u00fcgner et al. (2018)</ref> propose Nettack, which manipulates both ed ology</head><p>Different from previous works <ref type=\"bibr\" target=\"#b12\">(Dai et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Z\u00fcgner and G ation performs on the target node v 0 .</p><p>It is noteworthy that the surrogate model proposed in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> is typically used to generate perturbatio atures (e.g., mutually exclusive features), it will be easily detected. For this issue, the work in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> proposes a statistical test based on the  p>In this paper, we consider three methods designed for the traditional scenario, including Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcg tional scenario, including Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> and Metaattack <ref type=\"bibr\" target=\"# ection 3.2 which is designed specifically for the new attack scenario instead of the constraints in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>.</p><p>There are two strategies to adapt  d features. We call this strategy sequential injection.</p><p>Nettack for the new scenario. Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> addresses the bilevel optimization proble cores from the old scores after each iteration in constant time.</p><p>According to the analysis in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, the time complexity of Nettack in terms  ion of features, it needs O (\u2206 e n in f d) just like Nettack.</p><p>FGSM for the new scenario. FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> computes the gradients of L atk w.r.t. ed he detailed statistics of these datasets are shown in Table 1. Following the same attack setting in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, we only consider the largest connected c  one-time injection can be roughly treated as the special case of the attack scenario considered in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> as nodes are added in advance and perturb ous nodes directly because connections to the target node usually lead to better attack performance <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>. However, we do not know the optimal numb  FGSM performs better than Nettack in the new scenario which is quite different from the results in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>. We hypothesize that it may be due to the features. Considering that the results of target nodes with higher degrees are harder to be mislead <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, we attack the target nodes with d v 0 /2 the attack performance significantly. This is in contrast to the findings in the attack scenario of <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, where the authors observe that manip-ula res are not very important for successful attacks. Such contrast exists because, in the scenario of <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, the attacker can only perturb features o of vicious nodes to the target node, which is also consistent with our intuition and the results in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018</ref>) that (more) connections with target node . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to-translations it has already been shown that a suitable structure for \u03c6 is a tensor field network <ref type=\"bibr\" target=\"#b24\">[25]</ref>, explained below. Note that Romero et al. <ref type=\"bibr\" q. <ref type=\"bibr\" target=\"#b4\">(5)</ref>.</p><p>Tensor Field Networks Tensor field networks (TFN) <ref type=\"bibr\" target=\"#b24\">[25]</ref> are neural networks, which map point clouds to point cloud annels, but we omit it here. Weiler et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>, Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Kondor <ref type=\"bibr\" target=\"#b12\">[13]</ref> showe duces the kernel to a scalar w multiplied by the identity, W = w I, referred to as self-interaction <ref type=\"bibr\" target=\"#b24\">[25]</ref>. As such we can rewrite the TFN layer as</p><formula xml:i c c = w c c per representation degree, shared across all points.</p><p>As proposed in Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>, this is followed by a norm-based non-linearity.</p><p>Att tron-proton simulation.</p><p>Linear DeepSet <ref type=\"bibr\" target=\"#b39\">[40]</ref> Tensor Field <ref type=\"bibr\" target=\"#b24\">[25]</ref> Set Transformer <ref type=\"bibr\" target=\"#b13\">[14]</ref>   type=\"bibr\" target=\"#b13\">[14]</ref>, a non-equivariant attention model, and Tensor Field Networks <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which is similar to SE(3)-Transformer but does not levera s to train significantly larger versions of both the SE(3)-Transformer and the Tensor Field network <ref type=\"bibr\" target=\"#b24\">[25]</ref> and to apply these models to real-world datasets.</p><p>Ou be found in many mathematical physics libraries.</p><p>Tensor Field Layers In Tensor Field Networks <ref type=\"bibr\" target=\"#b24\">[25]</ref> and 3D Steerable CNNs <ref type=\"bibr\" target=\"#b32\">[33]<  that all the Tensor Field networks we trained were significantly bigger than in the original paper <ref type=\"bibr\" target=\"#b24\">[25]</ref>, mostly enabled by the faster computation of the spherical k to obtain stable training. We used a norm based non-linearity for the Tensor Field network (as in <ref type=\"bibr\" target=\"#b24\">[25]</ref>) and no extra non-linearity (beyond the softmax in the sel he Tensor Field network and the linear baseline are SE(3) equivariant. For the Tensor Field Network <ref type=\"bibr\" target=\"#b24\">[25]</ref> baseline, we used the same hyper parameters as for the SE( linear self-interaction and an additional norm-based nonlinearity in each layer as in Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>. For the DeepSet <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: training data, our improved training procedure improves upon the published BERT results on the GLUE <ref type=\"bibr\" target=\"#b52\">(Wang et al., 2019b)</ref> and SQuAD <ref type=\"bibr\" target=\"#b41\">( g on downstream tasks:</p><p>\u2022 GLUE: The General Language Understanding Evaluation (GLUE) benchmark <ref type=\"bibr\" target=\"#b52\">(Wang et al., 2019b)</ref> is a collection of 9 datasets for evaluati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. Discrete hashing t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: entation learning algorithms that use a contrastive loss have outperformed even supervised learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ rvation that a larger number of negative/positive examples in the objective leads to better results <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. The last two terms  </ref>, or different views of the same scene <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Chen et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> extensively study verious data augmentation methods. For lan br\" target=\"#b22\">[23]</ref> and STL10 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with ResNet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as  ef type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. Following <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we set the temperature t = 0.5 and the dimension of the lat /ns/1.0\"><head>B Experiment Details</head><p>Cifar10 and STL10 We adopt PyTorch to implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with Resnet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng from position s. This neural network combines the roles of both policy network and value network <ref type=\"bibr\" target=\"#b11\">12</ref> into a single architecture. The neural network consists of m perfect information. We follow the formalism of alternating Markov games described in previous work <ref type=\"bibr\" target=\"#b11\">12</ref> , noting that algorithms based on value or policy iteration  ompare three distinct versions of AlphaGo:</p><p>1. AlphaGo Fan is the previously published program <ref type=\"bibr\" target=\"#b11\">12</ref> that played against Fan Hui in October 2015. This program wa described in this paper. However, it uses the same handcrafted features and rollouts as AlphaGo Lee <ref type=\"bibr\" target=\"#b11\">12</ref> and training was initialised by supervised learning from hum ue component, it was possible to avoid overfitting to the values (a problem described in prior work <ref type=\"bibr\" target=\"#b11\">12</ref> ). After 72 hours the move prediction accuracy exceeded the  the KGS test set; the value prediction error was also substantially better than previously reported <ref type=\"bibr\" target=\"#b11\">12</ref> . The validation set was composed of professional games from of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored to the tournament values from prior work <ref type=\"bibr\" target=\"#b11\">12</ref> , and correspond to the players reported in that work. The r lso performed against baseline players with Elo ratings anchored to the previously published values <ref type=\"bibr\" target=\"#b11\">12</ref> .</p><p>We measured the head-to-head performance of AlphaGo  hat maximise an upper confidence bound Q(s, a) + U (s, a), where U (s, a) \u221d P (s, a)/(1 + N (s, a)) <ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b23\">24</ref> , until a leaf node  After 72 hours the move prediction accuracy exceeded the state of the art reported in previous work <ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: stimates between-image labels using the clustering technique <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> or kNN-based methods . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Knowledge bases (KBs), such as Freebase <ref type=\"bibr\" target=\"#b1\">(Bollacker et al., 2008)</ref>, NELL <ref type=\"bibr\" target=\"#b19\">(M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hebyshev polynomials to the graph Laplacian, spatially localized filtering is obtained. Kipf et al. <ref type=\"bibr\" target=\"#b18\">[18]</ref> approximate the polynomials using a re-normalized first-or rmula_4\">E ijp = \u00caijp N j=1 \u00caijp<label>(5)</label></formula><p>or symmetric normalization as in GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref>:</p><formula xml:id=\"formula_5\">E ijp = \u00caijp N i=1 \u00caijp N  e our EGNN(C) layer from the formula of EGNN(A) layer. Indeed, the essential difference between GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref> is wheth nd 20% sized subsets, which is called \"dense\" splitting.</p><p>Following the experiment settings of <ref type=\"bibr\" target=\"#b18\">[18]</ref>[27], we use two layers of EGNN in all of our experiments f nd are penalized more in the loss than a majority class.</p><p>The baseline methods we used are GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>. To inve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: CAREER Award CCR -0133777.</p><p>technology, the larger the cache, the slower the cache will become <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and larger caches increase the cost of manufacturing. Anoth he. An inclusive cache system implies that the contents of the L1 cache be a subset of the L2 cache <ref type=\"bibr\" target=\"#b1\">[2]</ref>. This decreases the effective cache capacity available for u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s.</p><p>The idea of extracting features for NLP using convolutional DNN was previously explored by <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>, in the context of POS tagging, chunking  Recognition (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. In <ref type=\"bibr\" target=\"#b3\">(Collo tation component, each input word token is transformed into a vector by looking up word embeddings. <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> reported that word embeddings learned fr ural network, the convolution approach is a natural method to merge all of the features. Similar to <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>, we first process the output of Window P , we heuristically choose d e = 5. Finally, the word dimension and learning rate are the same as in <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. Table <ref type=\"table\">2 reports</ref> ons.org/licenses/by/4.0/ 1 A word embedding is a distributed representation for a word. For example,<ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> use a 50-dimensional vector to represent -words model</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"3\" xml:id=\"foot_1\"><ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> proposed a pairwise ranking approach to  ares similar intuition with that of <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. In <ref type=\"bibr\" target=\"#b3\">(Collobert et al., 2011)</ref>, all of the tasks are considered as the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: orithms, they usually generate enormous amounts of intermediate data. GPM systems such as Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, and Fr ly proportional to the graph size, but also increases exponentially as the embedding size increases <ref type=\"bibr\" target=\"#b80\">[81]</ref>. Furthermore, GPM problems require compute-intensive opera describe two of these GPM systems briefly and then discuss their major limitations.</p><p>Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref> is a distributed GPM system. It proposes \"think like an em anonical test for each embedding, which has been demonstrated to be very expensive for large graphs <ref type=\"bibr\" target=\"#b80\">[81]</ref>.</p><p>Materialization of Data Structures: The list or arr oking the isomorphism test, embeddings in the worklist are first reduced using their quick patterns <ref type=\"bibr\" target=\"#b80\">[81]</ref>, and then quick patterns are aggregated using their canoni Experimental Setup</head><p>We compare Pangolin with the state-of-the-art GPM frameworks: Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, G-Mine r ease of programming.</p><p>GPM Frameworks: For ease-of-programming, GPM systems such as Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, G-Mine head n=\"2.3\">Existing GPM Frameworks</head><p>Existing GPM systems target either distributed-memory <ref type=\"bibr\" target=\"#b80\">[81,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cantly improved image classification <ref type=\"bibr\" target=\"#b13\">[14]</ref> and object detection <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> accuracy. Compared t task that requires more complex methods to solve. Due to this complexity, current approaches (e.g., <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ n this paper, we streamline the training process for stateof-the-art ConvNet-based object detectors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. We propose a single egressors, rather than training a softmax classifier, SVMs, and regressors in three separate stages <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. The components of t  very deep detection network (VGG16 <ref type=\"bibr\" target=\"#b19\">[20]</ref>) 9\u00d7 faster than R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref> and 3\u00d7 faster than SPPnet <ref type=\"bibr\" target=\"#b10\">[11 1.0\"><head n=\"1.1.\">R-CNN and SPPnet</head><p>The Region-based Convolutional Network method (R-CNN) <ref type=\"bibr\" target=\"#b8\">[9]</ref> achieves excellent object detection accuracy by using a deep  k h , for each of the K object classes, indexed by k. We use the parameterization for t k given in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, in which t k specifies a scale-invariant translation and lo eparates localization and classification. OverFeat <ref type=\"bibr\" target=\"#b18\">[19]</ref>, R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and SPPnet <ref type=\"bibr\" target=\"#b10\">[11]</ref> also t tions of the dataset). We use mini-batches of size R = 128, sampling 64 RoIs from each image. As in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we take 25% of the RoIs from object proposals that have int rm non-maximum suppression independently for each class using the algorithm and settings from R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= he first is the CaffeNet (essentially AlexNet <ref type=\"bibr\" target=\"#b13\">[14]</ref>) from R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. We alternatively refer to this CaffeNet as model S, for \"sm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b22\">Melamud et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b7\">Devlin et al., 2019)</ref>.</p><p>Interestingly, it is also becoming a  with the one-best prompt extracted by our method raising accuracy from 31.1% to 34.1% on BERT-base <ref type=\"bibr\" target=\"#b7\">(Devlin et al., 2019)</ref>, with similar gains being obtained with BE REx T-REx-train #subject-</formula><p>Models As the models to probe, we use BERTbase and BERT-large <ref type=\"bibr\" target=\"#b7\">(Devlin et al., 2019)</ref>.</p><p>Evaluation Metrics We use two metri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nship between features, we introduce explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> to CCA. Finally, using the features that are projected to t use. In contrast, recent advances of explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> can convert nonlinear problems to linear problems, which ca  computation complexity, one can use explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref>. Let \u03c6(x) denote an explicit feature mapping such that K i  rnel. All other histogram-based features were mapped using the exact Bhattacharyya kernel map- ping <ref type=\"bibr\" target=\"#b20\">[13]</ref>. Finally, similar to <ref type=\"bibr\" target=\"#b16\">[9]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e to the space limit, we will skip the description of the basic chain LSTM and readers can refer to <ref type=\"bibr\" target=\"#b11\">Hochreiter and Schmidhuber (1997)</ref> for details. Briefly, when mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  found helpful for providing context-aware recommendations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, a text recomme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are control flow checking is to partition the program into basic blocks (branch-free parts of code) <ref type=\"bibr\" target=\"#b14\">[14]</ref>. For each block a deterministic signature is calculated an > and On-line control flow error detection using relationship signatures among basic blocks (RSCFC) <ref type=\"bibr\" target=\"#b14\">[14]</ref>.</p><p>ECCA, firstly, assigns a unique prime number identi rget=\"#b15\">[15]</ref> technique to the original code, \uf06c a safe one, obtained by applying the RSCFC <ref type=\"bibr\" target=\"#b14\">[14]</ref> technique to the original code, \uf06c a safe one, obtained by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cedure. We learn residuals only and use extremely high learning rates (10 4 times higher than SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref>) enabled by adjustable gradient clipping. Our proposed metho ely, random forest <ref type=\"bibr\" target=\"#b20\">[18]</ref> and convolutional neural network (CNN) <ref type=\"bibr\" target=\"#b8\">[6]</ref> have also been used with large improvements in accuracy.</p> 8\">[6]</ref> have also been used with large improvements in accuracy.</p><p>Among them, Dong et al. <ref type=\"bibr\" target=\"#b8\">[6]</ref> has demonstrated that a CNN can be used to learn a mapping f  are highly correlated. Moreover, our initial learning rate is 10 4 times higher than that of SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref>. This is enabled by residual-learning and gradient clipping. d reconstruction. Filters of spatial sizes 9 \u00d7 9, 1 \u00d7 1, and 5 \u00d7 5 were used respectively.</p><p>In <ref type=\"bibr\" target=\"#b8\">[6]</ref>, Dong et al. attempted to prepare deeper models, but failed  ce. We successfully use 20 weight layers (3 \u00d7 3 for each layer). Our network is very deep (20 vs. 3 <ref type=\"bibr\" target=\"#b8\">[6]</ref>) and information used for reconstruction (receptive field) i  for Very Deep Networks Training deep models can fail to converge in realistic limit of time. SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref> fails to show superior performance with more than three weig  a network to converge within a week on a common GPU. Looking at Fig. <ref type=\"figure\">9</ref> of <ref type=\"bibr\" target=\"#b8\">[6]</ref>, it is not easy to say their deeper networks have converged  n of 200 images from Berkeley Segmentation Dataset <ref type=\"bibr\" target=\"#b18\">[16]</ref>. SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref> uses a very large ImageNet dataset.</p><p>We use 291 images   The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al <ref type=\"bibr\" target=\"#b8\">[6]</ref> in their paper based on a GPU implementation.</p><p>In Figur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: with the success of deep neural networks (DNNs), some researches have applied DNNs to precipitation <ref type=\"bibr\" target=\"#b20\">[21]</ref> and radar echo <ref type=\"bibr\" target=\"#b31\">[32]</ref> n ation and intensity of rain and snow. To solve the problem of spatiotemporal dependency, Shi et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> developed the conventional LSTM and propose convolutional  , with a goal to overcome the drawbacks of FC-LSTM in handling spatial-temporal data such as videos <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Specifically, in the ConvLSTM network, the fully-connecte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eturn-into-libc attack by allowing the attacker arbitrary computation without calling any functions <ref type=\"bibr\" target=\"#b7\">[8]</ref>. In a traditional return-into libc attack, an attacker could. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b27\">[1]</ref>) or lack a clear objective function tailored for network embedding (e.g., <ref type=\"bibr\" target=\"#b42\">[16]</ref>). We anticipate that a new model with a carefully designed e for both undirected and directed graphs.</p><p>The most recent work related with ours is DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>, which deploys a truncated random walk for social network  /ref> . The Flickr network is denser than the Youtube network (the same network as used in DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>). (3) Citation Networks. Two types of citation networks ar rk can be represented as an affinity matrix, and is able to represent each vertex with a \u2022 DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>. DeepWalk is an approach recently proposed for social netw r\" target=\"#b39\">[13]</ref>. For other networks, the dimension is set as 128 by default, as used in <ref type=\"bibr\" target=\"#b42\">[16]</ref>. Other default settings include: the number of negative sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to the minimal feature engineering requirements, which contributes to a higher domain independence <ref type=\"bibr\" target=\"#b19\">(Yadav and Bethard, 2018)</ref>. The CharWNN model <ref type=\"bibr\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sian distributions instead of vectors <ref type=\"bibr\" target=\"#b9\">[10]</ref>, use an auto-encoder <ref type=\"bibr\" target=\"#b30\">[31]</ref>, or train an encoder by maximizing the mutual information . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t works are no longer limited to model structure, but considers sample-based knowledge distillation <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. In this paper, we tudy rather than the past knowledge distillation approaches such as considering the level of sample <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> and model structur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t, instead of using the QueueRunner, we use a more efficient data queue using tf.data in Tensorflow <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Second, instead of pre-calculating information about room erver which reads speech utterance and transcript data from sharded TFRecords defined in Tensorflow <ref type=\"bibr\" target=\"#b37\">[38]</ref>. The TFRecord format is a simple format in Tensorflow for   data are stored in sharded TFRecords. The data pipeline is implemented using tf.data in Tensorflow <ref type=\"bibr\" target=\"#b37\">[38]</ref>, and contains the data augmentation and feature extraction e=\"bibr\" target=\"#b54\">[55]</ref>, which is implemented as tf.clip by global norm API in Tensorflow <ref type=\"bibr\" target=\"#b37\">[38]</ref>. We use six layers of encoders and one layer of decoder fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b9\">(Kikuchi et al., 2016;</ref><ref type=\"bibr\" target=\"#b4\">Fan et al., 2017;</ref><ref type=\"bibr\" target=\"#b25\">Scarton and Specia, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Nishiha ummary more focused on a given named entity <ref type=\"bibr\" target=\"#b4\">(Fan et al., 2017)</ref>. <ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref> and <ref type=\"bibr\" target=\"#b17\">Ni t\" n=\"2\" xml:id=\"foot_1\">We did not investigate predicting ratios on a per sentence basis as done by<ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref>, and leave this for future work. End-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targe >19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> for GP. Very recently, the authors of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> find that the vertex-cut methods can achieve better performa rtex-cut has attracted more and more attention from DGC research community. For example, PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> adopts a random vertex-cut method and two greedy variants fo sly guarantee good workload balance. \u2022 DBH can be implemented as an execution engine for PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and hence all PowerGraph applications can be seamlessly sup hines. Hence, |A(v)| is the number of replicas of v among different machines. Similar to PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>, one of the replicas of a vertex is chosen as the master and  the \u03b1 is, the more skewed a graph will be. This power-law degree distribution makes GP challenging <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Although vertex-cut methods can achieve better performance  though vertex-cut methods can achieve better performance than edge-cut methods for power-law graphs <ref type=\"bibr\" target=\"#b5\">[6]</ref>, existing vertex-cut methods, such as random method in Power ysis for our DBH method. For comparison, the random vertex-cut method (called Random) of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> and the grid-based constrained solution (called Grid) of Gra ly to the p machines via a randomized hash function. The result can be directly got from PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Lemma 1. Assume that we have a sequence of n vertices {v i   theorem says that our DBH method has smaller expected replication factor than Random of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Next we turn to the analysis of the balance constrain \"5.2\">Baselines and Evaluation Metric</head><p>In our experiment, we adopt the Random of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> and the Grid of GraphBuilder [8]<ref type=\"foot\" target=\"#fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  wireless spectrum allocation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and wireless crowdsourcing <ref type=\"bibr\" target=\"#b13\">[14]</ref>.</p><p>The celebrated VCG mechanism <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hesizing it from scratch. It is most similar to recent work on sequence-tosequence voice conversion <ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" t #b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref>. <ref type=\"bibr\" target=\"#b15\">[16]</ref> uses a similar end-toend model, conditioned on speaker ide dard speech. In the future, we plan to test it on other speech disorders, and adopt techniques from <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> to preserve the sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: exibility of the sentence. The second class of methods inspired by Neural Machine Translation (NMT) <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref> map video sequence t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ts were trained using temporal-difference learning <ref type=\"bibr\" target=\"#b41\">41</ref> in chess <ref type=\"bibr\" target=\"#b42\">42,</ref><ref type=\"bibr\" target=\"#b43\">43</ref> , checkers <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t-flips in <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, a newly proposed Bit-Flip Attack (BFA) <ref type=\"bibr\" target=\"#b16\">[17]</ref> whose progressive bit searching algorithm can successfully ial Weight Attack</head><p>The bit-flip based adversarial weight attack, aka. Bit-Flip Attack (BFA) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, is an adversarial attack variant which performs weight fa loss increment. Thus, the bit searching in iteration i can be formulated as an optimization process <ref type=\"bibr\" target=\"#b16\">[17]</ref>:</p><formula xml:id=\"formula_0\">max { Bi l } L f x; { Bi l e acceleration of modern AI applications. To clarify, we use the same threat model as in prior work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which is listed in Table <ref type=\"table\">1</ref>.</p></ cted in Fig. <ref type=\"figure\" target=\"#fig_2\">2</ref>, the progressive bit search proposed in BFA <ref type=\"bibr\" target=\"#b16\">[17]</ref> is prone to identify vulnerable bit in the weight whose ab .   BFA Configuration. To evaluate the effectiveness of the proposed defense methods, the code from <ref type=\"bibr\" target=\"#b16\">[17]</ref> is utilized with further modification. The number of bit-f  trials. Note that, all the quantized DNN reported hereafter still uses the uniform quantizer as in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, but with quantization-aware training instead of post-trai ns/1.0\"><head n=\"5.3.\">Comparison of Alternative Defense Methods</head><p>Adversarial weight attack <ref type=\"bibr\" target=\"#b16\">[17]</ref> is a recently developed security threat model for modern D Trained adversarial defense <ref type=\"bibr\" target=\"#b14\">[15]</ref> with strong weight attack BFA <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Again, adversarial input defense fails to defend BFA, req. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ediction accuracy. Similar attention mechanisms have been proposed for natural image classification <ref type=\"bibr\" target=\"#b10\">[11]</ref> and captioning <ref type=\"bibr\" target=\"#b0\">[1]</ref> to  ntributions of this work can be summarised as follows: \u2022 We take the attention approach proposed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> a step further by proposing grid-based gating that allows   intermediate space. In image captioning <ref type=\"bibr\" target=\"#b0\">[1]</ref> and classification <ref type=\"bibr\" target=\"#b10\">[11]</ref> tasks, the  softmax activation function is used to normali n. This results experimentally in better training convergence for the AG parameters. In contrast to <ref type=\"bibr\" target=\"#b10\">[11]</ref> we propose a grid-attention technique. In this case, gatin  the feature-maps and map them to lower dimensional space for the gating operation. As suggested in <ref type=\"bibr\" target=\"#b10\">[11]</ref>, low-level feature-maps, i.e. the first skip connections,  <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" ta [2,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and more recently applied to image classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. In <ref type=\"bib  was the top-performer in the ILSVRC 2017 image classification challenge. Self-attention techniques <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> have been proposed tention is used in <ref type=\"bibr\" target=\"#b32\">[33]</ref> to capture long range dependencies. In <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> self-attention is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e-based Masked Language Model task is inspired by the Masked Language Model (MLM) objective of BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, and semantic mask for ASR task <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: veral extensions of our own. Surprisingly, we find that a single-parameter variant of Platt scaling <ref type=\"bibr\" target=\"#b40\">(Platt et al., 1999</ref>) -which we refer to as temperature scaling  od P(D | S = s). This allows us to compute P(qte | pte , D) for any test input.</p><p>Platt scaling <ref type=\"bibr\" target=\"#b40\">(Platt et al., 1999)</ref> is a parametric approach to calibration, u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atasets, CNNs have achieved state-of-the-art results in SISR <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" tar ainly focus on designing a deeper or wider network structure <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b13\">13,</ref><ref type=\"bibr\" tar tion. Some loss functions have been widely used, such as L 2 <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b29\">29,</ref><ref type=\"bibr\" tar (SRCNN) for image SR, which achieves impressive performance. Later, Kim et al. designed deeper VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[13]</ref> with mo e-of-the-art CNN-based SR methods: SR-CNN [1], FSRCNN <ref type=\"bibr\" target=\"#b3\">[3]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref>, LapSRN <ref type=\"bibr\" target=\"#b14\">[14]</ref>, Mem-Net SRCNN <ref type=\"bibr\" target=\"#b2\">[2]</ref>, FSRCNN <ref type=\"bibr\" target=\"#b3\">[3]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref>, IR-CNN <ref type=\"bibr\" target=\"#b35\">[35]</ref>, SRMD <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the activations of deep neural networks. For this, we consider the \"natural pre-image\" technique of <ref type=\"bibr\" target=\"#b20\">[21]</ref>, whose goal is to characterize the invariants learned by a n untrained deep convolutional generator can be used to replace the surrogate natural prior used in <ref type=\"bibr\" target=\"#b20\">[21]</ref> (the TV norm) with dramatically improved results. Since th antic segmentation) is highly detrimental.</p><p>Natural pre-image. The natural pre-image method of <ref type=\"bibr\" target=\"#b20\">[21]</ref> is a diagnostic tool to study the invariances of a lossy f e obtained by restricting the pre-image to a set X of natural images, called a natural pre-image in <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>In practice, finding points in the natural pre-imag inding points in the natural pre-image can  Inversion with deep image prior Inversion with TV prior <ref type=\"bibr\" target=\"#b20\">[21]</ref> Pre-trained deep inverting network <ref type=\"bibr\" target  on ImageNet ISLVRC) using three different regularizers: the Deep Image prior, the TV norm prior of <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and the network trained to invert representations on a ho ne by regularizing the data term similarly to the other inverse problems seen above. The authors of <ref type=\"bibr\" target=\"#b20\">[21]</ref> prefer to use the TV norm, which is a weak natural image p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#foot_0\">3</ref>  <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. They analyzed the relationship between internal properties bibr\" target=\"#b4\">[5]</ref>, narcissism <ref type=\"bibr\" target=\"#b6\">[7]</ref>, self-presentation <ref type=\"bibr\" target=\"#b12\">[13]</ref> and the corresponding user profiles on Facebook.</p><p>We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ral angle is a commonly used distance metric to measure the difference between two spectral vectors <ref type=\"bibr\" target=\"#b22\">(Kruse et al., 1993)</ref>. The reflectance of individual pixel is re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: end-to-end models. Previous approaches in general had long short-term memory neural networks (LSTM) <ref type=\"bibr\" target=\"#b2\">[3]</ref> or time-delay neural networks <ref type=\"bibr\" target=\"#b3\"> experiments with similar depth suggest that self-attention performs competitively compared to LSTMs <ref type=\"bibr\" target=\"#b2\">[3]</ref> or TDNNs <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The former. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: me works have shown that users can reduce their costs by using spot rather than on-demand instances <ref type=\"bibr\" target=\"#b26\">[28,</ref><ref type=\"bibr\" target=\"#b35\">37]</ref>, they only conside. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar  that causes the performance degradation in previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>.</p><p>In this pape  state-of-the-arts for defending against adversarial attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar ing with random normal perturbations) or adversarial noise <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, fail to improve ac pe=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Meanwhile, recent works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" ta etter recognition than the vanilla training baseline. These results contradict previous conclusions <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" ta -1 accuracy on ImageNet, which beats the vanilla training baseline by 0.6%. However, previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> show adversarial t r\" target=\"#b15\">16]</ref> show adversarial training always degrades performance.</p><p>Compared to <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, we make two chang ting noise. However, all previous attempts, by augmenting either with random noise (e.g., Tab. 5 in <ref type=\"bibr\" target=\"#b17\">[18]</ref> shows the result of training with random normal perturbati ojection step in PGD; or (2) we skip the random noise initialization step in PGD, turn it to I-FGSM <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Other attack hyper-parameters are unchanged: the maximum  Vanilla Training 81.7 83.7 84.5 PGD <ref type=\"bibr\" target=\"#b22\">[23]</ref> 81.8 84.3 85.2 I-FGSM <ref type=\"bibr\" target=\"#b17\">[18]</ref> 81.9 84. In Sec. 5.3, we show that adversarial training ca re of adversarial examples and clean images, as suggested in <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>,</p><formula xml:id=\"formula_3\">arg min \u03b8 E (x,y)\u223cD L(\u03b8, x, al and clean domains. However, as observed in former studies <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, directly optimizing Eq. ( <ref type=\"formula\" target=\"#for  stronger performance than the adversarial training baseline <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Besides, compared to the fine-tuning strategy in Sec. 3, A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: for labeling when there are already some labeled samples. They include uncertainty sampling methods <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tional machine learning models are simple but have yield strong baselines. For example, Pang et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> proposed a SVM categorization model based on n-gram approach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ectures (e.g., NASNet, AmoebaNet) are not efficient for inference. Recent hardwareaware NAS methods <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Tan et al., 201 ers and skip the last N \u2212 D layers, rather than keeping any D layers as done in current NAS methods <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Wu et al., 2019 nd input image size<ref type=\"foot\" target=\"#foot_1\">2</ref> . We also build a latency lookup table <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref> on each target hardware platform to predict t forms (Figure <ref type=\"figure\" target=\"#fig_7\">7</ref>) using the ProxylessNAS architecture space <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref>. OFA consistently improves the trade-off betw ). It is impossible for previous NAS methods <ref type=\"bibr\" target=\"#b29\">(Tan et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Cai et al., 2019)</ref> due to the prohibitive training cost.</p><p>Re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: vely search the tree-structured architecture space. Motivated by these AutoML frameworks, He et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> leveraged the reinforcement learning to automatically prun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref> to motion regression <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b46\">47]</ref>. However, DNNs are a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ks, including information retrieval <ref type=\"bibr\">[Chen et al., 2015]</ref>, relation extraction <ref type=\"bibr\" target=\"#b0\">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b22\">(Sutton et al. 2000)</ref> and the monto-carlo based policy gradient method <ref type=\"bibr\" target=\"#b27\">(Williams 1992</ref>) to sample M action-state trajectories, based on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uential data, have seen limited use on algorithms operating on a single static image. Socher et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> used a convolutional network in a separate stage to first . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion information to the model. These position encodings can be a deterministic function of position <ref type=\"bibr\" target=\"#b7\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b10\">Vaswani . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a non-empty intersection contained in a set of measure zero <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>.</p><p>The optimal transport (OT) distance is an alternativ s on a low dimensional manifold of the input embedding space <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>, which is the case for natural images. It has been widely a ng the difficulties encountered in the original GAN training <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. This technique has been further extended to generating dis  generative modeling <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar s coupled over all input samples. Wasserstein GAN and OT-GAN <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Generative Adversar act minimization of Eqn.(4) over T is intractable in general <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  learning methods have been proposed to address this problem <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. However, they either do not apply to bipartite graphs or a  target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> try to utilize GCN to do unsupervised learning on graphs by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: more sophisticated operator \u03c4 , such as borrowing the structure of descriptors in manifold geometry <ref type=\"bibr\" target=\"#b9\">(Kokkinos et al., 2012;</ref><ref type=\"bibr\" target=\"#b13\">Monti et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>) have been proposed. Athalye et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> showed that obfuscated gradients give a false sense of robus that models trained using SADS are robust and does not exhibit obfuscated gradients (Athalye et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> demonstrated that models exhibiting obfuscated gradients are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n technique for achieving truthfulness in online auctions is based on the concept of a supply curve <ref type=\"bibr\" target=\"#b21\">[22]</ref>, as applied by Zhang et al. <ref type=\"bibr\" target=\"#b3\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> and speech recognition <ref type=\"bibr\" target=\"#b11\">[12]</ref> to machine translation <ref type=\"bibr\" target=\"#b23\">[24]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ns and proposed Definition 1 which emphasizes the reasoning engine of knowledge graphs. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> proposed a definition as a multi-relational graph in Definit ormation into an ontology and applies a reasoner to derive new knowledge. Definition 2 (Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref>). A knowledge graph is a multirelational graph composed of e \"#b5\">[6]</ref>, Chinese knowledge graph construction <ref type=\"bibr\" target=\"#b9\">[10]</ref>, KGE <ref type=\"bibr\" target=\"#b7\">[8]</ref> or KRL <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The latter </ref> presented KRL in a linear manner, with a concentration on quantitative analysis. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> categorized KRL according to scoring functions, and specific s of auxiliary information for KRL such as attributes, relation path and logical rules. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> gave a detailed review on these information. This paper disc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecifically, the features learned by the two baseline CNNs and the IL-CNN are visualized using t-SNE <ref type=\"bibr\" target=\"#b41\">[42]</ref>, which is widely employed to visualize high dimensional da. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ias by using corpus level constraints, but is only practical for models with specialized structure. <ref type=\"bibr\" target=\"#b13\">Kusner et al. (2017)</ref> propose the method based on causal inferen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion system to utilize the entire MEDLINE data set. By using state-of-the-art tools, such as ToPMine <ref type=\"bibr\" target=\"#b12\">[16]</ref> and FastText <ref type=\"bibr\" target=\"#b6\">[9]</ref>, we a Latent Dirichlet Allocation (LDA) <ref type=\"bibr\" target=\"#b5\">[8]</ref> and topical phrase mining <ref type=\"bibr\" target=\"#b12\">[16]</ref>, along with other data mining techniques to conceptually l word phrases from that corpus such as \"asthma a ack,\" allowing us to treat phrases as single tokens <ref type=\"bibr\" target=\"#b12\">[16]</ref>. Next, we send the corpus through FastText, the most recen urned to the UMLS SPECIALIST NLP toolset <ref type=\"bibr\" target=\"#b0\">[1]</ref> as well as ToPMine <ref type=\"bibr\" target=\"#b12\">[16]</ref> and FastText <ref type=\"bibr\" target=\"#b6\">[9,</ref><ref t o . It is also important to note that we modify the version of ToP-Mine distributed by El-Kishky in <ref type=\"bibr\" target=\"#b12\">[16]</ref> to allow phrases containing numbers, such as gene names li  over weigh a specialized language. However, phrase mining approaches that recover n-grams, such as <ref type=\"bibr\" target=\"#b12\">[16]</ref>, produce accurate methods without limiting the dictionary.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d video panoramas <ref type=\"bibr\" target=\"#b1\">[Agarwala et al. 2005]</ref>. Others have confirmed <ref type=\"bibr\" target=\"#b15\">[Levin et al. 2004;</ref><ref type=\"bibr\" target=\"#b31\">Zomet et al. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tate-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by <ref type=\"bibr\" target=\"#b16\">[16]</ref>.</p></div> \t\t\t</abstract> \t\t</profileDesc> \t</teiHeader> \t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  alternative modeling unit to phonemes for speech processing <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raining. However, in this case three-fold data augmentation was applied prior to feature extraction <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the acoustic features comprised 40-dimensional MFCCs (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> embedding trained  g we refer to in this paper is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> embedding, which h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b21\">(Salimans &amp; Kingma, 2016)</ref> with momentum 0.999 to all of them. We used leaky ReLU <ref type=\"bibr\" target=\"#b13\">(Maas et al., 2013)</ref> with \u03b1 = 0.1 as the non-linearity, and chos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blished work on MCTS, to provide the reader Fig. <ref type=\"figure\">1</ref>. The basic MCTS process <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>with the tools to solve new problems using MCTS and basic MCTS process is conceptually very simple, as shown in Figure <ref type=\"figure\">1</ref> (from <ref type=\"bibr\" target=\"#b16\">[17]</ref>). A tree 1 is built in an incremental and asymmetric manne t two moves and uses LGR-1 if there is no LGR-2 entry for the last two moves.</p><p>Baier and Drake <ref type=\"bibr\" target=\"#b16\">[17]</ref> propose an extension to LGR-1 and LGR-2 called Last Good R ests using the Last Good Reply heuristic (6.1.8) to inform simulations, modified by Baier and Drake <ref type=\"bibr\" target=\"#b16\">[17]</ref> to include the forgetting of bad moves. Most programs use . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ective defense mechanisms against adversarial perturbation <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>.</p><p>We evaluate a publicly available model trained throu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  loss from the image-space to a higher-level feature space of an object recognition system like VGG <ref type=\"bibr\" target=\"#b48\">[49]</ref>, resulting in sharper results despite lower PSNR values.</ etwork once to get the result. The exclusive use of 3\u00d73 filters is inspired by the VGG architecture <ref type=\"bibr\" target=\"#b48\">[49]</ref> and allows for deeper models at a low number of parameters  map \u03c6, we use a pre-trained implementation of the popular VGG-19 network <ref type=\"bibr\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. It consists of stacked convolutions coupled with pooling l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hieves better performance than the state-of-theart methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing the depth without adding any parameters,  eNet <ref type=\"bibr\" target=\"#b19\">[21]</ref>, Kim et al. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose two very deep convolutional networks for SR, both s ameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and non-DL <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref typ  testing sets, by citing the results of prior methods from <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. The two DRRN models outperforms all existing methods in al chieves better performance than the state-of-theart methods<ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing the depth without adding any parameters,   the performance and significantly outperforms VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30 <ref type=\"bibr\" target=\"#b16\">[17]</ref> by 0.3  the other hand, to control the model parameters, the Deeply-Recursive Convolutional Network (DRCN) <ref type=\"bibr\" target=\"#b13\">[14]</ref> introduces a very deep recursive layer via a chain structu ><p>(2) Recursive learning of residual units is proposed in DRRN to keep our model compact. In DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, a deep recursive layer (up to 16 convolutional recursions et <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> illustrat type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The blue dashed box refers to a recursive layer, among wh esNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and DRRN. U, d, T, and B are the numbers of residual units cursive block.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3.\">DRCN</head><p>DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> is motivated by the observation that adding more weight la s a given image x as feature maps H 0 . The inference net f 2 (H 0 ) stacks T recursions (T = 16 in <ref type=\"bibr\" target=\"#b13\">[14]</ref>) in a recursive layer, with shared weights among these rec \">[3]</ref>. Very deep models (d \u2265 20) include VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, RED <ref type=\"bibr\" target=\"#b16\">[17]</ref> and DRRN wi es the performance and significantly outperforms VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30<ref type=\"bibr\" target=\"#b16\">[17]</ref> by 0.37  type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref>. The blue dashed box refers to a recursive layer, among wh nt CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar mparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b21\">23]</ref>, we crop pixels nea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Cedar <ref type=\"bibr\" target=\"#b48\">[48]</ref>. The Lisp machine had a real-time garbage collector <ref type=\"bibr\" target=\"#b5\">[5]</ref>.</p><p>A number of research kernels are written in high-leve  or closures awkward <ref type=\"bibr\" target=\"#b26\">[26]</ref>.</p><p>Concurrent garbage collectors <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es' velocities and positions accordingly. Other techniques, such as \"position-based dynamics\" (PBD) <ref type=\"bibr\" target=\"#b23\">(M\u00fcller et al., 2007)</ref> and \"material point method\" (MPM) <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: able resources from better-resourced but unrelated languages <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, a system using a hy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ere has been a surge of interest in learning graph representations from data. For example, DeepWalk <ref type=\"bibr\" target=\"#b19\">[20]</ref>, one recent model, transforms a graph structure into a sam thod, which used stochastic gradient descent to optimize matrices from large graphs. Perozzi et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> presented an approach, which transformed graph structure i ep relational information and tuning the threshold of maximum number of vertices.</p><p>2. DeepWalk <ref type=\"bibr\" target=\"#b19\">[20]</ref>. DeepWalk is a method that learns the representation of so uction strategy for vertices with small degrees to achieve the optimal performance. As mentioned in <ref type=\"bibr\" target=\"#b19\">[20]</ref>, for DeepWalk and E-SGNS, we set window size as 10, walk l lti-label classification task by regarding the learned representations as features.</p><p>Following <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, we use the LibLin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d by deep learning based speech enhancement and separation <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar  as in <ref type=\"bibr\" target=\"#b20\">[21]</ref>, proposed in the same conference. A follow-up work <ref type=\"bibr\" target=\"#b18\">[19]</ref> of <ref type=\"bibr\" target=\"#b22\">[23]</ref> supplies clea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: igure\" target=\"#fig_0\">1</ref>) is found to be more effective in advanced distributed graph engines <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target ly, researchers demonstrated that edge partitioning performs better on many large real-world graphs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. This important findin nding attracts great interests in edge partitioning recently <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targe \">17]</ref>. Edge partitioning has been widely adopted in recent graph systems including PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Spark GraphX <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and C tleneck in graph computing due to the intrinsic dependency and expensive random access in the graph <ref type=\"bibr\" target=\"#b5\">[6]</ref>. While it is relatively easy to solve the first issue by par ree distribution of power-law graphs, but it leverages little graph structure, like RAND. Oblivious <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a streaming algorithm that considers the distribution of  e the communication cost and execution time on distributed graph processing systems like PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> and PowerLyra <ref type=\"bibr\" target=\"#b3\">[4]</ref> in Sec  with six existing edge partitioners, including METIS <ref type=\"bibr\" target=\"#b7\">[8]</ref>, RAND <ref type=\"bibr\" target=\"#b5\">[6]</ref>, DBH <ref type=\"bibr\" target=\"#b16\">[17]</ref>, Oblivious <r D <ref type=\"bibr\" target=\"#b5\">[6]</ref>, DBH <ref type=\"bibr\" target=\"#b16\">[17]</ref>, Oblivious <ref type=\"bibr\" target=\"#b5\">[6]</ref>, HDRF <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and Sheep <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hat backpropagation improves neither accuracy nor detectability of a GCN-based GNN model. Li et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> empirically analyzed GCN models with many layers under the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: linical notes <ref type=\"bibr\" target=\"#b0\">(Alsentzer et al., 2019)</ref>, human phenotype-gene RE <ref type=\"bibr\" target=\"#b26\">(Sousa et al., 2019)</ref> and clinical temporal RE <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd pursued in natural language processing <ref type=\"bibr\" target=\"#b9\">[10]</ref>, computer vision <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and other domains. To date, the most powerful solution is e how to define (dis)similar instances.</p><p>Q2: Define (dis)similar instances. In computer vision <ref type=\"bibr\" target=\"#b16\">[17]</ref>, two random data augmentations (e.g., random crop, random  d by the graph encoder, the final d-dimensional output vectors are then normalized by their L2-Norm <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>A running example. We illustrate a running example  ffectively build and maintain the dictionary, such as end-to-end (E2E) and momentum contrast (MoCo) <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We discuss the two strategies as follows.</p><p>E2E sampl propagation. The parameters of f k (denoted by \u03b8 k ) are not updated by gradient descent. He et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> propose a momentum-based update rule for \u03b8 k . More formal  the dictionary, such as memory bank <ref type=\"bibr\" target=\"#b58\">[59]</ref>. Recently, He et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> show that MoCo is a more effective option than memory bank >Contrastive loss mechanisms. The common belief is that MoCo has stronger expression power than E2E <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and a larger dictionary size K always helps. We also obse r, the effect of a large dictionary size is not as significant as reported in computer vision tasks <ref type=\"bibr\" target=\"#b16\">[17]</ref>. For example, MoCo (K = 16384) merely outperforms MoCo (K   in Table <ref type=\"table\" target=\"#tab_6\">5</ref> in the Appendix. Momentum. As mentioned in MoCo <ref type=\"bibr\" target=\"#b16\">[17]</ref>, momentum m plays a subtle role in learning high-quality r tasets. For US-Airport, the best performance is reached by m = 0.999, which is the desired value in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, showing that building a consistent dictionary is importan  brings better performance. Moreover, we do not observe the \"training loss oscillation\" reported in <ref type=\"bibr\" target=\"#b16\">[17]</ref> when setting m = 0. GCC (MoCo) converges well, but the acc ings.</p><p>In computer vision, a large collection of work <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" tar  objectives for graph structured data. Inspired by the recent success of contrastive learning in CV <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref> and NLP <ref type= ats each instance as a distinct class of its own and learns to discriminate between these instances <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>. The promise is th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ices but it may incur the loss of information during the training process. To this end, inspired by <ref type=\"bibr\" target=\"#b3\">[Dai et al., 2016]</ref>, we transform the binary optimization problem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: operties that could be used to craft adversarial samples <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b28\">[30]</ref>, <ref type=\"bibr\" target=\"#b34\">[36]</ref>. Simply put, th ages that are unrecognizable to humans, but are nonetheless labeled as recognizable objects by DNNs <ref type=\"bibr\" target=\"#b28\">[30]</ref>. For instance, they demonstrated how a DNN will classify a e backpropagation procedure used during network training <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b28\">[30]</ref>, <ref type=\"bibr\" target=\"#b34\">[36]</ref>. This approach . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"bibr\" target=\"#b1\">Byrne, 2007;</ref><ref type=\"bibr\" target=\"#b10\">Finkel and Manning, 2009;</ref><ref type=\"bibr\" target=\"#b25\">Lu and Roth, 2015;</ref><ref type=\"bibr\" target=\"#b16\">Katiyar and Ca trees. They made the assumption that one mention is fully contained by the other when they overlap. <ref type=\"bibr\" target=\"#b25\">Lu and Roth (2015)</ref> proposed to use mention hyper-graphs for rec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t adopts the densely connected structure in a global way (i.e., across the memory blocks). In Secs. <ref type=\"bibr\" target=\"#b2\">3</ref>  For the curve of the mth block, the left (m \u00d7 64) elements de ) The average and variance of the weight norms become smaller as the memory block number increases. <ref type=\"bibr\" target=\"#b2\">(3)</ref> In general, the short-term memories from the last recursion . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: intent-labeled speech data, and such data is usually scarce. <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> address this problem using a curriculum and transfer learni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by changing the parameters of the optimization algorithm to depend on the network activation values <ref type=\"bibr\" target=\"#b22\">(Wiesler et al., 2014;</ref><ref type=\"bibr\" target=\"#b14\">Raiko et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rformance considerably. The effectiveness of those techniques has shown on sets of independent jobs <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targ odern CMP on the performance of contemporary multithreaded applications. Many previous explorations <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targ for improving the performance of these programs, a contrast to previous results on independent jobs <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> and server programs  shared cache has been job co-scheduling including thread clustering. Many job co-scheduling studies <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tensions such as for example the use of residual connections <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, dense connections <ref type=\"bibr\" target=\"#b4\">[5]</ref> or b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> we favor this formulation over other variants <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. The dice loss is im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: puter vision methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar xperiments, we follow the protocol used in previous papers <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, which uses unseen  #foot_0\">1</ref> , 10 2 ]), where the implementation of traditional classifiers becomes challenging <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>.</p><p>Arguably, t rix of pairwise distances of a subset of the training set (i.e., the mini-batch) allows Song et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> to design of a new loss function that integrates all posit > (with and without FANNG <ref type=\"bibr\" target=\"#b4\">[5]</ref>), (2) lifted structured embedding <ref type=\"bibr\" target=\"#b20\">[21]</ref>, (3) N-pairs metric loss <ref type=\"bibr\" target=\"#b19\">[2 hod, and ( <ref type=\"formula\">5</ref>) ), we use the same training and test set split described in <ref type=\"bibr\" target=\"#b20\">[21]</ref> across all datasets. Specifically, the means CUB200-2011 < target=\"#b22\">[23]</ref> weights and randomly initialize the final fully connected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>. We set the embedding size to 64 <ref type=\"bibr\" target=\" nnected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>. We set the embedding size to 64 <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the learning rate for the randomly initialized fully c omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>For the experiments using triplet combined with glo =\"bibr\" target=\"#b22\">[23]</ref> and randomly initialize the final fully connected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref> . The learning rate for the randomly initialized fully con omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: zing GCN, S-GCN, ChebNet and related methods. Our architecture is analogous to the inception module <ref type=\"bibr\" target=\"#b43\">Szegedy et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b22\">Kazi et a ion ( <ref type=\"formula\" target=\"#formula_6\">4</ref>) is analogous to the popular Inception module <ref type=\"bibr\" target=\"#b43\">Szegedy et al. (2015)</ref> for classic CNN architectures (Figure <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ngle-sentence relation approaches, some previous works exist on cross-sentence relation extraction. <ref type=\"bibr\" target=\"#b25\">[26]</ref> proposes to construct cross-sentence relation data for ent ge number of unrelated and non-informative negative examples, we follow the minimumspan strategy in <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and limit sampled negative candidate pairs to the co-occu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>Our hybrid pointer-generator network facilitates copying words from the source text via pointing <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref>, which improves accuracy and handling of twork</head><p>Our pointer-generator network is a hybrid between our baseline and a pointer network <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref>, as it allows both copying words via poi plore sentence fusion using dependency trees.</p><p>Pointer-generator networks. The pointer network <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref> is a sequence-tosequence model that uses. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b22\">(Madotto et al., 2018)</ref>, but also some early works use text as additional information <ref type=\"bibr\" target=\"#b44\">(Xie et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">An et al., 201 target=\"#b37\">(Sun et al., 2019a)</ref> combines the advantages of both of them. Among these works, <ref type=\"bibr\" target=\"#b44\">Xie et al. (2016)</ref> propose to utilize entity descriptions as an . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ect from El-Kishky et al., is focused on discovering multi-word phrases from a large corpus of text <ref type=\"bibr\" target=\"#b16\">[20]</ref>. is project intelligently groups unigrams together to crea  is the most common topic modeling process and PLDA+ is a scalable implementation of this algorithm <ref type=\"bibr\" target=\"#b16\">[20,</ref><ref type=\"bibr\" target=\"#b24\">28]</ref>. Developed by Zhiy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ss in entity type and new fact predictions <ref type=\"bibr\" target=\"#b16\">[Nickel et al., 2012</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016</ref><ref type=\"bibr\" target=\"#b3\">, Dettmer bedding methods,viz., ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> and ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>, which do not make use of any ontologi raining can be done for the refinement task with a negative log-likelihood loss function as follows <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>.</p><formula xml:id=\"formula_0\">L(G) = ods can also be used to predict type labels of entities (the typeOf relation). We work with ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3 ., subtype and subproperty information-and also shows that state-of-the-art embeddings like ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>, <ref type=\"bibr\">SimplE [Kazemi and P valuate the performance of TypeE-X models in the KG refinement task, and compare them with Com-plEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3 \" target=\"#b15\">[Nickel et al., 2011</ref><ref type=\"bibr\" target=\"#b21\">, Socher et al., 2013</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016]</ref>.</p><p>An important step in learning . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#fig_0\">1</ref>, these are mining-based methods inspired by previous relation extraction methods <ref type=\"bibr\" target=\"#b34\">(Ravichandran and Hovy, 2002)</ref>, and paraphrasing-based methods t ased Generation</head><p>Our first method is inspired by template-based relation extraction methods <ref type=\"bibr\" target=\"#b34\">(Ravichandran and Hovy, 2002)</ref>, which are based on the observati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the dependency path between the entities might be even more indicative of the relation, as noted by <ref type=\"bibr\" target=\"#b40\">Toutanova et al. (2015)</ref>. It is quite possible that using these . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref>  Register traffic characteristics. We collect a number of characteristics concerning registers <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Our first characteristic is the average number of input ope. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: elow.</p><p>Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN <ref type=\"bibr\" target=\"#b15\">[16]</ref>. 3D-CNN has achieved state-of-the-art results in object cl /ref>. 3D-CNN has achieved state-of-the-art results in object classification on tridimensional data <ref type=\"bibr\" target=\"#b15\">[16]</ref>. 3D-CNN not only extracts features from each image frame, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t of user-level influence prediction models, most of which <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> consider complicate k, while external sources are assumed to be not present.</p><p>Problem 1. Social Influence Locality <ref type=\"bibr\" target=\"#b52\">[53]</ref> Social influence locality models the probability of v's ac get=\"#b53\">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The dataset is from <ref type=\"bibr\" target=\"#b52\">[53]</ref> and can be downloaded here. 7 The complete dataset contain  and the social action is defined to be whether a user retweets \"Higgs\" related tweets.</p><p>Weibo <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> Weibo 6 is the mos .</p><p>Data Preparation We process the above four datasets following the practice in existing work <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. More concretely, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ow-resolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance <ref type=\"bibr\" target=\"#b53\">[51,</ref><ref type=\"bibr\" target=\"#b28\">26,</ref><ref type=\"bibr\" ta <ref type=\"bibr\" target=\"#b28\">[26]</ref>, DBPN <ref type=\"bibr\" target=\"#b18\">[16]</ref>, and RCAN <ref type=\"bibr\" target=\"#b53\">[51]</ref>. However, these methods still suffer from the large space  nsists of several up-and down-sampling layers to iteratively produce LR and HR images. Zhang et al. <ref type=\"bibr\" target=\"#b53\">[51]</ref> propose the channel attention mechanism to build a deep mo nlike the baseline U-Net, we build each basic block using B residual channel attention block (RCAB) <ref type=\"bibr\" target=\"#b53\">[51]</ref> to improve the model capacity. Following <ref type=\"bibr\"  are 2 dual models for 4\u00d7 SR and 3 dual models for 8\u00d7 SR, respectively. Let B be the number of RCABs <ref type=\"bibr\" target=\"#b53\">[51]</ref> and F be the number of base feature channels. For 4\u00d7 SR, w ctionbased methods <ref type=\"bibr\" target=\"#b18\">[16,</ref><ref type=\"bibr\" target=\"#b27\">25,</ref><ref type=\"bibr\" target=\"#b53\">51]</ref>. Haris et al. <ref type=\"bibr\" target=\"#b18\">[16]</ref> pro ata, and augment the training data following the method in <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b53\">51]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b21\">(Samangouei et al., 2018)</ref> uses a Generative Adversarial Network <ref type=\"bibr\" target=\"#b8\">(Goodfellow et al., 2014a)</ref> to project samples onto the manifold . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , the content-based RS <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the collaborative filtering RS <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> are two widely used   prevalence of deep learning, neural networks are widely used. Neural collaborative filtering (NCF) <ref type=\"bibr\" target=\"#b6\">[7]</ref> first proposes to use the multi layer perceptron to approxim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type=\"bibr\" target=\"#b25\">(Peters et al., 2018)</ref>, GPT-2 <ref type=\"bibr\" target=\"#b26\">(Ra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ective that often comes from parallel corpus <ref type=\"bibr\" target=\"#b53\">(Zou et al., 2013;</ref><ref type=\"bibr\" target=\"#b16\">Gouws et al., 2015)</ref>. Recently, unsupervised approaches also hav. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  balance between effectiveness and efficiency. In this paper, we employ knowledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> which is a network compression technique by transferring th ulate the top-N recommendation problem. Then, we explain the concept of knowledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> and present rank distillation (RD) <ref type=\"bibr\" target= r proposed training strategies.</p><p>Temperature in the KD loss. One key factor of the original KD <ref type=\"bibr\" target=\"#b9\">[10]</ref> is to find a proper balance between the soft targets and ha ]</ref> is to find a proper balance between the soft targets and hard labels. To tackle this issue, <ref type=\"bibr\" target=\"#b9\">[10]</ref> introduces the notion of a temperature T . Although the sof. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  representation of publication and person name when training <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. However, our experimental study shows that such a straight asks, or using simple concatenation procedures when training <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. However, our exper s resulted from replacing the AM and PM modules with a concatenation procedure similar to Ma et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Hashimoto et al. <ref type=\"bibr\" target=\"#b5\">[6]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>.</p><p>Although consi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: edictors can achieve equivalent or higher prediction accuracies than conventional branch predictors <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>Once future control flow can be predicted at trace gr \">Trace Prediction.</head><p>We u s e t h e t r a c e p r e d i c t o r proposed by Jacobson et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Each trace is assigned a trace identifier obtained by combi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: osed approach is more practical because the training data is generally inaccessible to the attacker <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Our contributions can be summarized as follows:</p><p>\u2022 W  original training data. However, in practice the attacker often has no access to the training data <ref type=\"bibr\" target=\"#b31\">[32]</ref>. To overcome this limitation, Mopuri et al. propose to gen ome this limitation, Mopuri et al. propose to generate universal perturbation without training data <ref type=\"bibr\" target=\"#b31\">[32]</ref>. However, their approach is specifically designed for non- -agnostic) attacks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Our system extracts features from those scales using a similar concept to feature pyramid networks <ref type=\"bibr\" target=\"#b7\">[8]</ref>. From our base feature extractor we add several convolutiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Kipf &amp; Welling, 2016;</ref><ref type=\"bibr\" target=\"#b9\">Garcia Duran &amp; Niepert, 2017;</ref><ref type=\"bibr\" target=\"#b57\">Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Pan et al., 20 GAE (VGAE) <ref type=\"bibr\" target=\"#b23\">(Kipf &amp; Welling, 2016)</ref>, marginalized GAE (MGAE) <ref type=\"bibr\" target=\"#b57\">(Wang et al., 2017)</ref>, adversarially regularized GAE (ARGA) and V. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) <ref type=\"bibr\" target=\"#b25\">(Thorne et al., 2018a)</ref>. Experimental results show that the prop /ns/1.0\"><head n=\"4.1\">Dataset</head><p>We conduct our experiments on the large-scale dataset FEVER <ref type=\"bibr\" target=\"#b25\">(Thorne et al., 2018a)</ref>. The dataset consists of 185,455 annotat ed the top three results among 23 teams.</p><p>Existing methods mainly formulate FV as an NLI task. <ref type=\"bibr\" target=\"#b25\">Thorne et al. (2018a)</ref> simply concatenate all evidence together,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, as well as independent component analysis <ref type=\"bibr\" target=\"#b3\">[Cao et al. 2003]</ref>. <ref type=\"bibr\" target=\"#b4\">Cao et al. [2005]</ref> extract emotions using support vector machines lso present a user study rating the level of realism in emotion synthesis, covering several methods <ref type=\"bibr\" target=\"#b4\">[Cao et al. 2005;</ref><ref type=\"bibr\" target=\"#b27\">Liu and Osterman ing samples based on the apparent emotion <ref type=\"bibr\" target=\"#b0\">[Anderson et al. 2013;</ref><ref type=\"bibr\" target=\"#b4\">Cao et al. 2005;</ref><ref type=\"bibr\" target=\"#b11\">Deng et al. 2006;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch that it covers a predefined set of downsampling kernels <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>; using DNNs to capture only a natural-image prior which is  adation model and assumes that the downsampling kernels belong to a certain set of Gaussian filters <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Another approach  estimation methods <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Finally, we wo f>.</p><p>Finally, we would like to highlight major differences be-tween this paper and the work in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, whose \"kernel correction\" approach may be misunderstood a >[13]</ref>, whose \"kernel correction\" approach may be misunderstood as our \"correction filter\". In <ref type=\"bibr\" target=\"#b12\">[13]</ref>, three different DNNs (super-resolver, kernel estimator, a ned existing DNN methods (other than SRMD <ref type=\"bibr\" target=\"#b37\">[38]</ref>) can be used in <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Secondly, their approach is restricted by the offline tra ur approach. Thirdly, the concepts of these works are very different: The (iterative) correction in <ref type=\"bibr\" target=\"#b12\">[13]</ref> modifies the estimated downsampling kernel, while our corr erformed with the official code of each method. Unfortunately, such code has not been available for <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  be recovered exactly from k log d random linear projections (up to scaling and additive constants) <ref type=\"bibr\" target=\"#b0\">[2]</ref>. Since a one-hot encoding is a 1-sparse signal, the student . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as input.</p><p>Networks <ref type=\"bibr\" target=\"#b32\">[33]</ref> and Residual Networks (ResNets) <ref type=\"bibr\" target=\"#b10\">[11]</ref> have surpassed the 100-layer barrier.</p><p>As CNNs become d (or beginning) of the network. Many recent publications address this or related problems. ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Highway Networks <ref type=\"bibr\" target=\"#b32\">[33]</ uent layer. It changes the state but also passes on information that needs to be preserved. ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> make this information preservation explicit through additi tor that eases the training of these very deep networks. This point is further supported by ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref>, in which pure identity mappings are used as bypassing pat ng image recognition, localization, and detection tasks, such as ImageNet and COCO object detection <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Recently, stochastic depth was proposed as a way to succe =\"#b15\">[16]</ref>, which gives rise to the following layer transition: x \u2113 = H \u2113 (x \u2113\u22121 ). ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> add a skip-connection that bypasses the non-linear transfo ates that they do not suffer from overfitting or the optimization difficulties of residual networks <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>Parameter Efficiency. The results in Table <ref typ s, it typically has many more inputs. It has been noted in <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> that a 1\u00d71 convolution can be introduced as bottleneck laye a standard data augmentation scheme (mirroring/shifting) that is widely used for these two datasets <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta rget=\"#b11\">12]</ref>, and apply a single-crop or 10-crop with size 224\u00d7224 at test time. Following <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta  the same data augmentation scheme for training images as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, and apply a single. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend our prior work <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and propose a hybrid Transformer-LSTM based architecture. T ot only require external language models but also lead to a slow inference. To tackle this problem, <ref type=\"bibr\" target=\"#b0\">[1]</ref> has proposed long short term memory (LSTM)-based encoderdeco <p>In this work, we propose a hybrid Transformer-LSTM architecture which combines the advantages of <ref type=\"bibr\" target=\"#b0\">[1]</ref> and <ref type=\"bibr\" target=\"#b5\">[6]</ref>. It not only has l.</p><p>The paper is organized as follows. Section 2 describes baseline architectures mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref> and <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Then, the propo res 2.1. LSTM-based encoder-decoder architecture</head><p>A LSTM-based encoder-decoder architecture <ref type=\"bibr\" target=\"#b0\">[1]</ref>, denoted as A1 in the rest of this paper, consists of a Bidi ayers respectively. Figure <ref type=\"figure\">1</ref>: LSTM-based encoder-decoder architecture (A1) <ref type=\"bibr\" target=\"#b0\">[1]</ref>, where the decoder acts as an independent language model.</p ords, the LSTM acts as an independent language model that can be easily updated with text-only data <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= tune the transferred model. This avoids a so-called catastrophic forgetting problem as mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Specifically, at each training iteration, we mix a batch of cond step, the model is further fine-tuned with the labeled data of the target language. Similar to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we empirically found that the second step is necessary to i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.\">Background and Related Work</head><p>Previous work <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> describes support me ware support for thread activation and deactivation, as found in prior studies of thread scheduling <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. While those works u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, and knowledge distillation <ref type=\"bibr\" target=\"#b9\">[10 ref>). This structure is very similar to multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, in which different supervised tasks share the same input, as nowledge obtained from each task can be reused by the others <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. However, it is not u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a text word by word and stores the semantics of all the previous text in a fixed-sized hidden layer <ref type=\"bibr\" target=\"#b7\">(Elman 1990</ref>). The advantage of RecurrentNN is the ability to bet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: > identified particular hand gesture to be important to identify the act of deception. Cohen et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> found that fewer iconic hand gestures were a sign of a dec. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pproximate the convolutional filters by Chebyshev expansion of the graph Laplacian, and Kipf et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> propose a convolutional architecture via a first-order app arget=\"#b1\">2</ref> There are several candidate designs for the architecture of our model, e.g., GCN<ref type=\"bibr\" target=\"#b10\">[11]</ref> or GraphSAGE<ref type=\"bibr\" target=\"#b6\">[7]</ref>. Here  \"bibr\" target=\"#b10\">[11]</ref> or GraphSAGE<ref type=\"bibr\" target=\"#b6\">[7]</ref>. Here we use GCN<ref type=\"bibr\" target=\"#b10\">[11]</ref> as our base model.</note> \t\t\t<note xmlns=\"http://www.tei-c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b13\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b27\">Velickovic et al., 2018;</ref><ref type=\"bibr\" target=\"#b19\">Palm et al., 2018)</ref>, we propose an evidence reasoning network (E. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ethods have been presented, relying on different techniques such as matching networks and bi-LSTM ( <ref type=\"bibr\" target=\"#b13\">[14]</ref>) or memory-networks ( <ref type=\"bibr\" target=\"#b9\">[10]</ use the recent memory-augmented neural network, to integrate and store the new examples. Similarly, <ref type=\"bibr\" target=\"#b13\">[14]</ref> propose to rely on external memories for neural networks, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sic idea is very similar to instruction fetch in Multiscalar <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>: Multiscalar divides the sequential instruction stream into  i o n i s s i m i l a r t o t h a t u s e d b y Multiscalar <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. When a fragment is renamed, the hardware determines which  fragments. This is similar to the idea of traces <ref type=\"bibr\" target=\"#b19\">[20]</ref> or tasks <ref type=\"bibr\" target=\"#b20\">[21]</ref>, except that fragments are completely general, whereas the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b18\">(Hazan et al., 2016)</ref> for a collection of related work, and particularly the chapter <ref type=\"bibr\" target=\"#b25\">(Maddison, 2016)</ref> for a proof and generalization of this trick.<  n=1 \u03b1 k y \u2212\u03bb k ) n Thus Y d = X.</formula><p>2. Follows directly from (a) and the Gumbel-Max trick <ref type=\"bibr\" target=\"#b25\">(Maddison, 2016)</ref>.</p><p>3. Follows directly from (a) and the Gu \" target=\"#b25\">(Maddison, 2016)</ref>.</p><p>3. Follows directly from (a) and the Gumbel-Max trick <ref type=\"bibr\" target=\"#b25\">(Maddison, 2016)</ref>. 4. Let \u03bb \u2264 (n \u2212 1) \u22121 . The density of X can . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: veloped Transformers, a new neural architecture for even more effective natural language processing <ref type=\"bibr\" target=\"#b42\">(Vaswani et al., 2017)</ref>. Transformers overcome a major drawback  al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some Transformers uses positional encoding <ref type=\"bibr\" target=\"#b42\">(Vaswani et al., 2017)</ref> or positional embedding <ref type=\"bibr\" presented in Sec 2.3.1. For other details (especially on the multi-head attention), please refer to <ref type=\"bibr\" target=\"#b42\">Vaswani et al. (2017)</ref> and in particular, <ref type=\"bibr\">GPT-2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 512), are set using the validation set. To reduce the overfitting during training, we apply dropout <ref type=\"bibr\" target=\"#b23\">[24]</ref> with a rate of 0.5 on the output of fully connected layers. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f> which needs a CTC trained model to conduct pre-partition before the attention decoding.</p><p>In <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Li al. present the important Adaptive Computation Steps (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ional networks, have gained much attention and improved the state of the art in node classification <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l networks (SNNs), which are more bio-plausible and known as the next generation of neural networks <ref type=\"bibr\" target=\"#b6\">[7]</ref>. The integrate-and-fire neuron operates using spikes, which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng-standing research topic in information retrieval (IR) <ref type=\"bibr\" target=\"#b1\">[2]</ref>[4] <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Usually, the contextual information captured by models such. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eristics, respectively. Vertex domain designs such as <ref type=\"bibr\" target=\"#b99\">[100]</ref> or <ref type=\"bibr\" target=\"#b100\">[101]</ref> have the advantage of defining exactly localized basis f =\"#b65\">[66]</ref> were not critically sampled, unlike <ref type=\"bibr\" target=\"#b60\">[61]</ref> or <ref type=\"bibr\" target=\"#b100\">[101]</ref>. Thus, much recent work has focused on developing critic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral architectures, such as DRMM <ref type=\"bibr\" target=\"#b8\">(Guo et al., 2016)</ref> and Co-PACRR <ref type=\"bibr\" target=\"#b13\">(Hui et al., 2018)</ref>, adopt an interaction-based design. They ope. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed the equality of opportunity constraint and demonstrate limitations of a broad class of criteria. <ref type=\"bibr\" target=\"#b13\">Kleinberg et al. (2017)</ref> and Chouldechova (2016) point out the t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  in the literature are the techniques called Enhanced Control Flow Checking Using Assertions (ECCA) <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Control Flow Checking by Software Signatures (CFCSS) <  from the signature associated with the current node, it means an error has occurred in the program <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> ) in which the execution always enters at the first instruction and leaves via the last instruction <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tructure characteristics and alleviate the sparsity, thus improving the rec-ommendation performance <ref type=\"bibr\" target=\"#b25\">(Wang et al., 2019)</ref>. For example, as shown in Figure <ref type= bedding because of its powerful representation learning based on node features and graph structure. <ref type=\"bibr\" target=\"#b25\">Wang et al. (2019)</ref> explored the GNN to capture high-order conne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r research has shown a significant amount of useful work can be done in the shadow of a memory miss <ref type=\"bibr\" target=\"#b12\">[13]</ref>. CFP uses that observation to achieve memory latency toler dow of a long latency miss comprises miss-independent instructions for all benchmark suites. Others <ref type=\"bibr\" target=\"#b12\">[13]</ref> have also observed similar results for the SINT2K suite. T all fraction of mispredicted branches is dependent on a long-latency load miss. Karkhanis and Smith <ref type=\"bibr\" target=\"#b12\">[13]</ref> identified structural, data, and controlinduced stalls as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: PacketShader <ref type=\"bibr\" target=\"#b7\">[7]</ref>, DPDK <ref type=\"bibr\" target=\"#b8\">[8]</ref>, <ref type=\"bibr\" target=\"#b9\">[9]</ref>) has been proposed. In particular, DPDK (Data Plane Developm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b43\">44]</ref> and the possibility of poor margins <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, leading to reduced generalization performance. In practice. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s were not available on the test-std set, so we also include the test-dev results (and for Multipath<ref type=\"bibr\" target=\"#b39\">[40]</ref> on minival). \u2020 : http://image-net.org/challenges/ talks/20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: machine translation <ref type=\"bibr\" target=\"#b0\">(Bahdanau et al., 2014)</ref>, speech recognition <ref type=\"bibr\" target=\"#b8\">(Chorowski et al., 2015;</ref><ref type=\"bibr\" target=\"#b3\">Chan et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the abbreviation, as a short form of text, is prone to ambiguity. Word sense disambiguation methods <ref type=\"bibr\" target=\"#b22\">[23]</ref> have been studied to disambiguate word senses, however, de be expensive and the collected datasets are normally small in size.</p><p>Word sense disambiguation <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a type of technique used to distinguish ambiguous word  </p><p>Inspired by word sense disambiguation methods that label super sense types for word clusters <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we jointly predict the types for abbreviation candidates . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: corporation of different deep learning tools, for instance, zeroshot learning and domain adaptation <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. These methods all. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[49]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b51\">[52]</ref>, <ref type=\"bibr\" target=\"#b52\">[53]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: int locations based on hand-crafted features. Recent works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targe ]</ref> uses deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> and Zisserman et al. <ref type=\"bibr\" target=\"#b1\">[2]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ble to rate trust better in the form of discrete verbal statements, rather than continuous measures <ref type=\"bibr\" target=\"#b28\">[29]</ref> . Verbal statements must introduce subjective fuzziness. F. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe  going through an intermediate text transcript. There are many advantages of end-to-end SLU systems <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the most significant of which is that E2E systems can direc  trained on increasingly relevant data until it is fine-tuned on the actual domain data. Similarly, <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> advocate pre-trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: structions a di cult task, even in the presence of aiding devices like a hardware trace cache (HTC) <ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Indeed, a trace cach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: h as the element-wise mean E p [z] = [\u03c0 1 , ..., \u03c0 k ] of these vectors.</p><p>The Gumbel-Max trick <ref type=\"bibr\" target=\"#b8\">(Gumbel, 1954;</ref><ref type=\"bibr\" target=\"#b12\">Maddison et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  e.g., paper network <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" target=\"#b20\">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  this section, we consider introducing a third source of randomness, the random dropout of features <ref type=\"bibr\" target=\"#b12\">(Srivastava et al., 2014)</ref>, which is adopted in various GCN mode rom dropout (ND) of different estimators.</p><p>Our method is based on the weight scaling procedure <ref type=\"bibr\" target=\"#b12\">(Srivastava et al., 2014)</ref> to approximately compute the mean \u00b5</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b36\">36,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. Although considerable progress has been achieved in image  get=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b30\">30,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. Due to space limitation, we here briefly review works rela arget=\"#b13\">13,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. For example, Dong et al. <ref type=\"bibr\" target=\"#b2\">[ 2 ing discriminative ability of the network. Channel attention <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> has been shown to be effective for better discriminative re rget=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref  >[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>, we also adopt self-ensemble method to further improve our  f the network, some other networks, such as NLRN <ref type=\"bibr\" target=\"#b22\">[22]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>, improve the performance by considering feature correlatio classification.</p><p>Recently, SENet was introduced to deep CNNs to further improve SR performance <ref type=\"bibr\" target=\"#b38\">[38]</ref>. However, SENet only explores first-order statistics (e.g. se feature interdependencies. Difference to Residual Channel Attention Network (RCAN). Zhang et al. <ref type=\"bibr\" target=\"#b38\">[38]</ref> proposed a residual in residual structure to form a very d sidual blocks in each LSRAG, thus resulting in deep network with over 400 convolution layers. As in <ref type=\"bibr\" target=\"#b38\">[38]</ref>, we also add long and short skip connections in Base model BPN <ref type=\"bibr\" target=\"#b6\">[6]</ref>, RDN <ref type=\"bibr\" target=\"#b39\">[39]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=  <ref type=\"bibr\" target=\"#b36\">[36]</ref>, RDN <ref type=\"bibr\" target=\"#b39\">[39]</ref>, and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. All the results on 3\u00d7 are shown in Table <ref type=\"table. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">41]</ref> and graph classification <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, applying our GTNs to the other tasks can be interesting fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd then used conditional random fields to perform postclassification refinement. Kampffmeyer et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> used a weighted loss function to solve the problem of cate f the ISPRS Vaihingen dataset. We adopt the practice of <ref type=\"bibr\" target=\"#b8\">[9]</ref>[10] <ref type=\"bibr\" target=\"#b10\">[11]</ref> and do not report the accuracy of the clutter/background c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b24\">25]</ref>. The majority of existing works focus on grid data or independent samples <ref type=\"bibr\" target=\"#b25\">[26]</ref> whilst a few work investigate adversarial attack on graphs bibr\" target=\"#b7\">[8]</ref> while the latter degrades the overall performance of the trained model <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The targeted attack can be categorized into direct target 0]</ref> derive adversarial perturbations that poison the graph structure. Similarly, Z\u00fcgner et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> propose a non-targeted poisoning attacker by using meta-gr geted attack (Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>), and non-targeted attack (Mettack <ref type=\"bibr\" target=\"#b25\">[26]</ref>). In Mettack, we set the perturbation rate as 20% (i.e., \u2206 type=\"bibr\" target=\"#b7\">[8]</ref>, Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Mettack <ref type=\"bibr\" target=\"#b25\">[26]</ref>).</p><p>We use PyTorch DeepRobust package (https://github. ype=\"bibr\" target=\"#b17\">[18]</ref>. The attacker finds optimal perturbation A through optimization <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>:</p><formula xml:id= \">1</ref> Non-targeted attacks. The attacker aims to degrade overall GNN classification performance <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. Here, T = A = V t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b1\">[2]</ref> predicts the nonlinear LR-HR mapping via a fully convolution monality among the above CNN models is that their networks contain fewer than 5 layers, e.g., SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> uses 3 convolutional layers. Their deeper structures with 4  ween the input ILR image and the output HR image. There are three notes for VDSR: (1) Un-like SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> that only uses 3 layers, VDSR stacks 20 weight layers (3 \u00d7 3 arget=\"#b32\">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref> RFL <ref typ e as <ref type=\"bibr\" target=\"#b12\">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, VDSR <ref   PSyCo [20] and IA <ref type=\"bibr\" target=\"#b28\">[30]</ref>. The deep models (d \u2264 8) include SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, DJSR <ref type=\"bibr\" target=\"#b31\">[33]</ref>, CSCN <ref t rsity of images. Shi et al. <ref type=\"bibr\" target=\"#b23\">[25]</ref> observe that the prior models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b30\">32]</ref> increase LR image's  1</ref> shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ , k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ  only. Therefore, the input and output images are of the same size. For fair comparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ  type=\"bibr\" target=\"#b9\">[10]</ref> RFL <ref type=\"bibr\" target=\"#b21\">[23]</ref>   the results of <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\">20,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: to A, but a different relation to B). These queries are generally referred to as analogical queries <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Especially for complex information needs, the formulation query solving in the form of \"A is to B as C is to ?\" is a fundamental aspect of human intelligence <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Chan et al. <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, social networks analysis <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, and visual unders. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: earning is motivated by the self-training algorithm <ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al. model parameters, in a way similar to self-training <ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tei-c.org/ns/1.0\"><head n=\"4\">Attacking Reading Comprehension</head><p>We create triggers for SQuAD <ref type=\"bibr\" target=\"#b25\">(Rajpurkar et al., 2016)</ref>. We use an intentionally simple baseli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oc bottom-up fashion, where PMU designers attempted to cover key issues via \"dedicated miss events\" <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Yet, how does one pin-point performance issues that were no rchy's top level. This accurate classification distinguishes our method from previous approaches in <ref type=\"bibr\" target=\"#b0\">[1]</ref>[5][6].</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><h same symptom. Such scenarios of L1 hits and near caches' misses, are not handled by some approaches <ref type=\"bibr\" target=\"#b0\">[1]</ref> <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p><p>Note performa t=\"#b5\">[6]</ref>, nor complex structures with latency counters as in Accurate CPI Stacks proposals <ref type=\"bibr\" target=\"#b0\">[1]</ref>[8] <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div x tempted to accurately classify performance impacts on out-of-order architectures. Eyerman et al. in <ref type=\"bibr\" target=\"#b0\">[1]</ref>[9] use a simulation-based interval analysis model in order t reference model) is that it restricts all stalls to a fixed set of eight predefined miss events. In <ref type=\"bibr\" target=\"#b0\">[1]</ref>[4] <ref type=\"bibr\" target=\"#b4\">[5]</ref> there is no consi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b5\">(Gao et al. 2012</ref>). However, traditional hypergraph learning methods <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref> suffer from their high comput ployed to model high-order correlation among data.</p><p>Hypergraph learning is first introduced in <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref>, as a propagation process on  the hypergraph structure. The task can be formulated as a regularization framework as introduced by <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref>:</p><formula xml:id=\"formula_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pwords like of and on. Therefore, a better mechanism for local weights is needed.</p><p>Inspired by <ref type=\"bibr\" target=\"#b59\">[60]</ref>, we propose a local weight network based on distributed wo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d n=\"1\">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type=\"bibr\" target=\"#b9\">[Liu et al., 2011;</ref><ref type=\"bibr\" target=\"#b8\">Lian et al., 201. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpared to state-of-the-art self-supervised <ref type=\"bibr\" target=\"#b44\">[45]</ref> and supervised <ref type=\"bibr\" target=\"#b0\">[1]</ref> methods; and (ii) the controllability of the network using o ich may not model the full range of required expressions/deformations and the higher level details, <ref type=\"bibr\" target=\"#b0\">[1]</ref> propose a 2D warping method. Given only one source image, <r , <ref type=\"bibr\" target=\"#b0\">[1]</ref> propose a 2D warping method. Given only one source image, <ref type=\"bibr\" target=\"#b0\">[1]</ref> use facial landmarks in order to warp the expression of one  compare to two methods: CycleGAN <ref type=\"bibr\" target=\"#b44\">[45]</ref> which uses no labels and <ref type=\"bibr\" target=\"#b0\">[1]</ref> which is designed top down and demonstrates impressive resul given frame whereas CycleGAN samples from a latent space.</p><p>Comparison to Averbuch-Elor et. al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We compare to <ref type=\"bibr\" target=\"#b0\">[1]</ref> in Fi e.</p><p>Comparison to Averbuch-Elor et. al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We compare to <ref type=\"bibr\" target=\"#b0\">[1]</ref> in Fig. <ref type=\"figure\" target=\"#fig_2\">5</ref>. There ar (Fig. <ref type=\"figure\" target=\"#fig_2\">5b-c</ref>).</p><p>Second, ours has fewer assumptions: (1) <ref type=\"bibr\" target=\"#b0\">[1]</ref> assumes that the first frame of the driving video is in a fr s paper, our method can be augmented with the ideas from these methods. For example, as inspired by <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we can perform simple post-processing to add higher level d  generation quality is not as high as approaches specifically designed for transforming faces (e.g. <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: TECTURES</head><p>Inspired by the deep residual model <ref type=\"bibr\" target=\"#b6\">[7]</ref>, RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we p RCL) are performed with respect to the discrete time steps that are expressed according to the RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>. Let's consider the \ud835\udc65 \ud835\udc59 input sample in the \ud835\udc59 \ud835\udc61\u210e layer of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al. It contains 476 images with large occlusions caused by luggage and viewpoint changes. 4. CAVIAR <ref type=\"bibr\" target=\"#b1\">[2]</ref> The CAVIAR dataset contains 72 individuals captured by two c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bib s the substitution of an indirect method call with direct method calls in object-oriented languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bib nce impact due to virtual function calls. These approaches include the method cache in Smalltalk-80 <ref type=\"bibr\" target=\"#b10\">[11]</ref>, polymorphic inline caches <ref type=\"bibr\" target=\"#b23\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eir original image size. The encoder part of FCN consists of visual geometry group network (VGGNet) <ref type=\"bibr\" target=\"#b25\">[26]</ref> that is a famous CNN classification model and the decoder . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ition pattern is more complicated. \u2022 The recent approaches <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, which divide the u  the fairness and the convenience of comparison, we follow <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> to filter out sessi , s,i \u22121 ] with the last item s,i \u22121 as l abel . Following <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, for the Yoochoose  tention on the last item after encoding with a RNN. To alleviate the influence of time order, STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> only utilizes the self-attention mechanism without RNN. SR  to sum up as the session embedding. To further alleviate the bias introduced by time series, STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> entirely replaces the recurrent encoder with an attention  h enables the model to explicitly emphasize on the more important parts of the input.</p><p>\u2022 STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> uses attention layers to replace all RNN encoders in previ ith different lengths because the length varies greatly within one dataset. Following previous work <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, sessions in Yooch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r any combination of input parameters ( \u00a7 3).</p><p>\u2022 Based on the red-blue pebble game abstraction <ref type=\"bibr\" target=\"#b34\">[34]</ref>, we provide a new method of deriving I/O lower bounds (Lem o parents (or no children, respectively). Red-Blue Pebble Game Hong and Kung's red-blue pebble game <ref type=\"bibr\" target=\"#b34\">[34]</ref> models an execution of an algorithm in a two-level memory  achinery for deriving I/O lower bounds for general CDAGs. We extend the main lemma by Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref>, which provides a method to nd an I/O lower bound for a gi rresponding pebbling. Hong and Kung use a speci c variant of this partition, denoted as S-partition <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p><p>We rst introduce our generalization of S-partition, ation that each V i performs at least S I/O operations, we phrase the lemma by Hong and Kung: L 1 ( <ref type=\"bibr\" target=\"#b34\">[34]</ref>). e minimal number Q of I/O operations for any valid execu ulation of the CDAG, where a calculation is a sequence of allowed moves in the red-blue pebble game <ref type=\"bibr\" target=\"#b34\">[34]</ref>. Divide the complete calculation into h consecutive subcom d Lemma. We now use the above de nitions and observations to generalize the result of Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> he remaining properties of a valid X -partition S(X ), we use the same reasoning as originally done <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p><p>erefore, a complete calculation performing q &gt; ( , and therefore may be seen as the last step in the long sequence of improved bounds. Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref> derived an asymptotic bound \u2126 n 3 / \u221a S for the sequential  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"10.1\">General I/O Lower Bounds</head><p>Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref> analyzed the I/O complexity for general CDAGs in their the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: beling is also often performed on user-generated text, which may contain spelling mistakes or typos <ref type=\"bibr\" target=\"#b15\">(Derczynski et al., 2013)</ref>. Errors introduced in an upstream tas rated text is a rich source of informal language containing misspellings, typos, or scrambled words <ref type=\"bibr\" target=\"#b15\">(Derczynski et al., 2013)</ref>. Noise can also be introduced in an u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b14\">Nair95b]</ref> use predictors similar to a \"two-level\" adaptive branch predictor <ref type=\"bibr\" target=\"#b21\">[Yeh91]</ref>. Then, we demonstrate that these \"twolevel like\" predic  level and have one or more tables of 2-bit counters (pattern history tables) in their second level <ref type=\"bibr\" target=\"#b21\">[Yeh91]</ref>. The contents of the first level shift-registers are ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  directly incorporate multi-hop neighborhood information of a node without explicit message-passing <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Intuitively, propagation based on personalized PageRank c here the node influence decays exponentially with each layer. However, as proposed, Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>'s approach does not easily scale to large graphs since it  ng that can reduce predictive performance.</p><p>To tackle both of these challenges Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> suggest decoupling the feature transformation from the pro instead. Unfortunately, even a moderate number of power iteration evaluations (e.g. Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> used K = 10 to achieve a good approximation) is prohibitiv ency of the subsequent learning step. Both of these approaches are a special case of the PPNP model <ref type=\"bibr\" target=\"#b32\">[33]</ref> which experimentally shows higher classification performan . In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model <ref type=\"bibr\" target=\"#b32\">[33]</ref> which we build upon. The results are summarized in Table < ich experimentally shows higher classification performance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>.</p><p>Approximating PageRank. Recent approaches combine ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 12\">(Freeman &amp; Adelson, 1991;</ref><ref type=\"bibr\" target=\"#b36\">Simoncelli et al., 1992;</ref><ref type=\"bibr\" target=\"#b30\">Perona, 1995;</ref><ref type=\"bibr\" target=\"#b37\">Teo &amp; Hel-Or, 1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cores for all documents. To get the student predicted rank for this document, we apply Weston et al <ref type=\"bibr\" target=\"#b35\">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: urveillance systems to automatically detect suicidal behaviors and trigger an alarm. In this sense, <ref type=\"bibr\" target=\"#b9\">(Lee et al., 2014)</ref> presented a method for automatically analyzin ide by hanging attempts <ref type=\"bibr\" target=\"#b1\">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=\"bibr\" target=\"#b9\">(Lee et al., 2014)</ref>, we performed our experiments on a large data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by full convolution or full attention architectures <ref type=\"bibr\" target=\"#b19\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b9\">Gehring et al., 2017;</ref><ref type=\"bibr\" target=\"#b36\">Vaswani et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing performance for clustering, SSL, active learning, etc. <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. A simple yet effective way to overcome the limitations is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Zeiler and Fergus, 2014)</ref> of the input. The best networks are using more than 150 layers as in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a;</ref><ref type=\"bibr\" target=\"#b7\">He et al., 2016b n computer vision, in particular <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b6\">He et al., 2016a)</ref>.</p><p>This paper is structured as follows. Th ayers <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>. In the remainder of this paper, we describe  serman, 2015)</ref>. We have also investigated the same kind of \"ResNet shortcut\" connections as in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>, namely identity and 1 \u00d7 1 convolutions (see  ng even deeper degrades accuracy. Shortcut connections help reduce the degradation. As described in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>, the gain in accuracy due to the the increase onnections between convolutional blocks that allow the gradients to flow more easily in the network <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>.</p><p>We evaluate the impact of shortcut con works to temporal convolutions as we think this a milestone for going deeper in NLP. Residual units <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>  work and ImageNet is that the latter deals w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ates and synonyms for relations by searching the Internet and two real-world datasets, WebQuestions <ref type=\"bibr\" target=\"#b3\">(Berant et al., 2013)</ref> and WikiAnswers <ref type=\"bibr\" target=\"#  for different syntactical structures and paraphrases in real-world datasets including WebQuestions <ref type=\"bibr\" target=\"#b3\">(Berant et al., 2013)</ref> and WikiAnswers <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s often generated by the model itself. Some representative algorithms include structured perceptron <ref type=\"bibr\" target=\"#b2\">(Collins, 2002)</ref>, energy-based models <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt, of the text. In contrast, an end-to-end (E2E) SLU system <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: function <ref type=\"bibr\" target=\"#b20\">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type=\"bibr\" target=\"#b23\">[24]</ref> of keep probability, p = 0.5, is applied to the hidden lay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent obfuscation based defenses have proven vulnerable. In their recent seminal work, Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> presented a suite of strategies for estimating network gradi  range of possible attacks, including those having successfully circumvented many previous defenses <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Under these attacks, we compare the worst-case robustness o b32\">33]</ref>.</p><p>Unfortunately, many of these methods have proven vulnerable by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>, who introduced a set of attacking strategies, including a m .</p><p>Thus far, gradient obfuscation is generally considered vulnerable (and at least incomplete) <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We revisit gradient obfuscation, and our defense demonstrat ased defense mechanisms seem plausible. Yet they are all fragile. As demonstrated by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>, with random input transformation, adversarial examples can  -removal transformation is also ineffective. One can use Backward Pass Differentiable Approximation <ref type=\"bibr\" target=\"#b1\">[2]</ref> to easily construct effective adversarial examples. In short ation (BPDA).</head><p>To circumvent the defense using non-differentiable operators, Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> introduced a strategy called Backward Pass Differentiable Ap  attacking strategy. It causes the adversary to suffer from either exploding or vanishing gradients <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Figure <ref type=\"figure\" target=\"#fig_4\">3</ref>-left show #b32\">33]</ref>. Yet those defenses have been proven vulnerable under a reparameterization strategy <ref type=\"bibr\" target=\"#b1\">[2]</ref>. This strategy aims to find some differentiable function h(\u2022 those defenses, the transformed image g(x) remain similar to the input x. Consequently, as shown in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, those defenses can be easily circumvented by replacing g(\u2022) mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">BPDA Attack and the Variants</head><p>BPDA attack <ref type=\"bibr\" target=\"#b1\">[2]</ref>, as reviewed in Sec. 3.3, is a powerful way to estimate netw ck-box attacks, including the black-box transfer attack <ref type=\"bibr\" target=\"#b29\">[30]</ref>   <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We evaluate other methods using the code provided in the or er all tested attacks. The methods indicated by a star (*) are those circumvented by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We include their results therein as a reference. The other  nsformations to the input. But they have been circumvented by Expectation Over Transformation (EOT) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. EOT attack first esti rsarial examples of f b directly (so that g(h(\u2022)) = h(\u2022)), without solving the optimization problem <ref type=\"bibr\" target=\"#b1\">(2)</ref>. We argue that finding such an h(\u2022) is extremely hard. If h(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed by the original sentence. We did this because neural models are known to hallucinate information <ref type=\"bibr\" target=\"#b29\">(Rohrbach et al., 2018)</ref>. We report the average count of false i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b18\">(Schein et al., 2016;</ref><ref type=\"bibr\" target=\"#b1\">Ben-Younes et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Yang and Hospedales, 2017)</ref>, factorizes a tensor into a core ten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  mixing coefficients <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><formula xml:id=\"formula_0\">M = \u03b1B + \u03b2R,<label>(1)</lab ion results. Moreover, we introduce the gradient constraints <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> to make the model learning more effective, in which the edg  In this scenario, image priors such as different blur levels between the background and reflection <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, ghosting effects  \" target=\"#fig_3\">4</ref> 1 ) than previous linear functions <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar <p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type=\"bibr\" target=\"#b16\">[17]</ref> 0.801 0.829 21.77 WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ >, FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and LB14 <ref type=\"bibr\" target=\"#b16\">[17]</ref>. For a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 62\">[63]</ref>, action recognition <ref type=\"bibr\" target=\"#b45\">[46]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b5\">[6]</ref>, salient object detection <ref type=\"bibr\" target=\"#b1\">[2]< =\"#b40\">[41]</ref>, edge detection <ref type=\"bibr\" target=\"#b36\">[37]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b5\">[6]</ref>, salient object detection <ref type=\"bibr\" target=\"#b33\">[34 ns of the fully convolutional network (FCN) for semantic segmentation task. In DeepLab, Chen et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> introduces cascaded. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ressing these concerns, approaches utilizing a dynamic vocabulary of slot values have been proposed <ref type=\"bibr\" target=\"#b13\">(Rastogi, Gupta, and Hakkani-Tur 2018;</ref><ref type=\"bibr\" target=\" representations for potentially unseen inputs from new services. Recent pretrained models like ELMo <ref type=\"bibr\" target=\"#b13\">(Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b3\">(De. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: category loss term. To enable distribution q \u03d5 c (z|x q c ) differentiable, we follow previous work <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ssage passing, in order to capture richer topological properties. Our method draws inspiration from <ref type=\"bibr\" target=\"#b32\">[33]</ref>, where it was shown that GNNs become universal when the ve  target=\"#b35\">[36]</ref>.</p><p>It is important to note here that contrary to identifierbased GNNs <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bib rovide a unique identification of the vertices, then universality will also hold (Corollary 3.1. in <ref type=\"bibr\" target=\"#b32\">[33]</ref>).</p><p>We conjecture, that in real-world scenarios the nu ><p>Unique identifiers. From a different perspective, <ref type=\"bibr\" target=\"#b67\">[68]</ref> and <ref type=\"bibr\" target=\"#b32\">[33]</ref> showed the connections between GNNs and distributed local . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b0\">[1]</ref>, we use the randomly initialized LeNet for all experiments. L-BFGS <ref type=\"bibr\" target=\"#b10\">[11]</ref> with learning rate 1 is used as the optimizer. For fast tr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b22\">(Ravanbakhsh et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Gens and Domingos, 2014;</ref><ref type=\"bibr\" target=\"#b5\">Cohen and Welling, 2016)</ref> for finite groups and <ref type=\"bibr\"  ariance to translation and its relation to convolutions <ref type=\"bibr\">(Kondor et al., 2018;</ref><ref type=\"bibr\" target=\"#b5\">Cohen and Welling, 2016)</ref>, which are ubiquitous in image processi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: guage learners <ref type=\"bibr\" target=\"#b36\">(Xia et al., 2016)</ref> and people with low literacy <ref type=\"bibr\" target=\"#b33\">(Watanabe et al., 2009)</ref>. The type of simplification needed for . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /head><p>Previous survey papers on knowledge graphs mainly focus on statistical relational learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>, knowledge graph refinement <ref type=\"bibr\" target=\"#b5\">[6. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: based schemes. Bayesian networks also play a crucial role in diagnosis and decision support systems <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>Obviously, there's a computational problem in dealin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b2\">[3]</ref>[6][9] <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Salakhutdinov and Hinton <ref type=\"bibr\" target=\"#b8\">[9]</ref> demonstrated that the semantic structures can be extracted v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fically, to capture the nonlinear relationship between features, we introduce explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> to CCA. Finally, u high computational complexity and memory use. In contrast, recent advances of explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> can convert nonlin l trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref>. Let \u03c6(x) denote a IST features, attribute features, and SentiBank features, we use the random Fourier feature mapping <ref type=\"bibr\" target=\"#b19\">[12]</ref> to approximate the Gaussian kernel. All other histogram-ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eRank. Luckily, given the broad applicability of PageRank, many such algorithms have been developed <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> and backward search <ref type=\"bibr\" target=\"#b3\">[4]</ref> can be viewed as deterministic variants of the random walk s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>Finally, a recent trend in computer graphics has been the use of rendered images as textures <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a result, it has become desirable to unify the framebuff  accessed in parallel is possible if the texels are stored in a morton order within the cache lines <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Morton order implies that the texels are stored in 2x2 bloc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: m the corpus. 1 While surprising, this last property is important for detecting quality web content <ref type=\"bibr\" target=\"#b41\">[42]</ref>.  Query terms are laid out along the vertical axis, and th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms that are applied to object segmentation based on CNNs <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Because the performance of deep learning algorithms depen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f state-of-the-art models.</p><p>Graph Neural Networks (GNNs) <ref type=\"bibr\">(Micheli, 2009;</ref><ref type=\"bibr\" target=\"#b16\">Scarselli et al., 2008)</ref> have recently become the standard tool   state of neighboring nodes. Thanks to layering <ref type=\"bibr\">(Micheli, 2009)</ref> or recursive <ref type=\"bibr\" target=\"#b16\">(Scarselli et al., 2008)</ref> schemes, these models propagate inform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b47\">[48]</ref> or error reduction <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>. For example, in <ref type=\"bibr\" target=\"#b43\">[44]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to be susceptible to adversarial attacks <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow et al., 2015)</ref>. These attacks come in the form of adve the classifier with adversarial examples <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow et al., 2015)</ref>, (2) modifying the training procedure o le to be misclassified at inference time <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow et al., 2015;</ref><ref type=\"bibr\" target=\"#b16\">Papernot  aining dataset with adversarial examples <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow et al., 2015;</ref><ref type=\"bibr\" target=\"#b14\">Moosavi-D R n , resulting in the adversarial example x = x + \u03b4. The \u221e -norm of the perturbation is denoted by <ref type=\"bibr\" target=\"#b4\">(Goodfellow et al., 2015)</ref> and is chosen to be small enough so as  work, we focus on untargeted white-box attacks computed using the Fast Gradient Sign Method (FGSM) <ref type=\"bibr\" target=\"#b4\">(Goodfellow et al., 2015)</ref>, the Randomized Fast Gradient Sign Met the perturbation \u03b4 to:</p><formula xml:id=\"formula_0\">\u03b4 = \u2022 sign(\u2207 x J(x, y)).</formula><p>(1) FGSM <ref type=\"bibr\" target=\"#b4\">(Goodfellow et al., 2015)</ref> was designed to be extremely fast rath ormula xml:id=\"formula_8\">(i) 0 } R i=1 .</formula><p>We compare our method to adversarial training <ref type=\"bibr\" target=\"#b4\">(Goodfellow et al., 2015)</ref> and MagNet <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . . . , 0.003] (6)</p><p>Feature-space Maximum Likelihood Linear Regression (FMLLR) was explored in <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> for speaker ada. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hdalai and Yu, 2017)</ref>, GNN <ref type=\"bibr\" target=\"#b4\">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type=\"bibr\" target=\"#b10\">(Mishra et al., 2018)</ref>, Proto <ref type=\"bibr\" target=\"#b13\">(Sn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng other modalities, like music <ref type=\"bibr\" target=\"#b9\">(Huang et al., 2018)</ref> and images <ref type=\"bibr\" target=\"#b13\">(Parmar et al., 2018)</ref>, even longer sequences are commonplace. T erse data such as music scores <ref type=\"bibr\" target=\"#b9\">(Huang et al., 2018)</ref>, and images <ref type=\"bibr\" target=\"#b13\">(Parmar et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Ramachandra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n, that were applied to short-text summarization. We propose a novel variant of the coverage vector <ref type=\"bibr\" target=\"#b25\">(Tu et al., 2016)</ref> from Neural Machine Translation, which we use d n=\"2.3\">Coverage mechanism</head><p>Repetition is a common problem for sequenceto-sequence models <ref type=\"bibr\" target=\"#b25\">(Tu et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">Mi et al., 2016 erating multi-sentence text (see Figure <ref type=\"figure\">1</ref>). We adapt the coverage model of <ref type=\"bibr\" target=\"#b25\">Tu et al. (2016)</ref> to solve the problem. In our coverage model, w ine Translation <ref type=\"bibr\" target=\"#b10\">(Koehn, 2009)</ref>, coverage was adapted for NMT by <ref type=\"bibr\" target=\"#b25\">Tu et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b14\">Mi et al. (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ions of the defense models compared with normally trained models. We adopt class activation mapping <ref type=\"bibr\" target=\"#b39\">[38]</ref> to visualize the attention maps of three normally trained . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  regions at multiple scales of the original image.</p><p>Lastly, inspired by the recent Transformer <ref type=\"bibr\" target=\"#b22\">[23]</ref> model in dealing with a number of difficult NLP tasks such idal way, following by corresponding pooling strategy. Moreover, inspired by the recent Transformer <ref type=\"bibr\" target=\"#b22\">[23]</ref> model in dealing with a number of difficult NLP tasks such iation between these local regions are still not explored. Inspired by the recent Transformer model <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we implant a self attention module adapted to image featu g sentiment class number of datasets, and then tunes the parameters of all layers. \u2022 Self-Attention <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a variant derived from the Transformer model. Transform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n this work, we focus on three of these proposed schemes -the first based on working set signatures <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the second based n lead to unpredictable, non-optimal results. Consequently, algorithms such as the ones proposed in <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref> do not perform tu ets, BBVs, and conditional branch counters. In addition to instruction working set based techniques <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11], we evaluate branch and procedure working set based tec  are defined as the set of branches/procedures touched over the sampling interval. In previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11], we defined a similarity metric called the relative wor  and working sets are too large to be efficiently stored and compared in hardware. In previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>, we proposed a ha e</head><p>A working set signature is a lossy-compressed representation of the complete working set <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The signature is 512 bits) and accumulator tables (1024, 128, 32 entries) are similar to those used in previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11] <ref type=\"bibr\" target=\"#b16\">[19]</ref>. Procedure si  be used in tuning algorithms to reuse previously found optimal configurations for recurring phases <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11] <ref type=\"bibr\">[12] [19]</ref>. This eliminates a sig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on. We report the cosine similarity distribution based on other category definitions (attributes in <ref type=\"bibr\" target=\"#b18\">[19]</ref>) instead of semantic label in Fig. <ref type=\"figure\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  SMARTS <ref type=\"bibr\" target=\"#b28\">[29]</ref>) and representative sampling (as done in SimPoint <ref type=\"bibr\" target=\"#b21\">[22]</ref>). Our experimental results using the SPEC CPU2000 benchmar st well known representative sampling approach is the SimPoint approach proposed by Sherwood et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref>. SimPoint picks a small number of sampling units that accu onsiders multiple randomly chosen cluster centers and uses the Bayesian Information Criterion (BIC) <ref type=\"bibr\" target=\"#b21\">[22]</ref> to assess the quality of the clustering: the clustering wi ifferent ways for doing so, such as code working sets <ref type=\"bibr\" target=\"#b5\">[6]</ref>, BBVs <ref type=\"bibr\" target=\"#b21\">[22]</ref>, procedure calls <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: peech-to-text translation systems as a single neural network <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. Such end-to-end systems have advantages over a traditional c work has focused on training end-to-end ST in a single model <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>.</p><p>In order to utilize both fully supervised data and als end-to-end systems. Recently explored techniques to mitigate this issue include multi-task learning <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and pre-trained compo b8\">9]</ref>.</p><p>In order to utilize both fully supervised data and also weakly supervised data, <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> use multi-task learni the 1M ST set. We then adopt pretraining and multi-task learning as proposed in previous literature <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.\">MODELS</head><p>Similar to <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we make use of three sequence-to-sequence models. Each one . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  form of explicit graph-based regularization <ref type=\"bibr\" target=\"#b30\">(Zhu et al., 2003;</ref><ref type=\"bibr\" target=\"#b29\">Zhou et al., 2004;</ref><ref type=\"bibr\" target=\"#b1\">Belkin et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e rank range from 1 to K. As pointed out above, this scheme pre-determines the weight. Rendle et al <ref type=\"bibr\" target=\"#b28\">[29]</ref> proposed an empirical weight for sampling a single positio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: image sequence when compared to the static image, as shown by the two green dotted lines. deception <ref type=\"bibr\" target=\"#b11\">(Farah et al. 2014)</ref>. Additionally, the cost of the equipment an pe=\"bibr\">(Kozel et al. 2005;</ref><ref type=\"bibr\" target=\"#b17\">Langleben and Moriarty 2013;</ref><ref type=\"bibr\" target=\"#b11\">Farah et al. 2014)</ref>, important questions related to the working   are still open research problems <ref type=\"bibr\" target=\"#b17\">(Langleben and Moriarty 2013;</ref><ref type=\"bibr\" target=\"#b11\">Farah et al. 2014)</ref>. Furthermore, the above mentioned physiologi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"table\" target=\"#tab_7\">6</ref>    <ref type=\"bibr\" target=\"#b14\">(Ju et al., 2018;</ref><ref type=\"bibr\" target=\"#b28\">Sohrab and Miwa, 2018)</ref>. Short word first is good at identifying. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .0\"><head n=\"4.1\">Fast Fibonacci compression</head><p>In this method, we apply the Fibonacci coding <ref type=\"bibr\" target=\"#b1\">[2]</ref> which uses the Fibonacci numbers; 1, 2, 3, 5, 8, 13, . . . .. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ph embedding methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> to learn the embedding of each item, dubbed Base Graph Embe . Experiments are conducted to compare four methods: BGE, LINE, GES, and EGES. LINE was proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which captures the first-order and second-order proximity. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  genera-tive model based on a Laplacian pyramid framework (LAP-GAN) to generate realistic images in <ref type=\"bibr\" target=\"#b8\">[6]</ref>, which is the most related to our work. However, the propose. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b16\">[17]</ref>, or generating intermediate representation in speech-to-text translation <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our deliberation model has a similar structure as <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ble to adversarial attacks both at test time (evasion) as well as training time (poisoning attacks) <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b10\">Dai et al., r, they focus on targeted attacks, i.e. attacks designed to change the prediction of a single node. <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> consider both test-time and training-time   algorithm for global attacks on (deep) node classification models at training time. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>, we explicitly tackle the bilevel optimiza gorithm achieved after training on the data (i.e., graph) modified by our algorithm. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b10\">Dai et  in undiscovered, adversarial attacks should be unnoticeable. To account for this, we largely follow <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>'s attacker capabilities. First, we impose   are very likely to be noticed; to prevent such large changes to the degree distribution, we employ <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>'s unnoticeability constraint on the degree  GCN) to evaluate the performance degradation due to the attack. We use the same surrogate model as <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>, which is a linearized two-layer graph con e diagonal matrix of the node degrees, and \u03b8 = {W } the set of learnable parameters. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> we do not linearize the output (softmax) l raining the model for T steps. We compare against this baseline in our experiments; as also done in <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>. However, unlike the meta-gradient, this a compare our meta-gradient approach as well as its approximations with various baselines and Nettack <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018)</ref>. DICE ('delete internally, connect extern l our experiments, we enforce the unnoticeability constraint on the degree distribution proposed by <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018)</ref>. In Fig. <ref type=\"figure\" target=\"#fig_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> and FPGAs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><p>Activation unit is a non-linear function that some l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b42\">43]</ref> and have connections to the large literature on metric learning <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>.</p><p>As the name s function encourages learning from hard positives and hard negatives. We also show that triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref> is a special case of our loss when only a single positive  ss can thus be seen to be efficient in its training. Other contrastive losses, such as triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref>, often use the computationally expensive technique of hard .\">Connections to Triplet Loss</head><p>Contrastive learning is closely related to the triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref>, which is one of the widely-used alternatives to cross-ent  contrastive learning are metric learning and triplet losses <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. These losses have . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: er could generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targe sible to generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targe ier with adversarial examples, called adversarial training <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>; <ref type=\"bibr\" target=\"#b1\">(2)</ref> Training a classifie from normal examples <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Moosavi et al. showed that it was even possible to find one  \"2.3.1\">Fast gradient sign method(FGSM). Given a normal image</head><p>x, fast gradient sign method <ref type=\"bibr\" target=\"#b4\">[5]</ref> looks for a similar image x \u2032 in the L \u221e neighborhood of x t \"#b21\">22]</ref>, or mix the adversarial objective with the classification objective as regularizer <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Though this idea is promising, it is hard to reason about w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ang, 2001)</ref>, peer-to-peer systems <ref type=\"bibr\" target=\"#b7\">(Hailong &amp; Jun, 2004;</ref><ref type=\"bibr\" target=\"#b12\">John et al., 2000;</ref><ref type=\"bibr\" target=\"#b21\">Mohan &amp; Ka. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> were proposed. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>  ><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> were proposed. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on  e reconstructed some classic SR models, such as SRCNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>, EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> and SRResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>. During <ref type=\"bibr\" target=\"#b5\">[6]</ref>, SRResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Unfortunately, these models become more and more deeper and CN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, LapSRN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. For fair, we retrain most of these models (except for EDSR  <ref type=\"bibr\" target=\"#b8\">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, the results of EDSR provided by their original papers).</p> t upscaling factors and test-datasets. It can be seen that our results are slightly lower than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. But it is worth noting that EDSR <ref type=\"bibr\" target=\"#  slightly lower than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. But it is worth noting that EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> use  RGB channels for training, meanwhile, the data augment  nwhile, the data augment methods are different.</p><p>To better illustrate the difference with EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we show a comparison of model specifications in Table <ref   show a comparison of model specifications in Table <ref type=\"table\" target=\"#tab_3\">3</ref>. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> is an outstanding model gained amazing results. However, it   memory, space and datasets. In contrast, the specifications of our model is much smaller than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which makes it easier to reproduce and promote.</p><p>In Fi nts the upscaling factor) mixed training method is used in <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and geometric selfensemble method is proposed in <ref type= 4]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and geometric selfensemble method is proposed in <ref type=\"bibr\" target=\"#b8\">[9]</ref>. We believe that these training tricks can also improve our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dhuber, 1997)</ref> system can readily outperform the previous state-of-the-art system, CogCompTime <ref type=\"bibr\" target=\"#b32\">(Ning et al., 2018d)</ref>, by a large margin. The fact that a standa ead><label>2</label><figDesc>Performances on the MATRES test set (i.e., the PT section). CogCompTime<ref type=\"bibr\" target=\"#b32\">(Ning et al., 2018d)</ref> is the previous state-of-the-art feature-b hat in Table <ref type=\"table\" target=\"#tab_2\">2</ref>, CogCompTime performed slightly different to <ref type=\"bibr\" target=\"#b32\">Ning et al. (2018d)</ref>: Cog-CompTime reportedly had F 1 =65.9 (Tab f type=\"table\" target=\"#tab_2\">2</ref> Line 3 therein) and here we obtained F 1 =66.6. In addition, <ref type=\"bibr\" target=\"#b32\">Ning et al. (2018d)</ref> only reported F 1 scores, while we also use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p><p>Query-based black-box attacks can settle the suscept >32]</ref> or images <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. More related to our work is the regularization-based appro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on, called receiver class prediction optimization (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib ll with direct method calls in object-oriented languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib morphic inline caches <ref type=\"bibr\" target=\"#b23\">[23]</ref>, and type feedback/devirtualization <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. As we show in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hms to exploit the relationships between the entities. Graph Convolutional (Neural) Networks (GCNs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ he similarity scores for user-item pairs. With the success of graph (convolutional) neural networks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> mostly rely on the development of convolutional neural netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iction by reinterpretation of fully connected layers of the classifier as a fully convolution layer <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The FCN consists of an encoder of input images and a deco  overcome this problem, skip connection methods are used <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The features extracted from PPL are upsampled and concate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e Cheeger bound. This connection has inspired many spectral solutions to the problem, including ARV <ref type=\"bibr\" target=\"#b5\">[11]</ref> and the many works that followed.</p><p>However, spectral m y balanced 2-partitioning algorithm to approximate a balanced k-partitioning when k is a power of 2 <ref type=\"bibr\" target=\"#b5\">[11]</ref>.</p><p>While we are unaware of any previous work on the exa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b75\">76]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b33\">[34]</ref>, with which GDC achieves a linear runtime O(N ). Furthermo e=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, especially for clustering <ref type=\"bibr\" target=\"#b33\">[34]</ref>, semi-supervised classification <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, which depend on fast context switching to share processor . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: core function of \ud835\udf09 (particular \ud835\udc59), i.e., D \ud835\udf03 (\ud835\udc59 |\ud835\udc5e, \ud835\udc36) and G \ud835\udf19 (\ud835\udc59 |\ud835\udc5e, \ud835\udc36), using Plackett-Luce model <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Specifically, we have:</p><formula xml:id=\"formula_3\">DVGAN. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acoustics and first-pass text hypotheses for second-pass decoding based on the deliberation network <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The deliberation model has been used in state-of-the-art  lation <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our deliberation model has a similar structure as <ref type=\"bibr\" target=\"#b15\">[16]</ref>: An RNN-T model generates the first-pass hypotheses, and d head><p>A deliberation model is typically trained from scratch by jointly optimizing all components <ref type=\"bibr\" target=\"#b15\">[16]</ref>. However, we find training a two-pass model from scratch t get=\"#b0\">[1]</ref>, and a deliberation decoder, similar to <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. The shared encoder takes log-mel filterbank energies, x = . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ccuracies, limitations and pitfalls of the related technique known as negotiated-congestion routing <ref type=\"bibr\" target=\"#b27\">[28]</ref>. In Copyright (c) 2008 IEEE. Personal use of this material 5\">[6]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bib ested regions, often at the cost of increased wirelength.</p><p>Negotiated-congestion Routing (NCR) <ref type=\"bibr\" target=\"#b27\">[28]</ref> was introduced in the mid-1990s for global routing in FPGA  (b e ), added cost reflecting congestion history (h e ), and penalty for current congestion (p e ) <ref type=\"bibr\" target=\"#b27\">[28]</ref>. NCR seeks to minimize e c e .</p><p>To begin negotiated-c re-route is the same for each iteration, but can be chosen arbitrarily, according to the authors of <ref type=\"bibr\" target=\"#b27\">[28]</ref>, because the gradual cost increase in congested areas remo =\"formula_7\">c e = b e + h e \u2022 p e<label>(8)</label></formula><p>which is different than Equation 1 <ref type=\"bibr\" target=\"#b27\">[28]</ref>, but also is more intuitive since it preserves the base co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  of independent jobs <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> as well as parallel t target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> are concentrated on c target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 23\">[Kshirsagar and Magnenat-Thalmann 2000]</ref>, mapping text to phonemes and phonemes to visemes <ref type=\"bibr\" target=\"#b28\">[Malcangi 2010</ref>], or mapping input audio features to control par. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ling, Tagging, and Sequence-to-Sequence (Seq2Seq). Among these approaches, the table filling method <ref type=\"bibr\" target=\"#b7\">(Gupta, Sch\u00fctze, and Andrassy 2016;</ref><ref type=\"bibr\" target=\"#b0\"  neural models also focus on pipeline methods, include (1) Fully-Supervised Relation Classification <ref type=\"bibr\" target=\"#b7\">(Hendrickx et al. 2009)</ref> (2) Distant Supervised Relation Extracti elations. The models include history-based searching (Miwa and Sasaki 2014), neuralbased prediction <ref type=\"bibr\" target=\"#b7\">(Gupta, Sch\u00fctze, and Andrassy 2016)</ref> and global normalization (Ad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> work under the mech s, some baseline methods on GNN, for example, GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref> and GAT <ref type=\"bibr\" target=\"#b32\">[33]</ref>, are demonstrated to be capable of extracting features of  t can be used to generate node embeddings, e.g., GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GAT <ref type=\"bibr\" target=\"#b32\">[33]</ref> and gated graph networks <ref type=\"bibr\" target=\"#b17\">[1 get=\"#b19\">[20]</ref>.</p><p>As suggested in previous work <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, the multi-head attention can help to stabilize the trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: [39,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, forums <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, social networks <ref type=\"bibr\" target=\"#b20\">[21]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dversarial samples <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>; adversaries subtly alter legitimate inputs (call input per g on previous work <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> describing how adversaries can efficiently select perturbat f previous attacks <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> for knowledge of the target architecture and parameters. We ike neural networks<ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. In addition, we introduce new techniques to craft adversar ep neural network (DNN) using crafted inputs and output labels generated by the target \"victim\" DNN <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Thereafter, the local network was used to generate advers d in <ref type=\"bibr\" target=\"#b11\">[12]</ref> or the Jacobian-based iterative approach proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We only provide here a brief description of the fast grad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The latter components stabilizes the evolution of graph s  target=\"#b39\">40]</ref>), GNNGUARD determines importance weights using theory of network homophily <ref type=\"bibr\" target=\"#b13\">[14]</ref>, positing that similar nodes (i.e., nodes with similar fea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rg/ns/1.0\"><head>III. RU-NET AND R2U-NET ARCHITECTURES</head><p>Inspired by the deep residual model <ref type=\"bibr\" target=\"#b6\">[7]</ref>, RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and U-Net < type=\"bibr\" target=\"#b4\">[5]</ref>, GoogleNet <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Residual Net <ref type=\"bibr\" target=\"#b6\">[7]</ref>, DenseNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Capsu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: > and speech recognition (automatic speech recognition, ASR) <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> are two key tasks in 9\">30,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and ASR <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> require a large amou Grant 61822203, 61772297, 61632016, 61761146003, and the Zhongguancun Haihua Institute for Frontier <ref type=\"bibr\" target=\"#b9\">10</ref> The audio samples can be founded in https://speechresearch.gi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b8\">(Ekbal and Bandyopadhyay, 2010)</ref> and Conditional Random Fields(CRF) <ref type=\"bibr\" target=\"#b9\">(Feng et al., 2006)</ref>. \u2192 \"\u6c34(Water)\", the model incorrectly predict. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  segmentation <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In dilated convolutional layers, filter weights are emplo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  increasing of depth brings benefits to representation power <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, they still belong to the family of linear models and are claimed to be di cult to estimate <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Moreover, they are known to have only marginal improvemen ag recommendation. With one hidden layer only, our NFM signi cantly outperforms FM (the ocial LibFM <ref type=\"bibr\" target=\"#b27\">[28]</ref> implementation) with a 7.3% improvement. Compared to the s te) linear models. In other words, the predicted target \u02c6 (x) is linear w.r.t. each model parameter <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Formally, for each model parameter \u03b8 \u2208 {w 0 , {w i }, { i itive embedding-based models that are speci cally designed for sparse data prediction:</p><p>-LibFM <ref type=\"bibr\" target=\"#b27\">[28]</ref>. is is the o cial implementation <ref type=\"foot\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: till very effective Bag of Embeddings model <ref type=\"bibr\" target=\"#b45\">(White et al. 2015;</ref><ref type=\"bibr\" target=\"#b0\">Arora, Liang, and Ma 2017)</ref> showing that, even in this case, A re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng over graph structures using convolution operators that offer promise as an embedding methodology <ref type=\"bibr\" target=\"#b16\">[17]</ref>. So far, graph convolutional networks (GCNs) have only bee nificant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Lastly, we probe the expressive capability of our approac \"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN algorithm <ref type=\"bibr\" target=\"#b16\">[17]</ref> is designed for semi-supervised learning in a transductive r is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework <ref type=\"bibr\" target=\"#b16\">[17]</ref>. In particular, we can derive an inductive variant of the  regator convolutional since it is a rough, linear approximation of a localized spectral convolution <ref type=\"bibr\" target=\"#b16\">[17]</ref>. An important distinction between this convolutional aggre utional\" variant of GraphSAGE is an extended, inductive version of Kipf et al's semi-supervised GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we term this variant GraphSAGE-GCN. We test unsupervised  ctive setting, where it can be extensively trained on a single, fixed graph. (That said, Kipf et al <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref> found that GCN-b d=\"foot_2\">Note that this differs from Kipf et al's exact equation by a minor normalization constant<ref type=\"bibr\" target=\"#b16\">[17]</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place  convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this work we b our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN  less\" GCN approach has parameter dimension O(|V|), so this requirement is not entirely unreasonable <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>.</p><p>Following T \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>). The majority of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: usted so that it can fit different models and datasets.</p><p>Inspired by the success of Focal Loss <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we estimate \ud835\udf14 (\ud835\udc62, \ud835\udc56) with a function of \ud835\udc53 ( \u0177\ud835\udc62\ud835\udc56 ) that ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , such as graphs which encode the pairwise relationships. This includes examples of social networks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, protein interfaces <ref type=\"bibr\" target=\"#b3\">[4]</ref>, assification, including Cora, Citeseer, Pubmed <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Reddit <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Intensive experiments verify the effectiveness of our metho lf-attention strategy.</p><p>More recently, two kinds of sampling-based methods including GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGCN <ref type=\"bibr\" target=\"#b20\">[21]</ref> were d and Extensions</head><p>Relation to other sampling methods. We contrast our approach with GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGC-N <ref type=\"bibr\" target=\"#b20\">[21]</ref> regar and Pubmed <ref type=\"bibr\" target=\"#b10\">[11]</ref>  community different posts belong to in Reddit <ref type=\"bibr\" target=\"#b2\">[3]</ref>. These graphs are varying in sizes from small to large. Part  set to be 16. For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The numbers of the sampling nodes for all layers excluding  \"><head n=\"7.1\">Alation Studies on the Adaptive Sampling</head><p>Baselines. The codes of GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGCNN <ref type=\"bibr\" target=\"#b20\">[21]</ref> provi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: three comparatively simple U-Net models that contain only minor modifications to the original U-Net <ref type=\"bibr\" target=\"#b5\">[6]</ref>. We omit recently proposed extensions such as for example th  we focus our efforts on designing an automatic training pipeline for these models.</p><p>The U-Net <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a successful encoder-decoder network that has received a  tation framework for the medical domain that directly builds around the original U-Net architecture <ref type=\"bibr\" target=\"#b5\">[6]</ref> and dynamically adapts itself to the specifics of any given . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d by shared fully connected layers and a MLP for each student. A similar approach was also taken in <ref type=\"bibr\" target=\"#b4\">(Kandemir et al., 2014)</ref> for the prediction of affect (mood) by l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ning</head><p>We formulate the training set in the setting of Pairwise Approach of Learning to Rank <ref type=\"bibr\" target=\"#b60\">[61]</ref>. It means that D train = {(d q and support set S (i) . We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b23\">Hill et al., 2016;</ref><ref type=\"bibr\" target=\"#b11\">Conneau and Kiela, 2018;</ref><ref type=\"bibr\" target=\"#b39\">McCann et al., 2017;</ref><ref type=\"bibr\" target=\"#b48\">Peters et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: was created using alignments from an SGMM-HMM system trained using Kaldi recipe 's5', model 'tri4b' <ref type=\"bibr\" target=\"#b20\">(Povey et al., 2011)</ref>.</p><p>The 14 hour subset was first used t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: spectral interpretation and vertex domain localization <ref type=\"bibr\" target=\"#b129\">[130]</ref>, <ref type=\"bibr\" target=\"#b130\">[131]</ref>. Notions of stationarity can help develop probabilistic  essing methods leading to graph-based Wiener filtering <ref type=\"bibr\" target=\"#b131\">[132]</ref>, <ref type=\"bibr\" target=\"#b130\">[131]</ref>.</p><p>A study of vertex/spectral localization and uncer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: parameters including reflection of the environment <ref type=\"bibr\" target=\"#b20\">[21]</ref>, bumps <ref type=\"bibr\" target=\"#b8\">[9]</ref>, transparency <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecent embedding for node i. The use of max operator is inspired from learning on general point sets <ref type=\"bibr\" target=\"#b34\">(Qi et al., 2017)</ref>. By applying max-pooling operator element-wis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e majority voting and the weighted majority voting that takes worker reliability into consideration <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>.</p><p>In this pape eighted majority voting (WMV) by putting different weights on workers to measure worker reliability <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>.</p></div> <div xml e it is intractable to directly solve problem ( <ref type=\"formula\" target=\"#formula_9\">9</ref>) or <ref type=\"bibr\" target=\"#b9\">(10)</ref>, we introduce the structured mean-field assumption on the p <p>Infer y: Fixing the distributions of \u03a6 and \u03b7 at their optimum q * , we find y by solving problem <ref type=\"bibr\" target=\"#b9\">(10)</ref>. To make the prediction more efficient, we approximate the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ansposes and one batch matrix multiplication, which is a common pattern in the multi-head attention <ref type=\"bibr\" target=\"#b44\">[46]</ref> in language models. Similar to single operator benchmark, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, regularizes the spectral norm of the weight matrix at each i et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. <ref type=\"bibr\" target=\"#b7\">[8]</ref> utilizes Lipschitz p et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, keep \u03b2 constant across layers. These harder constraints over get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. <ref type=\"bibr\" target=\"#b7\">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustne  utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks. Unlike <ref type=\"bibr\" target=\"#b7\">[8]</ref>, our approach does not require a predetermined set of hyper- 11]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b7\">[8]</ref>. <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" their papers: \u03b2 = 1.0, 1.6, 2.0. The 2 works given in <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b7\">[8]</ref> may be seen as subsets of the works given in <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> allow end-to-end differentable losses over data with arbitr for unsupervised losses, most work follows the semi-supervised setting for node classification from <ref type=\"bibr\" target=\"#b28\">[29]</ref>. For a complete introductions to the vast topic we refer i ider transductive GNNs that output a single embedding per node. Graph convolutional networks (GCNs) <ref type=\"bibr\" target=\"#b28\">[29]</ref> are simple yet effective <ref type=\"bibr\" target=\"#b50\">[5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rs. The design of our architecture is inspired by recent progress in computer vision, in particular <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b6\">He  ch deeper networks <ref type=\"bibr\" target=\"#b12\">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref t erarchical manner. Our architecture can be in fact seen as a temporal adaptation of the VGG network <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>. We have also investigated the s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the entities due to their typeagnostic nature <ref type=\"bibr\" target=\"#b25\">[Xie et al., 2016</ref><ref type=\"bibr\" target=\"#b7\">, Jain et al., 2018]</ref>. Since PSL-KGI is able to predict entity ty sion.</p><p>(ii) Explicit type supervised models also outperform the implict type supervised models <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>.</p><p>The margin of improvement is large wh  on the use of ontological rules (exemplified by PSL-KGI) and embeddings (we use ComplEx, ConvE and <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>). Rule induction methods are orthogonal to o  target=\"#b3\">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a different approach <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> propose extending standard KG embeddings wit es for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> as follows:</p><p>Instead of just using the  are our explicitly supervised TypeE-X methods with the implicitly supervised embeddings proposed by <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>.</p><p>\u2022 In Section 6.3, we analyse how our  e supervision with the unsupervised type-compatible embeddings-based method proposed by Jain et al. <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>. As these results indicate, while explicitly nificantly improves the relation scores, improving weighted F1 up to 18% (over NELL).</p><p>Dataset <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>. We showed their performance on existing dat d>Table 7 :</head><label>7</label><figDesc>Weighted F1 scores on relation triples in the test set by<ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> and TypeE-ComplEx.Anecdotes. Looking at the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion is built.</p><p>Objective Function We optimize the parameters of PNet using REINFORCE algorithm <ref type=\"bibr\" target=\"#b24\">(Williams 1992</ref>) and policy gradient methods <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: th but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing  pe=\"bibr\" target=\"#b26\">28]</ref> on ImageNet <ref type=\"bibr\" target=\"#b19\">[21]</ref>, Kim et al. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose two very d /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">Datasets</head><p>By following <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b21\">23]</ref>, we use a training mmarizes quantitative results on the four testing sets, by citing the results of prior methods from <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. The two DRRN mode pth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods<ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing  meters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30   respectively. On the one hand, to accelerate the convergence speed of very deep networks, the VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> is trained with a very high learning rate (10 \u22121 , instead on focuses on three most related work to ours: ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Fig. < on. Denoting the input as x and the underlying mapping as H(x), the residual mapping is defined as  <ref type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c)   <ref type=\"table\">1</ref>. Strategies used in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and DRRN.  \">VDSR</head><p>Differing from ResNet that uses residual learning in every few stacked layers, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> introduces GRL, i.e., residual learning between the input  structure of DRRN is illustrated in Fig. <ref type=\"figure\" target=\"#fig_4\">5</ref>. Actually, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> can be viewed as a special case of DRRN, i.e., when U = 0, r that, for each original image, we have 7 additional augmented versions. Besides, inspired by VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, we also use scale augmentation to train our model, and im  epochs. Since a large learning rate is used in our work, we adopt the adjustable gradient clipping <ref type=\"bibr\" target=\"#b12\">[13]</ref> to boost the convergence rate while suppressing exploding   VDSR re-implementation also uses BN and ReLU as the activation functions, unlike the original VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> that does not use BN. These results are faithful since our ese results are faithful since our VDSR re-implementation achieves similar benchmark performance as <ref type=\"bibr\" target=\"#b12\">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref  RCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRRN are illustrated in Fig. <ref type=\"figure\" target /1.0\"><head n=\"4.5.\">Discussions</head><p>Since global residual learning has been well discussed in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, in this section, we mainly focus on local residual learni ucture. Local Residual Learning To demonstrate the effectiveness of LRL, DRRN is compared with VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which has no LRL. For fair comparison, the depth and numb 5]</ref> and FSRCNN <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Very deep models (d \u2265 20) include VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, RED <ref  ameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30<r  ResNet<ref type=\"bibr\" target=\"#b7\">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c)  Ratio (PSNR) performance of several recent CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar me depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and non-DL <ref typ images are of the same size. For fair comparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  as matching networks and bi-LSTM ( <ref type=\"bibr\" target=\"#b13\">[14]</ref>) or memory-networks ( <ref type=\"bibr\" target=\"#b9\">[10]</ref>) which are learned using a meta-learning approach: they aim n systems that learns to predict on novel problems based only on few labeled examples. For example, <ref type=\"bibr\" target=\"#b9\">[10]</ref> propose to use the recent memory-augmented neural network,  for now. The work of <ref type=\"bibr\" target=\"#b14\">[15]</ref> propose an extension of the model of <ref type=\"bibr\" target=\"#b9\">[10]</ref>, where the true label of the observed instance is withheld  llow a similar principle to what has been recently presented for one-shot learning problems, e.g in <ref type=\"bibr\" target=\"#b9\">[10]</ref>. It aims at extending the basic principle of training in ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ref><ref type=\"bibr\" target=\"#b27\">29,</ref><ref type=\"bibr\" target=\"#b24\">26]</ref>, optimization <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b76\">79,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le optimization methods usually can be divided into two groups: supervised and unsupervised methods <ref type=\"bibr\" target=\"#b15\">(Grybas et al., 2017)</ref>. Supervised scale optimization methods ar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: do not use gazetteers or any external labeled resources. The best score reported on this task is by <ref type=\"bibr\" target=\"#b26\">Luo et al. (2015)</ref>. They obtained a F 1 of 91.2 by jointly model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a contemporary CNN <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> that is fine-tuned on LR images. The remaining question is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g approach to solve image semantic segmentation problems <ref type=\"bibr\" target=\"#b2\">[3]</ref>[4] <ref type=\"bibr\" target=\"#b4\">[5]</ref>. For example, methods based on fully convolutional network ( and achieved better results compared with FCN-8s <ref type=\"bibr\" target=\"#b3\">[4]</ref> and SegNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>. In summary, these networks mainly change the depth and widt. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ally integrates the informations of different levels via upsam- pling and concatenating as HyperNet <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Different from the refinement strategy like stacked hourg mid output from the GlobalNet: 1) Concatenate (Concat) operation is directly attached like HyperNet <ref type=\"bibr\" target=\"#b20\">[21]</ref>,</p><p>2) a bottleneck block is attached first in each lay. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urrent work further shows the success of search-based unsupervised text generation for paraphrasing <ref type=\"bibr\" target=\"#b15\">(Liu et al., 2020)</ref> and summa-rization <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e Visual Instruction Set (VIS) in UltraSPARC TM <ref type=\"bibr\" target=\"#b15\">[16]</ref> and FBRAM <ref type=\"bibr\" target=\"#b17\">[18]</ref> from Sun Microsystems, MMX TM Technology <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rial examples, some methods attempt to detect them instead <ref type=\"bibr\" target=\"#b22\">[21,</ref><ref type=\"bibr\" target=\"#b24\">23]</ref>. However, most of the non-certified defenses demonstrate th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d parameter (Equation <ref type=\"formula\">7</ref>). We train a word piece convolutional LM (ConvLM) <ref type=\"bibr\" target=\"#b17\">[18]</ref> on the text data set described in Section 4.1 using the sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b20\">[21]</ref> and the introduction of the TAGE predictor by Seznec and Michaud <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In practice, on the traces distributed for the first two  e predictors targeting special categories of branches with a state-of-the-art main predictor (TAGE, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, OGEHL <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Piecewis L <ref type=\"bibr\" target=\"#b7\">[8]</ref>), e.g. a loop predictor with the TAGE predictor in L-TAGE <ref type=\"bibr\" target=\"#b25\">[26]</ref> or the address-branch correlator in <ref type=\"bibr\" targe ns/1.0\"><head n=\"3.\">Background on the TAGE Predictor</head><p>The TAGE predictor was introduced in <ref type=\"bibr\" target=\"#b25\">[26]</ref> and is the core predictor of the L-TAGE predictor that won h a single 4-bit counter USE_ALT_ON_NA was found to allow to (slightly) improve prediction accuracy <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The prediction computation algorithm is as follows:</p><p ts showed that one can use wider tag for long histories for a better tradeoff. Previous experiments <ref type=\"bibr\" target=\"#b25\">[26]</ref> have shown that the TAGE predictor performs efficiently on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: raining graph to boost accuracy without affecting the inference graph, including auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</re nvergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers <ref type=\"bibr\" target=\"#b18\">[19]</ref>. However, auxiliary classifiers require specific new desig tei-c.org/ns/1.0\"><head n=\"3.1\">Generation of training graph</head><p>Similar to auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we add several new classifier heads into the original net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e proposed NARM model uses 50-dimensional embeddings for the items. Optimization is done using Adam <ref type=\"bibr\" target=\"#b14\">[15]</ref> with the initial learning rate sets to 0.001, and the mini. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, natural language processing <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, computer vision <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecently, a novel density based clustering method, named Fast search-and-find of Density Peaks (FDP) <ref type=\"bibr\" target=\"#b32\">[33]</ref> , was proposed. This algorithm assumes that cluster center iguez and Laio proposed a novel density-based clustering method by finding density peaks called FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> . FDP discovers clusters by a two-phase process. First, lo  SNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> , KNNC <ref type=\"bibr\" target=\"#b39\">[40]</ref> , FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> , 3DC <ref type=\"bibr\" target=\"#b22\">[23]</ref> , STClu <r rget=\"#b7\">[8]</ref> . KNNC <ref type=\"bibr\" target=\"#b39\">[40]</ref> (ii) No other parameters. FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> (i)</p><p>The objects with top C t gamma values are chosen object pairs. We vary \u03b2 value in {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} according to the recommendation in <ref type=\"bibr\" target=\"#b32\">[33]</ref> .</p><p>(ii) Density based on K nearest neighbors. The par ch cluster. The statistical error of the estimated density on such a small set of pictures is large <ref type=\"bibr\" target=\"#b32\">[33]</ref> . Therefore, for datasets consisting of clusters with few . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sufficient internet infrastructure indicate that this function is fulfilled by radio phone-in shows <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atch from the surrounding patches by solving a large ridge regression problem extremely efficiently <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. It has proved to be r to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>.</p></div> <div xmln p://www.tei-c.org/ns/1.0\"><head n=\"2.\">Related work</head><p>Since the seminal work of Bolme et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the Correlation Filter has enjoyed great popularity within  To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window <ref type=\"bibr\" target=\"#b3\">[4]</ref> and the final template is cropped <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\">Uz-Zaman et al., 2013;</ref><ref type=\"bibr\" target=\"#b26\">Minard et al., 2015;</ref><ref type=\"bibr\" target=\"#b22\">Llorens et al., 2015;</ref><ref type=\"bibr\" target=\"#b29\">Ning et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b15\">16]</ref> further extend the deep models for multimodal learning. <ref type=\"bibr\" target=\"#b16\">[17]</ref> design a cross-media learning method based on DNN, and lev network (CNN) with cross autoencoders to learn the latent high-level attributes on crossmodal units <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Finally, we pro very tweets by utilizing a recently proposed cross-media model, namely the Cross Autoencoders (CAE) <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>An auto encoder is a basic unit in deep neural netw s for comparison with previous work, due to the different goal, our results are not comparable with <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Actually, the most related user-level prediction work is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on LR images directly and progressively reconstruct the sub-band residuals of HR images. Tai et al. <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed deep recursive residual network (DRRN) to address the estimated high-quality patch with the same resolution as the input low-quality patch. We follow <ref type=\"bibr\" target=\"#b33\">[34]</ref> to do data augmentation. For each task, we train a single . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Ps \u2206acc % VGG-16 FP (handcraft) <ref type=\"bibr\" target=\"#b31\">[31]</ref> 20% -14.6 RNP (handcraft) <ref type=\"bibr\" target=\"#b33\">[33]</ref> -3.58 SPP (handcraft) <ref type=\"bibr\" target=\"#b49\">[49]< sting state-of-the-art channel reduction methods: FP <ref type=\"bibr\" target=\"#b31\">[31]</ref>, RNP <ref type=\"bibr\" target=\"#b33\">[33]</ref> and SPP <ref type=\"bibr\" target=\"#b49\">[49]</ref>. All the ent pruned layers can be summed up linearly, which does not stand according to our experiments. RNP <ref type=\"bibr\" target=\"#b33\">[33]</ref> groups all convolutional channels into 4 sets and trains a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd listwise approaches <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Speci cally, the pairwise methods consider the preference  t <ref type=\"bibr\" target=\"#b1\">[2]</ref>, ListNet <ref type=\"bibr\" target=\"#b3\">[4]</ref>, AdaRank <ref type=\"bibr\" target=\"#b35\">[36]</ref>, and MDPRank <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b11\">(Dolan &amp; Brockett, 2005)</ref>, and natural language inference (NLI) <ref type=\"bibr\" target=\"#b8\">(Dagan et al., 2006;</ref><ref type=\"bibr\" target=\"#b0\">Bar-Haim et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1\">[12,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>, add compiler-inserted instruction prefetches <ref type=\"bibr\" target=\"#b18\">[19]</ref>, or perform call graph prefetching <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b35\">36]</ref> are matrix factorization extensions that add feature-related regularization terms. <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> model features as late. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al influence. Indeed, extensive work has been done on social influence prediction in the literature <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta ial equations extended from the classic 'Susceptible-Infected' (SI) model; Most recently, Li et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> proposed an end-toend predictor for inferring cascade size  efforts to detect those global patterns automatically using deep learning, e.g., the DeepCas model <ref type=\"bibr\" target=\"#b25\">[26]</ref> which formulate cascade prediction as a sequence problem a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: his paper, we follow a simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>: a query matches a ke et=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targe  x k can be images <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, patches <ref type=\"bibr\" target=\"#b45\">[46]</ref>, or cont k can be identical <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, partially shared <ref type=\"bibr\" target=\"#b45\">[46,</ref> stance discrimination task in <ref type=\"bibr\" target=\"#b60\">[61]</ref>, to which some recent works <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> are related.</p><p>F tive pair if they originate from the same image, and otherwise as a negative sample pair. Following <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, we take two random . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n shown to be critical for relevance matching <ref type=\"bibr\" target=\"#b5\">(Dai et al., 2018;</ref><ref type=\"bibr\" target=\"#b25\">Rao et al., 2019)</ref>. On the other hand, the contextual encoder en \">(Lin and Efron, 2013;</ref><ref type=\"bibr\" target=\"#b18\">Lin et al., 2014)</ref>, as prepared by <ref type=\"bibr\" target=\"#b25\">Rao et al. (2019)</ref>, where each dataset contains around 50 querie ults on each dataset.</p><p>For the tweet search task, we mostly follow the experimental setting in <ref type=\"bibr\" target=\"#b25\">Rao et al. (2019)</ref>. Baselines include the classic query likeliho ot\" target=\"#foot_1\">2</ref> For L2R, we used LambdaMART (Burges, 2010) on the same feature sets as <ref type=\"bibr\" target=\"#b25\">Rao et al. (2019)</ref> In our experiments, we use trainable 300d wor apture matching signals at different strength levels. Sharing similarities with our architecture is <ref type=\"bibr\" target=\"#b25\">Rao et al. (2019)</ref>, who developed a multi-perspective relevance . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , an optimal relay selection strategy is developed in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. In <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the dynamism of electricity price and the deferrability o onding Nash value V i k N (XN , RN ) using ( <ref type=\"formula\" target=\"#formula_15\">10</ref>) and <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p><formula xml:id=\"formula_21\">End For 1 \u2264 n \u2264 N \u2212 1</fo sponding Nash value V i k n (Xn, Rn) using ( <ref type=\"formula\" target=\"#formula_15\">10</ref>) and <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n the graph h t evw and updating them analogously to equations 1 and 2. Of the existing MPNNs, only <ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref> has used this idea.</p><p>Convolutional N  T v . Note the original work only defined the model for T = 1.</p><p>Molecular Graph Convolutions, <ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref> This work deviates slightly from other MP ed features we include two existing baseline MPNNs, the Molecular Graph Convolutions model (GC) from<ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref>, and the original GG-NN model<ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bust training (e.g. <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>), we tackle various additional challenges: Being the first  arget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> providing guarantees that no perturbation w.r.t. a specific ss of methods based on convex relaxations are of relevance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. They construct a convex relaxation for computing a lower b nity-norm or L2-norm <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, often e.g. \u03f5 &lt; 0.1 This is clearly not practical in our  can often been done efficiently, and by exploiting duality it enables to even train a robust model <ref type=\"bibr\" target=\"#b19\">[20]</ref>. As already mentioned, our work differs significantly from  the ReLU activation function. While there are many ways to achieve this, we follow the approach of <ref type=\"bibr\" target=\"#b19\">[20]</ref> in this work. The core idea is (i) to treat the matrices H  makes this approach rather slow. As an alternative, we can consider the dual of the linear program <ref type=\"bibr\" target=\"#b19\">[20]</ref>. There, any dual-feasible solution is a lower bound on the  appendix. Note that parts of the dual problem in Theorem 4.3 have a similar form to the problem in <ref type=\"bibr\" target=\"#b19\">[20]</ref>. For instance, we can interpret this dual problem as a bac akes the computation of robustness certificates extremely fast. For example, adopting the result of <ref type=\"bibr\" target=\"#b19\">[20]</ref>, instead of optimizing over \u2126 we can set it to</p><formula o this). While there exist more computationally involved algorithms to compute more accurate bounds <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we leave adaptation of such bounds to the graph domain fo odes in the graph. y * t denotes the (known) class label of node t.</p><p>To improve robustness, in <ref type=\"bibr\" target=\"#b19\">[20]</ref> (for classical neural networks) it has been proposed to in oblem of the above linear program is max</p><p>for l = 2, . . . L, (n, j) \u2208 I (l )</p><p>As done in <ref type=\"bibr\" target=\"#b19\">[20]</ref> we can exploit complementarity of the ReLU constraints cor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: opular approach for learning on hypergraph-structured data <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ranslation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequence model <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref>. We examine some of the critical paramet \"6.\">Conclusion</head><p>We presented a broad range of basic experiments with the Transformer model <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref> for English-to-Czech neural machine tran training steps is given but no indication on \"how much converged\" the model was at that point, e.g. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref>. Most probably, the training was run unti r faking the global_step stored in the checkpoint) to make sure the learning rate is not too small. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref> suggest to average the last 20 checkpoint. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: easure theoretical details, see <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> and <ref type=\"bibr\" target=\"#b1\">Daley and Vere-Jones (2008)</ref>.</p></div> <div xmlns=\"http://www.te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b41\">[42]</ref>, we use the distribution of the network parameter with dropout <ref type=\"bibr\" target=\"#b52\">[53]</ref> as q(\u03c9). Consider a neural network with only one layer, wh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b21\">(Levy et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">McCann et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019)</ref>, we propose a new framework that is capable of  formalized as answering the question \"What is the summary?\". Our work is significantly inspired by <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref>, which formalized the task of entity-relation  sk of entity-relation extraction as a multi-turn question answering task. Different from this work, <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> focused on relation extraction rather than NER  target=\"#b22\">Li et al. (2019)</ref> focused on relation extraction rather than NER. Additionally, <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> utilized a template-based procedure for constr nt influence on the final results. Different ways have been proposed for question generation, e.g., <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> utilized a template-based procedure for constr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a trainable and personalized relation scoring func  where GNNs can be used directly, while here we investigate GNNs for heterogeneous KGs. Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC a user-personalized weighted graph, which characterizes user's preferences. To this end, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero \u03bb is better than \u03bb = 0 (the case of Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref>), which justifies our claim that LS regularization can ass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed and distantly supervised entity extraction methods like <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> or weakly supervised phrase extraction approaches relying o domain knowledge bases, general entity recognition methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> do not fit our candidate mention extraction. With low occur #b21\">[22,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> in the text to the knowledge base entities <ref type=\"bibr\" target=\"#b18\">[19]</ref> and acquire relation labels. Their weaknesses lie in the f del with general phrase mining algorithms <ref type=\"bibr\" target=\"#b29\">[30]</ref>, entity linking <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and general cross-sentence relations with corresponding s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: een proposed and achieved state-of-the-art results in SISR <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar  have been proposed for a better performance. Lim et al. proposed a very deep and wide network EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref> by stacking modified residual blocks in which the batch no the performance. Fig. <ref type=\"figure\">3</ref>(Left) depicts a basic residual module used in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ESRGAN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The  ion, we investigate the combination of our RFA framework with the basic residual block used in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Different from the original residual block used in image  N <ref type=\"bibr\" target=\"#b14\">[15]</ref>, MemNet <ref type=\"bibr\" target=\"#b24\">[25]</ref>, EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>, SRMD <ref type=\"bibr\" target=\"#b35\">[36]</ref>, NLRN <ref n the top row, which can ease the training difficulty to some extent (e.g. residual scaling in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>).</p><p>(2) Feature maps after the attention mechanism ten rchitectures. Here we introduce one of the basic architecture used by some state-of-the-art methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  n=\"4.3.\">Evaluation Metrics</head><p>In this paper, we adopt two standard evaluation metrics: BLEU <ref type=\"bibr\" target=\"#b21\">[22]</ref> and METEOR <ref type=\"bibr\" target=\"#b6\">[7]</ref>, which . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction. While the first three strategies have been taken into account in existing performance models <ref type=\"bibr\" target=\"#b10\">[10]</ref>- <ref type=\"bibr\" target=\"#b12\">[12]</ref>, to the best of  of developing the vacation time as the whole succession of switch-over and processing times (as in <ref type=\"bibr\" target=\"#b10\">[10]</ref>), <ref type=\"bibr\" target=\"#b12\">[12]</ref> keeps in the v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ll describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> and <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: i-gate Mixture-of-Experts (MMoE) structure, which is inspired by the Mixture-of-Experts (MoE) model <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the recent MoE layer <ref type=\"bibr\" target=\"#b15\">[1 \"4\">MODELING APPROACHES 4.1 Mixture-of-Experts</head><p>The Original Mixture-of-Experts (MoE) Model <ref type=\"bibr\" target=\"#b20\">[21]</ref> can be formulated as:</p><formula xml:id=\"formula_5\">= n i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  true. In particular we demonstrate that the recently proposed AutoAugment data augmentation policy <ref type=\"bibr\" target=\"#b5\">[6]</ref> achieves state-of-the-art results on the CIFAR-10-C benchmar mentation strategies. Towards this end, we investigated the learned augmentation policy AutoAugment <ref type=\"bibr\" target=\"#b5\">[6]</ref>. AutoAugment applies a learned mixture of image transformati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: retrieval have focused on learning representations from data <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b40\">41]</ref> and modelling short-text similarities <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar e=\"bibr\" target=\"#b30\">31]</ref> or representation-focused <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" ta  the distributed model improves drastically in the presence of more data. Unlike some previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta nt. Our n-graph based input encoding is motivated by the trigraph encoding proposed by Huang et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, but unlike their approach we don't limit our input repres  on the retrieval task, as a baseline in this paper. Both the deep structured semantic model (DSSM) <ref type=\"bibr\" target=\"#b15\">[16]</ref> and its convolutional variant CDSSM <ref type=\"bibr\" targe lated papers that use short text such as title, for document ranking or related tasks. Huang et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> learn a distributed representation of query and title, for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ser intent.</p><p>As a general information modeling method, Heterogeneous Information Network (HIN) <ref type=\"bibr\" target=\"#b17\">[18]</ref>, consisting of multiple types of objects and links, has be y data mining tasks <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this paper, we propose to model the intent recommendati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: entation <ref type=\"bibr\" target=\"#b37\">(Wang et al., 2018)</ref> and multi-resolution segmentation <ref type=\"bibr\" target=\"#b0\">(Benz et al., 2004)</ref> have been widely used to generate image obje nitial segmentation results for HR images. MSEG is developed from the multi-resolution segmentation <ref type=\"bibr\" target=\"#b0\">(Benz et al., 2004)</ref>. It initially generates an oversegmentation  ough the segmentation algorithm used in this study is based on multi-resolution segmentation (MSEG) <ref type=\"bibr\" target=\"#b0\">(Benz et al. (2004)</ref>; <ref type=\"bibr\" target=\"#b36\">Tzotsos and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pplied to the task of single image super-resolution (SISR) <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar of image super-resolution. Thus, in recent image SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, batch normalizatio rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar f type=\"bibr\" target=\"#b13\">[14]</ref>, SRResNet <ref type=\"bibr\" target=\"#b16\">[17]</ref> and EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>). The increasing of depth brings benefits to representatio ding blocks for image super-resolution networks. Compared with vanilla residual blocks used in EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce WDSR-A which has a slim identity mapping pat 5]</ref> leads to better accuracy for deep super-resolution networks. Previous works including EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, BTSRN <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RDN <re e for training SR networks. However, with the increasing depth of neural networks for SR (e.g. MDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> has depth around 180), the networks without batch normaliz ing deeper and deeper (from 3-layer SRCNN <ref type=\"bibr\" target=\"#b3\">[4]</ref> to 160-layer MDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>), training becomes more difficult. Batch normalization lay f type=\"figure\">1</ref>. Two-layer residual blocks are specifically studied following baseline EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Assume the width of identity mapping pathway (Fig. <ref t <p>Figure <ref type=\"figure\">2</ref>: Demonstration of our simplified SR network compared with EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>In this part, we overview the WDSR network architec his part, we overview the WDSR network architectures. We made two major modifications based on EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> super-resolution network.</p><p>Global residual pathway Fi set <ref type=\"bibr\" target=\"#b34\">[35]</ref>  In this part, we show results of baseline model EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> and our proposed WDSR-A and WDSR-B for the task of image b e results suggest that our proposed WDSR-A and WDSR-B have better accuracy and efficiency than EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>. WDSR-B with wider activation also has better or similar p curacy drop with our simpler form.</p><p>Upsampling layer Different from previous state-of-the-arts <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> where one or more  lips and rotations following common data augmentation methods<ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. During training, the input images are also subtracted with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Autoencoder (VAE) <ref type=\"bibr\" target=\"#b6\">[7]</ref> and Generative Adversarial Network (GAN) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, are powerful architectures which can learn complicated dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  \"the deeper the better\" might not be the case in SR. Inspired by the success of very deep networks <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">27,</ref><ref type=\"bibr\" targ p networks could suffer from the performance degradation problem, as observed in visual recognition <ref type=\"bibr\" target=\"#b7\">[8]</ref> and image restoration <ref type=\"bibr\" target=\"#b16\">[17]</r nce Sec. 1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <re iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1.\">ResNet</head><p>The main idea of ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref> is to use a residual learning framework to ease the training iple weight layers in the residual unit) Table <ref type=\"table\">1</ref>. Strategies used in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref t bel>(1)</label></formula><p>where x is the output of the residual unit, h(x) is an identity mapping <ref type=\"bibr\" target=\"#b7\">[8]</ref> : h(x) = x, W is a set of weights (the biases are omitted to ng the recursive block structure, in which several residual units are stacked. Noted that in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, different residual units use different inputs for the ident </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1.\">Residual Unit</head><p>In ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the acti fig_1\"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Simplified structures of (a) ResNet<ref type=\"bibr\" target=\"#b7\">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d parameter (Equation <ref type=\"formula\">7</ref>). We train a word piece convolutional LM (ConvLM) <ref type=\"bibr\" target=\"#b17\">[18]</ref> on the text data set described in Section 4.1 using the sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to perform Chinese NER is to first perform word segmentation and then apply word sequence labeling <ref type=\"bibr\" target=\"#b7\">[Yang et al., 2016;</ref><ref type=\"bibr\">He and Sun, 2017]</ref>.</p> rd information for NER has attracted research attention <ref type=\"bibr\">[Passos et al., 2014;</ref><ref type=\"bibr\" target=\"#b7\">Zhang and Yang, 2018]</ref>. In particular, to exploit explicit word i ational efficiency <ref type=\"bibr\">[Strubell et al., 2017]</ref>.</p><p>Specifically, lattice LSTM <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref> employs double recurrent transition compu hinese word segmentation, character-based name taggers can outperform their word-based counterparts <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. <ref type=\"bibr\" target=\"#b7\">Zhang and  ilation. This method achieves great performance in the English NER task. Lattice LSTM. Lattice LSTM <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref> can model the characters in sequence and  ></head><label></label><figDesc>, Weibo NER[Peng and Dredze, 2015; He and Sun, 2016], and Resume NER<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<re NER<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>.The OntoNotes and MSRA are the newswire d utperform their word-based counterparts <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. <ref type=\"bibr\" target=\"#b7\">Zhang and Yang [2018]</ref> exploit an RNNbased lattice structure to s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b7\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b23\">Zhang et al., 2015a;</ref><ref type=\"bibr\" target=\"#b20\">Yang et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Wang et al., 2  out which words are useful and which words are useless. Therefore, we apply an attention mechanism <ref type=\"bibr\" target=\"#b20\">(Yang et al., 2016)</ref> to get those important words and assemble t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >[19]</ref> as a way to study brain function. We consider the simplest of many types of perceptrons <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a single-layer perceptron consisting of one artificial neur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, which extracts concepts from a collection of documents and everaging linguistic features, or clustering-based methods <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, which cluster concepts to induce an implicit hierarchy.</p 4]</ref> or hierarchical clustering of concepts which implicitly captures the hierarchical relation <ref type=\"bibr\" target=\"#b41\">[42]</ref>. These methods mainly focus on the general domain, harvest. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  restores the high-frequency information, it is widely used in applications such as medical imaging <ref type=\"bibr\" target=\"#b24\">[26]</ref>, satellite imaging <ref type=\"bibr\" target=\"#b27\">[29]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atures with a linear classifier can achieve good performance on different video analysis benchmarks <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>As for task of video-based emotion recognition, few w  3D ConvNets, the operations are performed spatio-temporally by adding an additional time dimension <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Hence such C3D networks preserve the temporal information o , and 2 fully connected layers, followed by a softmax output layer. Other parameters are similar to <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The specific C3D structure used in our implementation is sh del alone can achieve good performance in action recognition <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. And we found that the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ent works have focused on learning deep embeddings that can be used as universal object descriptors <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ  effect of incorporating the CF into the fully-convolutional Siamese framework of Bertinetto et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We find that the CF does not improve results for networks t e performance. For our method, we prefer to build upon the fully-convolutional Siamese architecture <ref type=\"bibr\" target=\"#b2\">[3]</ref>, as it enforces the prior that the appearance similarity fun .\">Fully-convolutional Siamese networks</head><p>Our starting point is a network similar to that of <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which we later modify in order to allow the model to be int t is necessary to combine this with a procedure that describes the logic of the tracker. Similar to <ref type=\"bibr\" target=\"#b2\">[3]</ref>, we employ a simplistic tracking algorithm to assess the uti r during training. We first compare against the symmetric Siamese architecture of Bertinetto et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We then compare the endto-end trained CFNet to a variant wh  random seeds, this would require significantly more resources. Our baseline diverges slightly from <ref type=\"bibr\" target=\"#b2\">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw ). We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Staple <ref type=\"bibr\" target=\"#b1\">[2]</ref> and LCT <ref s=\"http://www.tei-c.org/ns/1.0\"><head>A. Implementation details</head><p>We follow the procedure of <ref type=\"bibr\" target=\"#b2\">[3]</ref> to minimize the loss (equation 2) through SGD, with the Xavi  the following ReLU but not the following pooling layer (if any).Our baseline diverges slightly from<ref type=\"bibr\" target=\"#b2\">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw e xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">Note that this differs from<ref type=\"bibr\" target=\"#b2\">[3]</ref>, in which the target object and search area were instead den ve been introduced <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, raising interest in the tracking community for their simplic rget=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> with CNN features, as proposed in previous work <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s by studying the two-month history made available by Amazon <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b13\">15]</ref>. Though these statistics alone are sufficient for the user  are limited to statistical studies of historical spot prices <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b13\">15]</ref>.</p><p>Game theoretic pricing. Spot pricing is a distribute  tradeoff is the fact that user jobs can have long runtimes spanning many changes in the spot price <ref type=\"bibr\" target=\"#b13\">[15]</ref>. Users then face two key challenges: 1) Users must predict int ensures that the job is sufficiently interruptible. We use p to denote the optimal bid price to <ref type=\"bibr\" target=\"#b13\">(15)</ref>.</p><p>We now observe that the expected running time in <r  of the spot price monotonically decreases, i.e., F \u03c0 (p) is concave, the optimal bid price solving <ref type=\"bibr\" target=\"#b13\">(15)</ref> </p><formula xml:id=\"formula_29\">is p = \u03c8 \u22121 t k t r \u2212 1 , . Comparing <ref type=\"bibr\" target=\"#b17\">(19)</ref> to bidding for a single persistent request in <ref type=\"bibr\" target=\"#b13\">(15)</ref>, we see that <ref type=\"bibr\" target=\"#b17\">(19)</ref> can \"#b13\">(15)</ref>, we see that <ref type=\"bibr\" target=\"#b17\">(19)</ref> can be solved similarly to <ref type=\"bibr\" target=\"#b13\">(15)</ref> in Proposition 5.</p><p>By comparing the costs for multipl t k ts .</p><p>Proof of Proposition 5.</p><p>Proof. By taking the first-order derivative of \u03a6(p) in <ref type=\"bibr\" target=\"#b13\">(15)</ref>  <ref type=\"figure\" target=\"#fig_3\">3</ref>), F \u03c0 (p) is c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: :id=\"formula_3\">l M SE = 1 N N i=1 I i \u2212 \u00cei 2 2 .<label>(4)</label></formula><p>However, Lim et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> experimentally demonstrate that training with MSE loss is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: echniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks <ref type=\"bibr\" target=\"#b24\">(Scarselli et al., 2009)</ref>, which we modify to use gated recurren ucing a sequence of outputs. Here, (1) is mostly achieved by previous work on Graph Neural Networks <ref type=\"bibr\" target=\"#b24\">(Scarselli et al., 2009)</ref>; we make several minor adaptations of   on graphs, including Graph Neural Networks <ref type=\"bibr\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009)</ref>, spectral networks <ref type=\"bibr\" tar ion, we review Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009)</ref> and introduce notation and concepts tha irected edge v \u2192 v , but we note that the framework can easily be adapted to undirected graphs; see <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref>. The node vector (or node representatio v = f * (l v , l CO(v) , l NBR(v) , h (t\u22121) NBR(v)</formula><p>). Several variants are discussed in <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> including positional graph forms, node- l graph forms, node-specific updates, and alternative representations of neighborhoods. Concretely, <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> suggest decomposing f * (\u2022) to be a sum unction g(h v , l v ) that maps to an output. This is generally a linear or neural network mapping. <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> focus on outputs that are independent p r\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b6\">Di Massa et al., 2006;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009;</ref><ref type=\"bibr\">Uwents et al., 2011)</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b23\">[24]</ref> and modeling with uniform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  various forms of artificial noise during training. We distinct regularization methods like dropout <ref type=\"bibr\" target=\"#b46\">(Srivastava et al., 2014)</ref> and task-specific data augmentation t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"><head n=\"4.2.\">Image Classification</head><p>We learn a feature representation on ImageNet ILSVRC <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and compare our method with representative unsupervised l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ual unit rather than bottlenecks.</p><p>For ImageNet, we used the original ResNet architecture from <ref type=\"bibr\" target=\"#b8\">He et al. (2016a)</ref> implemented in PyTorch<ref type=\"foot\" target= </ref> as our large target model for both CIFAR10 and CIFAR100. Note that as originally proposed in <ref type=\"bibr\" target=\"#b8\">He et al. (2016a)</ref>, the smaller, proxy models are also ResNet arc s validation error in all conditions.</p><p>ImageNet. we used the original ResNet architecture from <ref type=\"bibr\" target=\"#b8\">He et al. (2016a)</ref> implemented in Py-Torch<ref type=\"foot\" target h across a wide range of model architectures <ref type=\"bibr\" target=\"#b46\">(Xie et al., 2017;</ref><ref type=\"bibr\" target=\"#b8\">He et al., 2016a;</ref><ref type=\"bibr\" target=\"#b28\">Sandler et al.,  gh across a wide range of model architectures<ref type=\"bibr\" target=\"#b46\">(Xie et al., 2017;</ref><ref type=\"bibr\" target=\"#b8\">He et al., 2016a;</ref><ref type=\"bibr\" target=\"#b28\">Sandler et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  style within or cross speakers based on end-toend TTS model <ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Deep generativ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing process, which leads to better generalization. We develop an approach based on label smoothness <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, which assumes tha therefore learnable <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Inspired by these methods, we design a module of label smo get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Therefore, more re r the unlabeled nodes l * u (E\\V). To solve the issue, we propose minimizing the leave-one-out loss <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Suppose we hold out a single item v and treat it unlabele. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> and have connections to the large literature on metric lear otivated by noise contrastive estimation and N-pair losses <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, wherein the ability to discriminate between signal and noi  <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> or N-pair losses <ref type=\"bibr\" target=\"#b42\">[43]</ref>. Typically, the loss is applied at the last layer of a dee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  consistent results for graph classification (e.g. 2.5 percentage points with GCN on the DD dataset <ref type=\"bibr\" target=\"#b17\">[18]</ref>). We found no improvement for the inductive  <ref type=\"fi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ical, fetch-modify-consistent views of the file system. We have formally specified fork consistency <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and, assuming digital signatures and a collision-resistan hus cannot ever see each other's operations without detecting the attack (as proven in earlier work <ref type=\"bibr\" target=\"#b15\">[16]</ref>).</p><p>One optimization worth mentioning is that SUNDR am cs for an untrusted server. An unimplemented but previously published version of the SUNDR protocol <ref type=\"bibr\" target=\"#b15\">[16]</ref> had no groups and thus did not address write-after-write c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t information on the ConvNet architectures used; they are variants of the Network-in-Network design <ref type=\"bibr\" target=\"#b16\">[17]</ref>. All other methods are initialized from the same pre-train </label><figDesc>VOC 2010 test detection average precision (%). BabyLearning uses a network based on<ref type=\"bibr\" target=\"#b16\">[17]</ref>. All other methods use VGG16.</figDesc><table><row><cell>B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es not preserve the eigenvectors and its effect therefore cannot be calculated precisely. Wu et al. <ref type=\"bibr\" target=\"#b76\">[77]</ref> empirically found that adding self-loops shrinks the graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t item embeddings. We adopt the dynamic setting in our model, more details will be described in \u00a73. <ref type=\"bibr\" target=\"#b3\">4</ref>.</p><p>The basic idea of our work is to learn a recommendation egularization is also included to avoid coincidental high similarities between rarely visited items <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. \u2022 BPR-MF: BPR-MF <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he summation operation may impede the information flow in deep networks.</p><p>Inspired by Densenet <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>, we propose a densely-connected recurrent n et al. 2016;</ref><ref type=\"bibr\" target=\"#b31\">Wu et al. 2016</ref>). More recently, Huang et al. <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref> enable the features to be connected from lo , the summation operation in the residual connection may impede the information flow in the network <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>. Motivated by Densenet <ref type=\"bibr\" tar flow in the network <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>. Motivated by Densenet <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>, we employ direct connections using the con ch to the uppermost layer and all the previous features work for prediction as collective knowledge <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org  max-valued features of every layer affect the loss function and perform a kind of deep supervision <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017</ref>). Thus, we could cautiously interpret the cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng, most notably the group of NLP models known as word2vec <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. A number of recent research publications have proposed wor learn the distributed representations of words in a corpus <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Inspired by it, DeepWalk <ref type=\"bibr\" target=\"#b21\">[2 lelized by using the same mechanism as word2vec and node2vec <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. All codes are implemented in C and C++ and our experiments e distributed representations of words in natural language <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Building on word2vec, Perozzi et al. suggested that the \"c ghborhoods with network semantics for various types of nodes. Second, we extend the skip-gram model <ref type=\"bibr\" target=\"#b17\">[18]</ref> to facilitate the modeling of geographically and semantica p 2 &amp; p 3 ).</p><p>To achieve e cient optimization, Mikolov et al. introduced negative sampling <ref type=\"bibr\" target=\"#b17\">[18]</ref>, in which a relatively small set of words (nodes) are samp aximize the network probability in terms of local structures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, that is:</p><formu d as a so max function <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, that is:</p><formu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: phinx <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Julius <ref type=\"bibr\" target=\"#b3\">[4]</ref>, RASR <ref type=\"bibr\" target=\"#b4\">[5]</ref> in addition to general research activities. These open sourc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
