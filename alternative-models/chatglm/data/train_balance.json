{"content": "The context is: rediction tasks have achieved major advances recently. Encoder-decoder architectures like the U-Net <ref type=\"bibr\" target=\"#b31\">(Ronneberger et al., 2015)</ref> are state-of-the-art methods for the tion. Similar to pixelwise prediction tasks <ref type=\"bibr\" target=\"#b15\">(Gong et al., 2014;</ref><ref type=\"bibr\" target=\"#b31\">Ronneberger et al., 2015)</ref>, node classification tasks aim to mak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  are highly related, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=\"bibr\" target=\"#b1\">(Caruana 1997</ref>) by sharing some meta-knowledge.</p><p>We propose   from existing work, we regard the MRC with unanswerable questions as a multi-task learning problem <ref type=\"bibr\" target=\"#b1\">(Caruana 1997</ref>) by sharing some metaknowledge. Intuitively, answe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ensive concerns in the field of drug design. Structure-based virtual screening, 3D-CNN-based models <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2019)</ref> and other models trained on DUD-E dataset <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b10\">(Graves et al., 2013)</ref> with a Connectionist Temporal Classification (CTC) output layer <ref type=\"bibr\" target=\"#b9\">(Graves et al., 2006;</ref><ref type=\"bibr\">Graves, 2012, Chapter 7)</ _8\">15</ref>) can be efficiently evaluated and differentiated using a dynamic programming algorithm <ref type=\"bibr\" target=\"#b9\">(Graves et al., 2006)</ref>. Given a target transcription y * , the ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: een value predictors to reverse the trust model, trusting the predicted value over the faulted value<ref type=\"bibr\" target=\"#b29\">[30]</ref>,<ref type=\"bibr\" target=\"#b30\">[31]</ref>,<ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  has not interacted before. We use the widely-used protocols <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>: Precision@K, Recall@K, and NDCG@K to evaluate the performa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: targeted network, which corresponds to a more restrictive black box threat model.</p><p>Recent work <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b1\">Bhagoji et al., =\"bibr\" target=\"#b10\">Ilyas et al., 2017)</ref> provides a number of attacks for this threat model. <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> show how to use a basic primitive of zeroth o (x, ) (x l\u22121 + \u03b7 s l ) with s l = \u03a0 \u2202Bp(0,1) \u2207 x L(x l\u22121 , y)<label>(4)</label></formula><p>Indeed, <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> were the first to use finite differences meth ty d=268,203 and thus this method would require 268,204 queries. (It is worth noting, however, that <ref type=\"bibr\" target=\"#b4\">Chen et al. (2017)</ref> developed additional methods to, at least par. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tures in recent papers <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Such deep networks reach top competitive results in the hi riginally used for a face verification task. We also evaluate DCN network architecture described in <ref type=\"bibr\" target=\"#b18\">[19]</ref> without any pre-training. The faces are resized to a fixed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  on social media given users' posts, connections among users, and a small number of labelled users. <ref type=\"bibr\" target=\"#b34\">Rahimi et al. (2018)</ref> apply GCNs with highway connections on thi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: label field is one of the most effective way to construct the connection between neighboring pixels <ref type=\"bibr\" target=\"#b4\">[5]</ref> . It decreases with the number of pixels having the same lab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b5\">5]</ref>,and trusted measurement is a key problem of this technology <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7]</ref>. Trusted computing tre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mental monitoring, intelligent agriculture <ref type=\"bibr\" target=\"#b4\">[5]</ref>, disaster relief <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, urban planning <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r scheduling is also related to other work on DSL for GPUs <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: refetcher, offset prefetchers, and the sandbox method for selecting the prefetch offset dynamically <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Offset prefetching is a generalization of next-line prefe </ref> (this list is not exhaustive).</p><p>Recently, Pugsley et al. introduced Sandbox prefetching <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The Sandbox prefetcher prefetches line X + D when line X  dge, the first published full-fledged offset prefetcher is the Sandbox prefetcher by Pugsley et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref>. However, the offset selection mechanism in the Sandbox pr owledge, the SBP prefetcher of Pugsley et al. is the first published full-fledged offset prefetcher <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The SBP prefetcher is cost-effective and was shown to out  with actual prefetches.</p><p>We implemented the SBP prefetcher as described in the original paper <ref type=\"bibr\" target=\"#b25\">[26]</ref>, but with a few modifications to make the comparison with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -end (E2E) SLU system <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ <ref type=\"bibr\" target=\"#b1\">(Budzianowski et al. 2018)</ref>, M2M <ref type=\"bibr\" target=\"#b15\">(  Asri et al. 2017)</ref>, M2M <ref type=\"bibr\" target=\"#b15\">(Shah et al. 2018</ref>) and Multi-WOZ <ref type=\"bibr\" target=\"#b1\">(Budzianowski et al. 2018</ref>). These datasets have utilized a varie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b8\">[9]</ref>. Thanks to these advances, voice assistant devices such as Google Home <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> , Amazon Alexa or Sam ype=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b34\">35</ref>]. An \"acoustic simulator\" <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> is used to generate  /ref>. The acoustic simulator in Fig. <ref type=\"figure\">1</ref> is similar to what we described in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. One difference comp ract LengthPerturbation (VTLP)<ref type=\"bibr\" target=\"#b0\">[1]</ref> , and Acoustics Simulator (AS)<ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, respectively.</figD \">[2,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. One difference compared to our previous one in <ref type=\"bibr\" target=\"#b1\">[2]</ref> is that we do not pre-calculate room impulse responses, but  rver, we ran the VTLP data augmentation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, acoustic simulator <ref type=\"bibr\" target=\"#b1\">[2]</ref> and feature extraction modules shown in Fig. <ref type=\"figu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the goal of humancomputer interaction popular, and it has been a research hotspot in recent decades <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Automatic Speech Recognition (ASR) refers to the task of an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: search of graph embedding ( <ref type=\"bibr\" target=\"#b15\">Perozzi, Al-Rfou, and Skiena, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Grover and Leskovec, 2016)</ref>, where graph topology and node relati he performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref>, VGAE <ref type=\"bibr\" target=\"#b9\"> ds. DeepWalk <ref type=\"bibr\" target=\"#b15\">(Perozzi, Al-Rfou, and Skiena, 2014)</ref> and Node2vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref> are representative random walk-based ns from nodes' raw features, without using any graph structure information incorporated. \u2022 Node2Vec <ref type=\"bibr\" target=\"#b4\">(Grover and Leskovec, 2016)</ref>: This approach is an extension of Wo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b0\">[1]</ref>, Giffin, et al., <ref type=\"bibr\" target=\"#b22\">[23]</ref>, Spivey <ref type=\"bibr\" target=\"#b23\">[24]</ref>, Bond and McKinley <ref type=\"bibr\" target=\"#b24\">[25]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ER score, significantly outperforming previous BERT and Graph Neural Network (GNN) based approaches <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>. Our experiments demonstrate KGAT's strong   2018)</ref> further incorporates evidence identification to improve claim verification.</p><p>GEAR <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> formulates claim verification as a graph re tei-c.org/ns/1.0\"><head n=\"3.1\">Reasoning with Evidence Graph</head><p>Similar to previous research <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, KGAT constructs the evidence graph G by us narios and produces a probability P (y|c, D) to predict claim label y. Different from previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, we follow the standard graph label predict sentation v p . The aggregation is done by a graph attention mechanism, the same with previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>.</p><p>It first calculate the attention wei n</head><p>The per-node predictions are combined by the \"readout\" function in graph neural networks <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>, where KGAT uses node kernels to learn the  ds without pre-training. BERT-pair, BERT-concat and GEAR are three baselines from the previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>. BERT-pair and BERTconcat regard claim-evid eriments are all based on ESIM sentence retrieval, which is the one used by GEAR, our main baseline <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org head n=\"6\">Case Study</head><p>Table <ref type=\"table\">5</ref> shows the example claim used in GEAR <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> and the evidence sentences retrieved by ESI htweight backpacker, inventor, author and global adventurer. Label: SUPPORT Table5: An example claim<ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019)</ref> whose verification requires multiple pieces \"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Li et al., 2019;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Soleimani et a d is kept the same with previous work <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Soleimani et a l keeps the same as the previous work <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et al., 2019)</ref>. The base version of BERT is used to impleme  KGAT is the best on all testing scenarios. With ESIM sentence retrieval, same as the previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Hanselowski et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  a non-empty intersection contained in a set of measure zero <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>.</p><p>The optimal transport (OT) distance is an alternativ s on a low dimensional manifold of the input embedding space <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>, which is the case for natural images. It has been widely a ng the difficulties encountered in the original GAN training <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. This technique has been further extended to generating dis  generative modeling <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar s coupled over all input samples. Wasserstein GAN and OT-GAN <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Generative Adversar act minimization of Eqn.(4) over T is intractable in general <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: R-10 dataset with ResNet-32, ResNet-110 <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and DenseNet-40-12 <ref type=\"bibr\" target=\"#b10\">[11]</ref>. ResNets and DenseNets for CIFAR are all designed to have . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (amino acids). Specifically, we augment the autoregressive self-attention of recent sequence models <ref type=\"bibr\" target=\"#b6\">[7]</ref> with graph-based descriptions of the 3D structure. By compos dependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention <ref type=\"bibr\" target=\"#b6\">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d  Structured Transformer model that draws inspiration from the selfattention based Transformer model <ref type=\"bibr\" target=\"#b6\">[7]</ref> and is augmented for scalable incorporation of relational in an attend to a separate subspace of the embeddings via learned query, key and value transformations <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>The queries are derived from the current embedding at een these self-attention layers and position-wise feedforward layers as in the original Transformer <ref type=\"bibr\" target=\"#b6\">[7]</ref>. We stack multiple layers atop each other, and thereby obtai rained models using the learning rate schedule and initialization of the original Transformer paper <ref type=\"bibr\" target=\"#b6\">[7]</ref>, a dropout rate of 10% <ref type=\"bibr\" target=\"#b41\">[42]</ f their structure. Our model augments the traditional sequence-level self-attention of Transformers <ref type=\"bibr\" target=\"#b6\">[7]</ref> with relational 3D structural encodings and is able to lever ndependent, contextual embeddings of each residue in the 3D structure with multi-head self-attention<ref type=\"bibr\" target=\"#b6\">[7]</ref> on the spatial k-nearest neigbors graph. An autoregressive d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . 2017b) by 37.0%, the Deep3 system (Raychev et al., 2016a) by 29.7%, and an adaptation of Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> for code prediction by 30.0%. These are sig \"bibr\" target=\"#b45\">, Yang and Xiang, 2019)</ref>. We include an adaptation of path-based Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> in our evaluations and show that our models Devanbu, 2017b)</ref>, Deep3 <ref type=\"bibr\" target=\"#b35\">(Raychev et al., 2016a)</ref>, Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref>).</p><p>Fig 3 puts these models in perspect #b35\">(Raychev et al., 2016a</ref>)) vs. TravTrans+ ; \u2022 from 43.6% to 73.6% when comparing Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref>  Thus, we argue that our proposal of using  \">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequences. We also include Code2Seq <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> to compare our efforts against a popular co ven a method body, how well can Code2Seq generate the correct method name? The training proposed in <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a)</ref> is not well suited for next token predictio tsis et al., 2020</ref><ref type=\"bibr\" target=\"#b28\">, Li et al., 2018)</ref>), to paths in an AST <ref type=\"bibr\" target=\"#b7\">(Alon et al., 2019a</ref><ref type=\"bibr\">(Alon et al., ,b, 2020))</re iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Code2Seq</head><p>Code2Seq is a model by <ref type=\"bibr\" target=\"#b7\">Alon et al. 2019a</ref> that embeds code snippets by embedding AST pat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hts into global model behavior.</p><p>Triggers are a new form of universal adversarial perturbation <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017)</ref> adapted to discrete textual inp or anyone to fool machine learning models. Moreover, universal attacks often transfer across models <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017)</ref>, which further decreases attack e adversarial threat is higher if an attack is universal: using the exact same attack for any input <ref type=\"bibr\" target=\"#b18\">(Moosavi-Dezfooli et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Br. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> on Chinese poetry generation have been mostly rulebased or te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: timization <ref type=\"bibr\" target=\"#b3\">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type=\"bibr\" target=\"#b11\">(Finn et al., 2017)</ref>. In essence, we turn the gradient-based opt </ref> or initial weights that enable rapid adaptation to new tasks or domains in few-shot learning <ref type=\"bibr\" target=\"#b11\">(Finn et al., 2017)</ref>.</p><p>Meta-gradients (e.g., gradients w.r. s is expensive both from a computational and a memory point-of-view.</p><p>To alleviate this issue, <ref type=\"bibr\" target=\"#b11\">Finn et al. (2017)</ref> propose a first-order approximation, leading thus more knowledge than all competing methods. First-order refers to the approximation proposed by <ref type=\"bibr\" target=\"#b11\">Finn et al. (2017)</ref>, i.e. ignoring all second-order derivatives.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nal GNNs can operate on the transformed homogeneous graphs <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. This is a two-stage approach and requires hand-crafted met <head n=\"3.2\">Meta-Path Generation</head><p>Previous works <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> require manually defined meta-paths and perform Graph Neura. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ierarchical GRU model so that the model can better capture the important information of a document. <ref type=\"bibr\" target=\"#b27\">Wang and Tian (2016)</ref> incorporate the residual networks <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ) method <ref type=\"bibr\" target=\"#b12\">[13]</ref> exceeds previous non-Deep SR methods (supervised <ref type=\"bibr\" target=\"#b21\">[22]</ref> or unsupervised <ref type=\"bibr\" target=\"#b4\">[5,</ref><re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: While previous learning simulation approaches <ref type=\"bibr\" target=\"#b17\">(Li et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Ummenhofer et al., 2020)</ref> have been highly specialized for parti lution 3D water scenario with randomized water position, initial velocity and volume, comparable to <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>'s containers of water. We used SPlisHS d boundary particles, a loss function that weights slow particles with few neighbors more heavily). <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> reported CConv outperformed DPI, so we  is to, during training, provide the model with its own predictions by rolling out short sequences. <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>, for example, train with two-step pred e=\"bibr\" target=\"#b14\">(Kipf &amp; Welling, 2016)</ref> work. The full CConv update as described in <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> is,</p><formula xml:id=\"formula_19\">f  e comparisons.</head><p>We implemented the CConv model, loss and training procedure as described by <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>. For simplicity, we only tested the CC  appended a particle type learned embedding to the input node features.</p><p>To be consistent with <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref>, we used their batch size of 16, learn \"><head>D. Supplementary baseline comparisons D.1. Continuous convolution (CConv)</head><p>Recently <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> presented Continuous Convolution (CCon eral tasks.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Interpretation.</head><p>While <ref type=\"bibr\" target=\"#b33\">Ummenhofer et al. (2020)</ref> state that \"Unlike previous approaches. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on. Some two-stage approximation methods like BCCF <ref type=\"bibr\">[Zhou and Zha, 2012]</ref>, PPH <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2014]</ref>, CH <ref type=\"bibr\" target=\"#b10\">[Liu et 14]</ref>, CH <ref type=\"bibr\" target=\"#b10\">[Liu et al., 2014]</ref> incur large quantization loss <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref  loss <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2016]</ref> is easy to fall into a local optimum becau ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless, <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2016]</ref> proposed that those two-stage methods suff sentation. Later, following this, some two stage methods <ref type=\"bibr\">[Zhou and Zha, 2012;</ref><ref type=\"bibr\" target=\"#b12\">Zhang et al., 2014]</ref> which relaxed binary constraints at first a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >[12]</ref>.</p><p>On the other hand, many researchers have found convolutional networks (ConvNets) <ref type=\"bibr\" target=\"#b16\">[17]</ref>  <ref type=\"bibr\" target=\"#b17\">[18]</ref> are useful in e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rs. For example, Gaussian-process-based optimization methods could incorporate gradient information <ref type=\"bibr\" target=\"#b35\">(Solak et al., 2003)</ref>. Such methods could make use of parallel e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r any combination of input parameters ( \u00a7 3).</p><p>\u2022 Based on the red-blue pebble game abstraction <ref type=\"bibr\" target=\"#b34\">[34]</ref>, we provide a new method of deriving I/O lower bounds (Lem o parents (or no children, respectively). Red-Blue Pebble Game Hong and Kung's red-blue pebble game <ref type=\"bibr\" target=\"#b34\">[34]</ref> models an execution of an algorithm in a two-level memory  achinery for deriving I/O lower bounds for general CDAGs. We extend the main lemma by Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref>, which provides a method to nd an I/O lower bound for a gi rresponding pebbling. Hong and Kung use a speci c variant of this partition, denoted as S-partition <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p><p>We rst introduce our generalization of S-partition, ation that each V i performs at least S I/O operations, we phrase the lemma by Hong and Kung: L 1 ( <ref type=\"bibr\" target=\"#b34\">[34]</ref>). e minimal number Q of I/O operations for any valid execu ulation of the CDAG, where a calculation is a sequence of allowed moves in the red-blue pebble game <ref type=\"bibr\" target=\"#b34\">[34]</ref>. Divide the complete calculation into h consecutive subcom d Lemma. We now use the above de nitions and observations to generalize the result of Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> he remaining properties of a valid X -partition S(X ), we use the same reasoning as originally done <ref type=\"bibr\" target=\"#b34\">[34]</ref>.</p><p>erefore, a complete calculation performing q &gt; ( , and therefore may be seen as the last step in the long sequence of improved bounds. Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref> derived an asymptotic bound \u2126 n 3 / \u221a S for the sequential  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"10.1\">General I/O Lower Bounds</head><p>Hong and Kung <ref type=\"bibr\" target=\"#b34\">[34]</ref> analyzed the I/O complexity for general CDAGs in their the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, and knowledge distillation <ref type=\"bibr\" target=\"#b9\">[10 ref>). This structure is very similar to multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, in which different supervised tasks share the same input, as nowledge obtained from each task can be reused by the others <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. However, it is not u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ehring et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Kalchbrenner et al., 2016)</ref>, attention <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref>, or a combination of recurrence and atte ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Transformer</head><p>The Transformer <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref> employs an encoder-decoder structure, co  for all sequences, heads, and positions in a batch using parallel matrix multiplication operations <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref>. Without relative position representatio d><p>We compared our model using only relative position representations to the baseline Transformer <ref type=\"bibr\" target=\"#b10\">(Vaswani et al., 2017)</ref> with sinusoidal position encodings. We g e a deterministic function of position <ref type=\"bibr\" target=\"#b7\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b10\">Vaswani et al., 2017)</ref> or learned representations. Convolutional se in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as <ref type=\"bibr\" target=\"#b10\">Vaswani et al. (2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o  1 = 0.9, \u03b2 2 = 0.98, and = 10 \u22129 . We used the same warmup and decay strategy for learning rate as <ref type=\"bibr\" target=\"#b10\">Vaswani et al. (2017)</ref>, with 4,000 warmup steps. During training. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: een subsequently applied to sentence classification <ref type=\"bibr\" target=\"#b11\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b10\">Kalchbrenner et al., 2014;</ref><ref type=\"bibr\">Zhang et al., 2015)< on six data sets, in particular Stanford Sentiment Treebank (SST). A similar system was proposed in <ref type=\"bibr\" target=\"#b10\">(Kalchbrenner et al., 2014)</ref>, but using five convolutional layer  layer</p><formula xml:id=\"formula_0\">(i)</formula><p>where k is such that the resolution is halved <ref type=\"bibr\" target=\"#b10\">(Kalchbrenner et al., 2014</ref>).</p><p>(iii) K i is followed by max. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fline model lack fine-grained estimation and customized models are not general as desired. Timeloop <ref type=\"bibr\" target=\"#b21\">[21]</ref> and Eyeriss <ref type=\"bibr\" target=\"#b22\">[22]</ref> use . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: orks as a composition of learned components has a long history, dating back at least to the work of <ref type=\"bibr\" target=\"#b12\">Hinton (1988)</ref> on assembling neural networks according to a fami. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, is that repetitive control flow graph traversals lead to r temporal streaming <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> from prefetching approaches that only retrieve a constant n y counting successful prefetches. Hence, like past designs <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, TIFS uses the next-best option, the Recent heuristic, as i target data accesses <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. TIFS adds three logical structures to the chip: a set of S cts the anatomy of the SVB. Our SVB design is adapted from <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. The SVB contains a small fully-associative buffer for temp arget=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. These prefetchers target primarily off-chip data reference  prefetcher to retrieve instruction-cache blocks ahead of the fetch unit for the rest of the stream <ref type=\"bibr\" target=\"#b36\">[37]</ref>.</p><p>Figure <ref type=\"figure\">5</ref> shows the cumulat  the L2 cache (see <ref type=\"bibr\">Section 5)</ref>.</p><p>The term temporal stream, introduced in <ref type=\"bibr\" target=\"#b36\">[37]</ref>, refers to extended sequences of data references that recu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n a single natural image has been long recognized as a powerful prior in many computer vision tasks <ref type=\"bibr\" target=\"#b63\">[64]</ref>. Classical examples include denoising <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b40\">42]</ref>.</p><p>Single image target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" tar tep. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images <ref type=\"bibr\" target=\"#b22\">[24]</ref>. (d) Iterative up and downsampling approach is proposed by can preserve HR components better.</p><p>(c) Progressive upsampling was recently proposed in LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>. It progressively reconstructs the multiple SR images with N <ref type=\"bibr\" target=\"#b20\">[22]</ref>, DRRN <ref type=\"bibr\" target=\"#b40\">[42]</ref>, LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>) on Set5 dataset for 4\u00d7 enlargement.</p><p>the-art methods N <ref type=\"bibr\" target=\"#b20\">[22]</ref>, DRRN <ref type=\"bibr\" target=\"#b40\">[42]</ref>, LapSRN <ref type=\"bibr\" target=\"#b22\">[24]</ref>, and EDSR <ref type=\"bibr\" target=\"#b28\">[30]</ref>. We ca step. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images<ref type=\"bibr\" target=\"#b22\">[24]</ref>. (d) Iterative up and downsampling approach is proposed by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: stimates between-image labels using the clustering technique <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> or kNN-based methods . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, with adversarial training <ref type=\"bibr\" target=\"#b20\">[21]</ref> being one of the most effective methods. It formulates tra  Typically, using more attack iterations (higher value of k) produces stronger adversarial examples <ref type=\"bibr\" target=\"#b20\">[21]</ref>. However, each attack iteration needs to compute the gradi ct to traditional PGD-40.</p><p>We apply our technique on Madry's Adversarial Training method (MAT) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref> and e arget=\"#b38\">39]</ref> focuse on analyzing and improving adversarial machine learning. Madry et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> first formulate adversarial training as a min-max optimiza higher value of k (more attack iterations), PGDk can generate adversarial examples with higher loss <ref type=\"bibr\" target=\"#b20\">[21]</ref> els. This property is named as transferability. This prope rbations from previous epochs. To compare the attack strength of two attacks, we use Madry's method <ref type=\"bibr\" target=\"#b20\">[21]</ref> to adversarially train two models on MNIST and CIFAR10 and we integrate ATTA with two popular adversarial training methods: Madry's Adversarial Training (MAT) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref>. By e  efficiency</head><p>We select four state-of-the-art adversarial training methods as baselines: MAT <ref type=\"bibr\" target=\"#b20\">[21]</ref>, TRADES <ref type=\"bibr\" target=\"#b38\">[39]</ref>, YOPO <r ed in <ref type=\"bibr\" target=\"#b15\">[16]</ref> and is formulated as a min-max optimization problem <ref type=\"bibr\" target=\"#b20\">[21]</ref>. As one of the most effective defense methods, lots of wor , are widely adopted in various adversarial training methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" tar rget=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar arget=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" tar jected back to S, k-step projected gradient descent method <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> (PGDk) has been widely adopted to generate adversarial exam  gradient descent <ref type=\"bibr\" target=\"#b15\">[16]</ref>) is adopted to conduct iterative attack <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Setup</head><p>Following the literature <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta  convolutional layers followed by three full-connected layers which is same architecture as used in <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. The adversarial p  with size = 0.3.</p><p>For the CIFAR10 dataset, we use the wide residual network  which is same as <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>. The perturbation   a new perspective (e.g., improving transferability between epochs). which is same with other works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta re, and hyper-parameters used in this work.</p><p>MNIST. We use the same model architecture used in <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta  step size and set decay factor as 1 for M-PGD (momentum PGD).</p><p>CIFAR10. Following other works <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ry is not assumed; for a much more thorough treatment with all the measure theoretical details, see <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> and <ref type=\"bibr\" target=\"#b1\">Da fies the mean number of events in a region conditional on the past. Here we use the notation * from <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> to remind ourselves that this densit. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e for the 'happy' category while the others not. In this paper, inspired by the attention mechanism <ref type=\"bibr\" target=\"#b13\">[14]</ref> of machine translation and the neural aggregation networks. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b20\">[21]</ref> and the introduction of the TAGE predictor by Seznec and Michaud <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In practice, on the traces distributed for the first two  e predictors targeting special categories of branches with a state-of-the-art main predictor (TAGE, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, OGEHL <ref type=\"bibr\" target=\"#b20\">[21]</ref>, Piecewis L <ref type=\"bibr\" target=\"#b7\">[8]</ref>), e.g. a loop predictor with the TAGE predictor in L-TAGE <ref type=\"bibr\" target=\"#b25\">[26]</ref> or the address-branch correlator in <ref type=\"bibr\" targe ns/1.0\"><head n=\"3.\">Background on the TAGE Predictor</head><p>The TAGE predictor was introduced in <ref type=\"bibr\" target=\"#b25\">[26]</ref> and is the core predictor of the L-TAGE predictor that won h a single 4-bit counter USE_ALT_ON_NA was found to allow to (slightly) improve prediction accuracy <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The prediction computation algorithm is as follows:</p><p ts showed that one can use wider tag for long histories for a better tradeoff. Previous experiments <ref type=\"bibr\" target=\"#b25\">[26]</ref> have shown that the TAGE predictor performs efficiently on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: earning is motivated by the self-training algorithm <ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al. model parameters, in a way similar to self-training <ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>\u2022 SVD <ref type=\"bibr\" target=\"#b11\">[12]</ref> is a classic CF-based model using inner product to model u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ://www.tei-c.org/ns/1.0\"><p>We present a variational approximation to the information bottleneck of <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>. This variational approach allows us to pa oot_2\">3</ref> This approach is known as the information bottleneck (IB), and was first proposed in <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>. Intuitively, the first term in R IB encou challenging. There are two notable exceptions: the first is when X, Y and Z are all discrete, as in <ref type=\"bibr\" target=\"#b33\">Tishby et al. (1999)</ref>; this can be used to cluster discrete data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ven their complementary strengths and weaknesses <ref type=\"bibr\">(d'Avila Garcez et al. 2015;</ref><ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\" target=\"#b47\">Yang nowledge is compiled into a neural network architecture <ref type=\"bibr\">(Bo\u0161njak et al. 2017;</ref><ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\">Evans and Grefenst retability and generalisation, thereby inheriting the best of both worlds. Among such systems, NTPs <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017;</ref><ref type=\"bibr\" target=\"#b32\">Min div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>End-to-end Differentiable Proving</head><p>NTPs <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017)</ref> recursively build a neural networ l atoms in the body need to be proven, and because Z is a free variable with many possible bindings <ref type=\"bibr\" target=\"#b39\">(Rockt\u00e4schel and Riedel 2017)</ref>. We consider two problems -given  (F) log[1 \u2212 ntp K \u03b8 ( F, d)]<label>(4)</label></formula><p>NTPs can also learn interpretable rules. <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> show that it is possible to learn tes. Although NTPs can be used for learning interpretable rules from data, the solution proposed by <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> can be quite inefficient, as the  <ref type=\"bibr\" target=\"#b21\">(Kemp et al. 2006</ref>) -following the same evaluation protocols as <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>. Furthermore, since GNTPs allows   Prediction Results. We compare GNTPs and NTPs on a set of link prediction benchmarks, also used in <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>. Results, presented in Table 1, s  predicates, 111 unary predicates, 14 constants and 2565 true facts. We follow the protocol used by <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> and split every dataset into trai ww.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_0\">For consistency, we use the same notation as<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>.</note> \t\t\t<note xmlns=\"http://ww s parallel inference to be implemented very efficiently on GPU. This optimisation is also present in<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>.</note> \t\t\t<note xmlns=\"http://ww  \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"7\" xml:id=\"foot_5\">Results reported in<ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref> were calculated with an incorrect 158 facts about the neighbourhood of countries, and the location of countries and subregions. As in <ref type=\"bibr\" target=\"#b39\">Rockt\u00e4schel and Riedel (2017)</ref>, we randomly split countries into. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> considering edge attributes <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion methods include linear functions <ref type=\"bibr\" target=\"#b8\">[9]</ref>, boosted weak learners <ref type=\"bibr\" target=\"#b18\">[19]</ref>, gradientboosted trees <ref type=\"bibr\" target=\"#b2\">[3,</ ibr\" target=\"#b10\">[11]</ref>, smoothing scores <ref type=\"bibr\" target=\"#b14\">[15]</ref>, boosting <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and approximating the metric <ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ss in entity type and new fact predictions <ref type=\"bibr\" target=\"#b16\">[Nickel et al., 2012</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016</ref><ref type=\"bibr\" target=\"#b3\">, Dettmer bedding methods,viz., ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> and ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>, which do not make use of any ontologi raining can be done for the refinement task with a negative log-likelihood loss function as follows <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>.</p><formula xml:id=\"formula_0\">L(G) = ods can also be used to predict type labels of entities (the typeOf relation). We work with ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3 ., subtype and subproperty information-and also shows that state-of-the-art embeddings like ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref>, <ref type=\"bibr\">SimplE [Kazemi and P valuate the performance of TypeE-X models in the KG refinement task, and compare them with Com-plEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3 \" target=\"#b15\">[Nickel et al., 2011</ref><ref type=\"bibr\" target=\"#b21\">, Socher et al., 2013</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016]</ref>.</p><p>An important step in learning . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: M attention <ref type=\"bibr\" target=\"#b8\">[9]</ref>, LSTM-based decoder with zoneout regularization <ref type=\"bibr\" target=\"#b9\">[10]</ref> and phoneme inputs derived from normalized text. We use Gri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b8\">(Bordes et al., 2015;</ref><ref type=\"bibr\" target=\"#b28\">Xu et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Savenkov and Agichtein, 2017)</ref>. In comparison, reasoning over mu et al., 2015)</ref>, KVMemN2N <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016)</ref> and EviNet <ref type=\"bibr\" target=\"#b21\">(Savenkov and Agichtein, 2017)</ref> transferred the reading comprehe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Conventional studies concentrate on the area of multilingual r\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> for a long time, these researches are commonly limited to mak uage dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM <ref type=\"bibr\" target=\"#b4\">[5]</ref> further explores long short-term memory (LSTM) <ref type=\"bi nder the condition of language information being known during training. A comparison with SHL-MLSTM <ref type=\"bibr\" target=\"#b4\">[5]</ref> with residual learning is investigated on CALL-HOME datasets .tei-c.org/ns/1.0\"><head n=\"4.4.\">Results</head><p>The baseline systems come from our previous work <ref type=\"bibr\" target=\"#b4\">[5]</ref> and all results are summarized in Table <ref type=\"table\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y are unfortunately not the only weak spot in machine learning systems.</p><p>Recently, Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> have demonstrated that data preprocessing used in machine  ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Image-Scaling Attacks</head><p>Recently, Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> have shown that scaling algorithms are vulnerable to attac aling algorithm. Both matrices can be computed in advance and are reusable. We refer to Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> for a description how to calculate L and R.</p><p>Based on  assignment.</p><p>We implement image-scaling attacks in the strong variant proposed by Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref>. We make a slight improvement to the original attacks: Ins  on rectangular blocks instead of columns and rows. As a result, the original attack by Xiao et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> is not applicable to this scaling algorithm. To attack are. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: atch from the surrounding patches by solving a large ridge regression problem extremely efficiently <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. It has proved to be r to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>.</p></div> <div xmln p://www.tei-c.org/ns/1.0\"><head n=\"2.\">Related work</head><p>Since the seminal work of Bolme et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the Correlation Filter has enjoyed great popularity within  To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window <ref type=\"bibr\" target=\"#b3\">[4]</ref> and the final template is cropped <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b33\">[25]</ref>. The system extracts researchers' pro les automatically from the Web <ref type=\"bibr\" target=\"#b32\">[24]</ref> and integrates them with published papers after name disam. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b23\">(Meignier and Merlin 2010)</ref> and then perform MWER segmentation with RWTH toolkit <ref type=\"bibr\" target=\"#b4\">(Bender et al. 2004</ref>). Case-insensitive BLEU is used as evaluatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  that pulls representations of noisy samples away from clean ones. Finally, mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> has recently demonstrated outstanding robu ushing the state-of-the-art one step forward by combining our approach with mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref>.</p><p>4. Guiding mixup data augmentation   bootstrapping <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> to deal with the closed-set label noise sc i-c.org/ns/1.0\"><head n=\"3.3.\">Joint label correction and mixup data augmentation</head><p>Recently <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> proposed a data augmentation technique nam ref type=\"bibr\" target=\"#b9\">(Hendrycks et al., 2018)</ref>), and use the configuration reported in <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> for mixup. We outperform the related work  tab_8\">6</ref> shows the results of the proposed approaches M-DYR-H and MD-DYR-SH compared to mixup <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> on TinyImageNet to demonstrate that our ap onstrate that our approach is useful far from CIFAR data. The proposed approach clearly outperforms <ref type=\"bibr\" target=\"#b34\">(Zhang et al., 2018)</ref> for different levels of label noise, obtai ref type=\"bibr\" target=\"#b11\">Jiang et al., 2018b;</ref><ref type=\"bibr\">Patrini et al., 2017;</ref><ref type=\"bibr\" target=\"#b34\">Zhang et al., 2018)</ref> modify either the loss directly, or the pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ate to handle multi-relation QA due to the lack of reasoning ability.</p><p>Recent reasoning models <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016;</ref><ref type=\"bibr\" target=\"#b26\">Wang et al. manner during reasoning. MemNN <ref type=\"bibr\" target=\"#b27\">(Weston et al., 2015)</ref>, KVMemN2N <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016)</ref> and EviNet <ref type=\"bibr\" target=\"#b21\" re the settings are the same as <ref type=\"bibr\" target=\"#b8\">(Bordes et al., 2015)</ref>. KVMemN2N <ref type=\"bibr\" target=\"#b16\">(Miller et al., 2016)</ref> improves the MemN2N for KBQA as it divide. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns and proposed Definition 1 which emphasizes the reasoning engine of knowledge graphs. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> proposed a definition as a multi-relational graph in Definit ormation into an ontology and applies a reasoner to derive new knowledge. Definition 2 (Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref>). A knowledge graph is a multirelational graph composed of e \"#b5\">[6]</ref>, Chinese knowledge graph construction <ref type=\"bibr\" target=\"#b9\">[10]</ref>, KGE <ref type=\"bibr\" target=\"#b7\">[8]</ref> or KRL <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The latter </ref> presented KRL in a linear manner, with a concentration on quantitative analysis. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> categorized KRL according to scoring functions, and specific s of auxiliary information for KRL such as attributes, relation path and logical rules. Wang et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> gave a detailed review on these information. This paper disc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ble to adversarial attacks both at test time (evasion) as well as training time (poisoning attacks) <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b10\">Dai et al., r, they focus on targeted attacks, i.e. attacks designed to change the prediction of a single node. <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> consider both test-time and training-time   algorithm for global attacks on (deep) node classification models at training time. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>, we explicitly tackle the bilevel optimiza gorithm achieved after training on the data (i.e., graph) modified by our algorithm. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b10\">Dai et  in undiscovered, adversarial attacks should be unnoticeable. To account for this, we largely follow <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>'s attacker capabilities. First, we impose   are very likely to be noticed; to prevent such large changes to the degree distribution, we employ <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>'s unnoticeability constraint on the degree  GCN) to evaluate the performance degradation due to the attack. We use the same surrogate model as <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>, which is a linearized two-layer graph con e diagonal matrix of the node degrees, and \u03b8 = {W } the set of learnable parameters. In contrast to <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref> we do not linearize the output (softmax) l raining the model for T steps. We compare against this baseline in our experiments; as also done in <ref type=\"bibr\" target=\"#b28\">Z\u00fcgner et al. (2018)</ref>. However, unlike the meta-gradient, this a compare our meta-gradient approach as well as its approximations with various baselines and Nettack <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018)</ref>. DICE ('delete internally, connect extern l our experiments, we enforce the unnoticeability constraint on the degree distribution proposed by <ref type=\"bibr\" target=\"#b28\">(Z\u00fcgner et al., 2018)</ref>. In Fig. <ref type=\"figure\" target=\"#fig_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, regularizes the spectral norm of the weight matrix at each i et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. <ref type=\"bibr\" target=\"#b7\">[8]</ref> utilizes Lipschitz p et=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, keep \u03b2 constant across layers. These harder constraints over get=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. <ref type=\"bibr\" target=\"#b7\">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustne  utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks. Unlike <ref type=\"bibr\" target=\"#b7\">[8]</ref>, our approach does not require a predetermined set of hyper- 11]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b7\">[8]</ref>. <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" their papers: \u03b2 = 1.0, 1.6, 2.0. The 2 works given in <ref type=\"bibr\" target=\"#b36\">[37]</ref> and <ref type=\"bibr\" target=\"#b7\">[8]</ref> may be seen as subsets of the works given in <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ures</head><p>There have been a large number of works and debates on NIC offloading of TCP features <ref type=\"bibr\" target=\"#b33\">[35,</ref><ref type=\"bibr\" target=\"#b45\">47,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The latter components stabilizes the evolution of graph s  target=\"#b39\">40]</ref>), GNNGUARD determines importance weights using theory of network homophily <ref type=\"bibr\" target=\"#b13\">[14]</ref>, positing that similar nodes (i.e., nodes with similar fea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  attention to decide when to stop and then performing soft attention to calculate, also rather than <ref type=\"bibr\" target=\"#b12\">[13]</ref> which needs a CTC trained model to conduct pre-partition b  shows a clear performance advantage than other soft and monotonic models (e.g. triggered attention <ref type=\"bibr\" target=\"#b12\">[13]</ref>), but also matches or surpasses most of the published resu int CTC-attention model / ESPNet <ref type=\"bibr\" target=\"#b15\">[16]</ref> 27.4 Triggered Attention <ref type=\"bibr\" target=\"#b12\">[13]</ref> 30 where the membrane potential Um is constantly simulated. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t in time. This is especially true for Long Short Term Memory (LSTM) networks-a popular type of RNN <ref type=\"bibr\" target=\"#b14\">[16]</ref>.</p><p>Recurrent neural networks are competitive or state- =\"formula_3\">h 0 h 1 h 2 h 3 h T x 1 x 2 x 3 x T</formula><p>Long Short Term Memory (LSTM) networks <ref type=\"bibr\" target=\"#b14\">[16]</ref> are a more complex variant of RNNs that often prove more p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ethods aim to model the temporal or motion information in videos. The Long Short-Term Memory (LSTM) <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and C3D <ref type=\"bibr\" target=\"#b6\">[7]</ref> are two wid. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for word embedding has been shown to be an implicit factorization of a certain word-context matrix <ref type=\"bibr\" target=\"#b25\">[24]</ref>, and there is recent effort to theoretically explaining th ol(G) log \u0434 \u2212x \u22a4 i y j .</formula><p>Let us define z i, j = x \u22a4 i y j . Following Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref>, where the authors suggested that for a sufficient large e w i\u2212T , \u2022 \u2022 \u2022 , w i\u22121 , w i+1 , \u2022 \u2022 \u2022 , w i+T .</formula><p>Following the work by Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref>, SGNS is implicitly factorizing</p><formula xml:id=\"formul x is not only ill-defined (since log 0 = \u2212\u221e), but also dense. Inspired by the Shifted PPMI approach <ref type=\"bibr\" target=\"#b25\">[24]</ref>, we define M \u2032 such that M \u2032 i, j = max(M i, j , 1) (Line  </ref>. Recently, there has been effort in understanding this model. For example, Levy and Goldberg <ref type=\"bibr\" target=\"#b25\">[24]</ref> prove that SGNS is actually conducting an implicit matrix   target=\"#b19\">[18]</ref> frame word embedding as a metric learning problem. Built upon the work in <ref type=\"bibr\" target=\"#b25\">[24]</ref>, we theoretically analyze popular skip-gram based network . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ure\" target=\"#fig_8\">7</ref>, right.</p><p>This approach is derived from Analytic Hierarchy Process <ref type=\"bibr\" target=\"#b73\">[76]</ref>, a method widely used in operations research to create a t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . QuO's QDLs are based on the separation of concerns advocated by Aspect-Oriented Programming (AoP) <ref type=\"bibr\" target=\"#b22\">[23]</ref>. The QuO middleware adds significant value to adaptive rea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rks include random walk-based methods (DeepWalk <ref type=\"bibr\" target=\"#b16\">[17]</ref>, Node2Vec <ref type=\"bibr\" target=\"#b4\">[5]</ref>), where only graph topology and node relations are embedded   performance of our algorithm with several unsupervised representation learning baselines: Node2Vec <ref type=\"bibr\" target=\"#b4\">[5]</ref>, VGAE <ref type=\"bibr\" target=\"#b9\">[10]</ref>, GraphSAGE <r or the classification model, without using any graph structure information incorporated. \u2022 Node2Vec <ref type=\"bibr\" target=\"#b4\">[5]</ref>: This approach is an extension of Word2Vec <ref type=\"bibr\"  sentation with the skip-gram model. DeepWalk <ref type=\"bibr\" target=\"#b16\">[17]</ref> and Node2vec <ref type=\"bibr\" target=\"#b4\">[5]</ref> are typically representative of these methods to model homog. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el-bypass TCP stacks <ref type=\"bibr\" target=\"#b3\">[5,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b53\">55,</ref><ref type=\"bibr\" tar ow-level parallelism <ref type=\"bibr\" target=\"#b3\">[5,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b39\">41,</ref><ref type=\"bibr\" target=\"#b57\">59]</ref> or by steering the  .</p><p>Our evaluation shows that AccelTCP brings an enormous performance gain. It outperforms mTCP <ref type=\"bibr\" target=\"#b39\">[41]</ref> by 2.2x to 3.8x while it enables non-persistent connection </ref><ref type=\"bibr\" target=\"#b37\">39,</ref><ref type=\"bibr\" target=\"#b58\">60]</ref>, we use mTCP <ref type=\"bibr\" target=\"#b39\">[41]</ref>, a scalable user-level TCP stack on DPDK <ref type=\"bibr\"   stack is logically independent of the NIC stack. While our current implementation is based on mTCP <ref type=\"bibr\" target=\"#b39\">[41]</ref>, one can extend any TCP stack to harness our NIC offloadin 9100-ON switch, configured to run at 40 GbE speed. For TCP stacks, we compare AccelTCP against mTCP <ref type=\"bibr\" target=\"#b39\">[41]</ref> and IX <ref type=\"bibr\" target=\"#b28\">[30]</ref>. All TCP  tions and L7 proxying, and compare against the performance of the state-of-the-art TCP stacks: mTCP <ref type=\"bibr\" target=\"#b39\">[41]</ref> and IX <ref type=\"bibr\" target=\"#b28\">[30]</ref>.</p></div  n=\"7\">Related Work</head><p>Kernel-bypass TCP stacks: Modern kernel-bypass TCP stacks such as mTCP <ref type=\"bibr\" target=\"#b39\">[41]</ref>, IX <ref type=\"bibr\" target=\"#b28\">[30]</ref>, SandStorm <  ranges from 400 to 700 bytes even on recent implementations <ref type=\"bibr\" target=\"#b3\">[5,</ref><ref type=\"bibr\" target=\"#b39\">41]</ref>. However, we find that many of the fields are unnecessary f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: les <ref type=\"bibr\" target=\"#b50\">[51]</ref>. The related study is presented in our previous paper <ref type=\"bibr\" target=\"#b49\">[50]</ref>.</p><formula xml:id=\"formula_15\">Hmatch 2 = 1/(norm(|P U \u2212 6\"><head></head><label></label><figDesc>) T U describes learning styles. Referring to other research<ref type=\"bibr\" target=\"#b49\">[50]</ref>, we design the elements of learning styles as: T U = {CL, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b46\">47,</ref><ref type=\"bibr\" tar e a novel face-focused cross-stream network (FFCSN). Different from the popular two-stream networks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" targ d for action recognition in videos and has been popular for many human-centric video analysis tasks <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>. Various improvement. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper <ref type=\"bibr\" target=\"#b15\">(Kim et al., 2011)</ref>. REL is a special case of text classificatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pwords like of and on. Therefore, a better mechanism for local weights is needed.</p><p>Inspired by <ref type=\"bibr\" target=\"#b59\">[60]</ref>, we propose a local weight network based on distributed wo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Kipf &amp; Welling, 2016;</ref><ref type=\"bibr\" target=\"#b9\">Garcia Duran &amp; Niepert, 2017;</ref><ref type=\"bibr\" target=\"#b57\">Wang et al., 2017;</ref><ref type=\"bibr\" target=\"#b40\">Pan et al., 20 GAE (VGAE) <ref type=\"bibr\" target=\"#b23\">(Kipf &amp; Welling, 2016)</ref>, marginalized GAE (MGAE) <ref type=\"bibr\" target=\"#b57\">(Wang et al., 2017)</ref>, adversarially regularized GAE (ARGA) and V. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  this form in NLP. In log-linear models, including both the original work on maximum-entropy models <ref type=\"bibr\" target=\"#b1\">(Berger et al., 1996)</ref>, and later work on conditional random fiel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b20\">[21]</ref>. Recently, with the great impact of neural networks on computer vision <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and natural langua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tories also requires a palette of tools such as CVS <ref type=\"bibr\" target=\"#b3\">[4]</ref> and SSH <ref type=\"bibr\" target=\"#b34\">[35]</ref>, many of which have had remotely exploitable bugs.</p><p>W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: graph).</p><p>Our method also connects to PinSage <ref type=\"bibr\" target=\"#b20\">[21]</ref> and GAT <ref type=\"bibr\" target=\"#b14\">[15]</ref>. But note that both PinSage and GAT are designed for homog ula><p>(5) 2 The knowledge graph G is treated undirected. 3 Technically, S(v) \u2022 Neighbor aggregator <ref type=\"bibr\" target=\"#b14\">[15]</ref> directly takes the neighborhood representation of entity v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et al. (2010)</ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and <ref type=\"bibr\" target=\"#b15\">Hillman et al. (2012)</ref> determined that increased speech promptin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b18\">[22,</ref><ref type=\"bibr\" target=\"#b20\">24,</ref><ref type=\"bibr\" target=\"#b21\">25,</ref><ref type=\"bibr\" target=\"#b38\">42]</ref>. These are mainly (1) to ease the customizing and debugging. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2015) )</ref> and recurrent neural network <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Zhou et al., 2016)</ref>. To automatically obtain a large training da ying relations, we apply an attention mechanism over a BiLSTM Encoder, which is first introduced in <ref type=\"bibr\" target=\"#b23\">(Zhou et al., 2016)</ref> for RC. The model architecture is illustrat g et al., 2015)</ref> is also commonly used for RE with the help of position embeddings. BiLSTM+ATT <ref type=\"bibr\" target=\"#b23\">(Zhou et al., 2016)</ref> adds an attention mechanism into BiLSTM to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  e.g., paper network <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" target=\"#b20\">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: al neighborhood <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p><p>More interestingly, Z \u00fcgner et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref> focused on the node classification using GCN, and proposed ations based on the obtained embedding vectors could be affected correspondingly.</p><p>Inspired by <ref type=\"bibr\" target=\"#b40\">[41]</ref>, in this paper, we propose a new fast gradient attack (FGA et node, then randomly connect the target node to K \u2212 b nodes of different classes.</p><p>\u2022 NETTACK <ref type=\"bibr\" target=\"#b40\">[41]</ref>. NETTACK generates adversarial network iteratively. In eac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: int locations based on hand-crafted features. Recent works <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targe ]</ref> uses deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> and Zisserman et al. <ref type=\"bibr\" target=\"#b1\">[2]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: spectral interpretation and vertex domain localization <ref type=\"bibr\" target=\"#b129\">[130]</ref>, <ref type=\"bibr\" target=\"#b130\">[131]</ref>. Notions of stationarity can help develop probabilistic  essing methods leading to graph-based Wiener filtering <ref type=\"bibr\" target=\"#b131\">[132]</ref>, <ref type=\"bibr\" target=\"#b130\">[131]</ref>.</p><p>A study of vertex/spectral localization and uncer. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: end in computer vision where significant improvements have been reported using much deeper networks <ref type=\"bibr\" target=\"#b12\">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: was the leading detection paradigm in classic computer vision, with the resurgence of deep learning <ref type=\"bibr\" target=\"#b16\">[17]</ref>, two-stage detectors, described next, quickly came to domi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ices but it may incur the loss of information during the training process. To this end, inspired by <ref type=\"bibr\" target=\"#b3\">[Dai et al., 2016]</ref>, we transform the binary optimization problem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: operties that could be used to craft adversarial samples <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b28\">[30]</ref>, <ref type=\"bibr\" target=\"#b34\">[36]</ref>. Simply put, th ages that are unrecognizable to humans, but are nonetheless labeled as recognizable objects by DNNs <ref type=\"bibr\" target=\"#b28\">[30]</ref>. For instance, they demonstrated how a DNN will classify a e backpropagation procedure used during network training <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b28\">[30]</ref>, <ref type=\"bibr\" target=\"#b34\">[36]</ref>. This approach . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ey define a density function and discover the target mainly us-ing structural features. Other works <ref type=\"bibr\" target=\"#b7\">[Zhao and Yu, 2013;</ref><ref type=\"bibr\" target=\"#b4\">McConville et a graph, while the method mainly focuses on structural features and cannot catch long-term anomalies. <ref type=\"bibr\" target=\"#b7\">[Yu et al., 2018]</ref> proposed NetWalk, a dynamic graph embedding mo btain <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref type=\"bibr\" target=\"#b7\">[Yu et al., 2018]</ref> to inject anomalous edges into two datasets.</ ure and historical behavior near an edge to measure whether the edge is anomalous or not. \u2022 NetWalk <ref type=\"bibr\" target=\"#b7\">[Yu et al., 2018]</ref>. The method first builds node embeddings based esults are shown in Table <ref type=\"table\">1</ref>, in which the data of baselines are reported by <ref type=\"bibr\" target=\"#b7\">[Yu et al., 2018]</ref>. We can see that Ad-dGraph beats all baselines 3\">2</ref> illustrates the results on dynamic graph, in which the data of baselines are reported by <ref type=\"bibr\" target=\"#b7\">[Yu et al., 2018]</ref>. The results indicate that AddGraph beats base. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: combination of the query-document relevance and the document novelty, to select the document. xQuAD <ref type=\"bibr\" target=\"#b23\">[24]</ref> directly models di erent aspects of a query and estimates  tic approach in which the document is selected according to maximal marginal relevance.</p><p>xQuAD <ref type=\"bibr\" target=\"#b23\">[24]</ref>: a representative method which explicitly models di erent . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  underlying graph, for example to education <ref type=\"bibr\" target=\"#b56\">[57]</ref> or employment <ref type=\"bibr\" target=\"#b40\">[41]</ref> in social graphs. GNNs have been shown to benefit from lev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b29\">[30]</ref>. We used ADAM with the default hyperparameters described in <ref type=\"bibr\" target=\"#b30\">[31]</ref>, however we decayed the learning rate from 1e\u22123 to 1e\u22124 af. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , such as graphs which encode the pairwise relationships. This includes examples of social networks <ref type=\"bibr\" target=\"#b2\">[3]</ref>, protein interfaces <ref type=\"bibr\" target=\"#b3\">[4]</ref>, assification, including Cora, Citeseer, Pubmed <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Reddit <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Intensive experiments verify the effectiveness of our metho lf-attention strategy.</p><p>More recently, two kinds of sampling-based methods including GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGCN <ref type=\"bibr\" target=\"#b20\">[21]</ref> were d and Extensions</head><p>Relation to other sampling methods. We contrast our approach with GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGC-N <ref type=\"bibr\" target=\"#b20\">[21]</ref> regar and Pubmed <ref type=\"bibr\" target=\"#b10\">[11]</ref>  community different posts belong to in Reddit <ref type=\"bibr\" target=\"#b2\">[3]</ref>. These graphs are varying in sizes from small to large. Part  set to be 16. For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The numbers of the sampling nodes for all layers excluding  \"><head n=\"7.1\">Alation Studies on the Adaptive Sampling</head><p>Baselines. The codes of GraphSAGE <ref type=\"bibr\" target=\"#b2\">[3]</ref> and FastGCNN <ref type=\"bibr\" target=\"#b20\">[21]</ref> provi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n employing a flexible, stochastic measure of node similarity based on random walks, e.g., DeepWalk <ref type=\"bibr\" target=\"#b22\">[22]</ref>, node2vec <ref type=\"bibr\" target=\"#b1\">[2]</ref>, LINE <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6,032 images in the test set, and 531,131 images for additional training. Following common practice <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as the training progresses <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Madry, Makelov, Schmidt, et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> used adversarial training on the cifar dataset, which stil gest known attack for this metric. PGD has been conjectured to be a near-optimal first-order attack <ref type=\"bibr\" target=\"#b24\">[25]</ref>. We use the Fo olBox library for the implementation of the we also wish to address ac o n c e r nr a i s e db yM a d r y ,M a k e l o v ,S c h m i d t ,et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which is the computational cost of a threat model. They a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: exhibit value locality, meaning that the same static instruction often produces a predictable value <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In the case of load instructions, it is also possible to pr  type=\"table\" target=\"#tab_0\">I</ref>, to determine how they complement Last Value Prediction (LVP) <ref type=\"bibr\" target=\"#b0\">[1]</ref> Stride Address Prediction (SAP) <ref type=\"bibr\" target=\"#b5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stic gradient-type algorithms, e.g., ADAM <ref type=\"bibr\" target=\"#b14\">(Kingma and Ba, 2014;</ref><ref type=\"bibr\" target=\"#b21\">Liu et al., 2020)</ref>. Following <ref type=\"bibr\" target=\"#b30\">Raf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  huge boost in performance using Deep-Learning based methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target yer. The network input is interpolated to the output size. As done in previous CNN-based SR methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targe orted numerical were produced using the evaluation script of <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b18\">Nie et al. (2019)</ref>; <ref type=\"bibr\" target=\"#b31\">Yoneda et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> have achieved the top three results a laimevidence pair individually and then aggregate all NLI predictions for final verification. Then, <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>; <ref type=\"bibr\" target=\"#b31\">Yoned the task. In the document retrieval and sentence selection stages, we simply follow the method from <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> since their method has the highest sc ose noisy evidence.</p><p>In the document retrieval step, we adopt the entity linking approach from <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>. Given a claim, the method first util ent selects the most relevant evidence for the claim from all sentences in the retrieved documents. <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref> modify the ESIM 2 https://www.mediawi he OFEVER scores of our model and models from other teams. After running the same model proposed by <ref type=\"bibr\" target=\"#b10\">Hanselowski et al. (2018)</ref>, we find our OFEVER score is slightly cted to form the final evidence set in the original method.</p><p>In addition to the original model <ref type=\"bibr\" target=\"#b10\">(Hanselowski et al., 2018)</ref>, we add a relevance score filter wit models from the FEVER shared task as our baselines.</p><p>The Athene UKP TU Darmstadt team (Athene) <ref type=\"bibr\" target=\"#b10\">(Hanselowski et al., 2018)</ref> combines five inference vectors from. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: enerating the entire sequence at once <ref type=\"bibr\" target=\"#b24\">[25]</ref> or in small batches <ref type=\"bibr\" target=\"#b19\">[20]</ref>. However, this introduces a lag in the generation process,  to capture temporal dependencies but requires fixed length videos. This limitation was overcome in <ref type=\"bibr\" target=\"#b19\">[20]</ref> but constraints need to be imposed in the latent space to  ght-forward adaptations of GANs for videos are proposed in <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, replacing the 2D convolutional layers with 3D convolutiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (FDP) <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Address Map Pattern Matching Prefetching (AMPM) <ref type=\"bibr\" target=\"#b11\">[12]</ref>. We now describe both of these techniques in some detail.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b44\">Wang et al., 2011;</ref><ref type=\"bibr\" target=\"#b45\">Wang and Zeng, 2013;</ref><ref type=\"bibr\" target=\"#b47\">Yamanishi et al., 2008)</ref>.</p><p>With the rapid development of de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b22\">[23]</ref>, VoxForge <ref type=\"bibr\" target=\"#b23\">[24]</ref>, CHiME-4/5 <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, etc. Thus, ESPnet far-field speech recognition tasks including AMI <ref type=\"bibr\" target=\"#b21\">[22]</ref>, CHiME-4 <ref type=\"bibr\" target=\"#b24\">[25]</ref>, and CHiME-5 tasks <ref type=\"bibr\" target=\"#b25\">[26]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  consider the degrees of importance of items based on their rankings.</p><p>Recently, Tang and Wang <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposed a KD model to address the ranking problem, called ledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> and present rank distillation (RD) <ref type=\"bibr\" target=\"#b10\">[11]</ref> that applies knowledge distillation to recommender models. ommendation problem because Fig. <ref type=\"figure\">1</ref>. Illustration of rank distillation (RD) <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The teacher model transfers manipulated top-k items as th t may have worse performance than the original student model. Rank distillation (RD). Tang and Wang <ref type=\"bibr\" target=\"#b10\">[11]</ref> proposed ranking distillation (RD) that applies KD for ran e rated by less than 5 users. Table I reports the detailed statistics of these datasets.</p><p>\u2022 RD <ref type=\"bibr\" target=\"#b10\">[11]</ref>: To define the KD loss in equation ( <ref type=\"formula\" t een \u03bb that appears in RD and CD. Specifically, we used the following parameter settings.</p><p>\u2022 RD <ref type=\"bibr\" target=\"#b10\">[11]</ref> and RD-Rank: We set \u03c1 to be 0.5. For CDAE, the number of i er, we used the public PyTorch implementation <ref type=\"foot\" target=\"#foot_5\">6</ref> provided in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. All experiments were conducted on a desktop with 128 GB m KD. Also, the gain indicates how additional accuracy achieved by the proposed model over that of RD <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>Based on this evaluation, we found several interest rform RD over all datasets. Note that the improvement gap for RD is somewhat different from that in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. It is because we used leave-one-out evaluation while <ref  in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. It is because we used leave-one-out evaluation while <ref type=\"bibr\" target=\"#b10\">[11]</ref> used cross-validation evaluation. Our models are consisten el size and efficiency. The model size is proportional to the accuracy of our model, as observed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> as well. The same tendency consistently holds in different oot\" n=\"4\" xml:id=\"foot_3\">http://dawenl.github.io/data/gowalla pro.zip Competitive models. Since RD<ref type=\"bibr\" target=\"#b10\">[11]</ref> is the state-of-the-art KD model for top-N recommendation,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns=\"http://www.tei-c.org/ns/1.0\"><head>Class</head><p>Ontological Rule</p><p>Uncertain Extractions  <ref type=\"bibr\" target=\"#b0\">[Blum and Mitchell, 1998]</ref>, to combine the strengths of PSL-KGI a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ow dimensional subspace by cross-view quadratic discriminant analysis for metric learning. 4. kLFDA <ref type=\"bibr\" target=\"#b23\">[24]</ref> Kernelized Local Fisher Discriminant Classifier is a close. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: w York Times (NYT) <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010)</ref> and WebNLG <ref type=\"bibr\" target=\"#b6\">(Gardent et al. 2017)</ref>. NYT comes from the distant supervised rel ramework <ref type=\"bibr\" target=\"#b15\">(Sutskever, Vinyals, and Le 2014)</ref> with copy mechanism <ref type=\"bibr\" target=\"#b6\">(Gu et al. 2016)</ref>. But it cannot predict the entire entities. In . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: //www.tei-c.org/ns/1.0\"><head>Introduction</head><p>Large-scale knowledge graphs (KGs) such as YAGO <ref type=\"bibr\" target=\"#b10\">(Suchanek, Kasneci, and Weikum 2007)</ref>, NELL <ref type=\"bibr\" tar e=\"bibr\" target=\"#b0\">(Carlson et al. 2010), and</ref><ref type=\"bibr\">Wikidata (Vrande\u010di\u0107 and</ref><ref type=\"bibr\" target=\"#b10\">Kr\u00f6tzsch 2014)</ref> usually represent facts in the form of relations =\"bibr\" target=\"#b9\">Socher et al. 2013;</ref><ref type=\"bibr\" target=\"#b11\">Yang et al. 2015;</ref><ref type=\"bibr\" target=\"#b10\">Trouillon et al. 2016;</ref><ref type=\"bibr\" target=\"#b8\">Schlichtkru sume available, sufficient training instances for all relations.</p><p>In light of the above issue, <ref type=\"bibr\" target=\"#b10\">Xiong et al. (2018)</ref> proposed GMatching which introduces a local )</ref> have been proposed to learn entity embeddings by using relational information, Xiong et al. <ref type=\"bibr\" target=\"#b10\">(Xiong et al. 2018</ref>) demonstrated that explicitly encoding graph spectively. In order to measure the similarity between two vectors, we employ a recurrent processor <ref type=\"bibr\" target=\"#b10\">(Vinyals et al. 2016</ref>) f \u00b5 to perform multiple steps matching. T. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b11\">[8]</ref>, along with its representative updated descendants, e.g. Fast R-CNN <ref type=\"bibr\" target=\"#b10\">[7]</ref> and Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[26]</ref>, l methods, which opens the deep learning era in object detection. Its descendants (e.g., Fast R-CNN <ref type=\"bibr\" target=\"#b10\">[7]</ref>, Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[26]</ref>) up. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: st time.</p><p>As a simple example of this principle, consider Figure <ref type=\"figure\">8</ref> in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. The authors experimented with training models on a \"cheat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Subject independent approaches have been proposed that transform audio features to video frames <ref type=\"bibr\" target=\"#b4\">[5]</ref> but there is still no method to directly transform raw audio rating natural facial expressions. Some methods generate frames based solely on present information <ref type=\"bibr\" target=\"#b4\">[5]</ref>, without taking into account the facial dynamics. This makes for capturing articulation dynamics and estimating the 3D points of the mesh. Finally, Chung et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> proposed a CNN applied on Mel-frequency cepstral coefficient  works that are closest to ours are those proposed in <ref type=\"bibr\" target=\"#b21\">[22]</ref> and <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The former method is subject dependent and requires a large  static method that produces video frames using a sliding window of audio samples like that used in <ref type=\"bibr\" target=\"#b4\">[5]</ref>. This is a GAN-based method that uses a combination of an L  rator but also due to the use of the conditional Sequence Discriminator. Unlike previous approaches <ref type=\"bibr\" target=\"#b4\">[5]</ref> that prohibit the generation of facial expressions, the adve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: timates <ref type=\"bibr\" target=\"#b5\">[7]</ref>, estimating prior knowledge for individual learners <ref type=\"bibr\" target=\"#b31\">[33]</ref>, and estimating problem difficulty <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )</ref>, Adam <ref type=\"bibr\" target=\"#b9\">(Kingma &amp; Ba, 2014)</ref> and most recently AMSGrad <ref type=\"bibr\" target=\"#b15\">(Reddi et al., 2018)</ref> have become a default method of choice for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utions.</formula><p>as a means for, e.g., model debugging or architecture selection. A recent paper <ref type=\"bibr\" target=\"#b3\">(Jain and Wallace, 2019)</ref> points to possible pitfalls that may ca ntion Might be Explanation</head><p>In this section, we briefly describe the experimental design of <ref type=\"bibr\" target=\"#b3\">Jain and Wallace (2019)</ref> and look at the results they provide to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g/ns/1.0\"><head n=\"4.1\">The Base Model: BLSTM-CRF</head><p>Many recent sequence labeling frameworks <ref type=\"bibr\" target=\"#b25\">(Ma and Hovy, 2016b;</ref><ref type=\"bibr\" target=\"#b27\">Misawa et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ction on transformed elements in the point set to approximate a general function defined on the set <ref type=\"bibr\" target=\"#b20\">[21]</ref>:</p><formula xml:id=\"formula_12\">AGGREGATE({x 1 , \u2022 \u2022 \u2022 , . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the page-level and line-level structure. The state-of-the-art for person name in academic homepages <ref type=\"bibr\" target=\"#b0\">[1]</ref> uses a co-guided neural network to learn from fine-grained a  publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> and for person names recognition <ref type=\"bibr\" target=\"#b0\">[1]</ref> use Bi-LSTM-CRF based models to recognise information from t t for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> and person name recognition <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Table <ref type=\"table\" target=\"#tab_0\">2</ref> summarises  fferent universities and research institutes with 12,796 publications annotated. \u2022 HomeName dataset <ref type=\"bibr\" target=\"#b0\">[1]</ref> is constructed from the HomePub dataset by further labeling  e person names with the knowledge from the publication recognition task. PAM also outperforms CogNN <ref type=\"bibr\" target=\"#b0\">[1]</ref> by 1.40% on token level and 2.06% on name level in F1 score. ofthe-art models for the two tasks (i.e., PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> and CogNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>) and our proposed PAM model on 50 randomly selected homepage n by the page owenr but not a publication.</p><p>For person name recognition, we observe that CogNN <ref type=\"bibr\" target=\"#b0\">[1]</ref> tends to produce false negative predictions in groups, i.e., plain text of an academic homepage is saved first, then the recognition tasks are conducted on text <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Given the plain tex resented as a d e -dimension word embedding, i.e., S \u2208 R n\u00d7d e . Following state-of-the-art methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, we use GloVe <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b30\">[31]</ref> to capture all these information over a long period. Yao et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> at-tempt to dynamically select multiple visual representat ssification. The extracted features {v i } n i=1 form the original video representation. Similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref>, temporal soft-attention Attend is used to select visual i tion Attend is used to select visual information most related to each word. But very different from <ref type=\"bibr\" target=\"#b36\">[37]</ref> using the hidden states from a LSTM decoder, we guide the  ce generator and a paragraph generator. To emphasize the mapping from video to sentence, Yao et al. <ref type=\"bibr\" target=\"#b36\">[37]</ref> propose a temporal attention model to align the most relev l></formula><p>where W r , U \u03b1 , b \u03b1 , and w are the parameters to be learned.</p><p>Different from <ref type=\"bibr\" target=\"#b36\">[37]</ref>, here we incorporate the content read from multimodal memo o add masks to both sentences and visual features for the convenience of batch training. Similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref>, the sentences with length larger than 30 in MSVD and the  #b25\">[26]</ref>, respectively. Table <ref type=\"table\">3</ref>. The performance comparison with SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> using different visual features on MSR-VTT. Here V and C d t approaches( <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib 13]</ref> and GoogleNet <ref type=\"bibr\" target=\"#b25\">[26]</ref>. Among these compared methods, SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> is the most similar method to ours, which also has an atte ltiple visual feature fusion, we compare our model with the other five state-of-the-art approaches( <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bib rget=\"#b1\">[2]</ref>). The comparison results are shown in Table <ref type=\"table\">2</ref>. SA-G-3C <ref type=\"bibr\" target=\"#b36\">[37]</ref> uses the combination of GoogleNet feature and 3D-CNN featu  pairs. Considering that there are few methods tested on this dataset, we compare our model with SA <ref type=\"bibr\" target=\"#b36\">[37]</ref> which is the most similar work to ours. Similarly, we perf <ref type=\"figure\">2</ref> illustrates several descriptions generated by our M 3 -google, SA-google <ref type=\"bibr\" target=\"#b36\">[37]</ref> and human-annotated ground truth on the test set of MSVD.   method. Fig. <ref type=\"figure\">3</ref> shows the attention shift of our M 3 -google and SA-google <ref type=\"bibr\" target=\"#b36\">[37]</ref> across multiple frames when generating the sentence. There about 40 sentences. So there are about 80,000 video-description pairs. Following the standard split <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, we divide the ori. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the graph h t evw and updating them analogously to equations 1 and 2. Of the existing MPNNs, only <ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref> has used this idea.</p><p>Convolutional N  T v . Note the original work only defined the model for T = 1.</p><p>Molecular Graph Convolutions, <ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref> This work deviates slightly from other MP ed features we include two existing baseline MPNNs, the Molecular Graph Convolutions model (GC) from<ref type=\"bibr\" target=\"#b16\">Kearnes et al. (2016)</ref>, and the original GG-NN model<ref type=\"b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: even if they are semantically similar. On the other hand, recently, some representationbased models <ref type=\"bibr\" target=\"#b12\">[12]</ref>, <ref type=\"bibr\" target=\"#b14\">[14]</ref> can successfull ct and soft matches, we adopt the interaction-based models <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b12\">[12]</ref>, <ref type=\"bibr\" target=\"#b52\">[43]</ref> widely used in  ERT <ref type=\"bibr\" target=\"#b6\">[6]</ref>.</p><p>Aggregation Function. For sentence matching, CNN <ref type=\"bibr\" target=\"#b12\">[12]</ref>, <ref type=\"bibr\" target=\"#b25\">[25]</ref> and RNN <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and it was used in <ref type=\"bibr\" target=\"#b1\">[2]</ref>  nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=\"bibr\" target=\"#b0\">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=\"bibr\" target=\"#b0\">(1)</ref>, and so now we generate results by performing a second pass . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  is used to generate test directives designed to accurately hit the coverage tasks. For example, in <ref type=\"bibr\" target=\"#b13\">[14]</ref> an FSM model of pipelines is used to generate tests that c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent label prediction <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b54\">55,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. Adversarial examples have been shown to be ubiquitous beyo \" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b70\">71]</ref>. Among them, adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> is one of the most side in a large, contiguous region and a significant portion of the adversarial subspaces is shared <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta e sense of robustness against adversarial attacks due to gradient masking, and adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta se method against adversarial attacks. It improves model robustness by solving a minimax problem as <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>:</p><formula xml:i he proposed formulation deviates from the conventional minimax formulation for adversarial training <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. More specifically  \u2261 i L \u03b8 (x i , y i ), the proposed approach reduces to the conventional adversarial training setup <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. The overall proce gn method (FGSM) for adversarial attack generation is developed and used in adversarial training in <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Many variants of attacks have been developed later <ref t  inner maximization can be solved approximately, using for example a one-step approach such as FGSM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, or a multi-step projected gradient descent (PGD) method < y measuring the accuracy of the model under different adversarial attacks, including white-box FGSM <ref type=\"bibr\" target=\"#b23\">[24]</ref>, PGD <ref type=\"bibr\" target=\"#b35\">[36]</ref>, CW <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ww.tei-c.org/ns/1.0\"><head>Test results appear in</head><p>The results for this task are given in   <ref type=\"bibr\" target=\"#b23\">Reynolds, 2002)</ref>, which uses inductive predicates to describe ab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff an ref type=\"bibr\" target=\"#b52\">(Zhu, 2006;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: echniques for generation of functional test programs for manufacturing testing of microprocessors ( <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ices. Furthermore, many machine learning and data mining algorithms can also be modeled with graphs <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Hence, machine learning based on distributed graph-comput ermined by the number of mirrors of the vertices. Most traditional DGC frameworks, such as GraphLab <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Pregel <ref type=\"bibr\" target=\"#b14\">[15]</ref>, use  ph-computing (DGC) frameworks has attracted much attention from big data machine learning community <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: te positive training examples from the table without human effort.We first used a table parsing tool<ref type=\"bibr\" target=\"#b5\">[6]</ref> 3 to extract tables from raw pdf files of papers. Then we pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: echniques to mitigate this issue include multi-task learning <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and pre-trained components <ref type=\"bibr\" target=\"#b10\">[1  both fully supervised data and also weakly supervised data, <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> use multi-task learning to train the ST model jointly with t g and multi-task learning as proposed in previous literature <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ccuracies, limitations and pitfalls of the related technique known as negotiated-congestion routing <ref type=\"bibr\" target=\"#b27\">[28]</ref>. In Copyright (c) 2008 IEEE. Personal use of this material 5\">[6]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bib ested regions, often at the cost of increased wirelength.</p><p>Negotiated-congestion Routing (NCR) <ref type=\"bibr\" target=\"#b27\">[28]</ref> was introduced in the mid-1990s for global routing in FPGA  (b e ), added cost reflecting congestion history (h e ), and penalty for current congestion (p e ) <ref type=\"bibr\" target=\"#b27\">[28]</ref>. NCR seeks to minimize e c e .</p><p>To begin negotiated-c re-route is the same for each iteration, but can be chosen arbitrarily, according to the authors of <ref type=\"bibr\" target=\"#b27\">[28]</ref>, because the gradual cost increase in congested areas remo =\"formula_7\">c e = b e + h e \u2022 p e<label>(8)</label></formula><p>which is different than Equation 1 <ref type=\"bibr\" target=\"#b27\">[28]</ref>, but also is more intuitive since it preserves the base co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  2017)</ref> to graphstructured inputs, building on the recent Graph Attention Network architecture <ref type=\"bibr\" target=\"#b31\">(Veli\u010dkovi\u0107 et al., 2018)</ref>. The result is a powerful, general mo trong baselines. In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of <ref type=\"bibr\" target=\"#b31\">(Veli\u010dkovi\u0107 et al., 2018)</ref>. This encoder consists of PReLU activ ibr\" target=\"#b9\">(Kipf and Welling, 2017)</ref>. Our model extends the graph attention networks of <ref type=\"bibr\" target=\"#b31\">Veli\u010dkovi\u0107 et al. (2018)</ref>, a direct descendant of the convolutio  loss of infor- Graph Transformer Our model is most similar to the Graph Attention Network (GAT) of <ref type=\"bibr\" target=\"#b31\">Veli\u010dkovi\u0107 et al. (2018)</ref>, which computes the hidden representat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e nodes into a K-dimensional vector space, which preserves certain properties among nodes. Deepwalk <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref>, LINE <ref type=\"bibr\" target=\"#b6\">[Tang et tain properties among nodes. Deepwalk <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref>, LINE <ref type=\"bibr\" target=\"#b6\">[Tang et al., 2015]</ref> and Node2vec <ref type=\"bibr\" target=\"#b2\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cture <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> has been commonly used in NER task <ref type=\"bibr\" target=\"#b2\">(Castro et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">de Araujo et  sification is then performed by the CRF layer. Several pre-trained word embeddings were explored by <ref type=\"bibr\" target=\"#b2\">Castro et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b4\">Fernandes l 10 classes. This is the same setup used by <ref type=\"bibr\">Santos and Guimaraes (2015)</ref> and <ref type=\"bibr\" target=\"#b2\">Castro et al. (2018)</ref>.</p><p>Vagueness and indeterminacy: some te narios (total and selective) to the works of <ref type=\"bibr\">Santos and Guimaraes (2015)</ref> and <ref type=\"bibr\" target=\"#b2\">Castro et al. (2018)</ref>. To make the results comparable to both wor f the fine-tuning approach, as expected. BERT-LSTM-CRF outperforms the reported results of LSTM-CRF <ref type=\"bibr\" target=\"#b2\">(Castro et al., 2018)</ref> by about 1.5 points on the selective scena. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: h node features and graph topological structural information to make predictions. Velickovic et al. <ref type=\"bibr\" target=\"#b28\">[27]</ref> adopt attention mechanism into graph learning, and propose ctions of the contents of a neighborhood or sequence. Therefore, they are adaptive to the contents. <ref type=\"bibr\" target=\"#b28\">[27]</ref> adapts an attention mechanism to graph learning and propos  thus can help stabilize the process, compared with the previously used row normalization as in GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>:</p><formula xml:id=\"formula_4\">E ijp = \u00caijp N j=1 \u00caijp<la ention based EGNN layer</head><p>We describe the attention based EGNN layer. The original GAT model <ref type=\"bibr\" target=\"#b28\">[27]</ref> is only able to handle one dimensional binary edge feature ula xml:id=\"formula_9\">X l\u22121 i\u2022 , X l\u22121 j\u2022</formula><p>and E ijp . In existing attention mechanisms <ref type=\"bibr\" target=\"#b28\">[27]</ref>, the attention coefficient depends on X i\u2022 and X j\u2022 only.  yer. Indeed, the essential difference between GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref> is whether we use the attention coefficients (i.e., matrix The three citation network datasets are also used in <ref type=\"bibr\" target=\"#b33\">[32]</ref> [18] <ref type=\"bibr\" target=\"#b28\">[27]</ref>. However, they all use a pre-processed version which disca class.</p><p>The baseline methods we used are GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>. To investigate the effectivenesses of each components, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b7\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b23\">Zhang et al., 2015a;</ref><ref type=\"bibr\" target=\"#b20\">Yang et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Wang et al., 2  out which words are useful and which words are useless. Therefore, we apply an attention mechanism <ref type=\"bibr\" target=\"#b20\">(Yang et al., 2016)</ref> to get those important words and assemble t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ut <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>When the graph structure of the input is known, <ref type=\"bibr\" target=\"#b1\">[2]</ref> introduced a model to generalize ConvNets using low learning r parameters. Our main contributions can be summarized as follows:</p><p>\u2022 We extend the ideas from <ref type=\"bibr\" target=\"#b1\">[2]</ref> to large-scale classification problems, specifically Imagene vised fashion. However, it does not attempt to exploit any weight-sharing strategy.</p><p>Recently, <ref type=\"bibr\" target=\"#b1\">[2]</ref> proposed a generalization of convolutions to graphs via the  v xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Spectral Networks</head><p>Our work builds upon <ref type=\"bibr\" target=\"#b1\">[2]</ref> which introduced spectral networks. We recall the definition mula_4\">\u2202 k x(\u03be) \u2202\u03be k \u2264 C |u| k |x(u)|du ,</formula><p>where x(\u03be) is the Fourier transform of x. In <ref type=\"bibr\" target=\"#b1\">[2]</ref> it was suggested to use the same principle in a general grap ction</head><p>Whereas some recognition tasks in non-Euclidean domains, such as those considered in <ref type=\"bibr\" target=\"#b1\">[2]</ref> or <ref type=\"bibr\" target=\"#b11\">[12]</ref>, might have a p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: artitioning with some run-time support to improve workload balance is the Multicluster architecture <ref type=\"bibr\" target=\"#b5\">[6]</ref>. In this case, the processor consists of several identical c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>Speech animation for rigged models. Several related methods produce animation curves for speech <ref type=\"bibr\" target=\"#b12\">[Edwards et al. 2016;</ref><ref type=\"bibr\" target=\"#b59\">Taylor et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ification dataset -FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> propose a hybrid attention-based prototypical   and useless at the same time.</p><p>So we apply a CNN-based feature attention mechanism similar to <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> proposed as a class feature extractor. It depe 17)</ref> which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by <ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref> which includes Proto and PHATT. For a fair com ><table /><note>* reported by<ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> and \u25c7 reported by<ref type=\"bibr\" target=\"#b3\">Gao et al. (2019)</ref>.</note></figure> \t\t\t<note xmlns=\"http://www.te assification and few-shot text classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Gao et al., 2019)</ref> tasks respectively, so our model is based on p shra et al., 2018)</ref>, Proto <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> and PHATT <ref type=\"bibr\" target=\"#b3\">(Gao et al., 2019)</ref> respectively.</p></div> <div xmlns=\"http://ww. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ral architectures, such as DRMM <ref type=\"bibr\" target=\"#b8\">(Guo et al., 2016)</ref> and Co-PACRR <ref type=\"bibr\" target=\"#b13\">(Hui et al., 2018)</ref>, adopt an interaction-based design. They ope. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nformation retrieval to capture the exact and soft matches between a query and a candidate document <ref type=\"bibr\" target=\"#b10\">[Xiong et al., 2017]</ref>. Specifically, we apply the basic BERT uni ors are disordered and independent from each other. Thus we adopt a RBF kernel aggregation function <ref type=\"bibr\" target=\"#b10\">[Xiong et al., 2017]</ref> to extract features about the accumulation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b8\">Jin, Barzilay, and Jaakkola, 2018)</ref>, social networks analysis <ref type=\"bibr\" target=\"#b21\">(Wang, Cui, and Zhu, 2016;</ref><ref type=\"bibr\" target=\"#b16\">Qiu et (Tang et al., 2015)</ref>, GraRep <ref type=\"bibr\" target=\"#b0\">(Cao, Lu, and Xu, 2015)</ref>, SDNE <ref type=\"bibr\" target=\"#b21\">(Wang, Cui, and Zhu, 2016)</ref>, DVNE <ref type=\"bibr\" target=\"#b24\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nese poetry generation have been mostly rulebased or template-based. Recurrent Neural Network (RNN) <ref type=\"bibr\" target=\"#b10\">[11]</ref> was recently introduced as it has been proved to be effect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  this form in NLP. In log-linear models, including both the original work on maximum-entropy models <ref type=\"bibr\" target=\"#b1\">(Berger et al., 1996)</ref>, and later work on conditional random fiel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in RDB has access to all the subsequent layers and passes on information that needs to be preserved <ref type=\"bibr\" target=\"#b10\">[7]</ref>. Concatenating the states of preceding RDB and all the prec oposed DenseNet, which allows direct connections between any two layers within the same dense block <ref type=\"bibr\" target=\"#b10\">[7]</ref>. With the local dense connections, each layer reads informa e the bias term is omitted for simplicity. We assume F d,c consists of G (also known as growth rate <ref type=\"bibr\" target=\"#b10\">[7]</ref>) feature-maps.</p><formula xml:id=\"formula_6\">[F d\u22121 ,F d,1 k architecture as residual dense block (RDB). More differences between RDB and original dense block <ref type=\"bibr\" target=\"#b10\">[7]</ref> would be summarized in Section 4.</p></div> <div xmlns=\"htt .tei-c.org/ns/1.0\"><head n=\"4.\">Discussions</head><p>Difference to DenseNet. Inspired from DenseNet <ref type=\"bibr\" target=\"#b10\">[7]</ref>, we adopt the local dense connections into our proposed res ne is the design of basic building block. SRDenseNet introduces the basic dense block from DenseNet <ref type=\"bibr\" target=\"#b10\">[7]</ref>. Our residual dense block (RDB) improves it in three ways:  <ref type=\"bibr\" target=\"#b6\">[3]</ref> and also demonstrates that stacking many basic dense blocks <ref type=\"bibr\" target=\"#b10\">[7]</ref> in a very deep network would not result in better performan. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: regating, or bagging, multiple networks are trained independently based on subsets of training data <ref type=\"bibr\" target=\"#b1\">(Breiman, 1996)</ref>. This results in an ensemble that is more stable. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> show that GNNs are vulnerable to poisoning attacks which ad get=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. As already noted, such attacks can be (i) node specific, a ) node specific, as in the case of a target evasion attack <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> that is designed to ensure that the GNNs are fooled into mi graph. As shown by <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b43\">44,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>, both node specific and non-target attacks can be executed  n adversary can attack the GNNs by poisoning the graph data used for training. For example, Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> shows that by adding the adversarial perturbations on the  des in the graph so as to reduce the accuracy of the resulting graph neural networks.</p><p>Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> is one of the first methods that perturbs the graph data t  Baseline Methods. Though there are several adversarial attack algorithms on graphs such as Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> and RL-S2v <ref type=\"bibr\" target=\"#b11\">[12]</ref>, most //www.tei-c.org/ns/1.0\"><head n=\"5.2.1\">Node Classification on Clean</head><p>Graph. As the Nettack <ref type=\"bibr\" target=\"#b50\">[51]</ref> points out that \"poisoning attacks are in general harder a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: jectives as the minimum logarithmic or linear arrangements <ref type=\"bibr\" target=\"#b31\">[35,</ref><ref type=\"bibr\" target=\"#b32\">36]</ref>. On a mixture of K \u2212 K, A \u2212 K, and A \u2212 A edges we anticipat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arning techniques for code, beyond code completion. These include techniques for code summarization <ref type=\"bibr\" target=\"#b8\">(Alon et al., 2019b)</ref>, bug finding <ref type=\"bibr\" target=\"#b6\"> n problem-a non-neural but tree aware engine could outperform RNNs. In the same spirit, Alon et al. <ref type=\"bibr\" target=\"#b8\">(Alon et al., 2019b)</ref> had found-for code summarization problem (t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata by predicting users' next action given the last action <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. Zimdars et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> p n <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. Zimdars et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> propose a sequential recommender based on Markov chains an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion in recent years <ref type=\"bibr\" target=\"#b11\">[4]</ref><ref type=\"bibr\" target=\"#b12\">[5]</ref><ref type=\"bibr\" target=\"#b13\">[6]</ref><ref type=\"bibr\" target=\"#b14\">[7]</ref>.</p><p>Conventional olarity classifiers <ref type=\"bibr\" target=\"#b11\">[4]</ref><ref type=\"bibr\" target=\"#b12\">[5]</ref><ref type=\"bibr\" target=\"#b13\">[6]</ref>. However, due to the affective gap between lowlevel visual  ges on social media <ref type=\"bibr\" target=\"#b11\">[4]</ref><ref type=\"bibr\" target=\"#b12\">[5]</ref><ref type=\"bibr\" target=\"#b13\">[6]</ref><ref type=\"bibr\" target=\"#b14\">[7]</ref>. Typically, the goa image. Similarly, attribute features including facial expression were used as mid-level features in <ref type=\"bibr\" target=\"#b13\">[6]</ref>. These conventional methods focus on how to design visual r  performance improvement. We will introduce additional views or features such as facial expressions <ref type=\"bibr\" target=\"#b13\">[6]</ref>. In addition, we will introduce the deep learning-based fea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tly unroll a fixed number of iterations <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Maclaurin et al. <ref type=\"bibr\" target=\"#b22\">[23]</ref> go further and back-propagate gradients through an entire . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e x i t s h a 1 _ b a s e 6 4 = \" t A  We use primitives from an existing code generation framework <ref type=\"bibr\" target=\"#b8\">[9]</ref> to form S e . Our search space includes multi-level tiling o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: m considered in this work distinctive from the conventional distributed resource allocation problems<ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  machine learning approaches, a number of differential privacy based methods have been proposed. In <ref type=\"bibr\" target=\"#b21\">[22]</ref>, a differentially-private stochastic gradient descent algo ective deep learning model that can benefit from the data of all users. Our work is most related to <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bib \"bibr\" target=\"#b29\">[30]</ref>, but is quite different in several aspects. The proposed schemes in <ref type=\"bibr\" target=\"#b21\">[22]</ref> and <ref type=\"bibr\" target=\"#b22\">[23]</ref> were not des l privacy which can hide the existence of participants, and uses the moment accountant technique in <ref type=\"bibr\" target=\"#b21\">[22]</ref> to track the privacy loss. However, both methods did not c gression task.</p><p>Since the approaches proposed in <ref type=\"bibr\" target=\"#b22\">[23]</ref> and <ref type=\"bibr\" target=\"#b21\">[22]</ref> are not specially designed for collaborative learning, we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef><ref type=\"bibr\" target=\"#b15\">16]</ref> further extend the deep models for multimodal learning. <ref type=\"bibr\" target=\"#b16\">[17]</ref> design a cross-media learning method based on DNN, and lev network (CNN) with cross autoencoders to learn the latent high-level attributes on crossmodal units <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Finally, we pro very tweets by utilizing a recently proposed cross-media model, namely the Cross Autoencoders (CAE) <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>An auto encoder is a basic unit in deep neural netw s for comparison with previous work, due to the different goal, our results are not comparable with <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Actually, the most related user-level prediction work is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ab_0\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b26\">Wiese et al. (2017)</ref>,<ref type=\"bibr\" target=\"#b12\">Kirkpatrick et al. (2017)</ref>, and<ref type=\"bibr\" target=\"#b17\">Sc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: distance score function. TransH <ref type=\"bibr\" target=\"#b20\">(Wang et al., 2014)</ref> and TransR <ref type=\"bibr\" target=\"#b10\">(Lin et al., 2015b)</ref> are two typical models using different meth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b3\">(Han et al. 2004;</ref><ref type=\"bibr\" target=\"#b4\">Huang, Ertekin, and Giles 2006;</ref><ref type=\"bibr\" target=\"#b12\">Yoshida et al. 2010</ref>) usually leverage supervised learning algor t=\"#b4\">Huang, Ertekin, and Giles 2006;</ref><ref type=\"bibr\" target=\"#b8\">Louppe et al. 2016;</ref><ref type=\"bibr\" target=\"#b12\">Yoshida et al. 2010)</ref>, which usually solve the problem in a disc name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=\"bibr\" target=\"#b12\">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eters, and (3) knowledge distillation (KD).</p><p>First, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref> proposed the binary encoding of model parameters. Under th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >Related Work</head><p>Early works conduct ST in a pipeline manner (Ney 1999; Matusov, Kanthak, and <ref type=\"bibr\" target=\"#b22\">Ney 2005)</ref>, where the ASR output are fed into an MT system to ge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref> which capitalize on FlowNet-s <ref type=\"bibr\" target=\"#b58\">[59]</ref> to produce optical flow, PWC-Net <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 011)</ref>. Besides, gazetteers can also be easily constructed from knowledge bases (e.g., Freebase <ref type=\"bibr\" target=\"#b0\">(Bollacker et al., 2008)</ref>) or com- While such background knowledg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Active learning is widely studied to solve this kind of sample selection problem. As discussed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>, active learning methods can be divided into two categorie ertain number of labeled samples to evaluate the uncertainty of the unlabeled data or sampling bias <ref type=\"bibr\" target=\"#b17\">[18]</ref> will result. It is therefore recommended that such methods ive learning algorithms are referred to as early active learning or early stage experimental design <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We illustrate the procedures of and example of the tradit a><p>Finding the optimal subset V \u2282 X in Eq. ( <ref type=\"formula\">7</ref>) is NP-hard. Inspired by <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we relax the problem to the following problem by introduc ime, the least squared loss used in Eq. ( <ref type=\"formula\">8</ref>) is sensitive to the outliers <ref type=\"bibr\" target=\"#b17\">[18]</ref>, which makes the algorithm not robust.</p><p>We note that  type=\"bibr\" target=\"#b25\">26]</ref>, the 2,1 -norm is used instead of the 2,0 -norm. It is shown in <ref type=\"bibr\" target=\"#b17\">[18]</ref> that the 2,1 -norm is the minimum convex hull of the 2,0 - 1.0\"><head n=\"2.\">K-means</head><p>We use the K-means algorithm as another baseline algorithm as in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In each experiment, samples are ranked by their distances It formulates a regularized linear regression problem which minimizes reconstruction error. 5. RRSS <ref type=\"bibr\" target=\"#b17\">[18]</ref> Early active learning via Robust Representation and Struct ance of the linear methods with our algorithm. This is consistent with the mathematical analysis in <ref type=\"bibr\" target=\"#b17\">[18]</ref> that kernelization produces more discriminative representa ithm not robust.</p><p>We note that in previous researches <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>, the 2,1 -norm is u s, minimization of A 2,1 will achieve the same result as A 2,0 when A is row-sparse. As analyzed in <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, the 2,1 -norm can. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tation models (PLMs) such as ELMo <ref type=\"bibr\" target=\"#b29\">(Peters et al., 2018a)</ref>, BERT <ref type=\"bibr\" target=\"#b9\">(Devlin et al., 2019a)</ref> and XLNet <ref type=\"bibr\" target=\"#b46\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hniques in tasks like gene name recognition <ref type=\"bibr\" target=\"#b9\">(Kuksa and Qi, 2010;</ref><ref type=\"bibr\" target=\"#b23\">Tang et al., 2014;</ref><ref type=\"bibr\" target=\"#b24\">Vlachos and Ga. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tering transforms or certain CNN architectures have been shown to be stable to spatial deformations <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ ze the stability of GCNs to small deformation of the underlying random graph model. Similar to CNNs <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, studying GCNs in th es of stability are often balanced by discussions on how the representation preserves signal (e.g., <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ ine intuitive notions of deformations and stability in the continuous world like the Euclidean case <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ p><p>Related work on stability. The study of stability to deformations has been pioneered by Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref> in the context of the scattering transform for signals on  ph models and to obtain deformation stability bounds that are similar to those on Euclidean domains <ref type=\"bibr\" target=\"#b31\">[32]</ref>. We note that <ref type=\"bibr\" target=\"#b28\">[29]</ref> al eformations is an essential feature for the generalization properties of deep architectures. Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref> studied the stability to small deformations of the wavelet for small enough \u2207\u03c4 \u221e , we obtain N P (\u03c4 ) d \u2207\u03c4 \u221e , recovering the more standard quantity of Mallat <ref type=\"bibr\" target=\"#b31\">[32]</ref>. In this case, we also have the bound</p><formula xml:id=\"  ). Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>Proposition 4 (Signal deformation). Consider a GCN  \u03c4 \u221e , the GCN is invariant to translations and stable to deformations, similar to Euclidean domains <ref type=\"bibr\" target=\"#b31\">[32]</ref>. We note that studies of stability are often balanced by d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hat backpropagation improves neither accuracy nor detectability of a GCN-based GNN model. Li et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref> empirically analyzed GCN models with many layers under the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p>Existing literature on training with noisy labels focuses primarily on loss correction approaches <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015;</ref><ref type=\"bibr\" target=\"#b9\">Hendrycks et a ling using the network predictions to predict hard or soft labels.</p><p>Loss correction approaches <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015;</ref><ref type=\"bibr\" target=\"#b11\">Jiang et al., pe=\"bibr\" target=\"#b11\">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrapping loss <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>, which introduces a perceptual consistency  ilities used to compute it, to compensate for the incorrect guidance provided by the noisy samples. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> extend the loss with a perceptual term that is unsupervised model to implement a loss correction approach that benefits both from bootstrapping <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> and mixup data augmentation <ref type=\"bibr ibr\" target=\"#b33\">(Zhang et al., 2017)</ref>.</p><p>The static hard bootstrapping loss proposed in <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> provides a mechanism to deal with label noi ) ,<label>(10)</label></formula><p>where w i weights the model prediction z i in the loss function. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> use w i = 0.2, \u2200i. We refer to this approac Reed et al., 2015)</ref> use w i = 0.2, \u2200i. We refer to this approach as static hard bootstrapping. <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> also proposed a static soft bootstrapping l beling. We also run our proposed approach under these conditions in Subsection 4.5 for comparison.  <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>. The overall results demonstrate that apply sed dynamic hard bootstrapping exhibits better performance than the state-of-the-art static version <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref>. It is, however, not better than the perfor d the 300 epochs training scheme (see Subsection 4.1) . We introduce bootstrapping in epoch 105 for <ref type=\"bibr\" target=\"#b22\">(Reed et al., 2015)</ref> for the proposed methods, estimate the T ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r model with five state-of-the-art fewshot learning models based on neural networks, they are MetaN <ref type=\"bibr\" target=\"#b11\">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type=\"bibr\" target=\"#b4\">(G. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igure\" target=\"#fig_0\">1</ref>). For the training data scarcity problem, we introduce meta learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta used as a data augmentation strategy to deal with the lack of training data. However, meta-learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta number of model parameters. To deal with the data scarcity problem, we propose to use meta learning <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b51\">52,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  obtained from meta-training tasks for a newly seen few-shot task such as intention classification, <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> present a relation classification dataset -Few  one layer convolutional neural networks (CNN). For ease of comparison, its details are the same as <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> proposed. Hierarchical Attention In order to g  with the CNN encoder. For the neural networks based baselines, we use the same hyper parameters as <ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> proposed.</p><p>For our hierarchical attention or 5 way 5 shot and 10 way 5 shot settings on FewRel test set.</figDesc><table /><note>* reported by<ref type=\"bibr\" target=\"#b5\">Han et al. (2018)</ref> and \u25c7 reported by<ref type=\"bibr\" target=\"#b3\" as achieved excellent performance in few-shot image classification and few-shot text classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Gao et al., 2019 ttp://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Datasets</head><p>FewRel Few-Shot Relation Classification <ref type=\"bibr\" target=\"#b5\">(Han et al., 2018)</ref>  </p></div> <div xmlns=\"http://www.tei-c.org/. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d on the results recommended users with similar interests on the Twitter network to the target user <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Ramage et al. improved the accuracy of LDA based user recom. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">Introduction</head><p>Deep convolutional neural networks <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> have led to a series of breakthroughs for image classificat  type=\"bibr\" target=\"#b20\">21]</ref> have led to a series of breakthroughs for image classification <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" ta 1.0\"><head n=\"3.4.\">Implementation</head><p>Our implementation for ImageNet follows the practice in <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. The image is resi 4 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted <ref type=\"bibr\" target=\"#b20\">[21]</ref>. The standard color augmentation in <ref type=\"bibr\" targe pixel mean subtracted <ref type=\"bibr\" target=\"#b20\">[21]</ref>. The standard color augmentation in <ref type=\"bibr\" target=\"#b20\">[21]</ref> is used. We adopt batch normalization (BN) <ref type=\"bibr t=\"#b15\">[16]</ref>.</p><p>In testing, for comparison studies we adopt the standard 10-crop testing <ref type=\"bibr\" target=\"#b20\">[21]</ref>. For best results, we adopt the fullyconvolutional form as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.\">Background and Related Work</head><p>Previous work <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> describes support me ware support for thread activation and deactivation, as found in prior studies of thread scheduling <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. While those works u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: isplays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref>. The gesture annotation is performed usin es is motivated by previous research that has suggested that deceivers' speech has lower complexity <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref>. We use the tool described in <ref type=\" or gaze) and nod (Side-Turn-R) more frequently than truth-tellers. This agrees with the findings in <ref type=\"bibr\" target=\"#b8\">(Depaulo et al., 2003)</ref> that liars who are more motivated to get . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dition, we will introduce the deep learning-based features <ref type=\"bibr\" target=\"#b37\">[30,</ref><ref type=\"bibr\" target=\"#b38\">31]</ref>, which have significantly improved many computer vision tas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ht construct we call a 'notify chain'. These are similar in structure to MCS queue based spin-locks <ref type=\"bibr\" target=\"#b6\">[7]</ref> with one crucial difference. Rather than spinning on a parti p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. MCS Lock Benchmark</head><p>The MCS Lock <ref type=\"bibr\" target=\"#b6\">[7]</ref> is a standard way of implementing a scalable spin-lock. Any   accesses.</p><p>We have implemented the MCS lock on MIPS64 using the CAS primitive as described in <ref type=\"bibr\" target=\"#b6\">[7]</ref>. In Mamba a notify chain as described above is used. When a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iscriminative. Generative approaches learn to generate or otherwise model pixels in the input space <ref type=\"bibr\" target=\"#b23\">(Hinton et al., 2006;</ref><ref type=\"bibr\" target=\"#b28\">Kingma &amp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a trainable and personalized relation scoring func  where GCNs can be used directly, while here we investigate GCNs for heterogeneous KGs. Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC o a user-personalized weighted graph that characterizes user's preferences. To this end, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero \u03bb is better than \u03bb = 0 (the case of Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref>), which justifies our claim that LS regularization can ass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e performance overhead of 15.8%, the simple heuristic it uses only covers 33.9% of SDCs. SymPLIFIED <ref type=\"bibr\" target=\"#b13\">[14]</ref> identifies SDC-causing instructions by symbolic execution,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: it feedback, implicit feedback is more difficult to utilize because of the lack of negative samples <ref type=\"bibr\" target=\"#b10\">[Pan et al., 2008]</ref>. Secondly, generating top-k preferred items  it operation. Finding approximate top-K items can be even finished in sublinear or logarithmic time <ref type=\"bibr\" target=\"#b10\">[Wang et al., 2012;</ref><ref type=\"bibr\" target=\"#b10\">Muja and Lowe n finished in sublinear or logarithmic time <ref type=\"bibr\" target=\"#b10\">[Wang et al., 2012;</ref><ref type=\"bibr\" target=\"#b10\">Muja and Lowe, 2009]</ref> by making use of index technique.</p><p>Se bibr\">[Zhou and Zha, 2012]</ref>, PPH <ref type=\"bibr\" target=\"#b12\">[Zhang et al., 2014]</ref>, CH <ref type=\"bibr\" target=\"#b10\">[Liu et al., 2014]</ref> incur large quantization loss <ref type=\"bib rm for f (x) and \u03b2 is its penalty coefficient. <ref type=\"bibr\">[Giannessi and Tardella, 1998;</ref><ref type=\"bibr\" target=\"#b10\">Lucidi and Rinaldi, 2010]</ref> show that the above two problems are  13)</label></formula><p>In terms of the loss function, we employ the popular and effective BPR loss <ref type=\"bibr\" target=\"#b10\">[Rendle et al., 2009]</ref>. In particular, given a user matrix U and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ures they consider relevant <ref type=\"bibr\" target=\"#b58\">(Zhang et al., 2014)</ref>. The study by <ref type=\"bibr\" target=\"#b34\">Palangi et al., (2016)</ref> proposed a deep model employing recurren. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -c.org/ns/1.0\"><head n=\"3.2.2.\">Attention</head><p>ESPnet uses a location-aware attention mechanism <ref type=\"bibr\" target=\"#b34\">[35]</ref>, as a default attention. A dot-product attention <ref type conditions (e.g., <ref type=\"bibr\" target=\"#b32\">[33]</ref> does not use any language models, while <ref type=\"bibr\" target=\"#b34\">[35]</ref> and <ref type=\"bibr\" target=\"#b10\">[11]</ref> use a word-b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , AutoMine).</p><p>To address these challenges, general-purpose graph mining systems like Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref>, Fracta e 16-core machine outperforms state-of-the-art distributed graph mining systems including Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref> and G-M </formula><p>Step 1</p><p>Step 2</p><p>Step 3  <ref type=\"bibr\" target=\"#b70\">[57]</ref>, Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> and Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref>. Num </ref> (a realworld graph dataset), RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref> and Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> generate over a billion partial matches for clique countin neral purpose graph mining systems 3 : Fractal <ref type=\"bibr\" target=\"#b25\">[12]</ref>, Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref>, RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref> and G-M  is because its breadth-first exploration generates large amounts of partial matches which must be  <ref type=\"bibr\" target=\"#b65\">[52]</ref> and RStream <ref type=\"bibr\" target=\"#b70\">[57]</ref>. '\u00d7' 4,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>. Arabesque <ref type=\"bibr\" target=\"#b65\">[52]</ref> is a distributed graph mining system that follows a filter n memory or on disk) so that they can be extended. While systems based on breadth-first exploration <ref type=\"bibr\" target=\"#b65\">[52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref> demand high memory oesn't need to separately define the exploration strategy, as done in other pattern-unaware systems <ref type=\"bibr\" target=\"#b65\">[52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>.</p></div> <div xm get=\"#b21\">[8,</ref><ref type=\"bibr\" target=\"#b25\">12,</ref><ref type=\"bibr\" target=\"#b47\">34,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>, they are not patte ico and labeled Patents have been used by previous systems <ref type=\"bibr\" target=\"#b25\">[12,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref> to evaluate FSM whi get=\"#b25\">12,</ref><ref type=\"bibr\" target=\"#b35\">22,</ref><ref type=\"bibr\" target=\"#b47\">34,</ref><ref type=\"bibr\" target=\"#b65\">52,</ref><ref type=\"bibr\" target=\"#b70\">57]</ref>. Arabesque <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  MT model. In contrast, synthesizing speech inputs using TTS is more similar to MT back-translation <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on-based models <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b12\">[12]</ref>, <ref type=\"bibr\" target=\"#b52\">[43]</ref> widely used in information retrieval. The interaction-base d by each person to 100.</p><p>The hyper-parameters of the RBF kernel functions are set the same as <ref type=\"bibr\" target=\"#b52\">[43]</ref>. We use 11 RBF kernels, with the hyper-parameters m = f1; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  predict execution time of programs by using a set of hand-crafted features of high level programs. <ref type=\"bibr\" target=\"#b9\">Dubach et al. (2007)</ref> uses neural networks with hand-crafted feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Algorithms that do consider object articulations <ref type ct's pose and scale relative to a category-specific canonical representation. Recently, Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> extended the object coordinate based approach to perform c re normalized and the orientations are aligned for objects in a given category. Whereas the work by <ref type=\"bibr\" target=\"#b27\">[28]</ref> focuses on pose and size estimation for rigid objects, the NCSH representation is inspired by and closely related to Normalized Object Coordinate Space (NOCS) <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which we briefly review here. NOCS is defined as a 3D spa iefly review here. NOCS is defined as a 3D space contained within a unit cube and was introduced in <ref type=\"bibr\" target=\"#b27\">[28]</ref> to estimate the category-level 6D pose and size of rigid o  closed. In addition to normalizing the articulations, NAOCS applies the same normalization used in <ref type=\"bibr\" target=\"#b27\">[28]</ref> to the objects, including zero-centering, aligning orienta head><p>For each part, NPCS further zero-centers its position and uniformly scales it as is done in <ref type=\"bibr\" target=\"#b27\">[28]</ref>, while at the same time keeps its orientation unchanged as ime keeps its orientation unchanged as in NAOCS. In this respect, NPCS is defined similarly to NOCS <ref type=\"bibr\" target=\"#b27\">[28]</ref> but for individual parts instead of whole objects. NPCS pr s {p i \u2208 S (j) }, we have their corresponding NPCS predictions {c i |p i \u2208 S (j) }. We could follow <ref type=\"bibr\" target=\"#b27\">[28]</ref> to perform pose fitting, where the Umeyama algorithm <ref  j) }, as is commonly done for bundle adjustment <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we also use RANSAC for outlier removal.</p><p>Finally, fo  , t (j) , s (j) and the NPCS {c i |p i \u2208 S (j) } to compute an amodal bounding box, the same as in <ref type=\"bibr\" target=\"#b27\">[28]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualization <ref type=\"bibr\" target=\"#b28\">[28]</ref>. This optimization statically converts an indirect branch  ly a subset of indirect branches with a limited number of targets that can be determined statically <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Our proposed VPC prediction mechanism provides the benefi 24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> classify the devirtualization techniques into guarded devi on can overcome this limitation, but it requires an expensive mechanism called on-stack replacement <ref type=\"bibr\" target=\"#b28\">[28]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  tion based on static analysis requires type analysis, which in turn requires whole program analysis <ref type=\"bibr\" target=\"#b28\">[28]</ref>, and unsafe languages like C\u00fe\u00fe also require pointer alias  or large applications. Due to the limited applicability of static devirtualization, Ishizaki et al. <ref type=\"bibr\" target=\"#b28\">[28]</ref> report only an average 40 percent reduction in the number  et=\"#b23\">[23]</ref>, and type feedback/devirtualization <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. As we show in Section 6, the benefit of devirtualization . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , an optimal relay selection strategy is developed in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. In <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the dynamism of electricity price and the deferrability o onding Nash value V i k N (XN , RN ) using ( <ref type=\"formula\" target=\"#formula_15\">10</ref>) and <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p><formula xml:id=\"formula_21\">End For 1 \u2264 n \u2264 N \u2212 1</fo sponding Nash value V i k n (Xn, Rn) using ( <ref type=\"formula\" target=\"#formula_15\">10</ref>) and <ref type=\"bibr\" target=\"#b11\">(12)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b33\">34]</ref>, and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these app hs, which are hard to tune in practice. (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above t ht for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.</p><p>\u2022 RippleNet <ref type=\"bibr\" target=\"#b23\">[24]</ref> is a representative of hybrid methods, which is a memory-n. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by changing the parameters of the optimization algorithm to depend on the network activation values <ref type=\"bibr\" target=\"#b22\">(Wiesler et al., 2014;</ref><ref type=\"bibr\" target=\"#b14\">Raiko et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[36]</ref> (e) EDSR <ref type=\"bibr\" target=\"#b36\">[36]</ref> (f) DBPN <ref type=\"bibr\" target=\"#b20\">[20]</ref> (g) RDN <ref type=\"bibr\" target=\"#b6\">[6]</ref> (h) Ours</ with more than 16 layers based on residual learning. To further improve the performance, Lim et al. <ref type=\"bibr\" target=\"#b20\">[20]</ref> proposed a very deep and wide network EDSR by stacking mod  <ref type=\"bibr\" target=\"#b14\">[14]</ref>, Mem-Net <ref type=\"bibr\" target=\"#b30\">[30]</ref>, EDSR <ref type=\"bibr\" target=\"#b20\">[20]</ref>, SRMD <ref type=\"bibr\" target=\"#b36\">[36]</ref>, NLRN <ref module, and reconstruction part. Given I LR and I SR as the input and output of SAN. As explored in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>, we apply only one ndencies.</p><p>It has been verified that stacking residual blocks is helpful to form a deep CNN in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>. However, very dee filter are set as 3 \u00d7 3 and C =6 4 , respectively. For upscale part H \u2191 (\u2022), we follow the works in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref> and apply ESPCNN < ments</head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">Setup</head><p>Following <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" targ <ref type=\"bibr\" target=\"#b39\">[39]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" ta 30\">30]</ref>, L 1 <ref type=\"bibr\" target=\"#b14\">[14,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>, perceptual losses . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the encoder architecture and use the Adam optimizer <ref type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. Followin 50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as the encoder architecture and use the Adam optimizer <ref type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. We set t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rained language models (e.g., ELMo <ref type=\"bibr\" target=\"#b29\">(Peters et al., 2018)</ref>, BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, XLnet <ref type=\"bibr\" target=\"#b45\">(Yan e achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Liu et al.,  entations of the fully supervised NER methods attain very close to the state-of-the-art performance <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b19\">Limsopatham . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on operation is set to 2, then all the potential words can easily fuse into corresponding positions <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref>. As shown in Figure <ref type=\"figure\" target=\"#fig_ tional efficiency. In general, end-to-end CNNs in NLP have mainly been used for text classification <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref>. For sequence labeling tasks, CNNs have been mainly  state \u2190 \u2212 h w i , which are concatenated for the NER prediction.</p><p>CNN. We apply a standard CNN <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref> structure on the character or word sequence to obtai 0\"><head n=\"4.3\">Hyper-Parameter Settings</head><p>For all four of the datasets, we used the Adamax <ref type=\"bibr\" target=\"#b4\">[Kingma and Ba, 2014]</ref> optimization to train our networks. The in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: er that predicts a property of interest <ref type=\"bibr\" target=\"#b62\">(Veldhoen et al., 2016;</ref><ref type=\"bibr\" target=\"#b10\">Conneau et al., 2018;</ref><ref type=\"bibr\" target=\"#b0\">Adi et al.,  \"bibr\" target=\"#b35\">Mickus et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">Adi et al., 2016;</ref><ref type=\"bibr\" target=\"#b10\">Conneau et al., 2018</ref>), BERT's learned knowledge of syntax <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d language modeling <ref type=\"bibr\" target=\"#b25\">(Ling et al., 2015b)</ref> or dependency parsing <ref type=\"bibr\" target=\"#b2\">(Ballesteros et al., 2015)</ref>. Figure <ref type=\"figure\" target=\"#f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b34\">[35]</ref>, which was then used to develop a multimodal deception detection system <ref type=\"bibr\" target=\"#b1\">[2]</ref>. An extensive review of approaches for evaluating human cred sent useful clues for deception, their performance is often similar to that of the n-grams features <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Since in our current work we are not focusing on the insigh. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Le and Zuidema, 2015)</ref>, which extends the chain LSTM to a recurs. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uto-encoder. Most recently, a Deep Structured Semantic Models (DSSM) for Web search was proposed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, which is reported to outperform significantly semantic hash in an input word sequence into a feature vector using the technique called word hashing proposed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>. For example, the word is represented by a count vector of i hastic gradient ascent. Learning of the C-DSSM is similar to that of learning the DSSM described in <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: use ratio) and generic annotations (e.g., vectorization, unrolling, thread binding). We use XGBoost <ref type=\"bibr\" target=\"#b6\">[7]</ref>, which has proven to be a strong feature-based model in past. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 62\">[63]</ref>, action recognition <ref type=\"bibr\" target=\"#b45\">[46]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b5\">[6]</ref>, salient object detection <ref type=\"bibr\" target=\"#b1\">[2]< =\"#b40\">[41]</ref>, edge detection <ref type=\"bibr\" target=\"#b36\">[37]</ref>, semantic segmentation <ref type=\"bibr\" target=\"#b5\">[6]</ref>, salient object detection <ref type=\"bibr\" target=\"#b33\">[34 ns of the fully convolutional network (FCN) for semantic segmentation task. In DeepLab, Chen et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> introduces cascaded. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  various forms of artificial noise during training. We distinct regularization methods like dropout <ref type=\"bibr\" target=\"#b46\">(Srivastava et al., 2014)</ref> and task-specific data augmentation t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ut regard to their distance in the input or output sequences <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In all but a few cases <ref type=\"bibr\" target=\"#b21\">[22]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eir original image size. The encoder part of FCN consists of visual geometry group network (VGGNet) <ref type=\"bibr\" target=\"#b25\">[26]</ref> that is a famous CNN classification model and the decoder . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: variants such as momentum <ref type=\"bibr\" target=\"#b19\">(Sutskever et al., 2013)</ref> and Adagrad <ref type=\"bibr\" target=\"#b3\">(Duchi et al., 2011)</ref> have been used to achieve state of the art  astic Gradient Descent with a mini-batch size m &gt; 1, or with any of its variants such as Adagrad <ref type=\"bibr\" target=\"#b3\">(Duchi et al., 2011)</ref>. The normalization of activations that depe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <head n=\"4.3\">Detailed Implementation</head><p>We implement the proposed method based on Tensorflow <ref type=\"bibr\" target=\"#b0\">[1]</ref>. For our method, we set the dimension of term embedding as 6. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  give a brief introduction to GNNs, and one can refer to <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref> for a more detailed information. GNNs deal with learning p able.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Gra here u \u2208 N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>: It uses the MLP and sum pooling as the aggregation and co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nvariant semantic relationships between lower level sample features and higher level class features <ref type=\"bibr\" target=\"#b7\">(Hinton et al., 2011)</ref>. To ensure the class vector encapsulates t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igner. We call this method pronunciation-assisted sub-word modeling (PASM), which adopts fast align <ref type=\"bibr\" target=\"#b8\">[9]</ref> to align a pronunciation lexicon arXiv:1811.04284v2 [cs.CL] . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targe >19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> for GP. Very recently, the authors of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> find that the vertex-cut methods can achieve better performa rtex-cut has attracted more and more attention from DGC research community. For example, PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> adopts a random vertex-cut method and two greedy variants fo sly guarantee good workload balance. \u2022 DBH can be implemented as an execution engine for PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and hence all PowerGraph applications can be seamlessly sup hines. Hence, |A(v)| is the number of replicas of v among different machines. Similar to PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>, one of the replicas of a vertex is chosen as the master and  the \u03b1 is, the more skewed a graph will be. This power-law degree distribution makes GP challenging <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Although vertex-cut methods can achieve better performance  though vertex-cut methods can achieve better performance than edge-cut methods for power-law graphs <ref type=\"bibr\" target=\"#b5\">[6]</ref>, existing vertex-cut methods, such as random method in Power ysis for our DBH method. For comparison, the random vertex-cut method (called Random) of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> and the grid-based constrained solution (called Grid) of Gra ly to the p machines via a randomized hash function. The result can be directly got from PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Lemma 1. Assume that we have a sequence of n vertices {v i   theorem says that our DBH method has smaller expected replication factor than Random of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Next we turn to the analysis of the balance constrain \"5.2\">Baselines and Evaluation Metric</head><p>In our experiment, we adopt the Random of PowerGraph <ref type=\"bibr\" target=\"#b5\">[6]</ref> and the Grid of GraphBuilder [8]<ref type=\"foot\" target=\"#fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cements in the computer vision community on very deep CNNs <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that have not been * Work done as Google Brain interns. exp  build such deeper models. NiN has seen great success in computer vision, building very deep models <ref type=\"bibr\" target=\"#b17\">[18]</ref>. We show how to apply NiN principles in hierarchical Recur on that led to the success of very deep networks in vision <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on. The frame sharpness is evaluated using the cumulative probability blur detection (CPBD) measure <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which determines blur based on the presence of edges in t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arning siamese neural networks which employs an unique structure to rank similarity between inputs. <ref type=\"bibr\" target=\"#b17\">Vinyals et al. (2016)</ref> use matching networks to map a small labe best hyper parameters, and D test to evaluate the model.</p><p>The \"episode\" training strategy that <ref type=\"bibr\" target=\"#b17\">Vinyals et al. (2016)</ref> proposed has proved to be effective. For . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  . , h T \u2032 ) (T \u2032 \u2264 T ), interleaved with subsampling layers to reduce the computational complexity <ref type=\"bibr\" target=\"#b25\">[26]</ref>. The decoder network generates a probability distribution . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: en some research in using rule-based reasoning and KG embeddings together in an iterative manner in <ref type=\"bibr\" target=\"#b26\">[Zhang et al., 2019]</ref>.</p><p>They achieve improvements in the pe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: anism on Amazon EC2, which was enhanced by subsequent work <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. A series of recent work further study auction mechanism des ntly, a series of auction mechanisms are designed for VM allocation in cloud computing. Wang et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref> apply the critical value method, and derive a mechanism that anisms</head><p>Now we compare our online auction algorithm AucBS to another auction algorithm MUCA <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The settings in MUCA are the most similar in the existing l ponential number of possible integer solutions , and hence an exponential number of variables in LP <ref type=\"bibr\" target=\"#b5\">(6)</ref>. We therefore resort to its dual, formulated in <ref type=\"b ber of integer solutions to the oneround allocation problem (3) and the probabilities , which solve <ref type=\"bibr\" target=\"#b5\">(6)</ref>, in polynomial time.</p></div> <div xmlns=\"http://www.tei-c. e the fractional payment by VCG payment rule. 3: Solve the pair of primal-dual decomposition LPs in <ref type=\"bibr\" target=\"#b5\">(6)</ref> and <ref type=\"bibr\" target=\"#b6\">(7)</ref> using the ellips. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uts a human player would have. In contrast to previous work <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> , our approach incorporates 'end-to-end' reinforcement learn as inputs to the neural network by some previous approaches <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> . The main drawback of this type of architecture is that a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Knowledge bases (KBs), such as Freebase <ref type=\"bibr\" target=\"#b1\">(Bollacker et al., 2008)</ref>, NELL <ref type=\"bibr\" target=\"#b19\">(M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  one for relational classification using local features and an aggregation operator as described in <ref type=\"bibr\" target=\"#b22\">Sen et al. (2008)</ref>. We first train the local classifier using al ng the local classifier). L2 regularization parameter and aggregation operator (count vs. prop, see <ref type=\"bibr\" target=\"#b22\">Sen et al. (2008)</ref>) are chosen based on validation set performan aset statistics are summarized in Table1. In the citation network datasets-Citeseer, Cora and Pubmed<ref type=\"bibr\" target=\"#b22\">(Sen et al., 2008)</ref>-nodes are documents and edges are citation l We report results on a 5-fold cross-validation experiment on the Cora, Citeseer and Pubmed datasets <ref type=\"bibr\" target=\"#b22\">(Sen et al., 2008</ref>) using all labels. In addition to the standar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al., 2018)</ref>, OpenAI GPT <ref type=\"bibr\" target=\"#b15\">(Radford et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2018)</ref>. It has been shown that language modeling   target=\"#b2\">(Castro et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">de Araujo et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Fernandes et al., 2018)</ref>. The model is composed of two bidirectio ained word embeddings were explored by <ref type=\"bibr\" target=\"#b2\">Castro et al. (2018)</ref> and <ref type=\"bibr\" target=\"#b4\">Fernandes et al. (2018)</ref> compared it to 3 other architectures. Th mming. During evaluation, the most likely sequence is obtained by Viterbi decoding. As described in <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>, WordPiece tokenization requires prediction ead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>. The resulting architecture resembles the L , we use document context for input examples instead of sentence context. Following the approach of <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref> on the SQuAD dataset, examples larger than  pment set comprised of 10% of the First HAREM training set. We use the customized Adam optimizer of <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2018)</ref>.</p><p>For the feature-based approach, we u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <p>Identifying compound-protein interaction (CPI) plays an import role in discovering hit compounds <ref type=\"bibr\" target=\"#b39\">(Vamathevan et al., 2019)</ref>. Conventional methods, such as struct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: l correlation similar to prior studies of spatial footprints <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. We define a spatial region as a fixed-size portion of the  with the code and/or data address that initiates the pattern <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Whereas existing spatial pattern prefetching designs are e ow that the cache-coupled structures used in previous work ( <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>) are suboptimal for observing spatial correlation. Accesses rate predictions when correlation table storage is unbounded <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. By combining both quantities, which we call PC+address ind pproximated by combining the PC with a spatial region offset <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. The spatial region offset of a data address is the distanc \"4.2.\">Indexing</head><p>Prior studies of spatial predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> advocate predictor indices that include address information tries, consequently polluting the PHT.</p><p>Past predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> couple the predictor training structure to a sectored (i.e. y practical implementation evaluated on server workloads provides less than 20% miss rate reduction <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>In this paper, we reconsider prediction and streami is work demonstrates: \u2022 Effective spatial correlation and prediction. Contrary to previous findings <ref type=\"bibr\" target=\"#b16\">[17]</ref>, address-based correlation is not needed to predict the ac region generation is defined can significantly impact the accuracy and coverage of spatial patterns <ref type=\"bibr\" target=\"#b16\">[17]</ref>. A generation must be defined to ensure that, when SMS str t distinguish among distinct access patterns to different data structures by the same code (e.g.,   <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which indicated that PC+address provides superior coverag  experience worse conflict behavior. To mitigate this disadvantage, the spatial footprint predictor <ref type=\"bibr\" target=\"#b16\">[17]</ref> employed a decoupled sectored cache <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ype=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and pre-trained components <ref type=\"bibr\" target=\"#b10\">[11]</ref> in order to utilize weakly supervised data, i.e. speech-to ing so, both of them achieved better performance with the end-to-end model than the cascaded model. <ref type=\"bibr\" target=\"#b10\">[11]</ref> conducts experiments on a larger 236 hour English-to-Frenc n previous literature <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> in order to improve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion operations are legal only when the lock owning the condition is held by the current thread (See <ref type=\"bibr\" target=\"#b3\">[4]</ref> for discussion of alternatives). Thus, a ConditionObject att. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: been applied in the transductive setting with fixed graphs <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this work we both extend GCNs to the task of inductive  aph convolutional network (GCN), introduced by Kipf et al. <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN algorithm <ref type=\"bibr\" target=\"#b16\"> n O(|V|), so this requirement is not entirely unreasonable <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>.</p><p>Following Theorem 1, we let x v \u2208 U, \u2200v \u2208 V denote t  trained on a single, fixed graph. (That said, Kipf et al <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref> found that GCN-based approach consistently outperformed De. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  i to see how well the model performs on that task. The goal of Model-Agnostic Meta-Learning (MAML) <ref type=\"bibr\" target=\"#b9\">[9]</ref> is to obtain a parameter initialization \u03b8 * that can adapt t nowledge across meta-training tasks and is the optimal parameter to adapt to unseen tasks quickly.  <ref type=\"bibr\" target=\"#b9\">[9]</ref> switches ProtoNet to MAML as the meta-learner. All experimen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nd in particular, to code prediction <ref type=\"bibr\" target=\"#b13\">(Brockschmidt et al., 2019</ref><ref type=\"bibr\" target=\"#b24\">, Hindle et al., 2016</ref><ref type=\"bibr\" target=\"#b28\">, Li et al. r Code Completion. Some of the early ML models for code prediction relied on n-gram language models <ref type=\"bibr\" target=\"#b24\">(Hindle et al., 2016</ref><ref type=\"bibr\" target=\"#b31\">, Nguyen et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ical, fetch-modify-consistent views of the file system. We have formally specified fork consistency <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and, assuming digital signatures and a collision-resistan hus cannot ever see each other's operations without detecting the attack (as proven in earlier work <ref type=\"bibr\" target=\"#b15\">[16]</ref>).</p><p>One optimization worth mentioning is that SUNDR am cs for an untrusted server. An unimplemented but previously published version of the SUNDR protocol <ref type=\"bibr\" target=\"#b15\">[16]</ref> had no groups and thus did not address write-after-write c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aluated and validated for solo-use cache, including the two initial studies of the footprint theory <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Independent va. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: feedback as a composition of user result examination and relevance judgment. Examination hypothesis <ref type=\"bibr\" target=\"#b7\">[8]</ref>, which is a fundamental assumption in click modeling, postul ns; and among them result examination plays a central role <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. Unfortunately, most applications of bandit algorithms simply the selected arm a t . Based on the examination hypothesis <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, when C at = 1, the chosen a t must be relevant to the user's. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: urveillance systems to automatically detect suicidal behaviors and trigger an alarm. In this sense, <ref type=\"bibr\" target=\"#b9\">(Lee et al., 2014)</ref> presented a method for automatically analyzin ide by hanging attempts <ref type=\"bibr\" target=\"#b1\">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=\"bibr\" target=\"#b9\">(Lee et al., 2014)</ref>, we performed our experiments on a large data. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: erial images and LiDAR data. The developed network is based on a modified residual learning network <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> that extracts robust low/mid/high-level featu res in the input images <ref type=\"bibr\" target=\"#b50\">(Zhang et al., 2016)</ref>. Previous studies <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> have found that increasing the depth of neura e=\"figure\" target=\"#fig_2\">1</ref>).</p><p>A more detailed description of ResNet-50 can be found in <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> and here ResNet-50 is modified as follows to  cy may degrade after a saturation. This phenomenon is often referred to as the degradation problem. <ref type=\"bibr\" target=\"#b13\">He et al. (2016)</ref> recently proposed a Residual Network (ResNet) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ing progress in the application of machine learning (ML) techniques to developer productivity tools <ref type=\"bibr\" target=\"#b5\">(Allamanis et al., 2018a)</ref>, and in particular, to code prediction suggestions provided to a user by learning the statistical property of code, exploiting naturalness <ref type=\"bibr\" target=\"#b5\">(Allamanis et al., 2018a)</ref>.</p><p>Traditional ML-based techniques. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata, such as recommender systems <ref type=\"bibr\" target=\"#b24\">[25]</ref>, social network analysis <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and drug discovery <ref type=\"bibr\" target=\"#b14\">[15]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stness under noise <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>, are well aligned with their framework <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ectedness, or determining the relevance of specific agents <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" t http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"3\" xml:id=\"foot_2\">Note that the graph defined by Ac in<ref type=\"bibr\" target=\"#b6\">(7)</ref> is directed in order to match exactly the behavior of shifts. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . We are interested in how one's citation behaviors are influenced by her collaborators.</p><p>Digg <ref type=\"bibr\" target=\"#b22\">[23]</ref> Digg is a news aggregator which allows people to vote web . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f texts, which allows us to exploit the widely used natural language processing technique, word2vec <ref type=\"bibr\" target=\"#b34\">[35]</ref>, to embed the original features of queries and items into . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef> SNE <ref type=\"bibr\" target=\"#b19\">[20]</ref> DANE <ref type=\"bibr\" target=\"#b8\">[9]</ref> ANRL <ref type=\"bibr\" target=\"#b43\">[44]</ref> Heterogeneous Network (HEN) PTE <ref type=\"bibr\" target=\"# nlinearity and preserve various proximities in both topological structure and node attributes. ANRL <ref type=\"bibr\" target=\"#b43\">[44]</ref> uses a neighbor enhancement autoencoder to model the node  ed network embedding model.</p><p>Attributed Network Embedding Methods. The compared method is ANRL <ref type=\"bibr\" target=\"#b43\">[44]</ref>. ANRL uses a neighbor enhancement autoencoder to model the  Alibaba distributed cloud platform.</p><p>A.2.4 Attributed Network Embedding Methods.</p><p>\u2022 ANRL <ref type=\"bibr\" target=\"#b43\">[44]</ref>. We use the codes from Alibaba's GitHub 14 . As YouTube an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b8\">Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b20\">Kipf &amp; Welling, 2016;</ref><ref type=\"bibr\" target=\"#b15\">Gilmer et al., 2017;</ref><ref type=\"bibr\" target=\"#b18\">Hamilton et  ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, or any general message passing layer <ref type=\"bibr\" target=\"#b15\">(Gilmer et al., 2017)</ref>. Formally, let H l \u2208 R n\u00d7k be a matrix co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  To deal with the partial observation problem, we further extend the model to the inductive context <ref type=\"bibr\" target=\"#b41\">[42]</ref> and present a new inductive model named as GATNE-I. For bo rved data. However, in many real-world applications, the networked data is often partially observed <ref type=\"bibr\" target=\"#b41\">[42]</ref>. We then extend our model to the inductive context and pre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .0\"><head n=\"4.1\">Fast Fibonacci compression</head><p>In this method, we apply the Fibonacci coding <ref type=\"bibr\" target=\"#b1\">[2]</ref> which uses the Fibonacci numbers; 1, 2, 3, 5, 8, 13, . . . .. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: co-occurrence based methods <ref type=\"bibr\" target=\"#b9\">[10]</ref>, word-similarity based methods <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and supervised relation extraction methods <ref type=\"bib core based on word embedding similarity, where the embedding is pretrained with the Skip-Gram model <ref type=\"bibr\" target=\"#b19\">[20]</ref>    <ref type=\"table\" target=\"#tab_4\">4</ref> shows the man. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" targ periments:</p><p>\u2022 AMiner-AND<ref type=\"foot\" target=\"#foot_0\">1</ref> . The dataset is released by <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, which contains 500 author names for traini  e.g., paper network <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" target=\"#b13\">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature  (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type=\"bibr\" target=\"#b13\">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: exed by nodes of an arbitrary directed or undirected graph <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>. This choice is satisfying in the sense that, when the sig  \u2022 1 is norm 1, and A norm = 1 \u03bbmax A. Other norms could be used to define the total variation, see <ref type=\"bibr\" target=\"#b50\">[51]</ref> <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Using this, graph ustified theoretically that the frequency bases obtained from the shift operator tend to be ordered <ref type=\"bibr\" target=\"#b50\">[51]</ref>.</p><p>Up to this point, we have focused primarily on freq odel makes it possible to detect outliers or abnormal values by highpass filtering and thresholding <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b154\">[155]</ref>, or to build  f type=\"bibr\" target=\"#b4\">[5]</ref>, optimizing the prediction of unknown labels in classification <ref type=\"bibr\" target=\"#b50\">[51]</ref> or semisupervised learning problems <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: osed approach is more practical because the training data is generally inaccessible to the attacker <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Our contributions can be summarized as follows:</p><p>\u2022 W  original training data. However, in practice the attacker often has no access to the training data <ref type=\"bibr\" target=\"#b31\">[32]</ref>. To overcome this limitation, Mopuri et al. propose to gen ome this limitation, Mopuri et al. propose to generate universal perturbation without training data <ref type=\"bibr\" target=\"#b31\">[32]</ref>. However, their approach is specifically designed for non- -agnostic) attacks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to handle the variety of models of parallelism that appear in HPC programs. Toward this end, PEBIL <ref type=\"bibr\" target=\"#b0\">[1]</ref> has recently added support for handling multithreaded x86 64 afety</head><p>PEBIL generates and inserts code into the program which has two principle functions: <ref type=\"bibr\" target=\"#b0\">(1)</ref> to add functionality to a program, functionality which is us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: vely search the tree-structured architecture space. Motivated by these AutoML frameworks, He et al. <ref type=\"bibr\" target=\"#b10\">[11]</ref> leveraged the reinforcement learning to automatically prun. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: period under study, which was just over 14% and we were long 653 out of 1077 days.)</p><p>As Sharpe <ref type=\"bibr\" target=\"#b5\">(6)</ref> points out, instead of buying and selling short the DJIA, we. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e of a virtual switch, a couple of solutions (e.g., Netmap <ref type=\"bibr\" target=\"#b4\">[4]</ref>, <ref type=\"bibr\" target=\"#b5\">[5]</ref>, OpenOnloadn <ref type=\"bibr\" target=\"#b6\">[6]</ref>, Packet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raged to launch Distributed Denial-of-Service (DDoS) attacks, in particular, the reflection attacks <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Due to the absence of a method to block packet header modif eans conclude on the filtering policies of the whole AS-they reveal SAV compliance for a part of it <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target ysis of the SAV deployment at the longest matching prefix is another commonly used unit of analysis <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>.</p><p>\u2022 /24 IPv4 ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s. Although the time cost was reduced, it brought a large number of false positives.</p><p>The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a configurable protection technique for SDC-causi s, machine learning based methods are introduced to identify the SDC-causing instructions. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposes a machine learning algorithm based model, namely  pproach in detail. We first define some terms used in this paper, some of which are drawn from work <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>Dynamic Dependency Graph: A Dynamic Dependency Grap ased on prior work <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" t  are based on duplicating the backward slices of the instructions to protect, similar to prior work <ref type=\"bibr\" target=\"#b15\">[16]</ref>.</p><p>We insert a check immediately after the instruction tion efficiency and SDC impact are imperative parameters for evaluating our approach. In literature <ref type=\"bibr\" target=\"#b15\">[16]</ref>, the SDC detection efficiency (DE) is defined as the ratio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the activations of deep neural networks. For this, we consider the \"natural pre-image\" technique of <ref type=\"bibr\" target=\"#b20\">[21]</ref>, whose goal is to characterize the invariants learned by a n untrained deep convolutional generator can be used to replace the surrogate natural prior used in <ref type=\"bibr\" target=\"#b20\">[21]</ref> (the TV norm) with dramatically improved results. Since th antic segmentation) is highly detrimental.</p><p>Natural pre-image. The natural pre-image method of <ref type=\"bibr\" target=\"#b20\">[21]</ref> is a diagnostic tool to study the invariances of a lossy f e obtained by restricting the pre-image to a set X of natural images, called a natural pre-image in <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>In practice, finding points in the natural pre-imag inding points in the natural pre-image can  Inversion with deep image prior Inversion with TV prior <ref type=\"bibr\" target=\"#b20\">[21]</ref> Pre-trained deep inverting network <ref type=\"bibr\" target  on ImageNet ISLVRC) using three different regularizers: the Deep Image prior, the TV norm prior of <ref type=\"bibr\" target=\"#b20\">[21]</ref>, and the network trained to invert representations on a ho ne by regularizing the data term similarly to the other inverse problems seen above. The authors of <ref type=\"bibr\" target=\"#b20\">[21]</ref> prefer to use the TV norm, which is a weak natural image p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and incorporating character-level features to ConvNets <ref type=\"bibr\" target=\"#b27\">[28]</ref>  <ref type=\"bibr\" target=\"#b28\">[29]</ref>. In particular,  these ConvNet approaches use words as a basis, in which character-level features extracted at word <ref type=\"bibr\" target=\"#b27\">[28]</ref> or word n-gram <ref type=\"bibr\" target=\"#b28\">[29]</ref> l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: preliminary work of a neural influence Diff usion Network (i.e., DiffNet) for social recommendation <ref type=\"bibr\" target=\"#b36\">[37]</ref>. DiffNet models the recursive social diffusion process for \">[17]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>.</p><p>In fact, as users play a central role in social pla \">[45]</ref>, <ref type=\"bibr\" target=\"#b34\">[35]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. On one hand, given the useritem interest graph, NGCF is p  that the higher-order social structure is directly modeled in the recursive user embedding process <ref type=\"bibr\" target=\"#b36\">[37]</ref>. These graph based models showed superior performance comp ary, our main contributions are listed as follows:</p><p>\u2022 Compared to our previous work of DiffNet <ref type=\"bibr\" target=\"#b36\">[37]</ref>, we revisit the social recommendation problem as predictin d social recommendation model, DiffNet, for modeling the social diffusion process in recommendation <ref type=\"bibr\" target=\"#b36\">[37]</ref>. DiffNet advances classical embedding based models with ca at the up to K-th order social network structure is injected into the social recommendation process <ref type=\"bibr\" target=\"#b36\">[37]</ref>. In this part, we propose DiffNet++, an enhanced model of  t, in order to transform this model for the recommendation task. For our proposed models of DiffNet <ref type=\"bibr\" target=\"#b36\">[37]</ref> and DiffNet++, since both models are flexible and could be 8]</ref> and Normalized Discounted Cummulative Gain (NDCG) <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Specifically, HR measures the percentage of hit items in  formance with large itemset, similar as many other works <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, to evaluate the performance, for each user, we randomly s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b15\">(Luken et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Yoneda et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Hanselowski et al., 2018)</ref>. TwoWingOS <ref type=\"bibr\" target=\"#b ntence retrieval, same as the previous work <ref type=\"bibr\" target=\"#b34\">(Zhou et al., 2019;</ref><ref type=\"bibr\" target=\"#b9\">Hanselowski et al., 2018)</ref>, KGAT outperforms the graph attention  .0 task and BERT based models.</p><p>Three top models in FEVER 1.0 shared task are compared. Athene <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018)</ref> and UNC NLP <ref type=\"bibr\" target=\" e Me-diaWiki API<ref type=\"foot\" target=\"#foot_1\">4</ref> . Then the convinced article are reserved <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018)</ref>.</p><p>Sentence retrieval. The senten ad></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Dev</head><p>Test LA FEVER LA FEVER Athene <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018)</ref> 68.49 64.74 65.46 61.58 UCL MRG <ref  e document retrieval step retrieves related Wikipedia pages and is kept the same with previous work <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et ERT based sentence retrieval. The ESIM based sentence retrieval keeps the same as the previous work <ref type=\"bibr\" target=\"#b9\">(Hanselowski et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Zhou et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ors into coarse clusters typically through K-means algorithm, and the other is product quantization <ref type=\"bibr\" target=\"#b7\">[8]</ref> which does a fine-grained quantization to enable efficient c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the MDP state, which consists of the query, the preceding documents, and the remaining candidates <ref type=\"bibr\" target=\"#b32\">[33]</ref>.</p><p>The greedy sequential document selection simpli es  28]</ref> for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking <ref type=\"bibr\" target=\"#b32\">[33]</ref> with the Monte Carlo tree search (MCTS), for alleviating t anism <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Xia et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> proposed to model the dynamics of the document utility wit <ref type=\"bibr\" target=\"#b37\">[38]</ref>, the log-based document re-ranking is modeled as a POMDP. <ref type=\"bibr\" target=\"#b32\">[33]</ref> and <ref type=\"bibr\" target=\"#b29\">[30]</ref> propose to m <p>In our experiments, for e ective training of the model parameters and following the practices in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, we combined four TREC datasets and constructed a new data approach which automatically learns novelty features based on neural tensor networks.</p><p>MDP-DIV <ref type=\"bibr\" target=\"#b32\">[33]</ref>: a state-of-the-art learning approach which uses an MDP fo ning approach which uses an MDP for modeling the diverse ranking process. Following the practice in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, we con gured the reward function in MDP-DIV as \u03b1-DCG and  minary representations of the queries and the documents as their inputs. Following the practices in <ref type=\"bibr\" target=\"#b32\">[33]</ref>, in the experiments we used the query vector and document . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ms have been proposed <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS <ref type=\"bibr\" target=\"#b8\">[8]</ref>). We take this idea further by proposing a stochastic multi- p>We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis <ref type=\"bibr\" target=\"#b8\">[8]</ref> and Graclus <ref type=\"bibr\" target=\"#b4\">[4]</ref> aim to c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: method of changing speed has the lowest implementation cost and achieve stateof-the-art performance <ref type=\"bibr\" target=\"#b22\">[23]</ref>. In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, A new metho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -to-video generation <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> to text-to-video generation <ref type=\"bibr\" target=\"#b22\">[2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: of high inter-class similarities. Most recently, an additional center loss was introduced into CNNs <ref type=\"bibr\" target=\"#b43\">[44]</ref> to reduce the intra-class variations of the learned featur suffers from drastic data expansion when constructing image pairs from the training set. Wen et al. <ref type=\"bibr\" target=\"#b43\">[44]</ref> introduced a center loss for face recognition, which targe eview of Center Loss</head><p>As illustrated in Fig. <ref type=\"figure\">1</ref>(b), the center loss <ref type=\"bibr\" target=\"#b43\">[44]</ref> explicitly reduces the intra-class variations by pushing s  the CNN training.</p><p>1) Forward propagation: The center loss denoted as L C is defined in Eq. 1 <ref type=\"bibr\" target=\"#b43\">[44]</ref> as the summation of squared distances between samples and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ediction. There are two types of indirect branch predictors: history based and precomputation based <ref type=\"bibr\" target=\"#b46\">[46]</ref>. The technique we introduce in this paper utilizes history ormance impact of indirect branches that are hard to predict with VPC prediction.</p><p>Roth et al. <ref type=\"bibr\" target=\"#b46\">[46]</ref> proposed dependence-based precomputation, which precompute. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t al. 2017;</ref><ref type=\"bibr\" target=\"#b6\">B\u00e9rard et al. 2018</ref>) and pretraining techniques <ref type=\"bibr\" target=\"#b3\">(Bansal et al. 2019)</ref> have been applied to end-to-end ST model to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r text classification. The architecture is a direct application of CNNs, as used in computer vision <ref type=\"bibr\" target=\"#b12\">(LeCun et al., 1998)</ref>, albeit with NLP interpretations. <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ith kernel machines, one typically observes an increase in performance when using soft-DTW over DTW <ref type=\"bibr\" target=\"#b6\">(Cuturi, 2011)</ref> for classification.</p><p>Our contributions. We e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: with weight decay 0.0005 is applied to the weights W and a. Moreover, exponential linear unit (ELU) <ref type=\"bibr\" target=\"#b10\">[10]</ref> is employed as nonlinear activations for hidden layers.</p ied to parameters of the models except for bias parameters. Moreover, exponential linear unit (ELU) <ref type=\"bibr\" target=\"#b10\">[10]</ref> is employed as nonlinear activations for hidden layers.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s) and directly harness the resultant example to fool the remote target model (i.e., victim models) <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>Among these t \" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, and the other one is transfer-based <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta or fair comparisons, we adopt default parameters as recommended in benchmark approaches and Foolbox <ref type=\"bibr\" target=\"#b40\">[41,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. The random noise   of the source model <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Specifically, althou ork is the regularization-based approach: transferable adversarial perturbation (TAP) introduced by <ref type=\"bibr\" target=\"#b40\">[41]</ref>. TAP injects two regularization terms into the vanilla tra 19\">[20,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. We follow the protocol of the baseline method <ref type=\"bibr\" target=\"#b40\">[41]</ref> to curate experimental datasets and target models for fair cannot strictly meet the l \u221e budget, we employ the modified l \u221e version of C&amp;W as introduced by <ref type=\"bibr\" target=\"#b40\">[41]</ref>, which can explicitly satisfy the l \u221e norm constraint. Sim b40\">[41]</ref>, which can explicitly satisfy the l \u221e norm constraint. Similar to our strategy, TAP <ref type=\"bibr\" target=\"#b40\">[41]</ref> boosts adversarial transferability through two regularizat e random noise is sampled from a clipped normal distribution with mean 0 and variance 1.  Following <ref type=\"bibr\" target=\"#b40\">[41]</ref>, we fix the perturbation budget \u01eb to 16 for all methods. W ext attack models defended by adversarial training. For fair comparisons with the baseline approach <ref type=\"bibr\" target=\"#b40\">[41]</ref>, we stick to employing undefended models as local source m d the other one is the regularization-based transferable adversarial perturbation (TAP) proposed by <ref type=\"bibr\" target=\"#b40\">[41]</ref>. With the integrated attacks, we conduct experiments simil. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ge R-CNN framework <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, where detection is et=\"#b12\">[13]</ref> introduced the idea of region-wise feature extraction. Later, the Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[30]</ref> achieved further speeds-up by introducing a Region Proposa  network. SSD <ref type=\"bibr\" target=\"#b24\">[25]</ref> detects objects in a way similar to the RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, but uses multiple feature maps at different resolutions t We focus on modeling a multistage detection sub-network, and adopt, but are not limited to, the RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref> for proposal detection.</p></div> <div xmlns=\"http://www.t evels. At inference, since the majority of the hypotheses produced by a proposal detector, e.g. RPN <ref type=\"bibr\" target=\"#b29\">[30]</ref> or selective search <ref type=\"bibr\" target=\"#b32\">[33]</r \">Object Detection</head><p>In this paper, we extend the two-stage architecture of the Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, shown in Figure < zed by its mean and variance, i.e. is replaced by \u2032 =( \u2212 )/ . This is widely used in the literature <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" targ PN+.</p><p>Detection Performance: Again, our implementations are better than the original detectors <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" targ  was further experimented on PAS-CAL VOC dataset <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Following <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, the models were t e noted. The sampling of the first detection stage follows <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. In the following stages, resampling is implemented by simp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >, show substantially better effectiveness at the cost of orders of magnitude longer inference time <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" ta ><p>Recently, the issue of efficiency gained traction in the neural IR community. Hofst\u00e4tter et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> establish efficiency baselines for common neural IR models. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: yers and instead builds the pyramid starting from high up in the network (e.g., conv4 3 of VGG nets <ref type=\"bibr\" target=\"#b35\">[36]</ref>) and then by adding several new layers. Thus it misses the pendent of the backbone convolutional architectures (e.g., <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>), and in this paper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ervation that the salient objects in an image can be accurately detected by modern object detectors <ref type=\"bibr\" target=\"#b29\">[29]</ref>, and that these objects are often mentioned in the paired  ly extracted from over-sampled regions <ref type=\"bibr\">[2]</ref> via Faster R-CNN object detectors <ref type=\"bibr\" target=\"#b29\">[29]</ref>, which inevitably results in overlaps among image regions  s follows. Given an image with K regions of objects (normally over-sampled and noisy), Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[29]</ref> is used to extract the visual semantics of each region as  introduce the bottom-up mechanism to represent an image as a set of visual regions via Faster R-CNN <ref type=\"bibr\" target=\"#b29\">[29]</ref>, each with an associated feature vector. It enables attent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hod for a special type of graphs (i.e., knowledge graph).</p><p>Our method also connects to PinSage <ref type=\"bibr\" target=\"#b20\">[21]</ref> and GAT <ref type=\"bibr\" target=\"#b14\">[15]</ref>. But not. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" tar ng baseline. These results contradict previous conclusions <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> that the performanc  adversarial noise <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, fail to improve accuracy on clean images. ial examples to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: not visible at the node-level.</p><p>Graph kernels based on the k-WL have been proposed in the past <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref>. However, a key advantage o ref type=\"bibr\" target=\"#b35\">(Shervashidze et al. 2011)</ref> as well as its higher-order variants <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref>. Graphlet and Weisfeiler-Le  <ref type=\"bibr\" target=\"#b22\">(Kriege, Giscard, and Wilson 2016)</ref>, and the global-local k-WL <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref> with k in {2, 3} as kernel  \"foot_0\">Note that the definition of the local neighborhood is different from the the one defined in<ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017)</ref> which is a superset of our  e that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., <ref type=\"bibr\" target=\"#b27\">(Morris, Kersting, and Mutzel 2017;</ref><ref type=\"bibr\" target=\"#b1. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  predict execution time of programs by using a set of hand-crafted features of high level programs. <ref type=\"bibr\" target=\"#b9\">Dubach et al. (2007)</ref> uses neural networks with hand-crafted feat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he high-frequency details in HR images.</p><p>Recent works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> have successfully used very deep convolutional neural netwo rget=\"#b1\">[2]</ref>. Among them, the CNN-based approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> have recently set state of the art for SISR. A network with ctions between layers to effectively train a very deep network.</p><p>A skip connection was used in <ref type=\"bibr\" target=\"#b11\">[12]</ref> to link the input data and the final reconstruction layer  e input data and the final reconstruction layer in SR. State-of-the-art SR results were achieved in <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, only a single skip connection was adopted in <re in <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, only a single skip connection was adopted in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which may not fully explore the advantages of skip connec f> for image restoration tasks. However, the improvement of the SR performance over the DRCN method <ref type=\"bibr\" target=\"#b11\">[12]</ref> that used a single skip connection is marginal. An effecti NN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> and DRCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The implementations of these methods have been released o . This can help the training of very deep networks and improve the reconstruction performance in SR <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Several techniques were proposed to improve the accuracy  > SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> DRCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>   In addition, the proposed framework not only achieves im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d by deep learning based speech enhancement and separation <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar  as in <ref type=\"bibr\" target=\"#b20\">[21]</ref>, proposed in the same conference. A follow-up work <ref type=\"bibr\" target=\"#b18\">[19]</ref> of <ref type=\"bibr\" target=\"#b22\">[23]</ref> supplies clea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b9\">Chang and Ezzat 2005;</ref><ref type=\"bibr\" target=\"#b13\">Ezzat et al. 2002;</ref><ref type=\"bibr\" target=\"#b36\">Liu and Ostermann 2011]</ref> others are performance-driven <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#foot_0\">3</ref>  <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. They analyzed the relationship between internal properties bibr\" target=\"#b4\">[5]</ref>, narcissism <ref type=\"bibr\" target=\"#b6\">[7]</ref>, self-presentation <ref type=\"bibr\" target=\"#b12\">[13]</ref> and the corresponding user profiles on Facebook.</p><p>We . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ods (GCN <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b12\">[13]</ref>), etc.</p><p>Graph-level embedding. The most intuitive way. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ples is needed to address this issue. <ref type=\"bibr\" target=\"#b1\">Goodfellow et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b14\">Fawzi et al. (2018)</ref> pointed out that susceptibility of DNN clas. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2.\">MODEL</head><p>Our sequence-to-sequence model is an encoder-decoder architecture with attention <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ere has been a surge of interest in learning graph representations from data. For example, DeepWalk <ref type=\"bibr\" target=\"#b19\">[20]</ref>, one recent model, transforms a graph structure into a sam thod, which used stochastic gradient descent to optimize matrices from large graphs. Perozzi et al. <ref type=\"bibr\" target=\"#b19\">[20]</ref> presented an approach, which transformed graph structure i ep relational information and tuning the threshold of maximum number of vertices.</p><p>2. DeepWalk <ref type=\"bibr\" target=\"#b19\">[20]</ref>. DeepWalk is a method that learns the representation of so uction strategy for vertices with small degrees to achieve the optimal performance. As mentioned in <ref type=\"bibr\" target=\"#b19\">[20]</ref>, for DeepWalk and E-SGNS, we set window size as 10, walk l lti-label classification task by regarding the learned representations as features.</p><p>Following <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, we use the LibLin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y bandwidth. While a  GPU's optimized memory interconnects can achieve a high bandwidth of 112 GB/s <ref type=\"bibr\" target=\"#b15\">[16]</ref>, Xilinx ZC706 board <ref type=\"bibr\" target=\"#b23\">[24]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  spans and linking these nodes with confidenceweighted relation types and coreferences. Other works <ref type=\"bibr\" target=\"#b29\">(Muis and Lu, 2017;</ref><ref type=\"bibr\" target=\"#b43\">Sohrab and Mi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  learning methods have been proposed to address this problem <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. However, they either do not apply to bipartite graphs or a  target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> try to utilize GCN to do unsupervised learning on graphs by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  limitations of deep SISR. SISR performance was boosted right after the non-local attention modules <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" targ   <ref type=\"bibr\" target=\"#b19\">[20]</ref>, RNAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> and SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, incorporate non-local operation into their networks in orde o the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from <ref type=\"bibr\" target=\"#b1\">[2]</ref> and a deconvolution layer for upscaling the module outputs.  ution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we divide the feature maps into region grids, where the int N <ref type=\"bibr\" target=\"#b17\">[18]</ref>, OISR <ref type=\"bibr\" target=\"#b11\">[12]</ref> and SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>Quantitative Evaluations In Table <ref type=\"table\" t f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours Urban100 (4\u00d7): img 078 HR Bicubic LapSRN <ref type=\"bib f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours Urban100 (4\u00d7): img 047 HR Bicubic LapSRN <ref type=\"bib f> RDN <ref type=\"bibr\" target=\"#b38\">[39]</ref> RCAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> SAN <ref type=\"bibr\" target=\"#b1\">[2]</ref> Ours which only needs 20% parameters of RCAN and SAN, but ac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b20\">21]</ref>. A DCNN based brain tumor segmentation and detection method was proposed in <ref type=\"bibr\" target=\"#b21\">[22]</ref>.</p><p>From an architectural point of view, the CNN model . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rk. This is inspired by the idea of pre-training and fine-tuning in the literature of deep learning <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>In joint training, all three types of networks are us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  in areas where large scale training corpora are available <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. Some of the lack of positive results from neural models in ts on a variety of tasks such as speech recognition, visual object recognition and object detection <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>This paper learns a text representation end-to-end . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eled through iterative sparse matrix vector multiplication <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. In each iteration, the system traverses all active vertice stems that explicitly maintain states for subgraph patterns <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" tar 6,</ref><ref type=\"bibr\" target=\"#b50\">51,</ref><ref type=\"bibr\" target=\"#b54\">55]</ref>. Arabesque <ref type=\"bibr\" target=\"#b45\">[46]</ref> is the first distributed system that proposes the \"think l .tei-c.org/ns/1.0\"><head n=\"9\">Related Work</head><p>Graph mining systems and algorithms. Arabesque <ref type=\"bibr\" target=\"#b45\">[46]</ref> built on Giraph <ref type=\"bibr\" target=\"#b0\">[1]</ref> is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: data sources. We use natural language processing methods, such as Latent Dirichlet Allocation (LDA) <ref type=\"bibr\" target=\"#b5\">[8]</ref> and topical phrase mining <ref type=\"bibr\" target=\"#b12\">[16 as their topical similarity using a process they call Phrase LDA.</p><p>Latent Dirichlet Allocation <ref type=\"bibr\" target=\"#b5\">[8]</ref> is the most common topic modeling process and PLDA+ is a sca. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nowledge can be captured for recovering the high-frequency details in HR images.</p><p>Recent works <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> have successfully  tional neural network <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Among them, the CNN-based approaches <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> have recently set  ork, making it easy to train. In addition, in previous works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only high-level features at top layers were used in the re formance. Instead of using interpolation for upscaling as in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, recent studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref ere studied and compared in our work. As in previous methods <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only the feature maps at the top layer are used as input f  to learn an end-to-end mapping for SR. Subsequently, a deep network with 20 layers was proposed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> to improve the reconstruction accuracy of CNN. The residua on accuracy of CNN. The residuals between the HR images and the interpolated LR images were used in <ref type=\"bibr\" target=\"#b10\">[11]</ref> to speedup the converging speed in training and also to im yers</head><p>In previous SR methods such as SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> and VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, bicubic interpolation is used to upscale LR images to the plus <ref type=\"bibr\" target=\"#b23\">[24]</ref>, SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> and DRCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>. The im  <ref type=\"bibr\" target=\"#b1\">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> with 20-layer CNN. It should be mentioned that the most si  Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref> SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> DRCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>   In additi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bedding module. The module maps each word to a 128-dimensional vector. We also tried word2vec (W2V) <ref type=\"bibr\" target=\"#b16\">[17]</ref> trained on the same corpus as the word embedding module.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as short involuntary expressions, which could potentially indicate deceptive behavior. Caso et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> identified particular hand gesture to be important to iden. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b4\">[5]</ref>, however, a spectral relaxation of the problem can be solved efficiently <ref type=\"bibr\" target=\"#b36\">[37]</ref>. Let C \u2208 0, 1 n\u00d7k be the cluster assignment matrix and d b r iteration or Lanczos algorithm.</p><p>One can then obtain clusters by means of spectral bisection <ref type=\"bibr\" target=\"#b36\">[37]</ref> with iterative refinement akin to Kernighan-Lin algorithm . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y anomalous edges detection, is then highly needed before the data are fed into the following tasks <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Ranshous et a d model which is inspired by <ref type=\"bibr\" target=\"#b3\">[Liu et al., 2017]</ref> and proposed by <ref type=\"bibr\" target=\"#b1\">[Cui et al., 2017]</ref>. In our framework, we construct short state o l></formula><p>GRU is a variant of LSTM network. It is simpler and more effective than LSTM network <ref type=\"bibr\" target=\"#b1\">[Chung et al., 2014]</ref>. GRU can record long-term information, and  oss entropy to distinguish the existing edges and the generated ones. We then take the same idea in <ref type=\"bibr\" target=\"#b1\">[Bordes et al., 2013]</ref> and use marginbased pairwise loss in train ally build the required datasets because the ground-truth for the test phase is difficult to obtain <ref type=\"bibr\" target=\"#b1\">[Akoglu et al., 2015]</ref>, and we follow the approach used in <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: emantic segmentation and achieved sound semantic segmentation results. For example, Audebert et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> based on SegNet network, combined with multimodal data, ac. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mbeddings as image representations. Many of these approaches rely either on auto-encoding of images <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly work mostly deals with simple graphs with unlabeled edges, recently proposed relation-aware GNNs <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> consider multi-rel pe=\"bibr\" target=\"#b38\">[39]</ref> consider direction and relation types, respectively. Also, R-GCN <ref type=\"bibr\" target=\"#b37\">[38]</ref> considers direction and relation types simultaneously. Rec e=\"bibr\" target=\"#b44\">45]</ref>. 5) R-GCN. This is a GNN-based method for modeling relational data <ref type=\"bibr\" target=\"#b37\">[38]</ref>. 6) MEAN. 7) LAN. These are GNN models for a out-of-knowle get=\"#b14\">[15]</ref>. 3) R-GCN. The same model used in the entity prediction on KG completion task <ref type=\"bibr\" target=\"#b37\">[38]</ref>. 4) I-GEN. Inductive GEN, which only uses feature represen nds the graph convolutional network to consider multi-relational structure, by Schlichtkrull et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p><p>6) MEAN. This model computes the embedding of entit s W r and W r to prevent the excessive increase in the model size, proposed in Schlichtkrull et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref>: W r = B b=1 a r b V b , where B is the number of basis, a twork based methods <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. While they require ggested by several recent works on multi-relational graphs <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, where directed rel ne in previous works <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, we measure the ranks in a filtered setting where we do not. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  directly incorporate multi-hop neighborhood information of a node without explicit message-passing <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Intuitively, propagation based on personalized PageRank c here the node influence decays exponentially with each layer. However, as proposed, Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>'s approach does not easily scale to large graphs since it  ng that can reduce predictive performance.</p><p>To tackle both of these challenges Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> suggest decoupling the feature transformation from the pro instead. Unfortunately, even a moderate number of power iteration evaluations (e.g. Klicpera et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref> used K = 10 to achieve a good approximation) is prohibitiv ency of the subsequent learning step. Both of these approaches are a special case of the PPNP model <ref type=\"bibr\" target=\"#b32\">[33]</ref> which experimentally shows higher classification performan . In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model <ref type=\"bibr\" target=\"#b32\">[33]</ref> which we build upon. The results are summarized in Table < ich experimentally shows higher classification performance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>.</p><p>Approximating PageRank. Recent approaches combine ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.\">E2E setting</head><p>ESPnet toolkit <ref type=\"bibr\" target=\"#b20\">[21]</ref> is used to train our E2E architectures. We use 80 mel-scal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion predictions by utilizing the explicit or implicit ratings of the current user and similar users <ref type=\"bibr\" target=\"#b21\">[22]</ref>. However, in article recommendation, collaborative filteri active selection strategy is to maximize the Expected Loss for Ranking without density according to <ref type=\"bibr\" target=\"#b21\">(22)</ref>.</p><p>\u2022 POLAR++DWELO DWELO (Density-Weighted Expected Los. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b23\">(Peters et al., 2018;</ref><ref type=\"bibr\" target=\"#b24\">Radford et al., 2018;</ref><ref type=\"bibr\" target=\"#b5\">Devlin et al., 2019)</ref>, which has improved performances on various ngth of n by the same WordPiece tokenizer <ref type=\"bibr\" target=\"#b35\">(Wu et al., 2016)</ref> in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>. Next, as shown in Fig. <ref type=\"figure\"> s generated by the cross-modality encoder. For the cross-modality output, following the practice in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>, we append a special token [CLS] (denoted a  and each of them only focuses on a single modality (i.e., language or vision). Different from BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, which applies the transformer encoder onl om branch of Fig. <ref type=\"figure\" target=\"#fig_0\">2</ref>, the task setup is almost same to BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>: words are randomly masked with a probabil n image and a sentence match each other. This task is similar to 'Next Sentence Prediction' in BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>.</p><p>Image Question Answering (QA) In or  by the WordPiece tokenizer <ref type=\"bibr\" target=\"#b35\">(Wu et al., 2016)</ref> provided in BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> image to maximize the pre-training compute s. We take Adam (Kingma and Ba, 2014) as the optimizer with a linear-decayed learning-rate schedule <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> and a peak learning rate at 1e \u2212 4. We tra .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">BERT versus LXMERT</head><p>BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019</ref>) is a pre-trained language encoder which im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b32\">[33]</ref> bypass signal from one layer to the next via identity connections. Stochastic depth <ref type=\"bibr\" target=\"#b12\">[13]</ref> shortens ResNets by randomly dropping layers during traini ation preservation explicit through additive identity transformations. Recent variations of ResNets <ref type=\"bibr\" target=\"#b12\">[13]</ref> show that many layers contribute very little and can in fa ]</ref>. Recently, stochastic depth was proposed as a way to successfully train a 1202-layer ResNet <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Stochastic depth improves the training of deep residual n tion between dense convolutional net-works and stochastic depth regularization of residual networks <ref type=\"bibr\" target=\"#b12\">[13]</ref>. In stochastic depth, layers in residual networks are rand oring/shifting) that is widely used for these two datasets <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar 31 images for additional training. Following common practice <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar st time. Following <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, we report classification errors on the validation set.</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s.</p><p>The idea of extracting features for NLP using convolutional DNN was previously explored by <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>, in the context of POS tagging, chunking  Recognition (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. In <ref type=\"bibr\" target=\"#b3\">(Collo tation component, each input word token is transformed into a vector by looking up word embeddings. <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> reported that word embeddings learned fr ural network, the convolution approach is a natural method to merge all of the features. Similar to <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>, we first process the output of Window P , we heuristically choose d e = 5. Finally, the word dimension and learning rate are the same as in <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. Table <ref type=\"table\">2 reports</ref> ons.org/licenses/by/4.0/ 1 A word embedding is a distributed representation for a word. For example,<ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> use a 50-dimensional vector to represent -words model</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"3\" xml:id=\"foot_1\"><ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref> proposed a pairwise ranking approach to  ares similar intuition with that of <ref type=\"bibr\" target=\"#b3\">Collobert et al. (2011)</ref>. In <ref type=\"bibr\" target=\"#b3\">(Collobert et al., 2011)</ref>, all of the tasks are considered as the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e to the space limit, we will skip the description of the basic chain LSTM and readers can refer to <ref type=\"bibr\" target=\"#b11\">Hochreiter and Schmidhuber (1997)</ref> for details. Briefly, when mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed learning in recent years in the image and video domains <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar ntrastive learning <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" tar just one positive pair, selected using either co-occurence <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref> or using data augme rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>), the loss takes the following form.</p><formula xml:id=\"fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b13\">Yang et al. (2017)</ref>  <ref type=\"bibr\" target=\"#b13\">[14]</ref> and <ref type=\"bibr\" target=\"#b12\">Wang et al. (2016)</ref>  <ref type=\"bibr\" target=\"#b12\">[13]</ref> e f type=\"bibr\" target=\"#b13\">[14]</ref> and <ref type=\"bibr\" target=\"#b12\">Wang et al. (2016)</ref>  <ref type=\"bibr\" target=\"#b12\">[13]</ref> employ a two-stage approach,</p><formula xml:id=\"formula_3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion along the z-axis cannot be aggregated and taken into consideration. However, there is evidence <ref type=\"bibr\" target=\"#b12\">[13]</ref> that conventional 3D segmentation methods deteriorate in p tion of the variant proposed in <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Based on past experience <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> we favor this formul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\">(PBMT, Wubben et al., 2012;</ref><ref type=\"bibr\" target=\"#b21\">Narayan and Gardent, 2014;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2016)</ref> or neural machine translation (NMT, <ref type= olingual phrase-based machine translation <ref type=\"bibr\" target=\"#b36\">(Wubben et al., 2012;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2016)</ref>. Further, syntactic information was also consi al., 2012)</ref>, which re-ranks sentences generated by PBMT for diverse simplifications; SBMT-SARI <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref>, which uses an external paraphrasing database omatic evaluation on the Newsela and WikiLarge datasets, respectively.</p><p>We use the SARI metric <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref> to measure the simplicity of the generated se s not improve performance, as it is known that WikiLarge does not focus on syntactic simplification <ref type=\"bibr\" target=\"#b38\">(Xu et al., 2016)</ref>. The best performance for this experiment is  =\"#b24\">(Papineni et al., 2002)</ref> to measure the closeness between a candidate and a reference. <ref type=\"bibr\" target=\"#b38\">Xu et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b32\">Sulem et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng works has paid special attention to embed bipartite networks. While a recent work by Dong et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed metapath2vec++ for embedding heterogeneous networ ght be suboptimal for learning vertex representations for a bipartite network.</p><p>Metapath2vec++ <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HNE <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EOE <re  We assign a probability to stop a random walk in each step. In contrast to DeepWalk and other work <ref type=\"bibr\" target=\"#b13\">[14]</ref> that apply a fixed length on the random walk, we allow the  hyper-parameters p and q are set to 0.5 which has empirically shown good results. \u2022 Metapath2vec++ <ref type=\"bibr\" target=\"#b13\">[14]</ref>: This is the state-of-the-art method for embedding heterog. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  are done during internship at Microsoft   <ref type=\"bibr\" target=\"#b7\">(Berard et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Bansal et al., 2019)</ref>, where they leverage the available ASR and  bibr\" target=\"#b38\">Weiss et al., 2017;</ref><ref type=\"bibr\" target=\"#b3\">Bansal et al., 2018</ref><ref type=\"bibr\" target=\"#b4\">Bansal et al., , 2019;;</ref><ref type=\"bibr\" target=\"#b33\">Sperber et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: phs. To address this issue, different sampling-based methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>Recently, graph embedding-based methods, which lear. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n this work, we focus on three of these proposed schemes -the first based on working set signatures <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the second based n lead to unpredictable, non-optimal results. Consequently, algorithms such as the ones proposed in <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref> do not perform tu ets, BBVs, and conditional branch counters. In addition to instruction working set based techniques <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11], we evaluate branch and procedure working set based tec  are defined as the set of branches/procedures touched over the sampling interval. In previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11], we defined a similarity metric called the relative wor  and working sets are too large to be efficiently stored and compared in hardware. In previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>, we proposed a ha e</head><p>A working set signature is a lossy-compressed representation of the complete working set <ref type=\"bibr\" target=\"#b9\">[10]</ref> <ref type=\"bibr\" target=\"#b10\">[11]</ref>. The signature is 512 bits) and accumulator tables (1024, 128, 32 entries) are similar to those used in previous work <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11] <ref type=\"bibr\" target=\"#b16\">[19]</ref>. Procedure si  be used in tuning algorithms to reuse previously found optimal configurations for recurring phases <ref type=\"bibr\" target=\"#b9\">[10]</ref>[11] <ref type=\"bibr\">[12] [19]</ref>. This eliminates a sig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t of user-level influence prediction models, most of which <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> consider complicate k, while external sources are assumed to be not present.</p><p>Problem 1. Social Influence Locality <ref type=\"bibr\" target=\"#b52\">[53]</ref> Social influence locality models the probability of v's ac get=\"#b53\">54]</ref> Weibo 6 is the most popular Chinese microblogging service. The dataset is from <ref type=\"bibr\" target=\"#b52\">[53]</ref> and can be downloaded here. 7 The complete dataset contain  and the social action is defined to be whether a user retweets \"Higgs\" related tweets.</p><p>Weibo <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref> Weibo 6 is the mos .</p><p>Data Preparation We process the above four datasets following the practice in existing work <ref type=\"bibr\" target=\"#b52\">[53,</ref><ref type=\"bibr\" target=\"#b53\">54]</ref>. More concretely, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: k] in GCN-CF model, we hope that the rank keeps [i, j, k] in the binary model. According to ListNet <ref type=\"bibr\" target=\"#b1\">[Cao et al., 2007]</ref>, we can characterize sorting information in t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch, their technique follows the general pseudo-ensemble agreement (PEA) regularization framework of <ref type=\"bibr\" target=\"#b0\">Bachman et al. (2014)</ref>. In addition, they employ a mutual exclusi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  vision <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref type=\"bibr\" target=\"#b12\">Cheng et al., 2018)</ref>, and ASR <ref type=\"bibr\" target=\"#b45\">(Sp 6)</ref> presented a general method to stabilize model predictions against small input distortions. <ref type=\"bibr\" target=\"#b12\">Cheng et al. (2018)</ref> continued their work and developed the adve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he relational graph and apply it to both tasks.</p><p>Our entity classification model, similarly to <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref>, uses softmax classifiers at each node  nction or simply a linear transformation g m (h i , h j ) = W h j with a weight matrix W such as in <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref>. This type of transformation has been s tion. While we only consider such a featureless approach in this work, we note that it was shown in <ref type=\"bibr\" target=\"#b16\">Kipf and Welling (2017)</ref> that it is possible for this class of m that operate on local graph neighborhoods <ref type=\"bibr\" target=\"#b8\">(Duvenaud et al. 2015;</ref><ref type=\"bibr\" target=\"#b16\">Kipf and Welling 2017)</ref> to large-scale relational data. These an d et al. 2015;</ref><ref type=\"bibr\" target=\"#b6\">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type=\"bibr\" target=\"#b16\">Kipf and Welling 2017)</ref> for large-scale and highly multi-relatio <ref type=\"bibr\" target=\"#b8\">(Duvenaud et al. 2015)</ref> and graph-based semi-supervised learning <ref type=\"bibr\" target=\"#b16\">(Kipf and Welling 2017)</ref>.</p><p>Motivated by these architectures. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: efully consider where to place cuts so as to minimize disruptions to the overall audio-visual flow. <ref type=\"bibr\" target=\"#b2\">Berthouzoz et al. [2012]</ref> introduce a text-based approach for edi g-head videos, such as those introduced by moving or deleting words. It is based on the approach of <ref type=\"bibr\" target=\"#b2\">Berthouzoz et al. [2012]</ref>, requires the performer to be relativel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b7\">[9]</ref>, secondary spectrum access <ref type=\"bibr\" target=\"#b14\">[16]</ref>, grid computing <ref type=\"bibr\" target=\"#b17\">[19]</ref>, or Internet data <ref type=\"bibr\" target=\"#b22\">[24]</ref int ensures that the cost is lower than that of running the job on an on-demand instance. Comparing <ref type=\"bibr\" target=\"#b17\">(19)</ref> to bidding for a single persistent request in <ref type=\"b o bidding for a single persistent request in <ref type=\"bibr\" target=\"#b13\">(15)</ref>, we see that <ref type=\"bibr\" target=\"#b17\">(19)</ref> can be solved similarly to <ref type=\"bibr\" target=\"#b13\">  for a one-time single instance request (Proposition 4) and for multiple persistent requests (as in <ref type=\"bibr\" target=\"#b17\">(19)</ref>). The first constraint is satisfied if the user submits su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Most importantly, there are fast approximations for both PPR <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b75\">76]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b33\">[34]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y perturbed inputs that fool classifiers <ref type=\"bibr\" target=\"#b27\">(Szegedy et al., 2013;</ref><ref type=\"bibr\" target=\"#b2\">Biggio et al., 2013)</ref>. These adversarial examples can potentially. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and natural language processing <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>The novelty of Caser is to represent the previous L where d is the number of latent dimensions and the rows preserve the order of the items. Similar to <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we regard this embedding matrix as the \"image\" of the L i r\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and natural language processing <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref  sing <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Borrows the idea of using CNN in text classi cation <ref type=\"bibr\" target=\"#b11\">[12]</ref>, our approach regards the L \u00d7 d matrix E as the \"image\" of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion system to utilize the entire MEDLINE data set. By using state-of-the-art tools, such as ToPMine <ref type=\"bibr\" target=\"#b12\">[16]</ref> and FastText <ref type=\"bibr\" target=\"#b6\">[9]</ref>, we a Latent Dirichlet Allocation (LDA) <ref type=\"bibr\" target=\"#b5\">[8]</ref> and topical phrase mining <ref type=\"bibr\" target=\"#b12\">[16]</ref>, along with other data mining techniques to conceptually l word phrases from that corpus such as \"asthma a ack,\" allowing us to treat phrases as single tokens <ref type=\"bibr\" target=\"#b12\">[16]</ref>. Next, we send the corpus through FastText, the most recen urned to the UMLS SPECIALIST NLP toolset <ref type=\"bibr\" target=\"#b0\">[1]</ref> as well as ToPMine <ref type=\"bibr\" target=\"#b12\">[16]</ref> and FastText <ref type=\"bibr\" target=\"#b6\">[9,</ref><ref t o . It is also important to note that we modify the version of ToP-Mine distributed by El-Kishky in <ref type=\"bibr\" target=\"#b12\">[16]</ref> to allow phrases containing numbers, such as gene names li  over weigh a specialized language. However, phrase mining approaches that recover n-grams, such as <ref type=\"bibr\" target=\"#b12\">[16]</ref>, produce accurate methods without limiting the dictionary.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >1</ref> depicts an example of 3-shot link prediction in KGs.</p><p>To do few-shot link prediction, <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref> made the first trial and proposed GMatching ed parameters, it's like \"a gradient through a gradient\".</p><p>As far as we know, work proposed by <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref> is the first research on few-shot learning  and Evaluation Metrics</head><p>We use two datasets, NELL-One and Wiki-One which are constructed by <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref>. NELL-One and Wiki-One are derived from NEL  simple TransE embedding model, denoted as -g -r. The result under the third setting is copied from <ref type=\"bibr\" target=\"#b22\">Xiong et al. (2018)</ref>. It uses the triples from background graph, e transferring relation meta to incomplete triples during prediction.</p><p>Compared with GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018</ref>) which relies on a background knowledge gra e heavily rely on rich training instances <ref type=\"bibr\" target=\"#b26\">(Zhang et al., 2019b;</ref><ref type=\"bibr\" target=\"#b22\">Xiong et al., 2018)</ref>, thus are limited to do few-shot link predi r\" target=\"#b17\">Vinyals et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Snell et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Xiong et al., 2018)</ref>, which tries to learn a matching metric bet f> is a typical method using symmetric twin networks to compute the metric of two inputs. GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018)</ref>, the first trial on one-shot link predicti own in Table <ref type=\"table\" target=\"#tab_6\">4</ref>. The baseline in our experiment is GMatching <ref type=\"bibr\" target=\"#b22\">(Xiong et al., 2018)</ref>, which made the first trial on few-shot li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: at compression removes high frequency information, JPEG compression has been proposed several times <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ffective. As an alternative, there are retrieval methods such as the phrase-based translation model <ref type=\"bibr\" target=\"#b4\">[5]</ref> that directly model phrases (or word n-grams), but they ofte mercial search engine. On average, each query is associated with 65 Web documents (URLs). Following <ref type=\"bibr\" target=\"#b4\">[5]</ref>, we only used the title field of a Web document for ranking.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b8\">[9]</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which is a standard multimodal annotation scheme for interp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ction. While the first three strategies have been taken into account in existing performance models <ref type=\"bibr\" target=\"#b10\">[10]</ref>- <ref type=\"bibr\" target=\"#b12\">[12]</ref>, to the best of  of developing the vacation time as the whole succession of switch-over and processing times (as in <ref type=\"bibr\" target=\"#b10\">[10]</ref>), <ref type=\"bibr\" target=\"#b12\">[12]</ref> keeps in the v. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /p><p>In our experiments, we use two different image datasets: the MNIST handwritten digits dataset <ref type=\"bibr\" target=\"#b10\">(LeCun et al., 1998)</ref> and the Fashion-MNIST (F-MNIST) clothing a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b2\">Baydin &amp; Pearlmutter (2014)</ref>, and applied to small problems by <ref type=\"bibr\" target=\"#b12\">Domke (2012)</ref>. However, the na\u00efve approach fails for real-sized  s=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.\">Related work</head><p>The most closely-related work is <ref type=\"bibr\" target=\"#b12\">Domke (2012)</ref>, who derived algorithms to compute reverse-mode de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion in the rest of the text. The training was performed on a large-scale, distributed architecture <ref type=\"bibr\" target=\"#b1\">(Dean et al., 2012)</ref>, using 5 concurrent steps on each of 10 mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al with uneven degree distribution.</p><p>Graph embeddings <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b58\">59</ref>] can be thought of a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rmance degradation as we will show later. Schlichtkrull et al. also propose using GNNs to model KGs <ref type=\"bibr\" target=\"#b16\">[17]</ref>, but not for the purpose of recommendations.</p></div> <di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and processor resources they consume, the RT-CORBA specification defines a server thread pool model <ref type=\"bibr\" target=\"#b7\">[8]</ref>. There are two types of thread pools in RT-CORBA:</p><p>Thre 12]</ref> and pluggable protocol <ref type=\"bibr\" target=\"#b13\">[14]</ref> integration, synchronous <ref type=\"bibr\" target=\"#b7\">[8]</ref> and asynchronous <ref type=\"bibr\" target=\"#b14\">[15]</ref> O. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ies were integrated in order to find a combination of multimodal features with superior performance <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. A multimodal decept. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b10\">[11]</ref>. Image priors such as edge features <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref>, statistics <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  cases. In recent years, unsupervised learning has received increasing attention from the community <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Our novel appro  metric in an unsupervised fashion, without any human annotations.</p><p>Exemplar CNN. Exemplar CNN <ref type=\"bibr\" target=\"#b4\">[5]</ref> appears similar to our work. The fundamental difference is t tecture <ref type=\"bibr\" target=\"#b17\">[18]</ref> in their original papers, except for exemplar CNN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, whose results are reported with ResNet-101 <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: at uses both deep features and hand-crafted features to perform semantic labeling of aerial images. <ref type=\"bibr\" target=\"#b47\">Yuan (2017)</ref> designed a deep convolutional network that integrat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: entional video description task that generates a human-like sentence to describe the video contents <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref>, GVD has advantages of modelling the video b mance Comparisons</head><p>We compare HAST-Graph2Seq with the SOTA models, i.e., Masked Transformer <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=\"bibr\" target=\" i.e., Masked Transformer <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=\"bibr\" target=\"#b8\">[Z. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  optimizing pipeline parallelism for synchronous training <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>. This approach requires necessary gradients synchronizatio llelism.</p><p>Pipeline parallelism. Pipeline Parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re-ranking model's effectiveness and its efficiency. While IR-specific networks are reasonably fast <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ rms in a single interaction match matrix, followed by softhistogram scoring based on kernel-pooling <ref type=\"bibr\" target=\"#b35\">[36]</ref>. This allows us to explain scoring reasons by probing the   of a hard histogram method and the resulting lack of fine-tuned word representations. Xiong et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> improve on the idea and propose the kernel-pooling techniq  qi, dj)<label>(4)</label></formula><p>Then, we transform each entry in M with a set of RBF-kernels <ref type=\"bibr\" target=\"#b35\">[36]</ref>. Each kernel focuses on a specific similarity range with c  similarity range with center \u00b5 k . The size of all ranges is set by \u03c3. In contrast to Xiong et al. <ref type=\"bibr\" target=\"#b35\">[36]</ref> we do not employ an exact match kernel -as contextualized  alysis unfeasible.</p><p>The differences of TK to previous kernel-pooling methods are:</p><p>\u2022 KNRM <ref type=\"bibr\" target=\"#b35\">[36]</ref> uses only word embeddings, therefore a match does not have  improves the robustness of PACRR's pooling strategy with randomization during training.</p><p>KNRM <ref type=\"bibr\" target=\"#b35\">[36]</ref> uses a soft-histogram (differentiable Gaussian kernel func. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o-rank (LTR) is challenging due to its biased nature. To address this bias problem, Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref> proposed a counterfactual inference approach, providing an f type=\"bibr\" target=\"#b18\">[18]</ref>.</p><p>To handle biases in a principled way, Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref> introduced an unbiased learning-to-rank framework, which i ecting the examination bias in learning to rank from implicit feedback. As shown by Joachims et al. <ref type=\"bibr\" target=\"#b19\">[19]</ref>, the parameters of the PBM can serve as propensity estimat ent d for query q.</p><p>While Pr(E = 1|k) can be used as an estimate of the examination propensity <ref type=\"bibr\" target=\"#b19\">[19]</ref>, it is a rather simplistic model since it assumes that exa max ]. In this case, randomly swapping results at positions k and k \u2032 before presenting the ranking <ref type=\"bibr\" target=\"#b19\">[19]</ref> makes the expected relevance of results at the two positio ck-through rates is a consistent estimator of the relative propensities p k and p k \u2032 under the PBM <ref type=\"bibr\" target=\"#b19\">[19]</ref>. Note that knowing the relative propensities with respect   sufficient, since the counterfactual ERM learning objective is invariant to multiplicative scaling <ref type=\"bibr\" target=\"#b19\">[19]</ref>.</p><p>While this ratio estimator is a sensible approach f interventions were then used to get a gold-standard estimate of the propensities via the methods in <ref type=\"bibr\" target=\"#b19\">[19]</ref>. To avoid any confounding due to changes in the query dist  <ref type=\"table\" target=\"#tab_0\">1</ref>. We then use the gold-standard propensity estimator from <ref type=\"bibr\" target=\"#b19\">[19]</ref> to learn two PBM models from the swap intervention data, o be expected, given that AllPairs makes more efficient use of the data than the ratio-estimates from <ref type=\"bibr\" target=\"#b19\">[19]</ref>.</p><p>Can AllPairs learn CPBM models with many context fe nce compared to using the propensities from the PBM.</p><p>We trained a Clipped Propensity SVM-Rank <ref type=\"bibr\" target=\"#b19\">[19]</ref> for each of the following three propensity models: PBM est ed via cross-validation. For rank r &gt; 21, we impute the propensity p r (x) = p 21 (x). Following <ref type=\"bibr\" target=\"#b19\">[19]</ref>, we measure test-set ranking performance via the average s imitations of existing propensity estimation methods for LTR <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b27\">27]</ref>. First, existing me or LTR algorithms like <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b19\">19]</ref>. We evaluate the fidelity of the CPBM model and the effecti  <ref type=\"bibr\" target=\"#b23\">[23]</ref>. The most effective methods use randomized interventions <ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref>, which unfortunate first review how explicit interventions have been used for estimating p k := Pr(E = 1|k) in the PBM <ref type=\"bibr\" target=\"#b19\">[19,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref>. The PBM requires . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ons of deep clustering <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, deep attractor networks <ref type=\"bibr\" target=\"#b3\">[4,</r et=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. However, this usually only leads to small improvements, even proach for supervised speech separation is via T-F masking <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. The proposed approach is expected to produce even better sep  reconstruction, it is necessary to first obtain a good enough magnitude estimate. Our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a novel multi-task learning approach combining the  gs:</p><formula xml:id=\"formula_0\">LDC,classic = V V T \u2212 Y Y T 2 F (1)</formula><p>Our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> suggests that an alternative loss function, which whitens th  be discussed in Section 3.4. Following <ref type=\"bibr\" target=\"#b21\">[22]</ref>, our recent study <ref type=\"bibr\" target=\"#b2\">[3]</ref> proposed a chimera++ network combining the two approaches vi \" target=\"#b13\">[14]</ref> only performs iterative reconstruction for each source independently. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref ty es remain fixed during iterations, while the phase of each source are iteratively reconstructed. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, the phase reconstruction was only added as a post-processin tained when applying five iterations of Griffin-Lim on each source independently, as is reported in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Performing end-to-end optimization using LWA improves the r ates directly in the time domain. Our result is 1.1 dB better than the previous state-of-the-art by <ref type=\"bibr\" target=\"#b2\">[3]</ref> in terms of both SI-SDR and SDR.</p></div> <div xmlns=\"http:. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  works have been done as the defense for adversarial images on convolutional neural networks (e.g., <ref type=\"bibr\" target=\"#b9\">[Xu et al., 2018;</ref><ref type=\"bibr\">Papernot and McDaniel, 2018]</ target=\"#b8\">[Xie et al., 2018]</ref>. Other forms of input pre-processing, such as local smoothing <ref type=\"bibr\" target=\"#b9\">[Xu et al., 2018]</ref> and image compression <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  nodes in GCN are inclined to converge to a certain value and thus become indistinguishable. ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref> solves a similar problem in computer vision w he -th weight matrix W ( ) . Initial residual connection. To simulate the skip connection in ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref>, <ref type=\"bibr\" target=\"#b16\">(Kipf &amp; W ations for introducing identity mapping into our model.</p><p>\u2022 Similar to the motivation of ResNet <ref type=\"bibr\" target=\"#b13\">(He et al., 2016)</ref>, identity mapping ensures that a deep GCNII m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph embedding methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> to learn the embedding of each item, dubbed Base Graph Embe . Experiments are conducted to compare four methods: BGE, LINE, GES, and EGES. LINE was proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which captures the first-order and second-order proximity. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: kolov et al., 2013)</ref>, ELMo <ref type=\"bibr\" target=\"#b21\">(Peters et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b3\">(Devlin et al., 2019)</ref> are trained and tested mainly on datasets  )</ref> uses machine translation to embed context information into word representations.</p><p>BERT <ref type=\"bibr\" target=\"#b3\">(Devlin et al., 2019)</ref> is a contextualized word representation mo r\" target=\"#b10\">(Krallinger et al., 2017)</ref>. Due to the space limitations, we refer readers to <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref> for a more detailed description of BERT.</p pora were used for pre-training, we initialized BioBERT with the pre-trained BERT model provided by <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref>. We define BioBERT as a language representa han an hour as the size of the training data is much smaller than that of the training data used by <ref type=\"bibr\" target=\"#b3\">Devlin et al. (2019)</ref>. On the other hand, it takes more than 20 e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dels of behavior prediction, to suggest the next resource a learner is likely to spend time on next <ref type=\"bibr\" target=\"#b23\">[24]</ref>. In the context of higher education, they have been used i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar algorithm design, our work is most closely related to Hamilton et al. (2017a)'s GraphSAGE algorithm <ref type=\"bibr\" target=\"#b17\">[18]</ref> and the closely related follow-up work of <ref type=\"bibr\"  nodes to aggregate from allows us to control the memory footprint of the algorithm during training <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Second, it allows Algorithm 1 to take into account the im ean); \u2022 mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. \u2022 mean-pooling-hard is the same as mean-pooling, except t ing and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. <ref type=\"bibr\" target=\"#b17\">[18]</ref>-other variants (e.g., based on Kipf et al. <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type=\"bibr\" target=\"#b21\">Shi et al. (2016)</ref> replace convolution filters with deep LSTM, w of recurrent units. We find that using GRU as recurrent units outperforms LSTM which is utilized by <ref type=\"bibr\" target=\"#b21\">Shi et al. (2016)</ref>.</p><formula xml:id=\"formula_0\">w 1 w 2 w 3 w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, is an approach to improve the robustness of the target net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is set to 1, since we did not observe performance gain in increasing layers. We use Adam optimizer <ref type=\"bibr\" target=\"#b15\">[16]</ref> with a learning rate 0.001. In training, the batch size is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ments for fair comparison. Throughout the experiments in this subsection, we use the Adam optimizer <ref type=\"bibr\" target=\"#b17\">[17]</ref> with learning rate 0.005. An early stopping strategy with  o and Freesolv, the mean squared error loss is employed. The networks are trained by Adam optimizer <ref type=\"bibr\" target=\"#b17\">[17]</ref> with learning rate 0.0005. An early stopping strategy with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >, as well as independent component analysis <ref type=\"bibr\" target=\"#b3\">[Cao et al. 2003]</ref>. <ref type=\"bibr\" target=\"#b4\">Cao et al. [2005]</ref> extract emotions using support vector machines lso present a user study rating the level of realism in emotion synthesis, covering several methods <ref type=\"bibr\" target=\"#b4\">[Cao et al. 2005;</ref><ref type=\"bibr\" target=\"#b27\">Liu and Osterman ing samples based on the apparent emotion <ref type=\"bibr\" target=\"#b0\">[Anderson et al. 2013;</ref><ref type=\"bibr\" target=\"#b4\">Cao et al. 2005;</ref><ref type=\"bibr\" target=\"#b11\">Deng et al. 2006;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Large-scale knowledge graphs <ref type=\"bibr\" target=\"#b29\">(Suchanek et al., 2007;</ref><ref type=\"bibr\" target=\"#b34\">Vrande\u010di\u0107. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pretrain the word embeddings -they are learned from scratch during training. We train using Adagrad <ref type=\"bibr\" target=\"#b5\">(Duchi et al., 2011)</ref> with learning rate 0.15 and an initial accu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Simonyan &amp; Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b44\">Srivastava et al., 2015;</ref><ref type=\"bibr\" target=\"#b18\">He et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Huang et al., 20 et (left) <ref type=\"bibr\" target=\"#b34\">(LeCun et al., 1998)</ref> with a 110-layer ResNet (right) <ref type=\"bibr\" target=\"#b18\">(He et al., 2016)</ref> on the CIFAR-100 dataset. The top row shows t e normalization techniques have enabled the development of very deep architectures, such as ResNets <ref type=\"bibr\" target=\"#b18\">(He et al., 2016)</ref> and DenseNets <ref type=\"bibr\" target=\"#b22\"> he past few years.</p><p>It is now common to see networks with hundreds, if not thousands of layers <ref type=\"bibr\" target=\"#b18\">(He et al., 2016;</ref><ref type=\"bibr\" target=\"#b21\">Huang et al., 2 ageNet models of 2015 all use an order of magnitude less weight decay than models of previous years <ref type=\"bibr\" target=\"#b18\">(He et al., 2016;</ref><ref type=\"bibr\" target=\"#b41\">Simonyan &amp; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p>Many recent sequence labeling frameworks <ref type=\"bibr\" target=\"#b25\">(Ma and Hovy, 2016b;</ref><ref type=\"bibr\" target=\"#b27\">Misawa et al., 2017)</ref> share a very basic structure: a bidirectio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esults. Our work is partly inspired by the works on generating and refining score maps. Yang et al. <ref type=\"bibr\" target=\"#b42\">[43]</ref> adopts pyramid features as inputs of the network in the pr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s h a 1 _ b a s e 6 4 = \" e A Z 8 7  This paper presents progress in diffusion probabilistic models <ref type=\"bibr\" target=\"#b49\">[50]</ref>. A diffusion probabilistic model (which we will call a \"di onals in p \u03b8 (x t\u22121 |x t ), because both processes have the same functional form when \u03b2 t are small <ref type=\"bibr\" target=\"#b49\">[50]</ref>. A notable property of the forward process is that it admi ing to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance <ref type=\"bibr\" target=\"#b49\">[50]</ref>.</p><p>Second, to represent the mean \u00b5 \u03b8 (x t , t), we pro orithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. <ref type=\"bibr\" target=\"#b49\">[50]</ref>, not yet as a practical compression system. </p></div> <di educed variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. <ref type=\"bibr\" target=\"#b49\">[50]</ref>; we include it here only for completeness.</p><formula xml ments so that the number of neural network evaluations needed during sampling matches previous work <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>. We set the forwar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oposed dense network layout has already been studied in the neural networks literature in the 1980s <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Their pioneering work focuses on fully connected multi-laye. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b15\">16]</ref>, node classi cation <ref type=\"bibr\" target=\"#b32\">[32]</ref>, relational mining <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and role discovery <ref type=\"bibr\" target=\"#b8\">[9]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f its neighbor pairs <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. For example, SiGMa <ref type=\"bibr\" target=\"#b8\">[9]</ref> enerate candidate user pairs from all the pairs. Following <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>, we only keep the user pairs if their names are similar to  gate the matching scores from a confidential seed set of user pairs to their neighbor pairs. COSNET <ref type=\"bibr\" target=\"#b27\">[28]</ref> proposed a supervised method to infer the marginal probabi ls of the unlabeled pairs and update the model based on the inferred labels and the user attributes <ref type=\"bibr\" target=\"#b27\">[28]</ref>. However, error propagations may be introduced in above me fore resorting to the model, we can easily select the most useful neighbor pairs by heuristic rules <ref type=\"bibr\" target=\"#b27\">[28]</ref>. This paper simply selects the neighbor pairs if their nam  by propagating the matching scores (predicted by SVM) through the two input networks.</p><p>COSNET <ref type=\"bibr\" target=\"#b27\">[28]</ref>: is a factor graph model that incorporates the attributes . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, an ADD model is non-contact and non-invasive. This indirectn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rious heuristic approaches have been proposed to improve the the robustness to adversarial examples <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, such heuristics are often broken by new attack m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b20\">(Pasupat and Liang, 2015;</ref><ref type=\"bibr\" target=\"#b31\">Yih et al., 2016;</ref><ref type=\"bibr\" target=\"#b0\">Abujabal et al., 2017)</ref>. These systems are effective but at the c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  networking APIs, by either i) modifying the internal implementation but leaving the APIs untouched <ref type=\"bibr\" target=\"#b16\">[20,</ref><ref type=\"bibr\" target=\"#b29\">33,</ref><ref type=\"bibr\" ta ion in our discussion.</p><p>Contention on Accept Queue (multi-core): As explained in previous work <ref type=\"bibr\" target=\"#b16\">[20,</ref><ref type=\"bibr\" target=\"#b29\">33]</ref>, a single listenin ickly become overloaded since those globally visible objects cause system-wide synchronization cost <ref type=\"bibr\" target=\"#b16\">[20]</ref>. In our microbenchmark, the VFS overhead for socket alloca ti-core systems since the kernel maintains the inode and dentry as globally visible data structures <ref type=\"bibr\" target=\"#b16\">[20]</ref>.</p><p>To address the above issues, we propose lightweight  and differences between Affinity-Accept <ref type=\"bibr\" target=\"#b29\">[33]</ref> and MegaPipe. In <ref type=\"bibr\" target=\"#b16\">[20]</ref>, the authors address the scalability issues in VFS, namely or linear scalability of network I/O on multi-core systems <ref type=\"bibr\" target=\"#b15\">[19,</ref><ref type=\"bibr\" target=\"#b16\">20,</ref><ref type=\"bibr\" target=\"#b29\">33,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e field, and then they can quickly identify their interests, and easily acquire desired information <ref type=\"bibr\" target=\"#b36\">[35]</ref>. A high-quality taxonomy for business reviews on Yelp<ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: el complexity. Through evaluation on simple KG-QA with and without neural networks, Mohammed et al. <ref type=\"bibr\" target=\"#b158\">[159]</ref> found that sophisticated deep models such as LSTM and ga. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d semantic relationship between two sentences is not trivial due to the problem of the semantic gap <ref type=\"bibr\" target=\"#b14\">(Liu et al. 2016)</ref>.</p><p>Recent advances of deep neural network. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6\">[7]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bib  illumination or dim illumination. Following the previous work evaluated on the Oulu-CASIA database <ref type=\"bibr\" target=\"#b12\">[13]</ref>, only the 480 videos collected by the VIS System under nor N <ref type=\"bibr\" target=\"#b6\">[7]</ref>, PPDN <ref type=\"bibr\" target=\"#b51\">[52]</ref> and DTAGN <ref type=\"bibr\" target=\"#b12\">[13]</ref>).</p><p>1) Results on CK+ dataset: The confusion matrix of ef> 96.40 7 Dynamic N/A FN2EN <ref type=\"bibr\" target=\"#b6\">[7]</ref> 96.80 8 Static 10 folds DTAGN <ref type=\"bibr\" target=\"#b12\">[13]</ref> 97.25 7 Dynamic 10 folds PPDN <ref type=\"bibr\" target=\"#b5  Dynamic 10 folds 3DCNN-DAP <ref type=\"bibr\" target=\"#b19\">[20]</ref> 63.4 6 Dynamic 20 folds DTAGN <ref type=\"bibr\" target=\"#b12\">[13]</ref> 70.24 6 Dynamic 10 folds IACNN <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  use, where possible, the free space in nodes. Other studies <ref type=\"bibr\" target=\"#b4\">[4,</ref><ref type=\"bibr\" target=\"#b12\">12]</ref> have focused on adapting the B + -tree structure to obtain . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atasets and continuously set new stateof-the-art performance <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar  \u2212 L)X is understood as features averaging and propagation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. In graph signal pr rast to the recent design principle of graph neural networks <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, our results sugges 9]</ref>. Started with the early success of ChebNet <ref type=\"bibr\" target=\"#b5\">[6]</ref> and GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> at vertex classification, many variants of GNN have been p e observe that the parameters of a graph convolutional layer in a Graph Convolutional Network (GCN) <ref type=\"bibr\" target=\"#b15\">[16]</ref> only contribute to overfitting. Similar observations have   problem and provide insights to the mechanism underlying the most commonly used baseline model GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and its simplified variant SGC <ref type=\"bibr\" target=\"# onding NNs using true features.</p><p>Theorem 7 implies that, under Assumption 1, both gfNN and GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> have similar high performance. Since gfNN does not require Network model by removing nonlinearity in the neural network and only averaging features.</p><p>GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> Graph Convolutional Neural Network ($) is the most commonl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ased document embeddings from GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> (as Doc2vec implementation <ref type=\"bibr\" target=\"#b32\"> they encode the segments with GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref> and Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> and compute their similarity to determine whether papers a ref type=\"bibr\" target=\"#b34\">35]</ref> but unable to represent entire documents. Paragraph Vectors <ref type=\"bibr\" target=\"#b22\">[23]</ref> (also known as Doc2vec), extends word2vec to learn embeddi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bination of bidirectional LSTM and CTC has been applied to characterlevel speech recognition before <ref type=\"bibr\" target=\"#b5\">(Eyben et al., 2009)</ref>, however the relatively shallow architectur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y predicted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy ibr\" target=\"#b15\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Lample et al., 2016)</ref>, using LSTM-CRF as the main network struct epresentations Both character CNN <ref type=\"bibr\" target=\"#b27\">(Ma and Hovy, 2016)</ref> and LSTM <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016)</ref> have been used for representing the chara. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  genera-tive model based on a Laplacian pyramid framework (LAP-GAN) to generate realistic images in <ref type=\"bibr\" target=\"#b8\">[6]</ref>, which is the most related to our work. However, the propose. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mentation <ref type=\"bibr\" target=\"#b32\">[33]</ref>), and deep contextual language models from BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a v  GloVe <ref type=\"bibr\" target=\"#b30\">[31]</ref>, to contextual embeddings as the ones used in BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XL-Net <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The  tations based on the Transformer architecture <ref type=\"bibr\" target=\"#b36\">[37]</ref>, named BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The t r Siamese</figDesc><table /><note>XLNet-512 (most complex Transformer architecture). As suggested in<ref type=\"bibr\" target=\"#b14\">[15]</ref>, the Transformer training is performed with batch size b =. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ssage passing, in order to capture richer topological properties. Our method draws inspiration from <ref type=\"bibr\" target=\"#b32\">[33]</ref>, where it was shown that GNNs become universal when the ve  target=\"#b35\">[36]</ref>.</p><p>It is important to note here that contrary to identifierbased GNNs <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bib rovide a unique identification of the vertices, then universality will also hold (Corollary 3.1. in <ref type=\"bibr\" target=\"#b32\">[33]</ref>).</p><p>We conjecture, that in real-world scenarios the nu ><p>Unique identifiers. From a different perspective, <ref type=\"bibr\" target=\"#b67\">[68]</ref> and <ref type=\"bibr\" target=\"#b32\">[33]</ref> showed the connections between GNNs and distributed local . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: )</ref> has been commonly used in NER task <ref type=\"bibr\" target=\"#b2\">(Castro et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">de Araujo et al., 2018;</ref><ref type=\"bibr\" target=\"#b4\">Fernandes e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tes the feature representation in each position by weighted sum the features of all other positions <ref type=\"bibr\" target=\"#b14\">[15]</ref> <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Thus, it can mo , it can model the longrange context information for semantic segmentation task. For example, DANet <ref type=\"bibr\" target=\"#b14\">[15]</ref> uses two self-attention mechanisms to model long-range con. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) <ref type=\"bibr\" target=\"#b30\">(Park et al., 2018)</ref> is a most similar model to ours, which also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: presentation learning for generic graphs (GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>). In general, GNNs recursively update each node's feature by icult. Although sampling methods, such as GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> and FastGCN <ref type=\"bibr\" target=\"#b0\">[1]</ref>), have b  type=\"bibr\" target=\"#b9\">[10]</ref>, GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We use a large-scale bipartite graph dataset from the Tence igh memory cost. Sampling methods like GraphSAGE <ref type=\"bibr\" target=\"#b5\">[6]</ref> and AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref> have been proposed to deal with this issue by reducing the n ons: GCN and MEAN aggregator. Node-wise sampling is used to address the scalability issue. \u2022 AS-GCN <ref type=\"bibr\" target=\"#b7\">[8]</ref>: This method uses adaptive sampling between each layer to de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  top-1 accuracy. It has been argued that this measure is sensitive to errors in the ImageNet labels <ref type=\"bibr\" target=\"#b30\">[31]</ref>. However, the top-5 metrics, which is more robust, tends t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b4\">[5]</ref> or heuristic methods <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref> to learn the similarity between publications. Some methods <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: comes unfeasible. A recent relaxation in language modeling <ref type=\"bibr\" target=\"#b28\">[27,</ref><ref type=\"bibr\" target=\"#b29\">28]</ref> turns the prediction problem on its head. First, instead of. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed with the supply and demand for cloud resources. Thus, unlike prior works on bidding optimization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, our model not only explicitly accounts for the interplay b old on 2017 spot data (see Figure <ref type=\"figure\" target=\"#fig_4\">3</ref>).</p><p>The authors of <ref type=\"bibr\" target=\"#b9\">[10]</ref> used a profit-maximization model to understand spot price d y explicitly considering job deadlines <ref type=\"bibr\" target=\"#b15\">[16]</ref>, cost minimization <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and task depend n-demand instances and a set B t \u2282 R + of bids from B t = |B t | spot instance requests. Many works <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr f this weak assumption has basis in both previous analyses of spot markets and other auctions (e.g. <ref type=\"bibr\" target=\"#b9\">[10]</ref> assumes bids are drawn from U[\u03c0, \u03c0]) as well in the simple  ese variables by the total number of instances, i.e. define n t = N b t instead of B t ), we follow <ref type=\"bibr\" target=\"#b9\">[10]</ref> and assume that all bids b \u2208 B t are drawn independently fr family. A better strategy would be to consider the collective behavior of the spot prices over time <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which we do in this section by accounting for their tempor bly ill-posed (it tends to \u2212\u221e as b t \u2192 0), and the fact that we have constraints on the state space <ref type=\"bibr\" target=\"#b9\">(10)</ref>. Therefore, we need to resort to algorithms that support mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ments, we form the term set T by extracting high-quality phrases from the corpus D using AutoPhrase <ref type=\"bibr\" target=\"#b27\">[26]</ref>.</p><p>\u2022 Network Structure: A heterogeneous information ne s performance is limited due to (1) the poor phrase quality compared to the state-of-the-art method <ref type=\"bibr\" target=\"#b27\">[26]</ref> and</p><p>(2) the poor term clustering results compared to  are the extracted from raw texts by the state-of-the-art distantly supervised phrase mining method <ref type=\"bibr\" target=\"#b27\">[26]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion information to the model. These position encodings can be a deterministic function of position <ref type=\"bibr\" target=\"#b7\">(Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b10\">Vaswani . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: recent literature on cloud resource allocation and pricing <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Spot Instance <ref type=\"bibr\" target=\"#b4\">[5]</ref> is a  st-step attempt to apply the auction mechanism on Amazon EC2, which was enhanced by subsequent work <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. A series of recent the concept of a supply curve <ref type=\"bibr\" target=\"#b21\">[22]</ref>, as applied by Zhang et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> in their design of an online cloud auction algorithm. The bi me lengths.</p><p>Finally, we compare our algorithm AucBS to another online auction algorithm, COCA <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The main idea of COCA is to calculate an estimated payment . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: od of a node and then leverages a heterogeneous skip-gram model to perform node embeddings. Hin2Vec <ref type=\"bibr\" target=\"#b23\">[24]</ref>: Hin2Vec is also an unweighted heterogeneous network embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 019)</ref> for recent reviews. For regular-grid graphs, they match classical convolutional networks <ref type=\"bibr\" target=\"#b17\">(LeCun et al., 1989)</ref> which by design can only approximate trans. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oss on labeled data, thus it will face the same vulnerability issue as the standard neural networks <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and 2) the additional smoothness constraint will exacerba mic regularization technique that proactively simulates the perturbations during the training phase <ref type=\"bibr\" target=\"#b13\">[14]</ref>. It has been empirically shown to be able to stabilize neu ive, and then learn over these adversarial examples by minimizing an additional regularization term <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b15\">[16]</ref>, <ref type=\"bib rding the target of the training objective. In supervised learning tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, supervised loss <ref type=\"bibr\" target=\"#b13\">[14]</ref> earning tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, supervised loss <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bib  against perturbations for a wide range of standard classification tasks such as visual recognition <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bib o obtain the closedform solution of r g i . Inspired by the linear approximation method proposed in <ref type=\"bibr\" target=\"#b13\">[14]</ref> for standard adversarial training, we also design a linear um players Approximation. For labeled nodes, r \u2032 i can be easily evaluated via linear approximation <ref type=\"bibr\" target=\"#b13\">[14]</ref>, i.e., calculating the gradient of D(f (x i , G| \u0398), \u1ef9i )  aphSGAN) in the training phase. Moreover, the results are consistent with findings in previous work <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b18\">(Schein et al., 2016;</ref><ref type=\"bibr\" target=\"#b1\">Ben-Younes et al., 2017;</ref><ref type=\"bibr\" target=\"#b30\">Yang and Hospedales, 2017)</ref>, factorizes a tensor into a core ten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ype=\"bibr\" target=\"#b9\">Daum\u00e9 III, 2009;</ref><ref type=\"bibr\" target=\"#b5\">Coke et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">Littell et al., 2017)</ref>.</p><p>In this study, we examine whether  h feature vectors from previous work based on the genetic and geographic distance between languages <ref type=\"bibr\" target=\"#b22\">(Littell et al., 2017)</ref>. Results show that the extracted represe up</head><p>Typology Database: To perform our analysis, we use the URIEL language typology database <ref type=\"bibr\" target=\"#b22\">(Littell et al., 2017)</ref>, which is a collection of binary feature not necessarily require pre-existing knowledge of the typological features in the language at hand, <ref type=\"bibr\" target=\"#b22\">Littell et al. (2017)</ref> have proposed a method for inferring typo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Usage-based pricing can affect overall demand levels, but does not even out short-term fluctuations <ref type=\"bibr\" target=\"#b11\">[13]</ref>. To manage these fluctuations in demand for a fixed amount re have a shorter expected running time. Job interruptibility. We can use the expected running time <ref type=\"bibr\" target=\"#b11\">(13)</ref> to observe the effect of the recovery time parameter, t r  s feasible at any price.</p><p>The optimal bid price. We can now multiply the expected running time <ref type=\"bibr\" target=\"#b11\">(13)</ref> with the expected spot price <ref type=\"bibr\" target=\"#b7\" o <ref type=\"bibr\" target=\"#b13\">(15)</ref>.</p><p>We now observe that the expected running time in <ref type=\"bibr\" target=\"#b11\">(13)</ref> decreases with the bid price, while the expected spot pric very, execution, and overhead times. Hence, we can extend the result for a single persistent bid in <ref type=\"bibr\" target=\"#b11\">(13)</ref> as</p><formula xml:id=\"formula_32\">M i=1 T i F \u03c0 (p) = t s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ting technology are trusted computing base and trusted chain <ref type=\"bibr\" target=\"#b4\">[4,</ref><ref type=\"bibr\" target=\"#b5\">5]</ref>,and trusted measurement is a key problem of this technology <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e minded users. Later, an analogous item-oriented approach <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> became popular. In those methods, a rating is estimated usi  make the itemoriented approach more favorable in many cases <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. In addition, item-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by powerful baseline systems, such as the Fast/Faster R-CNN <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29]</ref> and Fully Convolutional Network (FCN) <ref type=\"bibr\" targ target=\"#b14\">[9]</ref>. N is 64 for the C4 backbone (as in <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29]</ref>) and 512 for FPN (as in <ref type=\"bibr\" target=\"#b27\">[22] state-of-the-art instance segmentation results. Our method, called Mask R-CNN, extends Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref> by adding a branch for predicting segmentation masks on ea ding to RoIs on feature maps using RoIPool, leading to fast speed and better accuracy. Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref> advanced this stream by learning the attention mechanism w e of Fast/Faster R-CNN.</p><p>Faster R-CNN: We begin by briefly reviewing the Faster R-CNN detector <ref type=\"bibr\" target=\"#b34\">[29]</ref>. Faster R-CNN consists of two stages. The first stage, cal are shareable.</p><p>Inference: At test time, the proposal number is 300 for the C4 backbone (as in <ref type=\"bibr\" target=\"#b34\">[29]</ref>) and 1000 for FPN (as in <ref type=\"bibr\" target=\"#b27\">[2 hares features between the RPN and Mask R-CNN stages, following the 4-step training of Faster R-CNN <ref type=\"bibr\" target=\"#b34\">[29]</ref>. This model runs at 195ms per image on an Nvidia Tesla M40  hyper-parameters following existing Fast/Faster R-CNN work <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29,</ref><ref type=\"bibr\" target=\"#b27\">22]</ref>. Although these dec decisions were made for object detection in original papers <ref type=\"bibr\" target=\"#b14\">[9,</ref><ref type=\"bibr\" target=\"#b34\">29,</ref><ref type=\"bibr\" target=\"#b27\">22]</ref>, we found our insta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: leased our implementation of ApproxNDCG in Tensorflow in the open-source Tensorflow Ranking library <ref type=\"bibr\" target=\"#b11\">[12]</ref>. <ref type=\"foot\" target=\"#foot_0\">1</ref></p></div> <div . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  no packet-level authentication mechanism to ensure that the source addresses have not been altered <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The modification of a source IP address is referred to as \" rver inspects received packets (if any) and analyzes whether spoofing is allowed and to what extent <ref type=\"bibr\" target=\"#b1\">[2]</ref>. For every client running the software, its /24 IPv4 address tering, packets may be dropped due to reasons not related to IP spoofing such as network congestion <ref type=\"bibr\" target=\"#b1\">[2]</ref>.</p><p>In this study, we distinguish between two types of lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" t static pre-trained models <ref type=\"bibr\" target=\"#b16\">[17]</ref>. In recent work, Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> proposed Boundary Attack, which generates adversarial exam ttacks</head><p>Most related to our work is the Boundary Attack method introduced by Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Boundary Attack is an iterative algorithm based on reject s: We compare HopSkipJumpAttack with three state-of-the-art decision-based attacks: Boundary Attack <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Limited Attack <ref type=\"bibr\" target=\"#b8\">[9]</ref> an ibr\" target=\"#b5\">[6]</ref>. A version normalized by image dimension was employed by Brendel et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> for evaluating Boundary Attack. The As an alternative metr <formula xml:id=\"formula_38\">|E[\u03c6 x (x t + \u03b4 t u)]| &gt; 0,</formula><p>as we can see from Equation <ref type=\"bibr\" target=\"#b13\">(14)</ref>. To attempt to control the variance, we introduce a baseli. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> and speech generation <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> tasks. VAE has man ws the good performance of this method.</p><p>We have become aware of recent work by Akuzawa et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> which combines an autoregressive speech synthesis model wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rameters in NAOCS, correspondingly. The network is based on two modules adapted from the PointNet++ <ref type=\"bibr\" target=\"#b20\">[21]</ref> segmentation architecture. The part segmentation head pred. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e ensemble approaches <ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref><ref type=\"bibr\" target=\"#b9\">[10]</ref> fused different text features and achieved promising result. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  k with d \u2032 input channels and d \u2032\u2032 output channels, the total number of multiply add required is h <ref type=\"bibr\" target=\"#b0\">(1)</ref> this expression has an extra term, as indeed we have an extr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">20,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" tar arget=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b35\">37,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b19\">21,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" tar pping, only map rich representations of the input to  <ref type=\"bibr\" target=\"#b5\">[6]</ref>, VDSR <ref type=\"bibr\" target=\"#b19\">[21]</ref>, DRRN <ref type=\"bibr\" target=\"#b40\">[42]</ref>) commonly  e-of-Figure <ref type=\"figure\">6</ref>. The depth analysis of DBPNs compare to other networks (VDSR <ref type=\"bibr\" target=\"#b19\">[21]</ref>, DRCN <ref type=\"bibr\" target=\"#b20\">[22]</ref>, DRRN <ref SRCNN <ref type=\"bibr\" target=\"#b5\">[6]</ref>, FSRCNN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, VDSR <ref type=\"bibr\" target=\"#b19\">[21]</ref>,    DRCN <ref type=\"bibr\" target=\"#b20\">[22]</ref>, DRRN < Deep Network SR. (a) Predefined upsampling (e.g., SRCNN<ref type=\"bibr\" target=\"#b5\">[6]</ref>, VDSR<ref type=\"bibr\" target=\"#b19\">[21]</ref>, DRRN<ref type=\"bibr\" target=\"#b40\">[42]</ref>) commonly u  mapping with simple convolutional layers. Later, the improved networks exploited residual learning <ref type=\"bibr\" target=\"#b19\">[21,</ref><ref type=\"bibr\" target=\"#b40\">42]</ref> and recursive laye. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the abbreviation, as a short form of text, is prone to ambiguity. Word sense disambiguation methods <ref type=\"bibr\" target=\"#b22\">[23]</ref> have been studied to disambiguate word senses, however, de be expensive and the collected datasets are normally small in size.</p><p>Word sense disambiguation <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a type of technique used to distinguish ambiguous word  </p><p>Inspired by word sense disambiguation methods that label super sense types for word clusters <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we jointly predict the types for abbreviation candidates . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mprove results because the tasks influence each other through a shared representation (the ConvNet) <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Does multi-task training improve object detection accuracy . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e rank range from 1 to K. As pointed out above, this scheme pre-determines the weight. Rendle et al <ref type=\"bibr\" target=\"#b28\">[29]</ref> proposed an empirical weight for sampling a single positio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Radford et al. (2018)</ref> demonstrate a pre-trained generative model (GPT) and its effects, while <ref type=\"bibr\" target=\"#b10\">Devlin et al. (2019b)</ref> release a pre-trained deep Bidirectional  entation from Transformers (BERT), achieving state-of-the-arts on dozens of benchmarks.</p><p>After <ref type=\"bibr\" target=\"#b10\">Devlin et al. (2019b)</ref>, similar pre-trained encoders spring up r  many alternatives for pre-trained language representation can be used, e.g., masked language model <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref>. Note that those two tasks only share th  output at [CLS] is often used as the sentence representation.</p><p>PLM Objective Inspired by BERT <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref>, MLM randomly selects 15% of input token LS] output for sentence-level prediction and the outputs of all tokens for sequence labelling tasks <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b)</ref> </p></div> <div xmlns=\"http://www.tei-c.  use the transformer architecture <ref type=\"bibr\" target=\"#b41\">(Vaswani et al., 2017)</ref> as in <ref type=\"bibr\" target=\"#b10\">(Devlin et al., 2019b;</ref><ref type=\"bibr\" target=\"#b20\">Liu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Count Dictionary\" <ref type=\"bibr\" target=\"#b19\">[20]</ref>. The simplified Chinese LIWC dictionary <ref type=\"bibr\" target=\"#b20\">[21]</ref> is developed by Chinese psychologists and linguists, based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Weisfeiler &amp; Lehman, 1968;</ref><ref type=\"bibr\" target=\"#b31\">Read &amp; Corneil, 1977;</ref><ref type=\"bibr\" target=\"#b3\">Cai et al., 1992)</ref>, and so does the related Weisfeiler-Lehman ker. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:   The MT synthetic data in this work helps the system in a manner similar to knowledge distillation <ref type=\"bibr\" target=\"#b19\">[20]</ref>, since the network is trained to predict outputs from a pr  Translate service to obtain such translations. This procedure is similar to knowledge distillation <ref type=\"bibr\" target=\"#b19\">[20]</ref>, except that it uses the final predictions as training tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games <ref type=\"bibr\" target=\"#b6\">12</ref> . We demonstrate that the deep Q-network agent, receiving onl best result obtained by a linear function approximator on different types of hand designed features <ref type=\"bibr\" target=\"#b6\">12</ref> . Contingency (SARSA) agent figures are the results obtained  ing methods from the reinforcement learning literature on the 49 games where results were available <ref type=\"bibr\" target=\"#b6\">12,</ref><ref type=\"bibr\" target=\"#b9\">15</ref> . In addition to the l ed experiments on 49 Atari 2600 games where results were available for all other comparable methods <ref type=\"bibr\" target=\"#b6\">12,</ref><ref type=\"bibr\" target=\"#b9\">15</ref> . A different network  table\">2</ref> | Comparison of games scores obtained by DQN agents with methods from the literature <ref type=\"bibr\" target=\"#b6\">12,</ref><ref type=\"bibr\" target=\"#b9\">15</ref> and a professional hum. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kloads can be classified into two categories: services and data analytics workloads as mentioned in <ref type=\"bibr\" target=\"#b43\">[44]</ref> and <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For the dat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ust first discuss the concept of Mip Mapping.</p><p>The goal of Mip Mapping, introduced by Williams <ref type=\"bibr\" target=\"#b14\">[15]</ref>, is to efficiently avoid aliasing artifacts by quickly fil ntation.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Previous Work</head><p>In <ref type=\"bibr\" target=\"#b14\">[15]</ref>, Williams also described a clever memory organization and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b40\">[42,</ref><ref type=\"bibr\" target=\"#b93\">96,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b74\">77,</ref><ref type=\"bibr\" target=\"#b48\">50,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion problem.</p><p>Learning upscaling filters was briefly suggested in the footnote of Dong et.al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>. However, the importance of integrating it into the CNN as p eration was not fully recognised and the option not explored. Additionally, as noted by Dong et al. <ref type=\"bibr\" target=\"#b5\">[6]</ref>, there are no efficient implementations of a convolution lay uate the power of the sub-pixel convolution layer by comparing against SRCNN's standard 9-1-5 model <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Here, we follow the approach in <ref type=\"bibr\" target=\"#b CNN's standard 9-1-5 model <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Here, we follow the approach in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, using relu as the activation function for our models in thi ard comparison with results from previous published results<ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b5\">6]</ref>.</note> \t\t</body> \t\t<back> \t\t\t<div type=\"references\">  \t\t\t\t<l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar d has witnessed a variety of network architectures, such as a deeper network with residual learning <ref type=\"bibr\" target=\"#b21\">[22]</ref>, Laplacian pyramid structure <ref type=\"bibr\" target=\"#b25 models including PSNR-oriented methods, such as SRCNN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, VDSR <ref type=\"bibr\" target=\"#b21\">[22]</ref>, LapSRN <ref type=\"bibr\" target=\"#b25\">[26]</ref>, DRRN <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Our system extracts features from those scales using a similar concept to feature pyramid networks <ref type=\"bibr\" target=\"#b7\">[8]</ref>. From our base feature extractor we add several convolutiona. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  not address the load balancing problem.</p><p>Clustering can also be applied to VLIW architectures <ref type=\"bibr\" target=\"#b7\">[8]</ref>  <ref type=\"bibr\" target=\"#b13\">[14]</ref>. In this case the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  expected to characterize user preferences and item features. More recently, deep learning models ( <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b11\">13]</ref>), which can learn m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e earlier works, including IsoMAP <ref type=\"bibr\" target=\"#b10\">[11]</ref>, local linear embedding <ref type=\"bibr\" target=\"#b11\">[12]</ref> and Laplacian eigenmap <ref type=\"bibr\" target=\"#b12\">[13]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"table\" target=\"#tab_0\">1</ref> is a summary. The IMDB<ref type=\"foot\" target=\"#foot_2\">1</ref>  <ref type=\"bibr\" target=\"#b12\">[13]</ref> data set consists of numerous film movie reviews. It is co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  future process generations. In this section, we review a technique called pipeline reconfiguration <ref type=\"bibr\" target=\"#b20\">[20]</ref>, that allows a large logical design to be implemented on a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: zation encourages the confidence of predictions and is commonly used in the semisupervised learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: blished work on MCTS, to provide the reader Fig. <ref type=\"figure\">1</ref>. The basic MCTS process <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>with the tools to solve new problems using MCTS and basic MCTS process is conceptually very simple, as shown in Figure <ref type=\"figure\">1</ref> (from <ref type=\"bibr\" target=\"#b16\">[17]</ref>). A tree 1 is built in an incremental and asymmetric manne t two moves and uses LGR-1 if there is no LGR-2 entry for the last two moves.</p><p>Baier and Drake <ref type=\"bibr\" target=\"#b16\">[17]</ref> propose an extension to LGR-1 and LGR-2 called Last Good R ests using the Last Good Reply heuristic (6.1.8) to inform simulations, modified by Baier and Drake <ref type=\"bibr\" target=\"#b16\">[17]</ref> to include the forgetting of bad moves. Most programs use . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he Cray XMT <ref type=\"bibr\" target=\"#b9\">[10]</ref> building on earlier work done by the J-Machine <ref type=\"bibr\" target=\"#b10\">[11]</ref> and HEP <ref type=\"bibr\" target=\"#b11\">[12]</ref>, utilize. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sociations. Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets  gion embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> for improving accuracy.</p></div> <di ing each word in the text to a word vector (word embedding). We take a more general viewpoint as in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> and consider text region embedding -e w-1 as input, serves as an unsupervised embedding function in the model for text categorization. In <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> unsupervised embeddings obtained this ings. Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in <ref type=\"bibr\" target=\"#b6\">(Johnson and Zhang, 2015b)</ref> as a semi-supervised extension of <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ntiment analysis are mainly based on hand-crafted features, such as global and local RGB histograms <ref type=\"bibr\" target=\"#b2\">[3]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>, texture features < f>, Wiccest and Gabor features <ref type=\"bibr\" target=\"#b5\">[6]</ref>, SIFT-based bags of features <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Gist features <ref sis, which are mainly based on the hand-crafted features, including global and local RGB histograms <ref type=\"bibr\" target=\"#b2\">[3]</ref>- <ref type=\"bibr\" target=\"#b4\">[5]</ref>, texture features < f>, Wiccest and Gabor features <ref type=\"bibr\" target=\"#b5\">[6]</ref>, SIFT-based bags of features <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, Gist features <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: only provide few partitions and often degrade performance, D-NUCA schemes seldom use them. ASP-NUCA <ref type=\"bibr\" target=\"#b11\">[12]</ref>, ESP-NUCA <ref type=\"bibr\" target=\"#b30\">[31]</ref>, and E tency of private caches. These schemes often size partitions using hill-climbing (e.g., shadow tags <ref type=\"bibr\" target=\"#b11\">[12]</ref> or LRU way hit counters <ref type=\"bibr\" target=\"#b15\">[16. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: echniques for generation of functional test programs for manufacturing testing of microprocessors ( <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce the L1 hybrid is used to filter easily predicted highly biased branches, a confidence estimator <ref type=\"bibr\" target=\"#b13\">[14]</ref> indicates whether the branch is more difficult to predict  ch prediction process involve correlating the actual branch register values with the branch outcome <ref type=\"bibr\" target=\"#b13\">[14]</ref> using a conventional value predictor. The authors of the s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecent embedding for node i. The use of max operator is inspired from learning on general point sets <ref type=\"bibr\" target=\"#b34\">(Qi et al., 2017)</ref>. By applying max-pooling operator element-wis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: re roughly two sorts of black-box attacks according to the mechanism they adopt. One is query-based <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targ y, attackers can approximate the loss gradient of the target model through training a local replica <ref type=\"bibr\" target=\"#b23\">[24]</ref> or finite difference techniques <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f a premise and hypothesis through tree-LSTM <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Le and Zuidema,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> allow end-to-end differentable losses over data with arbitr for unsupervised losses, most work follows the semi-supervised setting for node classification from <ref type=\"bibr\" target=\"#b28\">[29]</ref>. For a complete introductions to the vast topic we refer i ider transductive GNNs that output a single embedding per node. Graph convolutional networks (GCNs) <ref type=\"bibr\" target=\"#b28\">[29]</ref> are simple yet effective <ref type=\"bibr\" target=\"#b50\">[5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ead n=\"4.\">Experiments</head><p>For the experiments, we follow the protocol used in previous papers <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" ta  (2) lifted structured embedding <ref type=\"bibr\" target=\"#b20\">[21]</ref>, (3) N-pairs metric loss <ref type=\"bibr\" target=\"#b19\">[20]</ref>, (4) clustering <ref type=\"bibr\" target=\"#b14\">[15]</ref>,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: i-c.org/ns/1.0\"><head n=\"1.\">Introduction</head><p>Transformer models were originally introduced by <ref type=\"bibr\" target=\"#b36\">Vaswani et al. (2017)</ref> in the context of neural machine translat p><p>Initially, in \u00a7 3.1, we introduce a formulation for the transformer architecture introduced in <ref type=\"bibr\" target=\"#b36\">(Vaswani et al., 2017)</ref>. Subsequently, in \u00a7 3.2 and \u00a7 3.3 we pre the memory consumption with respect to the self attention layer. In all experiments, we use softmax <ref type=\"bibr\" target=\"#b36\">(Vaswani et al., 2017)</ref> to refer to the standard transformer arc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ickr and Yelp were introduced in <ref type=\"bibr\" target=\"#b51\">Zeng et al. (2019)</ref> and PPI in <ref type=\"bibr\" target=\"#b55\">Zitnik and Leskovec (2017)</ref>. In agreement with <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  systems. A more detailed description of the protocol and a proof of its security were presented in <ref type=\"bibr\" target=\"#b12\">[14]</ref>, though the published version of that paper did not presen s modifications to the group in the PVL.</p><p>A proof of the consistency protocol was presented in <ref type=\"bibr\" target=\"#b12\">[14]</ref>, but is beyond the scope of this paper. At a high level, h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e best AUC improvement on 12.6% on the arXiv dataset over the best performing baseline (Adamic-Adar <ref type=\"bibr\" target=\"#b0\">[1]</ref>). Amongst the feature learning algorithms, node2vec outperfo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpare our proposed model with the following single-task models for publication recognition:\u2022 ParsCit<ref type=\"bibr\" target=\"#b3\">[4]</ref> is an open-source package3 for parsing publications based on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lection can be modeled as a hierarchical graph, in which a document is regarded as a graph-of-words <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and then a set of documents are interconnected via the ci. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b40\">[41]</ref>, and graph pattern mining <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. Although many techniques such as search order optimization raphs from data graphs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref>. Graph mining is essential for several problems involving t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dio fidelity using a much simplified voice building pipeline <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. However, such models t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: [2]</ref>). All of the models mentioned above are shallow networks (less than 5 layers). Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> first introduced the residual architecture for training mu ly concatenate together, which leads to the underutilization of local features. In 2016, Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed a residual learning framework (Fig. <ref type=\"fi r module, we design a set of comparative experiments to compare the performance with residual block <ref type=\"bibr\" target=\"#b11\">[12]</ref>, dense block <ref type=\"bibr\" target=\"#b23\">[24]</ref>   < figDesc>Fig. 5. Quantitative comparison of three different feature extraction blocks (residual block<ref type=\"bibr\" target=\"#b11\">[12]</ref>, dense block<ref type=\"bibr\" target=\"#b23\">[24]</ref>, and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b27\">[1]</ref>) or lack a clear objective function tailored for network embedding (e.g., <ref type=\"bibr\" target=\"#b42\">[16]</ref>). We anticipate that a new model with a carefully designed e for both undirected and directed graphs.</p><p>The most recent work related with ours is DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>, which deploys a truncated random walk for social network  /ref> . The Flickr network is denser than the Youtube network (the same network as used in DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>). (3) Citation Networks. Two types of citation networks ar rk can be represented as an affinity matrix, and is able to represent each vertex with a \u2022 DeepWalk <ref type=\"bibr\" target=\"#b42\">[16]</ref>. DeepWalk is an approach recently proposed for social netw r\" target=\"#b39\">[13]</ref>. For other networks, the dimension is set as 128 by default, as used in <ref type=\"bibr\" target=\"#b42\">[16]</ref>. Other default settings include: the number of negative sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: us with two intermediate minibatches (g k (x), y) and (g k (x ), y ). Third, we perform Input Mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> on these intermediate minibatches. This pr <p>Here, (y, y ) are one-hot labels, and the mixing coefficient \u03bb \u223c Beta(\u03b1, \u03b1) as proposed in mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>. For instance, \u03b1 = 1.0 is equivalent to sa  to substantial improvements, achieving 2.45% test error on CIFAR-10 when combined with Input Mixup <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>. As AgrLearn is complimentary to Input Mix rs: no regularization, AdaMix, Input Mixup, and Manifold Mixup. We follow the training procedure of <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref>, which is to use SGD with momentum, a weig e=\"table\">1</ref>: Classification errors on (a) CIFAR-10 and (b) CIFAR-100. We include results from <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018)</ref> \u2020 and <ref type=\"bibr\" target=\"#b9\">(Guo e  more detail). In the case where S = {0}, Manifold Mixup reduces to the original mixup algorithm of <ref type=\"bibr\" target=\"#b27\">Zhang et al. (2018)</ref>. While one could try to reduce the variance random interpolations between training examples and perform the same interpolation for their labels <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2018;</ref><ref type=\"bibr\" target=\"#b24\">Tokozume et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  way that GNNs exchange that information between nodes makes them vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Adversarial attacks on graphs, which carefully rewire the g ning attacks poisoning attacks poisoning attacks poisoning attacks poisoning attacks (e.g., Nettack <ref type=\"bibr\" target=\"#b7\">[8]</ref>) that perturb the graph in training-time and evasion attacks \"#b28\">[29]</ref>. The former deceives the model to misclassify a specific node (i.e., target node) <ref type=\"bibr\" target=\"#b7\">[8]</ref> while the latter degrades the overall performance of the tra e targeted attack where the attacker only manipulates edges of the target node's neighbors. Nettack <ref type=\"bibr\" target=\"#b7\">[8]</ref> generates perturbations by modifying graph structure (i.e.,  . Also, <ref type=\"bibr\" target=\"#b15\">[16]</ref> is designed specifically for the Nettack attacker <ref type=\"bibr\" target=\"#b7\">[8]</ref> and so is less versatile. Another technique <ref type=\"bibr\" our model to baselines under three kinds of adversarial attacks: direct targeted attack (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>), influence targeted attack (Nettack-In <ref type=\"bibr\" tar  attack (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>), influence targeted attack (Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>), and non-targeted attack (Mettack <ref type=\"bibr\" target=\" or all neighbors. In the targeted attack, we select 40 correctly classified target nodes (following <ref type=\"bibr\" target=\"#b7\">[8]</ref>): 10 nodes with the largest classification margin, 20 random taset into training (10%), validation (10%), and test set (80%) following the experimental setup in <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Ap ef type=\"bibr\" target=\"#b15\">[16]</ref>), and models for generating adversarial attacks (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Met  for generating adversarial attacks (Nettack-Di <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Nettack-In <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Mettack <ref type=\"bibr\" target=\"#b25\">[26]</ref>).</p> y loss using Adam optimizer and learning rate of 0.01. For other parameters, we follow the setup in <ref type=\"bibr\" target=\"#b7\">[8]</ref>.</p></div>\t\t\t</div> \t\t\t<div type=\"references\">  \t\t\t\t<listBib attacker finds optimal perturbation A through optimization <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>:</p><formula xml:id=\"formula_3\">argmin A \u2208P G \u2206 L attack (f(A s. The attacker aims to destroy prediction for target node u by manipulating the incident edges of u<ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. Here, T = A = {u}. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: good models, e.g., <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar >19]</ref> and NAS <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>.</p><p>We emphasize. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  image as points in it. Pixels representing the same object naturally cluster in the spectral space <ref type=\"bibr\" target=\"#b3\">[4]</ref> . This property provides us an opportunity to segment pixels. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, convolutional neural network (CNN) <ref type=\"bibr\" target  values. For this reason, several super-resolution methods <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" tar  the performance of our method on several datasets. We first describe datasets used Ground Truth A+ <ref type=\"bibr\" target=\"#b28\">[29]</ref> SRCNN <ref type=\"bibr\" target=\"#b4\">[5]</ref> RFL <ref typ ad><p>We provide quantitative and qualitative comparisons. For benchmark, we use public code for A+ <ref type=\"bibr\" target=\"#b28\">[29]</ref>, SRCNN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, RFL <ref t sion is much more sensitive to details in intensity than in color.</p><p>As some methods such as A+ <ref type=\"bibr\" target=\"#b28\">[29]</ref> and RFL <ref type=\"bibr\" target=\"#b22\">[23]</ref> do not p =\"#b18\">[19]</ref> and Set14 <ref type=\"bibr\" target=\"#b31\">[32]</ref> are often used for benchmark <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lso are sensitive to small, worst-case perturbations of the input, so-called \"adversarial examples\" <ref type=\"bibr\" target=\"#b32\">(Szegedy et al., 2014)</ref>. This latter phenomenon has struck many   et al., 2004;</ref><ref type=\"bibr\" target=\"#b3\">Biggio &amp; Roli, 2018)</ref>. Since the work of <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref>, a subfield has focused specifically on t ly on the phenomenon of small adversarial perturbations of the input, or \"adversarial examples.\" In <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref> it was proposed these adversarial example mproved robustness to small perturbations.</p><p>In the introduction we referred to a question from <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref> about why we find errors so close to our  lem for every point in the test set <ref type=\"bibr\" target=\"#b23\">(Katz et al., 2017)</ref>. Since <ref type=\"bibr\" target=\"#b32\">Szegedy et al. (2014)</ref>, hundreds of adversarial defense papers h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ly during generation. Some neural methods <ref type=\"bibr\" target=\"#b14\">(Kiddon et al., 2016;</ref><ref type=\"bibr\" target=\"#b9\">Feng et al., 2018)</ref> propose to record the accumulated attention d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nclean-100\" subset of LibriSpeech as the labelled data set <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. Table <ref type=\"table\" target=\"#tab_0\">1</ref> shows the  an LM.</p><p>\"train-clean-100\" as well as several other results from the literature. Hayashi et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> use a sequence-to-sequence model with a BiLSTM-based encod a synthetic data set, but target hidden state representations instead of acoustic features directly <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Alternatively, both unpaired audio and text can be used b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  into an open-source big data benchmark suite-BigDataBench <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, which is publicly available from <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng from position s. This neural network combines the roles of both policy network and value network <ref type=\"bibr\" target=\"#b11\">12</ref> into a single architecture. The neural network consists of m perfect information. We follow the formalism of alternating Markov games described in previous work <ref type=\"bibr\" target=\"#b11\">12</ref> , noting that algorithms based on value or policy iteration  ompare three distinct versions of AlphaGo:</p><p>1. AlphaGo Fan is the previously published program <ref type=\"bibr\" target=\"#b11\">12</ref> that played against Fan Hui in October 2015. This program wa described in this paper. However, it uses the same handcrafted features and rollouts as AlphaGo Lee <ref type=\"bibr\" target=\"#b11\">12</ref> and training was initialised by supervised learning from hum ue component, it was possible to avoid overfitting to the values (a problem described in prior work <ref type=\"bibr\" target=\"#b11\">12</ref> ). After 72 hours the move prediction accuracy exceeded the  the KGS test set; the value prediction error was also substantially better than previously reported <ref type=\"bibr\" target=\"#b11\">12</ref> . The validation set was composed of professional games from of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored to the tournament values from prior work <ref type=\"bibr\" target=\"#b11\">12</ref> , and correspond to the players reported in that work. The r lso performed against baseline players with Elo ratings anchored to the previously published values <ref type=\"bibr\" target=\"#b11\">12</ref> .</p><p>We measured the head-to-head performance of AlphaGo  hat maximise an upper confidence bound Q(s, a) + U (s, a), where U (s, a) \u221d P (s, a)/(1 + N (s, a)) <ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b23\">24</ref> , until a leaf node  After 72 hours the move prediction accuracy exceeded the state of the art reported in previous work <ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b29\">[30]</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: et=\"#b51\">[52,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b48\">49,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. A popular learning paradigm is graphbased / hypergraph-bas ral network f (G, X) <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> (X contains the initial features on the vertices for exampl learning problem on the approximation. While the state-of-the-art hypergraph neural networks (HGNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> approximates each hyperedge by a clique and hence requires  detailed experimentation, we demonstrate their effectiveness compared to the state-of-the art HGNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> and other baselines (Sections 5, and 7). \u2022 We thoroughly d =\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Hypergraph neural networks <ref type=\"bibr\" target=\"#b16\">[17]</ref> and their variants <ref type=\"bibr\" target=\"#b22\">[23,</re 2 . Our approach requires at most a linear number of edges (1 and 2|e| \u2212 3 respectively) while HGNN <ref type=\"bibr\" target=\"#b16\">[17]</ref> requires a quadratic number of edges for each hyperedge. < yperGCN and FastHyperGCN against the following baselines:</p><p>\u2022 Hypergraph neural networks (HGNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses the clique expansion <ref type=\"bibr\" target=\"#b51\">[  [7].Our approach requires at most a linear number of edges (1 and 2|e| \u2212 3 respectively) while HGNN<ref type=\"bibr\" target=\"#b16\">[17]</ref> requires a quadratic number of edges for each hyperedge.</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ition pattern is more complicated. \u2022 The recent approaches <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, which divide the u  the fairness and the convenience of comparison, we follow <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> to filter out sessi , s,i \u22121 ] with the last item s,i \u22121 as l abel . Following <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, for the Yoochoose  tention on the last item after encoding with a RNN. To alleviate the influence of time order, STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> only utilizes the self-attention mechanism without RNN. SR  to sum up as the session embedding. To further alleviate the bias introduced by time series, STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> entirely replaces the recurrent encoder with an attention  h enables the model to explicitly emphasize on the more important parts of the input.</p><p>\u2022 STAMP <ref type=\"bibr\" target=\"#b18\">[19]</ref> uses attention layers to replace all RNN encoders in previ ith different lengths because the length varies greatly within one dataset. Following previous work <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, sessions in Yooch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  noisy labeling, previous studies adopt multi-instance learning to consider the noises of instances <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010;</ref><ref type=\"bibr\" target=\"#b5\">H te this issue, many studies formulated relation classification as a multi-instance learning problem <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010;</ref><ref type=\"bibr\" target=\"#b5\">H the framework of reinforcement learning <ref type=\"bibr\" target=\"#b16\">(Sutton and Barto 1998;</ref><ref type=\"bibr\" target=\"#b13\">Narasimhan, Yala, and Barzilay 2016)</ref> and then predicts relation 4</ref> generated by the sentences in NYT<ref type=\"foot\" target=\"#foot_2\">5</ref> and developed by <ref type=\"bibr\" target=\"#b13\">(Riedel, Yao, and McCallum 2010)</ref>. There are 522,611 sentences, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ormally, given the reference frame I t and each support frame I t+\u03c4 , Feature Pyramid Network (FPN) <ref type=\"bibr\" target=\"#b55\">[56]</ref> is leveraged to extract multi-scale pyramidal feature maps ad><p>Feature Pyramid Network. FPN is built at the top of ResNet-101 pre-trained on ImageNet. As in <ref type=\"bibr\" target=\"#b55\">[56]</ref>, P3, P4, Algorithm 2 Inference Algorithm of our SSVD </p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mation than that of end-to-end learning <ref type=\"bibr\" target=\"#b13\">(Hamanaka et al., 2017;</ref><ref type=\"bibr\" target=\"#b37\">Tian et al., 2016;</ref><ref type=\"bibr\" target=\"#b42\">Wan and Zeng, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b7\">(Hochreiter and Schmidhuber, 1997)</ref> as recurrent networks. We use dropout <ref type=\"bibr\" target=\"#b27\">(Srivastava et al., 2014)</ref>  sional projections. In Block layers,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: applications.</p><p>Various efforts have been made to refine the injection framework. CriticalFault <ref type=\"bibr\" target=\"#b9\">[10]</ref> applied vulnerability analysis to avoid the injections that l fault injection (SFI) to model the soft error rate (SER) of targeted systems.</p><p>CriticalFault <ref type=\"bibr\" target=\"#b9\">[10]</ref> proposes a biased injection framework that employs vulnerab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eature interactions from raw data automatically. A popular approach is factorization machines (FMs) <ref type=\"bibr\" target=\"#b26\">[27]</ref>, which embeds features into a latent space and models the  underlying structure, FM may not be expressive enough. Although higher-order FMs have been proposed <ref type=\"bibr\" target=\"#b26\">[27]</ref>, they still belong to the family of linear models and are  ing the second-order factorized interactions between features. By specifying input features, Rendle <ref type=\"bibr\" target=\"#b26\">[27]</ref> showed that FM can mimic many speci c factorization models  value, rather than simply an embedding table lookup, so as to account for the real valued features <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>Bi-Interaction Layer. We then feed the embedding se nsorFlow implementation<ref type=\"foot\" target=\"#foot_7\">7</ref> of higherorder FM, as described in <ref type=\"bibr\" target=\"#b26\">[27]</ref>. We experimented with order size 3, since the MovieLens da n Machines</head><p>Factorization machines are originally proposed for collaborative recommendation <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Given a real valu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b43\">(Yoshikawa et al., 2009;</ref><ref type=\"bibr\" target=\"#b19\">Leeuwenberg and Moens, 2017;</ref><ref type=\"bibr\" target=\"#b28\">Ning et al., 2017)</ref>.</p><p>Since TempRel is a specific relation  rget=\"#b9\">(Chambers and Jurafsky, 2008b;</ref><ref type=\"bibr\" target=\"#b15\">Do et al., 2012;</ref><ref type=\"bibr\" target=\"#b28\">Ning et al., 2017</ref>). An overview of the proposed network structu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  segmentation <ref type=\"bibr\" target=\"#b19\">[20]</ref>, <ref type=\"bibr\" target=\"#b27\">[28]</ref>, <ref type=\"bibr\" target=\"#b38\">[39]</ref>. In dilated convolutional layers, filter weights are emplo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: chniques have been developed to model the mapping from LR to HR space, including neighbor embedding <ref type=\"bibr\" target=\"#b3\">[4]</ref>, random forest <ref type=\"bibr\" target=\"#b19\">[20]</ref> and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raining. However, in this case three-fold data augmentation was applied prior to feature extraction <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the acoustic features comprised 40-dimensional MFCCs (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: er could generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targe sible to generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" targe ier with adversarial examples, called adversarial training <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>; <ref type=\"bibr\" target=\"#b1\">(2)</ref> Training a classifie from normal examples <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Moosavi et al. showed that it was even possible to find one  \"2.3.1\">Fast gradient sign method(FGSM). Given a normal image</head><p>x, fast gradient sign method <ref type=\"bibr\" target=\"#b4\">[5]</ref> looks for a similar image x \u2032 in the L \u221e neighborhood of x t \"#b21\">22]</ref>, or mix the adversarial objective with the classification objective as regularizer <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Though this idea is promising, it is hard to reason about w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mal preprocessing scheme.</p><p>The spectrograms are processed by a deep bidirectional LSTM network <ref type=\"bibr\" target=\"#b10\">(Graves et al., 2013)</ref> with a Connectionist Temporal Classificat M is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM <ref type=\"bibr\" target=\"#b10\">(Graves et al., 2013)</ref>.</p></div> <div xmlns=\"http://www.tei-c.o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying <ref type=\"bibr\" target=\"#b22\">[24]</ref>. The node-copying model can be used to produce sample grap type=\"bibr\" target=\"#b21\">[23]</ref> uses a non-parametric model for the graph generative model and <ref type=\"bibr\" target=\"#b22\">[24]</ref> proposes a node copying model to achieve flexibility in th rnative, we use a more general generative model for graphs based on copying nodes, as introduced in <ref type=\"bibr\" target=\"#b22\">[24]</ref>. We demonstrate in the following sections that this model   setting.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Node Copying</head><p>In <ref type=\"bibr\" target=\"#b22\">[24]</ref>, Pal et al. introduce the node copying model for \ud835\udc5d (G). Sa etworks (BGNNs). The proposed BGNN incorporates a random graph generative model based on nodecopying<ref type=\"bibr\" target=\"#b22\">[24]</ref>. The node-copying model can be used to produce sample grap ode classification when there are very few training labels <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" tar node classification when there are very few training labels<ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" tar s. These limitations were addressed in the follow-up works <ref type=\"bibr\" target=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24]</ref>, where <ref type=\"bibr\" target=\"#b21\">[23]</ref> uses a non. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  few data but achieve good performance. A typical example of this approach is prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref>, which averages the vector of few support  http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">Prototypical Networks</head><p>The prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> has achieved excellent performance in few- cia and Bruna, 2018)</ref>, SNAIL <ref type=\"bibr\" target=\"#b10\">(Mishra et al., 2018)</ref>, Proto <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017)</ref> and PHATT <ref type=\"bibr\" target=\"#b3\">(G  its label, and obviate the need for fine-tuning to adapt to new class types. Prototypical networks <ref type=\"bibr\" target=\"#b13\">(Snell et al., 2017</ref>) learns a metric space in which the model c  (y = l i q) = exp(\u2212d(g \u03b8 (q), c i ) \u03a3 L l=1 exp(\u2212d(g \u03b8 (q), c l )<label>(9)</label></formula><p>As <ref type=\"bibr\" target=\"#b13\">Snell et al. (2017)</ref> mentioned, squared Euclidean distance is a  he implementation details are as follows.</p><p>For FewRel dataset, we cite the results reported by <ref type=\"bibr\" target=\"#b13\">Snell et al. (2017)</ref> which includes Finetune, kNN, MetaN, GNN, a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ased loss is inappropriate for transferring activation boundaries, and thus suggested a hinge loss. <ref type=\"bibr\" target=\"#b21\">[22]</ref> and <ref type=\"bibr\" target=\"#b22\">[23]</ref> employed adv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f type=\"bibr\" target=\"#b31\">[32]</ref>, also referred to as Message Passing Neural Networks (MPNNs) <ref type=\"bibr\" target=\"#b22\">[23]</ref> are the prevalent approach in this field but they only pas </ref>, which are based on the eigendecomposition of the graph Laplacian, and spatial-based methods <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wise approach.</p><p>\u2022 Combining the proposed model with densityweighted Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we introduce active learning into POLAR <ref type=\"bibr\"  ediction the parameters under the posterior disagree about are selected. Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref> selects the instance that maximizes the expected loss base ected Loss Optimization</head><p>The active learning metric we choose is Expected Loss Optimization <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The basic idea is to choose the instance that maximizes t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, but each of these . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hebyshev polynomials to the graph Laplacian, spatially localized filtering is obtained. Kipf et al. <ref type=\"bibr\" target=\"#b18\">[18]</ref> approximate the polynomials using a re-normalized first-or rmula_4\">E ijp = \u00caijp N j=1 \u00caijp<label>(5)</label></formula><p>or symmetric normalization as in GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref>:</p><formula xml:id=\"formula_5\">E ijp = \u00caijp N i=1 \u00caijp N  e our EGNN(C) layer from the formula of EGNN(A) layer. Indeed, the essential difference between GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref> is wheth nd 20% sized subsets, which is called \"dense\" splitting.</p><p>Following the experiment settings of <ref type=\"bibr\" target=\"#b18\">[18]</ref>[27], we use two layers of EGNN in all of our experiments f nd are penalized more in the loss than a majority class.</p><p>The baseline methods we used are GCN <ref type=\"bibr\" target=\"#b18\">[18]</ref> and GAT <ref type=\"bibr\" target=\"#b28\">[27]</ref>. To inve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ref> to alleviate the burden on the model to attend to local features. We add residual connections <ref type=\"bibr\" target=\"#b11\">[12]</ref> to both multihead attention and convolutional layers. The . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  MI alone, and the choice of encoder and MI estimators have a significant impact on the performance <ref type=\"bibr\" target=\"#b52\">(Tschannen et al., 2020)</ref>.</p><p>Figure <ref type=\"figure\">1</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b65\">[66]</ref>, and the co-purchase graphs AMAZON COMPUTERS and AMAZON PHOTO <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>. We only use their. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get length, we apply frame stack and downsample similar to <ref type=\"bibr\" target=\"#b22\">[24,</ref><ref type=\"bibr\" target=\"#b23\">25]</ref>. The final acoustic feature sequence is S = (s1, s2, \u2022 \u2022 \u2022 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2017;</ref><ref type=\"bibr\" target=\"#b39\">Veli\u010dkovi\u0107 et al., 2018;</ref><ref type=\"bibr\">2019;</ref><ref type=\"bibr\" target=\"#b33\">Qu et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Gao &amp; Ji, 20 rs and the edge weights between them correspond to the degree of trust between the users. Following <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>, we treat edges with weights greater than 3 a t state-of-the-art methods GAT <ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref>, GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref> and Graph U-Net <ref type=\"bibr\" target=\"#b13 the results of GraphMix(GCN) are comparable with the recently proposed state-of-the-art method GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>. Since GraphMix consists of various component  \u00b1 0.3% GraphScan <ref type=\"bibr\" target=\"#b11\">(Ding et al., 2018)</ref> 83.3 \u00b11.3 73.1\u00b11.8 -GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref> 83.7% 73.1% 81.8% DisenGCN <ref type=\"bibr\" t ; Welling, 2017)</ref>, GAT <ref type=\"bibr\" target=\"#b39\">(Veli\u010dkovi\u0107 et al., 2018)</ref> and GMNN <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019)</ref>, among others. This architecture has one hidd hence much of the recent attention is dedicated to proposing architectural changes to these methods <ref type=\"bibr\" target=\"#b33\">(Qu et al., 2019;</ref><ref type=\"bibr\" target=\"#b13\">Gao &amp; Ji, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: educes the effectiveness of prefetching.</p><p>Recent solutions use the Global History Buffer (GHB) <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which organizes correlation information by storing recent uce the GHB as a general structure for prefetching streams of temporally correlated memory requests <ref type=\"bibr\" target=\"#b27\">[28]</ref>. However, when used to record address correlation <ref typ /DC prefetcher, which which learns the deltas, or differences, between consecutive memory addresses <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Delta correlation allows PC/DC to store all meta-data on  http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\">Using Nesbit and Smith's terminology<ref type=\"bibr\" target=\"#b27\">[28]</ref>, in which the name before the slash describes the referenc streams based on the PC of the loading instruction, which is known to improve coverage and accuracy <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" ta fetchers, SMS <ref type=\"bibr\" target=\"#b38\">[39]</ref>, which exploits spatial locality, and PC/DC <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, which uses delta  long streams. Rather than use address correlation, other GHBbased prefetchers use delta correlation <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>, whose space requi -based prefetchers <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar ype=\"bibr\" target=\"#b42\">[43]</ref> or address correlation <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, sacrificing signif. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: cedure. We learn residuals only and use extremely high learning rates (10 4 times higher than SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref>) enabled by adjustable gradient clipping. Our proposed metho ely, random forest <ref type=\"bibr\" target=\"#b20\">[18]</ref> and convolutional neural network (CNN) <ref type=\"bibr\" target=\"#b8\">[6]</ref> have also been used with large improvements in accuracy.</p> 8\">[6]</ref> have also been used with large improvements in accuracy.</p><p>Among them, Dong et al. <ref type=\"bibr\" target=\"#b8\">[6]</ref> has demonstrated that a CNN can be used to learn a mapping f  are highly correlated. Moreover, our initial learning rate is 10 4 times higher than that of SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref>. This is enabled by residual-learning and gradient clipping. d reconstruction. Filters of spatial sizes 9 \u00d7 9, 1 \u00d7 1, and 5 \u00d7 5 were used respectively.</p><p>In <ref type=\"bibr\" target=\"#b8\">[6]</ref>, Dong et al. attempted to prepare deeper models, but failed  ce. We successfully use 20 weight layers (3 \u00d7 3 for each layer). Our network is very deep (20 vs. 3 <ref type=\"bibr\" target=\"#b8\">[6]</ref>) and information used for reconstruction (receptive field) i  for Very Deep Networks Training deep models can fail to converge in realistic limit of time. SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref> fails to show superior performance with more than three weig  a network to converge within a week on a common GPU. Looking at Fig. <ref type=\"figure\">9</ref> of <ref type=\"bibr\" target=\"#b8\">[6]</ref>, it is not easy to say their deeper networks have converged  n of 200 images from Berkeley Segmentation Dataset <ref type=\"bibr\" target=\"#b18\">[16]</ref>. SRCNN <ref type=\"bibr\" target=\"#b8\">[6]</ref> uses a very large ImageNet dataset.</p><p>We use 291 images   The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al <ref type=\"bibr\" target=\"#b8\">[6]</ref> in their paper based on a GPU implementation.</p><p>In Figur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: le. Two-way distillation. Co-distillation of two instances of the same neural network is studied in <ref type=\"bibr\" target=\"#b1\">[2]</ref> with a focus on training speed-up in a distributed learning   with each head one-by-one. This algorithm is used in both <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. In fact, alternative optimization is popular in generative a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b17\">[18]</ref> such as U-Net <ref type=\"bibr\" target=\"#b23\">[24]</ref>, DeepMedic <ref type=\"bibr\" target=\"#b12\">[13]</ref> and holistically nested networks <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o-Encoder (CAE) to learn the joint representation using Denoising Auto-Encoder (DAE) style learning <ref type=\"bibr\" target=\"#b14\">[15]</ref>. Fig. <ref type=\"figure\" target=\"#fig_4\">5</ref> shows a s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref>, speech recognition <ref type=\"bibr\" target=\"#b8\">[8]</ref>, and natural language processing <ref type=\"bibr\" target=\"#b1\">[1,</ref><ref type=\"bibr\">7]</ref>.</p></div> <div xmlns=\"http://www.t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /expressing emotions and emotionally interacting with the interlocutors. In literature, Zhou et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> successfully build an emotional chat machine (ECM) that is enerate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Hence, the response generation problem faces a significan tional factors, which are most related to our proposed conversation generation problem. Zhou et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> develop an Emotional Chat Machine (ECM) model using three  \" target=\"#b32\">[33]</ref>, to evaluate our experimental results. In particular, we follow the work <ref type=\"bibr\" target=\"#b45\">[46]</ref> to train an emotion classifier for assigning emotional lab ifferent datasets, i.e., NLPCC 2013 2 and NLPCC 2014 3 emotion classification datasets by following <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which contain 29, 417 manually annotated data in total, a o any emotion information, and rare emotion categories like fear are removed. In particular, unlike <ref type=\"bibr\" target=\"#b45\">[46]</ref> using solely one label for classification, we consider bot rget=\"#b35\">[36]</ref>, the traditional Seq2seq model is adopted as one of our baselines.</p><p>ECM <ref type=\"bibr\" target=\"#b45\">[46]</ref>, as mentioned, ECM model is improper to directly be as the onal matrix (if used). The parameters of imemory and ememory in ECM are the same as the settings in <ref type=\"bibr\" target=\"#b45\">[46]</ref>. We use stochastic gradient descent (SGD) with mini-batch  <note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">Here we follow the work<ref type=\"bibr\" target=\"#b45\">[46]</ref>, where the emotion categories are {Angry, Disgust, Happy,  o the detected post's emotion over EIPs.</p><p>Seq2seq-emb <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>, Seq2seq with emotion embedding (Seq2seqemb) is also adopte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ling inference in human language is very challenging. With the availability of large annotated data <ref type=\"bibr\" target=\"#b1\">(Bowman et al., 2015)</ref>, it has recently become feasible to train  creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset <ref type=\"bibr\" target=\"#b1\">(Bowman et al., 2015)</ref>. The corpus has 570,000 human-written Engl ><head n=\"4\">Experimental Setup</head><p>Data The Stanford Natural Language Inference (SNLI) corpus <ref type=\"bibr\" target=\"#b1\">(Bowman et al., 2015)</ref> focuses on three basic relationships betwe rge annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI <ref type=\"bibr\" target=\"#b1\">(Bowman et al., 2015</ref><ref type=\"bibr\" target=\"#b2\">(Bowman et al. ef type=\"bibr\" target=\"#b12\">Iftene and Balahur-Dobrescu, 2007)</ref>, among others. More recently, <ref type=\"bibr\" target=\"#b1\">Bowman et al. (2015)</ref> made available the SNLI dataset with 570,00 ple human annotators. As in the related work, we remove this category. We used the same split as in <ref type=\"bibr\" target=\"#b1\">Bowman et al. (2015)</ref> and other previous work.</p><p>The parse tr >1</ref> shows the results of different models. The first row is a baseline classifier presented by <ref type=\"bibr\" target=\"#b1\">Bowman et al. (2015)</ref> that considers handcrafted features such as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r, a pointwise non-linearity, and either an invariant or equivariant linear output layer. Recently, <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> showed that by allowing higherorder tensor =\"bibr\" target=\"#b13\">Hornik et al., 1989;</ref><ref type=\"bibr\" target=\"#b20\">Pinkus, 1999)</ref>. <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> recently proved that certain invariant GNN aces of continuous invariant (resp. equivariant) functions.</p><p>2 The case of invariant functions <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> recently proved that invariant GNNs simila n the whole set G inv. , that is, for all numbers of nodes n n max simultaneously. On the contrary, <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> work with a fixed n, and it does not seem   on the order of tensorization k s . Indeed, through Noether's theorem on polynomials, the proof of <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref> shows that k s n d (n d \u2212 1)/2 is sufficie  previous invariant case could be easily extended to invariance to subgroups of O n , as is done by <ref type=\"bibr\" target=\"#b19\">Maron et al. (2019b)</ref>, for the equivariant case our theorem only ions in the rest of the introduction, in Section 2 we provide an alternative proof of the result of <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref> for invariant GNNs (Theorem 1), which wil t. Theorem 1. For any \u03c1 \u2208 F MLP , N inv. (\u03c1) is dense in C(G inv. , d edit ).</p><p>Comparison with <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <re th <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. A variant of Theorem 1 was proved in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>. The two proofs are however different: th See the next subsection for details.</p><p>One improvement of our result with respect to the one of <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref> is that it can handle graphs of varying s bibr\" target=\"#b0\">(Battaglia et al., 2016)</ref>. Another outstanding open question, formulated in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b)</ref>, is the characterization of the approxima ing a single hidden layer of such equivariant operators followed by an invariant layer is proved in <ref type=\"bibr\" target=\"#b19\">(Maron et al., 2019b</ref>) (see also <ref type=\"bibr\">(Kondor et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: roximator such as a neural network is used to represent the action-value (also known as Q) function <ref type=\"bibr\" target=\"#b14\">20</ref> . This instability has several causes: the correlations pres  arise and the parameters could get stuck in a poor local minimum, or even diverge catastrophically <ref type=\"bibr\" target=\"#b14\">20</ref> . By using experience replay the behaviour distribution is a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -Encoders (VAEs) <ref type=\"bibr\" target=\"#b8\">[9]</ref> and Generative Adversarial Networks (GANs) <ref type=\"bibr\" target=\"#b3\">[4]</ref> are well known examples of this approach. Because VAEs focus s offer much more flexibility in the definition of the objective function, including Jensen-Shannon <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and all f -divergences <ref type=\"bibr\" target=\"#b16\">[17]< nator is a sum of deltas on the points the discriminator assigns the highest values, as observed by <ref type=\"bibr\" target=\"#b3\">[4]</ref> and highlighted in <ref type=\"bibr\" target=\"#b10\">[11]</ref> N with a convolutional architecture trained with the standard GAN procedure using the \u2212 log D trick <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The generated samples are 3-channel images of 64x64 pixels  ningful (DCGAN generator, top right plot) and in other cases collapse to a single nonsensical image <ref type=\"bibr\" target=\"#b3\">[4]</ref>. This last phenomenon has been theoretically explained in <r 1]</ref> and highlighted in <ref type=\"bibr\" target=\"#b10\">[11]</ref>. When using the \u2212 log D trick <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the discriminator loss and the generator loss are different. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e often due to the lack of type compatibility between the entities due to their typeagnostic nature <ref type=\"bibr\" target=\"#b25\">[Xie et al., 2016</ref><ref type=\"bibr\" target=\"#b7\">, Jain et al., 2 here are some recent efforts to incorporate type hierarchy information in KG embeddings -e.g., TKRL <ref type=\"bibr\" target=\"#b25\">[Xie et al., 2016]</ref> and TransC <ref type=\"bibr\" target=\"#b10\">[L ot have ontological and type label information. Therefore, we use the type labels for entities from <ref type=\"bibr\" target=\"#b25\">[Xie et al., 2016]</ref> which also provides the domain and range inf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1.0\"><head n=\"2.5\">Discussions</head><p>In the subsection, we first show how NGCF generalizes SVD++ <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>In what follows, we analyze the time complexity of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: er a reliable option for deep networks-based representations of user behaviors and item aspects (X. <ref type=\"bibr\" target=\"#b16\">He et al., 2017)</ref>, <ref type=\"bibr\" target=\"#b3\">(Bengio, Duchar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  even if they are explicitly mentioned in the text. An analysis of the inner Transformer components <ref type=\"bibr\" target=\"#b10\">[11]</ref> is a subject for future work.</p></div> <div xmlns=\"http:/. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  using cross-entropy <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. This architecture can also be used as a \"bottleneck\" featu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ss this, some recent studies have proposed greedy methods <ref type=\"bibr\">[Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b11\">Z\u00fcgner et al., 2018]</ref> to attack the graph-based deep learning sy tures in pixel color space. However, recent explorations in the graph adversarial attack techniques <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Dai et al.,  ce, this process can be trivial as many statistics can be pre-computed or re-computed incrementally <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>.</p><p>Algorithm 1: IG-JSMA -Integrated G prediction score for its ground-truth class. The adversarial graph was constructed by using nettack <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>. Without any defense, the target node is  ifficult to attack than those with less neighbors. This is also consistent with the observations in <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref> that nodes with higher degrees have highe ber of neighbors which have low similarity scores to the target nodes. This also stands for nettack <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>. For example, we enable both feature and   contribute much to the predictive capabilities of GCN models but introduce unnecessary complexity. <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref> uses a simplified surrogate model to achi y, IG-JSMA is quite stable as the classification margins have much less variance. Just as stated in <ref type=\"bibr\" target=\"#b11\">[Z\u00fcgner et al., 2018]</ref>, the vanilla gradient-based methods, such. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: B), or achieve the best known result (WiC). The outside results for COPA, MultiRC, and RTE are from <ref type=\"bibr\" target=\"#b56\">Sap et al. (2019</ref><ref type=\"bibr\" target=\"#b61\">), Trivedi et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: c. Methods focusing on the content information <ref type=\"bibr\" target=\"#b6\">(Han et al. 2004;</ref><ref type=\"bibr\" target=\"#b8\">Huang, Ertekin, and Giles 2006;</ref><ref type=\"bibr\">Yoshida et al. 2 The first are based on the content information <ref type=\"bibr\" target=\"#b6\">(Han et al. 2004;</ref><ref type=\"bibr\" target=\"#b8\">Huang, Ertekin, and Giles 2006;</ref><ref type=\"bibr\" target=\"#b12\">Lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: form single image super-resolution (SISR), and significant improvements over shallow CNN structures <ref type=\"bibr\" target=\"#b1\">[2]</ref> have been observed. One benefit from using deeper networks i [4]</ref>, random forest <ref type=\"bibr\" target=\"#b19\">[20]</ref> and convolutional neural network <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Among them, the CNN-based approaches <ref type=\"bibr\" targe ef> have recently set state of the art for SISR. A network with three layers was first developed in <ref type=\"bibr\" target=\"#b1\">[2]</ref> to learn an end-to-end mapping for SR. Subsequently, a deep  tei-c.org/ns/1.0\"><head n=\"3.2.\">Deconvolution layers</head><p>In previous SR methods such as SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> and VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref>, bicubic  using other SISR methods, including bicubic, Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref>, SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> and DRCN <re datasets. On average, an increase of about 1.0 dB using the proposed method was achieved over SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> with 3-layer CNN and an increase of about 0.5 dB over VDSR < llowing us to train very deep Dataset Bicubic Aplus <ref type=\"bibr\" target=\"#b23\">[24]</ref> SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> VDSR <ref type=\"bibr\" target=\"#b10\">[11]</ref> DRCN <ref typ formation and gradient through the network, making it easy to train. In addition, in previous works <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only high-level fea  also to improve the reconstruction performance. Instead of using interpolation for upscaling as in <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, recent studies <ref different types of network structures were studied and compared in our work. As in previous methods <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>, only the feature ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: resolution networks. Previous works including EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, BTSRN <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RDN <ref type=\"bibr\" target=\"#b41\">[42]</ref> found that br\" target=\"#b30\">31]</ref>. It is also experimentally proved in single image super-resolution task <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ \"#b11\">[12]</ref> hinders the accuracy of image super-resolution. Thus, in recent image SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" targ small image patches (e.g. 48 \u00d7 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ ny kinds of regularizers, for examples, weight decaying and dropout, are not adopted in SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ a is augmented with random horizontal flips and rotations following common data augmentation methods<ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. During training, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b6\">[6]</ref>) and alternative decoding approach (e.g., dynamic decoders <ref type=\"bibr\" target=\"#b32\">[31]</ref>). Also, we are interested to apply Eesen to various langua. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hnologies for detecting stress has been accomplished. Few use heart rate and heart rate variability <ref type=\"bibr\" target=\"#b17\">(Vrijkotte et al., 2000)</ref>, cortisol levels <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -order and 2nd-order proximities between vertices to embed homogeneous networks. Specifically, LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref> learns two separated embeddings for 1st-order and 2nd-orde ignal on constructing the bipartite network. Similar to the modeling of 1st-order proximity in LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we model explicit relations by considering the local prox  of vertex sequences. Then the word2vec is applied on the corpus to learn vertex embeddings. \u2022 LINE <ref type=\"bibr\" target=\"#b19\">[20]</ref>: This approach optimizes both the 1st-order and 2nd-order  vec inspire many works <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> to use inner product to model the interaction between two e ural embedding methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, we parameterize the conditional probability P(u c |u i ) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  network (G v ), the graph sampling for GCC follows the three steps-random walks with restart (RWR) <ref type=\"bibr\" target=\"#b50\">[51]</ref>, subgraph induction, and anonymization <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l language understanding and generation tasks <ref type=\"bibr\" target=\"#b9\">(Dai et al., 2019;</ref><ref type=\"bibr\" target=\"#b38\">Shaw et al., 2018)</ref>. The proposed Disentangled Attention mechani tent, and position-to-position<ref type=\"foot\" target=\"#foot_0\">1</ref> .</p><p>Existing approaches <ref type=\"bibr\" target=\"#b38\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Huang et al., LEMENTATION</head><p>For an input sequence of length N , it requires a space complexity of OpN 2 dq <ref type=\"bibr\" target=\"#b38\">(Shaw et al., 2018;</ref><ref type=\"bibr\" target=\"#b18\">Huang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng human credibility using physiological, visual, acoustic, and linguistic features is available in <ref type=\"bibr\" target=\"#b29\">[30]</ref>.</p><p>To our knowledge, no work to date has considered th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd other conventional semantic models.</p><p>In this study, based on a convolutional neural network <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we present a new Convolutional Deep Structured Semantic Mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b61\">62,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" targe ting <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>, sketch2image <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, and other image-to-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: that are likely to be consecutive in the dynamic program are also consecutive in the static program <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Bayesian active learning algorithm for deep learning in image data is proposed based on the idea in <ref type=\"bibr\" target=\"#b41\">[42]</ref>.</p><p>Most works that apply active learning to recommende  basic equation of Variational Inference <ref type=\"bibr\" target=\"#b51\">[52]</ref>.</p><p>Following <ref type=\"bibr\" target=\"#b41\">[42]</ref>, we use the distribution of the network parameter with dro nsidered as the smoothed version of hinge loss.</p><p>As for the second term in (2), it's proved in <ref type=\"bibr\" target=\"#b41\">[42]</ref> that it can be approximated by L2 regularization term</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the dependency path between the entities might be even more indicative of the relation, as noted by <ref type=\"bibr\" target=\"#b40\">Toutanova et al. (2015)</ref>. It is quite possible that using these . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ne for person name recognition and the other for publication recognition. Specifically, we use LSTM <ref type=\"bibr\" target=\"#b6\">[7]</ref> as the RNN unit:</p><formula xml:id=\"formula_0\">N = LST M (S. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ine learning tasks involve graph structured datasets, such as classifying posts in a social network <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, predicting interfaces between protein o aggregate a local set of lower-level features. We refer to such an operator as a graph aggregator <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> and the set of local nodes as the rece ref type=\"bibr\" target=\"#b15\">Kipf and Welling, 2017</ref>) can be interpreted as graph aggregators <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>.</p><p>Graph aggregators are the basic t to the inductive node classification problem. We also improve the sampling strategy introduced in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> to reduce the memory cost and increase oral forecasting problem. Extensive experiments on two node classification datasets, PPI and Reddit <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, and one traffic speed forecasting dat rtional to the total number of nodes, which could be hundreds of thousands of nodes in large graphs <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computatio \"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. To reduce memory usage and computational cost, <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> proposed the GraphSAGE framework that  ble and the goal is to predict the labels of the unseen testing nodes. Our approach follows that of <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, where a mini-batch of nodes are sampl dels in our framework and a two-layer fully connected neural network on the PPI and Reddit datasets <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. The five baseline aggregators include  the effectiveness of incorporating graph structures, we also evaluate a two-layer fully-connected  <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref> (61.2)<ref type=\"foot\" target=\"#foot_0  hyperparameters for training. The training, validation, and testing splits are the same as that in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>. The micro-averaged F1 score is used t ith the previous state-of-the-art methods on inductive node classification. This includes GraphSAGE <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>, GAT <ref type=\"bibr\" target=\"#b27\">(V  can see steady improvement with larger sampling sizes, which is consistent with the observation in <ref type=\"bibr\" target=\"#b10\">(Hamilton et al., 2017a)</ref>.</p><p>Effect of output dimensions in  r\" target=\"#b15\">Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b8\">Fout et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b27\">Veli\u010dkovi d on either pooling over neighborhoods <ref type=\"bibr\" target=\"#b15\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a)</ref> or computing a weighted sum of the neig rget=\"#b7\">(Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b15\">Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b10\">Hamilton et al., 2017a)</ref>, while others integrated edge features . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e development of anti-viral therapy for the AIDS treatment <ref type=\"bibr\" target=\"#b21\">[25,</ref><ref type=\"bibr\" target=\"#b25\">29]</ref>.</p><p>More recently, DDX3 activity was found to be involve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in natural language tasks <ref type=\"bibr\" target=\"#b19\">[20]</ref> and image/video generation task <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" ta f><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>. Pumarola et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> generated facial expression conditioned on action units an or to generate consistent pixels along temporal axis.</p><p>As mentioned in Sec. 2, Pumarola et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> exploited a generator that regresses an attention mask and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tropy) estimator<ref type=\"foot\" target=\"#foot_0\">1</ref>  <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref>, and their variants. In contrast, a discriminative approach (DS) estimator <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and the minimax entropy (Entropy) estimator <ref type=\"bibr\" target=\"#b24\">[25]</ref>. For Entropy estimator, we use the implementation provided rmula\" target=\"#formula_1\">2</ref>).</p><p>Note that the likelihood-based cross-validation strategy <ref type=\"bibr\" target=\"#b24\">[25]</ref> is not suitable for CrowdSVM, because this strategy uses m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: udy of the distinguishing power of some GNN variants has been initiated. In two independent studies <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., ee-aware MPNNs that do use degree information. The former class of MPNNs covers the GNNs studied in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., s bounded by the WL algorithm. This result can be seen as a slight generalisation of the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al.,  and degree-aware MPNNs matches that of the WL algorithm.</p><p>For anonymous MPNNs related to GNNs <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., or short) is well understood. Indeed, as we will shortly see, it follows from two independent works <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., L , as is indicated in Figure <ref type=\"figure\" target=\"#fig_1\">1</ref>. Proposition 5.2 (Based on <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., ymous MPNN by using an injection h : A s \u2192 Q. What follows is in fact an adaptation of Lemma 5 from <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> itself based on <ref type=\"bibr\">[Zaheer et a gue that M anon is weaker than M WL . The proof is a trivial adaptation of the proofs of Lemma 2 in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=\"bibr\" target=\"#b1 t) w = (\u2113 \u2113 \u2113 (t) M ) w ,</formula><p>as desired.</p><p>We remark that we cannot use the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et x because the class M anon is more general than the class considered in those papers. The proofs in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et can be written in the form g (t) u\u2208NG(v) h (t) (\u2113 \u2113 \u2113 (t\u22121) u</formula><p>) , based on Lemma 5 from <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref>.</p><p>Suppose that \u03bd \u03bd \u03bd : V \u2192 A s0 . It now. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, multiple input spectrogram inverse (MISI) <ref type=\"bibr\" target=\"#b14\">[15]</ref>, ISSIR <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and consistent Wiener filtering <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  neural baselines such as CNN-sentence <ref type=\"bibr\" target=\"#b9\">[10]</ref> and Bi-LSTM-CNN-CRF <ref type=\"bibr\" target=\"#b13\">[14]</ref> is over 15.47% in terms of F1 score since CNN-sentence and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ric learning methods <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, showing that dimensionality does not significantly affect . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b19\">18]</ref> have been widely adopted to extract useful structural infor oss various domains, such as neuroscience <ref type=\"bibr\" target=\"#b31\">[30]</ref>, bioinformatics <ref type=\"bibr\" target=\"#b19\">[18]</ref>, and information networks <ref type=\"bibr\" target=\"#b6\">[5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  is passed into a bidirectional convolutional LSTM (CLSTM) <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> layer using a 1 \u00d7 3 filter, i.e. convolving only across the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng to a single point, triplet loss enables documents with the same identity to reside on a manifold <ref type=\"bibr\" target=\"#b28\">[20]</ref>, and at the same time maintain a distance from other docum. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> and universal (i.e. image-agnostic) attacks <ref type=\"bibr\"  ariety of works ranging from optimization based techniques <ref type=\"bibr\" target=\"#b41\">[42,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> to FGSM related techniques <ref type=\"bibr\" target=\"#b13\">[14 ng on data availability. Note that similar techniques of clamping the logits have also been used in <ref type=\"bibr\" target=\"#b4\">[5]</ref>, however, their motivation is to obtain minimum-magnitude (i '. \u00b1 0.6 55.4 \u00b1 1.0 70.8 \u00b1 1.5 55.2 \u00b1 2.2 89.1 \u00b1 0.3 75.9 \u00b1 0.9 87.9 \u00b1 0.5 70.8 \u00b1 1.1 78.2 \u00b1 0.9 66.<ref type=\"bibr\" target=\"#b4\">5</ref> \u00b1 1.3 LL 89.2 \u00b1 0.4 47.1 \u00b1 1.1 71.6 \u00b1 0.8 56.9 \u00b1 1.1 91.0 \u00b1 0.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>, crowdsourcing <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b51\">52]</ref>, or machine learning <ref type=\"bibr\" target=\"#b45\">[46,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tures across languages which can be mapped to the same space <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Authors in <ref type=\"bibr\" target=\"#b11\">[12]</ref> looke. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sic idea is very similar to instruction fetch in Multiscalar <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>: Multiscalar divides the sequential instruction stream into  i o n i s s i m i l a r t o t h a t u s e d b y Multiscalar <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>. When a fragment is renamed, the hardware determines which  fragments. This is similar to the idea of traces <ref type=\"bibr\" target=\"#b19\">[20]</ref> or tasks <ref type=\"bibr\" target=\"#b20\">[21]</ref>, except that fragments are completely general, whereas the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  achieved great successes for machine reading comprehension <ref type=\"bibr\">(Seo et al. 2016;</ref><ref type=\"bibr\" target=\"#b6\">Kumar et al. 2015;</ref><ref type=\"bibr\" target=\"#b11\">Sukhbaatar et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecifically, the features learned by the two baseline CNNs and the IL-CNN are visualized using t-SNE <ref type=\"bibr\" target=\"#b41\">[42]</ref>, which is widely employed to visualize high dimensional da. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and multi-page search <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The query change model <ref type=\"bibr\" target=\"#b36\">[37. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: raining graph to boost accuracy without affecting the inference graph, including auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</re nvergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers <ref type=\"bibr\" target=\"#b18\">[19]</ref>. However, auxiliary classifiers require specific new desig tei-c.org/ns/1.0\"><head n=\"3.1\">Generation of training graph</head><p>Similar to auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we add several new classifier heads into the original net. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: plemented in various ways: lightweight userlevel threading <ref type=\"bibr\" target=\"#b19\">[23,</ref><ref type=\"bibr\" target=\"#b35\">39]</ref>, closures or coroutines <ref type=\"bibr\" target=\"#b1\">[4,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  xml:id=\"formula_13\">V i k n (X n+1 , R n \u2212 1)</formula><p>|X n by definition. Other expressions in <ref type=\"bibr\" target=\"#b6\">(7)</ref> and ( <ref type=\"formula\" target=\"#formula_11\">8</ref>) are  e expected future Nash value V i k n+1 (X n+1 , R n \u2212 1)|X n throughout the game. Then according to <ref type=\"bibr\" target=\"#b6\">(7)</ref> and ( <ref type=\"formula\">8</ref>), it can be verified that  rom the conventional distributed resource allocation problems<ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng marginalized graph autoencoder. Its training objective is reconstructing the feature matrix. AGC <ref type=\"bibr\" target=\"#b37\">[38]</ref> exploits high-order graph convolution to filter node featu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nalysis to prove that our transductive model is a more general form than existing models (e.g., MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>). \u2022 Efficient and scalable learning algorithms for GATNE h beds networks with multiple views in a single collaborated embedding using attention mechanism. MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref> uses one common embedding and several additional embedding  r \u2208 R s\u00d7d is a trainable transformation matrix.</p><p>Connection with Previous Work. We choose MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>, a recent representative work for MHEN, as the base model   PMNE <ref type=\"bibr\" target=\"#b21\">[22]</ref>, MVE <ref type=\"bibr\" target=\"#b29\">[30]</ref>, MNE <ref type=\"bibr\" target=\"#b42\">[43]</ref>. We denote the three methods of PMNE as PMNE(n), PMNE(r) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  two-layer LSTMs and the parameters are estimated by MLE (note that the current state-of-the-art is <ref type=\"bibr\" target=\"#b10\">(Yang et al., 2018)</ref>). <ref type=\"bibr\" target=\"#b11\">Zaremba et. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ion in detail and then extrapolate to the entire execution <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. A major challenge in sampled evaluation however is to quic  is transferable across both hardware and software changes <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Functional warming  warms up the microarchitecture state b es not allow for software changes. Functional warming (FW) <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> does not incur any storage overhead, allows for software ch ect number of representative detailed regions that are evaluated in detail to then extrapolate from <ref type=\"bibr\" target=\"#b33\">[34]</ref>. The key challenge in sampling is to get (i) the correct a unctional fast-forwarding, checkpointing and virtualized fastforwarding. Functional fast-forwarding <ref type=\"bibr\" target=\"#b33\">[34]</ref> leverages functional simulation to get to the next represe a detailed warm-up using a small number of instructions (e.g., 30,000) prior to the detailed region <ref type=\"bibr\" target=\"#b33\">[34]</ref>. With this small amount of warming, only a small part of t instructions. Prior research shows that the highest accuracy is achieved for small detailed regions <ref type=\"bibr\" target=\"#b33\">[34]</ref>; larger detailed regions will likely make DeLorean even mo d to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS <ref type=\"bibr\" target=\"#b33\">[34]</ref>. \u2022 CoolSim: Randomized Statistical Warming (RSW) is employ ture state using all memory references between two consecutive detailed regions, which is very slow <ref type=\"bibr\" target=\"#b33\">[34]</ref>. Various approaches have been proposed to reduce the warm-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the relationship between the preference factors and the disentangled embeddings.</p><p>According to <ref type=\"bibr\" target=\"#b30\">(Yang et al., 2018)</ref>, the mutual information maximization can be. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) <ref type=\"bibr\" target=\"#b24\">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (B. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ei-c.org/ns/1.0\"><head n=\"4.1\">Stressor Event and Subject Dictionaries</head><p>The word embeddings <ref type=\"bibr\" target=\"#b8\">[Mikolov et al., 2013]</ref> have been found effective in estimating t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#fig_0\">1</ref>, these are mining-based methods inspired by previous relation extraction methods <ref type=\"bibr\" target=\"#b34\">(Ravichandran and Hovy, 2002)</ref>, and paraphrasing-based methods t ased Generation</head><p>Our first method is inspired by template-based relation extraction methods <ref type=\"bibr\" target=\"#b34\">(Ravichandran and Hovy, 2002)</ref>, which are based on the observati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: search efforts focus on speeding up the DPM pipeline <ref type=\"bibr\" target=\"#b29\">[30]</ref> [37] <ref type=\"bibr\" target=\"#b4\">[5]</ref>. They speed up HOG computation, use cascades, and push compu ead><p>Many research efforts in object detection focus on making standard detection pipelines fast. <ref type=\"bibr\" target=\"#b4\">[5]</ref> [37] <ref type=\"bibr\" target=\"#b29\">[30]</ref> [14] <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cale recommendation systems have adopted multi-task learning using Deep Neural Network (DNN) models <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>Researchers have reported multi-task learning models  commendations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, a text recommendation task is improved by sharing feature r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b3\">[4]</ref> and neural machine translation <ref type=\"bibr\" target=\"#b4\">[5]</ref> <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Due to lack of a pronunciation dictionary, most end-t t=\"#b15\">[16]</ref> to segment words into wordpiece which maximizes the language model probability. <ref type=\"bibr\" target=\"#b5\">[6]</ref> augments the training data with subword segmentation sampled. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng unlimited bandwidth, and DRAM banks having 50% of peak bandwidth (to account for cache overheads <ref type=\"bibr\" target=\"#b12\">[13]</ref>, bank conflicts, suboptimal scheduling, etc.). Though more. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b14\">Dosovitskiy et al., 2014;</ref><ref type=\"bibr\" target=\"#b41\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>.</p><p>In this work, we introduce a simple rget=\"#b33\">(Krizhevsky et al., 2012;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>, it has not been considered as a systemati br\" target=\"#b22\">H\u00e9naff et al., 2019;</ref><ref type=\"bibr\" target=\"#b24\">Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref>. However, it is not clear if the success o tation learning methods:</p><p>\u2022 DIM/AMDIM <ref type=\"bibr\" target=\"#b24\">(Hjelm et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019)</ref> achieve global-to-local/local-to-neighbor  gure\" target=\"#fig_0\">1</ref>), but it is also simpler, requiring neither specialized architectures <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et a ibr\" target=\"#b20\">(Zhang et al., 2016;</ref><ref type=\"bibr\" target=\"#b41\">Oord et al., 2018;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Kolesnikov e n is useful for self-supervised learning <ref type=\"bibr\" target=\"#b11\">(Doersch et al., 2015;</ref><ref type=\"bibr\" target=\"#b1\">Bachman et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">H\u00e9naff et al the default nonlinear projection with one additional hidden layer (and ReLU activation), similar to <ref type=\"bibr\" target=\"#b1\">Bachman et al. (2019)</ref>. We observe that a nonlinear projection is tch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM <ref type=\"bibr\" target=\"#b1\">(Bachman et al., 2019)</ref>, which achieves 91.2% with a model 25\u00d7 la. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref> and MT <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bib \">[29</ref></note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"6\" xml:id=\"foot_2\">]<ref type=\"bibr\" target=\"#b5\">6</ref> , we apply frequent casing for the IWSLT tasks while lowercase. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o path-based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, embedding-based methods <ref type=\"bibr\" target=\"#b8\">[9,< ) Path-based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> explore various patterns of connections among items in a KG. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ><p>To obtain typical workload pairs among these benchmarks, we refer to the balanced random method <ref type=\"bibr\" target=\"#b29\">[32]</ref> to select 48 pairs of benchmarks for experiments. Then we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a user pair can be matched, based on which we can update the matching scores of its neighbor pairs <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" targ ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. For example, SiGMa <ref type=\"bibr\" target=\"#b8\">[9]</ref> proposed an unsupervised method to iteratively propagate the airs together. For example, Lacoste et al. solved the problem by an unsupervised propagation method <ref type=\"bibr\" target=\"#b8\">[9]</ref>, where the similarities of the attributes in the method are  ure in SVM over all the neighbor pairs in a matched ego network as additional features.</p><p>SiGMa <ref type=\"bibr\" target=\"#b8\">[9]</ref>: begins from a seed set of user pairs (output by Name-Match . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </ref> show the potential to scale to millions of separately administered volunteer nodes, with CFS <ref type=\"bibr\" target=\"#b4\">[5]</ref> layering a read-only file system on top of such a highly dis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> to address the lack of highquality, large- eters et al., 2018)</ref>, GPT <ref type=\"bibr\" target=\"#b25\">(Radford et al., 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>, unsupervised pretraining of language mode s=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Methods</head><p>Background The BERT model architecture <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> is based on a multilayer bidirectional Tra ead n=\"3.3\">Pretrained BERT Variants</head><p>BERT-Base We use the pretrained weights for BERT-Base <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref> released with the original BERT code. <ref 9\">(Howard and Ruder, 2018)</ref> which is equivalent to the linear warmup followed by linear decay <ref type=\"bibr\" target=\"#b5\">(Devlin et al., 2019)</ref>. For each dataset and BERT variant, we pic h using AllenNLP <ref type=\"bibr\" target=\"#b8\">(Gardner et al., 2017)</ref>.</p><p>Casing We follow <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref> in using the cased models for NER and the u T</head><p>We mostly follow the same architecture, optimization, and hyperparameter choices used in <ref type=\"bibr\" target=\"#b5\">Devlin et al. (2019)</ref>. For text classification (i.e. CLS and REL). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: atasets, CNNs have achieved state-of-the-art results in SISR <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" tar ainly focus on designing a deeper or wider network structure <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b13\">13,</ref><ref type=\"bibr\" tar tion. Some loss functions have been widely used, such as L 2 <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b29\">29,</ref><ref type=\"bibr\" tar (SRCNN) for image SR, which achieves impressive performance. Later, Kim et al. designed deeper VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[13]</ref> with mo e-of-the-art CNN-based SR methods: SR-CNN [1], FSRCNN <ref type=\"bibr\" target=\"#b3\">[3]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref>, LapSRN <ref type=\"bibr\" target=\"#b14\">[14]</ref>, Mem-Net SRCNN <ref type=\"bibr\" target=\"#b2\">[2]</ref>, FSRCNN <ref type=\"bibr\" target=\"#b3\">[3]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref>, IR-CNN <ref type=\"bibr\" target=\"#b35\">[35]</ref>, SRMD <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ine and artificial neural networks have been studied in <ref type=\"bibr\" target=\"#b7\">[8]</ref> and <ref type=\"bibr\" target=\"#b8\">[9]</ref>. The automatic methods are objective without human intervent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6)</ref> partially motivate the ranking-based variant throught the importance sampling viewpoint of <ref type=\"bibr\" target=\"#b0\">Bengio and Sen\u00e9cal (2008)</ref>. However there are two critical differ 0\">Bengio and Sen\u00e9cal (2008)</ref>. However there are two critical differences: 1) the algorithm of <ref type=\"bibr\" target=\"#b0\">Bengio and Sen\u00e9cal (2008)</ref> does not lead to the same objective L . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: itation context-based paper summarization <ref type=\"bibr\" target=\"#b48\">(Teufel et al., 2006;</ref><ref type=\"bibr\" target=\"#b41\">Qazvinian and Radev, 2008;</ref><ref type=\"bibr\" target=\"#b11\">Cohan . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b0\">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type=\"bibr\" target=\"#b7\">[8]</ref>, CIFAR-100 <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and LFW . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ocks. And of these, there are two main candidates: variants of Mellor-Crummey and Scott (MCS) locks <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and variants of Craig, Landin, and Hagersten (CLH) locks . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fense strategies into consideration <ref type=\"bibr\">(Athalye et al., 2018b)</ref>.</p><p>Recently, <ref type=\"bibr\" target=\"#b13\">Shafahi et al. (2019)</ref> showed that, for two classes of data dist for general classifiers, and their relationship to some recent works in literature.</p><p>Recently, <ref type=\"bibr\" target=\"#b13\">Shafahi et al. (2019)</ref> shows that no classifier can achieve low . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ectures (e.g., NASNet, AmoebaNet) are not efficient for inference. Recent hardwareaware NAS methods <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019;</ref><ref type=\"bibr\" target=\"#b29\">Tan et al., 201 ers and skip the last N \u2212 D layers, rather than keeping any D layers as done in current NAS methods <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019;</ref><ref type=\"bibr\" target=\"#b32\">Wu et al., 2019 nd input image size<ref type=\"foot\" target=\"#foot_1\">2</ref> . We also build a latency lookup table <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref> on each target hardware platform to predict t forms (Figure <ref type=\"figure\" target=\"#fig_7\">7</ref>) using the ProxylessNAS architecture space <ref type=\"bibr\" target=\"#b3\">(Cai et al., 2019)</ref>. OFA consistently improves the trade-off betw ). It is impossible for previous NAS methods <ref type=\"bibr\" target=\"#b29\">(Tan et al., 2019;</ref><ref type=\"bibr\" target=\"#b3\">Cai et al., 2019)</ref> due to the prohibitive training cost.</p><p>Re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ustness for classical neural networks/robust training (e.g. <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>), we tackle various ertifiable robustness <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> providing guarantee ginal sample measured by, e.g., the infinity-norm or L2-norm <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, often e.g. \u03f5 &lt;  /p><p>For this work, specifically the class of methods based on convex relaxations are of relevance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. They construct a   the remaining layers, since the input to them is no longer binary, we adapt the bounds proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Generalized to the GNN we therefore obtain:</p><formula x. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b58\">59]</ref>, join cardinality reduction <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>, and index based fi ng at a time listing <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" tar  different size from 3 to 50 nodes similar to existing works <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b49\">50]</ref>. Iteratively, a new \u00d7 respectively. Also, it outperforms CFLMatch <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Tur-boIso <ref type=\"bibr\" target=\"#b16\">[17]</ref> by 2.7\u00d7 and 2.72\u00d7 to enumerate the first 1,024 embeddings. ve the smallest number of matches to minimize the number of embedding clusters. Similar to TurboIso <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we choose the root vertex</p><formula xml:id=\"formula_1\">  path first <ref type=\"bibr\" target=\"#b58\">[59]</ref>, locally optimized order for each exploration <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and dense region first <ref type=\"bibr\" target=\"#b3\">[4]< ts, adopting edge-ranked visit order <ref type=\"bibr\" target=\"#b52\">[53]</ref> or path-ranked order <ref type=\"bibr\" target=\"#b16\">[17]</ref> provided up to 34.5% speedup over using naive BFS matching  1, 0}}.</p><p>To list each embedding only once, we have combined the concepts proposed by TurboIso <ref type=\"bibr\" target=\"#b16\">[17]</ref> for finding NECequivalence group with ordering based symme t all the edges in the data graph can be its candidates. The space complexity of index for TurboIso <ref type=\"bibr\" target=\"#b16\">[17]</ref> and CFLMatch <ref type=\"bibr\" target=\"#b3\">[4]</ref> are O hich contains two versions of TurboIso. The first implementation, TurboIso, replicates the works on <ref type=\"bibr\" target=\"#b16\">[17]</ref> while the second version, Boosted-TurboIso, speeds up Turb irement by loading only a small portion of graph into memory at a time. On the other hand, TurboIso <ref type=\"bibr\" target=\"#b16\">[17]</ref> saves memory by serializing the auxiliary data creation an rsally best visit order and each filtering approach is good for only some datasets. Later, TurboIso <ref type=\"bibr\" target=\"#b16\">[17]</ref> exploits the vertex similarity on query graph to reduce th  obvious choice for the query tree in several existing works <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> because they represent the topology of the query graph accu od is different from existing subgraph enumeration solutions <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> because they only have auxiliary data structure equivalent . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. In contrast to triplet-based approaches, the classificatio ieval tasks have some success adopting classification loss <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Though classification-based training alleviates the need f e spatial locations <ref type=\"bibr\" target=\"#b25\">[26]</ref>, or partitioning the training classes <ref type=\"bibr\" target=\"#b28\">[29]</ref>. However, such ensembled embeddings trade off image retrie on contributions from the loss functions, we leave comparisons against methods that ensemble models <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, modify the featur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b4\">Fan et al., 2017;</ref><ref type=\"bibr\" target=\"#b25\">Scarton and Specia, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Nishihara et al., 2019)</ref> where a Sequence-to-Sequence (Seq2Seq)  =\"#b4\">(Fan et al., 2017)</ref>. <ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref> and <ref type=\"bibr\" target=\"#b17\">Nishihara et al. (2019)</ref> similarly showed that adding control to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hallenges above, we propose to model the attributed networks with graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b15\">[16]</ref>. GCN, which takes the topological structure and nodal attr se a new type of attributed network encoder inspired by the graph convolutional network (GCN) model <ref type=\"bibr\" target=\"#b15\">[16]</ref>. Specifically, GCN considers the high-order node proximity a particular layer, the convolution operation is D \u2212 1 2 A D \u2212 1 2 XW, and its complexity is O(mdh) <ref type=\"bibr\" target=\"#b15\">[16]</ref> as AX can be efficiently implemented using sparse-dense ma rning performance by considering neighbors of nodes that are multiple hops away. In particular, GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> takes the structure and attribute information as input, an  autoencoder architecture. Meanwhile, recent research advances on graph convolutional network (GCN) <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: global-history based predictor derived from PPM. PPM was originally introduced for text compression <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and it was used in <ref type=\"bibr\" target=\"#b1\">[2]</ref>  nd of the PCF, we build S * and sort sequences in decreasing potentials. Then we compute m(T ) from <ref type=\"bibr\" target=\"#b0\">(1)</ref>. Because of memory limitations on our machines, when |S| exc rting at this step, the content of T is allowed to change dynamically. We can no longer use formula <ref type=\"bibr\" target=\"#b0\">(1)</ref>, and so now we generate results by performing a second pass . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or the pre-trained word embedding will recognize it. Inspired by the character level language model <ref type=\"bibr\" target=\"#b4\">[Kim et al., 2016]</ref>, we combine the input wordconcept embedding w pe=\"bibr\" target=\"#b1\">[Hill et al., 2016]</ref> focus on learning the embedding of a phrase, while <ref type=\"bibr\" target=\"#b4\">[Palangi et al., 2016]</ref>, <ref type=\"bibr\">[Kalchbrenner et al., 2 \" target=\"#b4\">[Palangi et al., 2016]</ref>, <ref type=\"bibr\">[Kalchbrenner et al., 2014]</ref> and <ref type=\"bibr\" target=\"#b4\">[Le and Mikolov, 2014]</ref> focuses on learning the embedding of sent ollobert et al., 2011]</ref> first use CNN with pre-trained word embedding for text classification. <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref> further improves the performance by using multi-chan requency of each unigram. CNN. This method uses a one-layer CNN for text classification proposed by <ref type=\"bibr\" target=\"#b4\">[Kim, 2014]</ref>. It uses a multi-channel architecture for text embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: do not use gazetteers or any external labeled resources. The best score reported on this task is by <ref type=\"bibr\" target=\"#b26\">Luo et al. (2015)</ref>. They obtained a F 1 of 91.2 by jointly model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l in MT task. Recently, this model also begins to be used in ASR task, showing a decent performance <ref type=\"bibr\" target=\"#b20\">[22,</ref><ref type=\"bibr\" target=\"#b21\">23]</ref>. In this section, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: se approach <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2018)</ref> and its layer-dependent variant <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2018)</ref>. Specifically, GAT <ref type=\"bibr\" target on et al., 2017)</ref>, FastGCN <ref type=\"bibr\" target=\"#b2\">(Chen et al., 2018)</ref>, and AS-GCN <ref type=\"bibr\" target=\"#b13\">(Huang et al., 2018)</ref>. We name this category of approaches as Dr e <ref type=\"table\" target=\"#tab_1\">2</ref>; for the SOTA methods, we reuse the results reported in <ref type=\"bibr\" target=\"#b13\">Huang et al. (2018)</ref>.</p><p>We have these findings: (1) Clearly, ing the testing nodes are unseen for training. We apply the full-supervised training fashion used in<ref type=\"bibr\" target=\"#b13\">Huang et al. (2018)</ref> and<ref type=\"bibr\" target=\"#b2\">Chen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed somewhat implicit until residual networks <ref type=\"bibr\" target=\"#b5\">(He et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>) explicitly introduced a reparameterization of  ead><p>Since the advent of residual networks <ref type=\"bibr\" target=\"#b5\">(He et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>), most state-of-the-art networks for image clas d of a sequence of such residual blocks. In comparison with the full pre-activation architecture in <ref type=\"bibr\" target=\"#b6\">He et al. (2016)</ref>, we remove two batch normalization layers and o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: table\">3</ref> provides a brief description of the other two C\u00fe\u00fe benchmarks.</p><p>We use Pinpoints <ref type=\"bibr\" target=\"#b45\">[45]</ref> to select a representative simulation region for each benc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  datasets, where training and testing set share the same categories. Specifically, ResNet18 network <ref type=\"bibr\" target=\"#b14\">[15]</ref> is adopted as the backbone and the output embedding featur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and German vocabularies are generated using sentencepiece<ref type=\"foot\" target=\"#foot_1\">3</ref>  <ref type=\"bibr\" target=\"#b18\">(Kudo 2018)</ref> with a fixed size of 5k tokens. For Librispeech En-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tworks have achieved unprecedented performance in visual domains: for example, image classification <ref type=\"bibr\" target=\"#b17\">17</ref> , face recognition <ref type=\"bibr\" target=\"#b18\">18</ref> ,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b16\">[Nickel et al., 2012</ref><ref type=\"bibr\" target=\"#b23\">, Trouillon et al., 2016</ref><ref type=\"bibr\" target=\"#b3\">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, wi porate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> and ComplEx <ref type=\"bibr\" target=\"#b2 tion). We work with ComplEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> embeddings which have shown state of the #b23\">[Trouillon et al., 2016]</ref>, <ref type=\"bibr\">SimplE [Kazemi and Poole, 2018]</ref>, ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a dif Then all facts upto length 3 in the hierarchy of taxonomy were included.</p><p>FB15K-237: FB15K-237 <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref>, another popular benchmark does not have  compare them with Com-plEx <ref type=\"bibr\" target=\"#b23\">[Trouillon et al., 2016]</ref> and ConvE <ref type=\"bibr\" target=\"#b3\">[Dettmers et al., 2018]</ref>, two state-of-the-art KG embeddings meth g the same class balance, and use them as our validation and test split.</p><p>YAGO3-10: YAGO3-10 [ <ref type=\"bibr\" target=\"#b3\">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type=\"bibr\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ediction accuracy. Similar attention mechanisms have been proposed for natural image classification <ref type=\"bibr\" target=\"#b10\">[11]</ref> and captioning <ref type=\"bibr\" target=\"#b0\">[1]</ref> to  ntributions of this work can be summarised as follows: \u2022 We take the attention approach proposed in <ref type=\"bibr\" target=\"#b10\">[11]</ref> a step further by proposing grid-based gating that allows   intermediate space. In image captioning <ref type=\"bibr\" target=\"#b0\">[1]</ref> and classification <ref type=\"bibr\" target=\"#b10\">[11]</ref> tasks, the  softmax activation function is used to normali n. This results experimentally in better training convergence for the AG parameters. In contrast to <ref type=\"bibr\" target=\"#b10\">[11]</ref> we propose a grid-attention technique. In this case, gatin  the feature-maps and map them to lower dimensional space for the gating operation. As suggested in <ref type=\"bibr\" target=\"#b10\">[11]</ref>, low-level feature-maps, i.e. the first skip connections,  <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" ta [2,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and more recently applied to image classification <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. In <ref type=\"bib  was the top-performer in the ILSVRC 2017 image classification challenge. Self-attention techniques <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref> have been proposed tention is used in <ref type=\"bibr\" target=\"#b32\">[33]</ref> to capture long range dependencies. In <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> self-attention is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g future lightning occurrences. However, although extrapolationbased methods for weather nowcasting <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b2\">[3]</ref> can be migrated to  re sensitive to different dimensions are assembled to predict mobile events in the city. Shi et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> proposed convolutional LSTM (ConvLSTM) for precipitation now t\u22121 .</formula><p>The ConvLSTM in this paper does not include peephole connections, as mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The data first enters the CNN modules, where sequentially a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ity and its performance drops dramatically for similarity based tasks, e.g. nearest neighbor search <ref type=\"bibr\" target=\"#b45\">[46,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" ta s usually built on classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memorized features <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which has limited efficiency and discriminability. We pro  images and predefined noise signals, which constrains the distribution between raw data and noises <ref type=\"bibr\" target=\"#b45\">[46]</ref>. Bolztmann Machines (RBMs) <ref type=\"bibr\" target=\"#b23\"> discriminability. Softmax Embedding with Memory Bank. To improve the inferior efficiency, Wu et al. <ref type=\"bibr\" target=\"#b45\">[46]</ref> propose to set up a memory bank to store the instance feat tance feature rather than classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memory bank <ref type=\"bibr\" target=\"#b45\">[46]</ref>. To achieve the goal that features of the same instance un =\"bibr\" target=\"#b2\">[3]</ref> 67.6 Exemplar <ref type=\"bibr\" target=\"#b7\">[8]</ref> 74.5 NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80.8 NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80. </p ype=\"bibr\" target=\"#b7\">[8]</ref> 74.5 NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80.8 NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> 80. </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><h ead n=\"4.1.\">Experiments on Seen Testing Categories</head><p>We follow the experimental settings in <ref type=\"bibr\" target=\"#b45\">[46]</ref> to conduct the experiments on CIFAR-10 <ref type=\"bibr\" ta  ColorJitter, RandomHorizontalFlip) in PyTorch with default parameters are adopted.</p><p>Following <ref type=\"bibr\" target=\"#b45\">[46]</ref>, we adopt weighted kNN classifier to evaluate the performa  200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label <ref type=\"bibr\" target=\"#b45\">[46]</ref>   plar CNN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoft  type=\"bibr\" target=\"#b45\">[46]</ref>   plar CNN <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Triplet N <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and Triplet loss with and without hard mining. Triplet (ha and the margin parameter is set to 0.5. DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> represent the state-of-the-art unsupervised feature learni  classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which u hod outperforms it by 9.1%. Compared to NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref> and NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, which use memorized feature for optimizing, the proposed  target=\"#fig_4\">3</ref>. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while <ref type=\"bibr\" target=\"#b45\">[46]</ref> takes 25 epochs and <ref type=\"bibr\" target=\"#b7\">[8]</ref ance features rather than classifier weights <ref type=\"bibr\" target=\"#b7\">[8]</ref> or memory bank <ref type=\"bibr\" target=\"#b45\">[46]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  d images in ten The classifier is used to predict the label of test samples. We implement NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepClu ct the label of test samples. We implement NPSoftmax <ref type=\"bibr\" target=\"#b45\">[46]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> (c the best accuracy with both classifiers (kNN: 74.1%, Linear: 69.5%), which are much better than NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref> un  three state-of-the-art unsupervised methods (Exemplar <ref type=\"bibr\" target=\"#b7\">[8]</ref>, NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref> and DeepCluster <ref type=\"bibr\" target=\"#b2\">[3]</ref>) o n Table <ref type=\"table\">3</ref>.</p><p>Generally, the instance-wise feature learning methods (NCE <ref type=\"bibr\" target=\"#b45\">[46]</ref>, Examplar <ref type=\"bibr\" target=\"#b7\">[8]</ref>, Ours) o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: yed to exploit the properties of the background and reflection layers, including the sparsity prior <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>, the blur level di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es a relational graph convolutional network to link prediction task and entity classification task. <ref type=\"bibr\" target=\"#b35\">[36]</ref> propose a heterogeneous graph neural network model which c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b4\">[5]</ref>, Superthreaded <ref type=\"bibr\" target=\"#b19\">[20]</ref>, Trace Processors <ref type=\"bibr\" target=\"#b16\">[17]</ref> [21], Speculative Multithreaded <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tensions such as for example the use of residual connections <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, dense connections <ref type=\"bibr\" target=\"#b4\">[5]</ref> or b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> we favor this formulation over other variants <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. The dice loss is im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: chine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  assification, these perturbations cause the legitimate sample to be misclassified at inference time <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  , adversarial training which augments the training data of the classifier with adversarial examples <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow  xamples designed to fool the substitute often end up being misclassified by the targeted classifier <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b19\">Papernot e ch to defend against adversarial noise is to augment the training dataset with adversarial examples <ref type=\"bibr\" target=\"#b20\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b4\">Goodfellow . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ct and have high intersubject variability. To learn personalized models for each student, we follow <ref type=\"bibr\" target=\"#b3\">(Jaques et al., 2017)</ref> and use a Multitask approach which compris. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref ty N <ref type=\"bibr\" target=\"#b6\">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016</ref>) is unable to distinguish edges with different. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: .org/ns/1.0\"><head n=\"1.\">Introduction</head><p>Adversarial attacks to image classification systems <ref type=\"bibr\" target=\"#b19\">[20]</ref> add small perturbations to images that lead these systems . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  a bi-level optimization framework. Relatively few works consider meta-learning with GNNs. Meta-GNN <ref type=\"bibr\" target=\"#b58\">[59]</ref> uses meta-learning for few-shot node classification, and M. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: duced if the processor already supports the prediction of multiple conditional branches in parallel <ref type=\"bibr\" target=\"#b51\">[51]</ref>. The prediction logic can perform the calculation of VPCA . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rgence, we replace the ReLU activation function in the feed-forward network with gated linear units <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We empirically set the left context of every node in the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: TECTURES</head><p>Inspired by the deep residual model <ref type=\"bibr\" target=\"#b6\">[7]</ref>, RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and U-Net <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we p RCL) are performed with respect to the discrete time steps that are expressed according to the RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>. Let's consider the \ud835\udc65 \ud835\udc59 input sample in the \ud835\udc59 \ud835\udc61\u210e layer of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> directly predict all keypoints at first and assemble them i stimation results are obtained when person clusters are combined with labeled body parts. DeeperCut <ref type=\"bibr\" target=\"#b18\">[19]</ref> improves DeepCut <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ence (seq2seq) models. Sub-word units were used in seq2seq <ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref> and RNNT <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e of the sentiment implied in the images has received increasing research attention in recent years <ref type=\"bibr\" target=\"#b11\">[4]</ref><ref type=\"bibr\" target=\"#b12\">[5]</ref><ref type=\"bibr\" tar analysis have aimed to design effective visual features for training sentiment polarity classifiers <ref type=\"bibr\" target=\"#b11\">[4]</ref><ref type=\"bibr\" target=\"#b12\">[5]</ref><ref type=\"bibr\" tar ent works have started to analyze the sentiments of unconstrained real-world images on social media <ref type=\"bibr\" target=\"#b11\">[4]</ref><ref type=\"bibr\" target=\"#b12\">[5]</ref><ref type=\"bibr\" tar ain a sen-timent polarity classifier, color histogram and SIFT-based features of images are used in <ref type=\"bibr\" target=\"#b11\">[4]</ref>. In <ref type=\"bibr\" target=\"#b12\">[5]</ref>, emotion-relat ional methods, which exploit either visual or textual view: a low-level visual feature-based method <ref type=\"bibr\" target=\"#b11\">[4]</ref> (denoted as Low), a mid-level visual feature-based method < >[10]</ref> (denoted as SentiStrength<ref type=\"foot\" target=\"#foot_3\">3</ref> ). Note that for Low <ref type=\"bibr\" target=\"#b11\">[4]</ref>, we use the same Table <ref type=\"table\">2</ref>. Average a  classification accuracy of image sentiment polarity for 10 runs in each dataset. Note that for Low <ref type=\"bibr\" target=\"#b11\">[4]</ref>, we use the same visual feature set as those described in S iBank outputs.</p><p>Method Flickr dataset Instagram dataset Random 49.78 \u00b1 1.05% 50.06 \u00b1 1.09% Low <ref type=\"bibr\" target=\"#b11\">[4]</ref> 69.44 \u00b1 0.85% 67.16 \u00b1 1.28% SentiBank <ref type=\"bibr\" targ ifier.</p><p>In this paper, we exploit a linear SVM, which is also used in the conventional methods <ref type=\"bibr\" target=\"#b11\">[4,</ref><ref type=\"bibr\" target=\"#b12\">5]</ref>. Note that although   Note that although this paper focuses on binary classification as well as the conventional methods <ref type=\"bibr\" target=\"#b11\">[4,</ref><ref type=\"bibr\" target=\"#b12\">5]</ref>, our method can be e hods exploited pseudo sentiment labels using the automatic annotation algorithm based on image tags <ref type=\"bibr\" target=\"#b11\">[4,</ref><ref type=\"bibr\" target=\"#b14\">7]</ref>, but it is unreliabl et. Since this experiment targets on the binary classification problem following the previous works <ref type=\"bibr\" target=\"#b11\">[4,</ref><ref type=\"bibr\" target=\"#b12\">5]</ref>, we discarded the im. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b19\">[20]</ref> and sparse representation <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>. While these approaches are effective, the extracted featur  target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, we use 91 images from Yang et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> and 200 images from Berkeley Segmentation Dataset (BSD) <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nting with the binary classification subset of their tasks, and on models with an LSTM architecture <ref type=\"bibr\" target=\"#b2\">(Hochreiter and Schmidhuber, 1997)</ref>, the only one the authors mak. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b34\">[36]</ref>, BSD100 <ref type=\"bibr\" target=\"#b17\">[18]</ref> and Urban100 <ref type=\"bibr\" target=\"#b9\">[10]</ref>, which have 5, 14, 100 and 100 images respectively.</p></di M) <ref type=\"foot\" target=\"#foot_0\">1</ref> . Especially on the recent difficult Ur-ban100 dataset <ref type=\"bibr\" target=\"#b9\">[10]</ref>, DRRN significantly advances the state of the art, with the ted in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref> RFL <ref type=\"bibr\" target=\"#b21\">[23]</ref>   the results eported in Tab. Qualitative comparisons among SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRRN ar et5 and Set14. Shallow (non-DL) models include A+ <ref type=\"bibr\" target=\"#b29\">[31]</ref>, SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, RFL <ref type=\"bibr\" target=\"#b21\">[23]</ref>, NBSRF <ref  2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and non-DL <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\">20,</ref><ref type=\"bibr\" target=\"#b21\">23] L <ref type=\"bibr\" target=\"#b21\">[23]</ref>   the results of <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\">20,</ref><ref type=\"bibr\" target=\"#b21\">23]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: aborative Filtering (CF), a term coined by the developers of the first recommender system -Tapestry <ref type=\"bibr\" target=\"#b7\">[8]</ref>. CF analyzes relationships between users and interdependenci. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: makeup where even small errors in alignment can drive the rendered effect into the \"uncanny valley\" <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We built a lipstick rendering solution (Figure <ref type=\"f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nd pursued in natural language processing <ref type=\"bibr\" target=\"#b9\">[10]</ref>, computer vision <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and other domains. To date, the most powerful solution is e how to define (dis)similar instances.</p><p>Q2: Define (dis)similar instances. In computer vision <ref type=\"bibr\" target=\"#b16\">[17]</ref>, two random data augmentations (e.g., random crop, random  d by the graph encoder, the final d-dimensional output vectors are then normalized by their L2-Norm <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>A running example. We illustrate a running example  ffectively build and maintain the dictionary, such as end-to-end (E2E) and momentum contrast (MoCo) <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We discuss the two strategies as follows.</p><p>E2E sampl propagation. The parameters of f k (denoted by \u03b8 k ) are not updated by gradient descent. He et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> propose a momentum-based update rule for \u03b8 k . More formal  the dictionary, such as memory bank <ref type=\"bibr\" target=\"#b58\">[59]</ref>. Recently, He et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> show that MoCo is a more effective option than memory bank >Contrastive loss mechanisms. The common belief is that MoCo has stronger expression power than E2E <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and a larger dictionary size K always helps. We also obse r, the effect of a large dictionary size is not as significant as reported in computer vision tasks <ref type=\"bibr\" target=\"#b16\">[17]</ref>. For example, MoCo (K = 16384) merely outperforms MoCo (K   in Table <ref type=\"table\" target=\"#tab_6\">5</ref> in the Appendix. Momentum. As mentioned in MoCo <ref type=\"bibr\" target=\"#b16\">[17]</ref>, momentum m plays a subtle role in learning high-quality r tasets. For US-Airport, the best performance is reached by m = 0.999, which is the desired value in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, showing that building a consistent dictionary is importan  brings better performance. Moreover, we do not observe the \"training loss oscillation\" reported in <ref type=\"bibr\" target=\"#b16\">[17]</ref> when setting m = 0. GCC (MoCo) converges well, but the acc ings.</p><p>In computer vision, a large collection of work <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" tar  objectives for graph structured data. Inspired by the recent success of contrastive learning in CV <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref> and NLP <ref type= ats each instance as a distinct class of its own and learns to discriminate between these instances <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b58\">59]</ref>. The promise is th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecently, a novel density based clustering method, named Fast search-and-find of Density Peaks (FDP) <ref type=\"bibr\" target=\"#b32\">[33]</ref> , was proposed. This algorithm assumes that cluster center iguez and Laio proposed a novel density-based clustering method by finding density peaks called FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> . FDP discovers clusters by a two-phase process. First, lo  SNN <ref type=\"bibr\" target=\"#b7\">[8]</ref> , KNNC <ref type=\"bibr\" target=\"#b39\">[40]</ref> , FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> , 3DC <ref type=\"bibr\" target=\"#b22\">[23]</ref> , STClu <r rget=\"#b7\">[8]</ref> . KNNC <ref type=\"bibr\" target=\"#b39\">[40]</ref> (ii) No other parameters. FDP <ref type=\"bibr\" target=\"#b32\">[33]</ref> (i)</p><p>The objects with top C t gamma values are chosen object pairs. We vary \u03b2 value in {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} according to the recommendation in <ref type=\"bibr\" target=\"#b32\">[33]</ref> .</p><p>(ii) Density based on K nearest neighbors. The par ch cluster. The statistical error of the estimated density on such a small set of pictures is large <ref type=\"bibr\" target=\"#b32\">[33]</ref> . Therefore, for datasets consisting of clusters with few . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to recommender models involves several challenges: (1) Implicit user feedback is extremely sparse. <ref type=\"bibr\" target=\"#b1\">(2)</ref> As users only provide positive feedback in implicit datasets. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: omising solution for mining user preference over items in various online services such as Ecommerce <ref type=\"bibr\" target=\"#b28\">[29]</ref>, news portals <ref type=\"bibr\" target=\"#b31\">[32]</ref> an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ce and is not structured for human communication (e.g., unlike words).</p><p>Several recent studies <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" ta  be used with various pretext tasks. In this paper, we follow a simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" ta 8\">[29]</ref>. Contrastive learning is at the core of several recent works on unsupervised learning <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" ta </ref>. Overall, all three mechanisms benefit from a larger K. A similar trend has been observed in <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> under the memory b t tasks can be based on some form of contrastive loss functions. The instance discrimination method <ref type=\"bibr\" target=\"#b60\">[61]</ref> is related to the exemplar-based task <ref type=\"bibr\" tar +/\u03c4 ) K i=0 exp(q\u2022ki/\u03c4 )<label>(1)</label></formula><p>where \u03c4 is a temperature hyper-parameter per <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The sum is over one positive and K negative samples. Intu these networks to downstream tasks.</p><p>Another mechanism is the memory bank approach proposed by <ref type=\"bibr\" target=\"#b60\">[61]</ref> (Figure <ref type=\"figure\">2b</ref>). A memory bank consis ver the past epoch and thus are less consistent. A momentum update is adopted on the memory bank in <ref type=\"bibr\" target=\"#b60\">[61]</ref>. Its momentum update is on the representations of the same igning a new pretext task, we use a simple one mainly following the instance discrimination task in <ref type=\"bibr\" target=\"#b60\">[61]</ref>, to which some recent works <ref type=\"bibr\" target=\"#b62\" =\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> are related.</p><p>Following <ref type=\"bibr\" target=\"#b60\">[61]</ref>, we consider a query and a key as a positive pair if they  ose last fully-connected layer (after global average pooling) has a fixed-dimensional output (128-D <ref type=\"bibr\" target=\"#b60\">[61]</ref>). This output vector is normalized by its L2-norm <ref typ  (128-D <ref type=\"bibr\" target=\"#b60\">[61]</ref>). This output vector is normalized by its L2-norm <ref type=\"bibr\" target=\"#b60\">[61]</ref>. This is the representation of the query or key. The tempe  or key. The temperature \u03c4 in Eqn.( <ref type=\"formula\" target=\"#formula_0\">1</ref>) is set as 0.07 <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The data augmentation setting follows <ref type=\"bibr\" ta f>) is set as 0.07 <ref type=\"bibr\" target=\"#b60\">[61]</ref>. The data augmentation setting follows <ref type=\"bibr\" target=\"#b60\">[61]</ref>: a 224\u00d7224-pixel crop is taken from a randomly resized ima ate of 0.03. We train for 200 epochs with the learning rate multiplied by 0.1 at 120 and 160 epochs <ref type=\"bibr\" target=\"#b60\">[61]</ref>, taking \u223c53 hours training ResNet-50. For IG-1B, we use a  ecause the positive key is in the same mini-batch). The network is ResNet-50.</p><p>The memory bank <ref type=\"bibr\" target=\"#b60\">[61]</ref> mechanism can support a larger dictionary size. But it is   We hope an advanced pretext task will improve this. Beyond the simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61]</ref>, it is possible to adopt MoCo for pretext tasks like maske 1\">Here 58.0% is with InfoNCE and K=65536. We reproduce 54.3% when using NCE and K=4096 (the same as<ref type=\"bibr\" target=\"#b60\">[61]</ref>), close to 54.0% in<ref type=\"bibr\" target=\"#b60\">[61]</re  when using NCE and K=4096 (the same as<ref type=\"bibr\" target=\"#b60\">[61]</ref>), close to 54.0% in<ref type=\"bibr\" target=\"#b60\">[61]</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place sed on other forms <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, such as margin-bas specific pretext task. The input x q and x k can be images <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, patches <ref type= backbone, by default used in existing ResNet-based results <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref> and the trace cache architecture as proposed by Rotenberg, h prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref>. The branch prediction mechanism is a fully autonomous eng ion cache is then driven by the requests stored in the FTQ.</p><p>Another important contribution of <ref type=\"bibr\" target=\"#b29\">[30]</ref> is the Fetch Target Buffer (FTB). It extends the BTB by al ssibly containing multiple basic blocks.</p><p>The use of an FTQ is not novel, it was introduced in <ref type=\"bibr\" target=\"#b29\">[30]</ref>. It decouples the branch prediction from the memory access .0\"><head n=\"3.3.\">Fetch target queue</head><p>Following the proposal of Reinman, Austin and Calder <ref type=\"bibr\" target=\"#b29\">[30]</ref> we have decoupled the branch prediction stage from the ins ream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture <ref type=\"bibr\" target=\"#b29\">[30]</ref> using a perceptron branch predictor <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ble research attention <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. State-of-the-art GCNs usually follow a \"message-passing\" f ion of the Laplacian matrix is avoided, thus reducing the overall time complexity. Kipf and Welling <ref type=\"bibr\" target=\"#b17\">[18]</ref> further propose to simplify the graph convolution using on  GCN methods have been proposed, here we focus on a representative one proposed by Kipf and Welling <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Here, the (l + 1) t h convolutional layer is defined as:< istributions by using our Gaussian-based Graph Convolutions.</p><p>Following the original GCN model <ref type=\"bibr\" target=\"#b17\">[18]</ref>, we also impose L 2 regularization on parameters of the fi To evaluate the robustness of RGCN, we compare it with two state-of-the-art GCN models:</p><p>\u2022 GCN <ref type=\"bibr\" target=\"#b17\">[18]</ref>: As introduced in Section 3.2 , this is the original GCN m  methods is evaluated on a separate test set of 1000 labels. We adopt the same dataset splits as in <ref type=\"bibr\" target=\"#b17\">[18]</ref> and report the average results of 10 runs. In experiments, ectiveness of our proposed method, we adopt three citation networks commonly used in previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>: Cora, Citeseer an \"table\" target=\"#tab_0\">1</ref>.</p><p>We closely follow the experimental setting in previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Specifically, we  . In experiments, we set the number of layers as two for all methods as suggested by previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. For GCN and RGCN,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b11\">[12]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref> are implemented on a static collection extracted from DLs, rity metric. Then they use affinity propagation clustering algorithm to group result into clusters. <ref type=\"bibr\" target=\"#b12\">[13]</ref> introduce a pairwise factor graph model which can be exten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: academic communities <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" tar /head><p>GCNs are showing great potential in various tasks <ref type=\"bibr\" target=\"#b16\">[16,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" tar ted by recursively aggregating and transforming the representation vectors of its neighbor vertices <ref type=\"bibr\" target=\"#b18\">[18,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" ta to sample a subset from the neighbor vertices of each vertex <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b18\">18]</ref> as the new neighbors, specifically,</p><formula xml:id=\"for ling to alleviate receptive field expansion that effectively trades off accuracy and execution time <ref type=\"bibr\" target=\"#b18\">[18]</ref>. It is formulated as</p><formula xml:id=\"formula_4\">a k v  ing preprocessing <ref type=\"bibr\" target=\"#b20\">[20]</ref> or with random selection during runtime <ref type=\"bibr\" target=\"#b18\">[18]</ref>. Aggregation aggregates the features from its 1-hop neighb he execution time breakdown of GCN (GCN) <ref type=\"bibr\" target=\"#b25\">[25]</ref>, GraphSage (GSC) <ref type=\"bibr\" target=\"#b18\">[18]</ref>, and GINConv (GIN) <ref type=\"bibr\" target=\"#b39\">[39]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d to anomaly detection which aims to find patterns in data that do not conform to expected behavior <ref type=\"bibr\" target=\"#b8\">[9]</ref>. In the anomaly detection, the most related line of research. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the entities due to their typeagnostic nature <ref type=\"bibr\" target=\"#b25\">[Xie et al., 2016</ref><ref type=\"bibr\" target=\"#b7\">, Jain et al., 2018]</ref>. Since PSL-KGI is able to predict entity ty sion.</p><p>(ii) Explicit type supervised models also outperform the implict type supervised models <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>.</p><p>The margin of improvement is large wh  on the use of ontological rules (exemplified by PSL-KGI) and embeddings (we use ComplEx, ConvE and <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>). Rule induction methods are orthogonal to o  target=\"#b3\">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a different approach <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> propose extending standard KG embeddings wit es for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> as follows:</p><p>Instead of just using the  are our explicitly supervised TypeE-X methods with the implicitly supervised embeddings proposed by <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>.</p><p>\u2022 In Section 6.3, we analyse how our  e supervision with the unsupervised type-compatible embeddings-based method proposed by Jain et al. <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>. As these results indicate, while explicitly nificantly improves the relation scores, improving weighted F1 up to 18% (over NELL).</p><p>Dataset <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>  </p></div> <div xmlns=\"http://www.tei-c.org ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx <ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref>. We showed their performance on existing dat d>Table 7 :</head><label>7</label><figDesc>Weighted F1 scores on relation triples in the test set by<ref type=\"bibr\" target=\"#b7\">[Jain et al., 2018]</ref> and TypeE-ComplEx.Anecdotes. Looking at the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ate a variable number of features to a fixed-length vector. Fisher Vectors were first introduced in <ref type=\"bibr\" target=\"#b15\">(Jaakkola and Haussler 1999)</ref> to combine the advantages of gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ref type=\"bibr\" target=\"#b13\">[14]</ref> of machine translation and the neural aggregation networks <ref type=\"bibr\" target=\"#b14\">[15]</ref> of video face recognition, we propose the Frame Attention . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . (2015)</ref> apply a character-level CNN for text classification and achieve competitive results. <ref type=\"bibr\" target=\"#b22\">Socher et al. (2013)</ref>   explore the structure of a sentence and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l models on optical flow images and perform late fusion akin to the two-stream hypothesis following <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>. The performance o ligned faces as inputs, give a competitive result. Also, optical flow of faces are tested following <ref type=\"bibr\" target=\"#b20\">[21]</ref>, but no improvements are obtained for emotion classificati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, patterns for ORB extensibility <ref type=\"bibr\" target=\"#b5\">[6]</ref> and ORB performance <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In this section, we compare our work on TAO's RT-CORBA th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ingDB dataset contains 39 747 positive examples and 31 218 negative examples from a public database <ref type=\"bibr\" target=\"#b10\">(Gilson et al., 2016)</ref>. The training, valid and test sets of Bin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b28\">Kovaleva et al., 2019;</ref><ref type=\"bibr\" target=\"#b22\">Hoover et al., 2019;</ref><ref type=\"bibr\" target=\"#b63\">Vig, 2019)</ref>. Recent work has begun to analyze attention in Trans. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed representation learning, has been proven to be successful techniques contributing to the success <ref type=\"bibr\" target=\"#b1\">[2]</ref>. In essence, embedding is a way to represent a sparse vector. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: abulous semantic segmentation models such as <ref type=\"bibr\">FCN [Long et al., 2015]</ref>, SegNet <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref>, <ref type=\"bibr\">DeepLab-v3 [Chen work is based on the classic encoderdecoder network architecture without the fully-connected layers <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref> and we improve it by adding the re semantic segmentation networks -FCN <ref type=\"bibr\" target=\"#b7\">[Long et al., 2015]</ref>, SegNet <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017]</ref>, and DeepLab-V3 <ref type=\"bibr\" t r network with that of <ref type=\"bibr\">FCN-32s, FCN-16s, FCN-8s [Long et al., 2015]</ref>, Seg-Net <ref type=\"bibr\" target=\"#b0\">[Badrinarayanan et al., 2017], and</ref><ref type=\"bibr\">DeepLab-v3 [C. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"bibr\" target=\"#b46\">[47]</ref> network (trained on the DIVerse 2K resolution image (DIV2K) dataset <ref type=\"bibr\" target=\"#b47\">[48]</ref>), which uses a hierarchy of such residual blocks. While ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Normalization (GN) as a simple alternative to BN. We notice that many classical features like SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref> and HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref> are grou ><p>The channels of visual representations are not entirely independent. Classical features of SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and GIST <  more abstract and their behaviors are not as intuitive. However, in addition to orientations (SIFT <ref type=\"bibr\" target=\"#b13\">[14]</ref>, HOG <ref type=\"bibr\" target=\"#b14\">[15]</ref>, or <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  al <ref type=\"bibr\" target=\"#b35\">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref type=\"bibr\" target=\"#b15\">[16]</ref>. As described in Algorithm 1, for the r -th document \u03c0 r ,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mentioning branch predictor warmup is by Haskins and Conte <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> in which they propose memory reference reuse latency (MRRL) mup length per sampling unit. (Note that the MRRL approach <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> corresponds to a zero BHM history length.) We further obser. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ble to DT-ORS-Net applications. To this end, the two-player Markov stopping game (MSG) developed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> can serve as a good starting point, which extends the clas e theoretic setting so as to handle the potential conflicts between the two players.</p><p>However, <ref type=\"bibr\" target=\"#b16\">[17]</ref> does not provide a systematic method to deal with a genera  the optimal strategy for each player in such situations, the two-player MSG framework developed in <ref type=\"bibr\" target=\"#b16\">[17]</ref> may serve as a basis. Particularly, in the two-player MSG, yer. However, a systematic method for handling a general number of players in a MSG is missing from <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Considering this, a general M-MSG is proposed in the next  needed. Particularly, when two players coexist in the MSG, the randomized stopping time is used in <ref type=\"bibr\" target=\"#b16\">[17]</ref> to deal with the potential competition from the other play ynamism in the number of players as the game evolving, the concept of selection time is proposed in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, constructing the selection time essentially requ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hms to exploit the relationships between the entities. Graph Convolutional (Neural) Networks (GCNs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ he similarity scores for user-item pairs. With the success of graph (convolutional) neural networks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  has made abstractive summarization viable <ref type=\"bibr\" target=\"#b3\">(Chopra et al., 2016;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016;</ref><ref type=\"bibr\" target=\"#b20\">Rush et a arget=\"#b7\">Gulcehre et al., 2016;</ref><ref type=\"bibr\" target=\"#b15\">Miao and Blunsom, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016;</ref><ref type=\"bibr\" target=\"#b28\">Zeng et a cently-introduced CNN/ Daily Mail dataset <ref type=\"bibr\" target=\"#b8\">(Hermann et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016)</ref>, which contains news articles (39 sente head><p>We use the CNN/Daily Mail dataset <ref type=\"bibr\" target=\"#b8\">(Hermann et al., 2015;</ref><ref type=\"bibr\" target=\"#b17\">Nallapati et al., 2016)</ref>, which contains online news articles (7 ad n=\"2.1\">Sequence-to-sequence attentional model</head><p>Our baseline model is similar to that of <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>, and is depicted in Figure <ref type=\"f e on those datasets.</p><p>However, large-scale datasets for summarization of longer text are rare. <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> adapted the DeepMind question-answering  considerably different from that of <ref type=\"bibr\" target=\"#b7\">Gulcehre et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>. Those works train their pointer compon with multi-sentence summaries (3.75 sentences or 56 tokens on average). We used scripts supplied by <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> to obtain the same version of the the d and b ptr in equation 8), and coverage adds 512 extra parameters (w c in equation 11).</p><p>Unlike <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref>, we do not pretrain the word embeddings is (+1.1 ROUGE-1, +2.0 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> by (+4.07 ROUGE-1, +3.98 ROUGE-2, +3.73  scene. (...) Summary: more questions than answers emerge in controversial s.c. police shooting. of <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016)</ref> by several ROUGE points. Despite the br g Representations <ref type=\"bibr\" target=\"#b24\">(Takase et al., 2016)</ref>, hierarchical networks <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>, variational autoencoders <ref type=\"b nique that has been applied to <ref type=\"bibr\">NMT (Sankaran et al., 2016)</ref> and summarization <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>. In this approach, each attention dist rget=\"#tab_2\">1</ref>), compared to the smaller boost given by temporal attention for the same task <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref>.</p></div> <div xmlns=\"http://www.tei- he first three sentences of the article as a summary), and compare to the only existing abstractive <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016)</ref> and extractive <ref type=\"bibr\" target training pairs, 13,368 validation pairs and 11,490 test pairs. Both the dataset's published results <ref type=\"bibr\" target=\"#b17\">(Nallapati et al., 2016</ref><ref type=\"bibr\" target=\"#b16\">(Nallapat le online. <ref type=\"foot\" target=\"#foot_5\">6</ref>Given that we generate plain-text summaries but <ref type=\"bibr\" target=\"#b17\">Nallapati et al. (2016;</ref><ref type=\"bibr\" target=\"#b16\">2017)</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: unknown features of the language <ref type=\"bibr\" target=\"#b10\">(Daum\u00e9 III and Campbell, 2007;</ref><ref type=\"bibr\" target=\"#b36\">Takamura et al., 2016;</ref><ref type=\"bibr\" target=\"#b5\">Coke et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b6\">[8,</ref><ref type=\"bibr\" target=\"#b12\">14,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref>, most have taken user demands as a given input. Yet user de rget=\"#b6\">[8,</ref><ref type=\"bibr\" target=\"#b12\">14,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b31\">33]</ref>. Others incorporate pricing considerations, e.g., dynamical. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: roaches for representation learning of graphs <ref type=\"bibr\" target=\"#b23\">(Li et al., 2016;</ref><ref type=\"bibr\" target=\"#b13\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b21\">Kipf &amp arget=\"#b6\">Defferrard et al., 2016;</ref><ref type=\"bibr\" target=\"#b8\">Duvenaud et al., 2015;</ref><ref type=\"bibr\" target=\"#b13\">Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b19\">Kearnes e erage via attention <ref type=\"bibr\" target=\"#b34\">(Velickovic et al., 2018)</ref> and LSTM pooling <ref type=\"bibr\" target=\"#b13\">(Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b24\">Murphy e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  two major end-to-end ASR implementations based on both connectionist temporal classification (CTC) <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: soning dataset where some existing approaches <ref type=\"bibr\" target=\"#b12\">(Hu et al., 2017;</ref><ref type=\"bibr\" target=\"#b22\">Perez et al., 2018)</ref> fail, and the SotA method is 'MaxEnt' in <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .org/ns/1.0\"><head n=\"1.\">Introduction</head><p>Adversarial attacks to image classification systems <ref type=\"bibr\" target=\"#b19\">[20]</ref> add small perturbations to images that lead these systems . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b6\">7]</ref>. This important finding attracts great interests in edge partitioning recently <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target xisting partitioners, METIS gives the lowest replication factor which is consistent with literature <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. However, METIS runs titioning algorithm METIS <ref type=\"bibr\" target=\"#b7\">[8]</ref> is extended for edge partitioning <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which makes full access to the graph structure by partition alanced if max i \u2208[p] {|E i |} \u2264 \u2308 \u03b1 |E | /p\u2309.<label>(1)</label></formula><p>The replication factor <ref type=\"bibr\" target=\"#b2\">[3]</ref> of a partitioning is defined as</p><formula xml:id=\"formula_ <head n=\"2.3\">NP-Hardness</head><p>The p-edge partitioning problem has been proved to be NP-hard in <ref type=\"bibr\" target=\"#b2\">[3]</ref> when p grows with n = |V |. To our best knowledge, it has no weight. One can turn a vertex-partitioner into an edge-partitioner while preserving its performance <ref type=\"bibr\" target=\"#b2\">[3]</ref>. To transform METIS to an edge-partitioner, we first call ME </table></figure> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">In<ref type=\"bibr\" target=\"#b2\">[3]</ref>, the NP-hardness is proved by a reduction from 3-partition p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: c.org/ns/1.0\"><head n=\"4.2.2\">Predicting Performance in Solo-Mode.</head><p>Referring to prior work <ref type=\"bibr\" target=\"#b11\">[12]</ref>, we design shadow solo-cycle accounting (SSCA) approach to e prediction method of QoSMT is inspired by PTA. PTA uses MLP correction to achieve higher accuracy <ref type=\"bibr\" target=\"#b11\">[12]</ref>. However, we can not get an application's MLP without offl SMT throughput and fairness, but they did not take performance control into account. Eyerman et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposed the per-thread cycle accounting (PTA) mechanism t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b15\">Ezzat et al. 2002;</ref><ref type=\"bibr\" target=\"#b15\">Fan et al. 2016;</ref><ref type=\"bibr\" target=\"#b27\">Liu and Ostermann 2011;</ref><ref type=\"bibr\" target=\"#b48\">Wang and  in emotion synthesis, covering several methods <ref type=\"bibr\" target=\"#b4\">[Cao et al. 2005;</ref><ref type=\"bibr\" target=\"#b27\">Liu and Ostermann 2011;</ref><ref type=\"bibr\" target=\"#b33\">Melenchon. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings and the softmax matrix are usually shared in many sequence generation tasks for better accuracy<ref type=\"bibr\" target=\"#b28\">[29]</ref>.the second phase, we also add the unpaired speech whose sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  first content-hash-based block server. In fact, bsrv's design takes its inspiration from the Venti <ref type=\"bibr\" target=\"#b17\">[19]</ref> block server. Venti appends variable-sized blocks sequenti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sion reduction occurs in the recurrent convolutional neural networks used for semantic segmentation <ref type=\"bibr\" target=\"#b21\">[22]</ref>. As SR methods predict full-sized images, dimension reduct. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Term Memory (LSTM) <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b13\">13,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b15\">15]</ref>. For example, a mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ommendation algorithms <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, and propose a more general framework that combines both co stateof-the-art models <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> are special cases of our framework (e.g. using MSE-loss/Log rve that the current recommendation models based on MSE-loss <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> can be improved by others such as SG-loss and pairwise loss bsumes existing models <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>Stochastic Gradient Descent <ref type=\"bibr\" target= id=\"formula_1\">-(u,v )\u2208D log \u03c3 (f T u g v ) + \u03bbE v \u2032 \u223cPn log \u03c3 (\u2212f T u g v \u2032 )</formula><p>MSE-loss <ref type=\"bibr\" target=\"#b29\">[30]</ref>:</p><formula xml:id=\"formula_2\">(u,v )\u2208D ( r + uv \u2212 f T u  have been applied to such problems in many existing work. In <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, mean square loss ( ng and neural networks <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>. <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral machine translation (NMT) for text simplification, using a sequence-to-sequence (Seq2Seq) model <ref type=\"bibr\" target=\"#b34\">(Sutskever et al., 2014)</ref>. <ref type=\"bibr\" target=\"#b39\">Zhang . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ccess, to generate a speculative value that does not necessarily exhibit value locality (e.g., DLVP <ref type=\"bibr\" target=\"#b2\">[3]</ref>). While value predictors can generate speculative results fo rk has shown that load-only predictors are most efficient with a modest hardware budget (e.g., 8KB) <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>In this stud r\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref> Context Address Prediction (CAP) <ref type=\"bibr\" target=\"#b2\">[3]</ref> one another. We found that no individual predictor is strict e focus only on predicting load values since that is most effective with limited hardware resources <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xml sm needed to communicate the predicted values from the value-predicted producers to their consumers <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Consumers of the load can use the prediction by reading the s practical implementations of value prediction, we encourage the readers to visit prior art papers <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" t about the baseline ISA, microarchitecture, and storage constraints (Sheikh reports similar findings <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b26\">[27]</ref>).</p><p>In all of \"#b7\">[8]</ref>, and subsequent work confirmed the same is true for load instructions in particular <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p><p>Our implemen an directly generating values from We use the state-of-the-art DLVP predictor as a reference design <ref type=\"bibr\" target=\"#b2\">[3]</ref>. The predictor consists of one tagged table indexed by a has. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: usal model which could measure this causal relationship i.e. Linear Structual Equation Models (SEM) <ref type=\"bibr\" target=\"#b20\">(Shimizu et al., 2006)</ref>. Existing methods for disentangled repre et=\"#b6\">(Hoyer et al., 2009;</ref><ref type=\"bibr\" target=\"#b24\">Zhang &amp; Hyvarinen, 2012;</ref><ref type=\"bibr\" target=\"#b20\">Shimizu et al., 2006)</ref>. <ref type=\"bibr\" target=\"#b19\">Pearl (20 2009)</ref> introduce a probabilistic graphical model based framework to learn causality from data. <ref type=\"bibr\" target=\"#b20\">Shimizu et al. (2006)</ref> proposed an effective method called LiNGA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arge in order to express the huge number of interest profiles at Tmall. Deep Interest Network (DIN) <ref type=\"bibr\" target=\"#b29\">[30]</ref> makes the user representation vary over different items wi \">[3]</ref>. Besides the industrial applications proposed by <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>, various types of deep models have gained significant atten. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n, that were applied to short-text summarization. We propose a novel variant of the coverage vector <ref type=\"bibr\" target=\"#b25\">(Tu et al., 2016)</ref> from Neural Machine Translation, which we use d n=\"2.3\">Coverage mechanism</head><p>Repetition is a common problem for sequenceto-sequence models <ref type=\"bibr\" target=\"#b25\">(Tu et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">Mi et al., 2016 erating multi-sentence text (see Figure <ref type=\"figure\">1</ref>). We adapt the coverage model of <ref type=\"bibr\" target=\"#b25\">Tu et al. (2016)</ref> to solve the problem. In our coverage model, w ine Translation <ref type=\"bibr\" target=\"#b10\">(Koehn, 2009)</ref>, coverage was adapted for NMT by <ref type=\"bibr\" target=\"#b25\">Tu et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b14\">Mi et al. (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: asy yet accurate way of taking into account batches in the packet processing in two existing models <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref>. Note that i ef type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref>. Note that in <ref type=\"bibr\" target=\"#b11\">[11]</ref> we came up with a first modeling framework that does not a ODELS WITHOUT BATCH SERVICE</head><p>We start this section by reviewing two of our previous models, <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref> that work wi \"http://www.tei-c.org/ns/1.0\"><head>B. Model for a negligible switch-over time: Model 1</head><p>In <ref type=\"bibr\" target=\"#b11\">[11]</ref> it is assumed that the switch-over time is negligible, whi te with each queue i of the decomposed model, the continuous-time Markov chain depicted in Figure 5 <ref type=\"bibr\" target=\"#b11\">[11]</ref>. A state (k, P ) of this chain, k = 1..., K, corresponds t lution of all Markov chains, the model relies on a fixed-point iterative technique, as developed in <ref type=\"bibr\" target=\"#b11\">[11]</ref>.</p><p>3) Performance parameters: Once all Markov chains ( d><p>The model presented in <ref type=\"bibr\" target=\"#b12\">[12]</ref> is an extension of the one in <ref type=\"bibr\" target=\"#b11\">[11]</ref> that takes into account the switch-over times, and thus co 1 in the case without batches (M = 1), i.e., T M + T R . Let us consider an example (extracted from <ref type=\"bibr\" target=\"#b11\">[11]</ref>) of a virtual switch comprising N = 4 input ports served b ng to standard operational modes. This is exactly what we propose to do in this paper: using models <ref type=\"bibr\" target=\"#b11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12]</ref> (presented i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b19\">Maclaurin et al., 2015;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2015;</ref><ref type=\"bibr\" target=\"#b0\">Abadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b23\">Paszke et al., ribution, and code generation (see, e.g., <ref type=\"bibr\" target=\"#b2\">Bergstra et al., 2010;</ref><ref type=\"bibr\" target=\"#b0\">Abadi et al., 2016)</ref>. But, because declarative DSLs prevent users bed in \u00a74.6. TensorFlow graphs come with their own set of design principles, which are presented in <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>. The following terminology will be used in  devices and parallelizes operations when possible. Readers interested in the runtime should consult <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>.</p><p>The function decorator supports code. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: kloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We adopt larger input data sets varying from 147 to 187 G  both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in the memory system. And for each workload, we collect t  performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type=\"bibr\" target=\"#b16\">[17]</ref>).</p><p>We find that big data analytics applications share chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type=\"bibr\" target=\"#b16\">[17]</ref>) workloads. Meanwhile the service workloads in data center s, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>\u2022 Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and service workloa the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type=\"bibr\" target=\"#b16\">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall b vel cache), respectively. For the service workloads, our observations corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><p>\u2022 For the big data ana rk of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, CloudSuite paper is biased towards online servic . The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, our approach are more pragmatic. We adopt a larger data i  in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in memory. The number of instructions retired of the big  , HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and compared them with big data analytics workloads.</p>< as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the o n fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The front end inefficiency may caused by high-level langu  Figure <ref type=\"figure\" target=\"#fig_3\">5</ref>.</p><p>Implications: Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and the service wor ns, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and large binary size complicated by high-level language, sor and save the die area. For the service workloads, our observation corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id=\"formula_ s in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type=\"bibr\" target=\"#b16\">[17]</ref>. According to our correlation analysis in this section, ar ten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Moreover, for mod s prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and etc. Narayanan et al. <ref type=\"bibr\" target=\"#b32\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rable to different noise distributions at test time? Inspired by recent research in computer vision <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, Neural Machine Translation (NMT; <ref typ  model using a mixture of noisy and clean samples.</p><p>\u2022 We implement a stability training method <ref type=\"bibr\" target=\"#b51\">(Zheng et al., 2016)</ref>, adapted to the sequence labeling scenario r method to improve robustness is to design a representation that is less sensitive to noisy input. <ref type=\"bibr\" target=\"#b51\">Zheng et al. (2016)</ref> presented a general method to stabilize mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" targe n training strategy, <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b0\">1,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> can generate arbitrar e the robustness of our approach.</p><p>The contributions of our work can be summarized as follows: <ref type=\"bibr\" target=\"#b0\">(1)</ref> We propose a novel cascade network structure to reduce the e sed Dynamic Pixel-wise Loss</head><p>Recent works on video generation adopt either GANbased methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" targ type=\"bibr\" target=\"#b0\">[1]</ref>. We compare our model with other three state-of-the-art meth-ods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target 6]</ref> is helpful for generating sharp images in GAN/VAE <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref>. Perceptual loss utilizes high-level features to compare gene  correspond to the input audio, we adopt the evaluation matrix Landmarks Distance (LMD) proposed in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We compare our model with other three state-of-the-art meth. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed learning for computing meaningful and interpretable clusters on input graphs. On the other hand, <ref type=\"bibr\" target=\"#b21\">[22]</ref> proposes an approach that automatically constructs an easy such as link prediction, e-commerce recommendation, etc, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. There are some recent works that learn hierarchical graph raph representation is e-commerce taxonomy for offering a personalized dynamic shopping navigation. <ref type=\"bibr\" target=\"#b21\">[22]</ref> illstrates a topic-driven hierarchical taxonomy based on u </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D. Experiments and Results</head><p>SHOAL <ref type=\"bibr\" target=\"#b21\">[22]</ref> is Alibaba's current topic-driven taxonomy solution deploy iveness, we compare our proposed method with Alibaba's current topic-driven taxonomy solution SHOAL <ref type=\"bibr\" target=\"#b21\">[22]</ref>. In the parameter setting, we set the level number of the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: k] in GCN-CF model, we hope that the rank keeps [i, j, k] in the binary model. According to ListNet <ref type=\"bibr\" target=\"#b1\">[Cao et al., 2007]</ref>, we can characterize sorting information in t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ph signal processing literature to measure the smoothness of a signal defined over nodes of a graph <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2015)</ref>. More specifically, given a graph with the a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s topic has attracted considerable attention in recent years <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target  to identify the same person in different camera views among a potentially huge number of imposters <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. At the same time, p  Laplacian matrix and D is the degree matrix with each element D ii = j S V (i, j). As discussed in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, minimizing the pairwise constraint will force the similar r  images of a person have a high probability of sharing the similar representation features in re-id <ref type=\"bibr\" target=\"#b8\">[9]</ref>, this will make early active learning schema more suitable f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: zation (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualizati ted languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" ircle class at runtime, the compiler can convert the indirect call to multiple guarded direct calls <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  parameters of network, we use the grouped convolution layer <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> in the second and fourth layers in each enhancement unit wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: model with fewer parameters. The recently-introduced factorised time-delay neural networks (TDNN-F) <ref type=\"bibr\" target=\"#b12\">[13]</ref> utilise half the number of parameters than the hybrid netw to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Consequently, a TDNN-F acoustic model (10 time-delay laye. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e Cheeger bound. This connection has inspired many spectral solutions to the problem, including ARV <ref type=\"bibr\" target=\"#b5\">[11]</ref> and the many works that followed.</p><p>However, spectral m y balanced 2-partitioning algorithm to approximate a balanced k-partitioning when k is a power of 2 <ref type=\"bibr\" target=\"#b5\">[11]</ref>.</p><p>While we are unaware of any previous work on the exa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: m. Numerous studies have been conducted and many strategies have been proposed for data prefetching <ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as been achieved by using LSTM-CRF models <ref type=\"bibr\" target=\"#b17\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b5\">Chiu and Nichol ad><p>We follow the best English NER model <ref type=\"bibr\" target=\"#b15\">(Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b17\">Lample et al.,  concatenated as its representation:</p><p>Integrating character representations Both character CNN <ref type=\"bibr\" target=\"#b27\">(Ma and Hovy, 2016)</ref> and LSTM <ref type=\"bibr\" target=\"#b17\">(La. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Ns) <ref type=\"bibr\" target=\"#b12\">[13]</ref>- <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and U-Net <ref type=\"bibr\" target=\"#b17\">[18]</ref> algorithms have been used.</p><p>Although it is possible f er for upsampling.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B. U-Net</head><p>U-Net <ref type=\"bibr\" target=\"#b17\">[18]</ref> is a modified FCN for yielding more precise segmentation.  formation such as boundaries of objects. To overcome this problem, skip connection methods are used <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The features e U-Net. It is important to note that the compared U-Net is not the original architecture proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref> but a highly calibrated model for the enhanced capability . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed by a factor of 0.2 at epochs 60, 120, 160.</p><p>Models trained on ImageNet. The ResNet-50 model <ref type=\"bibr\" target=\"#b20\">(He et al., 2016)</ref> was trained with a learning rate of 1.6, batc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: approach. Demographic parity and related formulations have been considered in numerous papers (e.g. <ref type=\"bibr\" target=\"#b1\">Calders et al., 2009;</ref><ref type=\"bibr\" target=\"#b18\">Zafar et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b4\">[5]</ref> and feature-based matrix factorization <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b31\">[32]</ref><ref type=\"bibr\" target=\"#b32\">[33]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pendix A. Note that when sample \u03bb from the inverse Gaussian distribution, a fast sampling algorithm <ref type=\"bibr\" target=\"#b12\">[13]</ref> can be applied with O(1) time complexity. And for the hidd. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ETS</head><p>Dilated convolutions were originally proposed for the computation of wavelet transform <ref type=\"bibr\" target=\"#b37\">[38]</ref> and employed in the deep learning context (as an alternati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dels, using recent developments in the vision community.</p><p>Convolutional Neural Networks (CNNs) <ref type=\"bibr\" target=\"#b7\">[8]</ref> have been successfully applied to many ASR tasks <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ction based models <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b31\">21,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>. Interaction based models thrive with encoding word-word tr to ranking signals <ref type=\"bibr\" target=\"#b20\">[11,</ref><ref type=\"bibr\" target=\"#b22\">13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>. Learned end-to-end from user feedbacks <ref type=\"bibr\" ta t=\"#b39\">29]</ref>. Learned end-to-end from user feedbacks <ref type=\"bibr\" target=\"#b33\">[23,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>, the word embeddings can encode so matches tailored for rel nce than score-based ones like mean-pooling or max-pooling <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>.</p><p>Kernel-pooling is applied to each M h q ,h d matrix  hes are more e ective than weight-summing the similarities <ref type=\"bibr\" target=\"#b22\">[13,</ref><ref type=\"bibr\" target=\"#b39\">29]</ref>-\"similarity does not necessarily mean relevance\" <ref type= lored for relevance ranking, which has signi cant advantages over traditional feature-based methods <ref type=\"bibr\" target=\"#b39\">[29,</ref><ref type=\"bibr\" target=\"#b40\">30]</ref>. ese initial succe d learningto-rank techniques are then used to combine the n-gram somatches to the nal ranking score <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>e CNN is the key to modeling n-grams. Typical IR ap [30]</ref>.</p><p>K-NRM uni ed the progress of IR customized embeddings and interaction based model <ref type=\"bibr\" target=\"#b39\">[29]</ref>. It rst embeds words and builds the translation matrix usi  log, K-NRM outperforms both neural IR methods and feature-based learning-to-rank by a large margin <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>ough the so matching of n-grams in information retr -torank layer to calculate the ranking score using the n-gram translations M. is part extends K-NRM <ref type=\"bibr\" target=\"#b39\">[29]</ref> to n-grams. Kernel-pooling is a pooling technique that use ead><p>Conv-KNRM adds the ability of so matching n-grams to the recent state-of-the-art K-NRM model <ref type=\"bibr\" target=\"#b39\">[29]</ref> with convolutional neural networks (CNNs). Without CNNs, C g Conv-KNRM requires large-scale training data, for example, user clicks in a commercial search log <ref type=\"bibr\" target=\"#b39\">[29]</ref> or industryscale annotations <ref type=\"bibr\" target=\"#b31 raining data. ey are then used in the target domain to generate so -TF features \u03a6(M). Xiong, et al. <ref type=\"bibr\" target=\"#b39\">[29]</ref> showed that kernel-pooled so -TF features reveal di erent  rnel is of low importance in search logs as all candidate documents already contain the query words <ref type=\"bibr\" target=\"#b39\">[29]</ref>; however, synonyms can be a strong signal in a recall-orie Log: Sogou.com is a major Chinese commercial search engine.</p><p>e same se ings as K-NRM were used <ref type=\"bibr\" target=\"#b39\">[29]</ref>. e same sample of Sogou log and training-testing splits ar IR baselines for stronger baseline performance. Body texts of training documents were not available <ref type=\"bibr\" target=\"#b39\">[29]</ref>. e Chinese text was segmented by ICTCLASS <ref type=\"bibr\" ad><p>Training and testing labels on Sogou-Log and Bing-Log were generated following prior research <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> ref type=\"bibr\" target=\"#b33\">[23]</ref>, DRMM <ref type=\"bibr\" target=\"#b22\">[13]</ref>, and K-NRM <ref type=\"bibr\" target=\"#b39\">[29]</ref>.</p><p>CDSSM <ref type=\"bibr\" target=\"#b36\">[26]</ref> is  ry and document representations on their words' le er-tri-grams (or Chinese characters in Sogou-Log <ref type=\"bibr\" target=\"#b39\">[29]</ref>). e ranking scores are calculated by the similarity betwee  erwards.</p><p>K-NRM is a state-of-the-art neural model previously tested on the Sogou-Log dataset <ref type=\"bibr\" target=\"#b39\">[29]</ref>. It uses kernel-pooling instead of DRMM's histogram poolin earch logs, 5-fold cross validation were used to be consistent with the previous study on Sogou-Log <ref type=\"bibr\" target=\"#b39\">[29]</ref>. On ClueWeb09-B, the 10-fold cross validation splits from  n Sogou-Log, traditional IR methods used both title and body, and neural IR methods only used title <ref type=\"bibr\" target=\"#b39\">[29]</ref>, as discussed in section 5.1. On Bing-Log, all methods use were all learned end-to-end using the query logs. For Sogou-log, we set embedding dimension L = 300 <ref type=\"bibr\" target=\"#b39\">[29]</ref> . For Bing-Log, we set L = 100 because our pilot study sho ers were: \u00b5 1 = 0.9, \u00b5 2 = 0.7, ..., \u00b5 10 = \u22120.9.</p><p>e \u03c3 of the so match bins were set to be 0.1 <ref type=\"bibr\" target=\"#b39\">[29]</ref>. Model Implementation and E ciency: e model was implemente bout 12 hours on an AWS GPU machine. e training time is similar with prior work using only unigrams <ref type=\"bibr\" target=\"#b39\">[29]</ref>. Most computation time was spent on the embedding layer; t rrent neural IR methods to provide additional improvements <ref type=\"bibr\" target=\"#b32\">[22,</ref><ref type=\"bibr\" target=\"#b39\">29,</ref><ref type=\"bibr\" target=\"#b40\">30]</ref> Comparing the two s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b45\">[43,</ref><ref type=\"bibr\" target=\"#b56\">54]</ref>. Based on Cycle-GAN <ref type=\"bibr\" target=\"#b58\">[56]</ref>, Yuan et al. <ref type=\"bibr\" target=\"#b45\">[43]</ref> pro scheme has also been used to perform image translation without paired training data, e.g., CycleGAN <ref type=\"bibr\" target=\"#b58\">[56]</ref> and DualGAN <ref type=\"bibr\" target=\"#b44\">[42]</ref>. Spe avoid the possible mode collapse issue when solving the under-constrained image translation problem <ref type=\"bibr\" target=\"#b58\">[56]</ref>. Unlike these methods, we seek to improve the performance  es collected from YouTube. Thus, there are 3 DRN-adapt models in total. And We also train a CinCGAN <ref type=\"bibr\" target=\"#b58\">[56]</ref> model for each kind of unpaired data for comparison. Based  Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods <ref type=\"bibr\" target=\"#b58\">[56,</ref><ref type=\"bibr\" target=\"#b6\">4,</ref><ref type=\"bibr\" targ  CycleGAN based SR methods. First, Cycle-GAN based methods <ref type=\"bibr\" target=\"#b45\">[43,</ref><ref type=\"bibr\" target=\"#b58\">56]</ref> use a cycle consistency loss to avoid the possible mode col. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  of a single device.</p><p>Recently, pipeline parallelism <ref type=\"bibr\" target=\"#b9\">[10]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref> has been proposed as a promising approach for training lar een those nodes during backpropagation. The other category is asynchronous(async) pipeline training <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This manner inserts mini-batches into pipeline continuous lism, and hybrid approaches combining both. Current state-of-theart pipeline partitioning algorithm <ref type=\"bibr\" target=\"#b11\">[12]</ref> is not able to be applied to synchronous training effectiv evice assignment affects communication efficiency and computing resource utilization. Previous work <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses hierarchical planning and works well for asynchronous D. Contributions over previous work</head><p>Previous works on pipeline planning includes PipeDream <ref type=\"bibr\" target=\"#b11\">[12]</ref> (for asynchronous training) and torchgpipe <ref type=\"bibr it the micro-batch further into 2 even slices, and assign each to a device. An alternative approach <ref type=\"bibr\" target=\"#b11\">[12]</ref> (Fig. <ref type=\"figure\" target=\"#fig_6\">8(b)</ref>) is no. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: fective, but combined with an LSTM brought marginal improvement and greater interpretability, while <ref type=\"bibr\" target=\"#b8\">[9]</ref> did not find any notable improvement using the Transformer i e Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Though self-attention has provided various benefits such as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hniques in tasks like gene name recognition <ref type=\"bibr\" target=\"#b9\">(Kuksa and Qi, 2010;</ref><ref type=\"bibr\" target=\"#b23\">Tang et al., 2014;</ref><ref type=\"bibr\" target=\"#b24\">Vlachos and Ga. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e locations per image but only a few locations contain objects. This imbalance causes two problems: <ref type=\"bibr\" target=\"#b0\">(1)</ref> training is inefficient as most locations are easy negatives  benchmark <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p><p>For training, we follow common practice <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> and use the COCO tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  coarsens graphs through a differentiable pooling map that can be pre-computed. Similarly, DiffPool <ref type=\"bibr\" target=\"#b28\">(Ying et al., 2018)</ref> proposes an adaptive pooling mechanism that  popularity. In particular, we selected DGCNN <ref type=\"bibr\">(Zhang et al., 2018)</ref>, DiffPool <ref type=\"bibr\" target=\"#b28\">(Ying et al., 2018)</ref>, ECC <ref type=\"bibr\" target=\"#b20\">(Simono. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: shuffle the training and test data sets; (4) RCV1, a large benchmark corpus for text classification <ref type=\"bibr\" target=\"#b11\">[12]</ref> <ref type=\"foot\" target=\"#foot_3\">4</ref> . Four subsets i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref><ref type=\"bibr\" target=\"#b33\">35]</ref> or other learning paradigms <ref type=\"bibr\">[20,</ref><ref type=\"bibr\" target=\"#b20\">22,</ref><ref type=\"bibr\" target=\"#b21\">23,</ref><ref type=\"bibr\" tar lfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, RFL <ref type=\"bibr\" target=\"#b21\">[23]</ref>, NBSRF <ref type=\"bibr\" target=\"#b20\">[22]</ref>, PSyCo [20] and IA <ref type=\"bibr\" target=\"#b28\">[30]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b38\">[38]</ref><ref type=\"bibr\" target=\"#b39\">[39]</ref><ref type=\"bibr\" target=\"#b40\">[40]</ref>. CF-based models leverage users' feedbacks (e.g. implicit . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bility than RNNs. However, in the NER task, Transformer encoder has been reported to perform poorly <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>, our experiments also confirm this result. T =\"#tab_3\">3</ref>. The poor performance of the Transformer in the NER datasets was also reported by <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>. Although performance of the Transformer is  ibr\" target=\"#b13\">(Guo et al., 2019)</ref>. Although performance of the Transformer is higher than <ref type=\"bibr\" target=\"#b13\">(Guo et al., 2019)</ref>, it still lags behind the BiLSTM-based model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: trained word embeddings over randomly initialized ones. Embeddings are pretrained using skip-n-gram <ref type=\"bibr\" target=\"#b24\">(Ling et al., 2015a)</ref>, a variation of word2vec <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s have been achieved dramatic improvement in SR. Dong et al. <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> first exploit a three-layer convolutional neural network, nam posed method with other SR methods, including bicubic, SRCNN <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRC-N <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: es before ReLU, since simply adding more parameters is inefficient for real-time image SR scenarios <ref type=\"bibr\" target=\"#b7\">[8]</ref>. We first introduce SR residual network WDSR-A, which has a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cores for all documents. To get the student predicted rank for this document, we apply Weston et al <ref type=\"bibr\" target=\"#b35\">[36]</ref>'s sequential sampling, and do it in a parallel manner <ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b11\">(Jiang et al., 2018b;</ref><ref type=\"bibr\" target=\"#b29\">Wang et al., 2018a;</ref><ref type=\"bibr\" target=\"#b31\">Wu et al., 2018;</ref><ref type=\"bibr\" target=\"#b10\">Jiang et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: fically, to capture the nonlinear relationship between features, we introduce explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> to CCA. Finally, u high computational complexity and memory use. In contrast, recent advances of explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> can convert nonlin l trick is used in Eq. (1). To reduce the computation complexity, one can use explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref>. Let \u03c6(x) denote a IST features, attribute features, and SentiBank features, we use the random Fourier feature mapping <ref type=\"bibr\" target=\"#b19\">[12]</ref> to approximate the Gaussian kernel. All other histogram-ba. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ercome this problem, contemporary methods such as those based on deep convolutional neural networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target below 30kB, we obtained roughly 450k training images. During training, we followed existing studies <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> to obtain LR images  ifold, new losses are proposed to replace the conventional pixel-wise mean squared error (MSE) loss <ref type=\"bibr\" target=\"#b6\">[7]</ref> that tends to encourage blurry and overly-smoothed results.  ef type=\"bibr\" target=\"#b38\">[39]</ref>. As an instantiation of learning-based methods, Dong et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref> propose SRCNN for learning the mapping of LR and HR images i f> shows the qualitative results of different models including PSNR-oriented methods, such as SRCNN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, VDSR <ref type=\"bibr\" target=\"#b21\">[22]</ref>, LapSRN <ref tural and realistic textures.  <ref type=\"bibr\">Figure 6</ref>. User study results of ranking SRCNN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, MemNet <ref type=\"bibr\" target=\"#b43\">[44]</ref>, SFT-GAN ( ocused on PSNR-oriented baselines. The users were requested to rank 4 versions of each image: SRCNN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, MemNet <ref type=\"bibr\" target=\"#b43\">[44]</ref> (the state. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 128 filters of the size 3 \u00d7 3.</p><p>For weight initialization, we use the same method as He et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref>, which is shown to be suitable for networks utilizing ReLU. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ter. This approach <ref type=\"bibr\" target=\"#b74\">[76,</ref><ref type=\"bibr\" target=\"#b49\">50,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> first loads all the edges of the active vertices to constru. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ure is based on recent attention-based end-to-end ASR models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> and TTS models such as Tacotron <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Our proposed PAM model outperforms the baselines that use standard NER models, such as Stanford NER <ref type=\"bibr\" target=\"#b4\">[5]</ref> and Bi-LSTM-CRF <ref type=\"bibr\" target=\"#b8\">[9]</ref>, by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata</head><p>All experiments are performed on the publicly available Lib-riSpeech audio book corpus <ref type=\"bibr\" target=\"#b10\">[11]</ref>. We use the \"train-clean-100\" set as the paired data set,  blic domain books. The books were selected such that there is no overlap with the dev and test sets <ref type=\"bibr\" target=\"#b10\">[11]</ref>. On the other hand, the training data set transcriptions a for the apostrophe in contractions (we replace hyphens with a space). Unlike the original LM corpus <ref type=\"bibr\" target=\"#b10\">[11]</ref> we take no steps to replace non-standard words with a cano. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eech (TTS) synthesis <ref type=\"bibr\" target=\"#b3\">[4]</ref> and Automatic Speech Recognition (ASR) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, using a single neural network that directly generates the t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b21\">22]</ref>, there are several attempts to adopt GNNs to learn with heterogeneous networks <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta ntly, studies have attempted to extend GNNs for modeling heterogeneous graphs. Schlichtkrull et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> propose the relational graph convolutional networks (RGCN)  heterogeneous GNNs as baselines, including:</p><p>\u2022 Relational Graph Convolutional Networks (RGCN) <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which keeps a different weight for each relationship, i.e  refers to HGT +RT E +H e t e r .</p><p>GNN Models GCN <ref type=\"bibr\" target=\"#b8\">[9]</ref> RGCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> GAT <ref type=\"bibr\" target=\"#b21\">[22]</ref> HetGNN <ref . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: by m orthonormal matrices. We refer the readers to <ref type=\"bibr\" target=\"#b24\">(Wong, 1967;</ref><ref type=\"bibr\" target=\"#b0\">Absil et al., 2004)</ref> for details on the Riemannian geometry of th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet <ref type=\"bibr\" target=\"#b17\">[10]</ref>, which forms the sentiment view. Then, using a framework o ment aspect of the associate text. For this, we use an external knowledge base, called SentiWordNet <ref type=\"bibr\" target=\"#b17\">[10]</ref>. It is based on the well-known English lexical dictionary  ures with the mid-level features (denoted as Low&amp;SentiBank), and a textual feature-based method <ref type=\"bibr\" target=\"#b17\">[10]</ref> (denoted as SentiStrength<ref type=\"foot\" target=\"#foot_3\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: equences versus sequences problems, such as audio analysis <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, video captioning <ref type=\"bibr\" target=\"#b23\">[24,</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , are also possible <ref type=\"bibr\" target=\"#b8\">[10,</ref><ref type=\"bibr\" target=\"#b36\">38,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref>; in fact, some stud-ies have suggested that Amazon does not. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=\"bibr\" target=\"#b11\">[14]</ref> . It could obtain transformation rules automatically durin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rticulatory features <ref type=\"bibr\" target=\"#b7\">[8]</ref> to train HMM based systems. Authors in <ref type=\"bibr\" target=\"#b8\">[9]</ref> used subspace Gaussian mixture model to map phonemes of diff. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pplied to the task of single image super-resolution (SISR) <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" tar rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar of image super-resolution. Thus, in recent image SR networks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, batch normalizatio rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar rget=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar f type=\"bibr\" target=\"#b13\">[14]</ref>, SRResNet <ref type=\"bibr\" target=\"#b16\">[17]</ref> and EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>). The increasing of depth brings benefits to representatio ding blocks for image super-resolution networks. Compared with vanilla residual blocks used in EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce WDSR-A which has a slim identity mapping pat 5]</ref> leads to better accuracy for deep super-resolution networks. Previous works including EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>, BTSRN <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RDN <re e for training SR networks. However, with the increasing depth of neural networks for SR (e.g. MDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> has depth around 180), the networks without batch normaliz ing deeper and deeper (from 3-layer SRCNN <ref type=\"bibr\" target=\"#b3\">[4]</ref> to 160-layer MDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>), training becomes more difficult. Batch normalization lay f type=\"figure\">1</ref>. Two-layer residual blocks are specifically studied following baseline EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Assume the width of identity mapping pathway (Fig. <ref t <p>Figure <ref type=\"figure\">2</ref>: Demonstration of our simplified SR network compared with EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>.</p><p>In this part, we overview the WDSR network architec his part, we overview the WDSR network architectures. We made two major modifications based on EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> super-resolution network.</p><p>Global residual pathway Fi set <ref type=\"bibr\" target=\"#b34\">[35]</ref>  In this part, we show results of baseline model EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref> and our proposed WDSR-A and WDSR-B for the task of image b e results suggest that our proposed WDSR-A and WDSR-B have better accuracy and efficiency than EDSR <ref type=\"bibr\" target=\"#b18\">[19]</ref>. WDSR-B with wider activation also has better or similar p curacy drop with our simpler form.</p><p>Upsampling layer Different from previous state-of-the-arts <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> where one or more  lips and rotations following common data augmentation methods<ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. During training, the input images are also subtracted with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ddings <ref type=\"bibr\" target=\"#b30\">[20]</ref> are introduced to calculate the translation scores <ref type=\"bibr\" target=\"#b21\">[12]</ref>. How to combine the word-level translation scores to gener. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ead n=\"3.1\">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation <ref type=\"bibr\" target=\"#b49\">(Sennrich et al., 2016)</ref> in NMT, referring to training a backwar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tation tasks before the DL revolution, including amplitude segmentation based on histogram features <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the region based segmentation method <ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hich consist of a large number of heterogeneous components <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" ute C 2 with C 1 +\u03b4, in lines (2) and (3). Next, we observe that (T C 2 ) n \u2212(T C 1 ) n = n in line <ref type=\"bibr\" target=\"#b3\">(4)</ref>. Finally, we note that all the matrices in line ( <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sks, ranging from online advertising <ref type=\"bibr\" target=\"#b20\">[21]</ref>, microblog retrieval <ref type=\"bibr\" target=\"#b25\">[26]</ref>, to open relation extraction <ref type=\"bibr\" target=\"#b24. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: entation <ref type=\"bibr\" target=\"#b37\">(Wang et al., 2018)</ref> and multi-resolution segmentation <ref type=\"bibr\" target=\"#b0\">(Benz et al., 2004)</ref> have been widely used to generate image obje nitial segmentation results for HR images. MSEG is developed from the multi-resolution segmentation <ref type=\"bibr\" target=\"#b0\">(Benz et al., 2004)</ref>. It initially generates an oversegmentation  ough the segmentation algorithm used in this study is based on multi-resolution segmentation (MSEG) <ref type=\"bibr\" target=\"#b0\">(Benz et al. (2004)</ref>; <ref type=\"bibr\" target=\"#b36\">Tzotsos and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b91\">94,</ref><ref type=\"bibr\" target=\"#b55\">58,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b60\">63]</ref>. Here we make an attempt to actually find this structure. W arget=\"#b91\">94,</ref><ref type=\"bibr\" target=\"#b55\">58,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b60\">63]</ref>. Unlike multi-task learning, we explicitly model the relati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nce of the two NCE algorithms on a language modeling problem, using the Penn Treebank (PTB) dataset <ref type=\"bibr\" target=\"#b7\">(Marcus et al., 1993)</ref>. We choose <ref type=\"bibr\" target=\"#b11\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orms the sentiment view. Then, using a framework of multi-view canonical correlation analysis (CCA) <ref type=\"bibr\" target=\"#b18\">[11]</ref>, we calculate a latent embedding space in which correlatio s the linear relationship between random variables. Several nonlinear extensions such as kernel CCA <ref type=\"bibr\" target=\"#b18\">[11]</ref> and Deep CCA <ref type=\"bibr\" target=\"#b29\">[22]</ref> hav ions among multiple views using a framework of the generalization of canonical correlation analysis <ref type=\"bibr\" target=\"#b18\">[11]</ref>. Let X i (i \u2208 {v, t, s}) denote the feature matrix of the  at the distances in the resulting space between each pair of views for the same image are minimized <ref type=\"bibr\" target=\"#b18\">[11]</ref>. The objective function to learn the latent space is as fo  \u03d5 j (X j ), and w ik represents the k-th column of the matrix W i . In the conventional kernel CCA <ref type=\"bibr\" target=\"#b18\">[11]</ref>, kernel trick is used in Eq. (1). To reduce the computatio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: storical activations.</p><p>Step 3 and 4 are handled automatically by frameworks such as TensorFlow <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>. The computational graph at Step 2 is defin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> were proposed. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>  ><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" target=\"#b8\">[9]</ref> were proposed. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on  e reconstructed some classic SR models, such as SRCNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>, EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> and SRResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>. During <ref type=\"bibr\" target=\"#b5\">[6]</ref>, SRResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Unfortunately, these models become more and more deeper and CN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, LapSRN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. For fair, we retrain most of these models (except for EDSR  <ref type=\"bibr\" target=\"#b8\">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, the results of EDSR provided by their original papers).</p> t upscaling factors and test-datasets. It can be seen that our results are slightly lower than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. But it is worth noting that EDSR <ref type=\"bibr\" target=\"#  slightly lower than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>. But it is worth noting that EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> use  RGB channels for training, meanwhile, the data augment  nwhile, the data augment methods are different.</p><p>To better illustrate the difference with EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we show a comparison of model specifications in Table <ref   show a comparison of model specifications in Table <ref type=\"table\" target=\"#tab_3\">3</ref>. EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> is an outstanding model gained amazing results. However, it   memory, space and datasets. In contrast, the specifications of our model is much smaller than EDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref>, which makes it easier to reproduce and promote.</p><p>In Fi nts the upscaling factor) mixed training method is used in <ref type=\"bibr\" target=\"#b3\">[4]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and geometric selfensemble method is proposed in <ref type= 4]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and geometric selfensemble method is proposed in <ref type=\"bibr\" target=\"#b8\">[9]</ref>. We believe that these training tricks can also improve our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: zing GCN, S-GCN, ChebNet and related methods. Our architecture is analogous to the inception module <ref type=\"bibr\" target=\"#b43\">Szegedy et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b22\">Kazi et a ion ( <ref type=\"formula\" target=\"#formula_6\">4</ref>) is analogous to the popular Inception module <ref type=\"bibr\" target=\"#b43\">Szegedy et al. (2015)</ref> for classic CNN architectures (Figure <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n single sentence relation extraction with an exception of <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, which focus on general documents while not targeting on a  e extracted dependency parse tree, where the tree roots of different sentences are linked together. <ref type=\"bibr\" target=\"#b36\">[37]</ref> proposes a method using self-attention <ref type=\"bibr\" ta  the self-attention of the words, and use a convolutional layer in self-attention blocks similar to <ref type=\"bibr\" target=\"#b36\">[37]</ref> to alleviate the burden on the model to attend to local fe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: kloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. We adopt larger input data sets varying from 147 to 187 G  both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in the memory system. And for each workload, we collect t  performance data of the whole run time after the warm-up instead of a short period (180 seconds in <ref type=\"bibr\" target=\"#b16\">[17]</ref>).</p><p>We find that big data analytics applications share chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper <ref type=\"bibr\" target=\"#b16\">[17]</ref>) workloads. Meanwhile the service workloads in data center s, e.g., HPC-HPL, HPC-DGEMM and chip multiprocessors workloads.</p><p>\u2022 Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and service workloa the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite <ref type=\"bibr\" target=\"#b16\">[17]</ref>, SPECweb and TPC-W) in terms of processor pipeline stall b vel cache), respectively. For the service workloads, our observations corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><p>\u2022 For the big data ana rk of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, CloudSuite paper is biased towards online servic . The input data size varies from 147 to 187 GB. In comparison with that of CloudSuite described in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, our approach are more pragmatic. We adopt a larger data i  in both memory and disk systems instead of completely storing data (only 4.5 GB for Naive Bayes in <ref type=\"bibr\" target=\"#b16\">[17]</ref>) in memory. The number of instructions retired of the big  , HPCC, PARSEC, TPC-W, SPECweb 2005, and CloudSuite-a scale-out benchmark suite for cloud computing <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and compared them with big data analytics workloads.</p>< as one of the representative big data analytics workloads with a larger data input set (147 GB). In <ref type=\"bibr\" target=\"#b16\">[17]</ref>, the data input size is only 4.5 GB.</p><p>We set up the o n fetch stalls indicates the front end inefficiency. Our observation corroborates the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The front end inefficiency may caused by high-level langu  Figure <ref type=\"figure\" target=\"#fig_3\">5</ref>.</p><p>Implications: Corroborating previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, both the big data analytics workloads and the service wor ns, which may be caused by two factors: deep memory hierarchy with long latency in modern processor <ref type=\"bibr\" target=\"#b16\">[17]</ref>, and large binary size complicated by high-level language, sor and save the die area. For the service workloads, our observation corroborate the previous work <ref type=\"bibr\" target=\"#b16\">[17]</ref>: the L2 cache is ineffective.</p><formula xml:id=\"formula_ s in modern superscalar processors as previous work found e.g., on-chip bandwidth, die area and etc <ref type=\"bibr\" target=\"#b16\">[17]</ref>. According to our correlation analysis in this section, ar ten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Moreover, for mod s prevent us from breaking down the execution time precisely due to overlapped work in the pipeline <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" ta get=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and etc. Narayanan et al. <ref type=\"bibr\" target=\"#b32\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ser intent.</p><p>As a general information modeling method, Heterogeneous Information Network (HIN) <ref type=\"bibr\" target=\"#b17\">[18]</ref>, consisting of multiple types of objects and links, has be y data mining tasks <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this paper, we propose to model the intent recommendati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b38\">39]</ref>, as well as the SDR metric computed using the bss eval sources software <ref type=\"bibr\" target=\"#b39\">[40]</ref> because it is used by other groups. We believe SI-SDR is a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )] Inference Loss +\u03bb D K L (q \u03d5c (z |x q c ) | | q \u03d5c (z |x s c ))</formula><p>Bridging Regularizer <ref type=\"bibr\" target=\"#b9\">(10)</ref> In this paper, we assume q \u03d5 c (z|x q c ) and q \u03d5 c (z|x s  s related to natural language inference (NLI) problem. We select three state-of-the-art models ESIM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, Transformer <ref type=\"bibr\" target=\"#b23\">[24]</ref>, BER b27\">[28]</ref> (MultiNLI) corpus have promoted the development of many different neural NLI models <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992 <ref type=\"bibr\" target=\"#b13\">[14]</ref> . It could obtain transformation rules automatically durin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: parameters including reflection of the environment <ref type=\"bibr\" target=\"#b20\">[21]</ref>, bumps <ref type=\"bibr\" target=\"#b8\">[9]</ref>, transparency <ref type=\"bibr\" target=\"#b6\">[7]</ref>, and s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dhuber, 1997)</ref> system can readily outperform the previous state-of-the-art system, CogCompTime <ref type=\"bibr\" target=\"#b32\">(Ning et al., 2018d)</ref>, by a large margin. The fact that a standa ead><label>2</label><figDesc>Performances on the MATRES test set (i.e., the PT section). CogCompTime<ref type=\"bibr\" target=\"#b32\">(Ning et al., 2018d)</ref> is the previous state-of-the-art feature-b hat in Table <ref type=\"table\" target=\"#tab_2\">2</ref>, CogCompTime performed slightly different to <ref type=\"bibr\" target=\"#b32\">Ning et al. (2018d)</ref>: Cog-CompTime reportedly had F 1 =65.9 (Tab f type=\"table\" target=\"#tab_2\">2</ref> Line 3 therein) and here we obtained F 1 =66.6. In addition, <ref type=\"bibr\" target=\"#b32\">Ning et al. (2018d)</ref> only reported F 1 scores, while we also use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b12\">(Maal\u00f8e et al., 2016;</ref><ref type=\"bibr\" target=\"#b25\">Springenberg, 2016;</ref><ref type=\"bibr\" target=\"#b15\">Odena, 2016;</ref><ref type=\"bibr\">Salimans et al., 2016)</ref>. It A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b14\">14,</ref><ref type=\"bibr\" target=\"#b36\">36,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. Although considerable progress has been achieved in image  get=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b30\">30,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. Due to space limitation, we here briefly review works rela arget=\"#b13\">13,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>. For example, Dong et al. <ref type=\"bibr\" target=\"#b2\">[ 2 ing discriminative ability of the network. Channel attention <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> has been shown to be effective for better discriminative re rget=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b6\">6,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref  >[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b39\">39,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref>, we also adopt self-ensemble method to further improve our  f the network, some other networks, such as NLRN <ref type=\"bibr\" target=\"#b22\">[22]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>, improve the performance by considering feature correlatio classification.</p><p>Recently, SENet was introduced to deep CNNs to further improve SR performance <ref type=\"bibr\" target=\"#b38\">[38]</ref>. However, SENet only explores first-order statistics (e.g. se feature interdependencies. Difference to Residual Channel Attention Network (RCAN). Zhang et al. <ref type=\"bibr\" target=\"#b38\">[38]</ref> proposed a residual in residual structure to form a very d sidual blocks in each LSRAG, thus resulting in deep network with over 400 convolution layers. As in <ref type=\"bibr\" target=\"#b38\">[38]</ref>, we also add long and short skip connections in Base model BPN <ref type=\"bibr\" target=\"#b6\">[6]</ref>, RDN <ref type=\"bibr\" target=\"#b39\">[39]</ref> and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. As in <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=  <ref type=\"bibr\" target=\"#b36\">[36]</ref>, RDN <ref type=\"bibr\" target=\"#b39\">[39]</ref>, and RCAN <ref type=\"bibr\" target=\"#b38\">[38]</ref>. All the results on 3\u00d7 are shown in Table <ref type=\"table. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or a broad query. In recent years, many search result diversification approaches have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target sification can be divided into explicit approaches and implicit approaches. The implicit approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" targ \">24,</ref><ref type=\"bibr\" target=\"#b25\">26</ref>] are able to outperform the heuristic approaches <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target e way to solve the problem of query ambiguity, many models have been proposed to solve this problem <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target nts. The diversification score function of implicit approaches can be handcrafted rules such as MMR <ref type=\"bibr\" target=\"#b0\">[1]</ref> or a supervised measure such as R-LTR <ref type=\"bibr\" targe ments. In the early years' research on diversification, implicit methods are most unsupervised. MMR <ref type=\"bibr\" target=\"#b0\">[1]</ref> can be regarded as the foundation of implicit methods. Its d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arity score between two articles is produced by the cosine similarity of their representations.\u2022 WMD<ref type=\"bibr\" target=\"#b65\">[66]</ref>:The Word Mover's Distance (WMD) is the minimum distance re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Data stream strides. The data stream is characterized with respect to local and global data strides <ref type=\"bibr\" target=\"#b9\">[10]</ref>. A global stride is defined as the difference in the data m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: br\" target=\"#b21\">(Levy et al., 2017;</ref><ref type=\"bibr\" target=\"#b28\">McCann et al., 2018;</ref><ref type=\"bibr\" target=\"#b22\">Li et al., 2019)</ref>, we propose a new framework that is capable of  formalized as answering the question \"What is the summary?\". Our work is significantly inspired by <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref>, which formalized the task of entity-relation  sk of entity-relation extraction as a multi-turn question answering task. Different from this work, <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> focused on relation extraction rather than NER  target=\"#b22\">Li et al. (2019)</ref> focused on relation extraction rather than NER. Additionally, <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> utilized a template-based procedure for constr nt influence on the final results. Different ways have been proposed for question generation, e.g., <ref type=\"bibr\" target=\"#b22\">Li et al. (2019)</ref> utilized a template-based procedure for constr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  black blob). <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bib bibr\" target=\"#b55\">[56]</ref>, as well as squeeze and excitation (SE) block presented by Hu et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The proposed Res2Net module introduces the scale dimensio sily integrate the cardinality dimension <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the SE block <ref type=\"bibr\" target=\"#b24\">[25]</ref> with the proposed Res2Net module.</p></div> <div xmlns=\"ht mension cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> (replace conv with group conv) and SE <ref type=\"bibr\" target=\"#b24\">[25]</ref> blocks.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\" calibrates channel-wise feature responses by explicitly modelling inter-dependencies among channels <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Similar to <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we  y modelling inter-dependencies among channels <ref type=\"bibr\" target=\"#b24\">[25]</ref>. Similar to <ref type=\"bibr\" target=\"#b24\">[25]</ref>, we add the SE block right before the residual connections ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, SE-Net <ref type=\"bibr\" target=\"#b24\">[25]</ref>, bLResNet <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and DLA. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ality prediction. In future, we will improve the e ciency of NFM by resorting to hashing techniques <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> to make it more su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: al influence. Indeed, extensive work has been done on social influence prediction in the literature <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" ta ial equations extended from the classic 'Susceptible-Infected' (SI) model; Most recently, Li et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> proposed an end-toend predictor for inferring cascade size  efforts to detect those global patterns automatically using deep learning, e.g., the DeepCas model <ref type=\"bibr\" target=\"#b25\">[26]</ref> which formulate cascade prediction as a sequence problem a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eful tool for dealing with those aspects of compiler and program development. Models of performance <ref type=\"bibr\" target=\"#b5\">[6]</ref> and energy <ref type=\"bibr\" target=\"#b6\">[7]</ref> can also . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. The recent SotA (State of the Art) method <ref type=\"bibr\" \"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>. The recent SotA (State of the Art) method <ref type=\"bibr\" target=\"#b12\">[13]</ref> exceeds previous non-Deep SR methods (supervised <ref type r synthesize ones.</p><p>Lastly, we use a method similar to the geometric self-ensemble proposed in <ref type=\"bibr\" target=\"#b12\">[13]</ref> (which generates 8 different outputs for the 8 rotations+f ut quality, which is up to the user to choose.</p><p>For compariosn, the test-time of leading EDSR+ <ref type=\"bibr\" target=\"#b12\">[13]</ref> grows quadratically with the image size. While it is fast  f degradation types, the image-specific CNN obtains significantly better SR results than SotA EDSR+ <ref type=\"bibr\" target=\"#b12\">[13]</ref> (see Sec. 4). Similarly, in the case of non-ideal downscal gly, ZSSR produces competitive results (although VDSR <ref type=\"bibr\" target=\"#b8\">[9]</ref> EDSR+ <ref type=\"bibr\" target=\"#b12\">[13]</ref> Blind-SR <ref type=\"bibr\" target=\"#b14\">[15]</ref> ZSSR [e ails.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Ground Truth VDSR [9]</head><p>EDSR+ <ref type=\"bibr\" target=\"#b12\">[13]</ref> ZSSR (ours) (PSNR, SSIM) (20.11, 0.9136) (25.29 / 0.9627)  ing).</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Bicubic interpolation</head><p>EDSR+ <ref type=\"bibr\" target=\"#b12\">[13]</ref> ZSSR (ours) 27.9216 / 0.7504 27.5600 / 0.7135 28.6148 / 0. l. Table <ref type=\"table\">2</ref> compares our against the leading externallysupervised SR methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. We also compared ou. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  have a longer access time, and this may increase the critical path length and penalize performance <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>In this paper we propose a novel register renaming ap. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch cannot be simply integrated like other auxiliary information in multiple feedback recommendation <ref type=\"bibr\" target=\"#b5\">[Ding et al., 2018b;</ref><ref type=\"bibr\" target=\"#b6\">Gao et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 5]</ref>, <ref type=\"bibr\" target=\"#b105\">[106]</ref>, <ref type=\"bibr\" target=\"#b107\">[108]</ref>, <ref type=\"bibr\" target=\"#b108\">[109]</ref>, ii) development of improved filters by exploiting conve. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: evidence to be retrieved from Wikipedia.</p><p>We constructed a purpose-built dataset for this task <ref type=\"bibr\" target=\"#b15\">(Thorne et al., 2018)</ref> that contains 185,445 human-generated cla nce when constructing the dataset was the trade-off between annotation velocity and evidence recall <ref type=\"bibr\" target=\"#b15\">(Thorne et al., 2018)</ref>. Evidence selected by annotators was ofte ata was released through the FEVER website. 1 We used the reserved portion of the data presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>   </p></div> <div xmlns=\"http://www.tei-c. www.tei-c.org/ns/1.0\"><head n=\"2.2\">Scoring Metric</head><p>We used the scoring metric described in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref> to evaluate the submissions. The FEVER sha pe=\"table\" target=\"#tab_2\">2</ref>). 19 of these teams scored higher than the baseline presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>. All participating teams were invited to s her individually or as a group, can be used as evidence. We retained the annotation guidelines from <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref> (see Sections A.7.1, A.7.3 and A.8 from th rom 86 submissions from 23 teams. 19 of these teams exceeded the score of the baseline presented in <ref type=\"bibr\" target=\"#b15\">Thorne et al. (2018)</ref>. For the teams which provided a system des. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or NOT ENOUGH INFO by annotators. The dataset partition is kept the same with the FEVER Shared Task <ref type=\"bibr\" target=\"#b26\">(Thorne et al., 2018b)</ref> as shown in Table <ref type=\"table\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: y information for character-based model. To integrate words information into character-based model, <ref type=\"bibr\" target=\"#b40\">Zhang and Yang (2018)</ref> propose a lattice-structured LSTM model t characterbased model. The character baseline denotes the original character-based BiLSTM-CRF model. <ref type=\"bibr\" target=\"#b40\">Zhang and Yang (2018)</ref> propose a lattice LSTM to exploit word in y information into Chinese NER task. Another way to obtain word boundary information is proposed by <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, using a lattice LSTM to integrate word   where b &lt; i and c b,i matches a word in lexicon D. The lexicon D is the same as the one used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, which is built by using automatically s x c i with x \u2212 \u2192 ws i to utilize word information. And this is quite different from the way used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, since they use extra shortcut paths to  j , y j )}| N j=1 , we minimize the sentence-level negative loglikelihood loss to train the model:  <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>.</p><formula xml:id=\"formula_16\">L = \u2212 j rget=\"#tab_0\">1</ref>. Implementation Details. We utilize the character and word embeddings used in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, both of which are pre-trained on Chines ng, 2018)</ref>, both of which are pre-trained on Chinese Giga-Word using word2vec model. Following <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>, we use the word embedding dictionary as with other parameters.</p><p>For hyper-parameter configurations, we mostly refer to the settings in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>. We set both character embedding size an > 91.28 90.62 90.95 <ref type=\"bibr\" target=\"#b0\">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>   approach to integrating word informati n Chinese Resume dataset. Consistent with the previous results, our models outperform lattice model <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref>. The above experimental results strongly  some comparative experiments on training time and convergence speed. The lattice model proposed in <ref type=\"bibr\" target=\"#b40\">(Zhang and Yang, 2018)</ref> is our principal comparison object, sinc ns/1.0\" type=\"table\" xml:id=\"tab_5\"><head></head><label></label><figDesc>are the most common methods<ref type=\"bibr\" target=\"#b40\">Zhang and Yang, 2018)</ref> 94.81 94.11 94.46 Character baseline 93.2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <ref type=\"bibr\" target=\"#b20\">(Ranganath et al., 2014)</ref>, to include BERT representations. Our . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nd efficiently segmenting large point clouds <ref type=\"bibr\" target=\"#b45\">[Wang et al., 2018</ref><ref type=\"bibr\" target=\"#b29\">, Li et al., 2019b]</ref>. Recent works have looked at frameworks to  20\">, Huang et al., 2017</ref><ref type=\"bibr\" target=\"#b53\">, Yu and Koltun, 2016]</ref>, DeepGCNs <ref type=\"bibr\" target=\"#b29\">[Li et al., 2019b]</ref> propose to train very deep GCNs (56 layers)   to be either SoftMax_Agg \u03b2 (\u2022) or PowerMean_Agg p (\u2022).</p><p>Better Residual Connections. DeepGCNs <ref type=\"bibr\" target=\"#b29\">[Li et al., 2019b]</ref> show residual connections <ref type=\"bibr\" t ., 2016]</ref> is used in every layer before the activation function ReLU.</p><p>ResGCN. Similar to <ref type=\"bibr\" target=\"#b29\">Li et al. [2019b]</ref>, we construct ResGCN by adding residual conne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: irectly. Two popular neural network sequence models are Connectionist Temporal Classification (CTC) <ref type=\"bibr\" target=\"#b10\">[10]</ref> and recurrent models for sequence generation <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref> .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uestion answering <ref type=\"bibr\" target=\"#b22\">(Yu et al., 2017)</ref>, knowledge base population <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, and biomedical knowledge discovery <ref t  our group compared (1) and ( <ref type=\"formula\" target=\"#formula_1\">2</ref>) with sequence models <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, and we report these results; for (3) we r TM), and showed that it outperforms several CNN and dependency-based models by a substantial margin <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>. We compare with this strong baseline, and etup</head><p>We conduct experiments on two relation extraction datasets: (1) TACRED: Introduced in <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017)</ref>, TACRED contains over 106k mention pairs d f the two entities.</p><p>More recently, <ref type=\"bibr\" target=\"#b0\">Adel et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b25\">Zhang et al. (2017)</ref> have shown that relatively simple neural mo la_1\">2</ref> For fair comparisons on the TACRED dataset, we follow the evaluation protocol used in <ref type=\"bibr\" target=\"#b25\">(Zhang et al., 2017</ref>) by selecting the model with the median dev. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: usion are personalized PageRank (PPR) <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b35\">[36]</ref>. PPR corresponds to choosing T = T rw and \u03b8 PPR k = \u03b1(1 \u2212  cal graph learning <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, especially for clu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f> which needs a CTC trained model to conduct pre-partition before the attention decoding.</p><p>In <ref type=\"bibr\" target=\"#b13\">[14]</ref>, Li al. present the important Adaptive Computation Steps (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: k, however, such approaches are still suffering from data sparsity along with cold-start challenges <ref type=\"bibr\" target=\"#b35\">(Pan, 2016)</ref>.</p><p>The cold start dilemma raises when a predict. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b49\">(Rogers et al., 2020)</ref>, which specifically studies the BERT model <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>.</p><p>In this work, we adapt and extend   In NLP, transformers are the backbone of state-of-the-art pre-trained language models such as BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>. BERTology focuses on interpreting what t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t-dependent sentiment classification. The approach is an extension on long short-term memory (LSTM) <ref type=\"bibr\" target=\"#b6\">(Hochreiter and Schmidhuber, 1997)</ref> by incorporating target infor e. These gates adaptively remember input vector, forget previous history and generate output vector <ref type=\"bibr\" target=\"#b6\">(Hochreiter and Schmidhuber, 1997)</ref>. LSTM cell is calculated as f problem of gradient vanishing or exploding <ref type=\"bibr\" target=\"#b1\">(Bengio et al., 1994;</ref><ref type=\"bibr\" target=\"#b6\">Hochreiter and Schmidhuber, 1997)</ref>, where gradients may grow or d. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hesizing it from scratch. It is most similar to recent work on sequence-tosequence voice conversion <ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" t #b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref>. <ref type=\"bibr\" target=\"#b15\">[16]</ref> uses a similar end-toend model, conditioned on speaker ide dard speech. In the future, we plan to test it on other speech disorders, and adopt techniques from <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref> to preserve the sp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: urately approximated by tracking a subset of randomly selected reuse distances and memory locations <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Finally, statistical cache modeling has been generalized an ations and computes their reuses during the warm-up interval prior to a detailed region. Prior work <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> uses watchpoints to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r algorithms were used in the case of the R-tree compression <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n fficient method or we must use and test the bulkloading (see <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>). Additionally, we must test our method for a real data col. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref>[8] <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" ta d 100003FA, which is often observed and utilized for prediction in the Markov prefetching algorithm <ref type=\"bibr\" target=\"#b9\">[10]</ref> . The following section discusses data prefetching methodol strides are recognizable. To capture repetitiveness in data reference addresses, Markov prefetching <ref type=\"bibr\" target=\"#b9\">[10]</ref> was proposed. This strategy assumes the history might repea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  true. In particular we demonstrate that the recently proposed AutoAugment data augmentation policy <ref type=\"bibr\" target=\"#b5\">[6]</ref> achieves state-of-the-art results on the CIFAR-10-C benchmar mentation strategies. Towards this end, we investigated the learned augmentation policy AutoAugment <ref type=\"bibr\" target=\"#b5\">[6]</ref>. AutoAugment applies a learned mixture of image transformati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <p>We employ three types of regularization during training:</p><p>Residual Dropout We apply dropout <ref type=\"bibr\" target=\"#b26\">[27]</ref> to the output of each sub-layer, before it is added to the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" tar rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> directly predict all keypoints at first and assemble them i stimation results are obtained when person clusters are combined with labeled body parts. DeeperCut <ref type=\"bibr\" target=\"#b18\">[19]</ref> improves DeepCut <ref type=\"bibr\" target=\"#b29\">[30]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: and ReLU activations. The initial convolutional layer starts with a large kernel, as recommended in <ref type=\"bibr\" target=\"#b6\">[7]</ref>, which helps limit the depth of the network while ensuring t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene <ref type=\"bibr\" target=\"#b12\">[13]</ref>. These complex pipelines are slow and hard to optimize bec  Then, classifiers <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> or localizers <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arity function, we choose cosine similarity as it is one of the commonly used in embedding learning <ref type=\"bibr\" target=\"#b6\">[7]</ref>:</p><formula xml:id=\"formula_3\">S(Q, D) = cos(E Q , E D ) =  nt features that contributed to the major model improvements.</p><p>Text features. Character n-gram <ref type=\"bibr\" target=\"#b6\">[7]</ref> is a common approach to represent text for text embedding. I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  methods train image embeddings through the local relationships between images in the form of pairs <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> or triplets <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b5\">[6]</ref> adopts a random vertex-cut method and two greedy variants for GP.</p><p>GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref> provides some heuristics, such as the grid-based constrained sting vertex-cut methods, such as random method in PowerGraph and grid-based method in GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref>, cannot make effective use of the powerlaw distribution to a \"bibr\" target=\"#b5\">[6]</ref> and the grid-based constrained solution (called Grid) of GraphBuilder <ref type=\"bibr\" target=\"#b7\">[8]</ref> are adopted as baselines. Our analysis is based on randomiza. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  \t\t<body> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">INTRODUCTION</head><p>Score matching <ref type=\"bibr\" target=\"#b12\">(Hyv\u00e4rinen, 2005)</ref> is particularly suitable for learning unnorma MLE) can be difficult due to the intractable partition function Z \u03b8 . To avoid this, score matching <ref type=\"bibr\" target=\"#b12\">(Hyv\u00e4rinen, 2005)</ref> minimizes the Fisher divergence between p d a  not have access to the score function of the data s d (x).</p><p>By applying integration by parts, <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref> shows that L(\u03b8) can be written as L(\u03b8) = J(\u03b8)  </p><p>Other than our requirements on p v , the assumptions are exactly the same as in Theorem 1 of <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>. We advise the interested readers to read Appe he consistency of <ref type=\"bibr\">MLE (van der Vaart, 1998)</ref>. We also adopt the assumption in <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref> that all densities are strictly positive (Assu M . These two facts lead to consistency. For a complete proof, see Appendix B.3.</p><p>Remark 1. In <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>, the authors only showed that J(\u03b8) = 0 \u21d4 \u03b8 = \u03b8 s a constant w.r.t. \u03b8.</p><p>Proof. The basic idea of this proof is similar to that of Theorem 1 in <ref type=\"bibr\" target=\"#b12\">Hyv\u00e4rinen (2005)</ref>. First, note that L(\u03b8, p v ) can be expanded t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng pseudo-labels with a hybrid system can also give an improvement to WER on a large-scale data set <ref type=\"bibr\" target=\"#b32\">[33]</ref> and in another, a student-teacher approach without an exte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on of the ReLU activations similar to that in <ref type=\"bibr\" target=\"#b14\">(Kawaguchi, 2016;</ref><ref type=\"bibr\" target=\"#b2\">Choromanska et al., 2015)</ref>, we can draw connections between GCNs . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olution operations <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b40\">41,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>. GC-MC <ref type=\"bibr\" target=\"#b28\">[29]</ref> applies th t is captured on the level of item relations, rather than the collective user behaviors. SpectralCF <ref type=\"bibr\" target=\"#b41\">[42]</ref> proposes a spectral convolution operation to discover all  , is used as suggested in <ref type=\"bibr\" target=\"#b28\">[29]</ref>.</p><p>We also tried SpectralCF <ref type=\"bibr\" target=\"#b41\">[42]</ref> but found that the eigen-decomposition leads to high time . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>, global color shift <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Neural Machine Translation (NMT; <ref type=\"bibr\" target=\"#b12\">Cheng et al., 2018)</ref>, and ASR <ref type=\"bibr\" target=\"#b45\">(Sperber et al., 2017)</ref>, we propose two Noise-Aware Training (NA mputer vision <ref type=\"bibr\" target=\"#b27\">(Krizhevsky et al., 2012)</ref> and speech recognition <ref type=\"bibr\" target=\"#b45\">(Sperber et al., 2017)</ref>.</p><p>During training, we artificially . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e et al <ref type=\"bibr\" target=\"#b27\">[28]</ref> performed feature denoising using non-local means <ref type=\"bibr\" target=\"#b48\">[49]</ref> to improve the robustness of convolutional networks. Their. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: class, and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type=\"bibr\" target=\"#b1\">Caruana (1994)</ref> and <ref type=\"bibr\" target=\"#b0\">Bengio (2011)</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he transferability <ref type=\"bibr\" target=\"#b11\">[10,</ref><ref type=\"bibr\" target=\"#b20\">19,</ref><ref type=\"bibr\" target=\"#b8\">7]</ref>, an adversarial example is usually generated for a single inp box attacks than FGSM at the cost of worse transferability <ref type=\"bibr\" target=\"#b17\">[16,</ref><ref type=\"bibr\" target=\"#b8\">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) =\"#b20\">[19]</ref> of adversarial examples can be used to attack a black-box model. Several methods <ref type=\"bibr\" target=\"#b8\">[7,</ref><ref type=\"bibr\" target=\"#b38\">37]</ref> have been proposed t rg/ns/1.0\"><head n=\"3.2.\">Translation-Invariant Attack Method</head><p>Although many attack methods <ref type=\"bibr\" target=\"#b8\">[7,</ref><ref type=\"bibr\" target=\"#b38\">37]</ref> can generate adversa ref type=\"bibr\" target=\"#b8\">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type=\"bibr\" target=\"#b8\">[7]</ref> proposes to improve the transferability of adversarial examp formula_13\">)</formula><p>The translation-invariant method can be similarly integrated into MI-FGSM <ref type=\"bibr\" target=\"#b8\">[7]</ref> and DIM <ref type=\"bibr\" target=\"#b38\">[37]</ref> as TI-MI-F ) <ref type=\"bibr\" target=\"#b11\">[10]</ref>, momentum iterative fast gradient sign method (MI-FGSM) <ref type=\"bibr\" target=\"#b8\">[7]</ref>, and diverse inputs method (DIM) <ref type=\"bibr\" target=\"#b target=\"#b6\">[5]</ref> since that they are not good at generating transferable adversarial examples <ref type=\"bibr\" target=\"#b8\">[7]</ref>. We denote the attacks combined with our translation-invaria  more likely to transfer to another black-box model.</p><p>We adopt the ensemble method proposed in <ref type=\"bibr\" target=\"#b8\">[7]</ref>, which fuses the logit activations of different models. We a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is sur-prisingly difficult.\" <ref type=\"bibr\" target=\"#b18\">[18]</ref> If humans have a hard time telling the difference, how muc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: In the context of maintaining multiple versions of the data in a B + -tree, it has been proposed in <ref type=\"bibr\" target=\"#b3\">[3]</ref> to use, where possible, the free space in nodes. Other studi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: information retrieval community and search engine industry as the next generation search technology <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p><p>In general, a search engine comprises a recall laye l in Facebook search is not a text embedding problem, as is actively researched in the IR community <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Instead it is a more complex problem that requires unders. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ead n=\"3.2\">Tucker Decomposition</head><p>Tucker decomposition, named after Ledyard R.</p><p>Tucker <ref type=\"bibr\" target=\"#b26\">(Tucker, 1964)</ref>, decomposes a tensor into a set of matrices and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>.</p><p>Query-based black-box attacks can settle the suscept >32]</ref> or images <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. More related to our work is the regularization-based appro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: del well data with bi-modal distribution <ref type=\"bibr\" target=\"#b36\">[37]</ref>, such as the IRM <ref type=\"bibr\" target=\"#b37\">[38]</ref> and its variants <ref type=\"bibr\" target=\"#b35\">[36]</ref> /p><p>To obtain clean magnitudes, the oracle mask should be |Sc|/|X| (also known as the FFT mask in <ref type=\"bibr\" target=\"#b37\">[38]</ref> or the ideal amplitude mask in <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  SMARTS <ref type=\"bibr\" target=\"#b28\">[29]</ref>) and representative sampling (as done in SimPoint <ref type=\"bibr\" target=\"#b21\">[22]</ref>). Our experimental results using the SPEC CPU2000 benchmar st well known representative sampling approach is the SimPoint approach proposed by Sherwood et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref>. SimPoint picks a small number of sampling units that accu onsiders multiple randomly chosen cluster centers and uses the Bayesian Information Criterion (BIC) <ref type=\"bibr\" target=\"#b21\">[22]</ref> to assess the quality of the clustering: the clustering wi ifferent ways for doing so, such as code working sets <ref type=\"bibr\" target=\"#b5\">[6]</ref>, BBVs <ref type=\"bibr\" target=\"#b21\">[22]</ref>, procedure calls <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ilt before prefetching can be useful.</p><p>The closest prior work to ours is that of Nesbit et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> where a Global History Buffer (GHB) is proposed for holding  mlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_3\"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type=\"bibr\" target=\"#b3\">4</ref>. Performance with and without GHL-prefetching. Four cases are . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tences contribute to the classification decision which can be of value in applications and analysis <ref type=\"bibr\" target=\"#b21\">(Shen et al., 2014;</ref><ref type=\"bibr\">Gao et al., 2014)</ref>.</p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ges during training <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. An alternative to approximate the loss is to approximate t set as its own class <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref>. Dosovitskiy et al. <ref type=\"bibr\" target=\"#b15\">[16]</re ata. In addition, SwAV works with small and large batch sizes and does not need a large memory bank <ref type=\"bibr\" target=\"#b55\">[56]</ref> or a momentum encoder <ref type=\"bibr\" target=\"#b23\">[24]< h as many classes as images in the dataset. As this approach becomes quickly intractable, Wu et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref> mitigate this issue by replacing the classifier with a mem fferent augmentations of the same image. This solution is inspired by contrastive instance learning <ref type=\"bibr\" target=\"#b55\">[56]</ref> as we do not consider the codes as a target, but only enfo  feature. A similar comparison appears in contrastive learning where features are compared directly <ref type=\"bibr\" target=\"#b55\">[56]</ref>. In Fig. <ref type=\"figure\" target=\"#fig_0\">1</ref>, we il bel>2</label></formula><formula xml:id=\"formula_3\">)</formula><p>where \u03c4 is a temperature parameter <ref type=\"bibr\" target=\"#b55\">[56]</ref>. Taking this loss over all the images and pairs of data au cating pass forwards to the assignments. This is similar to the memory bank introduced by Wu et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, without momentum.</p><p>Assignment phase in DeepCluster-v .6 Image classification with KNN classifiers on ImageNet</head><p>Following previous work protocols <ref type=\"bibr\" target=\"#b55\">[56,</ref><ref type=\"bibr\" target=\"#b65\">66]</ref>, we evaluate the q. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ise insertion were conducted on data reconstruction and cross-entropy error objective functions. In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, a framework called DSSGD was proposed to ensure different aining sets of honest participants even if the model is trained in a privacy-preserving manner like <ref type=\"bibr\" target=\"#b23\">[24]</ref>. However, it was declared in <ref type=\"bibr\" target=\"#b28  from the data of all users. Our work is most related to <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, but is quite d e=\"bibr\" target=\"#b22\">[23]</ref> were not designed for the collaborative deep learning setting. In <ref type=\"bibr\" target=\"#b23\">[24]</ref>, participants only shared a subset of parameters with the  e privacy, but each pixel in the training data may be revealed by multiple gradients. Compared with <ref type=\"bibr\" target=\"#b23\">[24]</ref>, the scheme in <ref type=\"bibr\" target=\"#b29\">[30]</ref> c target=\"#b30\">[31]</ref> to obtain the sanitized parameters to minimize the privacy leakage. Unlike <ref type=\"bibr\" target=\"#b23\">[24]</ref> where noise is directly injected to the gradients, we appl ploads them to the server.</p><p>To achieve differential privacy, Laplace mechanism was utilized in <ref type=\"bibr\" target=\"#b23\">[24]</ref> to directly inject noise to the weights. However, their sc ion problem usually adopts cross-entropy error as the loss function. We construct a CNN the same as <ref type=\"bibr\" target=\"#b23\">[24]</ref> which has two convolutional layers, two max pooling layers  uploading, but it will not reveal the qualities or collude with some participants. As discussed in <ref type=\"bibr\" target=\"#b23\">[24]</ref>, anonymous authentication and communication techniques can ork, and n is the number of tuples in the test dataset.  For the binomial classification task, like <ref type=\"bibr\" target=\"#b23\">[24]</ref>, we use the MNIST and SVHN datasets as benchmarks. The MNI  are not specially designed for collaborative learning, we mainly compare all results with DSSGD in <ref type=\"bibr\" target=\"#b23\">[24]</ref> and <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and two bas nd perturbing objective functions to be the same. We fine tune the parameters in DSSGD according to <ref type=\"bibr\" target=\"#b23\">[24]</ref> and use the settings with the best performance (the parame. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  unknown true labels and some behavior assumptions, with examples of the Dawid-Skene (DS) estimator <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the minimax entropy (Entropy) estimator<ref type=\"foot\" tar ://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Dawid-Skene Estimator</head><p>The method of Dawid and Skene <ref type=\"bibr\" target=\"#b4\">[5]</ref> is a generative approach by considering worker confusability ed majority voting (IWMV) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the Dawid-Skene (DS) estimator <ref type=\"bibr\" target=\"#b4\">[5]</ref>, and the minimax entropy (Entropy) estimator <ref type=\"bibr m c = 2\u02c6[\u22128 : 0] and = <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> by the method in Sec. 6.2. As for Gibbs-CrowdSVM, we generate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: iv xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.3\">Software Based Polices</head><p>Eyerman et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a job scheduler based on sampling mechanism along. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Table <ref type=\"table\" target=\"#tab_0\">2</ref> is calculated according to the equation 2 and 3 of <ref type=\"bibr\" target=\"#b17\">[17]</ref>. Table <ref type=\"table\" target=\"#tab_0\">2</ref> shows tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad coverage of the protein universe, as found in the 17929 families of the recent Pfam 32.0 release <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Recent work that applies deep learning is either restrict database is carefully curated, at least 25% of sequences have no experimentally validation function <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and additional experimental functional characterization o of the art models including profile HMMs we use the highly curated Protein families (Pfam) database <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. The 17929 familie otKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Many domains have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 18\">[19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and transductive experimental design methods <ref type=\"bibr\" target=\"#b26\">[27]</ref>. These kinds of active learning algorithms are referred to ion matrix A. The selected samples are therefore considered to be the most representative.</p><p>In <ref type=\"bibr\" target=\"#b26\">[27]</ref>, an early active learning via a Transduction Experimental   problem to solve, thus an approximate solution by a sequential optimization problem is proposed in <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  nformative and representative examples for labeling using the min-max margin-based approach. 4. TED <ref type=\"bibr\" target=\"#b26\">[27]</ref> Active learning via Transduction Experimental Design is an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> embedding trained  g we refer to in this paper is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> embedding, which h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b27\">28]</ref> or multi-atlas techniques <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. In particular, atlas approaches benefit from implicit shap DSC) for atlas-based frameworks ranges from 69.6% to 73.9% <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. In <ref type=\"bibr\" target=\"#b38\">[39]</ref> a classificat le being an order of magnitude faster than, e.g., graph-cut and multi-atlas segmentation techniques <ref type=\"bibr\" target=\"#b33\">[34]</ref>. This is mainly attributed to the fact that (I) domain spe ble <ref type=\"table\" target=\"#tab_2\">3</ref>. U-Net model outperforms traditional atlas techniques <ref type=\"bibr\" target=\"#b33\">[34]</ref> although it was trained on a disjoint dataset. Moreover, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: also argue that training objectives of these algorithms (either reconstructing the adjacency matrix <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or feature matrix  CN as the encoder, then decode by inner product with cross-entropy loss. As variants of GAE (VGAE), <ref type=\"bibr\" target=\"#b22\">[23]</ref> exploits adversarially regularized method to learn more ro ional networks with the (variational) autoencoder for representation learning.</p><p>ARGA and ARVGA <ref type=\"bibr\" target=\"#b22\">[23]</ref> add adversarial constraints to GAE and VGAE respectively, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: omponent in many applications, such as web searching, information filtering, and sentiment analysis <ref type=\"bibr\" target=\"#b0\">(Aggarwal and Zhai 2012)</ref>. Therefore, it has attracted considerab. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >(Peters et al., 2018)</ref>, GPT <ref type=\"bibr\" target=\"#b39\">(Radford et al., 2018)</ref>, BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, XLM <ref type=\"bibr\" target=\"#b25\">(Lamp  with changes in data size or composition.</p><p>We present a replication study of BERT pretraining <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, which includes a careful evaluation of t cently published methods. We release our model, pretraining and fine-tuning code.</p><p>Setup: BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> takes as input a concatenation of two seg ive training formats:</p><p>\u2022 SEGMENT-PAIR+NSP: This follows the original input format used in BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>, with the NSP loss. Each input has a pair d diverse corpora, such as the ones considered in this work.</p><p>The original BERT implementation <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>  Early experiments revealed only minor di SQUAD RESULTS</head><p>We adopt a much simpler approach for SQuAD compared to past work. While BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> and XLNet <ref type=\"bibr\" target=\"#b56\">  submit RoBERTa to the public SQuAD 2.0 leaderboard. Most of the top systems build upon either BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref> or XLNet <ref type=\"bibr\" target=\"#b56\">( )</ref>. This formulation significantly simplifies the task, but is not directly comparable to BERT <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019)</ref>. Following recent work, we adopt the rank training with large batch sizes.</p><p>We pretrain with sequences of at most T = 512 tokens. Unlike <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>, we do not randomly inject short sequences b4\">(Bowman et al., 2015)</ref>, which require predicting relationships between pairs of sentences. <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> observe that removing NSP hurts performanc sults for the four different settings. We first compare the original SEGMENT-PAIR input format from <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> to the SENTENCE-PAIR format; both formats  that removing the NSP loss matches or slightly improves downstream task performance, in contrast to <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. It is possible that the original BERT imp  and end-task accuracy for BERT BASE as we increase the batch size, while tuning the learning rate. <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> originally trained BERT BASE for 1M steps  alf as many optimization steps, thus seeing four times as many sequences in pretraining compared to <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>.</p><p>To help disentangle the importance  ers). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. We pretrain our model using 1024 V100 GPU et=\"#b56\">Yang et al. (2019)</ref>.</p><p>For SQuAD v1.1 we follow the same finetuning procedure as <ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref>. For SQuAD v2.0, we additionally classify  ead>Table 7 :</head><label>7</label><figDesc>Comparison between the published BERT BASE results from<ref type=\"bibr\" target=\"#b11\">Devlin et al. (2019)</ref> to our reimplementation with either static ranslation <ref type=\"bibr\" target=\"#b30\">(McCann et al., 2017)</ref>, and masked language modeling <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b25\">Lample &amp  et al., 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref type=\"bibr\" target=\"#b11\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b1\">Baevski et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lue or a vector, depending on the choice of distribution). For more on the Hawkes process, see e.g. <ref type=\"bibr\" target=\"#b3\">Hawkes (1971b</ref><ref type=\"bibr\" target=\"#b4\">Hawkes ( ,a, 1972))</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the coverage analysis to automatically modify the directives to the test generator. For example, in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a genetic algorithm is used to select and modify test-cases. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar  that causes the performance degradation in previous works <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>.</p><p>In this pape  state-of-the-arts for defending against adversarial attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" tar ing with random normal perturbations) or adversarial noise <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref>, fail to improve ac pe=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. Meanwhile, recent works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" ta etter recognition than the vanilla training baseline. These results contradict previous conclusions <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b41\">42,</ref><ref type=\"bibr\" ta -1 accuracy on ImageNet, which beats the vanilla training baseline by 0.6%. However, previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> show adversarial t r\" target=\"#b15\">16]</ref> show adversarial training always degrades performance.</p><p>Compared to <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, we make two chang ting noise. However, all previous attempts, by augmenting either with random noise (e.g., Tab. 5 in <ref type=\"bibr\" target=\"#b17\">[18]</ref> shows the result of training with random normal perturbati ojection step in PGD; or (2) we skip the random noise initialization step in PGD, turn it to I-FGSM <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Other attack hyper-parameters are unchanged: the maximum  Vanilla Training 81.7 83.7 84.5 PGD <ref type=\"bibr\" target=\"#b22\">[23]</ref> 81.8 84.3 85.2 I-FGSM <ref type=\"bibr\" target=\"#b17\">[18]</ref> 81.9 84. In Sec. 5.3, we show that adversarial training ca re of adversarial examples and clean images, as suggested in <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>,</p><formula xml:id=\"formula_3\">arg min \u03b8 E (x,y)\u223cD L(\u03b8, x, al and clean domains. However, as observed in former studies <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>, directly optimizing Eq. ( <ref type=\"formula\" target=\"#for  stronger performance than the adversarial training baseline <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Besides, compared to the fine-tuning strategy in Sec. 3, A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: o this end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015;</ref><ref type=\"bibr\" target=\"#b30\">Tai et al., 20 \"#b20\">(Munkhdalai and Yu, 2016b)</ref>.</p><p>We ensemble our ESIM model with syntactic tree-LSTMs <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015)</ref> based on syntactic parse trees and achieve s here are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block <ref type=\"bibr\" target=\"#b35\">(Zhu et al., 2015)</ref> same as in model ( <ref type=\"formula\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: shown that ConvNets can be directly applied to distributed <ref type=\"bibr\" target=\"#b5\">[6]</ref>  <ref type=\"bibr\" target=\"#b15\">[16]</ref> or discrete <ref type=\"bibr\" target=\"#b12\">[13]</ref> embe arisons with both using the pretrained word2vec <ref type=\"bibr\" target=\"#b22\">[23]</ref> embedding <ref type=\"bibr\" target=\"#b15\">[16]</ref> and using lookup tables <ref type=\"bibr\" target=\"#b4\">[5]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he summation operation may impede the information flow in deep networks.</p><p>Inspired by Densenet <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>, we propose a densely-connected recurrent n et al. 2016;</ref><ref type=\"bibr\" target=\"#b31\">Wu et al. 2016</ref>). More recently, Huang et al. <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref> enable the features to be connected from lo , the summation operation in the residual connection may impede the information flow in the network <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>. Motivated by Densenet <ref type=\"bibr\" tar flow in the network <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>. Motivated by Densenet <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>, we employ direct connections using the con ch to the uppermost layer and all the previous features work for prediction as collective knowledge <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org  max-valued features of every layer affect the loss function and perform a kind of deep supervision <ref type=\"bibr\" target=\"#b12\">(Huang et al. 2017</ref>). Thus, we could cautiously interpret the cl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: trated its effectiveness on folding a single protein chain <ref type=\"bibr\" target=\"#b18\">[19]</ref><ref type=\"bibr\" target=\"#b19\">[20]</ref><ref type=\"bibr\" target=\"#b20\">[21]</ref><ref type=\"bibr\" t l information and pairwise potential between any two columns in a paired MSA. See our previous work <ref type=\"bibr\" target=\"#b19\">[20]</ref> for details.</p></div> <div xmlns=\"http://www.tei-c.org/ns atenated sequences and our training proteins. Please see our previous work on training our DL model <ref type=\"bibr\" target=\"#b19\">[20]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head> 42]</ref>.</p><p>We have developed a deep learning (DL) method for intra-protein contact prediction <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>, which greatly out  which predicts the probability of any two residues forming a contact. Please see our previous work <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref> for a detailed des. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e the foundation for building social recommender systems <ref type=\"bibr\" target=\"#b24\">[25]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bib he sparsity in CF and enhance recommendation performance <ref type=\"bibr\" target=\"#b25\">[26]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Due to the sup  also been extended with social circles <ref type=\"bibr\" target=\"#b28\">[29]</ref>, temporal context <ref type=\"bibr\" target=\"#b32\">[33]</ref>, rich contextual information <ref type=\"bibr\" target=\"#b35 ave been proposed to learn the social influence strength <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to guide the pixel denoiser; in contrast, our denoising is applied directly on features. Guo et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> transform the images via non-differentiable image preprocess ts of their non-differentiable computations <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In contrast to <ref type=\"bibr\" target=\"#b7\">[8]</ref>, our feature denoising models are differentiable, but are st y increases as the image is propagated through the network <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, and non-existing activations in the feature maps are halluci. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  samples. Both of them need an additional set of samples with true labels to identify wrong labels. <ref type=\"bibr\" target=\"#b26\">Liu and Tao (2014)</ref> propose an optimal importance weighting para. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  compared to DNNs <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Recently, very deep CNNs architectures <ref type=\"bibr\" target=\"#b13\">[14]</ref> have also been shown to be successful in ASR <ref type=\"bi R, recently there have been several advancements in the computer vision community on very deep CNNs <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref> that have not been TMs.</p><p>We are driven by same motivation that led to the success of very deep networks in vision <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  graph, including auxiliary training <ref type=\"bibr\" target=\"#b18\">[19]</ref>, multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, and knowledge distill  related tasks simultaneously so that knowledge obtained from each task can be reused by the others <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target ef type=\"figure\" target=\"#fig_1\">1 (c</ref>). This structure is very similar to multi-task learning <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, in which different su. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ion is built.</p><p>Objective Function We optimize the parameters of PNet using REINFORCE algorithm <ref type=\"bibr\" target=\"#b24\">(Williams 1992</ref>) and policy gradient methods <ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tentially enables. Our future work includes applications of our method to Recurrent Neural Networks <ref type=\"bibr\" target=\"#b12\">(Pascanu et al., 2013)</ref>, where the internal covariate shift and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion along the z-axis cannot be aggregated and taken into consideration. However, there is evidence <ref type=\"bibr\" target=\"#b12\">[13]</ref> that conventional 3D segmentation methods deteriorate in p tion of the variant proposed in <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Based on past experience <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> we favor this formul. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: able place to look is human cognition <ref type=\"bibr\" target=\"#b7\">(Davis &amp; Marcus, 2015;</ref><ref type=\"bibr\" target=\"#b29\">Lake et al., 2016;</ref><ref type=\"bibr\" target=\"#b42\">Marcus, 2001;<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">[33]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b47\">[47]</ref> require large hardware resources to store the target addre ional muxes used to select the predicted target address.</p><p>In a recent work, Seznec and Michaud <ref type=\"bibr\" target=\"#b47\">[47]</ref> proposed extending their TAGE conditional branch predictor \">[13]</ref>, <ref type=\"bibr\" target=\"#b13\">[14]</ref>, <ref type=\"bibr\" target=\"#b33\">[33]</ref>, <ref type=\"bibr\" target=\"#b47\">[47]</ref>, which further increases the overall complexity and develo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ings as input and directly predict the number of clusters.</p><p>There is also a thread of research <ref type=\"bibr\" target=\"#b29\">[21,</ref><ref type=\"bibr\" target=\"#b37\">29]</ref> solving this probl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nteraction-focused <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or representation-focused <ref type=\"bibr\" target=\"#b14\">[1 h positions. It is also similar to the indicator matching matrix proposed previously by Pang et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref>. While the interaction matrix X perfectly captures every q <ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>.</p><p>Pang et al. <ref type=\"bibr\" target=\"#b30\">[31]</ref> propose the use of matching matrices to represent the simi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  is large enough but the quality of it depends on some hyper-parameters such as the range of \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b3\">[4]</ref> of negative ranking samples, which may cause the model hard   that \ud835\udc46 is ranked. So the sampler also needs to re-rank \ud835\udc46 by the diversification metric like \ud835\udefc-nDCG <ref type=\"bibr\" target=\"#b3\">[4]</ref>. In practice, half of the selected document ranking \ud835\udc46 is sam <ref type=\"bibr\">[2-4, 20, 21]</ref>, we use ERR-IA <ref type=\"bibr\" target=\"#b1\">[2]</ref>, \ud835\udefc-NDCG <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and NRBP <ref type=\"bibr\" target=\"#b2\">[3]</ref> as our div  train. We use 5-fold cross validation to tune the parameters in all experiments based on \ud835\udefc-nDCG@20 <ref type=\"bibr\" target=\"#b3\">[4]</ref>. A brief introduction to these baselines is as follows.</p><. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  based merely on implicit feedbacks, i.e., user clicks, in the current session.</p><p>Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> apply recurrent neural networks (RNN) with Gated Recurrent al Networks (RNN) have been devised to model variable-length sequence data. Recently, Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> apply RNN to sessionbased recommendation and achieve signi ARM. We use a RNN with Gated Recurrent Units (GRU) rather than a standard RNN because Hidasi et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> demonstrate that GRU can outperform the Long Short-Term Me re S i , To learn the parameters of the model, we do not utilize the proposed training procedure in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, where the model is trained in a session-parallel, sequenc sessions of subsequent week for testing. Because we did not train NARM in a session-parallel manner <ref type=\"bibr\" target=\"#b11\">[12]</ref>, a </p><formula xml:id=\"formula_15\">],V (x 2 ), ([x 1 , x  nt representations when computing recommendation scores. \u2022 GRU-Rec: We denote the model proposed in <ref type=\"bibr\" target=\"#b11\">[12]</ref> as GRU-Rec, which utilizes session-parallel mini-batch tra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: peech-to-text translation systems as a single neural network <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>. Such end-to-end systems have advantages over a traditional c work has focused on training end-to-end ST in a single model <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>.</p><p>In order to utilize both fully supervised data and als end-to-end systems. Recently explored techniques to mitigate this issue include multi-task learning <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> and pre-trained compo b8\">9]</ref>.</p><p>In order to utilize both fully supervised data and also weakly supervised data, <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref> use multi-task learni the 1M ST set. We then adopt pretraining and multi-task learning as proposed in previous literature <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.\">MODELS</head><p>Similar to <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we make use of three sequence-to-sequence models. Each one . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 59.5 LSTM <ref type=\"bibr\" target=\"#b42\">(Yang et al., 2018)</ref> VD, WT, 2, AWD, MoC 22 57.6 LSTM <ref type=\"bibr\" target=\"#b32\">(Merity et al., 2017)</ref> VD, WT, 2, AWD 24 57.3 LSTM <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: l., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Gui et al., 2019b)</ref>.</p><p>Recently, Transformer <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, l \"#b34\">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, like machine translation <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>, language modeling <ref type=\"bibr\" targ mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Transformer</head><p>Transformer was introduced by <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>, which was mainly based on self-attentio =\"bibr\" target=\"#b7\">Devlin et al., 2018)</ref>. Instead of using the sinusoidal position embedding <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref> and learned absolute position embedding, 1\">Transformer Encoder Architecture</head><p>We first introduce the Transformer encoder proposed in <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>. The Transformer encoder takes in an mat e Transformer encoder includes layer normalization and Residual connection, we use them the same as <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>.</p></div> <div xmlns=\"http://www.tei-c. ng it unable to capture the sequential characteristic of languages. In order to solve this problem, <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref> suggested to use position embeddings gen  two tokens. For any fixed offset k, P E t+k can be represented by a linear transformation of P E t <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017)</ref>. In TENER, Transformer encoder is used n d in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used <ref type=\"bibr\" target=\"#b34\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b7\">Devlin et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: me works have shown that users can reduce their costs by using spot rather than on-demand instances <ref type=\"bibr\" target=\"#b26\">[28,</ref><ref type=\"bibr\" target=\"#b35\">37]</ref>, they only conside. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: encoder based models <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> have been proposed for network representation learning, and r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: onsisting of the whole historical transaction data. Another approach is called neighborhood methods <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which try to make recommendations based on item similarit RS model that always recommends items based on occurrence frequency in the training set. \u2022 Item-KNN <ref type=\"bibr\" target=\"#b13\">[14]</ref>: An item-to-item model which recommends items similar to t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ll adversarial perturbations to the data <ref type=\"bibr\" target=\"#b25\">(Szegedy et al., 2014;</ref><ref type=\"bibr\" target=\"#b12\">Goodfellow et al., 2015)</ref>. The vast majority of attacks and defe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch that it covers a predefined set of downsampling kernels <ref type=\"bibr\" target=\"#b37\">[38,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>; using DNNs to capture only a natural-image prior which is  adation model and assumes that the downsampling kernels belong to a certain set of Gaussian filters <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Another approach  estimation methods <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>.</p><p>Finally, we wo f>.</p><p>Finally, we would like to highlight major differences be-tween this paper and the work in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, whose \"kernel correction\" approach may be misunderstood a >[13]</ref>, whose \"kernel correction\" approach may be misunderstood as our \"correction filter\". In <ref type=\"bibr\" target=\"#b12\">[13]</ref>, three different DNNs (super-resolver, kernel estimator, a ned existing DNN methods (other than SRMD <ref type=\"bibr\" target=\"#b37\">[38]</ref>) can be used in <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Secondly, their approach is restricted by the offline tra ur approach. Thirdly, the concepts of these works are very different: The (iterative) correction in <ref type=\"bibr\" target=\"#b12\">[13]</ref> modifies the estimated downsampling kernel, while our corr erformed with the official code of each method. Unfortunately, such code has not been available for <ref type=\"bibr\" target=\"#b12\">[13]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: class, and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type=\"bibr\" target=\"#b1\">Caruana (1994)</ref> and <ref type=\"bibr\" target=\"#b0\">Bengio (2011)</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ich are \"unnoticeable\" to humans but can cause the learning models to misclassify some target nodes <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., >Limitations of Current Approaches. There are two common issues of existing works on attacking GCNs <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., ch puts a high demand on the scalability of the underlying attack models. However, existing efforts <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Dai et al., e directly and then treat the newly added nodes as existing nodes and apply the attacks proposed in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Z\u00fcgner and  e attackers can benefit more by additionally manipulating features of nodes (shown in Section 5.4). <ref type=\"bibr\" target=\"#b34\">Z\u00fcgner et al. (2018)</ref> propose Nettack, which manipulates both ed ology</head><p>Different from previous works <ref type=\"bibr\" target=\"#b12\">(Dai et al., 2018;</ref><ref type=\"bibr\" target=\"#b34\">Z\u00fcgner et al., 2018;</ref><ref type=\"bibr\" target=\"#b33\">Z\u00fcgner and G ation performs on the target node v 0 .</p><p>It is noteworthy that the surrogate model proposed in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> is typically used to generate perturbatio atures (e.g., mutually exclusive features), it will be easily detected. For this issue, the work in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> proposes a statistical test based on the  p>In this paper, we consider three methods designed for the traditional scenario, including Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcg tional scenario, including Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> and Metaattack <ref type=\"bibr\" target=\"# ection 3.2 which is designed specifically for the new attack scenario instead of the constraints in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>.</p><p>There are two strategies to adapt  d features. We call this strategy sequential injection.</p><p>Nettack for the new scenario. Nettack <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> addresses the bilevel optimization proble cores from the old scores after each iteration in constant time.</p><p>According to the analysis in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, the time complexity of Nettack in terms  ion of features, it needs O (\u2206 e n in f d) just like Nettack.</p><p>FGSM for the new scenario. FGSM <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> computes the gradients of L atk w.r.t. ed he detailed statistics of these datasets are shown in Table 1. Following the same attack setting in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, we only consider the largest connected c  one-time injection can be roughly treated as the special case of the attack scenario considered in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref> as nodes are added in advance and perturb ous nodes directly because connections to the target node usually lead to better attack performance <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>. However, we do not know the optimal numb  FGSM performs better than Nettack in the new scenario which is quite different from the results in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>. We hypothesize that it may be due to the features. Considering that the results of target nodes with higher degrees are harder to be mislead <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, we attack the target nodes with d v 0 /2 the attack performance significantly. This is in contrast to the findings in the attack scenario of <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, where the authors observe that manip-ula res are not very important for successful attacks. Such contrast exists because, in the scenario of <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018)</ref>, the attacker can only perturb features o of vicious nodes to the target node, which is also consistent with our intuition and the results in <ref type=\"bibr\" target=\"#b34\">(Z\u00fcgner et al., 2018</ref>) that (more) connections with target node . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref>.</p><p>The most common paradigm for CF is to learn latent f et=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> that typically aggregate extended neighbors and need to han edding learning.</p><p>To deepen the use of subgraph structure with high-hop neighbors, Wang et al. <ref type=\"bibr\" target=\"#b38\">[39]</ref> recently proposes NGCF and achieves state-of-the-art perfo <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">PRELIMINARIES</head><p>We first introduce NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, a representative and state-of-the-art GCN model for recom  the scores of NGCF are directly copied from the Table <ref type=\"table\" target=\"#tab_4\">3</ref> of <ref type=\"bibr\" target=\"#b38\">[39]</ref>. As can be seen, removing feature transformation (i.e., NG >transformation and nonlinear activation. The graph convolution operation (a.k.a., propagation rule <ref type=\"bibr\" target=\"#b38\">[39]</ref>) in LightGCN is defined as:</p><formula xml:id=\"formula_5\" ms) that have overlap on interacted items (users), and higher-layers capture higher-order proximity <ref type=\"bibr\" target=\"#b38\">[39]</ref>. Thus combining them will make the representation more com TS</head><p>We first describe experimental settings, and then conduct detailed comparison with NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, the method that is most relevant with LightGCN but more c e experiment workload and keep the comparison fair, we closely follow the settings of the NGCF work <ref type=\"bibr\" target=\"#b38\">[39]</ref>. We request the experimented datasets (including train/tes ation is that increasing the layer number from 0 (i.e., the matrix factorization model, results see <ref type=\"bibr\" target=\"#b38\">[39]</ref>) to 1 leads to the largest performance gain, and  using a  \" target=\"#b26\">27]</ref>. Motivated by the strength of graph convolution, recent efforts like NGCF <ref type=\"bibr\" target=\"#b38\">[39]</ref>, GC-MC <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and PinS ifferent layers. The scores of NGCF on Gowalla and Amazon-Book are directly copied from the Table3of<ref type=\"bibr\" target=\"#b38\">[39]</ref>; the scores of NGCF on Yelp2018 are re-run by us.</figDesc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b5\">(Gao et al. 2012</ref>). However, traditional hypergraph learning methods <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref> suffer from their high comput ployed to model high-order correlation among data.</p><p>Hypergraph learning is first introduced in <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref>, as a propagation process on  the hypergraph structure. The task can be formulated as a regularization framework as introduced by <ref type=\"bibr\" target=\"#b23\">(Zhou, Huang, and Sch\u00f6lkopf 2007)</ref>:</p><formula xml:id=\"formula_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: private-attribute inference attack can be naturally formulated as a problem of adversarial learning <ref type=\"bibr\" target=\"#b18\">[19]</ref>. In our proposed RAP, there are two components: a Bayesian. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r convolutional auto-encoder network named RED for image denoising and SISR. Moreover, Zhang et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref> propose a denoising convolutional neural network (DnCNN) t <ref type=\"bibr\" target=\"#b19\">[20]</ref>, DRCN <ref type=\"bibr\" target=\"#b20\">[21]</ref> and DnCNN <ref type=\"bibr\" target=\"#b39\">[40]</ref> (Fig. <ref type=\"figure\" target=\"#fig_0\">1(a)</ref>), adop  Residual learning and adjustable gradient clipping are used to speed up the training. Zhang et al. <ref type=\"bibr\" target=\"#b39\">[40]</ref> introduced batch normalization into a DnCNN model to joint N <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b19\">[20]</ref> and DnCNN <ref type=\"bibr\" target=\"#b39\">[40]</ref> are compared using their public codes. MemNet recovers rel blocking Tab. 5 shows the JPEG deblocking results on Classic5 and LIVE1, by citing the results from <ref type=\"bibr\" target=\"#b39\">[40]</ref>.</p><p>Our network significantly outperforms the other met the residual image <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. Therefore, our basic MemNet can be formulated as,</p><form. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: igh quality for most languages, which can potentially improve the performance of end-to-end systems <ref type=\"bibr\" target=\"#b6\">[7]</ref>.</p><p>Sub-word representations have recently seen their suc d proposed a worddependent silence model to improve ASR accuracy; for use in end-to-end ASR models, <ref type=\"bibr\" target=\"#b6\">[7]</ref> investigated the value of a lexicon in end-to-end ASR. Sub-w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a is usually very time-consuming and costly <ref type=\"bibr\" target=\"#b43\">(Tao and Dai, 2019;</ref><ref type=\"bibr\" target=\"#b44\">Tavanaei et al., 2019)</ref>. In contrast, obtaining coarsely labeled. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e demonstrated superiority over reconstruction-based methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b33\">35]</ref> or other learning paradigms <ref type=\"bibr\">[20,</ref><ref get=\"#b21\">23]</ref>, we use a training dataset of 291 images, where 91 images are from Yang et al. <ref type=\"bibr\" target=\"#b33\">[35]</ref> and other 200 images are from Berkeley Segmentation Datase. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: till very effective Bag of Embeddings model <ref type=\"bibr\" target=\"#b45\">(White et al. 2015;</ref><ref type=\"bibr\" target=\"#b0\">Arora, Liang, and Ma 2017)</ref> showing that, even in this case, A re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: the short-term pattern of nodes, we apply the contextual attention-based model which is inspired by <ref type=\"bibr\" target=\"#b3\">[Liu et al., 2017]</ref> and proposed by <ref type=\"bibr\" target=\"#b1\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: struction performance. <ref type=\"bibr\">Kim et al.</ref> propose a 20-layer CNN model known as VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, which adopts residual learning and adaptive gradient clip eover, the traditional convolutional networks usually adopt cascaded network topologies, e.g., VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref> and DRC-N <ref type=\"bibr\" target=\"#b12\">[13]</ref>. In th cise structure of the proposed IDN, it is much faster than several CNN-based SR methods, e.g., VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRCN <ref type=\"bibr\" target=\"#b12\">[13]</ref>, LapSRN <r o accelerate SRCNN in combination with smaller filter sizes and more convolution layers. Kim et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> propose a very deep CNN model with global residual archite nerate the residual image. The bias term of this transposed convolution can auto-Dataset Scale VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref> DRCN <ref type=\"bibr\" target=\"#b12\">[13]</ref> LapSRN <ref  bicubic, SRCNN <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, VDSR <ref type=\"bibr\" target=\"#b11\">[12]</ref>, DRC-N <ref type=\"bibr\" target=\"#b12\">[13]</ref>, LapSRN < v> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.1\">Training datasets</head><p>By following <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>.</p><p>This fetch  div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Fetch Models</head><p>The stream fetch engine <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> model is shown in  for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. However, having h ww.tei-c.org/ns/1.0\"><head n=\"5.1\">The Multiple Stream Predictor</head><p>The next stream predictor <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, which is shown in zed code layout. In addition, data are shown for the original single-stream predictor, described in <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, and a 2-stream mu p>To avoid this increase in the fetch engine complexity, we propose to use long instruction streams <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> as basic predictio /p><p>Our instruction cache setup uses wide cache lines, that is, 4 times the processor fetch width <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and 64KB total hardware budget. The trace fetch architect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <head n=\"1.\">Introduction</head><p>Deep embedding learning is a fundamental task in computer vision <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which aims at learning a feature embedding that has the f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: omain texts.</p><p>Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar <ref type=\"bibr\" target=\"#b1\">(Ammar et al., 2018)</ref>. This corpus consists of 18% papers from th d on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus <ref type=\"bibr\" target=\"#b1\">(Ammar et al., 2018)</ref>. Furthermore, SCIBERT uses an in-domain voc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ow-resolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance <ref type=\"bibr\" target=\"#b53\">[51,</ref><ref type=\"bibr\" target=\"#b28\">26,</ref><ref type=\"bibr\" ta <ref type=\"bibr\" target=\"#b28\">[26]</ref>, DBPN <ref type=\"bibr\" target=\"#b18\">[16]</ref>, and RCAN <ref type=\"bibr\" target=\"#b53\">[51]</ref>. However, these methods still suffer from the large space  nsists of several up-and down-sampling layers to iteratively produce LR and HR images. Zhang et al. <ref type=\"bibr\" target=\"#b53\">[51]</ref> propose the channel attention mechanism to build a deep mo nlike the baseline U-Net, we build each basic block using B residual channel attention block (RCAB) <ref type=\"bibr\" target=\"#b53\">[51]</ref> to improve the model capacity. Following <ref type=\"bibr\"  are 2 dual models for 4\u00d7 SR and 3 dual models for 8\u00d7 SR, respectively. Let B be the number of RCABs <ref type=\"bibr\" target=\"#b53\">[51]</ref> and F be the number of base feature channels. For 4\u00d7 SR, w ctionbased methods <ref type=\"bibr\" target=\"#b18\">[16,</ref><ref type=\"bibr\" target=\"#b27\">25,</ref><ref type=\"bibr\" target=\"#b53\">51]</ref>. Haris et al. <ref type=\"bibr\" target=\"#b18\">[16]</ref> pro ata, and augment the training data following the method in <ref type=\"bibr\" target=\"#b28\">[26,</ref><ref type=\"bibr\" target=\"#b53\">51]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nes the cache entries of each encountered internal node, before continuing with the regular search; <ref type=\"bibr\" target=\"#b2\">(2)</ref> The traversal path of each search is dynamically recorded in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng other modalities, like music <ref type=\"bibr\" target=\"#b9\">(Huang et al., 2018)</ref> and images <ref type=\"bibr\" target=\"#b13\">(Parmar et al., 2018)</ref>, even longer sequences are commonplace. T erse data such as music scores <ref type=\"bibr\" target=\"#b9\">(Huang et al., 2018)</ref>, and images <ref type=\"bibr\" target=\"#b13\">(Parmar et al., 2018;</ref><ref type=\"bibr\" target=\"#b16\">Ramachandra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b21\">Redmon et al., 2016;</ref><ref type=\"bibr\" target=\"#b35\">Zhao et al., 2017;</ref><ref type=\"bibr\" target=\"#b12\">Krishna et al., 2017)</ref>. Their widespread use is attributable to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bserved, low-dimensional state spaces. Here we use recent advances in training deep neural networks <ref type=\"bibr\" target=\"#b3\">[9]</ref><ref type=\"bibr\" target=\"#b4\">[10]</ref><ref type=\"bibr\" targ get=\"#b10\">16</ref> known as deep neural networks. Notably, recent advances in deep neural networks <ref type=\"bibr\" target=\"#b3\">[9]</ref><ref type=\"bibr\" target=\"#b4\">[10]</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: from cache sharing. For instance, many of them have used simulators rather than real machines; some <ref type=\"bibr\" target=\"#b24\">[25]</ref> have used old benchmark suites (e.g., SPLASH-2 <ref type=\" ormance of OpenMP applications on a Sun Fire V490 machine with private cache only. Tuck and Tullsen <ref type=\"bibr\" target=\"#b24\">[25]</ref> have measured the performance of SPLASH-2 when 2 threads c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e linked together. <ref type=\"bibr\" target=\"#b36\">[37]</ref> proposes a method using self-attention <ref type=\"bibr\" target=\"#b35\">[36]</ref> and bi-affine scoring algorithm to predict biological rela 14\">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Attention. We adapt Transformer <ref type=\"bibr\" target=\"#b35\">[36]</ref> to encode word sequences in a paragraph, where we calculat across sentences. We base on recent Transformer architecture <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> to build this module, due to its better performance in enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s (VAEs) have synthesized striking image and audio samples <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: /head><p>Previous survey papers on knowledge graphs mainly focus on statistical relational learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>, knowledge graph refinement <ref type=\"bibr\" target=\"#b5\">[6. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: vily dependent on the end-to-end data.</p><p>As our second contribution, we apply a two-stage model <ref type=\"bibr\" target=\"#b23\">(Tu et al., 2017;</ref><ref type=\"bibr\" target=\"#b10\">Kano et al., 20  intermediate representation closely tied to the source text. The architecture has been proposed by <ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref> to realize a reconstruction objective, and a s iliary ASR and MT training data ( \u00a73). This model is similar to the architecture first described by <ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref>. It combines two encoder-decoder models in a c , we apply beam search only for the second stage decoder. We do not use the two-phase beam search of<ref type=\"bibr\" target=\"#b23\">Tu et al. (2017)</ref> because of its prohibitive memory requirements. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: parameters using 600\u21e5600 images for training. In this paper, we focus on the ResNet-50 architecture <ref type=\"bibr\" target=\"#b10\">[11]</ref> due to its good accuracy/cost tradeoff (25.6M parameters)  -of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 <ref type=\"bibr\" target=\"#b10\">[11]</ref>. For larger experiments, we use PNASNet-5-Large <ref type=  although this means that sev- eral forward passes are required to classify one image. For example, <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" ta  Another performanceboosting strategy is to classify an image by feeding it at multiple resolutions <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: mental monitoring, intelligent agriculture <ref type=\"bibr\" target=\"#b4\">[5]</ref>, disaster relief <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, urban planning <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: eep learning entirely infeasible.</p><p>Applying RMD to hyperparameter optimization was proposed by <ref type=\"bibr\" target=\"#b3\">Bengio (2000)</ref> and <ref type=\"bibr\" target=\"#b2\">Baydin &amp; Pea , Eigenmann &amp; Nossek (1999)</ref>, <ref type=\"bibr\" target=\"#b9\">Chen &amp; Hagan (1999)</ref>, <ref type=\"bibr\" target=\"#b3\">Bengio (2000)</ref>, <ref type=\"bibr\" target=\"#b0\">Abdel-Gawad &amp; R. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) <ref type=\"bibr\" target=\"#b17\">(Sutskever et al., 2014)</ref> with attention paradigm <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  further analysis. We measure our performance on ImageNet [1] classification, COCO object detection <ref type=\"bibr\" target=\"#b1\">[2]</ref>, VOC image segmentation <ref type=\"bibr\" target=\"#b2\">[3]</r version of the Single Shot Detector (SSD) <ref type=\"bibr\" target=\"#b33\">[34]</ref> on COCO dataset <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We also compare to YOLOv2 <ref type=\"bibr\" target=\"#b34\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ation of a human operator, to segment objects from aerial images is widely used in segmenting roads <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and buildings <ref type=\"bibr\" target=\"#b6\">[7]</ref>. Also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lower resolution feature maps than the input images and shows coarse results in pixel-wise labeling <ref type=\"bibr\" target=\"#b3\">(Badrinarayanan et al., 2017)</ref>. <ref type=\"bibr\" target=\"#b25\">Lo /head><p>FCN models, such as FCN8s <ref type=\"bibr\" target=\"#b25\">(Long et al., 2015)</ref>, SegNet <ref type=\"bibr\" target=\"#b3\">(Badrinarayanan et al., 2017)</ref>, DeconvNet <ref type=\"bibr\" target ooling layer enlarges the features by reusing the locations of maxima within each max-pooling layer <ref type=\"bibr\" target=\"#b3\">(Badrinarayanan et al., 2017)</ref> and the upsampling layer is often  ed modules of CNNs that transmit the rich low/mid-level features into the upsampling stages. SegNet <ref type=\"bibr\" target=\"#b3\">(Badrinarayanan et al., 2017)</ref> upsamples the decoder features usi rative studies using different networks</head><p>Five state-of-the-art FCN models, including SegNet <ref type=\"bibr\" target=\"#b3\">(Badrinarayanan et al., 2017)</ref>, DeconvNet <ref type=\"bibr\" target s high efficiency. The pooling indices in the encoder part in SegNet are reused in the decoder part <ref type=\"bibr\" target=\"#b3\">(Badrinarayanan et al., 2017)</ref>.</p></div> <div xmlns=\"http://www. =\"bibr\" target=\"#b24\">Liu et al., 2017b)</ref> or reusing the maximum indices of the pooling layers <ref type=\"bibr\" target=\"#b3\">(Badrinarayanan et al., 2017;</ref><ref type=\"bibr\" target=\"#b31\">Noh . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the context of deep neural networks, and there is now a quickly growing body of work on this topic <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" targ e, the \u221e -ball around x has recently been studied as a natural notion for adversarial perturbations <ref type=\"bibr\" target=\"#b10\">[11]</ref>. While we focus on robustness against \u221e -bounded attacks i s. On the attack side, prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) <ref type=\"bibr\" target=\"#b10\">[11]</ref> and multiple variations of it <ref type=\"bibr\" target=\"#b1 rget=\"#b2\">[3]</ref> for an overview of earlier work).</p><p>Adversarial training was introduced in <ref type=\"bibr\" target=\"#b10\">[11]</ref>, however the adversary utilized was quite weak-it relied o arial training discusses the phenomenon of transferability <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>-adversarial example. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b34\">34,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">18,</ref><ref type=\"bibr\" target=\"#b17\">17]</ref> have been done on accelerating neural networks by compressi o and can be accelerated with specialized hardware such as <ref type=\"bibr\" target=\"#b18\">[18,</ref><ref type=\"bibr\" target=\"#b17\">17,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>. However, it requir. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ative filtering (CF) <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>, content-based methods <ref type=\"bibr\" target=\"#b1\">[2]</r rs' behavior history <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. In our work, using random walk in the item graph, we can c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: thor. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, paper-author network <ref type=\"bibr\" targ for experiments: \u2022 AMiner-AND<ref type=\"foot\" target=\"#foot_0\">1</ref> . The dataset is released by <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018)</ref>, which contains 500 author names for traini (Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of me fully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>): This model designs a supervised global sta D, we use 100 names for testing and compare the result with the results of other models reported in <ref type=\"bibr\" target=\"#b21\">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ncodes grouped sparsity by assuming that all tasks share a common set of features.</p><p>\u2022 cASO-MTL <ref type=\"bibr\" target=\"#b3\">[Chen et al., 2009]</ref>: It is a convex relaxation of the alternatin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: o path-based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, embedding-based methods <ref type=\"bibr\" target=\"#b8\">[9,< ) Path-based methods <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref> explore various patterns of connections among items in a KG. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: formulation makes our learning gradient efficient (see Section 3.2.3 for more details). The work in <ref type=\"bibr\" target=\"#b14\">[15]</ref> also uses a similar loss formulation to ours; however it i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b16\">[17]</ref>.</p><p>A more direct approach to LTR metric optimization was proposed by Qin et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref>, where the rank variable in the definition of metrics like Recent hardware and software advances in the training of neural networks, however, make the work in <ref type=\"bibr\" target=\"#b13\">[14]</ref> relevant again and potentially allow us to harvest the eff ng state-of-the-art LTR algorithms such as LambdaMART. We give an overview of LTR and in particular <ref type=\"bibr\" target=\"#b13\">[14]</ref> in Section 2. We discuss experimental results in Section 3 \"#b14\">[15]</ref>, boosting <ref type=\"bibr\" target=\"#b18\">[19]</ref>, and approximating the metric <ref type=\"bibr\" target=\"#b13\">[14]</ref>. It is the latter that can tightly bound any ranking metri \" target=\"#b13\">[14]</ref>. It is the latter that can tightly bound any ranking metric such as NDCG <ref type=\"bibr\" target=\"#b13\">[14]</ref> and can be easily optimized with gradient descent.</p><p>S adient descent.</p><p>Surprisingly, despite its attractive theoretical properties, the framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> has received little attention in LTR studies in the decade ts to optimize NDCG-referred to as ApproxNDCG. Our results show that  the theoretical guarantees in <ref type=\"bibr\" target=\"#b13\">[14]</ref> materialize in practice. Before we go any further, we give  the indicator which is 1 if s &lt; t and 0 otherwise. <ref type=\"bibr\">Qin et al.</ref> propose in <ref type=\"bibr\" target=\"#b13\">[14]</ref> a smooth approximation of Equation <ref type=\"formula\" tar ture of ranking utility functions.</p><p>In this work, we set out to revisit the work of Qin et al. <ref type=\"bibr\" target=\"#b13\">[14]</ref> which formulates a smooth approximation to any ranking met  any ranking metric such as NDCG. Unlike many other existing surrogate LTR losses, the framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> offers a way to directly optimize ranking metrics. Because g metrics rather than loosely related surrogate losses; and (b) that the approximation framework in <ref type=\"bibr\" target=\"#b13\">[14]</ref> could lay out the foundation of deep neural networks in LT. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: three comparatively simple U-Net models that contain only minor modifications to the original U-Net <ref type=\"bibr\" target=\"#b5\">[6]</ref>. We omit recently proposed extensions such as for example th  we focus our efforts on designing an automatic training pipeline for these models.</p><p>The U-Net <ref type=\"bibr\" target=\"#b5\">[6]</ref> is a successful encoder-decoder network that has received a  tation framework for the medical domain that directly builds around the original U-Net architecture <ref type=\"bibr\" target=\"#b5\">[6]</ref> and dynamically adapts itself to the specifics of any given . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ) by just changing u \u2032T j to u T j . We adopt the asynchronous stochastic gradient algorithm (ASGD) <ref type=\"bibr\" target=\"#b43\">[17]</ref> for optimizing Eqn. <ref type=\"bibr\" target=\"#b33\">(7)</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ystems that construct photo-realistic head models using sophisticated physical and optical modeling <ref type=\"bibr\" target=\"#b0\">[1]</ref>, it is still excessive for most practical telepresence scena. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ual speech recognition involved the use of language independent features like articulatory features <ref type=\"bibr\" target=\"#b7\">[8]</ref> to train HMM based systems. Authors in <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ed 10 subsets by sampling ID in ascending order with a step size of 10 as in several previous works <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>, and report the ov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d by the number of stacked layers (depth). Recent evidence <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> reveals that network depth is of crucial importance, and th rk depth is of crucial importance, and the leading results <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" tar to the output <ref type=\"bibr\" target=\"#b32\">[33,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>. In <ref type=\"bibr\" target=\"#b42\">[43,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, a few intermediat entering layer responses, gradients, and propagated errors, implemented by shortcut connections. In <ref type=\"bibr\" target=\"#b42\">[43]</ref>, an \"inception\" layer is composed of a shortcut branch and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and gave competitive results when compared to systems with mono-lingual features. Other approaches <ref type=\"bibr\" target=\"#b17\">[17,</ref><ref type=\"bibr\" target=\"#b18\">18]</ref> constructed a shar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  t f , of dimension 300 for an input text (transcript), t.</p><p>Audio Feature Extraction openSMILE <ref type=\"bibr\" target=\"#b21\">[22]</ref> is an open-source toolkit used to extract high dimensional. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , and those have been very promising <ref type=\"bibr\" target=\"#b17\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b6\">Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Gilmer et al b28\">(Shervashidze et al., 2011;</ref><ref type=\"bibr\" target=\"#b17\">Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b6\">Hamilton et al., 2017)</ref>.</p><p>Yet, such aggregation schemes some \u2022 u\u2208 N (v) (deg(v)deg(u)) \u22121/2 h (l\u22121) u (2)</formula><p>where deg(v) is the degree of node v in G. <ref type=\"bibr\" target=\"#b6\">Hamilton et al. (2017)</ref> derived a variant of GCN that also works   and can be viewed as a form of a \"skip connection\" between different layers.For COMBINE, GraphSAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> uses concatenation after a feature trans o select the important neighbors via an attention mechanism. The max-pooling operation in GraphSAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> implicitly selects the important nodes.  ords features for each document (node) and citation links (edges) between documents. (II) On Reddit <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>, the task is to predict the community to ataset contains word vectors as node features. (III) For protein-protein interaction networks (PPI) <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> We compare against three baselines: Grap lutional Networks (GCN) <ref type=\"bibr\" target=\"#b17\">(Kipf &amp; Welling, 2017)</ref>, Graph-SAGE <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref> and Graph Attention Networks (GAT) <ref  r gives 6 JK-Net variants. We follow exactly the same setting of GraphSAGE as in the original paper <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>, where the model consists of 2 hidden la  the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities <ref type=\"bibr\" target=\"#b6\">(Hamilton et al., 2017)</ref>. As a result, this graph is more regular. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: traction <ref type=\"bibr\" target=\"#b0\">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type=\"bibr\" target=\"#b2\">[Diefenbach et al., 2018]</ref>, and other applications. Compared with \"#fig_0\">2</ref>). To incorporate the lexicon feature effectively, we use the vectorbased attention <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2018]</ref> to combine the l gram feature with the word . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and to classify them into pre-defined categories, e.g., person names, organizations, and locations <ref type=\"bibr\" target=\"#b48\">(Tjong Kim Sang and De Meulder, 2003)</ref>. NER systems are often tr ary loss objectives ( \u00a73.3, \u00a73.4)<ref type=\"foot\" target=\"#foot_8\">9</ref> . We used the CoNLL 2003 <ref type=\"bibr\" target=\"#b48\">(Tjong Kim Sang and De Meulder, 2003)</ref> and the GermEval 2014 <re ortunately, such data is scarce. On the other hand, labeled clean text corpora are widely available <ref type=\"bibr\" target=\"#b48\">(Tjong Kim Sang and De Meulder, 2003;</ref><ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: iction by reinterpretation of fully connected layers of the classifier as a fully convolution layer <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The FCN consists of an encoder of input images and a deco  overcome this problem, skip connection methods are used <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b24\">[25]</ref>. The features extracted from PPL are upsampled and concate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ke other deep learning models, GNNs have also been shown to be vulnerable under adversarial attacks <ref type=\"bibr\" target=\"#b27\">[28]</ref>, which has recently attracted increasing research interest sting (evasion); the attacker may aim to mislead the prediction on specific nodes (targeted attack) <ref type=\"bibr\" target=\"#b27\">[28]</ref> or damage the overall task performance (untargeted attack) model parameters, input data, and labels; grey-box attacks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> have partial inform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref>  Table <ref type=\"table\">1</ref>: Word Error Rates (WERs) obtained using MFCC implemented in <ref type=\"bibr\" target=\"#b57\">[58]</ref> and power mel filterbank coefficients on the Librispeech c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is critical to realism.</p><p>In contrast to methods that use a parametric model of the human face <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we directly predict the positions of face mesh vertices in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g by different researchers, though<ref type=\"bibr\" target=\"#b24\">(Srivastava and Sutton, 2017;</ref><ref type=\"bibr\" target=\"#b25\">Wang et al., 2020)</ref>. Our first results with this model were also. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b6\">[7]</ref> processes speech input directly into intent without going th tly the most promising results under-perform or just barely outperform traditional cascaded systems <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. One reason is that d to-end speech-to-intent model, we need intent-labeled speech data, and such data is usually scarce. <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> address this problem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  type=\"bibr\" target=\"#b7\">[8]</ref> . It is widely used in image segmentation due to its simplicity <ref type=\"bibr\" target=\"#b8\">[9]</ref> . However, pixels far away from cluster centers are easily t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ommunity, while most of the works focus on news and web data <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Recent neural network based methods have achieved great su  Predicting whether an algorithm candidate pair is compared forms a multi-instance learning problem <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. For each pair, a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vides tolerance against incorrect labels.</p><p>The recently introduced transform/stability loss of <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> is based on the same principle as our wo rements is \u223c0.5 percentage points better than independent flips.</p><p>A principled comparison with <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> is difficult due to several reasons. The  paths, and comparing the outputs of the network instead of pre-activation data of the final layer. <ref type=\"bibr\" target=\"#b20\">Sajjadi et al. (2016b)</ref> recently introduced a new loss function . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: esteps the complicated machinery developed for classical ASR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target= w.tei-c.org/ns/1.0\"><head n=\"2.1.\">Listen, Attend and Spell</head><p>Listen, Attend and Spell (LAS) <ref type=\"bibr\" target=\"#b2\">[3]</ref> is an attention-based seq2seq model which learns to transcri TM) <ref type=\"bibr\" target=\"#b24\">[25]</ref> network with hierarchical subsampling as described in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. In our work, we replace Listen with a network of very deep   lower dimension and apply BN and ReLU non-linearity to replace the skip subsampling connections in <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Moreover, we further increase the depth of the network by a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: atures for language processing. These include using character-level n-grams with linear classifiers <ref type=\"bibr\" target=\"#b14\">[15]</ref>, and incorporating character-level features to ConvNets <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef type=\"bibr\" target=\"#b26\">[27]</ref>, attention mechanism <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> and different ranking loss functions <ref type=\"bibr\" targe t is worth noting that in previous sequential recommendation literatures, such as Caser, GRURec and <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta plitting or shifting the input sequence), such as shown in Eq. ( <ref type=\"formula\">3</ref>) ( see <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" ta ndencies between x 0 and x 48 . To remedy this defect, when t &gt; 5, we follow the common approach <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> by manually creati  type=\"bibr\" target=\"#b29\">[30]</ref> and other RNN variants <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  segment proposal system in <ref type=\"bibr\" target=\"#b10\">[5]</ref> and object detection system in <ref type=\"bibr\" target=\"#b13\">[8]</ref> for \"fully convolutional instance segmentation\" (FCIS). The olutional instance segmentation\" (FCIS). The common idea in <ref type=\"bibr\" target=\"#b10\">[5,</ref><ref type=\"bibr\" target=\"#b13\">8,</ref><ref type=\"bibr\" target=\"#b26\">21]</ref> is to predict a set . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uilt efficient and concurrent memory allocator that implements ideas from prior scalable allocators <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b63\">64,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  type=\"bibr\" target=\"#b36\">[35,</ref><ref type=\"bibr\" target=\"#b37\">36]</ref> and adaptive training <ref type=\"bibr\" target=\"#b38\">[37,</ref><ref type=\"bibr\" target=\"#b39\">38]</ref> techniques for the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ly discussed in one-class collaborative filtering (OCCF) <ref type=\"bibr\" target=\"#b11\">[12]</ref>- <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Given user u, I + u = {i \u2208 I|r ui = 1} is the set of item imilarity matrices between users and items to predict drug-target interaction. Moreover, Yao et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed dual regularization by combining the weighted-and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ig_5\">6</ref>. For networks with few hosts, we need more detailed information for further analysis. <ref type=\"bibr\" target=\"#b90\">Minarik et al. (2009)</ref> proposed a host behavior profiling based . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: anguage processing <ref type=\"bibr\" target=\"#b11\">[13,</ref><ref type=\"bibr\" target=\"#b14\">16,</ref><ref type=\"bibr\" target=\"#b19\">21]</ref>. The teacher and student model in conventional knowledge di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b39\">Toutanova et al., 2015;</ref><ref type=\"bibr\" target=\"#b12\">Han et al., 2016;</ref><ref type=\"bibr\" target=\"#b4\">Cao et al., 2017</ref><ref type=\"bibr\" target=\"#b3\">Cao et al., , 2018. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  to a conventional design due to an increase in the access time on a filter cache miss. The L-Cache <ref type=\"bibr\" target=\"#b5\">[6]</ref> similarly reduces switching activity by holding loop-nested . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: erformance analysis of the proposed early active learning algorithms on person re-id.</p><p>1. NFST <ref type=\"bibr\" target=\"#b27\">[28]</ref> Null Foley-Sammon Transform space learning is a re-id algo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: function from R to R 2 and simply computes ReLU(x) and ReLU(\u2212x). Two-sided ReLU has been studied in <ref type=\"bibr\" target=\"#b26\">Shang et al. (2016)</ref> in convolution layers for accuracy improvem. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lt, many recent models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ently flat and do not learn hierarchical representations of graphs. On one hand, it demonstrates in <ref type=\"bibr\" target=\"#b19\">[20]</ref> that hierarchical representations of graphs can be combine ng GNNs with different clustering processes. In particular, the recently proposed approach DIFFPOOL <ref type=\"bibr\" target=\"#b19\">[20]</ref>, a differentiable graph pooling module that can generate h ing to be effective in graph classification tasks, in addition to a user's individual embedding. In <ref type=\"bibr\" target=\"#b19\">[20]</ref>, authors make some efforts in effectively co-training two  world e-commerce tasks of such large scale, including <ref type=\"bibr\" target=\"#b29\">[30]</ref> and <ref type=\"bibr\" target=\"#b19\">[20]</ref>. Our baseline algorithms are as follows:</p><p>\u2022 CGNN: A g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on the validation loss, with most models stopping between 8 and 13 epochs. We use singlelayer LSTMs <ref type=\"bibr\" target=\"#b7\">(Hochreiter and Schmidhuber, 1997)</ref> as recurrent networks. We use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: with the success of deep neural networks (DNNs), some researches have applied DNNs to precipitation <ref type=\"bibr\" target=\"#b20\">[21]</ref> and radar echo <ref type=\"bibr\" target=\"#b31\">[32]</ref> n ation and intensity of rain and snow. To solve the problem of spatiotemporal dependency, Shi et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> developed the conventional LSTM and propose convolutional  , with a goal to overcome the drawbacks of FC-LSTM in handling spatial-temporal data such as videos <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Specifically, in the ConvLSTM network, the fully-connecte. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe  going through an intermediate text transcript. There are many advantages of end-to-end SLU systems <ref type=\"bibr\" target=\"#b4\">[5]</ref>, the most significant of which is that E2E systems can direc  trained on increasingly relevant data until it is fine-tuned on the actual domain data. Similarly, <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref> advocate pre-trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b38\">39,</ref><ref type=\"bibr\" target=\"#b41\">42]</ref> mostly rely on the development of convolutional neural netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ome video generation methods have dealt with this problem by generating the entire sequence at once <ref type=\"bibr\" target=\"#b24\">[25]</ref> or in small batches <ref type=\"bibr\" target=\"#b19\">[20]</r d to handle videos <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>.</p><p>Straight-for ibr\" target=\"#b23\">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed in <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, replacing the 2D . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ve neural networks</head><p>The depth of neural networks is of central importance for deep learning <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" targ omes more difficult. Batch normalization layers are one of the cures for this problem in many tasks <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. It is also introduc skip connection) for SISR. SRResNet <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposed a ResNetlike <ref type=\"bibr\" target=\"#b8\">[9]</ref> network. Densely connected networks <ref type=\"bibr\" target= ions.</p><p>1 \u00d7 1 convolutions are widely used for channel number expansion or reduction in ResNets <ref type=\"bibr\" target=\"#b8\">[9]</ref>, ResNeXts <ref type=\"bibr\" target=\"#b37\">[38]</ref> and Mobi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uage processing (NLP) and is particularly important for knowledge base construction. The goal of RC <ref type=\"bibr\" target=\"#b19\">(Zelenko et al., 2003)</ref> is to identify the relation type of a gi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as designed to be representative of next-generation sharedmemory programs for chip-multiprocessors\" <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Our experiments show that for those programs, no matter how </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Benchmarks</head><p>We use PARSEC <ref type=\"bibr\" target=\"#b2\">[3]</ref> as the benchmark suite. It is a recently released suite desi  Because there is no close-form expression for the equation, the program uses numerical computation <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>The input data file of this benchmark includes an arr 6MB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : see<ref type=\"bibr\" target=\"#b2\">[3]</ref> for detail.</note></figure> <figure xmlns=\"http://www.tei-c. , as well as systems applications that mimic large-scale multithreaded commercial programs. Studies <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> have shown that the su  0 {0 2 4 6},{1 3 5 7},{0 2 4 6},{1 3 5 7} 1 161.6 0 {0 2 1 3},{4 5 6 7},{0 2 1 3},{4 5 6 7} 4 161. <ref type=\"bibr\" target=\"#b2\">3</ref> No binding 165.7</p><p>In PARSEC, ferret and dedup are two suc urement are relevant to this current work. Bienia and others <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> have shown a detailed exploration of the characterization of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ort, baseball is false since Matt Flynn is an NFL player), incompatible entity types, and many more <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. It has also been observed that such nois fectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>.</p></div> <div xmlns=\"http://www.tei-c.o exclusive (MUT and RMUT); and inverse relations (INV). We reproduce the list of information used in <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> in tabular form in Table <ref type=\"table mal distributions: N (0.7, 0.2) for facts in the original KG and N (0.3, 0.2) for added noisy facts <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. The SAMEENT facts between entities are g hyper-parameter threshold as the cutoff for classifying a test triple based on the prediction score <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref>. Our experiments were run on Intel(R) Xeo the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx e the probabilistic sources of information such as the confidence scores obtained during extraction <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b8\">, Jiang et al pe=\"bibr\" target=\"#b8\">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b19\">[Pujara et a ref type=\"bibr\" target=\"#b1\">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type=\"bibr\" target=\"#b18\">[Pujara et al., 2013</ref><ref type=\"bibr\" target=\"#b8\">, Jiang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: neural IR models unsuccessfully tried to match single vector representations per query and document <ref type=\"bibr\" target=\"#b20\">[21]</ref>. Then, interaction-focused models moved to a more fine-gra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tle as possible. We will elaborate.</p><p>There are previous works that achieve the first condition <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017;</ref><ref type=\"bibr\" target=\"#b17\">Hein &amp; A ef><ref type=\"bibr\" target=\"#b16\">Haber &amp; Ruthotto, 2017)</ref>. For example, Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> bound the Lipschitz constant by requiring   leads to degradation in nominal accuracy, average confidence gap and robustness. Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> can be viewed as models without L c term,   weight matrices and shows its effect in reducing generalization gap. The work on Parseval networks <ref type=\"bibr\" target=\"#b10\">(Cisse et al., 2017)</ref> shows that it is possible to control Lipsc ing the first condition, allows greater degrees of freedom in parameter training than the scheme in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>; a new loss function is specially designed   |M i,j | (2)</formula><p>The above is where our linear and convolution layers differ from those in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>: they require W W T to be an identity matri s to be 1; they also propose to restrict aggregation operations. The reported robustness results of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>, however, are much weaker than those by adv  common parameter with (9).</p><p>ResNet-like reconvergence is referred to as aggregation layers in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> and a different formula was used:</p><formu abel></formula><p>) where \u03b1 \u2208 [0, 1] is a trainable parameter. Because splitting is not modified in <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref>, their scheme may seem approximately equiva  preserve distances. In contrast, because splitting is not modified, at reconvergence the scheme of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> must apply the shrinking factor of 1 \u2212 \u03b1 on ents. We can also have a different t per channel or even per entry.</p><p>To be fair, the scheme of <ref type=\"bibr\" target=\"#b10\">Cisse et al. (2017)</ref> has an advantage of being nonexpansive with. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  they have recently been shown to be particularly unstable to adversarial perturbations of the data <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, very small and often imperceptible perturbations e classifier, it seems that the adversarial perturbations are generalizable across different models <ref type=\"bibr\" target=\"#b17\">[18]</ref>. This can actually become a real concern from a security p of the relevant work. The phenomenon of adversarial instability was first introduced and studied in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The authors estimated adversarial examples by solving pen explaining the presence of adversarial examples. Unfortunately, the optimization method employed in <ref type=\"bibr\" target=\"#b17\">[18]</ref> is time-consuming and therefore does not scale to large da he training procedure that allows to boost the robustness of the classifier. Notably, the method in <ref type=\"bibr\" target=\"#b17\">[18]</ref> was applied in order to generate adversarial perturbations he proposed DeepFool approach to stateof-the-art techniques to compute adversarial perturbations in <ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The method in ref type=\"bibr\" target=\"#b17\">[18]</ref> and <ref type=\"bibr\" target=\"#b3\">[4]</ref>. The method in <ref type=\"bibr\" target=\"#b17\">[18]</ref> solves a series of penalized optimization problems to find ver that the proposed approach also yields slightly smaller perturbation vectors than the method in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. The proposed approach is hence more accurate in detecting mplexity aspect, the proposed approach is substantially faster than the standard method proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, while the approach <ref type=\"bibr\" target=\"#b17  standard method proposed in <ref type=\"bibr\" target=\"#b17\">[18]</ref>. In fact, while the approach <ref type=\"bibr\" target=\"#b17\">[18]</ref> involves a costly minimization of a series of objective fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: G to another by a similar linear mapping function as transferring between multi-lingual word spaces <ref type=\"bibr\" target=\"#b4\">[Conneau et al., 2018]</ref>.</p><p>Combining Graph Structures and Sid. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: gment every physical register with a Superseded bit and a Pending count. This support is similar to <ref type=\"bibr\" target=\"#b16\">[17]</ref>. The Superseded bit marks whether the instruction that sup  techniques.</p><p>The second category includes work related to register recycling. Moudgill et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> discuss performing early register recycling in out-of-orde er processors that support precise exceptions. However, the implementation of precise exceptions in <ref type=\"bibr\" target=\"#b16\">[17]</ref> relies on either checkpoint/rollback for every replay even. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">Rosas et al., 2014)</ref>, which was then used to develop a multimodal deception detection system <ref type=\"bibr\" target=\"#b1\">(Abouelenien et al., 2014)</ref>. An extensive review of approaches fo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: chers' noisy votes; for this purpose, we use the state-of-the-art moments accountant technique from <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, which tightens the privacy bound when the t effect, limiting applicability to logistic regression with convex loss. Also, unlike the methods of <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, which represent the state-of-the-art in dif e a privacy/utility tradeoff that equals or improves upon bespoke learning methods such as those of <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>.</p><p>Section 5 further discusses the relat g the need for supervision. \u2022 We present a new application of the moments accountant technique from <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> for improving the differential-privacy analy (8.19, 10 \u22126 ) for SVHN, respectively with accuracy of 98.00% and 90.66%. In comparison, for MNIST, <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> obtain a looser (8, 10 \u22125 ) privacy bound an y cost, we use recent advances in privacy cost accounting. The moments accountant was introduced by <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>, building on previous work <ref type=\"bibr\"  rivacy loss random variable.</p><p>The following properties of the moments accountant are proved in <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref>.</p><p>Theorem 1. 1. [Composability] Suppose 00% and 90.66%. These results improve the differential privacy state-of-the-art for these datasets. <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> previously obtained 97% accuracy with a (8,  he large number of parameters prevents the technique from providing a meaningful privacy guarantee. <ref type=\"bibr\" target=\"#b0\">Abadi et al. (2016)</ref> provided stricter bounds on the privacy loss  we keep track of the privacy budget throughout the student's training using the moments accountant <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016)</ref>. When teachers reach a strong quorum, this  e cost of assuming that nonprivate unlabeled data is available, an assumption that is not shared by <ref type=\"bibr\" target=\"#b0\">(Abadi et al., 2016;</ref><ref type=\"bibr\" target=\"#b33\">Shokri &amp; . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: .org/ns/1.0\"><head n=\"2.2\">MAML</head><p>We give an overview of Model-Agnostic Meta-Learning method <ref type=\"bibr\" target=\"#b11\">[12]</ref> which is a representative algorithm of optimization-based   problem, we propose to encode the information from support set into our parameter inspired by MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> and further we can obtain a category-specific model to acc n testing query data.</p><p>\u2022Meta-Learning We select two state-of-the-art meta learning models MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> and Meta-SGD <ref type=\"bibr\" target=\"#b20\">[21]</ref> as  ent based learning procedure for new task quick adaptation. In the optimization-based methods, MAML <ref type=\"bibr\" target=\"#b11\">[12]</ref> is a recent promising model which learns a set of model pa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>). The majority of these methods do not scale to large graph \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. However, our approach is closely related to the graph conv. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ied forms of self-attention algorithms to point cloud data <ref type=\"bibr\" target=\"#b38\">[39,</ref><ref type=\"bibr\" target=\"#b36\">37,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. One such example i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. The recognition process has become a necessary part of man aved first, then the recognition tasks are conducted on text <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>. Given the plain text of an academic homepage, we aim to re ding, i.e., S \u2208 R n\u00d7d e . Following state-of-the-art methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref>, we use GloVe <ref type=\"bibr\" target=\"#b18\">[19]</ref> to  ods have been developed to address these problems. The state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> uses a Bi-LSTM-CRF based model to learn the page-level and l language processing methods. For example, state-of-the-art techniques for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> and for person names recognition <ref type=\"bibr\" target=\" ifferent methods to capture the position patterns. The state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> trains webpage-level and line-level models together to cap nd Preprocessing. We use the same datasets used by the state-of-the-art for publication recognition <ref type=\"bibr\" target=\"#b30\">[31]</ref> and person name recognition <ref type=\"bibr\" target=\"#b0\"> ref type=\"table\" target=\"#tab_0\">2</ref> summarises the dataset statistics.</p><p>\u2022 HomePub dataset <ref type=\"bibr\" target=\"#b30\">[31]</ref> contains the plain text of 2,087 homepages from different  formation about the position patterns and person names. PAM also outperforms the hierarchical PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> model, which can capture the positional diversity, by 3.64 a manual inspection of recognition results of state-ofthe-art models for the two tasks (i.e., PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> and CogNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>) and our ublications and 5,542 person names.</p><p>For publication string recognition, we observe that PubSE <ref type=\"bibr\" target=\"#b30\">[31]</ref> misrecognises strings about patents, grants, and research . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t information for making matching decisions. The last technique, data augmentation, is adapted from <ref type=\"bibr\" target=\"#b30\">[31]</ref> for EM to help D learn \"harder\" to understand the data inv  address this issue, D applies MixDA, a recently proposed data augmentation technique for NLP tasks <ref type=\"bibr\" target=\"#b30\">[31]</ref> illustrated in Figure <ref type=\"figure\" target=\"#fig_3\">3 h the entry_swap operator. We compare the different combinations and report the best one. Following <ref type=\"bibr\" target=\"#b30\">[31]</ref>, we apply MixDA with the interpolation parameter \u03bb sampled arget=\"#b58\">59</ref>]. We designed a set of DA operators suitable for EM and apply them with MixDA <ref type=\"bibr\" target=\"#b30\">[31]</ref>, a recently proposed DA strategy based on convex interpola nd span_shuffle. These two operators are used in NLP tasks <ref type=\"bibr\" target=\"#b56\">[57,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> and shown to be effective for text classification. For span DA) has been extensively studied in computer vision and has recently received more attention in NLP <ref type=\"bibr\" target=\"#b30\">[31,</ref><ref type=\"bibr\" target=\"#b56\">57,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: g restrictions such as minimum time quantum limits or increased the priority of the current thread; <ref type=\"bibr\" target=\"#b1\">(2)</ref> In cases where frequent context switches are inevitable, e.g e literature. An early example of a prefetching architecture is Nextline Prefetching (NLP) by Smith <ref type=\"bibr\" target=\"#b1\">[2]</ref>, where each cache block was tagged with a bit indicating a p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ble to rate trust better in the form of discrete verbal statements, rather than continuous measures <ref type=\"bibr\" target=\"#b28\">[29]</ref> . Verbal statements must introduce subjective fuzziness. F. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b47\">48]</ref>.</p><p>Random walk  nhanced guarantees <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46]</ref>. For example Wei et al. <ref type=\"bibr\" target=\"#b47\">[48]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  presented an intelligent video-based system for automated detection of suicide by hanging attempts <ref type=\"bibr\" target=\"#b1\">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng over graph structures using convolution operators that offer promise as an embedding methodology <ref type=\"bibr\" target=\"#b16\">[17]</ref>. So far, graph convolutional networks (GCNs) have only bee nificant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Lastly, we probe the expressive capability of our approac \"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN algorithm <ref type=\"bibr\" target=\"#b16\">[17]</ref> is designed for semi-supervised learning in a transductive r is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework <ref type=\"bibr\" target=\"#b16\">[17]</ref>. In particular, we can derive an inductive variant of the  regator convolutional since it is a rough, linear approximation of a localized spectral convolution <ref type=\"bibr\" target=\"#b16\">[17]</ref>. An important distinction between this convolutional aggre utional\" variant of GraphSAGE is an extended, inductive version of Kipf et al's semi-supervised GCN <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we term this variant GraphSAGE-GCN. We test unsupervised  ctive setting, where it can be extensively trained on a single, fixed graph. (That said, Kipf et al <ref type=\"bibr\" target=\"#b16\">[17]</ref> <ref type=\"bibr\" target=\"#b17\">[18]</ref> found that GCN-b d=\"foot_2\">Note that this differs from Kipf et al's exact equation by a minor normalization constant<ref type=\"bibr\" target=\"#b16\">[17]</ref>.</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place  convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. In this work we b our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. The original GCN  less\" GCN approach has parameter dimension O(|V|), so this requirement is not entirely unreasonable <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>.</p><p>Following T \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>). The majority of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \"#b0\">1]</ref> we favor this formulation over other variants <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>. The dice loss is implemented as follows:</p><formula xml:i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s of these two datasets.</p><p>The PPI dataset was collected from the molecular signatures database <ref type=\"bibr\" target=\"#b25\">(Subramanian et al., 2005)</ref>. Each node represents a protein and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b40\">41]</ref> and modelling short-text similarities <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" tar e=\"bibr\" target=\"#b30\">31]</ref> or representation-focused <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b34\">[35]</ref><ref type=\"bibr\" ta  the distributed model improves drastically in the presence of more data. Unlike some previous work <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta nt. Our n-graph based input encoding is motivated by the trigraph encoding proposed by Huang et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref>, but unlike their approach we don't limit our input repres  on the retrieval task, as a baseline in this paper. Both the deep structured semantic model (DSSM) <ref type=\"bibr\" target=\"#b15\">[16]</ref> and its convolutional variant CDSSM <ref type=\"bibr\" targe lated papers that use short text such as title, for document ranking or related tasks. Huang et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> learn a distributed representation of query and title, for. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tein-protein interaction network we might be interested in predicting functional labels of proteins <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>. Similarly, in lin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ks often focused on smaller record generation datasets such as WeatherGov and RoboCup, but recently <ref type=\"bibr\" target=\"#b20\">Mei et al. (2016)</ref> showed how neural models can achieve strong r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f i (\u2022) with g(\u2022) on both the forward and backward pass is either completely ineffective (e.g. with <ref type=\"bibr\" target=\"#b24\">Song et al. (2018)</ref>) or many times less effective (e.g. with <re iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.4.1.\">PIXELDEFEND</head><p>Defense Details. <ref type=\"bibr\" target=\"#b24\">Song et al. (2018)</ref> propose using a PixelCNN generative model to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b21\">(Salimans &amp; Kingma, 2016)</ref> with momentum 0.999 to all of them. We used leaky ReLU <ref type=\"bibr\" target=\"#b13\">(Maas et al., 2013)</ref> with \u03b1 = 0.1 as the non-linearity, and chos. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t approach. Batch-mode (i.e each step can ask for several labels) have been studied for instance by <ref type=\"bibr\" target=\"#b6\">[7]</ref>, using a definition of the performance based on high likelih. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 018)</ref>, datato-document generation <ref type=\"bibr\" target=\"#b21\">(Moryossef et al., 2019;</ref><ref type=\"bibr\" target=\"#b15\">Koncel-Kedziorski et al., 2019)</ref> and interpretability of KGs in  mer.</p><p>Following previous work, we evaluate Graformer on two benchmarks: (i) the AGENDA dataset <ref type=\"bibr\" target=\"#b15\">(Koncel-Kedziorski et al., 2019)</ref>, i.e., the generation of scien ead><p>We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA <ref type=\"bibr\" target=\"#b15\">(Koncel-Kedziorski et al., 2019)</ref> and WebNLG <ref type=\"bibr\" ta ecoder architectures <ref type=\"bibr\" target=\"#b20\">(Marcheggiani and Perez-Beltrachini, 2018;</ref><ref type=\"bibr\" target=\"#b15\">Koncel-Kedziorski et al., 2019;</ref><ref type=\"bibr\" target=\"#b27\">R rms previous Transformerbased models that only consider first-order neighborhoods per encoder layer <ref type=\"bibr\" target=\"#b15\">(Koncel-Kedziorski et al., 2019;</ref><ref type=\"bibr\" target=\"#b2\">A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: echniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks <ref type=\"bibr\" target=\"#b24\">(Scarselli et al., 2009)</ref>, which we modify to use gated recurren ucing a sequence of outputs. Here, (1) is mostly achieved by previous work on Graph Neural Networks <ref type=\"bibr\" target=\"#b24\">(Scarselli et al., 2009)</ref>; we make several minor adaptations of   on graphs, including Graph Neural Networks <ref type=\"bibr\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009)</ref>, spectral networks <ref type=\"bibr\" tar ion, we review Graph Neural Networks (GNNs) <ref type=\"bibr\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009)</ref> and introduce notation and concepts tha irected edge v \u2192 v , but we note that the framework can easily be adapted to undirected graphs; see <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref>. The node vector (or node representatio v = f * (l v , l CO(v) , l NBR(v) , h (t\u22121) NBR(v)</formula><p>). Several variants are discussed in <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> including positional graph forms, node- l graph forms, node-specific updates, and alternative representations of neighborhoods. Concretely, <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> suggest decomposing f * (\u2022) to be a sum unction g(h v , l v ) that maps to an output. This is generally a linear or neural network mapping. <ref type=\"bibr\" target=\"#b24\">Scarselli et al. (2009)</ref> focus on outputs that are independent p r\" target=\"#b10\">(Gori et al., 2005;</ref><ref type=\"bibr\" target=\"#b6\">Di Massa et al., 2006;</ref><ref type=\"bibr\" target=\"#b24\">Scarselli et al., 2009;</ref><ref type=\"bibr\">Uwents et al., 2011)</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s on Graph Convolutional Networks (GCNs) <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b8\">Gao et al., 201  large graphs, layer sampling techniques <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b26\">Ying et al., 2 ly a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b14\">Huang et  he above edge sampler to perform layer sampling. Under the independent layer sampling assumption of <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref>, one would sample a connection u ( ) , v ( + b3\">Chen et al. (2018a)</ref>. Point (2) is due to the better interlayer connectivity compared with <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref>, and unbiased minibatch estimator compared w s to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b)</ref> performs sampling from another perspective. , 2016)</ref>, 2. GraphSAGE <ref type=\"bibr\" target=\"#b9\">(Hamilton et al., 2017)</ref>, 3. FastGCN <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b)</ref>, 4. S-GCN <ref type=\"bibr\" target=\"#b3\">(Ch of each layer independently. This is similar to the treatment of layers independently by prior work <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., ip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e attack) such that perturbations maximally destroy downstream GNN's predictions. Bojcheshki et al. <ref type=\"bibr\" target=\"#b29\">[30]</ref> derive adversarial perturbations that poison the graph str. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b75\">76]</ref> and the heat kernel <ref type=\"bibr\" target=\"#b33\">[34]</ref>, with which GDC achieves a linear runtime O(N ). Furthermo e=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, especially for clustering <ref type=\"bibr\" target=\"#b33\">[34]</ref>, semi-supervised classification <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  increasingly important role for mining users' reading interest and providing personalized contents <ref type=\"bibr\" target=\"#b11\">(IJntema et al., 2010;</ref><ref type=\"bibr\" target=\"#b17\">Liu et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as input.</p><p>Networks <ref type=\"bibr\" target=\"#b32\">[33]</ref> and Residual Networks (ResNets) <ref type=\"bibr\" target=\"#b10\">[11]</ref> have surpassed the 100-layer barrier.</p><p>As CNNs become d (or beginning) of the network. Many recent publications address this or related problems. ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> and Highway Networks <ref type=\"bibr\" target=\"#b32\">[33]</ uent layer. It changes the state but also passes on information that needs to be preserved. ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> make this information preservation explicit through additi tor that eases the training of these very deep networks. This point is further supported by ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref>, in which pure identity mappings are used as bypassing pat ng image recognition, localization, and detection tasks, such as ImageNet and COCO object detection <ref type=\"bibr\" target=\"#b10\">[11]</ref>. Recently, stochastic depth was proposed as a way to succe =\"#b15\">[16]</ref>, which gives rise to the following layer transition: x \u2113 = H \u2113 (x \u2113\u22121 ). ResNets <ref type=\"bibr\" target=\"#b10\">[11]</ref> add a skip-connection that bypasses the non-linear transfo ates that they do not suffer from overfitting or the optimization difficulties of residual networks <ref type=\"bibr\" target=\"#b10\">[11]</ref>.</p><p>Parameter Efficiency. The results in Table <ref typ s, it typically has many more inputs. It has been noted in <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> that a 1\u00d71 convolution can be introduced as bottleneck laye a standard data augmentation scheme (mirroring/shifting) that is widely used for these two datasets <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" ta rget=\"#b11\">12]</ref>, and apply a single-crop or 10-crop with size 224\u00d7224 at test time. Following <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" ta  the same data augmentation scheme for training images as in <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>, and apply a single. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ar how to properly train deep GCN architectures, where several works have studied their limitations <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" ta ref type=\"bibr\" target=\"#b52\">53]</ref> is an open problem in the graph learning space. Recent work <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b42\">43,</ref><ref type=\"bibr\" ta causes oversmoothing, eventually leading to features of graph vertices converging to the same value <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Due to these limitations, most state-of-the-art GCNs are  is limited to a small number of layers <ref type=\"bibr\" target=\"#b5\">(6)</ref>. Recently, Li et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> studied the depth limitations of GCNs and showed that deep. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t depend on sensors but simply try to discover the user's stress through self-reporting tools e.g., <ref type=\"bibr\" target=\"#b9\">(Rahman et al., 2014)</ref> and surveys like the Perceived Stress Scal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nal supervised learning scheme like factorization machines <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> that model the relations implicitly. For example, a recent . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent obfuscation based defenses have proven vulnerable. In their recent seminal work, Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> presented a suite of strategies for estimating network gradi  range of possible attacks, including those having successfully circumvented many previous defenses <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Under these attacks, we compare the worst-case robustness o b32\">33]</ref>.</p><p>Unfortunately, many of these methods have proven vulnerable by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>, who introduced a set of attacking strategies, including a m .</p><p>Thus far, gradient obfuscation is generally considered vulnerable (and at least incomplete) <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We revisit gradient obfuscation, and our defense demonstrat ased defense mechanisms seem plausible. Yet they are all fragile. As demonstrated by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>, with random input transformation, adversarial examples can  -removal transformation is also ineffective. One can use Backward Pass Differentiable Approximation <ref type=\"bibr\" target=\"#b1\">[2]</ref> to easily construct effective adversarial examples. In short ation (BPDA).</head><p>To circumvent the defense using non-differentiable operators, Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> introduced a strategy called Backward Pass Differentiable Ap  attacking strategy. It causes the adversary to suffer from either exploding or vanishing gradients <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Figure <ref type=\"figure\" target=\"#fig_4\">3</ref>-left show #b32\">33]</ref>. Yet those defenses have been proven vulnerable under a reparameterization strategy <ref type=\"bibr\" target=\"#b1\">[2]</ref>. This strategy aims to find some differentiable function h(\u2022 those defenses, the transformed image g(x) remain similar to the input x. Consequently, as shown in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, those defenses can be easily circumvented by replacing g(\u2022) mlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">BPDA Attack and the Variants</head><p>BPDA attack <ref type=\"bibr\" target=\"#b1\">[2]</ref>, as reviewed in Sec. 3.3, is a powerful way to estimate netw ck-box attacks, including the black-box transfer attack <ref type=\"bibr\" target=\"#b29\">[30]</ref>   <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We evaluate other methods using the code provided in the or er all tested attacks. The methods indicated by a star (*) are those circumvented by Athalye et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref>. We include their results therein as a reference. The other  nsformations to the input. But they have been circumvented by Expectation Over Transformation (EOT) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. EOT attack first esti rsarial examples of f b directly (so that g(h(\u2022)) = h(\u2022)), without solving the optimization problem <ref type=\"bibr\" target=\"#b1\">(2)</ref>. We argue that finding such an h(\u2022) is extremely hard. If h(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: usted so that it can fit different models and datasets.</p><p>Inspired by the success of Focal Loss <ref type=\"bibr\" target=\"#b29\">[30]</ref>, we estimate \ud835\udf14 (\ud835\udc62, \ud835\udc56) with a function of \ud835\udc53 ( \u0177\ud835\udc62\ud835\udc56 ) that ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s/1.0\"><head n=\"2.\">PROPOSED APPROACH</head><p>We use a baseline Tacotron architecture specified in <ref type=\"bibr\" target=\"#b7\">[8]</ref>, where we use a GMM attention <ref type=\"bibr\" target=\"#b8\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ch contain knowledge representative of the path as a whole. PLDA+, a scalable implementation of LDA <ref type=\"bibr\" target=\"#b24\">[28]</ref>, allows us to quickly nd topic models in these clouds. Unl truction process, all share common topics. We perform topic modeling on these documents using PLDA+ <ref type=\"bibr\" target=\"#b24\">[28]</ref>. e result is a set of plain text topics which represent di s and PLDA+ is a scalable implementation of this algorithm <ref type=\"bibr\" target=\"#b16\">[20,</ref><ref type=\"bibr\" target=\"#b24\">28]</ref>. Developed by Zhiyuan Liu et al., PLDA+ quickly identi es g. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ith a cross entropy loss. We trained for 60000 steps with batch size 10. We used the Adam optimizer <ref type=\"bibr\" target=\"#b10\">[11]</ref> with a start learning of 1e-2 and a reduction of the learn raining Details We trained each model for 100,000 steps with batch size 128 using an Adam optimizer <ref type=\"bibr\" target=\"#b10\">[11]</ref>. We used a fixed learning rate throughout training and con d dividing by the standard deviation of the training set.</p><p>We trained for 50 epochs using Adam <ref type=\"bibr\" target=\"#b10\">[11]</ref> at initial learning rate 1e-3 and a single-cycle cosine ra. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mor drug development <ref type=\"bibr\" target=\"#b7\">[10,</ref><ref type=\"bibr\" target=\"#b8\">11,</ref><ref type=\"bibr\" target=\"#b33\">37]</ref> and represents a case for repurposing target anti-viral dru. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing instructions out of order. We believe that a solution similar to those proposed by Stark et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> and Cher et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref> ca .</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Related Work</head><p>Stark et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> proposed a limited form of out-of-order instruction fetch . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016)</ref>. However, most existing methods require la \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016)</ref> to define the score of the predicted seque \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b3\">Finkel et al. \"bibr\" target=\"#b13\">(Liu et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy, 2016;</ref><ref type=\"bibr\" target=\"#b10\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Ratinov and  cannot deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTM-CRF <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> into a Fuzzy CRF layer, which allows each kens, such as \"Thus\" and \"by\", are labeled as O.</p><p>Fuzzy-LSTM-CRF. We revise the LSTM-CRF model <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> to the Fuzzy-LSTM-CRF model to support th -Disease datasets, LM-LSTM-CRF <ref type=\"bibr\" target=\"#b13\">(Liu et al., 2018)</ref> and LSTM-CRF <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016)</ref> achieve the state-of-the-art F 1 scores w cent advances in neural models have freed do-main experts from handcrafting features for NER tasks. <ref type=\"bibr\" target=\"#b10\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b14\">Ma and Hovy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: uage understanding tasks for the existing VLP models is VQA. The SoTA result for VQA is from UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large model. Table <ref type=\"table\" target=\"#tab_3\">6</ref> Oscar B is the best among the models with equivalent size, even slightly better (0.04%) than UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large. And the Oscar L improves the SoTA overall accuracy wi other major task for the existing VLP models is NLVR2. Similarly, the SoTA model on NLVR2 is UNITER <ref type=\"bibr\" target=\"#b6\">[6]</ref> large. As reported in Table <ref type=\"table\" target=\"#tab_4 =\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" target=\"#b10\">10]</ref> employ BERT-like objectives <ref type=\"bibr\" target=\"#b6\">[6]</ref> to learn crossmodal representations from a concatenated-sequ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s guaranteed to converge to a local optimum of it.</p><p>Inspired by the previous work on CycleGANs <ref type=\"bibr\" target=\"#b29\">(Zhu et al., 2017)</ref> and dual learning <ref type=\"bibr\" target=\"#. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: luding Memory-Augmented Network <ref type=\"bibr\" target=\"#b6\">[7]</ref> and LSTM-based meta-learner <ref type=\"bibr\" target=\"#b29\">[30]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ase of it. The \u03a0-model can also be seen as a simplification of the \u0393-model of the ladder network by <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref>, a previously presented network architectu he data is obtained.</p><p>Our approach is somewhat similar to the \u0393-model of the ladder network by <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref>, but conceptually simpler. In the \u03a0-model, ed the issue by shuffling the input sequences in such a way that stratification is guaranteed, e.g. <ref type=\"bibr\" target=\"#b17\">Rasmus et al. (2015)</ref> (confirmed from the authors). This kind of he ones that are most directly connected to our work.</p><p>\u0393-model is a subset of a ladder network <ref type=\"bibr\" target=\"#b17\">(Rasmus et al., 2015)</ref> that introduces lateral connections into . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: applied to recommendation problems. Auto-encoder framework <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and its variant denoising autoencoder <ref type=\"bibr\" targ ion Metrics. As in <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>, we evaluate a mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng data (i.e., the unobserved interactions) <ref type=\"bibr\" target=\"#b8\">[Jiang et al., 2018;</ref><ref type=\"bibr\" target=\"#b7\">He et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Lin et al., 2019]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nitial design space and the output is a refined design space of simpler or better models. Following <ref type=\"bibr\" target=\"#b20\">[21]</ref>, we characterize the quality of a design space by sampling gn, elevated to the population level and guided via distribution estimates of network design spaces <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>As a testbed for this paradigm, our focus is on exp essential to use a reliable comparison metric to guide our design process. Recently, the authors of <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposed a methodology for comparing and analyzing populat c scenario).</p><p>We rely on the concept of network design spaces introduced by Radosavovic et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref>. A design space is a large, possibly infinite, population  esign space is a large, possibly infinite, population of model architectures. The core insight from <ref type=\"bibr\" target=\"#b20\">[21]</ref> is that we can sample models from a design space, giving r ce design. To evaluate and compare design spaces, we use the tools introduced by Radosavovic et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref>, who propose to quantify the quality of a design space by  a single ResNet-50 <ref type=\"bibr\" target=\"#b7\">[8]</ref> model at 4GF for 100 epochs.</p><p>As in <ref type=\"bibr\" target=\"#b20\">[21]</ref>, our primary tool for analyzing design space quality is th i-c.org/ns/1.0\"><head>Appendix C: Optimization Settings</head><p>Our basic training settings follow <ref type=\"bibr\" target=\"#b20\">[21]</ref> as discussed in \u00a73. To tune the learning rate lr and weigh initial design space and the output is a refined design space of simpler or better models. Following<ref type=\"bibr\" target=\"#b20\">[21]</ref>, we characterize the quality of a design space by sampling tp://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">We use the term design space following<ref type=\"bibr\" target=\"#b20\">[21]</ref>, rather than search space, to emphasize that we are not se ://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"5\" xml:id=\"foot_4\">Our training setup in \u00a73 exactly follows<ref type=\"bibr\" target=\"#b20\">[21]</ref>. We use SGD with momentum of 0.9, mini-batch size of 128 o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  large-scale ST dataset, multitask learning <ref type=\"bibr\" target=\"#b32\">(Weiss et al. 2017;</ref><ref type=\"bibr\" target=\"#b6\">B\u00e9rard et al. 2018</ref>) and pretraining techniques <ref type=\"bibr\"  ficantly increases the learning difficulty.</p><p>\u2022 Non-pre-trained Attention Module: Previous work <ref type=\"bibr\" target=\"#b6\">(B\u00e9rard et al. 2018)</ref> trains attention modules for ASR, MT and ST. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rations are used to model the sequential document selection process in search result diversi cation <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and multi-page sea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arn both long-term interests and short-term interests of such implicit feedbacks. As Jannach et al. <ref type=\"bibr\" target=\"#b6\">[7]</ref> noted that both the users' short-term and long-term interest. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: h as the element-wise mean E p [z] = [\u03c0 1 , ..., \u03c0 k ] of these vectors.</p><p>The Gumbel-Max trick <ref type=\"bibr\" target=\"#b8\">(Gumbel, 1954;</ref><ref type=\"bibr\" target=\"#b12\">Maddison et al., 20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s <ref type=\"bibr\" target=\"#b14\">[15]</ref>, dynamic predication based on frequently executed paths <ref type=\"bibr\" target=\"#b13\">[14]</ref>, and predicate prediction <ref type=\"bibr\" target=\"#b25\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><p>Inspired by the success of augmentation methods in ASR <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, as a remedy to avoid overfitting while using lowresource t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: category loss term. To enable distribution q \u03d5 c (z|x q c ) differentiable, we follow previous work <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: stic depth for the Transformer inspired by the Stochastic Residual Network for image classification <ref type=\"bibr\" target=\"#b9\">[10]</ref>.</p><p>We discovered that its ability to regularize is the  r\" target=\"#b15\">[16]</ref>, and thus there are redundant layers. Motivated by the previous work of <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we propose to apply stochastic residual layers into our Tr  sub-layers inside). This way we have one hyper-parameter p for each layer.</p><p>\u2022 As suggested by <ref type=\"bibr\" target=\"#b9\">[10]</ref>, the lower layers of the networks handle raw-level acoustic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ictable response times, which is becoming increasingly important for modern datacenter applications <ref type=\"bibr\" target=\"#b29\">[33]</ref>.    </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  When the input distribution to a learning system changes, it is said to experience covariate shift <ref type=\"bibr\" target=\"#b17\">(Shimodaira, 2000)</ref>. This is typically handled via domain adapta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: vskaya et al., 2014)</ref> uses a classifier-based approach, which is improved by the latter system <ref type=\"bibr\" target=\"#b44\">(Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based \"bibr\" target=\"#b50\">(Susanto et al., 2014;</ref><ref type=\"bibr\">Chollampatt et al., 2016b,a;</ref><ref type=\"bibr\" target=\"#b44\">Rozovskaya and Roth, 2016;</ref><ref type=\"bibr\" target=\"#b26\">Junczy. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m . On March 7 , 2012 , he was named one of five finalists for the Naismith Award , which is 0.064  <ref type=\"bibr\" target=\"#b0\">(Baevski and Auli, 2019;</ref><ref type=\"bibr\" target=\"#b19\">Radford e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ww.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>The task of Grounded video description (GVD) <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> aims to generate more grounded and accurate  existing works either encode region proposals independently or using selfattention-based mechanisms <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>. Therefore, it either fails to consider impl a]</ref>, many works model the video in both global video features and regional object features. In <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>, they encode the objects with transformer <r /head><p>We model the video's global level feature by a Bi-directional LSTM network like most works <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ... ose a novel visual representation method from the perspective of regions. First of all, inspired by <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>, we enhance the proposal features by adding  <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Feature Enhancement</head><p>In this part, we follow <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>'s work, which fusing the spatial-temporal an  feature aggregation on the enhanced feature R.</p><p>We adopt the same classification loss just as <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> do denoted as L cls .</p></div> <div xmlns=\" > t , h f rame + h attention ). h t is used to generate descriptions. We adopt the same MLE loss as <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> which denoted by L sent .</p><p>Finally, the ad n=\"4.1\">Dataset</head><p>We conduct our experiments on the Grounded ActivityNet-Entities Dataset <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> for evaluation. It contains 15k video with 1 ph2Seq method. Data processing. For a fair comparison, the data processing procedure is the same to <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we u  2018]</ref>, BiM-STM+TempoAtnn <ref type=\"bibr\" target=\"#b7\">[Zhou et al., 2018]</ref> and ZhouGVD <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref> on Grounded ActivityNet Captions Dataset to  emove the hierarchical attention and replace it with the coarsegrain proposal attention proposed by <ref type=\"bibr\" target=\"#b8\">[Zhou et al., 2019]</ref>.</p><p>Table <ref type=\"table\" target=\"#tab_. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 0\"><head n=\"1\">Introduction</head><p>Bidirectional Encoder Representations from Transformers (BERT) <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref> has become enormously popular and proven t RT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by <ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>.</p><p>\u2022 As there are so many possibilitie  settings and data statistics in different task. \u2020 represents the dataset was also evaluated by BERT<ref type=\"bibr\" target=\"#b8\">(Devlin et al., 2019)</ref>. \u2021 represents the dataset was also evaluat , which is beneficial for the researcher to design more powerful models based on them.</p><p>Before <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref> releasing BERT with whole word masking, <re <ref type=\"foot\" target=\"#foot_1\">3</ref> , and pre-processed with WikiExtractor.py as suggested by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>  <ref type=\"bibr\" target=\"#b8\">Devlin et al sed with WikiExtractor.py as suggested by <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>  <ref type=\"bibr\" target=\"#b8\">Devlin et al. (2019)</ref>, for computation efficiency and learning lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b13\">(Kipf and Welling, 2017;</ref><ref type=\"bibr\" target=\"#b27\">Velickovic et al., 2018;</ref><ref type=\"bibr\" target=\"#b19\">Palm et al., 2018)</ref>, we propose an evidence reasoning network (E. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lay towards salient events, a phenomenon that characterizes empirically observed hippocampal replay <ref type=\"bibr\" target=\"#b23\">29</ref> , and relates to the notion of 'prioritized sweeping' <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: type=\"bibr\" target=\"#b19\">(Kim, 2014;</ref><ref type=\"bibr\" target=\"#b9\">Gehring et al., 2017;</ref><ref type=\"bibr\" target=\"#b36\">Vaswani et al., 2017b;</ref><ref type=\"bibr\" target=\"#b32\">Shen et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the target user and item based on historical interactions <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. For example, given two paths p 1 u 1 \u2192 i 1 \u2192 u 2 \u2192 i 2 and terests; meanwhile, the user groups can also profile items <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Hence, in each modality (e.g., visual), we aggregate signa e select LeakyReLU(\u2022) as the nonlinear activation function <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Such aggregation method assumes that different neighbors w ntegrate multi-modal features as the node features to learn the representation of each node. \u2022 NGCF <ref type=\"bibr\" target=\"#b41\">[41]</ref>. This method represent a novel recommendation framework to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he learned node embeddings, we visualize the node representations in 2D space using t-SNE algorithm <ref type=\"bibr\" target=\"#b28\">[29]</ref>. The figures are shown in Figure <ref type=\"figure\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ref>  Register traffic characteristics. We collect a number of characteristics concerning registers <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Our first characteristic is the average number of input ope. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ve used explicit Laplacian regularisation in the objective <ref type=\"bibr\" target=\"#b50\">[51,</ref><ref type=\"bibr\" target=\"#b52\">53,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: le optimization methods usually can be divided into two groups: supervised and unsupervised methods <ref type=\"bibr\" target=\"#b15\">(Grybas et al., 2017)</ref>. Supervised scale optimization methods ar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: on, called receiver class prediction optimization (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib ll with direct method calls in object-oriented languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib morphic inline caches <ref type=\"bibr\" target=\"#b23\">[23]</ref>, and type feedback/devirtualization <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. As we show in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  regions at multiple scales of the original image.</p><p>Lastly, inspired by the recent Transformer <ref type=\"bibr\" target=\"#b22\">[23]</ref> model in dealing with a number of difficult NLP tasks such idal way, following by corresponding pooling strategy. Moreover, inspired by the recent Transformer <ref type=\"bibr\" target=\"#b22\">[23]</ref> model in dealing with a number of difficult NLP tasks such iation between these local regions are still not explored. Inspired by the recent Transformer model <ref type=\"bibr\" target=\"#b22\">[23]</ref>, we implant a self attention module adapted to image featu g sentiment class number of datasets, and then tunes the parameters of all layers. \u2022 Self-Attention <ref type=\"bibr\" target=\"#b22\">[23]</ref> is a variant derived from the Transformer model. Transform. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef>, attacks of this form have proven difficult to prevent <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ect caches and a homogeneous workload (all threads running the same trace). Yamamoto and Nemirovsky <ref type=\"bibr\" target=\"#b27\">[28]</ref> simulate an SMT architecture with separate instruction que. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n or essential characteristics. However, there is no such wide-accepted formal definition. Paulheim <ref type=\"bibr\" target=\"#b5\">[6]</ref> defined four criteria for knowledge graphs. Ehrlinger and W  statistical relational learning <ref type=\"bibr\" target=\"#b8\">[9]</ref>, knowledge graph refinement <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Chinese knowledge graph construction <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d new items. State-of-art reinforcement learning methods usually apply the simple \u03f5-greedy strategy <ref type=\"bibr\" target=\"#b29\">[31]</ref> or Upper Confidence Bound (UCB) <ref type=\"bibr\" target=\"# dynamic nature of news characteristics and user preference, we propose to use Deep Q-Learning (DQN) <ref type=\"bibr\" target=\"#b29\">[31]</ref> framework. This framework can consider current reward and  avoid the harm to recommendation accuracy induced by classical exploration strategies like \u03f5-greedy <ref type=\"bibr\" target=\"#b29\">[31]</ref> and Upper Confidence Bound <ref type=\"bibr\" target=\"#b21\"> ss stored in the memory to update the network Q.</p><p>Here, we use the experience replay technique <ref type=\"bibr\" target=\"#b29\">[31]</ref> to update the network. Specifically, agent G maintains a m ture of news recommendation and the need to estimate future reward, we apply a Deep Q-Network (DQN) <ref type=\"bibr\" target=\"#b29\">[31]</ref> to model the probability that one user may click on one sp ead><p>The most straightforward strategies to do exploration in reinforcement learning are \u03f5-greedy <ref type=\"bibr\" target=\"#b29\">[31]</ref> and UCB <ref type=\"bibr\" target=\"#b21\">[23]</ref>. \u03f5-greed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s applications such as speech, object recognition, natural language processing, recommender systems <ref type=\"bibr\" target=\"#b8\">(Hsu et al., 2017;</ref><ref type=\"bibr\" target=\"#b16\">Ma et al., 2019. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 11\">[11]</ref> and <ref type=\"bibr\" target=\"#b12\">[12,</ref><ref type=\"bibr\" target=\"#b13\">13,</ref><ref type=\"bibr\" target=\"#b14\">14]</ref> and absorbs their advantages. CFCBS embed measurement instr ranode CFEs occur if the program control before and after the illegal jump resides in the same node <ref type=\"bibr\" target=\"#b14\">[14]</ref>.</p><p>Considering a Program Flow Graph { P ,V}, we define not constant and is determined by the length of the first two fields and consists of a series of 0s <ref type=\"bibr\" target=\"#b14\">[14]</ref>.(see Fig. <ref type=\"figure\" target=\"#fig_1\">1</ref>).</p>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lar to a teacher, and the role of the small model is similar to a student. Following this, DarkRank <ref type=\"bibr\" target=\"#b2\">[Chen et al., 2018]</ref> proposed a method combining deep metric lear Li et al., 2018b]</ref> points that setting a small temperature will harm the optimization process. <ref type=\"bibr\" target=\"#b2\">[Courbariaux et al., 2015]</ref> mentions that generating binary codes. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing to produce only a single texture and have so far not been applied to SISR. Adversarial networks <ref type=\"bibr\" target=\"#b17\">[18]</ref> have recently been shown to produce sharp results in a num ns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.4\">Adversarial training</head><p>Adversarial training <ref type=\"bibr\" target=\"#b17\">[18]</ref> is a recent technique that has proven to be a useful mecha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oustic model with a \"bottleneck\" layer using a frame based criterion on a large multilingual corpus <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target ilingual models can be adapted to the specific language to improve performance further. The work by <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> presented bottleneck ific softmax layers, which are trained using cross-entropy <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. This architecture ca ) if X \u2208 XL1 softmax(WL2e + bL2) if X \u2208 XL2 . . . softmax(WLne + bLn) if X \u2208 XLn</formula><p>Unlike <ref type=\"bibr\" target=\"#b4\">[5]</ref>, we do not have any bottleneck layer, and the whole model is. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: p><p>We compute a probability p of copying from the input using h t and c t in a fashion similar to <ref type=\"bibr\" target=\"#b26\">See et al. (2017)</ref>, that is:</p><formula xml:id=\"formula_7\">p = . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ef>. Similarly, techniques for hand pose estimation (e.g., <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>) leverages dense coordinate regression, which is then used . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: xt to speech, TTS) <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and speech recognit et=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> and ASR <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"b , we re-sample it to 16kHZ and convert the raw waveform into mel-spectrograms following Shen et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref> with 50ms frame size, 12.5ms hop size. For the text, we us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  early active learning, there are clustering-based methods <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and transductive experimental design methods <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b6\">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation methods of <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b4\">Jozefowicz et  pled per training example):</p><p>\u2022 For any K 1, a binary classification variant of NCE, as used by <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b8\">Mikolov et al. n square error as the MLE) as K ! 1.</p><p>\u2022 We discuss application of our results to approaches of <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b8\">Mikolov et al. history x. This is the most straightforward extension of NCE to the conditional case; it is used by <ref type=\"bibr\" target=\"#b9\">(Mnih and Teh, 2012)</ref>. It has the clear drawback however of intro o motivate the importance of the two algorithms, we now discuss their application in previous work. <ref type=\"bibr\" target=\"#b9\">Mnih and Teh (2012)</ref> consider language modeling, where x = w 1 w  n of the parameters c</p><p>x corresponding to normalization terms for each history. Interestingly, <ref type=\"bibr\" target=\"#b9\">Mnih and Teh (2012)</ref> acknowledge the difficulties in maintaining . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >12]</ref>.</p><p>Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, <ref type=\"bibr\" target=\"#b48\">[49]</ref>), a selfsupervised representation learning technique for r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: urrent work further shows the success of search-based unsupervised text generation for paraphrasing <ref type=\"bibr\" target=\"#b15\">(Liu et al., 2020)</ref> and summa-rization <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  balance between effectiveness and efficiency. In this paper, we employ knowledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> which is a network compression technique by transferring th ulate the top-N recommendation problem. Then, we explain the concept of knowledge distillation (KD) <ref type=\"bibr\" target=\"#b9\">[10]</ref> and present rank distillation (RD) <ref type=\"bibr\" target= r proposed training strategies.</p><p>Temperature in the KD loss. One key factor of the original KD <ref type=\"bibr\" target=\"#b9\">[10]</ref> is to find a proper balance between the soft targets and ha ]</ref> is to find a proper balance between the soft targets and hard labels. To tackle this issue, <ref type=\"bibr\" target=\"#b9\">[10]</ref> introduces the notion of a temperature T . Although the sof. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2014</ref><ref type=\"bibr\" target=\"#b20\">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015;</ref><ref type=\"bibr\" target=\"#b23\">Zhou et al., ef> is a revision of CNN which uses piecewise max-pooling to extract more relation features. BiLSTM <ref type=\"bibr\" target=\"#b22\">(Zhang et al., 2015)</ref> is also commonly used for RE with the help. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ly, a lot of works <ref type=\"bibr\" target=\"#b9\">[10]</ref><ref type=\"bibr\" target=\"#b10\">[11]</ref><ref type=\"bibr\" target=\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" t  reduced, the statistical fault injection (SFI) experiments are still time-consuming. SmartInjector <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposes an intelligent fault injection framework to ident nly one representative fault is selected to implement fault injection for each group. SmartInjector <ref type=\"bibr\" target=\"#b11\">[12]</ref> proposes an intelligent fault injection framework to ident. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  b \u2208 R d\u00d72d is a shared representation of various relations. For all the experiment, we use PyTorch <ref type=\"bibr\" target=\"#b34\">[35]</ref> and PyTorch geometric <ref type=\"bibr\" target=\"#b10\">[11]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: that the model manifold and the true distribution's support have a non-negligible intersection (see <ref type=\"bibr\" target=\"#b0\">[1]</ref>), and this means that the KL distance is not defined (or sim ining GANs is well known for being delicate and unstable, for reasons theoretically investigated in <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>In this paper, we direct our attention on the various  zero. This happens to be the case when two low dimensional manifolds intersect in general position <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p><p>Since the Wasserstein distance is much weaker than th ing gradients, as can be seen in Figure <ref type=\"figure\">1</ref> of this paper and Theorem 2.4 of <ref type=\"bibr\" target=\"#b0\">[1]</ref>. In Figure <ref type=\"figure\" target=\"#fig_1\">2</ref> we sho e <ref type=\"bibr\" target=\"#b3\">[4]</ref>. This last phenomenon has been theoretically explained in <ref type=\"bibr\" target=\"#b0\">[1]</ref> and highlighted in <ref type=\"bibr\" target=\"#b10\">[11]</ref> nerated image, when the pixels were already normalized to be in the range <ref type=\"bibr\">[0,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref>. This is a very high amount of noise, so much that when paper. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rom a member from one partition to a member of the other represents the user purchasing the product <ref type=\"bibr\" target=\"#b12\">[13]</ref>. The ability to utilize information from the graphical str. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng. It would be necessary to investigate whether and how the development in random-walk polynomials <ref type=\"bibr\" target=\"#b10\">[9]</ref> can support fast approximations of the closed-form matrices. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: utual information <ref type=\"bibr\" target=\"#b6\">(Cover and Thomas 2012)</ref>, or L1 regularization <ref type=\"bibr\" target=\"#b20\">(Ng 2004)</ref> to select useful features. Machine learning algorithm st two major methods for comparison: the context-free grammar (CFG) produced by the Berkeley parser <ref type=\"bibr\" target=\"#b20\">(Petrov et al. 2006)</ref>  </p></div> <div xmlns=\"http://www.tei-c.o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ry deep. Several architectures have been proposed recently to enable training of very deep networks <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: walk to generate node sequences and then perform skip-gram <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b24\">25]</ref> over the node sequences to learn embeddings. Since each vie. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b16\">Phang et al., 2018)</ref>.</p><p>When finetuning a big, pretrained language model, dropout <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014)</ref> has been used as a regularization tec er of that neuron as w during training, then we use (1 \u2212 p)w for that weight parameter at test time <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014)</ref>. This ensures that the expected outpu ght decay of \u03bb is equivalent to wdecay(0, \u03bb).</p><p>Probability for Dropout and Dropconnect Dropout <ref type=\"bibr\" target=\"#b20\">(Srivastava et al., 2014</ref>) is a regularization technique selecti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /www.tei-c.org/ns/1.0\"><head n=\"2.2.1.\">Rectifier Linear Unit</head><p>Rectifier Linear Unit (ReLU) <ref type=\"bibr\" target=\"#b22\">[22]</ref> is a piece-wise linear activation function that outputs ze. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dversarial samples <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>; adversaries subtly alter legitimate inputs (call input per g on previous work <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> describing how adversaries can efficiently select perturbat f previous attacks <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> for knowledge of the target architecture and parameters. We ike neural networks<ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. In addition, we introduce new techniques to craft adversar ep neural network (DNN) using crafted inputs and output labels generated by the target \"victim\" DNN <ref type=\"bibr\" target=\"#b18\">[19]</ref>. Thereafter, the local network was used to generate advers d in <ref type=\"bibr\" target=\"#b11\">[12]</ref> or the Jacobian-based iterative approach proposed in <ref type=\"bibr\" target=\"#b18\">[19]</ref>. We only provide here a brief description of the fast grad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f stable feature distillation, which consists of a deep global balancing regression (DGBR) algorithm<ref type=\"bibr\" target=\"#b12\">[13]</ref>, a teacher network and a student network. The DGBR algorit ner is another promising direction.</p><p>Feature-Based Module. The current stable feature approach <ref type=\"bibr\" target=\"#b12\">[13]</ref> needs much time and computing resources. For implementing . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: se performances than a single convolution due to overfitting. To overcome overfitting, Liang and Hu <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses a recurrent layer that takes feed-forward inputs into is in accordance with the limited success of previous methods using at most three recursions so far <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Among many reasons, two severe problems are vanishing and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ntations of sentences and their similarity <ref type=\"bibr\" target=\"#b25\">[26]</ref>. In prior work <ref type=\"bibr\" target=\"#b31\">[32]</ref>, we also utilized a Siamese BERT model to determine the di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Peters et al., 2018)</ref>, GPT-2 <ref type=\"bibr\" target=\"#b26\">(Radford et al., 2019)</ref>, BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, <ref type=\"bibr\">XLNet (Yang et al., 2019 as reestablished the new state-ofthe-art baselines across various tasks, such as question answering <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, coreference resolution <ref type=\"bibr\" t in the pre-training stage, such as updating the model using only short sequences in the early stage <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>.</p><p>Common strategies for reducing memo ction. Following the paradigm of language model pre-training and down-stream task fine-tuning, BERT <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref> consists of multiple layers of bidirection lti-head self-attention layer and a position-wise feed-forward layer. Using the same notation as in <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019)</ref>, we denote the number of Transformer layer ource-intensive process. For instance, the training of BERT-family models is notoriously expensive. <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref> report that it takes four days for pre-trai e compare BlockBERT with the following baselines:</p><p>Google BERT The pre-trained base model from <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref>.</p><p>RoBERTa-2seq and RoBERTa-1seq We com \"figure\">6</ref>.</p><p>For all the pre-trained models, we adopt the same fine-tuning QA setup from <ref type=\"bibr\" target=\"#b4\">Devlin et al. (2019)</ref>.</p><p>The tokenized paragraph (p 1 , \u2022 \u2022 \u2022 n units H leads to significant performance degradation <ref type=\"bibr\">(Vaswani et al., 2017;</ref><ref type=\"bibr\" target=\"#b4\">Devlin et al., 2019)</ref> and does not address the long sequence issu  target=\"#b3\">Dai et al., 2019)</ref> and its successful application on language model pre-training <ref type=\"bibr\" target=\"#b4\">(Devlin et al., 2019;</ref><ref type=\"bibr\" target=\"#b26\">Radford et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e leap in scaling NMT to realistic data size has been achieved by the introduction of subword units <ref type=\"bibr\" target=\"#b18\">(Sennrich et al., 2016)</ref>, but the long-term vision of the deep-l .tei-c.org/ns/1.0\" place=\"foot\" n=\"11\" xml:id=\"foot_6\">More details on T2T with BPE subword units by<ref type=\"bibr\" target=\"#b18\">Sennrich et al. (2016)</ref> vs. the internal implementation can be f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"bibr\" target=\"#b14\">(Hewitt and Manning, 2019;</ref><ref type=\"bibr\">Tenney et al., 2019a,b;</ref><ref type=\"bibr\" target=\"#b17\">Jawahar et al., 2019;</ref><ref type=\"bibr\" target=\"#b12\">Goldberg, 2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 1.0\"><head>Functional Unit Count Latency</head><p>Simple The processor has a lookup-free data cache <ref type=\"bibr\" target=\"#b6\">[7]</ref> that allows up to 8 pending misses to different cache lines.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tivation boundaries, and thus suggested a hinge loss. <ref type=\"bibr\" target=\"#b21\">[22]</ref> and <ref type=\"bibr\" target=\"#b22\">[23]</ref> employed adversarial learning into the KD framework. Recen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: target=\"#b1\">[2]</ref><ref type=\"bibr\" target=\"#b2\">[3]</ref><ref type=\"bibr\" target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" targe yers) and achieved great performance. After that, many SR models have been proposed, including DRCN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, DRNN <ref type=\"bibr\" target=\"#b6\">[7]</ref>, LapSRN <ref t FSRCNN <ref type=\"bibr\" target=\"#b2\">[3]</ref>, VDSR <ref type=\"bibr\" target=\"#b3\">[4]</ref>, DR-CN <ref type=\"bibr\" target=\"#b4\">[5]</ref>, LapSRN <ref type=\"bibr\" target=\"#b5\">[6]</ref> and EDSR <re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as matching networks and bi-LSTM ( <ref type=\"bibr\" target=\"#b13\">[14]</ref>) or memory-networks ( <ref type=\"bibr\" target=\"#b9\">[10]</ref>) which are learned using a meta-learning approach: they aim n systems that learns to predict on novel problems based only on few labeled examples. For example, <ref type=\"bibr\" target=\"#b9\">[10]</ref> propose to use the recent memory-augmented neural network,  for now. The work of <ref type=\"bibr\" target=\"#b14\">[15]</ref> propose an extension of the model of <ref type=\"bibr\" target=\"#b9\">[10]</ref>, where the true label of the observed instance is withheld  llow a similar principle to what has been recently presented for one-shot learning problems, e.g in <ref type=\"bibr\" target=\"#b9\">[10]</ref>. It aims at extending the basic principle of training in ma. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  found helpful for providing context-aware recommendations <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. In <ref type=\"bibr\" target=\"#b2\">[3]</ref>, a text recomme. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b14\">Nair95b]</ref> use predictors similar to a \"two-level\" adaptive branch predictor <ref type=\"bibr\" target=\"#b21\">[Yeh91]</ref>. Then, we demonstrate that these \"twolevel like\" predic  level and have one or more tables of 2-bit counters (pattern history tables) in their second level <ref type=\"bibr\" target=\"#b21\">[Yeh91]</ref>. The contents of the first level shift-registers are ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ora of work demonstrates the effectiveness of deep generative models in this regard, recent work of <ref type=\"bibr\" target=\"#b8\">Nalisnick et al. (2019a)</ref> and <ref type=\"bibr\" target=\"#b0\">Choi  y influences the phenomenon The results suggest that the intriguing phenomenon in VAEs discussed by <ref type=\"bibr\" target=\"#b8\">Nalisnick et al. (2019a)</ref> and <ref type=\"bibr\" target=\"#b0\">Choi  noise model from Bernoulli to Gaussian (and otherwise remaining in the same experimental setting as <ref type=\"bibr\" target=\"#b8\">Nalisnick et al. (2019a)</ref>), the issue of assigning higher likelih s hardly feasible, with below-1/2 AUC scores. Meanwhile, with a Bernoulli noise model (also used in <ref type=\"bibr\" target=\"#b8\">Nalisnick et al. (2019a)</ref>) both the likelihoodestimates and the K the table (where models are trained on MNIST) confirm the asymmetric behaviour already described by <ref type=\"bibr\" target=\"#b8\">Nalisnick et al. (2019a)</ref>, that is, switching the roles of the in re> <figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_1\"><head></head><label></label><figDesc><ref type=\"bibr\" target=\"#b8\">Nalisnick et al. (2019a)</ref> examine the phenomenon in detail, focus luation of generative models on OOD data <ref type=\"bibr\" target=\"#b12\">(Shafaei et al., 2018;</ref><ref type=\"bibr\" target=\"#b8\">Nalisnick et al., 2019a;</ref><ref type=\"bibr\" target=\"#b0\">Choi et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \"#b25\">26,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>. Started with the early success of ChebNet <ref type=\"bibr\" target=\"#b5\">[6]</ref> and GCN <ref type=\"bibr\" target=\"#b15\">[16]</ref> at vertex . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: can be summarized as the paths connecting the target user and item based on historical interactions <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. For example, give al behaviors of users reflect personal interests; meanwhile, the user groups can also profile items <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Hence, in each mo e d \u2032 m is the transformation size; and we select LeakyReLU(\u2022) as the nonlinear activation function <ref type=\"bibr\" target=\"#b38\">[38,</ref><ref type=\"bibr\" target=\"#b41\">41]</ref>. Such aggregation  y based on CF models <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b38\">[38]</ref><ref type=\"bibr\" target=\"#b39\">[39]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: een proposed and achieved state-of-the-art results in SISR <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar  have been proposed for a better performance. Lim et al. proposed a very deep and wide network EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref> by stacking modified residual blocks in which the batch no the performance. Fig. <ref type=\"figure\">3</ref>(Left) depicts a basic residual module used in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref> and ESRGAN <ref type=\"bibr\" target=\"#b30\">[31]</ref>. The  ion, we investigate the combination of our RFA framework with the basic residual block used in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Different from the original residual block used in image  N <ref type=\"bibr\" target=\"#b14\">[15]</ref>, MemNet <ref type=\"bibr\" target=\"#b24\">[25]</ref>, EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>, SRMD <ref type=\"bibr\" target=\"#b35\">[36]</ref>, NLRN <ref n the top row, which can ease the training difficulty to some extent (e.g. residual scaling in EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>).</p><p>(2) Feature maps after the attention mechanism ten rchitectures. Here we introduce one of the basic architecture used by some state-of-the-art methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ecommendations based on collaborative filtering principles <ref type=\"bibr\" target=\"#b14\">[15,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>, they have not been. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" target=\"#b38\">39]</ref> work under the mech s, some baseline methods on GNN, for example, GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref> and GAT <ref type=\"bibr\" target=\"#b32\">[33]</ref>, are demonstrated to be capable of extracting features of  t can be used to generate node embeddings, e.g., GCN <ref type=\"bibr\" target=\"#b11\">[12]</ref>, GAT <ref type=\"bibr\" target=\"#b32\">[33]</ref> and gated graph networks <ref type=\"bibr\" target=\"#b17\">[1 get=\"#b19\">[20]</ref>.</p><p>As suggested in previous work <ref type=\"bibr\" target=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b32\">33]</ref>, the multi-head attention can help to stabilize the trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: la\" target=\"#formula_0\">1</ref>) as an approximation of the normalization factor of edge likelihood <ref type=\"bibr\" target=\"#b22\">[23]</ref>. To improve efficiency when training with large batch size r\" target=\"#b27\">[28]</ref>. Textual annotation embeddings are trained using a Word2Vec-based model <ref type=\"bibr\" target=\"#b22\">[23]</ref>, where the context of an annotation consists of other anno. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng active learning <ref type=\"bibr\" target=\"#b14\">(Sterckx et al. 2014</ref>) and negative patterns <ref type=\"bibr\" target=\"#b17\">(Takamatsu, Sato, and Nakagawa 2012)</ref>.</p><p>Previous methods ar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[28]</ref>, <ref type=\"bibr\" target=\"#b30\">[31]</ref>, <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib r model with the other five state-of-the-art approaches( <ref type=\"bibr\" target=\"#b36\">[37]</ref>, <ref type=\"bibr\" target=\"#b29\">[30]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bib arget=\"#b36\">[37]</ref> uses the combination of GoogleNet feature and 3D-CNN feature. S2VT-rgb-flow <ref type=\"bibr\" target=\"#b29\">[30]</ref> uses the two-stream features consisting of RGB feature ext. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: orks are competitive or state-of-the-art for several time series tasks-for instance, speech to text <ref type=\"bibr\" target=\"#b13\">[15]</ref>, translation <ref type=\"bibr\" target=\"#b20\">[22]</ref>, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref> or devirtualization <ref type=\"bibr\" target=\"#b28\">[28]</ref \">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, <ref type=\"bibr\" target=\"#b28\">[28]</ref>. Ishizaki et al.   direct calls <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>, as shown in Fig. <ref type=\"figure\" target=\"#fig_0\">11b</re tion call and its devirtualized form. usually has higher accuracy than an indirect branch predictor <ref type=\"bibr\" target=\"#b5\">[6]</ref>. However, not all indirect calls can be converted to multipl form RCPO, the following conditions need to be fulfilled <ref type=\"bibr\" target=\"#b17\">[18]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>:</p><p>1. The number of frequent target addresses from a cal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b36\">37]</ref>, natural language processing <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>, etc. In this work, we propose, HyperGCN, a novel training . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ps. Subsequently, a full-connected layer with 300 neurons is used with rectified linear unit (ReLU) <ref type=\"bibr\" target=\"#b20\">[21]</ref> as the activation function. The activations of this full-c 1024 followed by a linear output layer. We use the rectified linear unit (ReLU) activation function <ref type=\"bibr\" target=\"#b20\">[21]</ref> for non-linearity at the hidden layer. A dropout <ref type. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ding block for many efficient neural network architectures <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> and we use them in . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: er, recent research showed that an attacker could generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ b0\">(1)</ref> Training the target classifier with adversarial examples, called adversarial training <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>; <ref type=\"bibr\" ta cally, researchers showed that it was possible to generate adversarial examples to fool classifiers <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" targ  one may use a mixture of normal and adversarial examples in the training set for data augmentation <ref type=\"bibr\" target=\"#b33\">[34,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, or mix the advers =\"2.3\">Existing attacks</head><p>Since the discovery of adversarial examples for neural networks in <ref type=\"bibr\" target=\"#b33\">[34]</ref>, researchers have found adversarial examples on various ne h leveraged gradient based optimization from normal examples <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>. Moosavi et al. showe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: eration for paraphrasing <ref type=\"bibr\" target=\"#b15\">(Liu et al., 2020)</ref> and summa-rization <ref type=\"bibr\" target=\"#b30\">(Schumann et al., 2020)</ref>.</p></div> <div xmlns=\"http://www.tei-c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d does not incur any storage overhead and is transferable across both hardware and software changes <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref>. Functional warmin t, requires huge storage overhead, and does not allow for software changes. Functional warming (FW) <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34]</ref> does not incur any on, which is fast but does not allow for changes to the software. Virtualized Fast-Forwarding (VFF) <ref type=\"bibr\" target=\"#b25\">[26]</ref> leverages hardware virtualization to quickly get to the ne 31]</ref> extend on the concept of BLRL using a form of hardware state checkpoints. Sandberg et al. <ref type=\"bibr\" target=\"#b25\">[26]</ref> propose a method that uses two parallel simulations, pessi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and overly-smoothed results. Specifically, perceptual loss <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> is introduced to optimize a superresolution model in a featur tions and lead to overly-smooth results. Perceptual losses <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> are proposed to enhance the visual quality by minimizing the  ><p>where (x i , y i ) are training pairs. Perceptual loss <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> and adversarial loss <ref type=\"bibr\" target=\"#b26\">[27,</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  target=\"#b13\">[14]</ref>, which was shown to be useful in large-scale conditional generation tasks <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b32\">34]</ref>.</p><p>The model-agn \"#b17\">[19]</ref>, but replace downsampling and upsampling layers with residual blocks similarly to <ref type=\"bibr\" target=\"#b5\">[6]</ref> (with batch normalization [15] replaced by instance normaliz tional and fully connected layers in all the networks. We also use self-attention blocks, following <ref type=\"bibr\" target=\"#b5\">[6]</ref> and <ref type=\"bibr\" target=\"#b40\">[42]</ref>. They are inse. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ;</ref><ref type=\"bibr\">Kitaev et al., 2019, i.a.)</ref>.</p><p>In this context, the GLUE benchmark <ref type=\"bibr\" target=\"#b62\">(Wang et al., 2019a)</ref> has become a prominent evaluation framewor s, we use an average of the metrics. More information on the tasks included in GLUE can be found in <ref type=\"bibr\" target=\"#b62\">Wang et al. (2019a)</ref> and in <ref type=\"bibr\">Warstadt et al. (20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">Publication</head><p>Recognition. The advantage of PAM over neural baselines such as CNN-sentence <ref type=\"bibr\" target=\"#b9\">[10]</ref> and Bi-LSTM-CNN-CRF <ref type=\"bibr\" target=\"#b13\">[14]</re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>. There are five levels of ratings from 1 to 5 (higher is better). IMDB reviews are obtained from <ref type=\"bibr\" target=\"#b2\">(Diao et al., 2014)</ref>. The ratings range from 1 to 10. Yahoo answe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as follows.</p><p>\u2022 Flickr dataset. From Flickr, we first downloaded a set of image IDs provided by <ref type=\"bibr\" target=\"#b35\">[28]</ref>. Some images were unavailable, and limiting the number of . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  early active learning, there are clustering-based methods <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref> and transductive experimental design methods <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: )</ref> proposed FloWorM system that includes tracker, analyzer and reporter based on NetFlow data. <ref type=\"bibr\" target=\"#b5\">Abdulla et al. (2011)</ref> presented a support vector machine (SVM) m nti and Rossi, 2011)</ref>, or data fusion with other log files such as Snort, DNS related requests <ref type=\"bibr\" target=\"#b5\">(Abdulla et al., 2011)</ref> (number of DNS requests, response, normal. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: train. Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion <ref type=\"bibr\" target=\"#b21\">[22]</ref>. No parameter tuning was performed during neural network t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: oc bottom-up fashion, where PMU designers attempted to cover key issues via \"dedicated miss events\" <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Yet, how does one pin-point performance issues that were no rchy's top level. This accurate classification distinguishes our method from previous approaches in <ref type=\"bibr\" target=\"#b0\">[1]</ref>[5][6].</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><h same symptom. Such scenarios of L1 hits and near caches' misses, are not handled by some approaches <ref type=\"bibr\" target=\"#b0\">[1]</ref> <ref type=\"bibr\" target=\"#b4\">[5]</ref>.</p><p>Note performa t=\"#b5\">[6]</ref>, nor complex structures with latency counters as in Accurate CPI Stacks proposals <ref type=\"bibr\" target=\"#b0\">[1]</ref>[8] <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div x tempted to accurately classify performance impacts on out-of-order architectures. Eyerman et al. in <ref type=\"bibr\" target=\"#b0\">[1]</ref>[9] use a simulation-based interval analysis model in order t reference model) is that it restricts all stalls to a fixed set of eight predefined miss events. In <ref type=\"bibr\" target=\"#b0\">[1]</ref>[4] <ref type=\"bibr\" target=\"#b4\">[5]</ref> there is no consi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: nctions. Furthermore, self-attention mechanism proposed by <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> is proved effective for machine translation, which can yiel. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rns in our framework to extract useful features from the heterogeneous textrich network. Meta-paths <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref> phs, can offer more flexibility and capture richer network semantics than the widely used meta-path <ref type=\"bibr\" target=\"#b32\">[31]</ref> patterns. Recent studies have shown that incorporating mot of two authors (i.e., \"Jure Leskovec\" and \"Jon Kleinberg\").</p><p>It is worth noting that meta-path <ref type=\"bibr\" target=\"#b32\">[31]</ref> can be viewed as a special case of motif patterns when the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b3\">(Fei-Fei et al., 2006)</ref>, complex gradient transfer between tasks <ref type=\"bibr\" target=\"#b12\">(Munkhdalai and Yu, 2017)</ref>, and fine-tuning the target problem <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d parameter (Equation <ref type=\"formula\">7</ref>). We train a word piece convolutional LM (ConvLM) <ref type=\"bibr\" target=\"#b17\">[18]</ref> on the text data set described in Section 4.1 using the sa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ata, such as recommender systems <ref type=\"bibr\" target=\"#b24\">[25]</ref>, social network analysis <ref type=\"bibr\" target=\"#b10\">[11]</ref>, and drug discovery <ref type=\"bibr\" target=\"#b14\">[15]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: obabilities. But complex and tricky training methods make it hard to implement. Triggered attention <ref type=\"bibr\" target=\"#b8\">[9]</ref> utilizes the spikes produced by connectionist temporal class. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ommonly used performance measure for person re-id algorithms <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. CMC calculates the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to perform Chinese NER is to first perform word segmentation and then apply word sequence labeling <ref type=\"bibr\" target=\"#b7\">[Yang et al., 2016;</ref><ref type=\"bibr\">He and Sun, 2017]</ref>.</p> rd information for NER has attracted research attention <ref type=\"bibr\">[Passos et al., 2014;</ref><ref type=\"bibr\" target=\"#b7\">Zhang and Yang, 2018]</ref>. In particular, to exploit explicit word i ational efficiency <ref type=\"bibr\">[Strubell et al., 2017]</ref>.</p><p>Specifically, lattice LSTM <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref> employs double recurrent transition compu hinese word segmentation, character-based name taggers can outperform their word-based counterparts <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. <ref type=\"bibr\" target=\"#b7\">Zhang and  ilation. This method achieves great performance in the English NER task. Lattice LSTM. Lattice LSTM <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref> can model the characters in sequence and  ></head><label></label><figDesc>, Weibo NER[Peng and Dredze, 2015; He and Sun, 2016], and Resume NER<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<re NER<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>.The OntoNotes and MSRA are the newswire d utperform their word-based counterparts <ref type=\"bibr\" target=\"#b7\">[Zhang and Yang, 2018]</ref>. <ref type=\"bibr\" target=\"#b7\">Zhang and Yang [2018]</ref> exploit an RNNbased lattice structure to s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: age classification <ref type=\"bibr\" target=\"#b20\">[18,</ref><ref type=\"bibr\" target=\"#b16\">14,</ref><ref type=\"bibr\" target=\"#b11\">9,</ref><ref type=\"bibr\" target=\"#b17\">15,</ref><ref type=\"bibr\" targ (DNNs) have achieved great success in image classification <ref type=\"bibr\" target=\"#b16\">[14,</ref><ref type=\"bibr\" target=\"#b11\">9,</ref><ref type=\"bibr\" target=\"#b15\">13,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hitecture of TransformerCPI</head><p>The model we proposed is based on the transformer architecture <ref type=\"bibr\" target=\"#b41\">(Vaswani et al., 2017)</ref>, which was originally devised for neural. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: i-gate Mixture-of-Experts (MMoE) structure, which is inspired by the Mixture-of-Experts (MoE) model <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the recent MoE layer <ref type=\"bibr\" target=\"#b15\">[1 \"4\">MODELING APPROACHES 4.1 Mixture-of-Experts</head><p>The Original Mixture-of-Experts (MoE) Model <ref type=\"bibr\" target=\"#b20\">[21]</ref> can be formulated as:</p><formula xml:id=\"formula_5\">= n i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e their remarkable performance, recent studies show that GCNs are vulnerable to adversarial attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, i.e. carefully desi bibr\" target=\"#b37\">38]</ref> try to attack the model by changing training data and evasion attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref> try to generate fake after (evasion attacks) the training phase of GCNs. \u2022 Targeted or Non-targeted. In targeted attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, the attacker focus  eted attacks can be further divided into two categories based on attack settings. In direct attacks <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b36\">37]</ref>, the attacker can di r example, the attacker tends to connect nodes from different communities to confuse the classifier <ref type=\"bibr\" target=\"#b6\">[7]</ref>. While plain vectors cannot adapt to such changes, Gaussian   into the graph. We regard this method as an illustrating example of non-targeted attacks. \u2022 RL-S2V <ref type=\"bibr\" target=\"#b6\">[7]</ref> <ref type=\"foot\" target=\"#foot_2\">3</ref> : This method gene. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: onsisting of multiple types of objects and links, has been widely applied to many data mining tasks <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: tion in the rest of the text. The training was performed on a large-scale, distributed architecture <ref type=\"bibr\" target=\"#b1\">(Dean et al., 2012)</ref>, using 5 concurrent steps on each of 10 mode. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  to improve the performance of CNNs for various tasks, such as image and video classification tasks <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b33\">33]</ref>. Wang et al. <ref ty ks. The second one is the way of enhancing discriminative ability of the network. Channel attention <ref type=\"bibr\" target=\"#b9\">[9,</ref><ref type=\"bibr\" target=\"#b38\">38]</ref> has been shown to be rate non-local operations for spatial attention in video classification. On the contrary, Hu et al. <ref type=\"bibr\" target=\"#b9\">[ 9]</ref> proposed SENet to exploit channel-wise relationships to ach N-based SR models do not consider the feature interdependencies. To utilize such information, SENet <ref type=\"bibr\" target=\"#b9\">[9]</ref> was introduced in CNNs to rescale the channelwise features f he aggregated information by global covariance pooling, we apply a gating mechanism. As explored in <ref type=\"bibr\" target=\"#b9\">[9]</ref>, the simple sigmoid function can serve as a proper gating fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ipelines, low-latency access to texture memory is needed.</p><p>The Silicon Graphics' RealityEngine <ref type=\"bibr\" target=\"#b13\">[14]</ref> is an example of a high-end parallel graphics architecture , shading, texture mapping and finally Z-buffering. The pipeline is similar to the one described in <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Specifically, the texture mapping implementation is based. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: elow.</p><p>Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN <ref type=\"bibr\" target=\"#b15\">[16]</ref>. 3D-CNN has achieved state-of-the-art results in object cl /ref>. 3D-CNN has achieved state-of-the-art results in object classification on tridimensional data <ref type=\"bibr\" target=\"#b15\">[16]</ref>. 3D-CNN not only extracts features from each image frame, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and forwarded them to an accurate model (e.g. Fast R-CNN). <ref type=\"bibr\" target=\"#b36\">[37,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref> also attempted to localize objects sequentially. However, t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nt training examples, thus results are much worse for elements with a few instances during training <ref type=\"bibr\" target=\"#b27\">(Zhang et al., 2019c)</ref>. However, few-shot problem widely exists . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> accepted to ICLR 2018. In this direction, adversarial trainin. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b5\">[5]</ref>, kernel functions <ref type=\"bibr\" target=\"#b29\">[28]</ref> <ref type=\"bibr\" target=\"#b25\">[24]</ref> or other hand-crafted features which measure local neighbo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s to Unicode bytes. The encoder network consists of 5 unidirectional Long Short-Term Memory (LSTMs) <ref type=\"bibr\" target=\"#b24\">[25]</ref> layers, with each layer having 1, 400 hidden units. The de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: RNA) <ref type=\"bibr\" target=\"#b11\">[12]</ref>, and the recurrent neural network transducer (RNN-T) <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. In particular, th nd-to-end models including attention-based models <ref type=\"bibr\" target=\"#b6\">[7]</ref> and RNN-T <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> trained on \u223c12,500 \"http://www.tei-c.org/ns/1.0\"><head n=\"2.\">RNN-TRANSDUCER</head><p>The RNN-T was proposed by Graves <ref type=\"bibr\" target=\"#b12\">[13]</ref> as an extension to the connectionist temporal classificati ure <ref type=\"figure\">1</ref>, consists of an encoder (referred to as the transcription network in <ref type=\"bibr\" target=\"#b12\">[13]</ref>), a prediction network and a joint network; as described i e=\"bibr\" target=\"#b13\">[14]</ref>. The entire network is trained jointly to optimize the RNN-T loss <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which marginalizes over all alignments of target labels w p><p>During inference, the most likely label sequence is computed using beam search as described in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, with a minor alteration which was found to make the algor nsive without degrading performance: we skip summation over prefixes in pref(y) (see Algorithm 1 in <ref type=\"bibr\" target=\"#b12\">[13]</ref>), unless multiple hypotheses are identical.</p><p>Note tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: r\" target=\"#b11\">(Le and Mikolov, 2014;</ref><ref type=\"bibr\" target=\"#b25\">Tang et al., 2015;</ref><ref type=\"bibr\" target=\"#b3\">Bhatia et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Yang et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th HR image y s . This multi-loss structure resembles the deeply-supervised nets for classification <ref type=\"bibr\" target=\"#b23\">[21]</ref> and edge detection <ref type=\"bibr\" target=\"#b36\">[34]</re  type=\"bibr\" target=\"#b36\">[34]</ref>. However, the labels used to supervise intermediate layers in <ref type=\"bibr\" target=\"#b23\">[21,</ref><ref type=\"bibr\" target=\"#b36\">34]</ref> are the same acros. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: urately approximated by tracking a subset of randomly selected reuse distances and memory locations <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Finally, statistical cache modeling has been generalized an ations and computes their reuses during the warm-up interval prior to a detailed region. Prior work <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> uses watchpoints to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b2\">[3]</ref> require extra VGG-M network pretrained on VGG Face dataset <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Wilels et al. <ref type=\"bibr\" target=\"#b34\">[35]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Related to our work, for example, <ref type=\"bibr\" target=\"# ibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Related to our work, for example, <ref type=\"bibr\" target=\"#b6\">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model rget=\"#b6\">[7]</ref> uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS <ref type=\"bibr\" target=\"#b6\">[7]</ref>. These studies consider learning methods within the traditio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ef type=\"bibr\" target=\"#b36\">[37]</ref> uses semantic segmentation for video deblurring. Zhu et al. <ref type=\"bibr\" target=\"#b51\">[52]</ref> propose an approach to generate new clothing on a wearer.  nal bias at the input layer.</p><p>2) Compositional mapping -This method is identical to Zhu et al. <ref type=\"bibr\" target=\"#b51\">[52]</ref>. It decomposes an LR image based on the predicted semantic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: wer the expected stochastic gradient variance. As shown in <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>, the reduction of variance can lead to faster convergence.  ques, such as stratified sampling <ref type=\"bibr\" target=\"#b34\">[35]</ref> and importance sampling <ref type=\"bibr\" target=\"#b35\">[36]</ref> are proposed to achieve the variance reduction. Different . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  Hassani, 2019)</ref>. For an overview see <ref type=\"bibr\" target=\"#b69\">(Zhang et al., 2020;</ref><ref type=\"bibr\" target=\"#b61\">Wu et al., 2020)</ref>.</p><p>GNNs mostly require task-dependent labe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olutional kernels such as a fixed window <ref type=\"bibr\" target=\"#b5\">(Collobert et al. 2011;</ref><ref type=\"bibr\" target=\"#b12\">Kalchbrenner and Blunsom 2013)</ref>. When using such kernels, it is  e=\"bibr\" target=\"#b17\">Mikolov (2012)</ref> uses recurrent neural network to build language models. <ref type=\"bibr\" target=\"#b12\">Kalchbrenner and Blunsom (2013)</ref> proposed a novel recurrent netw. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Some following literature modifies the framework for the purpose of the adversarial training. IRGAN <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2017)</ref>  </p></div> <div xmlns=\"http://www.tei-c.org ive samples iteratively. And to make the generative module aware of relation information, following <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2018a)</ref>, we design a random walk based generating s name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type=\"bibr\" target=\"#b18\">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bust training (e.g. <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>), we tackle various additional challenges: Being the first  arget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref> providing guarantees that no perturbation w.r.t. a specific ss of methods based on convex relaxations are of relevance <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>. They construct a convex relaxation for computing a lower b nity-norm or L2-norm <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b19\">20]</ref>, often e.g. \u03f5 &lt; 0.1 This is clearly not practical in our  can often been done efficiently, and by exploiting duality it enables to even train a robust model <ref type=\"bibr\" target=\"#b19\">[20]</ref>. As already mentioned, our work differs significantly from  the ReLU activation function. While there are many ways to achieve this, we follow the approach of <ref type=\"bibr\" target=\"#b19\">[20]</ref> in this work. The core idea is (i) to treat the matrices H  makes this approach rather slow. As an alternative, we can consider the dual of the linear program <ref type=\"bibr\" target=\"#b19\">[20]</ref>. There, any dual-feasible solution is a lower bound on the  appendix. Note that parts of the dual problem in Theorem 4.3 have a similar form to the problem in <ref type=\"bibr\" target=\"#b19\">[20]</ref>. For instance, we can interpret this dual problem as a bac akes the computation of robustness certificates extremely fast. For example, adopting the result of <ref type=\"bibr\" target=\"#b19\">[20]</ref>, instead of optimizing over \u2126 we can set it to</p><formula o this). While there exist more computationally involved algorithms to compute more accurate bounds <ref type=\"bibr\" target=\"#b19\">[20]</ref>, we leave adaptation of such bounds to the graph domain fo odes in the graph. y * t denotes the (known) class label of node t.</p><p>To improve robustness, in <ref type=\"bibr\" target=\"#b19\">[20]</ref> (for classical neural networks) it has been proposed to in oblem of the above linear program is max</p><p>for l = 2, . . . L, (n, j) \u2208 I (l )</p><p>As done in <ref type=\"bibr\" target=\"#b19\">[20]</ref> we can exploit complementarity of the ReLU constraints cor. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: isting nodes on the graph. First, GCNs are usually applied in large-scale graphs in various domains <ref type=\"bibr\" target=\"#b30\">(Ying et al., 2018;</ref><ref type=\"bibr\" target=\"#b15\">Hamilton et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hes have been proposed <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar ype=\"bibr\" target=\"#b23\">[24]</ref>. The explicit approaches <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar  to solve this problem <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar tly leverage subtopics to determine the diversity of results <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar  target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and supervised approaches such as DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref  xplicit method calculating the distribution by counting the relevant document of the subtopic. DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> introduces the machine learning method into explicit appro ent \ud835\udc36 to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> score function for the generator. We introduce the score f <p>In the training process, we first train R-LTR <ref type=\"bibr\" target=\"#b25\">[26]</ref> and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> respectively using MLE loss in both ways. It is because ou br\" target=\"#b5\">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type=\"bibr\" target=\"#b23\">[24]</ref>, and DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref> as supervised baseline methods. Top 20 results of Lemur ar b7\">[8]</ref> as the RNN cell for comparison. In our experiments, we conduct the list-pairwise loss <ref type=\"bibr\" target=\"#b11\">[12]</ref> to train DSSA method. The feature vector 1 http://playbigd DSSA <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ns <ref type=\"bibr\" target=\"#b10\">(Husz\u00e1r, 2017)</ref> and learning Wasserstein Auto-Encoders (WAE, <ref type=\"bibr\" target=\"#b31\">Tolstikhin et al. (2018)</ref>).</p><p>Finally, we evaluate the perfo  in the VAE experiments, and the only difference is that we made decoders implicit, as suggested in <ref type=\"bibr\" target=\"#b31\">Tolstikhin et al. (2018)</ref>. More details can be found in Appendix. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dustry practitioners have turned to search-based compilation <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b27\">29,</ref><ref type=\"bibr\" targ nually-written assembly code on large matrix multiplications <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b43\">45]</ref>, as the code has bee tation definition of matrix multiplication. ple compiler techniques have been introduced (e.g., TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref>, Halide <ref type=\"bibr\" target=\"#b36\">[38]</ref>, Tensor C ically, the compiler partitions the large computational graph of a DNN into several small subgraphs <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This partition has a negligible effect on the performance  arch and learned cost model performs the best among them, which is also used in our evaluation. TVM <ref type=\"bibr\" target=\"#b9\">[10]</ref> utilizes a similar scheduling language and includes a templ  search for GPU code automatically, but it is not yet meant to be used for compute-bounded problems <ref type=\"bibr\" target=\"#b9\">[10]</ref>. It cannot outperform TVM on operators like conv2d and matm f type=\"bibr\" target=\"#b9\">[10]</ref>. It cannot outperform TVM on operators like conv2d and matmul <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b42\">44]</ref>. This is because of graph level include layout optimizations <ref type=\"bibr\" target=\"#b27\">[29]</ref>, operator fusion <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b33\">35]</ref>, constant folding <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  as follows:</p><p>\u2022 CTC-based:</p><p>EESEN <ref type=\"bibr\" target=\"#b10\">[11]</ref>, Stanford CTC <ref type=\"bibr\" target=\"#b27\">[28]</ref>, Baidu Deepsppech <ref type=\"bibr\" target=\"#b11\">[12]</ref. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pe=\"bibr\" target=\"#b5\">[9]</ref>.</p><p>Yahoo: We used the HTTP trace collected from the Yahoo! CDN <ref type=\"bibr\" target=\"#b9\">[13]</ref>. In this workload, the number of HTTP requests per connecti omising applications of MegaPipe, since typical HTTP connections are short and carry small messages <ref type=\"bibr\" target=\"#b9\">[13]</ref>. We present the measurement result in Figure <ref type=\"fig. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ll describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref> and <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: )</ref>. In this paper we use Checkpoint Processing and Recovery (CPR) as the baseline architecture <ref type=\"bibr\" target=\"#b1\">[2]</ref> since it has been shown to outperform conventional ROB-based rview</head><p>CPR is a ROB-free proposal for building scalable large instruction window processors <ref type=\"bibr\" target=\"#b1\">[2]</ref>. CPR addresses the scalability and performance limitations o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , and deep contextual language models from BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref> in a vanilla and Siamese architecture <ref type=\"bibr\" tar contextual embeddings as the ones used in BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XL-Net <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The Transformer architecture allowed the efficient unsupe ype=\"bibr\" target=\"#b36\">[37]</ref>, named BERT <ref type=\"bibr\" target=\"#b14\">[15]</ref> and XLNet <ref type=\"bibr\" target=\"#b40\">[41]</ref>. The two Transformer models are originally designed to sol  <ref type=\"bibr\" target=\"#b42\">[43]</ref> alone, XLNet uses additional Web corpora for pretraining <ref type=\"bibr\" target=\"#b40\">[41]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head  pected is that BERT generally achieves slightly better results than XLNet. According to Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>, XLNet surpasses BERT on the related GLUE benchmark <ref t  may be attributed to two reasons, pretraining on different corpora, and smaller models compared to <ref type=\"bibr\" target=\"#b40\">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained 0\">[41]</ref>. We use the BASE, not the LARGE versions of the pretrained models used by Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>. Furthermore, the published XLNet BASE model we considered ublished XLNet BASE model we considered is pretrained on different data than the one in Yang et al. <ref type=\"bibr\" target=\"#b40\">[41]</ref>  <ref type=\"foot\" target=\"#foot_13\">15</ref> . In contrast. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t taken by a student on a particular learning task, predict aspects of their next interaction x t+1 <ref type=\"bibr\" target=\"#b4\">[6]</ref>.</p><p>In the most ubiquitous instantiation of knowledge tra f binary variables, each of which represents understanding or non-understanding of a single concept <ref type=\"bibr\" target=\"#b4\">[6]</ref>. A Hidden Markov Model (HMM) is used to update the probabili. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: atures with a linear classifier can achieve good performance on different video analysis benchmarks <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>As for task of video-based emotion recognition, few w  3D ConvNets, the operations are performed spatio-temporally by adding an additional time dimension <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Hence such C3D networks preserve the temporal information o , and 2 fully connected layers, followed by a softmax output layer. Other parameters are similar to <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The specific C3D structure used in our implementation is sh del alone can achieve good performance in action recognition <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. And we found that the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ll a challenge for training diversification models.</p><p>To tackle this problem, inspired by IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref ty e Carlo search. It is also used in the traditional information retrieval area, Wang proposed IR-GAN <ref type=\"bibr\" target=\"#b18\">[19]</ref> which consists of two information retrieval models in it.  \ud835\udc5e, \ud835\udc46)) 1 + exp(\ud835\udc53 \ud835\udf19 (\ud835\udc51 |\ud835\udc5e, \ud835\udc46)) .<label>(7)</label></formula><p>Please note that different from IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, DVGAN-doc has an additional component \ud835\udc46 to represent the  , it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN <ref type=\"bibr\" target=\"#b18\">[19]</ref>, we generate negative document set \ud835\udc37 \u2032 by selecting the do. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the classical works <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b32\">33,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d dice similarity are introduced for efficient training of classification and segmentation tasks in <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>.</p><p>Furthermore e 3D-Unet architecture for volumetric segmentation learns from sparsely annotated volumetric images <ref type=\"bibr\" target=\"#b12\">[13]</ref>. A powerful end-toend 3D medical image segmentation system y be applied deep learning models based on SegNet <ref type=\"bibr\" target=\"#b9\">[10]</ref>, 3D-UNet <ref type=\"bibr\" target=\"#b12\">[13]</ref>, and V-Net <ref type=\"bibr\" target=\"#b13\">[14]</ref> with . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: The field that gathers all these questions under a common umbrella is graph signal processing (GSP) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>.</p><p>While the pr of the prior work that is more directly connected and in the spirit of signal processing on graphs, <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We organize the di /ref>. We organize the discussion along two main lines; some parts of the exposition follow closely <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>.</p><p>1) From al te the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>. This choice is s /ref> studies time signals. Graph signal processing (GSP)<ref type=\"foot\" target=\"#foot_0\">1</ref>  <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref>, <ref type=\"bibr\" t erpretation of DSP can be extended to develop a linear time shift invariant Graph Signal Processing <ref type=\"bibr\" target=\"#b1\">[2]</ref>. Consider now a graph signal s \u2208 C N , where the entries of  lized Laplacian L = D \u22121/2 LD \u22121/2 .</formula><p>The adjacency matrix A can be adopted as the shift <ref type=\"bibr\" target=\"#b1\">[2]</ref> for this general graph. Other choices have been proposed, in nt if it commutes with the shift,</p><formula xml:id=\"formula_18\">AH = HA.</formula><p>As proven in <ref type=\"bibr\" target=\"#b1\">[2]</ref>, if the characteristic polynomial p A (z) and the minimum po aph filtering to two graph Fourier transforms and a pointwise multiplication in the spectral domain <ref type=\"bibr\" target=\"#b1\">[2]</ref>. With a notion of frequency we can now consider the GSP equi Section II-C). If these conditions do not hold, the Jordan canonical form is used to obtain the GFT <ref type=\"bibr\" target=\"#b1\">[2]</ref>, but this is well known to be a numerically unstable procedu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ed much attention from big data machine learning community <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar t traditional DGC frameworks, such as GraphLab <ref type=\"bibr\" target=\"#b12\">[13]</ref> and Pregel <ref type=\"bibr\" target=\"#b14\">[15]</ref>, use edge-cut methods <ref type=\"bibr\" target=\"#b8\">[9,</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b5\">Kario et al., 2003)</ref>, alterations of the brain causing differences in memory and cognition <ref type=\"bibr\" target=\"#b14\">(SJ et al., 2009)</ref>, suppression of the immune system <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: <p>Our hybrid pointer-generator network facilitates copying words from the source text via pointing <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref>, which improves accuracy and handling of twork</head><p>Our pointer-generator network is a hybrid between our baseline and a pointer network <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref>, as it allows both copying words via poi plore sentence fusion using dependency trees.</p><p>Pointer-generator networks. The pointer network <ref type=\"bibr\" target=\"#b26\">(Vinyals et al., 2015)</ref> is a sequence-tosequence model that uses. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e space <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. Authors in <ref type=\"bibr\" target=\"#b11\">[12]</ref> looked at unsupervised pretraining on different languages . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: easure theoretical details, see <ref type=\"bibr\" target=\"#b0\">Daley and Vere-Jones (2003)</ref> and <ref type=\"bibr\" target=\"#b1\">Daley and Vere-Jones (2008)</ref>.</p></div> <div xmlns=\"http://www.te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ther study on Somali automatic speech recognition (ASR) has so far been described in the literature <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Somali is an Afroasiatic language. It is the offici. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ring knowledge to tasks with limited or no supervision when they are pretrained with autoregressive <ref type=\"bibr\" target=\"#b26\">(Radford et al., 2018;</ref><ref type=\"bibr\">2019)</ref> or masked la. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: has been initiated. In two independent studies <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> the distinguishing power of GNNs is link rmer class of MPNNs covers the GNNs studied in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref>, the latter class covers the GCNs <ref t thm.</p><p>For anonymous MPNNs related to GNNs <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> and degree-aware MPNNs related to GCNs < er the graph neural network architectures <ref type=\"bibr\" target=\"#b4\">[Hamilton et al., 2017</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> defined by:</p><formula xml:id=\"formula_ tly see, it follows from two independent works <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> that the distinguishing power of aMPNNs  et=\"#fig_1\">1</ref>. Proposition 5.2 (Based on <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref>). The classes M anon and M WL are equall h between anonymous graph neural networks <ref type=\"bibr\" target=\"#b4\">[Hamilton et al., 2017</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019]</ref> and degree-aware graph neural networks < n as a slight generalisation of the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019</ref><ref type=\"bibr\" target=\"#b11\">, Morris et al., 2019</ref>]. (ii) The distinguishing power of degree ep-by-step, by GNNs that use ReLU or sign as activation function. This result refines the result in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> in that their simulation using the ReLU f of aMPNNs which are of special interest: those arising from the graph neural networks considered in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. In Example 3.1 we established that such  of the proofs of Lemma 2 in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and Theorem 5 in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. We show, by induction on the number of r remark that we cannot use the results in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> as a black box because the class M anon i onsidered in those papers. The proofs in <ref type=\"bibr\" target=\"#b15\">[Xu et al., 2019]</ref> and <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> relate to graph neural networks which, in nd M WL , and thus also M anon , are equally strong. The following results are known. Theorem 5.5 ( <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>). (i) The classes M sign GNN and M WL are side effect, we obtain a simpler aMPNN M in M GNN , satisfying M WL M , than the one constructed in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. The proof strategy is inspired by that o ef type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. The proof strategy is inspired by that of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. Crucial in the proof is the notion of ro  of two, at the cost of introducing an extra parameter p \u2208 A. Furthermore, the aMPNN constructed in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref> uses two distinct weight matrices in A (s By the induction hypothesis, these rows are linearly independent. Following the same argument as in <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>, this implies that there exists an (s t\u22121 ure the labelling \"refines\" \u2113 \u2113 \u2113 (t) MWL . To do so, we again follow closely the proof strategy of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>. More specifically, we will need an analo  entries having value 1 and whose size will be determined from the context. Lemma 5.9 (Lemma 9 from <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>). Let C \u2208 A m\u00d7w be a matrix in which all  ns of the non-zero entries in \u00b5 \u00b5 \u00b5 Regarding future work, we point out that, following the work of <ref type=\"bibr\" target=\"#b11\">[Morris et al., 2019]</ref>, we fix the input graph in our analysis. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: from <ref type=\"bibr\" target=\"#b67\">Yang et al., 2019)</ref> surpasses human performance (87.1 from <ref type=\"bibr\" target=\"#b45\">Nangia and Bowman, 2019)</ref>  Figure <ref type=\"figure\">1</ref>: GL \">Yang et al., 2019)</ref> have clearly surpassed estimates of non-expert human performance on GLUE <ref type=\"bibr\" target=\"#b45\">(Nangia and Bowman, 2019)</ref>. The success of these models on GLUE   a short training phase before proceeding to the annotation phase, modeled after the method used by <ref type=\"bibr\" target=\"#b45\">Nangia and Bowman (2019)</ref> for GLUE. See Appendix C for details.<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g structure holds on common datasets (ImageNet <ref type=\"bibr\" target=\"#b72\">[75]</ref> and Places <ref type=\"bibr\" target=\"#b98\">[101]</ref>).</p><p>Being fully computational and representation-base fication on our dataset. We then fine tuned our taskspecific networks on other datasets (MIT Places <ref type=\"bibr\" target=\"#b98\">[101]</ref> for scene classification, ImageNet <ref type=\"bibr\" targe ata of transfer functions by 16x, ImageNet <ref type=\"bibr\" target=\"#b72\">[75]</ref> and MIT Places <ref type=\"bibr\" target=\"#b98\">[101]</ref>. Y-axis shows accuracy on the external benchmark while ba rate them using Knowledge Distillation <ref type=\"bibr\" target=\"#b39\">[41]</ref> from known methods <ref type=\"bibr\" target=\"#b98\">[101,</ref><ref type=\"bibr\">55,</ref><ref type=\"bibr\" target=\"#b52\">5. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b18\">[17,</ref><ref type=\"bibr\" target=\"#b19\">18,</ref><ref type=\"bibr\" target=\"#b20\">19,</ref><ref type=\"bibr\" target=\"#b21\">20]</ref> as the RNN building blocks. Using the CTC objective functio. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  1} .</p><p>All parameters of the three modules are trained jointly by backpropagation. The Adagrad <ref type=\"bibr\" target=\"#b1\">(Duchi et al., 2011)</ref> is used on all parameters in each training . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he faulted value<ref type=\"bibr\" target=\"#b29\">[30]</ref>,<ref type=\"bibr\" target=\"#b30\">[31]</ref>,<ref type=\"bibr\" target=\"#b31\">[32]</ref>.</note> \t\t</body> \t\t<back>  \t\t\t<div type=\"acknowledgement\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: viewed as a sequence-to-sequence learning problem. We exploit deep recurrent neural networks (RNNs) <ref type=\"bibr\" target=\"#b16\">[15,</ref><ref type=\"bibr\" target=\"#b17\">16]</ref> as the acoustic mo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -c.org/ns/1.0\"><head n=\"4\">Related Work</head><p>Early works <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ect (SNAP), which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Similarly to the Yelp review dataset, we also constructed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e target problem <ref type=\"bibr\" target=\"#b14\">(Qi et al., 2018)</ref>. The approaches proposed by <ref type=\"bibr\" target=\"#b18\">Snell et al. (2017)</ref> and <ref type=\"bibr\" target=\"#b20\">Sung et  ication</head><p>Few-shot classification <ref type=\"bibr\" target=\"#b21\">(Vinyals et al., 2016;</ref><ref type=\"bibr\" target=\"#b18\">Snell et al., 2017)</ref> is a task in which a classifier must be ada ng Networks <ref type=\"bibr\" target=\"#b21\">(Vinyals et al., 2016)</ref> 65.73 Prototypical Networks <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017)</ref> 68.17 Graph Network <ref type=\"bibr\" targe </p><p>\u2022 Prototypical Networks: a deep metric-based method using sample average as class prototypes <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017)</ref>.</p><p>\u2022 Graph Network: a graph-based few- the performance by few-shot classification accuracy following previous studies in few-shot learning <ref type=\"bibr\" target=\"#b18\">(Snell et al., 2017;</ref><ref type=\"bibr\" target=\"#b20\">Sung et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: /ref>. To encourage monotonic alignments, the auxiliary Connectionist Temporal Classification (CTC) <ref type=\"bibr\" target=\"#b26\">[27]</ref> objective is linearly interpolated <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b21\">[22]</ref>) and region-based convolutional neural networks (R-CNNs) <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Although region-based CNNs were computationally expensive a b5\">[6]</ref>. Although region-based CNNs were computationally expensive as originally developed in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, their cost has been drastically reduced thanks to sharing c rk whose last fc layer simultaneously predicts multiple (e.g., 800) boxes, which are used for R-CNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> object detection. Their proposal network is applied on a sin rget=\"#foot_3\">3</ref>For regression, we adopt the parameterizations of the 4 coordinates following <ref type=\"bibr\" target=\"#b5\">[6]</ref>:</p><formula xml:id=\"formula_2\">t x = (x \u2212 x a )/w a , t y = odel for ImageNet classification <ref type=\"bibr\" target=\"#b16\">[17]</ref>, as is standard practice <ref type=\"bibr\" target=\"#b5\">[6]</ref>. We tune all layers of the ZF net, and conv3 1 and up for th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b30\">31]</ref> or feature matrix <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref>) are not compatibl denoising autoencoder to disturb the structure information. To build a symmetric graph autoencoder, <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes Laplacian sharpening as the counterpart of Laplac ing the latent representations to match a prior distribution for robust node embeddings.</p><p>GALA <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes a symmetric graph convolutional autoencoder recov. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b35\">36]</ref>.</p><p>Recently, several works have shown that in practical. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the ghosting effects <ref type=\"bibr\" target=\"#b21\">[22]</ref> and the Laplacian data fidelity term <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Other methods in this area remove reflections by virtue of  >[17]</ref> 0.801 0.829 21.77 WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=\"bibr\" target=\"#b4\">[5]</re leGAN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and LB14 <r rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b0\">1]</ref> with fixed coefficients.</p><p>Separator (S). We perform a di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: Graph-based methods, such as SCAN <ref type=\"bibr\" target=\"#b43\">[44]</ref> and spectral clustering <ref type=\"bibr\" target=\"#b36\">[37]</ref> , first construct a similarity graph from a dataset, and t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: variety of relevance match signals and shows strong performance in various ad-hoc retrieval dataset <ref type=\"bibr\" target=\"#b3\">(Dai and Callan, 2019)</ref>. Recent research also has shown kernels c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tithreaded processor prior to encountering the blocked operation.</p><p>In Datascalar architectures <ref type=\"bibr\" target=\"#b4\">[5]</ref>, multiple processors, each tightly coupled with part of the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: amounts of supervision. However, even systems that have relied extensively on unsupervised features <ref type=\"bibr\" target=\"#b7\">(Collobert et al., 2011;</ref><ref type=\"bibr\" target=\"#b36\">Turian et s, and present a hybrid tagging architecture. This architecture is similar to the ones presented by <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref> and <ref type=\"bibr\" target=\"#b19\">Huang ></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">Pretrained embeddings</head><p>As in <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref>, we use pretrained word embeddings to in g of our    Several other neural architectures have previously been proposed for NER. For instance, <ref type=\"bibr\" target=\"#b7\">Collobert et al. (2011)</ref> uses a CNN over a sequence of word embed. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s neighbors to compute its new feature vector <ref type=\"bibr\" target=\"#b37\">(Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>. After k iterations of aggregation, a node  GNNs can be represented similarly to Eq. 2.1 <ref type=\"bibr\" target=\"#b37\">(Xu et al., 2018;</ref><ref type=\"bibr\" target=\"#b12\">Gilmer et al., 2017)</ref>.</p><p>For node classification, the node r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: stette 2018)</ref>. Orthogonally, even when accurate, such methods cannot explain given predictions <ref type=\"bibr\" target=\"#b26\">(Lipton 2018)</ref>.</p><p>A promising strategy for overcoming these . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head>B. FIFO Queue Benchmark</head><p>A simple FIFO queue implementation was written based upon <ref type=\"bibr\" target=\"#b7\">[8]</ref>. The queue is based around a ring buffer, with read and writ re operations are needed (i.e. we do not need CAS). For MIPS64 this has been implemented exactly as <ref type=\"bibr\" target=\"#b7\">[8]</ref>, for Mamba a simple modification was made. The FIFO ring buf. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and the iterative aggregation of neighbor representations <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b33\">34,</ref><ref type=\"bibr\" target=\"#b44\">45]</ref>. <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: , the widespread use of such tools raises legitimate privacy concerns. For instance, Mislove et al. <ref type=\"bibr\" target=\"#b22\">[24]</ref> demonstrated how, by analysing Facebook's social network s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b7\">6,</ref><ref type=\"bibr\" target=\"#b16\">15,</ref><ref type=\"bibr\" target=\"#b21\">20,</ref><ref type=\"bibr\" target=\"#b29\">28,</ref><ref type=\"bibr\" target=\"#b48\">47]</ref>. Such patterns have. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: acoustics and first-pass text hypotheses for second-pass decoding based on the deliberation network <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The deliberation model has been used in state-of-the-art  lation <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our deliberation model has a similar structure as <ref type=\"bibr\" target=\"#b15\">[16]</ref>: An RNN-T model generates the first-pass hypotheses, and d head><p>A deliberation model is typically trained from scratch by jointly optimizing all components <ref type=\"bibr\" target=\"#b15\">[16]</ref>. However, we find training a two-pass model from scratch t get=\"#b0\">[1]</ref>, and a deliberation decoder, similar to <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. The shared encoder takes log-mel filterbank energies, x = . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  labeling process of DS and finds noisy patterns that mistakenly label a relation. Data programming <ref type=\"bibr\" target=\"#b13\">(Ratner et al., 2016</ref><ref type=\"bibr\" target=\"#b12\">(Ratner et a 7\">Takamatsu et al. (2012)</ref> directly models the labeling process of DS to find noisy patterns. <ref type=\"bibr\" target=\"#b13\">Ratner et al. (2016</ref><ref type=\"bibr\" target=\"#b12\">Ratner et al.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: vals must wait until the next core visit. This policy is usually known as \"gated M -limited\" policy <ref type=\"bibr\" target=\"#b13\">[13]</ref>. To complete the processing of a packet, a core needs at l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ive filtering <ref type=\"bibr\" target=\"#b42\">[43]</ref>, <ref type=\"bibr\" target=\"#b43\">[44]</ref>, <ref type=\"bibr\" target=\"#b44\">[45]</ref>. The active learning method is also called the Ask-To-Rate. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ).</p><p>To effect our study, we use a collection of abstracts from a corpus of scientific articles <ref type=\"bibr\" target=\"#b0\">(Ammar et al., 2018)</ref>. We extract entity, coreference, and relati  and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences <ref type=\"bibr\" target=\"#b0\">(Ammar et al., 2018)</ref>.</p><p>For each abstract, we create a knowl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion so far have been linear, based on various methods of factorizing the third-order binary tensor <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011;</ref><ref type=\"bibr\" target=\"#b28\">Yang et al.  expressiveness.</p><p>Finally, we show that several previous stateof-the-art linear models, RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref>, DistMult <ref type=\"bibr\" target=\"#b28\"> lated Work</head><p>Several linear models for link prediction have previously been proposed: RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref>  HypER <ref type=\"bibr\" target=\"#b0\">(Bal d><p>Several previous tensor factorization models can be viewed as a special case of TuckER: RESCAL <ref type=\"bibr\" target=\"#b16\">(Nickel et al., 2011)</ref> Following the notation introduced in Sect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  well-known that this incurs a phase inconsistency problem <ref type=\"bibr\" target=\"#b10\">[11,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b12\">13]</ref>, especially for spe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re used in Japanese NER, such as Maximum Entropy Model (MEM) <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> , Hidden Markov Model (HMM) <ref type=\"bibr\" target=\"#b3\">[4,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[5]</ref> 70.01 \u00b1 0.63% 67.26 \u00b1 1.12% Low&amp;SentiBank 70.54 \u00b1 1.00% 68.03 \u00b1 1.36% SentiStrength <ref type=\"bibr\" target=\"#b36\">[29]</ref> 59.30 \u00b1 0.87% 62.78 \u00b1 0.91% USEA <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ral angle is a commonly used distance metric to measure the difference between two spectral vectors <ref type=\"bibr\" target=\"#b22\">(Kruse et al., 1993)</ref>. The reflectance of individual pixel is re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ethod by considering the degree distributions of users/items in the graph. Lastly, Sindhwani et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> regarded unobserved feedback as optimization variables and. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: are control flow checking is to partition the program into basic blocks (branch-free parts of code) <ref type=\"bibr\" target=\"#b14\">[14]</ref>. For each block a deterministic signature is calculated an > and On-line control flow error detection using relationship signatures among basic blocks (RSCFC) <ref type=\"bibr\" target=\"#b14\">[14]</ref>.</p><p>ECCA, firstly, assigns a unique prime number identi rget=\"#b15\">[15]</ref> technique to the original code, \uf06c a safe one, obtained by applying the RSCFC <ref type=\"bibr\" target=\"#b14\">[14]</ref> technique to the original code, \uf06c a safe one, obtained by . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: narios: membership query synthesis <ref type=\"bibr\" target=\"#b31\">[32]</ref>, stream-based sampling <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and pool-based sampling <ref type=\"bibr\" target=\"#b33\">[3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: finition of black-box access as query access <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al., 2017;</ref><ref type=\"bibr\" target=\"#b11\">Hayes &amp; Dan rediction API with small datasets like MNIST and successfully demonstrated an untargeted attack. As <ref type=\"bibr\" target=\"#b15\">Liu et al. (2017)</ref> demonstrated, it is more difficult to transfe ularly when attacking models trained on large datasets like ImageNet. Using ensemble-based methods, <ref type=\"bibr\" target=\"#b15\">Liu et al. (2017)</ref> overcame these limitations to attack the Clar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e most widely used one, <ref type=\"bibr\">BERT [Devlin et al., 2018]</ref> builds on the Transformer <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> architecture and improves the pre-trainin -c.org/ns/1.0\"><head n=\"4.1\">Background of BERT</head><p>Based on a multi-layer Transformer encoder <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> (The transformer architecture has been ub /1.0\"><head>Visit Embedding</head><p>Similar to BERT, we use a multi-layer Transformer architecture <ref type=\"bibr\" target=\"#b4\">[Vaswani et al., 2017]</ref> as our visit encoder. The model takes the. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: number of clusters reaches the specified K.</p><p>When K is unknown, we adopt an optimal modularity <ref type=\"bibr\" target=\"#b21\">[22]</ref> partitioning mechanism to determine the partition of publi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ly related to our approach are the Defense-GAN <ref type=\"bibr\" target=\"#b36\">[37]</ref> and MagNet <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which first estimate the manifold of clean data to detect. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2\">[3,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> and FPGAs <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><p>Activation unit is a non-linear function that some l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: side information) of users and items <ref type=\"bibr\" target=\"#b14\">(Jain &amp; Dhillon, 2013;</ref><ref type=\"bibr\" target=\"#b38\">Xu et al., 2013)</ref>. In IMC, a rating is decomposed by r ij = x i . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  <ref type=\"bibr\" target=\"#b34\">(Wang et al., 2017)</ref>, and textual semantic similarity modeling <ref type=\"bibr\" target=\"#b9\">(He and Lin, 2016)</ref>. Many of these tasks can be treated as varian n et al., 2017b)</ref>, DecAtt <ref type=\"bibr\" target=\"#b21\">(Parikh et al., 2016)</ref>, and PWIM <ref type=\"bibr\" target=\"#b9\">(He and Lin, 2016)</ref>. Additionally, we report state-of-the-arts re presentations for similarity learning. Various neural network architectures, e.g., Siamese networks <ref type=\"bibr\" target=\"#b9\">(He et al., 2016)</ref> and attention <ref type=\"bibr\" target=\"#b26\">( atching</head><p>Recently, deep learning has achieved great success in many NLP and IR applications <ref type=\"bibr\" target=\"#b9\">(He and Lin, 2016;</ref><ref type=\"bibr\" target=\"#b29\">Sutskever et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t-flips in <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, a newly proposed Bit-Flip Attack (BFA) <ref type=\"bibr\" target=\"#b16\">[17]</ref> whose progressive bit searching algorithm can successfully ial Weight Attack</head><p>The bit-flip based adversarial weight attack, aka. Bit-Flip Attack (BFA) <ref type=\"bibr\" target=\"#b16\">[17]</ref>, is an adversarial attack variant which performs weight fa loss increment. Thus, the bit searching in iteration i can be formulated as an optimization process <ref type=\"bibr\" target=\"#b16\">[17]</ref>:</p><formula xml:id=\"formula_0\">max { Bi l } L f x; { Bi l e acceleration of modern AI applications. To clarify, we use the same threat model as in prior work <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which is listed in Table <ref type=\"table\">1</ref>.</p></ cted in Fig. <ref type=\"figure\" target=\"#fig_2\">2</ref>, the progressive bit search proposed in BFA <ref type=\"bibr\" target=\"#b16\">[17]</ref> is prone to identify vulnerable bit in the weight whose ab .   BFA Configuration. To evaluate the effectiveness of the proposed defense methods, the code from <ref type=\"bibr\" target=\"#b16\">[17]</ref> is utilized with further modification. The number of bit-f  trials. Note that, all the quantized DNN reported hereafter still uses the uniform quantizer as in <ref type=\"bibr\" target=\"#b16\">[17]</ref>, but with quantization-aware training instead of post-trai ns/1.0\"><head n=\"5.3.\">Comparison of Alternative Defense Methods</head><p>Adversarial weight attack <ref type=\"bibr\" target=\"#b16\">[17]</ref> is a recently developed security threat model for modern D Trained adversarial defense <ref type=\"bibr\" target=\"#b14\">[15]</ref> with strong weight attack BFA <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Again, adversarial input defense fails to defend BFA, req. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \">31]</ref>. As the pioneer CNN model for SR, Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b1\">[2]</ref> predicts the nonlinear LR-HR mapping via a fully convolution monality among the above CNN models is that their networks contain fewer than 5 layers, e.g., SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> uses 3 convolutional layers. Their deeper structures with 4  ween the input ILR image and the output HR image. There are three notes for VDSR: (1) Un-like SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> that only uses 3 layers, VDSR stacks 20 weight layers (3 \u00d7 3 arget=\"#b32\">[34]</ref>. The results are presented in Tab. 3. Note that Dataset Scale Bicubic SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref> SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref> RFL <ref typ e as <ref type=\"bibr\" target=\"#b12\">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, VDSR <ref   PSyCo [20] and IA <ref type=\"bibr\" target=\"#b28\">[30]</ref>. The deep models (d \u2264 8) include SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, DJSR <ref type=\"bibr\" target=\"#b31\">[33]</ref>, CSCN <ref t rsity of images. Shi et al. <ref type=\"bibr\" target=\"#b23\">[25]</ref> observe that the prior models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b30\">32]</ref> increase LR image's  1</ref> shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ , k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ  only. Therefore, the input and output images are of the same size. For fair comparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" targ  type=\"bibr\" target=\"#b9\">[10]</ref> RFL <ref type=\"bibr\" target=\"#b21\">[23]</ref>   the results of <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\">20,</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t harm their representational capacity for object recognition.</p><p>The Single Shot Detector (SSD) <ref type=\"bibr\" target=\"#b21\">[22]</ref> is one of the first attempts at using a ConvNet's pyramida tiple layers before computing predictions, which is equivalent to summing transformed features. SSD <ref type=\"bibr\" target=\"#b21\">[22]</ref> and MS-CNN <ref type=\"bibr\" target=\"#b2\">[3]</ref> predict >[35]</ref>, context modeling <ref type=\"bibr\" target=\"#b15\">[16]</ref>, stronger data augmentation <ref type=\"bibr\" target=\"#b21\">[22]</ref>, etc. These improvements are complementary to FPNs and sho. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the major execution time of GCNs through two primary execution phases: Aggregation and Combination <ref type=\"bibr\" target=\"#b15\">[15,</ref><ref type=\"bibr\" target=\"#b40\">40,</ref><ref type=\"bibr\" ta educing the number of vertices and the length of feature vectors. Readout can be a simple summation <ref type=\"bibr\" target=\"#b15\">[15]</ref> across vertices or further concatenation across iterations uct quantitative characterizations using a state-ofthe-art GCN software framework PyTorch Geometric <ref type=\"bibr\" target=\"#b15\">[15]</ref> on Intel Xeon CPU. The execution time breakdown of GCN (GC ce and energy consumption of HyGCN with state-of-the-art works, we evaluate PyTorch Geometric (PyG) <ref type=\"bibr\" target=\"#b15\">[15]</ref> on a Linux workstation equipped with two Intel Xeon E5-268 \" target=\"#b43\">43,</ref><ref type=\"bibr\" target=\"#b47\">47]</ref>. For instance, Py-Torch Geometric <ref type=\"bibr\" target=\"#b15\">[15]</ref> leverages message-passing framework to enhance its express are frameworks for hybrid-pattern GCNs are proposed recently <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">15,</ref><ref type=\"bibr\" target=\"#b28\">28,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr rkable progresses have been witnessed for object detection <ref type=\"bibr\" target=\"#b7\">[8]</ref>, <ref type=\"bibr\" target=\"#b9\">[10]</ref>, <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b11\">[12]</ref><ref type=\"bibr\" target=\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g and triplet losses <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. These losses have been used to learn powerful representati ss or category and the negative pair is chosen from other classes, often using hard-negative mining <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Self-supervised contrastive losses similarly use just one ization always improves performance, consistent with other papers that have used metric losses e.g. <ref type=\"bibr\" target=\"#b39\">[40]</ref>. We also find that the new supervised loss is able to trai n use the computationally expensive technique of hard negative mining to increase training efficacy <ref type=\"bibr\" target=\"#b39\">[40]</ref>. As a byproduct of this analysis, we motivate the addition ntations via supervised settings where hard negative mining leads to efficient contrastive learning <ref type=\"bibr\" target=\"#b39\">[40]</ref>. The triplet loss, which can only handle one positive and  ly, whereas triplet loss in practice requires computationally expensive hard negative mining (e.g., <ref type=\"bibr\" target=\"#b39\">[40]</ref>), the discussion in the previous section shows that the gr gatives have shown to improve classification accuracy when models are trained with the triplet loss <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Low temperatures are equivalent to optimizing for hard ne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r the sparsity and improve the performance of recommendation <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>.</p><p>A few recent studies <ref type=\"bibr\" target=\"#b8\">[. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b1\">[2]</ref>, a genetic algorithm is used to select and modify test-cases to increase coverage. In <ref type=\"bibr\" target=\"#b12\">[13]</ref>, coverage analysis data is used to modify the parameters o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ESPnet also uses Kaldi feature extraction for most of recipes, although multichannel end-to-end ASR <ref type=\"bibr\" target=\"#b30\">[31]</ref> includes speech enhancement and feature extraction with it. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e proposed method naturally extends our previous work of unsupervised information network embedding <ref type=\"bibr\" target=\"#b26\">[27]</ref> and first learns a low dimensional embedding for words thr e network is embedded into a low dimensional vector space that preserves the second-order proximity <ref type=\"bibr\" target=\"#b26\">[27]</ref> between the vertices in the network. The representation of is suitable for arbitrary types of information networks: undirected or directed, binary or weighted <ref type=\"bibr\" target=\"#b26\">[27]</ref>. The LINE model optimizes an objective function which aims vious work, we introduced the LINE model to learn the embedding of large-scale information networks <ref type=\"bibr\" target=\"#b26\">[27]</ref>. LINE is mainly designed for homogeneous networks, i.e., n l for embedding bipartite networks. The essential idea is to make use of the second-order proximity <ref type=\"bibr\" target=\"#b26\">[27]</ref> between vertices, which assumes vertices with similar neig jective (3) can be optimized with stochastic gradient descent using the techniques of edge sampling <ref type=\"bibr\" target=\"#b26\">[27]</ref> and negative sampling <ref type=\"bibr\" target=\"#b17\">[18]< descent in learning network embeddings. For the detailed optimization process, readers can refer to <ref type=\"bibr\" target=\"#b26\">[27]</ref>.</p><p>The embeddings of the word-word, word-document, and  (4) is to merge the all the edges in the three sets Eww, E wd , E wl and then deploy edge sampling <ref type=\"bibr\" target=\"#b26\">[27]</ref>, which samples an edge for model updating in each step, wi 0]</ref>.</p><p>\u2022 LINE: the large-scale information network embedding model proposed by Tang et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref>. We use the LINE model to learn unsupervised embeddings wi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dge bases are useful for various tasks, including training relation extractors and semantic parsers <ref type=\"bibr\" target=\"#b11\">(Hoffmann et al., 2011;</ref><ref type=\"bibr\" target=\"#b12\">Krishnamu s distantly-supervised relation extraction <ref type=\"bibr\" target=\"#b18\">(Mintz et al., 2009;</ref><ref type=\"bibr\" target=\"#b11\">Hoffmann et al., 2011;</ref><ref type=\"bibr\" target=\"#b27\">Surdeanu e. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t=\"#b22\">(Madotto et al., 2018)</ref>, but also some early works use text as additional information <ref type=\"bibr\" target=\"#b44\">(Xie et al., 2016;</ref><ref type=\"bibr\" target=\"#b22\">An et al., 201 target=\"#b37\">(Sun et al., 2019a)</ref> combines the advantages of both of them. Among these works, <ref type=\"bibr\" target=\"#b44\">Xie et al. (2016)</ref> propose to utilize entity descriptions as an . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  mixing coefficients <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref>.</p><formula xml:id=\"formula_0\">M = \u03b1B + \u03b2R,<label>(1)</lab ion results. Moreover, we introduce the gradient constraints <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> to make the model learning more effective, in which the edg  In this scenario, image priors such as different blur levels between the background and reflection <ref type=\"bibr\" target=\"#b25\">[26,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>, ghosting effects  \" target=\"#fig_3\">4</ref> 1 ) than previous linear functions <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar <p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type=\"bibr\" target=\"#b16\">[17]</ref> 0.801 0.829 21.77 WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ >, FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, WS16 <ref type=\"bibr\" target=\"#b25\">[26]</ref>, and LB14 <ref type=\"bibr\" target=\"#b16\">[17]</ref>. For a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rget=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Two alternative sampling implementations are commonly used nts in the program <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Several techniques have been proposed to warmup cache stat target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. The MTR adds multiprocessor and directory support to these itectural state is updated while fast-forwarding and then used to initialize the detailed simulator <ref type=\"bibr\" target=\"#b26\">[27]</ref>. Microarchitectural state is less amenable to checkpointin ed simulation needed to achieve small error rates with the desired confidence, the SMARTS framework <ref type=\"bibr\" target=\"#b26\">[27]</ref> recently proposed functional warming, which simulates larg. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rk based methods have achieved great success in relation extraction, including CNN-based approaches <ref type=\"bibr\" target=\"#b39\">[40,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and LSTMbased appr ions in a paragraph. To this end, our model consists of a single-sentence module with Piecewise CNN <ref type=\"bibr\" target=\"#b39\">[40]</ref>, and a cross-sentence module which leverages self-attentio based methods <ref type=\"bibr\" target=\"#b19\">[20]</ref>, and supervised relation extraction methods <ref type=\"bibr\" target=\"#b39\">[40]</ref>. The pattern-based method <ref type=\"bibr\" target=\"#b13\">[ . In the following we introduce evaluated methods in detail.</p><p>PCNN_single: Piecewise CNN model <ref type=\"bibr\" target=\"#b39\">[40]</ref>, which is one of the state of art single-sentence relation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tructural and functional properties. This field has seen tremendous progess in the past two decades <ref type=\"bibr\" target=\"#b0\">[1]</ref>, including the design of novel 3D folds <ref type=\"bibr\" tar g of structure and sequence, we refer the reader to a review of both methods and accomplishments in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Many of the major 'firsts' in protein design are due to Ros. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rotect the proprietary data. However, recent work by Zhu et al., \"Deep Leakage from Gradient\" (DLG) <ref type=\"bibr\" target=\"#b0\">[1]</ref> showed the possibility to steal the private training data fr nables us to always extract the ground-truth labels and significantly simplify the objective of DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> in order to extract good-quality data. Hence, we name our ap extraction with better fidelity.</p><p>\u2022 We empirically demonstrate the advantages of iDLG over DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> via comparing the accuracy of extracted labels and the fidel <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Methodology</head><p>Recent work by Zhu et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> presents an approach (DLG) to steal the proprietary data pro s</head><p>In this section, we empirically demonstrate the advantages of our (iDLG) method over DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref>. We perform experiments on the classification task over thre \" target=\"#b9\">[10]</ref> with 10, 100, and 5749 categories respectively. Following the settings in <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we use the randomly initialized LeNet for all experiments.   is used as the optimizer. For fast training, we resize all images in LFW to 32 \u00d7 32.</p><p>For DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref>, as described by the authors, we start the procedure with th % 100.0% LFW 79.1% 100.0% Table <ref type=\"table\">1</ref>: Accuracy of the extracted labels for DLG <ref type=\"bibr\" target=\"#b0\">[1]</ref> and iDLG. Note that iDLG always extracts the correct label a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Cedar <ref type=\"bibr\" target=\"#b48\">[48]</ref>. The Lisp machine had a real-time garbage collector <ref type=\"bibr\" target=\"#b5\">[5]</ref>.</p><p>A number of research kernels are written in high-leve  or closures awkward <ref type=\"bibr\" target=\"#b26\">[26]</ref>.</p><p>Concurrent garbage collectors <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b24\">24,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: an knowledge about the extraction. For a general introduction of first-order logic, please refer to <ref type=\"bibr\" target=\"#b11\">[12]</ref>.</p><p>Complete consistency describes the fact that the va. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: augmentation conditions instead of training on different subsets of data. In work parallel to ours, <ref type=\"bibr\" target=\"#b8\">Huang et al. (2017)</ref>   <ref type=\"bibr\">&amp; Ghahramani, 2002)</. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: uccessful use of CNNs in image tasks, a newly proposed sequential recommender, referred to as Caser <ref type=\"bibr\" target=\"#b28\">[29]</ref>, abandoned RNN structures, proposing instead a convolution la\">3</ref>) ( see <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b24\">25,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>).</p><formula xml:i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: e=\"bibr\" target=\"#b20\">[21]</ref> and examine six classification tasks: movie review sentiment (MR) <ref type=\"bibr\" target=\"#b28\">[29]</ref>, product reviews (CR) <ref type=\"bibr\" target=\"#b16\">[17]<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: br\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff and Jones, 1999)</ref>, which uses the prediction of models wi br\" target=\"#b22\">Li et al., 2008;</ref><ref type=\"bibr\" target=\"#b34\">Rosenberg et al., 2005;</ref><ref type=\"bibr\" target=\"#b33\">Riloff and Jones, 1999)</ref>. However, in order to prevent the deep . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: y objective used in word2vec ( <ref type=\"bibr\" target=\"#b8\">(Mikolov et al., 2013)</ref>, see also <ref type=\"bibr\" target=\"#b6\">(Levy and Goldberg, 2014</ref>)), and the Noise Contrastive Estimation br\" target=\"#b9\">(Mnih and Teh, 2012;</ref><ref type=\"bibr\" target=\"#b8\">Mikolov et al., 2013;</ref><ref type=\"bibr\" target=\"#b6\">Levy and Goldberg, 2014;</ref><ref type=\"bibr\" target=\"#b4\">Jozefowicz ) or equivalently PMI(x, y) = log p(y|x) p(y) = v 0 y \u2022 v x log H(\u2713)</formula><p>That is, following <ref type=\"bibr\" target=\"#b6\">(Levy and Goldberg, 2014)</ref>, the inner product v 0 y \u2022 v</p><p>x i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rogeneous textrich network. Meta-paths <ref type=\"bibr\" target=\"#b32\">[31]</ref> and motif patterns <ref type=\"bibr\" target=\"#b6\">[5,</ref><ref type=\"bibr\" target=\"#b19\">18]</ref> have been widely ado b31\">[30]</ref>, bioinformatics <ref type=\"bibr\" target=\"#b19\">[18]</ref>, and information networks <ref type=\"bibr\" target=\"#b6\">[5]</ref>. In the context of heterogeneous information networks, netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b2\">(Caruana, 1995;</ref><ref type=\"bibr\" target=\"#b1\">Bengio et al., 2011;</ref><ref type=\"bibr\" target=\"#b0\">Bengio, 2011)</ref>. In transfer learning, we first train a base netwo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><ref type=\"bibr\" target=\"#b5\">6]</ref>, previous model bias <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> and position bias < niform data directly <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref>. In this paper, we  f a recommender system, and that explicitly handling of the biases may help improve the performance <ref type=\"bibr\" target=\"#b15\">[16,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta f>.</p><p>A recent work has shown that a uniform data can alleviate the previous model bias problem <ref type=\"bibr\" target=\"#b15\">[16]</ref>. But the uniform data is always few and expensive to colle us on how to solve the bias problems in a recommender system with a uniform data. Along the line of <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we conduct empirical studies on a real advertising system /p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">MOTIVATION</head><p>In a recent work <ref type=\"bibr\" target=\"#b15\">[16]</ref>, it is shown that a uniform (i.e., unbiased) data can alle. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: or a router. As for its implementation, Open vSwitch (OVS) <ref type=\"bibr\" target=\"#b1\">[2]</ref>, <ref type=\"bibr\" target=\"#b2\">[3]</ref> is the most prominent open-source solution implementing a vi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: L as residual connection <ref type=\"bibr\" target=\"#b21\">[22]</ref>, followed by batch normalization <ref type=\"bibr\" target=\"#b31\">[32]</ref>. Then we feed the result into a fully connected feed-forwa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e 0.2. In addition, we use weight decay and tune this hyperparameter on each dataset using hyperopt <ref type=\"bibr\" target=\"#b5\">(Bergstra et al., 2015)</ref> for 60 iterations on the public split va. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: (GGNN) <ref type=\"bibr\" target=\"#b10\">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref type=\"bibr\" target=\"#b7\">(Lample et al., 2016)</ref> (BiLSTM-CRF), our model learns a weighted  are respec-tively 39.70%, 44.75%, 36.10% and 46.05%.</p><p>Models for Comparison. We use BiLSTM-CRF <ref type=\"bibr\" target=\"#b7\">(Lample et al., 2016)</ref> with character+bigram embedding without us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and labeling the individual body parts. Numerous type of features have been used such as silhouette <ref type=\"bibr\" target=\"#b6\">(Gouiaa and Meunier, 2015)</ref>, contour <ref type=\"bibr\" target=\"#b3. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref> and have connections to the large literature on metric lear otivated by noise contrastive estimation and N-pair losses <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, wherein the ability to discriminate between signal and noi  <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> or N-pair losses <ref type=\"bibr\" target=\"#b42\">[43]</ref>. Typically, the loss is applied at the last layer of a dee. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .0\"><head n=\"5.\">Language modelling</head><p>All language models were built using the SRILM toolkit <ref type=\"bibr\" target=\"#b18\">[19]</ref>. The vocabulary of the ASR system was drawn from the pool . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: oped for classical ASR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target= nd decoder models using recurrent models with LSTMs <ref type=\"bibr\" target=\"#b5\">[6]</ref> or GRUs <ref type=\"bibr\" target=\"#b3\">[4]</ref>. However, their use of hierarchy in the encoders demonstrate eep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Our best model achieves a WER of 10.53% where our baseline  oder depth of the baseline model without using any convolutional layers. Our baseline model follows <ref type=\"bibr\" target=\"#b3\">[4]</ref> using the skip connection technique in its time reduction. T btained 10.5% WER without a language model, an 8.5% absolute improvement over published best result <ref type=\"bibr\" target=\"#b3\">[4]</ref>. While we demonstrated our results only on the seq2seq task,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: puter vision methods <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b15\">16,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" tar xperiments, we follow the protocol used in previous papers <ref type=\"bibr\" target=\"#b19\">[20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, which uses unseen  #foot_0\">1</ref> , 10 2 ]), where the implementation of traditional classifiers becomes challenging <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>.</p><p>Arguably, t rix of pairwise distances of a subset of the training set (i.e., the mini-batch) allows Song et al. <ref type=\"bibr\" target=\"#b20\">[21]</ref> to design of a new loss function that integrates all posit > (with and without FANNG <ref type=\"bibr\" target=\"#b4\">[5]</ref>), (2) lifted structured embedding <ref type=\"bibr\" target=\"#b20\">[21]</ref>, (3) N-pairs metric loss <ref type=\"bibr\" target=\"#b19\">[2 hod, and ( <ref type=\"formula\">5</ref>) ), we use the same training and test set split described in <ref type=\"bibr\" target=\"#b20\">[21]</ref> across all datasets. Specifically, the means CUB200-2011 < target=\"#b22\">[23]</ref> weights and randomly initialize the final fully connected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>. We set the embedding size to 64 <ref type=\"bibr\" target=\" nnected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>. We set the embedding size to 64 <ref type=\"bibr\" target=\"#b20\">[21]</ref> and the learning rate for the randomly initialized fully c omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>For the experiments using triplet combined with glo =\"bibr\" target=\"#b22\">[23]</ref> and randomly initialize the final fully connected layer similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref> . The learning rate for the randomly initialized fully con omly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >[26]</ref>, satellite imaging <ref type=\"bibr\" target=\"#b27\">[29]</ref>, security and surveillance <ref type=\"bibr\" target=\"#b35\">[37]</ref>, where high-frequency details are greatly desired.</p><p>I. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ism Network, which is proved to be as powerful as the Weisfeiler-Lehman test for graph isomorphism. <ref type=\"bibr\" target=\"#b24\">Klicpera et al. (2019)</ref> separate the non-linear transformation f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: as SoftRank <ref type=\"bibr\" target=\"#b14\">[15]</ref> and indirect boosting methods like LambdaMART <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>A more direct approach to LTR metric optimization w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: variants of the architecture have been applied to parsing and other tasks requiring tree structures <ref type=\"bibr\" target=\"#b2\">(Blunsom et al., 2014)</ref>. However, the effectiveness of character-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b23\">20,</ref><ref type=\"bibr\" target=\"#b5\">2,</ref><ref type=\"bibr\" target=\"#b24\">21,</ref><ref type=\"bibr\" target=\"#b11\">8,</ref><ref type=\"bibr\" target=\"#b13\">10,</ref><ref type=\"bibr\" targ <ref type=\"bibr\" target=\"#b36\">[33]</ref>, B100 <ref type=\"bibr\" target=\"#b21\">[18]</ref>, Urban100 <ref type=\"bibr\" target=\"#b11\">[8]</ref>, and Manga109 <ref type=\"bibr\" target=\"#b22\">[19]</ref>. Th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r\" target=\"#b0\">2,</ref><ref type=\"bibr\" target=\"#b1\">3,</ref><ref type=\"bibr\" target=\"#b2\">4,</ref><ref type=\"bibr\" target=\"#b3\">5]</ref>. This pipeline system usually suffers from time delay, parame nd generates target words from left to right at each step [1, <ref type=\"bibr\" target=\"#b1\">3,</ref><ref type=\"bibr\" target=\"#b3\">5]</ref>. This model has also achieved promising results in ASR fields chitecture for all three tasks (ASR, ST and MT). The model architecture is similar with Transformer <ref type=\"bibr\" target=\"#b3\">[5]</ref>, which is the state-of-art model in MT task. Recently, this . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: allenge to process massive remote sensing images. In our work, two lightweight attention mechanisms <ref type=\"bibr\" target=\"#b16\">[17]</ref> which contains spatial attention and channel attention are ts a convolution operation with 7 \u00d7 7 kernel size.</p><p>In this study, we follow the method of Woo <ref type=\"bibr\" target=\"#b16\">[17]</ref>to integrate the two attention mechanisms. First, we use ch. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: te networks do not always transfer to the target model, especially when conducting targeted attacks <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b19\">Narodytska &am :</p><p>Black-box setting. In this paper, we use the definition of black-box access as query access <ref type=\"bibr\" target=\"#b6\">(Chen et al., 2017;</ref><ref type=\"bibr\" target=\"#b15\">Liu et al., 20 orders of magnitude more query-efficient than previous methods based on gradient estimation such as <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>. We show that our approach reliably produces  imated by querying the classifier rather than computed by autodifferentiation. This idea is used in <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>, where the gradient is estimated via pixel-by =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.2.\">BLACK-BOX ATTACKS WITH GRADIENT</head><p>ESTIMATION <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref> explore black-box gradient estimation methods mpability of the 2 and \u221e metric as well as the fixed-budget nature of the optimization algorithm in <ref type=\"bibr\" target=\"#b6\">Chen et al. (2017)</ref>, our method takes far fewer queries to genera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ttp://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Sequence-to-sequence (seq2seq) models <ref type=\"bibr\" target=\"#b1\">(Cho et al., 2014;</ref><ref type=\"bibr\" target=\"#b51\">Sutskever et al N) based encoder-decoder seq2seq model <ref type=\"bibr\" target=\"#b51\">(Sutskever et al., 2014;</ref><ref type=\"bibr\" target=\"#b1\">Cho et al., 2014)</ref> with attention mechanism <ref type=\"bibr\" targ r et al., 2014;</ref><ref type=\"bibr\" target=\"#b1\">Cho et al., 2014)</ref> with attention mechanism <ref type=\"bibr\" target=\"#b1\">(Bahdanau et al., 2014)</ref> to edit a raw sentence into the grammati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is information modality. GCNs are currently used to predict individual relations in social networks <ref type=\"bibr\" target=\"#b35\">[36]</ref>, model proteins for drug discovery <ref type=\"bibr\" target ding to greatly increasing interest in using GCNs for a variety of applications. In social networks <ref type=\"bibr\" target=\"#b35\">[36]</ref>, graphs represent connections between individuals based on. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: .tei-c.org/ns/1.0\"><head n=\"1.2\">Limitations of Previous Work</head><p>The Markov chain based model <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target v chains (FPMC) <ref type=\"bibr\" target=\"#b20\">[21]</ref> proposed by Rendle et al. and its variant <ref type=\"bibr\" target=\"#b1\">[2]</ref> improved this method by factorizing this transition matrix i  of E is latent for us, it is meaningless to interact with multiple successive columns at one time. <ref type=\"bibr\" target=\"#b1\">(2)</ref> There is no need to apply max pooling operation over the ver. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nce with other architectures such as Faster-RCNN <ref type=\"bibr\" target=\"#b35\">[36]</ref> and RFCN <ref type=\"bibr\" target=\"#b36\">[37]</ref> since our focus is on mobile/real-time models.</p><p>SSDLi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ling tool developed by Ammons et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> and Compaq's ProfileMe <ref type=\"bibr\" target=\"#b9\">[10]</ref>, leverage the on-chip performance counters found on modern . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: is nondifferentiable. Therefore, for S with model parameters \u03a6, we use the policy gradient based RL <ref type=\"bibr\" target=\"#b13\">[Sutton et al., 2000]</ref> to derive its gradient:</p><formula xml:i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: networks are using more than 150 layers as in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a;</ref><ref type=\"bibr\" target=\"#b7\">He et al., 2016b)</ref>.</p><p>Many NLP approaches consider words as b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, multiple input spectrogram inverse (MISI) <ref type=\"bibr\" target=\"#b14\">[15]</ref>, ISSIR <ref type=\"bibr\" target=\"#b15\">[16]</ref>, and consistent Wiener filtering <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: i-c.org/ns/1.0\"><head n=\"2\">Related Work</head><p>CNN Compression and Acceleration. Extensive works <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b19\">19,</ref><ref type=\"bibr\" ta time. We observe a correlation between the pre-fine-tune accuracy and the post fine-tuning accuracy <ref type=\"bibr\" target=\"#b20\">[20,</ref><ref type=\"bibr\" target=\"#b22\">22]</ref>. As shown in Table  For channel pruning, we use max response selection (pruning the weights according to the magnitude <ref type=\"bibr\" target=\"#b20\">[20]</ref>), and preserve Batch Normalization <ref type=\"bibr\" target /ref>. However, it requires iterative prune &amp; fine-tune procedure to achieve decent performance <ref type=\"bibr\" target=\"#b20\">[20]</ref>, and single-shot pruning without retraining will greatly h. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: orithms, they usually generate enormous amounts of intermediate data. GPM systems such as Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, and Fr ly proportional to the graph size, but also increases exponentially as the embedding size increases <ref type=\"bibr\" target=\"#b80\">[81]</ref>. Furthermore, GPM problems require compute-intensive opera describe two of these GPM systems briefly and then discuss their major limitations.</p><p>Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref> is a distributed GPM system. It proposes \"think like an em anonical test for each embedding, which has been demonstrated to be very expensive for large graphs <ref type=\"bibr\" target=\"#b80\">[81]</ref>.</p><p>Materialization of Data Structures: The list or arr oking the isomorphism test, embeddings in the worklist are first reduced using their quick patterns <ref type=\"bibr\" target=\"#b80\">[81]</ref>, and then quick patterns are aggregated using their canoni Experimental Setup</head><p>We compare Pangolin with the state-of-the-art GPM frameworks: Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, G-Mine r ease of programming.</p><p>GPM Frameworks: For ease-of-programming, GPM systems such as Arabesque <ref type=\"bibr\" target=\"#b80\">[81]</ref>, RStream <ref type=\"bibr\" target=\"#b84\">[85]</ref>, G-Mine head n=\"2.3\">Existing GPM Frameworks</head><p>Existing GPM systems target either distributed-memory <ref type=\"bibr\" target=\"#b80\">[81,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  is advantageous to existing methods for higher-order interaction learning, such as higher-Order FM <ref type=\"bibr\" target=\"#b2\">[3]</ref> and Exponential Machines <ref type=\"bibr\" target=\"#b22\">[23]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ors into coarse clusters typically through K-means algorithm, and the other is product quantization <ref type=\"bibr\" target=\"#b7\">[8]</ref> which does a fine-grained quantization to enable efficient c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ctly from characters have made rapid progress in recent years, and achieved very high voice quality <ref type=\"bibr\" target=\"#b0\">[1]</ref><ref type=\"bibr\" target=\"#b1\">[2]</ref><ref type=\"bibr\" targe s the latent variable does in VAE. Therefore, in this paper we intend to introduce VAE to Tacotron2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, a state-of-the-art end-to-end speech synthesis model, to le ate before add operation. The attention module and decoder have the same architecture as Tacotron 2 <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Then, WaveNet <ref type=\"bibr\" target=\"#b18\">[19]</ref> voc  usually neutral speaking style, is approaching the extreme quality close to human expert recording <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, the interests in expr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ections, background, and reflection images, respectively. Here, \u03b1 and \u03b2 are the mixing coefficients <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27,</ref><ref type=\"bibr\" targ  generation results and clearer separation results. Moreover, we introduce the gradient constraints <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26]</ref> to make the model le ><head n=\"4.1.\">Framework of the Proposed Scheme</head><p>In contrast to the conventional pipelines <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b28\">29,</ref><ref type=\"bibr\" targ ird columns in Figure <ref type=\"figure\" target=\"#fig_3\">4</ref> 1 ) than previous linear functions <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b25\">26,</ref><ref type=\"bibr\" targ n, and the background edge map (E) concurrently. Instead of one-toone framework in previous methods <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref>, our separator learn . Recently, deep learning based reflection removal methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref> with better generalization ability have been proposed to addr h previous methods <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, that heavily rely on the simplified model in Equation 1 and  learn the edge features of the reflections with the light field camera. The framework introduced in <ref type=\"bibr\" target=\"#b4\">[5]</ref> exploited the edge information when training the whole netwo and reflection, and three discriminator networks to produce the adversarial losses. Existing method <ref type=\"bibr\" target=\"#b4\">[5]</ref> can be treated as a special instance of our method when the   type=\"bibr\" target=\"#b28\">[29]</ref>, CycleGAN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, and FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>. The yellow boxes highlight some noticeable differences.</p> 5\">[26]</ref> 0.833 0.877 22.39 NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref> 0.832 0.882 23.70 FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type=\"bibr\" target=\"#b29\">[3 <ref type=\"bibr\" target=\"#b28\">[29]</ref>, CycleGAN <ref type=\"bibr\" target=\"#b29\">[30]</ref>, FY17 <ref type=\"bibr\" target=\"#b4\">[5]</ref>, NR17 <ref type=\"bibr\" target=\"#b0\">[1]</ref>, WS16 <ref typ able\" xml:id=\"tab_3\"><head>Table 3 .</head><label>3</label><figDesc>Efficiency comparisons with FY17<ref type=\"bibr\" target=\"#b4\">[5]</ref>, Zhang18<ref type=\"bibr\" target=\"#b28\">[29]</ref> and Wan18<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ased on learning a distributed representation for each word, which is also called a word embeddings <ref type=\"bibr\" target=\"#b17\">(Turian et al., 2010)</ref>. <ref type=\"bibr\" target=\"#b14\">Socher et arget=\"#foot_1\">3</ref> . However, there are many trained word embeddings that are freely available <ref type=\"bibr\" target=\"#b17\">(Turian et al., 2010)</ref>. A comparison of the available word embed beyond the scope of this paper. Our experiments directly utilize the trained embeddings provided by <ref type=\"bibr\" target=\"#b17\">Turian et al.(2010)</ref>.</p></div> <div xmlns=\"http://www.tei-c.org. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  for other network analysis tasks, such as link prediction <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b40\">41]</ref> and graph classification <ref type=\"bibr\" target=\"#b16\">[17. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  measure includes Entropy <ref type=\"bibr\" target=\"#b34\">[35]</ref> for classification and Variance <ref type=\"bibr\" target=\"#b35\">[36]</ref> for regression. The drawback of uncertainty sampling is th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  capitalize on FlowNet-s <ref type=\"bibr\" target=\"#b58\">[59]</ref> to produce optical flow, PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> is particularly remould in our motion stream. Compared to  4 2 , and 7 2 , respectively. Two-stream Feature Aggregation. For motion stream, we utilize PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> pre-trained on Flying Chairs dataset for optical flow esti t that the receptive field in sampling stream for offset prediction is smaller than that in PWC-Net <ref type=\"bibr\" target=\"#b59\">[60]</ref> for optical flow generation. As such, the range of estimat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dels of behavior prediction, to suggest the next resource a learner is likely to spend time on next <ref type=\"bibr\" target=\"#b23\">[24]</ref>. In the context of higher education, they have been used i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: his paper, we follow a simple instance discrimination task <ref type=\"bibr\" target=\"#b60\">[61,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>: a query matches a ke et=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b45\">46,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b62\">63,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" targe  x k can be images <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b60\">61,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, patches <ref type=\"bibr\" target=\"#b45\">[46]</ref>, or cont k can be identical <ref type=\"bibr\" target=\"#b28\">[29,</ref><ref type=\"bibr\" target=\"#b58\">59,</ref><ref type=\"bibr\" target=\"#b62\">63]</ref>, partially shared <ref type=\"bibr\" target=\"#b45\">[46,</ref> stance discrimination task in <ref type=\"bibr\" target=\"#b60\">[61]</ref>, to which some recent works <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> are related.</p><p>F tive pair if they originate from the same image, and otherwise as a negative sample pair. Following <ref type=\"bibr\" target=\"#b62\">[63,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, we take two random . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . . . , 0.003] (6)</p><p>Feature-space Maximum Likelihood Linear Regression (FMLLR) was explored in <ref type=\"bibr\" target=\"#b31\">[32]</ref>, <ref type=\"bibr\" target=\"#b32\">[33]</ref> for speaker ada. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n different ways of data filtering to improve pseudo-label quality, e.g. confidence-based filtering <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: et=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref>.</p><p>In this paper, we propose a novel Bayesian graph con et=\"#b21\">[23,</ref><ref type=\"bibr\" target=\"#b22\">24,</ref><ref type=\"bibr\" target=\"#b28\">30,</ref><ref type=\"bibr\" target=\"#b37\">39]</ref>.In this paper, we propose a novel Bayesian graph convolutio derived from noisy data. Addressing the uncertainty on the underlying graph was first considered in <ref type=\"bibr\" target=\"#b37\">[39]</ref> for the problem of node classification. In this work, the  xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Bayesian Graph Convolutional Networks</head><p>In <ref type=\"bibr\" target=\"#b37\">[39]</ref>, to alleviate the effect of the potential noise in the obs G \ud835\udc5c\ud835\udc4f\ud835\udc60 ) \ud835\udc5d (G|D, G \ud835\udc5c\ud835\udc4f\ud835\udc60 ) \ud835\udc51 G \ud835\udc51\ud835\udf40 .<label>(1)</label></formula><p>Zhang et al. presented this model in <ref type=\"bibr\" target=\"#b37\">[39]</ref> for the node classification task. In their case, the value. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: roposed solutions for different aspects of the problem <ref type=\"bibr\" target=\"#b119\">[120]</ref>, <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref>. In particu arget=\"#b121\">[122]</ref>. In particular, sampling results have been generalized to directed graphs <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref> and to othe ng set selection from an experiment design perspective <ref type=\"bibr\" target=\"#b123\">[124]</ref>, <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref> setting as  ge-scale graphs. Some techniques require computing and storing the first K basis vectors of the GFT <ref type=\"bibr\" target=\"#b120\">[121]</ref>. For larger graph sizes, where this may not be practical  always lead to performance comparable to those of more complex greedy optimization methods such as <ref type=\"bibr\" target=\"#b120\">[121]</ref>, <ref type=\"bibr\" target=\"#b121\">[122]</ref>.</p><p>Give. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b19\">Huang et al., 2015;</ref><ref type=\"bibr\" target=\"#b7\">Chiu and Nichols, 2016;</ref><ref type=\"bibr\" target=\"#b22\">Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b46\">Yang et al., model we use to perform NER. We will first describe the basic hierarchical neural CRF tagging model <ref type=\"bibr\" target=\"#b22\">(Lample et al., 2016;</ref><ref type=\"bibr\" target=\"#b27\">Ma and Hovy  labels and performs inference.</p><p>In this paper, we closely follow the architecture proposed by <ref type=\"bibr\" target=\"#b22\">Lample et al. (2016)</ref>, and use bidirectional LSTMs for both the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  users' next actions (click on an item) based on the sequence of the actions in the current session <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref>.Recent studies have  f which represent the state-of-the-art in SRS research field <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target= ,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Hidasi et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> use deep recurrent neural networks with a gated recurrent un om the CIKM Cup 2016 2 , for which only the transaction data is used in this study.</p><p>Following <ref type=\"bibr\" target=\"#b4\">[5]</ref> and <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we filter out . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: get=\"#b3\">4]</ref>, financial systems <ref type=\"bibr\" target=\"#b31\">[32]</ref>, medical treatments <ref type=\"bibr\" target=\"#b30\">[31]</ref>, information security <ref type=\"bibr\" target=\"#b11\">[12,<. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[28]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref>, have made sign o existing dimensions of depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>, width 2 , and cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref>. We state in Sec. 4.4 that increasing scale is more effect rformance of state-of-the-art CNNs, e.g., ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref>.</p><p> \">[28]</ref>, <ref type=\"bibr\" target=\"#b46\">[47]</ref>, <ref type=\"bibr\" target=\"#b50\">[51]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref>, <ref type=\"bibr\" target=\"#b59\">[60]</ref>, achieving stat modern backbone CNNs architectures, e.g., ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref>. Instea odules have been proposed in recent years, including cardinality dimension introduced by Xie et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, as well as squeeze and excitation (SE) block presented by nts. As shown in Fig. <ref type=\"figure\">3</ref>, we can easily integrate the cardinality dimension <ref type=\"bibr\" target=\"#b55\">[56]</ref> and the SE block <ref type=\"bibr\" target=\"#b24\">[25]</ref> sion cardinality.</head><p>The dimension cardinality indicates the number of groups within a filter <ref type=\"bibr\" target=\"#b55\">[56]</ref>. This dimension changes filters from single-branch to mult ig. <ref type=\"figure\">3</ref>: The Res2Net module can be integrated with the dimension cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> (replace conv with group conv) and SE <ref type=\"bibr\" tar  into the state-ofthe-art models, such as ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref> and Big-Lit and bLRes2Net-50, respectively.</p><p>The proposed scale dimension is orthogonal to the cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref> dimension and width <ref type=\"bibr\" target=\"#b22\">[23]</r 4]</ref> dataset, we mainly use the ResNet-50 <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt-50 <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA-60 <ref type=\"bibr\" target=\"#b59\">[60]</ref>, and bLR ments on the CIFAR <ref type=\"bibr\" target=\"#b26\">[27]</ref> dataset, we use the ResNeXt-29, 8c\u00d764w <ref type=\"bibr\" target=\"#b55\">[56]</ref> as our baseline model. Empirical evaluations and discussio ons, we use the Pytorch implementation of ResNet <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, DLA <ref type=\"bibr\" target=\"#b59\">[60]</ref> as well as  type=\"bibr\" target=\"#b22\">[23]</ref>. On the CIFAR dataset, we use the implementation of ResNeXt-29 <ref type=\"bibr\" target=\"#b55\">[56]</ref>. For all tasks, we use the original implementations of bas ement of 0.73% in terms of top-1 error over the  <ref type=\"bibr\" target=\"#b22\">[23]</ref>, ResNeXt <ref type=\"bibr\" target=\"#b55\">[56]</ref>, SE-Net <ref type=\"bibr\" target=\"#b24\">[25]</ref>, bLResNe ve been shown to have stronger representation capability <ref type=\"bibr\" target=\"#b22\">[23]</ref>, <ref type=\"bibr\" target=\"#b55\">[56]</ref> for vision tasks. To validate our model with greater depth  which contains 50k training images and 10k testing images from 100 classes. The ResNeXt-29, 8c\u00d764w <ref type=\"bibr\" target=\"#b55\">[56]</ref> is used as the baseline model. We only replace the origina iv xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Scale Variation</head><p>Similar to Xie et al. <ref type=\"bibr\" target=\"#b55\">[56]</ref>, we evaluate the test performance of the baseline model by ng different CNN dimensions, including scale (Equation ( <ref type=\"formula\">1</ref>)), cardinality <ref type=\"bibr\" target=\"#b55\">[56]</ref>, and depth <ref type=\"bibr\" target=\"#b46\">[47]</ref>. Whil fix all other dimensions. A series of networks are trained and evaluated under these changes. Since <ref type=\"bibr\" target=\"#b55\">[56]</ref> has already shown that increasing cardinality is more effe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: models can improve model predictions on all tasks by utilizing regularization and transfer learning <ref type=\"bibr\" target=\"#b7\">[8]</ref>. However, in practice, multi-task learning models do not alw <p>The backbone of MMoE is built upon the most commonly used Shared-Bottom multi-task DNN structure <ref type=\"bibr\" target=\"#b7\">[8]</ref>. The Shared-Bottom model structure is shown in Figure <ref t igure <ref type=\"figure\" target=\"#fig_0\">1</ref> (a), which is a framework proposed by Rich Caruana <ref type=\"bibr\" target=\"#b7\">[8]</ref> and widely adopted in many multi-task learning applications  sks.</p><p>Prior works <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> investigated task di erences in multi-task learning by assumi target=\"#b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>.</p><p>Instead of sharing hidden layers and same model parame lt in both improved e ciency and model quality for each task <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>. One of the widely us \" target=\"#b29\">30]</ref>. One of the widely used multi-task learning models is proposed by Caruana <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, which has a shared-bo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >4]</ref>. In the context of Automatic Speech Recognition, CNNs are usually combined with HMMs/GMMs <ref type=\"bibr\" target=\"#b5\">[5,</ref><ref type=\"bibr\" target=\"#b6\">6]</ref>, like regular Deep Neu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: he entire training set).</p><p>To further verify the planning diversity, we also computed self-BLEU <ref type=\"bibr\" target=\"#b46\">(Zhu et al., 2018)</ref> to evaluate how different planning results (. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b4\">5]</ref> , Condition Random Field (CRF) <ref type=\"bibr\" target=\"#b5\">[6]</ref> , Decision Tree <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> , Support Vector Machi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ch only scales to networks with a few hundred parameters. In <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b1\">[2]</ref> [3] <ref type=\"bibr\" target=\"#b3\">[4]</ref> and neural machine translation <ref type=\"bibr\" target=\"#b4\">[5]</ref> <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Due to lack o t recent tide of adopting sub-word representations is largely driven by neural machine translation. <ref type=\"bibr\" target=\"#b4\">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type=\"bibr\" ta e also compare our systems with BPE baselines. The BPE procedure follows the algorithm described in <ref type=\"bibr\" target=\"#b4\">[5]</ref>. All the PASM segmentation schemes are trained using the lex. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend our prior work <ref type=\"bibr\" target=\"#b0\">[1]</ref>, and propose a hybrid Transformer-LSTM based architecture. T ot only require external language models but also lead to a slow inference. To tackle this problem, <ref type=\"bibr\" target=\"#b0\">[1]</ref> has proposed long short term memory (LSTM)-based encoderdeco <p>In this work, we propose a hybrid Transformer-LSTM architecture which combines the advantages of <ref type=\"bibr\" target=\"#b0\">[1]</ref> and <ref type=\"bibr\" target=\"#b5\">[6]</ref>. It not only has l.</p><p>The paper is organized as follows. Section 2 describes baseline architectures mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref> and <ref type=\"bibr\" target=\"#b5\">[6]</ref>. Then, the propo res 2.1. LSTM-based encoder-decoder architecture</head><p>A LSTM-based encoder-decoder architecture <ref type=\"bibr\" target=\"#b0\">[1]</ref>, denoted as A1 in the rest of this paper, consists of a Bidi ayers respectively. Figure <ref type=\"figure\">1</ref>: LSTM-based encoder-decoder architecture (A1) <ref type=\"bibr\" target=\"#b0\">[1]</ref>, where the decoder acts as an independent language model.</p ords, the LSTM acts as an independent language model that can be easily updated with text-only data <ref type=\"bibr\" target=\"#b0\">[1]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= tune the transferred model. This avoids a so-called catastrophic forgetting problem as mentioned in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Specifically, at each training iteration, we mix a batch of cond step, the model is further fine-tuned with the labeled data of the target language. Similar to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we empirically found that the second step is necessary to i. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b14\">15]</ref>, large Transformer based models <ref type=\"bibr\" target=\"#b31\">[32]</ref>, such as BERT <ref type=\"bibr\" target=\"#b5\">[6]</ref>, sho il: markus.zlabinger@tuwien.ac.at 3 TU Wien, Austria, email: hanbury@ifs.tuwien.ac.at former layers <ref type=\"bibr\" target=\"#b31\">[32]</ref> (we evaluate up to three) can effectively contextualize qu  learning -a local contextualization, fixed by the n-gram size hyperparameter.</p><p>Vaswani et al. <ref type=\"bibr\" target=\"#b31\">[32]</ref> proposed the Transformer architecture in the context of la intensity of the contextualization. We calculate the context(t1:n) with a set of Transformer layers <ref type=\"bibr\" target=\"#b31\">[32]</ref>. First, the input sequence is fused with a positional enco. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: bibr\" target=\"#b43\">(Xiong et al., 2016;</ref><ref type=\"bibr\" target=\"#b31\">Seo et al., 2016;</ref><ref type=\"bibr\" target=\"#b2\">Bahdanau et al., 2015)</ref>. The resulting representation is encoded  l language.</p><p>In this work, we consider attention-based neural machine translation (NMT) models <ref type=\"bibr\" target=\"#b2\">Bahdanau et al. (2015)</ref>; <ref type=\"bibr\" target=\"#b24\">Luong et . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ]</ref> . Kernel function-based FCM defined the dissimilarity between pixels by RBF kernel function <ref type=\"bibr\" target=\"#b26\">[27]</ref> . Riemannian manifold space-based Manifold Projection (MP). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 6\">(Yao et al. 2015)</ref> and extracting the last hidden state of recurrent visual feature encoder <ref type=\"bibr\" target=\"#b20\">(Venugopalan et al. 2015)</ref>.</p><p>Those feature encoding methods Our basic video caption framework is extended from S2VT (sequence to sequence: video to text) model <ref type=\"bibr\" target=\"#b20\">(Venugopalan et al. 2015)</ref> and M 3 (multimodal memory modeling) . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . (2015)</ref> apply a character-level CNN for text classification and achieve competitive results. <ref type=\"bibr\" target=\"#b22\">Socher et al. (2013)</ref>   explore the structure of a sentence and . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: > <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>I. INTRODUCTION</head><p>Neural recommender models <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b8\">[9]</ref> have achieved bette. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ]</ref>, they still belong to the family of linear models and are claimed to be di cult to estimate <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Moreover, they are known to have only marginal improvemen ag recommendation. With one hidden layer only, our NFM signi cantly outperforms FM (the ocial LibFM <ref type=\"bibr\" target=\"#b27\">[28]</ref> implementation) with a 7.3% improvement. Compared to the s te) linear models. In other words, the predicted target \u02c6 (x) is linear w.r.t. each model parameter <ref type=\"bibr\" target=\"#b27\">[28]</ref>. Formally, for each model parameter \u03b8 \u2208 {w 0 , {w i }, { i itive embedding-based models that are speci cally designed for sparse data prediction:</p><p>-LibFM <ref type=\"bibr\" target=\"#b27\">[28]</ref>. is is the o cial implementation <ref type=\"foot\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 1.0\"><head>Functional Unit Count Latency</head><p>Simple The processor has a lookup-free data cache <ref type=\"bibr\" target=\"#b6\">[7]</ref> that allows up to 8 pending misses to different cache lines.. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f><ref type=\"bibr\">2017;</ref><ref type=\"bibr\">2019;</ref><ref type=\"bibr\" target=\"#b22\">2018;</ref><ref type=\"bibr\" target=\"#b7\">Gao et al., 2018;</ref><ref type=\"bibr\" target=\"#b21\">Nijkamp et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  attack-agnostic manner, except a few touches on denoising <ref type=\"bibr\" target=\"#b24\">[25,</ref><ref type=\"bibr\" target=\"#b28\">29]</ref> and obfuscating gradients <ref type=\"bibr\" target=\"#b10\">[1 le FPD can circumvent the structure-replaced white-box attack. Our proposal is partially related to <ref type=\"bibr\" target=\"#b28\">[29]</ref>, as the denoising layers in our FPD are inspired by their  ng layers in our FPD are inspired by their feature denoising approach. Nevertheless, different from <ref type=\"bibr\" target=\"#b28\">[29]</ref>, the principle behind our FPD is to improve the intrinsic  antic information. We will compare the performance between FPD-enhanced CNN and the CNN enhanced by <ref type=\"bibr\" target=\"#b28\">[29]</ref> in Section 4.1.</p></div> <div xmlns=\"http://www.tei-c.org  the Gaussian filtering operator, the dot product operator helps improve the adversarial robustness <ref type=\"bibr\" target=\"#b28\">[29]</ref>. Meanwhile, as the dot product operator does not involve e  framework structure through the exploration study. Moreover, we compare with the most related work <ref type=\"bibr\" target=\"#b28\">[29]</ref> as well. In the comparison experiments, we focus on compar a><p>Comparison with the Related Work As mentioned in Section 2, the denoising approach proposed in <ref type=\"bibr\" target=\"#b28\">[29]</ref> is similar to our denoising layers in FPD. Therefore, we c /ref> is similar to our denoising layers in FPD. Therefore, we conduct a comparison experiment with <ref type=\"bibr\" target=\"#b28\">[29]</ref> as well. In Table <ref type=\"table\" target=\"#tab_5\">1</ref /ref> as well. In Table <ref type=\"table\" target=\"#tab_5\">1</ref>, X represents the enhanced CNN by <ref type=\"bibr\" target=\"#b28\">[29]</ref>. We observe that our F 2I\u2212Mid outperforms X . Especially, . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: atasets, CNNs have achieved state-of-the-art results in SISR <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b14\">14,</ref><ref type=\"bibr\" tar ainly focus on designing a deeper or wider network structure <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b13\">13,</ref><ref type=\"bibr\" tar tion. Some loss functions have been widely used, such as L 2 <ref type=\"bibr\" target=\"#b2\">[2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b29\">29,</ref><ref type=\"bibr\" tar (SRCNN) for image SR, which achieves impressive performance. Later, Kim et al. designed deeper VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[13]</ref> with mo e-of-the-art CNN-based SR methods: SR-CNN [1], FSRCNN <ref type=\"bibr\" target=\"#b3\">[3]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref>, LapSRN <ref type=\"bibr\" target=\"#b14\">[14]</ref>, Mem-Net SRCNN <ref type=\"bibr\" target=\"#b2\">[2]</ref>, FSRCNN <ref type=\"bibr\" target=\"#b3\">[3]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[12]</ref>, IR-CNN <ref type=\"bibr\" target=\"#b35\">[35]</ref>, SRMD <r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: processing step on the magnitudes produced by deep learning based speech enhancement and separation <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ning and more specifically in sequenceto-sequence modeling have led to dramatic improvements in ASR <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> and MT <ref type=\"bibr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: dents from enrollment sequences to be accurate, particularly after a student's second year of study <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>The use of an RNN to predict student course grades . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e negative scope of LReLU is set as 0.05. We initialize the weights by using the method proposed in <ref type=\"bibr\" target=\"#b7\">[8]</ref> and the biases are set to zero. The proposed network is opti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM) <ref type=\"bibr\" target=\"#b29\">[30]</ref>, are performed to maintain a manageable balance between fo ves, focusing all attention on the hard negative examples.</p><p>Online Hard Example Mining (OHEM): <ref type=\"bibr\" target=\"#b29\">[30]</ref> proposed to improve training of two-stage detectors by con  hard example mining <ref type=\"bibr\" target=\"#b35\">[36,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30]</ref>.</p><p>In this paper, we propose a new loss function that a rous extensions to this framework have been proposed, e.g. <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar rget=\"#b31\">[32,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target=\"#b29\">30,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> that samples hard e nt performance saturates. (d) FL outperforms the best variants of online hard example mining (OHEM) <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b20\">21]</ref> by over 3 points A. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion. The leftmost bank on Figure <ref type=\"figure\" target=\"#fig_0\">1</ref> is a bimodal predictor <ref type=\"bibr\" target=\"#b4\">[5]</ref>. We refer to this bank as bank 0. It has 4k entries, and is  of groups of consecutive history bits, then it is XORed with the branch PC as in a gshare predictor <ref type=\"bibr\" target=\"#b4\">[5]</ref>. For example, bank 3 is indexed with 40 history bits, and th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: neural networks, they are MetaN <ref type=\"bibr\" target=\"#b11\">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type=\"bibr\" target=\"#b4\">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type=\"bibr\" target=\"#b10\">(. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cantly improved image classification <ref type=\"bibr\" target=\"#b13\">[14]</ref> and object detection <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref> accuracy. Compared t task that requires more complex methods to solve. Due to this complexity, current approaches (e.g., <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" targ n this paper, we streamline the training process for stateof-the-art ConvNet-based object detectors <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. We propose a single egressors, rather than training a softmax classifier, SVMs, and regressors in three separate stages <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref>. The components of t  very deep detection network (VGG16 <ref type=\"bibr\" target=\"#b19\">[20]</ref>) 9\u00d7 faster than R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref> and 3\u00d7 faster than SPPnet <ref type=\"bibr\" target=\"#b10\">[11 1.0\"><head n=\"1.1.\">R-CNN and SPPnet</head><p>The Region-based Convolutional Network method (R-CNN) <ref type=\"bibr\" target=\"#b8\">[9]</ref> achieves excellent object detection accuracy by using a deep  k h , for each of the K object classes, indexed by k. We use the parameterization for t k given in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, in which t k specifies a scale-invariant translation and lo eparates localization and classification. OverFeat <ref type=\"bibr\" target=\"#b18\">[19]</ref>, R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and SPPnet <ref type=\"bibr\" target=\"#b10\">[11]</ref> also t tions of the dataset). We use mini-batches of size R = 128, sampling 64 RoIs from each image. As in <ref type=\"bibr\" target=\"#b8\">[9]</ref>, we take 25% of the RoIs from object proposals that have int rm non-maximum suppression independently for each class using the algorithm and settings from R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= he first is the CaffeNet (essentially AlexNet <ref type=\"bibr\" target=\"#b13\">[14]</ref>) from R-CNN <ref type=\"bibr\" target=\"#b8\">[9]</ref>. We alternatively refer to this CaffeNet as model S, for \"sm. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tput mappings, potentially stochastic, with learnable parameters using directed acyclic graphs (see <ref type=\"bibr\" target=\"#b38\">Schulman et al. (2015)</ref> for a review). The state of each non-inp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: b9\">(Gao et al., 2005)</ref>, where segmentation information has been used as soft features for NER <ref type=\"bibr\" target=\"#b53\">(Zhao and Kit, 2008;</ref><ref type=\"bibr\" target=\"#b30\">Peng and Dre using segmentation as soft features for character-based NER models can lead to improved performance <ref type=\"bibr\" target=\"#b53\">(Zhao and Kit, 2008;</ref><ref type=\"bibr\" target=\"#b31\">Peng and Dre. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g/ns/1.0\"><head n=\"4.2.2\">Encoding Layer</head><p>Following we apply a convolutional neural network <ref type=\"bibr\" target=\"#b22\">Zeng et al. (2014)</ref> as encoding layer to get the hidden annotati. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing process, which leads to better generalization. We develop an approach based on label smoothness <ref type=\"bibr\" target=\"#b34\">[35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>, which assumes tha therefore learnable <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref>. Inspired by these methods, we design a module of label smo get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Therefore, more re r the unlabeled nodes l * u (E\\V). To solve the issue, we propose minimizing the leave-one-out loss <ref type=\"bibr\" target=\"#b34\">[35]</ref>. Suppose we hold out a single item v and treat it unlabele. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: no extra</head><p>queueing is introduced in the server. When combined with real-time I/O subsystems <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the Leader/Followers thread pool implementation can reduc /ref> operation scheduling, event processing <ref type=\"bibr\" target=\"#b6\">[7]</ref>, I/O subsystem <ref type=\"bibr\" target=\"#b11\">[12]</ref> and pluggable protocol <ref type=\"bibr\" target=\"#b13\">[14]. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: arget=\"#b30\">(Ratinov and Roth, 2009;</ref><ref type=\"bibr\" target=\"#b25\">Passos et al., 2014;</ref><ref type=\"bibr\" target=\"#b21\">Luo et al., 2015)</ref>. However, <ref type=\"bibr\" target=\"#b8\">Collo earlier version of the corpus with a different data split. 21 Numbers taken from the original paper <ref type=\"bibr\" target=\"#b21\">(Luo et al., 2015)</ref>. While the precision, recall, and F1 scores  ataset, but they did not evaluate on the CoNLL-2003 dataset due to lack of coreference annotations. <ref type=\"bibr\" target=\"#b21\">Luo et al. (2015)</ref> achieved state of the art results on CoNLL-20. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: models by fine-tuning pre-trained LMs in a simpler architecture.</p><p>Pre-trained LMs such as BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref> and GPT-2 <ref type=\"bibr\" target=\"#b40\">[41]</ref> have d task. See Appendix A for the model architecture. In D , we fine-tune the popular 12layer BERT model <ref type=\"bibr\" target=\"#b12\">[13]</ref>, RoBERTa <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and a  currently support 4 pre-trained models: Distil-BERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, RoBERTa <ref type=\"bibr\" target=\"#b28\">[29]</ref>, and XL Figure <ref type=\"figure\">6</ref> shows the model architecture of D 's language models such as BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DistilBERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, and  best performing pre-trained model among DistilBERT <ref type=\"bibr\" target=\"#b44\">[45]</ref>, BERT <ref type=\"bibr\" target=\"#b12\">[13]</ref>, XLNet <ref type=\"bibr\" target=\"#b60\">[61]</ref>, and RoBE. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: emic paper analysis.</p><p>The pipeline for creating S2ORC was used to construct the CORD-19 corpus <ref type=\"bibr\" target=\"#b2\">(Wang et al., 2020)</ref>, which saw fervent adoption as the canonical. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Deep Supervision</head><p>We propose to use deep supervision <ref type=\"bibr\" target=\"#b5\">[6]</ref> in UNet++, enabling the model to operate in two modes: (1) a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: one or very few examples challenges the standard fine-tuning method in deep learning. Early studies <ref type=\"bibr\" target=\"#b17\">(Salamon and Bello, 2017)</ref> applied data augmentation and regular. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: out the learning process would be beneficial to enhance the OOD discriminative power of the system. <ref type=\"bibr\" target=\"#b3\">Hendrycks et al. (2019)</ref> demonstrate that utilizing auxiliary dat ing higher likelihood estimates on unseen OOD samples. The ominous observation is presented also by <ref type=\"bibr\" target=\"#b3\">Hendrycks et al. (2019)</ref>, but they concentrate on improving the O r\" target=\"#b8\">Nalisnick et al., 2019a;</ref><ref type=\"bibr\" target=\"#b0\">Choi et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Hendrycks et al., 2019)</ref>. These works report that despite intuiti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: alistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27]  and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>.</p></div> \t\t\t</abstract> \t\t</profileDesc> \t</teiHeader> \t e instead of pixel space. Ledig et al. <ref type=\"bibr\" target=\"#b26\">[27]</ref> and Sajjadi et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> further propose adversarial loss to encourage the network  7]</ref> introduce an adversarial loss, generating images with more natural details. Sajjadi et al. <ref type=\"bibr\" target=\"#b37\">[38]</ref> develop a similar approach and further explore the local t pe=\"figure\">5</ref>. GAN-based methods (SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref>, EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref> and ours) clearly outperform PSNR-oriented approaches in t ef>, and GAN-based methods, such as SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and En-hanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. More results are provided in the supplementary material.  -GAN and the counterpart generated by SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> or EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. The users were asked to pick the image with more natural  on, our method is ranked higher than SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>, especially in building, animal, and grass categories. Com r studies, comparing our method with SRGAN <ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet <ref type=\"bibr\" target=\"#b37\">[38]</ref>. Second row: our methods produce visual results that are r ser studies, comparing our method with SRGAN<ref type=\"bibr\" target=\"#b26\">[27]</ref> and EnhanceNet<ref type=\"bibr\" target=\"#b37\">[38]</ref>. Second row: our methods produce visual results that are r ref type=\"bibr\" target=\"#b2\">3]</ref> and adversarial loss <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> are introduced to solve the regression-to-the-mean problem  ur framework is based on adversarial learning, inspired by <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>. Specifically, it consists of one generator G \u03b8 and one dis d n=\"3.3.\">Loss Function</head><p>We draw inspiration from <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref> and apply perceptual loss and adversarial loss in our model leasing textures, outperforming previous GAN-based methods <ref type=\"bibr\" target=\"#b26\">[27,</ref><ref type=\"bibr\" target=\"#b37\">38]</ref>.</p><p>Our work currently focuses on SR of outdoor scenes. . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lues by 255 so they are in the [0, 1] range.</p><p>ImageNet. The ILSVRC 2012 classification dataset <ref type=\"bibr\" target=\"#b1\">[2]</ref> consists 1.2 million images for training, and 50,000 for val. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m for the chest database creation or expansion, performing NER training and modeling using NeuroNER <ref type=\"bibr\" target=\"#b9\">[12]</ref> and then generating the current or the new model, and hence. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: etection architectures have also become popular, mostly due to their computational efficiency. YOLO <ref type=\"bibr\" target=\"#b28\">[29]</ref> outputs very sparse detection results and enables real tim. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b6\">[7]</ref> describes a binary search that copes with variable length entries.</p><p>Work <ref type=\"bibr\" target=\"#b8\">[9]</ref> describes a split technique for data. Rows are assigned tag  t inheriting some of the disadvantages of digital search trees and key compression techniques. Work <ref type=\"bibr\" target=\"#b8\">[9]</ref> describes an efficient columnar storage in B-trees. Column-o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: es with few contributions in a prediction. Inspired by the theory of hierarchical abstract machines <ref type=\"bibr\" target=\"#b15\">(Parr and Russell 1998)</ref>, we cast the task of profile reviser as. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing  pe=\"bibr\" target=\"#b26\">28]</ref> on ImageNet <ref type=\"bibr\" target=\"#b19\">[21]</ref>, Kim et al. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose two very d /head></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1.\">Datasets</head><p>By following <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b21\">23]</ref>, we use a training mmarizes quantitative results on the four testing sets, by citing the results of prior methods from <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. The two DRRN mode pth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods<ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing  meters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30   respectively. On the one hand, to accelerate the convergence speed of very deep networks, the VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> is trained with a very high learning rate (10 \u22121 , instead on focuses on three most related work to ours: ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Fig. < on. Denoting the input as x and the underlying mapping as H(x), the residual mapping is defined as  <ref type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c)   <ref type=\"table\">1</ref>. Strategies used in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and DRRN.  \">VDSR</head><p>Differing from ResNet that uses residual learning in every few stacked layers, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> introduces GRL, i.e., residual learning between the input  structure of DRRN is illustrated in Fig. <ref type=\"figure\" target=\"#fig_4\">5</ref>. Actually, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> can be viewed as a special case of DRRN, i.e., when U = 0, r that, for each original image, we have 7 additional augmented versions. Besides, inspired by VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, we also use scale augmentation to train our model, and im  epochs. Since a large learning rate is used in our work, we adopt the adjustable gradient clipping <ref type=\"bibr\" target=\"#b12\">[13]</ref> to boost the convergence rate while suppressing exploding   VDSR re-implementation also uses BN and ReLU as the activation functions, unlike the original VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> that does not use BN. These results are faithful since our ese results are faithful since our VDSR re-implementation achieves similar benchmark performance as <ref type=\"bibr\" target=\"#b12\">[13]</ref> reported in Tab. Qualitative comparisons among SRCNN <ref  RCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, SelfEx <ref type=\"bibr\" target=\"#b9\">[10]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRRN are illustrated in Fig. <ref type=\"figure\" target /1.0\"><head n=\"4.5.\">Discussions</head><p>Since global residual learning has been well discussed in <ref type=\"bibr\" target=\"#b12\">[13]</ref>, in this section, we mainly focus on local residual learni ucture. Local Residual Learning To demonstrate the effectiveness of LRL, DRRN is compared with VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, which has no LRL. For fair comparison, the depth and numb 5]</ref> and FSRCNN <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Very deep models (d \u2265 20) include VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, RED <ref  ameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30<r  ResNet<ref type=\"bibr\" target=\"#b7\">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c)  Ratio (PSNR) performance of several recent CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar me depth as VDSR and DRCN, but fewer parameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and non-DL <ref typ images are of the same size. For fair comparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ence model is an encoder-decoder architecture with attention <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Let X = [X 1 , . . . , P (y u | X, y &lt;u ) = h(S u , Q u ).<label>(4)</label></formula><p>The function g(\u2022) is a GRU RNN <ref type=\"bibr\" target=\"#b5\">[6]</ref> which encodes the previous token and query vector Q u\u22121 to p. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b6\">(Graves et al., 2014;</ref><ref type=\"bibr\" target=\"#b0\">Bahdanau et al., 2015;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015)</ref>. Our approach is data-driven, computat s of memory network in question answering <ref type=\"bibr\" target=\"#b35\">(Weston et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015)</ref>. We describe the background on memory  ple hops is that more abstractive evidences could be found based on previously extracted evidences. <ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al. (2015)</ref> demonstrate that multiple hops could u ion information in the attention model. The details are described below.</p><p>\u2022 Model 1. Following <ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al. (2015)</ref>, we calculate the memory vector m i wi gure\" target=\"#fig_0\">1</ref>, which is inspired by the use of memory network in question answering <ref type=\"bibr\" target=\"#b27\">(Sukhbaatar et al., 2015)</ref>. Our approach consists of multiple co r\" target=\"#b6\">(Graves et al., 2014;</ref><ref type=\"bibr\" target=\"#b35\">Weston et al., 2014;</ref><ref type=\"bibr\" target=\"#b27\">Sukhbaatar et al., 2015;</ref><ref type=\"bibr\" target=\"#b0\">Bahdanau . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: get=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. In this problem, the goal is to reconstruct a latent high-re , e.g. methods that are based on hand-crafted prior models such as sparsity or non-local similarity <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: d><p>Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes <ref type=\"bibr\" target=\"#b14\">[15]</ref>. The network predicts 4 coordinates for each bounding box, ocation of filter application using a sigmoid function. This figure blatantly self-plagiarized from <ref type=\"bibr\" target=\"#b14\">[15]</ref>.</p><p>is not the best but does overlap a ground truth obj location of filter application using a sigmoid function. This figure blatantly self-plagiarized from<ref type=\"bibr\" target=\"#b14\">[15]</ref>.</figDesc></figure> <figure xmlns=\"http://www.tei-c.org/ns. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  shift is a typical clustering algorithm which shifts the mean of the cluster to its center of mass <ref type=\"bibr\" target=\"#b5\">[6]</ref> . Recently, Yamasaki and Tanaka have studied the properties . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: olves these problems using the following techniques:</p><p>\u2022 Reversible layers, first introduced in <ref type=\"bibr\" target=\"#b7\">Gomez et al. (2017)</ref>, enable storing only a single copy of activa  term: the b \u2022 n h \u2022 l \u2022 d k ,</formula><p>RevNets. Reversible residual networks were introduced by <ref type=\"bibr\" target=\"#b7\">Gomez et al. (2017)</ref> where it was shown that they can replace Res mer model's self-attention mechanism <ref type=\"bibr\" target=\"#b21\">(Sukhbaatar et al., 2019a;</ref><ref type=\"bibr\" target=\"#b7\">b)</ref> have also recently been explored.</p><p>In particular, levera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: dels learn domain-invariant feature only <ref type=\"bibr\" target=\"#b7\">(Fernando et al., 2015;</ref><ref type=\"bibr\" target=\"#b22\">Long et al., 2014;</ref><ref type=\"bibr\" target=\"#b26\">Ming Harry Hsu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g agent is trained on the chemistry aware graph environment and learns to generate molecular graph. <ref type=\"bibr\" target=\"#b13\">[14]</ref> is another work which defines chemical molecular reaction . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ormance TCP stacks <ref type=\"bibr\" target=\"#b28\">[30,</ref><ref type=\"bibr\" target=\"#b59\">61,</ref><ref type=\"bibr\" target=\"#b66\">68]</ref> bypass the socket buffer to avoid extra memory copying. How. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: arget=\"#b1\">[2]</ref> based controller to update the memory, while Working Memory Network (W-MemNN) <ref type=\"bibr\" target=\"#b16\">[17]</ref> uses a multi-head attention <ref type=\"bibr\" target=\"#b25\"  <ref type=\"bibr\" target=\"#b25\">[26]</ref>, which is similar to that used in Working Memory Network <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Multi-head attention allows the model to jointly attend t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ral language generation tasks, such as style transfer, paraphrasing, and sentence error correction. <ref type=\"bibr\" target=\"#b14\">Li et al. (2018)</ref> proposed edit-based style transfer without par. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rn requires CFD's ISA, software, and hardware support.</p><p>Decoupled access/execute architectures <ref type=\"bibr\" target=\"#b29\">[30,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref> are alternative impl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lution streams. This paper represents a very substantial extension of our previous conference paper <ref type=\"bibr\" target=\"#b105\">[105]</ref> with an additional material added from our unpublished t t object detection and instance segmentation frameworks. The main technical novelties compared with <ref type=\"bibr\" target=\"#b105\">[105]</ref> lie in threefold. <ref type=\"bibr\" target=\"#b0\">(1)</ref efold. <ref type=\"bibr\" target=\"#b0\">(1)</ref> We extend the network (named as HRNetV1) proposed in <ref type=\"bibr\" target=\"#b105\">[105]</ref>, to two versions: HRNetV2 and HRNetV2p, which explore al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  and it's detections are publicly available.</p><p>We use the methodology and tools of Hoiem et al. <ref type=\"bibr\" target=\"#b18\">[19]</ref> For each category at test time we look at the top N predic. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pogation on just single hidden-layer feedforward neural networks. Recent attempts in this direction <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> propose efficient  r the search space through tunable parameters, in contrast to rigid search procedures in prior work <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Consequently, our of nodes. We contrast the performance of node2vec with state-of-the-art feature learning algorithms <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. We experiment wit odel, recent research established an analogy for networks by representing a network as a \"document\" <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. The same way as a r shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network <ref type=\"bibr\" target=\"#b23\">[24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. Our algorithm nod roceed by extending the Skip-gram architecture to networks <ref type=\"bibr\" target=\"#b20\">[21,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>. We seek to optimize the following objective function, whic  normalized Laplacian matrix of graph G as the feature vector representations for nodes. \u2022 DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>: This approach learns d-dimensional feature representation lude other matrix factorization approaches which have already been shown to be inferior to DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>. We also exclude a recent approach, GraRep <ref type=\"bibr the sampling procedure computationally efficient. We showed how random walks, also used in DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref>, allow the sampled nodes to be reused as neighborhoods for SION</head><p>Both DeepWalk and LINE can be seen as rigid search strategies over networks. DeepWalk <ref type=\"bibr\" target=\"#b23\">[24]</ref> proposes search using uniform random walks. The obvious li. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: While there is research work that has used court trial transcripts to identify deceptive statements <ref type=\"bibr\" target=\"#b13\">[14]</ref>, we are not aware of any previous work that took into cons cusing on real-life high-stake data. The work closest to ours is presented by Fornaciari and Poesio <ref type=\"bibr\" target=\"#b13\">[14]</ref>, which targets the identification of deception in statemen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in <ref type=\"bibr\" target=\"#b4\">[6]</ref>, Greedy <ref type=\"bibr\" target=\"#b5\">[7]</ref>, Infomap <ref type=\"bibr\" target=\"#b29\">[31]</ref> and Spinglass <ref type=\"bibr\" target=\"#b28\">[30]</ref>. A on-deterministic, and may yield different results on the same network). As can be seen, the Infomap <ref type=\"bibr\" target=\"#b29\">[31]</ref> algorithm seems to be the most difficult to fool.</p></div. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: consists of hardware and software components. Jenga hardware requires small changes over prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target  most of the heavy lifting to build virtual cache hierarchies. Specifically, Jenga builds on Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, which constructs sing systems with non-uniform SRAM banks, single-lookup NUCAs generally outperform directory-based NUCAs <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target ng just enough capacity to fit the working set at minimum latency and energy. In particular, Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> achieves this by letti rdware needs to be flexible and reconfigurable at low cost. We thus base Jenga's hardware on Jigsaw <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>, which supports applic  hardware components, emphasizing differences from Jigsaw at the end of the section. See prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref> for details of Jigsaw' g page reclassifications: Like Jigsaw and R-NUCA, Jenga uses a simple technique to map pages to VHs <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ vel hierarchy in a single curve, and thus can use the same partitioning algorithms as in prior work <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b55\">56]</ref> to allocate capacity ref type=\"bibr\" target=\"#b2\">[3]</ref>, R-NUCA <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Jigsaw <ref type=\"bibr\" target=\"#b6\">[7]</ref> do away with hierarchy entirely, adopting a single-lookup de type=\"bibr\" target=\"#b27\">28]</ref> and more aggressive than <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>). Table <ref type=\"ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ble, cannot be transcribed, or are otherwise hard to come by <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>.</p><p>The standard approach is to train a context dependent . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: rget=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b22\">23]</ref> or designing specific CNN-RNN networks <ref type=\"bibr\" target=\"#b4\">[5]</ref>. Such classifier that takes video sequences as input is beco erm Memory (LSTM) recurrent neural network is widely adopted <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b7\">8]</ref>. LSTM has memory abilit ion recognition using CNN or RNN structures in recent papers <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b18\">19]</ref>. Such deep networks r 18\">19]</ref>. Such deep networks reach top competitive results in the history of EmotiW challenges <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b26\">27]</ref>. Therefore we take s ork shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ><p>Many enhancements to these mechanisms have been proposed <ref type=\"bibr\" target=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" target=\"#b14\">15,</ref><ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  conditioned on the categorical priors. Both DBPN <ref type=\"bibr\" target=\"#b9\">[10]</ref> and DSRN <ref type=\"bibr\" target=\"#b8\">[9]</ref> made use of the mutual dependencies of low-and high-resoluti. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: <ref type=\"bibr\" target=\"#b20\">20,</ref><ref type=\"bibr\" target=\"#b39\">39]</ref>, perceptual losses <ref type=\"bibr\" target=\"#b11\">[11,</ref><ref type=\"bibr\" target=\"#b26\">26]</ref>. To verify the eff n-Schulz iteration to speed up the computation of covariance normalization. Specifically, from Equ. <ref type=\"bibr\" target=\"#b11\">(11)</ref>, the \u03a3 has square root as \u03a3 1/2 = Y = Udiag(\u03bb 1/2 i )U T .. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: t we will discuss.</p><p>With a randomization assumption of the ReLU activations similar to that in <ref type=\"bibr\" target=\"#b14\">(Kawaguchi, 2016;</ref><ref type=\"bibr\" target=\"#b2\">Choromanska et a. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  (h, r, t) and predict head or tail entities with scores of candidate entities. For example, TransE <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013)</ref> treats tail entities as translations of he gin, \u03c3 is the sigmoid function, and d r is the score function, for which we choose to follow TransE <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013)</ref> for its simplicity and efficiency,</p><for is trained on the training set and evaluated on the link Method MR MRR HITS@1 HITS@3 HITS@10 TransE <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013)</ref> 109370 0.253 0.170 0.311 0.392 DistMult <r 016)</ref>  prediction task.</p><p>We conduct 5 knowledge graph embedding models , including TransE <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013)</ref>, Dist-Mult <ref type=\"bibr\" target=\"#b45\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ule</head><p>This module is a bi-direction recurrent neural network with self-attention as shown in <ref type=\"bibr\" target=\"#b10\">Lin et al. (2017)</ref>. Given an input text x = (w 1 , w 2 , ..., w . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ariability in behavioural and environmental patterns have stymied predictive modeling of this kind. <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref> have tried to predict stress of student tp://www.tei-c.org/ns/1.0\"><head n=\"3.2.1.\">LOCATION FEATURE BASED MLP</head><p>In the work done by <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>, a Multilayer Perceptron (MLP) with 4 f ><head n=\"4.\">Result</head><p>Due to a heavy imbalance of class labels on a scale of 1-5, we follow <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>, converting the five stress label scale ses of 23 students, totaling to 1183 data points achieving roughly equal amount of training data in <ref type=\"bibr\" target=\"#b8\">(Mikelsons et al., 2018)</ref>. These 1183 data points have the follow. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lack-box security <ref type=\"bibr\" target=\"#b27\">(Tram\u00e8r et al., 2018)</ref>. We include one paper, <ref type=\"bibr\" target=\"#b14\">Ma et al. (2018)</ref>, that was not proposed as a defense per se, bu /head><p>LID is a general-purpose metric that measures the distance from an input to its neighbors. <ref type=\"bibr\" target=\"#b14\">Ma et al. (2018)</ref> propose using LID to characterize properties o  study made complete source code available <ref type=\"bibr\" target=\"#b15\">(Madry et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Ma et al., 2018;</ref><ref type=\"bibr\" target=\"#b9\">Guo et al., 2018;. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: the running time by implementing the sampling step on a static balanced binary tree as suggested in <ref type=\"bibr\" target=\"#b47\">[48]</ref>. The improved sampling step can run in time O(M + K ln(M ). Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nship between features, we introduce explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> to CCA. Finally, using the features that are projected to t use. In contrast, recent advances of explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref> can convert nonlinear problems to linear problems, which ca  computation complexity, one can use explicit feature maps <ref type=\"bibr\" target=\"#b19\">[12,</ref><ref type=\"bibr\" target=\"#b20\">13]</ref>. Let \u03c6(x) denote an explicit feature mapping such that K i  rnel. All other histogram-based features were mapped using the exact Bhattacharyya kernel map- ping <ref type=\"bibr\" target=\"#b20\">[13]</ref>. Finally, similar to <ref type=\"bibr\" target=\"#b16\">[9]</r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: =\"#b3\">[4]</ref> and Neural Network structure with Connectionist temporal classification (CTC) loss <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The hybrid hidden   by the acoustic model, and finally get better results. y * = arg max y log p(y|x) + \u03bb log P LM (y) <ref type=\"bibr\" target=\"#b4\">(5)</ref> where P LM (y) is provided by the LM, y * denotes the final . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ier, 2015)</ref>, contour <ref type=\"bibr\" target=\"#b3\">(Cheema et al., 2011)</ref> or optical flow <ref type=\"bibr\" target=\"#b5\">(Fathi and Mori, 2008)</ref>. Most proposed methods in this category o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: >43]</ref>. Hypergraph neural networks <ref type=\"bibr\" target=\"#b16\">[17]</ref> and their variants <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref> use the clique exp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ng, most notably the group of NLP models known as word2vec <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. A number of recent research publications have proposed wor learn the distributed representations of words in a corpus <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Inspired by it, DeepWalk <ref type=\"bibr\" target=\"#b21\">[2 lelized by using the same mechanism as word2vec and node2vec <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. All codes are implemented in C and C++ and our experiments e distributed representations of words in natural language <ref type=\"bibr\" target=\"#b16\">[17,</ref><ref type=\"bibr\" target=\"#b17\">18]</ref>. Building on word2vec, Perozzi et al. suggested that the \"c ghborhoods with network semantics for various types of nodes. Second, we extend the skip-gram model <ref type=\"bibr\" target=\"#b17\">[18]</ref> to facilitate the modeling of geographically and semantica p 2 &amp; p 3 ).</p><p>To achieve e cient optimization, Mikolov et al. introduced negative sampling <ref type=\"bibr\" target=\"#b17\">[18]</ref>, in which a relatively small set of words (nodes) are samp aximize the network probability in terms of local structures <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b21\">22]</ref>, that is:</p><formu d as a so max function <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b6\">7,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b23\">24]</ref>, that is:</p><formu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: b5\">Kario et al., 2003)</ref>, alterations of the brain causing differences in memory and cognition <ref type=\"bibr\" target=\"#b14\">(SJ et al., 2009)</ref>, suppression of the immune system <ref type=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Table <ref type=\"table\" target=\"#tab_0\">1</ref> lists all the methods evaluated with the split of <ref type=\"bibr\" target=\"#b100\">[101]</ref>. We select our model by tuning architecture and optimisa. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: sfluency candidates generated, the more helpful for training an error correction model. Inspired by <ref type=\"bibr\" target=\"#b22\">He et al. (2016)</ref> and <ref type=\"bibr\" target=\"#b61\">Zhang et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ad><p>We formalize our notion of spatial correlation similar to prior studies of spatial footprints <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. We define a spatial ed in hardware by correlating patterns with the code and/or data address that initiates the pattern <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. Whereas existing sp  tracking of spatial correlation. We show that the cache-coupled structures used in previous work ( <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>) are suboptimal for  dex consistently provides the most accurate predictions when correlation table storage is unbounded <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. By combining both q lications, PC+address indexing can be approximated by combining the PC with a spatial region offset <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref>. The spatial region  =\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2.\">Indexing</head><p>Prior studies of spatial predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> advocate predictor i  coverage and/or fragment prediction entries, consequently polluting the PHT.</p><p>Past predictors <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b16\">17]</ref> couple the predictor eas existing spatial pattern prefetching designs are effective for desktop/engineering applications <ref type=\"bibr\" target=\"#b3\">[4]</ref>, the only practical implementation evaluated on server workl e highest coverage.</p><p>For scientific applications, we corroborate the conclusions of prior work <ref type=\"bibr\" target=\"#b3\">[4]</ref> that indicate PC+offset indexing generally approaches the pe led sectored cache <ref type=\"bibr\" target=\"#b21\">[22]</ref>, whereas the spatial pattern predictor <ref type=\"bibr\" target=\"#b3\">[4]</ref> provided a logical sectored-cache tag array alongside a trad. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  the following metrics to evaluate the ranking results. Spearman's Rank Correlation Coefficient (\u03c1) <ref type=\"bibr\" target=\"#b38\">[39]</ref> and Kendall's Rank Correlation Coefficient (\u03c4 ) <ref type=. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rns of prompts that successfully extract knowledge from LMs. In open information extraction systems <ref type=\"bibr\" target=\"#b2\">(Banko et al., 2007)</ref>, manually defined patterns are often levera. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b18\">(Kukich, 1983;</ref><ref type=\"bibr\" target=\"#b5\">Dalianis and Hovy, 1993;</ref><ref type=\"bibr\" target=\"#b13\">Hovy, 1993)</ref> or automatically-learnt rules <ref type=\"bibr\" targ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ibr\" target=\"#b19\">Kaiser et al., 2017;</ref><ref type=\"bibr\" target=\"#b23\">Liu et al., 2017a;</ref><ref type=\"bibr\" target=\"#b39\">Sun et al., 2018)</ref>. <ref type=\"bibr\" target=\"#b28\">Mnih and Hint. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: mpled softmax <ref type=\"bibr\" target=\"#b4\">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type=\"bibr\" target=\"#b15\">(Titsias, 2017)</ref>. However, even with these approximations, discr. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ranslation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequence model <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref>. We examine some of the critical paramet \"6.\">Conclusion</head><p>We presented a broad range of basic experiments with the Transformer model <ref type=\"bibr\" target=\"#b22\">(Vaswani et al., 2017)</ref> for English-to-Czech neural machine tran training steps is given but no indication on \"how much converged\" the model was at that point, e.g. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref>. Most probably, the training was run unti r faking the global_step stored in the checkpoint) to make sure the learning rate is not too small. <ref type=\"bibr\" target=\"#b22\">Vaswani et al. (2017)</ref> suggest to average the last 20 checkpoint. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f>, and variants of Craig, Landin, and Hagersten (CLH) locks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b11\">12]</ref>. Historically, CLH lo. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: and hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref>. However, these approaches rely on manual feature engineeri (3) Hybrid methods <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b23\">24,</ref><ref type=\"bibr\" target=\"#b27\">28]</ref> combine the above two categories and learn user/item embedd ecommender systems <ref type=\"bibr\" target=\"#b13\">[14,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b27\">28,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" tar ized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a trainable and personalized relation scoring func  where GNNs can be used directly, while here we investigate GNNs for heterogeneous KGs. Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref> use GCNs in KGs for recommendation, but simply applying GC a user-personalized weighted graph, which characterizes user's preferences. To this end, similar to <ref type=\"bibr\" target=\"#b27\">[28]</ref>, we use a user-specific relation scoring function s u (r ) ear that the performance of KGNN-LS with a non-zero \u03bb is better than \u03bb = 0 (the case of Wang et al. <ref type=\"bibr\" target=\"#b27\">[28]</ref>), which justifies our claim that LS regularization can ass. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: \" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b4\">5,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>. Hidasi et al. <ref type=\"bibr\" target=\"#b4\">[5]</ref> use d each item a fixed weight based on the relative distance with response to the target item. Li et al. <ref type=\"bibr\" target=\"#b9\">[10]</ref> propose an RNN based encoder-decoder model (NARM), which ta transaction data is used in this study.</p><p>Following <ref type=\"bibr\" target=\"#b4\">[5]</ref> and <ref type=\"bibr\" target=\"#b9\">[10]</ref>, we filter out sessions of length 1 and items that appear l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >[19]</ref> as a way to study brain function. We consider the simplest of many types of perceptrons <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a single-layer perceptron consisting of one artificial neur. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: inimal group miss ratio) and required the assumption that the individual miss ratio curve be convex <ref type=\"bibr\" target=\"#b8\">[9]</ref>. Our algorithm uses dynamic programming to examine the entir he cache between instructions and data, and for multiprogramming by giving each process a partition <ref type=\"bibr\" target=\"#b8\">[9]</ref>.</p><p>In the way of optimizing their algorithm, they proved ss-rate derivative. The allocation is optimal if the miss-rate derivatives are as equal as possible <ref type=\"bibr\" target=\"#b8\">[9]</ref>. The optimality depends on several assumptions. One is that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ut the sequence at every step in the input. For brevity, the details of LSTM equations are given in <ref type=\"bibr\" target=\"#b2\">[Gers et al., 1999]</ref>. The conditional random field (CRF) <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: kloads can be classified into two categories: services and data analytics workloads as mentioned in <ref type=\"bibr\" target=\"#b43\">[44]</ref> and <ref type=\"bibr\" target=\"#b12\">[13]</ref>. For the dat. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: wer-law nonlinearity with a power coefficient between 1 15 <ref type=\"bibr\" target=\"#b43\">[44,</ref><ref type=\"bibr\" target=\"#b44\">45,</ref><ref type=\"bibr\" target=\"#b45\">46</ref>] and 1  10 [47], we . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: f>, reveal that the network depth is of crucial importance.</p><p>As the early attempt, Jain et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposed a simple CNN to recover a clean natural image fro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -Normalization (BN) layer in the deep neural network is customary to accelerate the training of DNN <ref type=\"bibr\" target=\"#b10\">[11]</ref>, by normalizing the hidden features that forwarded along t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: g for re-id on four widely referred benchmark datasets for person re-identification.</p><p>1. VIPeR <ref type=\"bibr\" target=\"#b3\">[4]</ref> The VIPeR dataset contains 1,264 images of 632 persons from . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  of localized spectral filters on graphs <ref type=\"bibr\" target=\"#b11\">(Hammond et al., 2011;</ref><ref type=\"bibr\" target=\"#b5\">Defferrard et al., 2016)</ref>.</p></div> <div xmlns=\"http://www.tei-c er neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. <ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> use this K-localized convolution to def pectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by<ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> with fast localized convolutions. In co  introduced to the original frameworks of<ref type=\"bibr\" target=\"#b3\">Bruna et al. (2014)</ref> and<ref type=\"bibr\" target=\"#b5\">Defferrard et al. (2016)</ref> that improve scalability and classifica. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s for PPI (0.995) and Reddit (0.970). * Equal contribution * The skip-connection design proposed by <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref> does not have such \"subset\" requirement, an ve the softmax step which normalizes attention values within the same neighborhood, as suggested by <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref>. See Appendix C.3.</p></div> \t\t\t</abstract> selected by one node in the next layer. <ref type=\"bibr\" target=\"#b4\">Chen et al. (2018b)</ref> and <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref> further propose samplers to restrict the ne ize in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref> improves FastGCN by an additional sampling   (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref> and clustering of <ref type=\"bibr\" target=\" its neighbors in the training graph. The removal of softmax is also seen in the attention design of <ref type=\"bibr\" target=\"#b14\">Huang et al. (2018)</ref>. Note that during the minibatch training, G e=\"bibr\" target=\"#b4\">Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b8\">Gao et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2018;</ref><ref type=\"bibr\" target=\"#b3\">Chen et al., 2 e=\"bibr\" target=\"#b3\">Chen et al., 2018a;</ref><ref type=\"bibr\" target=\"#b8\">Gao et al., 2018;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2018)</ref> have been proposed for efficient minibatch  tment of layers independently by prior work <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2018)</ref>. Consider a layer-( + 1) node v and a layer other hand, for some layer sampling methods <ref type=\"bibr\" target=\"#b4\">(Chen et al., 2018b;</ref><ref type=\"bibr\" target=\"#b14\">Huang et al., 2018)</ref>, extra modification to their samplers is re  et al., 2018b)</ref>, 4. S-GCN <ref type=\"bibr\" target=\"#b3\">(Chen et al., 2018a)</ref>, 5. AS-GCN <ref type=\"bibr\" target=\"#b14\">(Huang et al., 2018)</ref>, and 6. ClusterGCN <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tly. On one hand, prior non-uniform cache access (NUCA) work <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= NUCA by adaptively placing data close to the requesting core <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target=\"#b7\">8,</ref><ref type=\"bibr\" target= istance. However, these best-effort techniques often result in hotspots and additional interference <ref type=\"bibr\" target=\"#b2\">[3]</ref>. On the other hand, prior work has proposed a variety of par at D-NUCA often causes significant bank contention and uneven distribution of accesses across banks <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We also see this effect in Sec. VI -R-NUCA has the highest . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: opt the area under the receiver operator curve (AUC) to evaluate the performance of all the methods <ref type=\"bibr\" target=\"#b32\">[33]</ref>. AUC is the most popular evaluation metric on prediction t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n the entities. Graph Convolutional (Neural) Networks (GCNs) <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b14\">16]</ref> have proven to be a s. With the success of graph (convolutional) neural networks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b10\">11,</ref><ref type=\"bibr\" target=\"#b14\">16]</ref> on a wide range of  la_15\">BPR\u2212OPT G \ud835\udc5c\ud835\udc4f\ud835\udc60 := (\ud835\udc62,\ud835\udc56,\ud835\udc57) \u2208\ud835\udc37 \ud835\udc46 ln \ud835\udf0e x\ud835\udc62\ud835\udc56 \ud835\udc57 (\u0398, G \ud835\udc5c\ud835\udc4f\ud835\udc60 ) \u2212 \ud835\udf06 \u0398 ||\u0398|| 2 (13)</formula><p>Equation <ref type=\"bibr\" target=\"#b10\">(11)</ref>, in conjunction with <ref type=\"bibr\" target=\"#b6\">(7)</re cent industry application of graph representation learning for recommendation. It deploys Graph-Sage<ref type=\"bibr\" target=\"#b10\">[11]</ref> on an item-item graph with both image and text information verall architecture as PinSAGE but we replaced the mean aggregator with LSTM aggregation proposed in<ref type=\"bibr\" target=\"#b10\">[11]</ref>. Although LSTM is an undesirable aggregator because it is . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: el verification was performed through deterministic benchmarks, the procedure of which is available <ref type=\"bibr\" target=\"#b13\">[15]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \">[2]</ref>, phase-based sampling <ref type=\"bibr\" target=\"#b2\">[3]</ref>, and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4]</ref>. Of these techniques, the sampling based approaches typicall 8\">[9]</ref> extended SimPoint to provide statistical confidence measures.</p><p>Wunderlich, et al. <ref type=\"bibr\" target=\"#b3\">[4]</ref> developed the SMARTS framework, which applies statistical sa wn techniques for inferring statistics about a population given a sample of that population. SMARTS <ref type=\"bibr\" target=\"#b3\">[4]</ref> demonstrated that systematic sampling can be used to approxi mpared LiveSim with no sampling simulation and with a sampling mode that was very similar to SMARTS <ref type=\"bibr\" target=\"#b3\">[4]</ref>.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n= f type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref> and statistical sampling <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: 2.1.\">Multiscale segmentation used</head><p>A region based multiscale image segmentation named MSEG <ref type=\"bibr\" target=\"#b36\">(Tzotsos and Argialas, 2006</ref>) is used to generate the initial se D have a size of \u00d7 500 500 pixels. The multiscale segmentation used in this study is MSEG algorithm <ref type=\"bibr\" target=\"#b36\">(Tzotsos and Argialas, 2006)</ref>, which is implemented under C++ en ed on multi-resolution segmentation (MSEG) <ref type=\"bibr\" target=\"#b0\">(Benz et al. (2004)</ref>; <ref type=\"bibr\" target=\"#b36\">Tzotsos and Argialas, 2006)</ref>, the proposed method is implemented. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: , 2017)</ref>, online news <ref type=\"bibr\" target=\"#b21\">(Ross and Carter, 2011)</ref>, web search <ref type=\"bibr\" target=\"#b12\">(Kay et al., 2015)</ref> and advertisements <ref type=\"bibr\" target=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a synopsis of cbp1.5, which features 5 banks. It can be viewed as a 4 th order approximation to PPM <ref type=\"bibr\" target=\"#b2\">[3]</ref>, while YAGS <ref type=\"bibr\" target=\"#b3\">[4]</ref>, which i  are not folding a random value, but a global history value derived from the previous history value <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Figure <ref type=\"figure\" target=\"#fig_1\">2</ref> shows two. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: in that represents the DUT. The Markov Chain is then used to generate test-cases for the design. In <ref type=\"bibr\" target=\"#b10\">[11]</ref>, the coverage analysis results trigger a set of generation. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: xml:id=\"formula_0\">[McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94]</formula><p>Balanced scheduling <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> is an algorithm that can generate schedules that adapt m iss ratios, assuming a workstation-like memory model in which cache misses are normally distributed <ref type=\"bibr\" target=\"#b12\">[KE93]</ref>.</p><p>Since its success depends on the amount of instru no reuse information), which are balanced scheduled.</p><p>The original work on balanced scheduling <ref type=\"bibr\" target=\"#b12\">[KE93]</ref>, which compared it to the traditional approach and witho  produces schedules that are independent of the memory system implementation.</p><p>Initial results <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> indicated that balanced scheduling produced speedups ave results from the original comparison of balanced and traditional scheduling (without optimizations) <ref type=\"bibr\" target=\"#b12\">[KE93]</ref> illustrates both the limitations of simple architecture . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tion as suggested by Velickovic et al. <ref type=\"bibr\" target=\"#b48\">[49]</ref> and Vaswani et al. <ref type=\"bibr\" target=\"#b47\">[48]</ref>. The multi-head attention mechanism performs K independent. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b17\">17</ref> , face recognition <ref type=\"bibr\" target=\"#b18\">18</ref> , and playing Atari games <ref type=\"bibr\" target=\"#b19\">19</ref> . They use many layers of neurons, each arranged in overlapp. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ibr\" target=\"#b36\">[37]</ref> to tackle the speaker variability issue. As shown in our recent paper <ref type=\"bibr\" target=\"#b0\">[1]</ref>, VTLP is especially useful when the speaker variability in t  robustness against speaker variability, we apply an onthe-fly VTLP algorithm on the input waveform <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The warping factor is generated randomly for each input utt e frequency, and K is the DFT size. More details about our VTLP algorithm is described in detail in <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The acoustic simulator in Fig. <ref type=\"figure\">1</ref> i <ref type=\"bibr\" target=\"#b49\">[50]</ref>. In the example server, we ran the VTLP data augmentation <ref type=\"bibr\" target=\"#b0\">[1]</ref>, acoustic simulator <ref type=\"bibr\" target=\"#b1\">[2]</ref>  c). NBF, VTLP, and AS stand for Neural Beam Former (NBF) [59], Vocal Tract LengthPerturbation (VTLP)<ref type=\"bibr\" target=\"#b0\">[1]</ref> , and Acoustics Simulator (AS)<ref type=\"bibr\" target=\"#b1\"> MFCC features, we use the power mel filterbank energies, since it shows slightly better performance <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>. Motivated by our pr own scoring and Inverse Text Normalization (ITN) modules, support for power mel filterbank features <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b42\">43]</ref>, etc. We have tried . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n by GPT <ref type=\"bibr\" target=\"#b49\">[50,</ref><ref type=\"bibr\" target=\"#b50\">51]</ref> and BERT <ref type=\"bibr\" target=\"#b11\">[12]</ref>. But supervised pre-training is still dominant in computer </ref>, it is possible to adopt MoCo for pretext tasks like masked auto-encoding, e.g., in language <ref type=\"bibr\" target=\"#b11\">[12]</ref> and in vision <ref type=\"bibr\" target=\"#b45\">[46]</ref>. W. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: representation to implement optimizations, e.g., auto differentiation and dynamic memory management <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target tional Graphs</head><p>Computational graphs are a common way to represent programs in DL frameworks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">Related Work</head><p>Deep learning frameworks <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b3\">4,</ref><ref type=\"bibr\" target on graph DSLs are a typical way to represent and perform high-level optimizations. Tensorflow's XLA <ref type=\"bibr\" target=\"#b2\">[3]</ref> and the recently introduced DLVM <ref type=\"bibr\" target=\"#b. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: #b11\">[12]</ref>, which also proposed a combinatorial algorithm to compute embeddings. Ganea et al. <ref type=\"bibr\" target=\"#b16\">[17]</ref> and Gulcehre et al. <ref type=\"bibr\" target=\"#b20\">[21]</r  taking its outputs. An alternative to prevent such collapse would be to introduce bias terms as in <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Importantly, when applying the non-linearity directly on   )y 1 + 2 x, y + x 2 y 2<label>(8)</label></formula><p>Similar to the Euclidean case, and following <ref type=\"bibr\" target=\"#b16\">[17]</ref>, we use x = x 0 . On the Poincar\u00e9 ball, we employ pointwis. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: s by studying the two-month history made available by Amazon <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b13\">15]</ref>. Though these statistics alone are sufficient for the user  are limited to statistical studies of historical spot prices <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b13\">15]</ref>.</p><p>Game theoretic pricing. Spot pricing is a distribute  tradeoff is the fact that user jobs can have long runtimes spanning many changes in the spot price <ref type=\"bibr\" target=\"#b13\">[15]</ref>. Users then face two key challenges: 1) Users must predict int ensures that the job is sufficiently interruptible. We use p to denote the optimal bid price to <ref type=\"bibr\" target=\"#b13\">(15)</ref>.</p><p>We now observe that the expected running time in <r  of the spot price monotonically decreases, i.e., F \u03c0 (p) is concave, the optimal bid price solving <ref type=\"bibr\" target=\"#b13\">(15)</ref> </p><formula xml:id=\"formula_29\">is p = \u03c8 \u22121 t k t r \u2212 1 , . Comparing <ref type=\"bibr\" target=\"#b17\">(19)</ref> to bidding for a single persistent request in <ref type=\"bibr\" target=\"#b13\">(15)</ref>, we see that <ref type=\"bibr\" target=\"#b17\">(19)</ref> can \"#b13\">(15)</ref>, we see that <ref type=\"bibr\" target=\"#b17\">(19)</ref> can be solved similarly to <ref type=\"bibr\" target=\"#b13\">(15)</ref> in Proposition 5.</p><p>By comparing the costs for multipl t k ts .</p><p>Proof of Proposition 5.</p><p>Proof. By taking the first-order derivative of \u03a6(p) in <ref type=\"bibr\" target=\"#b13\">(15)</ref>  <ref type=\"figure\" target=\"#fig_3\">3</ref>), F \u03c0 (p) is c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rchy which have shown an advantageous performance over paradigms using user-item interactions alone <ref type=\"bibr\" target=\"#b18\">[19]</ref>- <ref type=\"bibr\" target=\"#b20\">[21]</ref>.</p><p>Generall representation involves extensive and unscalable computation with the adjacent matrix of the graph. <ref type=\"bibr\" target=\"#b18\">[19]</ref> learns a hierarchical representation of graphs by decompos nd becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, <ref type=\"bibr\" target=\"#b18\">[19]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>. There are some of our proposed method, which fixes the number of user levels to 2. The parameter of CGNN refers to <ref type=\"bibr\" target=\"#b18\">[19]</ref>. \u2022 DIN: A popular deep neural network method without graph. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rs. The design of our architecture is inspired by recent progress in computer vision, in particular <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b6\">He  ch deeper networks <ref type=\"bibr\" target=\"#b12\">(Krizhevsky et al., 2012)</ref>, namely 19 layers <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref t erarchical manner. Our architecture can be in fact seen as a temporal adaptation of the VGG network <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>. We have also investigated the s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the syntactic level <ref type=\"bibr\" target=\"#b31\">(Siddharthan, 2002)</ref> and the lexicon level <ref type=\"bibr\" target=\"#b2\">(Carroll et al., 1999)</ref>. Later, researchers adopted machine learn. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  as exponential service times, infinite buffers, and so on <ref type=\"bibr\" target=\"#b6\">[7]</ref>, <ref type=\"bibr\" target=\"#b8\">[9]</ref>, <ref type=\"bibr\" target=\"#b12\">[13]</ref>. Likewise, analys et=\"#b6\">[7]</ref> and hypercubes <ref type=\"bibr\" target=\"#b22\">[24]</ref>. The study presented in <ref type=\"bibr\" target=\"#b8\">[9]</ref> is not restricted to a particular topology, but it assumes a where T Bus is the service time of the Bus architecture and C Bus is the contention matrix given in <ref type=\"bibr\" target=\"#b8\">(9)</ref>. Finally, we note that when each buffer shown in Fig. <ref t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: t=\"#b29\">[30]</ref> and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in <ref type=\"bibr\" target=\"#b31\">[32]</ref>.</p><p>In Section 3 we describe our proposed stream fetch  f> shows a block diagram of the trace cache mechanism as proposed by Rotenberg, Benett and Smith in <ref type=\"bibr\" target=\"#b31\">[32]</ref>. The trace cache captures the dynamic instruction stream,  <ref type=\"bibr\" target=\"#b33\">[34]</ref>, and the trace cache architecture using a trace predictor <ref type=\"bibr\" target=\"#b31\">[32]</ref> and selective trace storage <ref type=\"bibr\" target=\"#b28\" rget=\"#b8\">[9,</ref><ref type=\"bibr\" target=\"#b22\">23,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b31\">32]</ref> is one such high fetch width mechanism, recently implemente. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ISHELL-2 <ref type=\"bibr\" target=\"#b17\">[18]</ref>) and the Mandarin telephone ASR benchmark (HKUST <ref type=\"bibr\" target=\"#b18\">[19]</ref>). For Librispeech, we use all the train data (960 hours) f. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  the network-in-training on different epochs to improve the performance of semi-supervised learning <ref type=\"bibr\" target=\"#b13\">[14]</ref>. However, it is hard to scale for a large dataset since te. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: resent in natural images, video, and speech. These properties are exploited efficiently by ConvNets <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>, which are designed to. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  Dynamic Memory Network (DMN) <ref type=\"bibr\" target=\"#b11\">[12]</ref> uses a gated recurrent unit <ref type=\"bibr\" target=\"#b1\">[2]</ref> based controller to update the memory, while Working Memory . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ing (AT) procedure <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b34\">35,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref> shows promising res al training (e.g., <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b17\">18,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b39\">40,</ref><ref type=\"bibr\" tar get=\"#b37\">38,</ref><ref type=\"bibr\" target=\"#b31\">32,</ref><ref type=\"bibr\" target=\"#b30\">31,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar unreliable for generating adversarial samples during single-step adversarial training. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that models trained using adversarial samples =\"bibr\" target=\"#b8\">9]</ref> accepted to ICLR 2018. In this direction, adversarial training method <ref type=\"bibr\" target=\"#b21\">[22]</ref>, shows promising results for learning robust deep learning ls trained using EAT are still susceptible to multi-step attacks in white-box setting. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that adversarially trained model can be made  show that over-fitting effect is the reason for failure to satisfy the criteria.</p><p>Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> demonstrated that it is possible to learn robust models us rameters (\u03b8) should be updated so as to decrease the loss on such adversarial samples. Madry et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> solves the maximization step by generating adversarial sam raining method <ref type=\"bibr\" target=\"#b12\">[13]</ref> and multi-step adversarial training method <ref type=\"bibr\" target=\"#b21\">[22]</ref>. Column-1 of Fig. <ref type=\"figure\" target=\"#fig_0\">1</re  for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by <ref type=\"bibr\" target=\"#b21\">[22]</ref>. At each iteration all the clean samples in the mini-batch d for EAT method. PGD Adversarial Training (PAT): Multi-step adversarial training method proposed by<ref type=\"bibr\" target=\"#b21\">[22]</ref>. At each iteration all the clean samples in the mini-batch s added to the image. In our experiments, we set \u03b1 = /steps.</p><p>Projected Gradient Descent (PGD) <ref type=\"bibr\" target=\"#b21\">[22]</ref>: Initially, a small random noise sampled from Uniform dist. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: structions a di cult task, even in the presence of aiding devices like a hardware trace cache (HTC) <ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>. Indeed, a trace cach. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: intent-labeled speech data, and such data is usually scarce. <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b10\">11]</ref> address this problem using a curriculum and transfer learni. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: p>We evaluate our SSVD on the large-scale benchmark for video object detection task, i.e., ImageNet <ref type=\"bibr\" target=\"#b62\">[63]</ref> object detection from video (VID) dataset, which contains . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: lar to recent work on sequence-tosequence voice conversion <ref type=\"bibr\" target=\"#b15\">[16]</ref><ref type=\"bibr\" target=\"#b16\">[17]</ref><ref type=\"bibr\" target=\"#b17\">[18]</ref>. <ref type=\"bibr\" r identities, to transform word segments from multiple speakers into multiple target voices. Unlike <ref type=\"bibr\" target=\"#b16\">[17]</ref>, which trained separate models for each source-target spea a pretrained speech recognizer to more explicitly capture phonemic information in the source speech <ref type=\"bibr\" target=\"#b16\">[17]</ref>. However, we do find it helpful to multitask train the mod. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: Zeiler and Fergus, 2014)</ref> of the input. The best networks are using more than 150 layers as in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a;</ref><ref type=\"bibr\" target=\"#b7\">He et al., 2016b n computer vision, in particular <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015;</ref><ref type=\"bibr\" target=\"#b6\">He et al., 2016a)</ref>.</p><p>This paper is structured as follows. Th ayers <ref type=\"bibr\" target=\"#b17\">(Simonyan and Zisserman, 2015)</ref>, or even up to 152 layers <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>. In the remainder of this paper, we describe  serman, 2015)</ref>. We have also investigated the same kind of \"ResNet shortcut\" connections as in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>, namely identity and 1 \u00d7 1 convolutions (see  ng even deeper degrades accuracy. Shortcut connections help reduce the degradation. As described in <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>, the gain in accuracy due to the the increase onnections between convolutional blocks that allow the gradients to flow more easily in the network <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>.</p><p>We evaluate the impact of shortcut con works to temporal convolutions as we think this a milestone for going deeper in NLP. Residual units <ref type=\"bibr\" target=\"#b6\">(He et al., 2016a)</ref>  work and ImageNet is that the latter deals w. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ias by using corpus level constraints, but is only practical for models with specialized structure. <ref type=\"bibr\" target=\"#b13\">Kusner et al. (2017)</ref> propose the method based on causal inferen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: direct branches <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, <ref type=\"bibr\" target=\"#b34\">[34]</ref>. The BTB implicitly-and usually inaccurately-assumes that . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: notations, but lack the ability to generalize to other domains. In contrast, embedding-based models <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b;</ref><ref type=\"bibr\" target=\"#b13\">Hao et al., en executing logical queries on incomplete KBs. Our work follows the line of Embedding-based models <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b;</ref><ref type=\"bibr\" target=\"#b10\">Dong et al. ated to these three OOV relations during the test.</p><p>Several baselines are included here: Embed <ref type=\"bibr\" target=\"#b7\">(Bordes et al., 2014b)</ref> deals with factoid QA over KB by matching. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: in a relatively small proportion of programs' data variables <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, and by selectively protecting these SDC-prone variables, one ref type=\"bibr\" target=\"#b18\">[19]</ref>. LLFI works at the LLVM compiler's intermediate code level <ref type=\"bibr\" target=\"#b8\">[9]</ref>, and allows fault injections to be performed at specific pro. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: learn node representations by utilizing information from distant neighbors. GCNs and their variants <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a;</ref><ref type=\"bibr\" target=\"#b14\">Veli\u010dkovi sed node classification <ref type=\"bibr\">(Kipf &amp; Welling, 2017)</ref>, inductive node embedding <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>, link prediction <ref type=\"bibr\" targe >Implementation Details</head><p>Training with the CV estimator is similar as with the NS estimator <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>. Particularly, each iteration of the al r all the other multi-class datasets. The model is GCN for the former 4 datasets and GraphSAGE-mean <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref> for the latter 2 datasets, see Appendix orted by <ref type=\"bibr\" target=\"#b2\">Chen et al. (2018)</ref>, while their NS baseline, GraphSAGE <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a)</ref>, does not implement the preprocessing t sets because of their slow convergence and the requirement to fit the entire dataset in GPU memory. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> make an initial attempt to develop stoch www.tei-c.org/ns/1.0\"><head n=\"2.3.\">Neighbor Sampling</head><p>To reduce the receptive field size, <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> propose a neighbor sampling (NS) algorit id=\"formula_8\">P (l) uv = n(u) D (l) P uv if v \u2208 n(l) (u)</formula><p>, and P (l) uv = 0 otherwise. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> propose to perform an approximate forwar D (l) needs to be large for NS, to keep comparable predictive performance with the exact algorithm. <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref> choose D (1) = 10 and D (2) = 25, and th r, Cora, PubMed and NELL from <ref type=\"bibr\">Kipf &amp; Welling (2017)</ref> and Reddit, PPI from <ref type=\"bibr\" target=\"#b4\">Hamilton et al. (2017a)</ref>, with the same train / validation / test ted to the task nor the model. Our algorithm is applicable to other models including GraphSAGE-mean <ref type=\"bibr\" target=\"#b4\">(Hamilton et al., 2017a</ref>) and graph attention networks (GAT) <ref e most GCNs only have two graph convolution layers <ref type=\"bibr\">(Kipf &amp; Welling, 2017;</ref><ref type=\"bibr\" target=\"#b4\">Hamilton et al., 2017a)</ref>, this gives a significant reduction of t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: quence Models for Peptide Binding Prediction</head><p>The approach builds on the UDSMProt-framework <ref type=\"bibr\" target=\"#b11\">[12]</ref> and related work in natural language processing <ref type=  with a concat pooling layer and two fully connected layers. The setup closely follows that used in <ref type=\"bibr\" target=\"#b11\">[12]</ref>, where protein properties were predicted. The smaller data  of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50. Similar to <ref type=\"bibr\" target=\"#b11\">[12]</ref>, the training procedure included 1-cycle learning rate sch e potential of unlabeled peptide data in order to observe similar improvements as seen for proteins <ref type=\"bibr\" target=\"#b11\">[12]</ref> in particular for small datasets.  Turning to MHC Class II uarcy of 0.137, which is is considerably lower than the accuracy of 0.41 reported in the literature <ref type=\"bibr\" target=\"#b11\">[12]</ref>. This effect is a direct consequence of the considerably s. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  that the cost benefits brought by spot offerings can be realized with intuitive bidding strategies <ref type=\"bibr\" target=\"#b14\">[15]</ref>. However, choosing between spot instances and bid levels a e on-demand price since the user only pays the spot price anyway, as is commonly advocated (e.g. in <ref type=\"bibr\" target=\"#b14\">[15]</ref>) and used in practice. Under the given model, the WSD assu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: architecture <ref type=\"bibr\" target=\"#b20\">[21]</ref> and Solihin et al.'s memory-side prefetching <ref type=\"bibr\" target=\"#b19\">[20]</ref> .</p><p>DCE and FE were proposed specifically for multi-co. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 0\"><head n=\"2.6.1\">Implementation</head><p>We implement the neural network using the torch7 library <ref type=\"bibr\" target=\"#b7\">(Collobert et al., 2011a)</ref>. Training and inference are done on a . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  corresponding relevance judgments. We followed a standard re-ranking methodology in prior research <ref type=\"bibr\" target=\"#b17\">[8,</ref><ref type=\"bibr\" target=\"#b38\">28]</ref>: re-rank the top 10. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN <ref type=\"bibr\" target=\"#b1\">(Dong et al., 2014)</ref>. It consists of more than one composition fu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: hieves better performance than the state-of-theart methods <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing the depth without adding any parameters,  eNet <ref type=\"bibr\" target=\"#b19\">[21]</ref>, Kim et al. <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> propose two very deep convolutional networks for SR, both s ameters. Both the DL <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> and non-DL <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref typ  testing sets, by citing the results of prior methods from <ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. The two DRRN models outperforms all existing methods in al chieves better performance than the state-of-theart methods<ref type=\"bibr\" target=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref>. After increasing the depth without adding any parameters,   the performance and significantly outperforms VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30 <ref type=\"bibr\" target=\"#b16\">[17]</ref> by 0.3  the other hand, to control the model parameters, the Deeply-Recursive Convolutional Network (DRCN) <ref type=\"bibr\" target=\"#b13\">[14]</ref> introduces a very deep recursive layer via a chain structu ><p>(2) Recursive learning of residual units is proposed in DRRN to keep our model compact. In DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, a deep recursive layer (up to 16 convolutional recursions et <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> illustrat type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>. The blue dashed box refers to a recursive layer, among wh esNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> and DRRN. U, d, T, and B are the numbers of residual units cursive block.</p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3.\">DRCN</head><p>DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref> is motivated by the observation that adding more weight la s a given image x as feature maps H 0 . The inference net f 2 (H 0 ) stacks T recursions (T = 16 in <ref type=\"bibr\" target=\"#b13\">[14]</ref>) in a recursive layer, with shared weights among these rec \">[3]</ref>. Very deep models (d \u2265 20) include VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref type=\"bibr\" target=\"#b13\">[14]</ref>, RED <ref type=\"bibr\" target=\"#b16\">[17]</ref> and DRRN wi es the performance and significantly outperforms VDSR<ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref> and RED30<ref type=\"bibr\" target=\"#b16\">[17]</ref> by 0.37  type=\"bibr\" target=\"#b12\">[13]</ref>. The purple line refers to a global identity mapping. (c) DRCN<ref type=\"bibr\" target=\"#b13\">[14]</ref>. The blue dashed box refers to a recursive layer, among wh nt CNN models for SR <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" tar mparison, similar to <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b12\">13,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b21\">23]</ref>, we crop pixels nea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: -sampling and down-sampling layers to provide an error feedback mechanism for each stage. Jo et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> introduced the dynamic upsampling filters for video super- cally generated from the arbitrary customized prior boxes. In the video super resolution, Jo et al. <ref type=\"bibr\" target=\"#b12\">[13]</ref> proposed a dynamic upsampling filters. The dynamic upsampl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  a single image has recently received a huge boost in performance using Deep-Learning based methods <ref type=\"bibr\" target=\"#b3\">[4,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe  CNN-based SR methods <ref type=\"bibr\" target=\"#b9\">[10,</ref><ref type=\"bibr\" target=\"#b8\">9,</ref><ref type=\"bibr\" target=\"#b3\">4]</ref>, we only learn the residual between the interpolated LR and i haustively trained for these conditions. In fact, ZSSR is significantly better than the older SRCNN <ref type=\"bibr\" target=\"#b3\">[4]</ref>, and in some cases achieves comparable or better results tha. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b8\">9]</ref>, dynamic frequency warping <ref type=\"bibr\" target=\"#b9\">[10]</ref>, and Gaussian mixture models <ref type=\"bibr\" target=\"#b10\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ng-standing research topic in information retrieval (IR) <ref type=\"bibr\" target=\"#b1\">[2]</ref>[4] <ref type=\"bibr\" target=\"#b7\">[8]</ref>. Usually, the contextual information captured by models such. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bibr\" target=\"#b9\">(Kikuchi et al., 2016;</ref><ref type=\"bibr\" target=\"#b4\">Fan et al., 2017;</ref><ref type=\"bibr\" target=\"#b25\">Scarton and Specia, 2018;</ref><ref type=\"bibr\" target=\"#b17\">Nishiha ummary more focused on a given named entity <ref type=\"bibr\" target=\"#b4\">(Fan et al., 2017)</ref>. <ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref> and <ref type=\"bibr\" target=\"#b17\">Ni t\" n=\"2\" xml:id=\"foot_1\">We did not investigate predicting ratios on a per sentence basis as done by<ref type=\"bibr\" target=\"#b25\">Scarton and Specia (2018)</ref>, and leave this for future work. End-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rg/ns/1.0\"><head>III. RU-NET AND R2U-NET ARCHITECTURES</head><p>Inspired by the deep residual model <ref type=\"bibr\" target=\"#b6\">[7]</ref>, RCNN <ref type=\"bibr\" target=\"#b40\">[41]</ref>, and U-Net < type=\"bibr\" target=\"#b4\">[5]</ref>, GoogleNet <ref type=\"bibr\" target=\"#b5\">[6]</ref>, Residual Net <ref type=\"bibr\" target=\"#b6\">[7]</ref>, DenseNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, and Capsu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ure the representation quality. All of these works sample negative examples from p(x). Arora et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> theoretically analyze the effect of contrastive representati erparameter. Without loss of generality, we set t = 1 for all theoretical results.</p><p>Similar to <ref type=\"bibr\" target=\"#b0\">[1]</ref>, we assume an underlying set of discrete latent classes C th f ) = inf W\u2208R K\u00d7d L Softmax (T , W f ).<label>(10)</label></formula><p>In line with the approach of <ref type=\"bibr\" target=\"#b0\">[1]</ref> we analyze the supervised loss of a mean classifier <ref typ commonly the case. The dependence on on N and T in Theorem 5 is roughly equivalent to the result in <ref type=\"bibr\" target=\"#b0\">[1]</ref>, but the two bounds are not directly comparable since the pr .</formula><p>In order to derive our bound we will exploit a concentration of measure result due to <ref type=\"bibr\" target=\"#b0\">[1]</ref>. They consider an objective of the form</p><formula xml:id=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: </ref> are independent to the targets, hence the results are computed despite what the targets are. <ref type=\"bibr\" target=\"#b7\">Hu and Liu (2004)</ref> regard the features of products as targets, an. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: =\"#b12\">[13]</ref><ref type=\"bibr\" target=\"#b13\">[14]</ref><ref type=\"bibr\" target=\"#b14\">[15]</ref><ref type=\"bibr\" target=\"#b15\">[16]</ref> try to improve the static injection framework. CriticalFau structions using symbolic execution, which enumerates all potential hardware errors.</p><p>The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> presents a selective protection technique that allows user tion technique that allows users to selectively protect these SDC-prone data. The main idea of work <ref type=\"bibr\" target=\"#b15\">[16]</ref> is predicting the SDC proneness of a program's data firstl put program are selected by GA.</p><p>Step 3 strengthens the identified vulnerable blocks. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> introduces a prediction model named SDCAuto to predict the causing errors. Fig. <ref type=\"figure\" target=\"#fig_1\">2</ref> illustrates the diagram of the work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> first  _1\">2</ref> illustrates the diagram of the work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. The work <ref type=\"bibr\" target=\"#b15\">[16]</ref> first compiles the source code into LLVM IR, and extracts  sidered, since it always causes illegal opcode exception rather than SDC.</p><p>Finally, as in work <ref type=\"bibr\" target=\"#b15\">[16]</ref>, we assume that at most one fault occurs during a program' e our results with the work <ref type=\"bibr\" target=\"#b14\">[15]</ref> and SDCAuto presented in work <ref type=\"bibr\" target=\"#b15\">[16]</ref>. We use our approach to maximize SDC coverage under the us /1.0\" xml:id=\"fig_1\"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of the work<ref type=\"bibr\" target=\"#b15\">[16]</ref> </figDesc><graphic url=\"image-2.png\" coords=\"4,99.47,451.0. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: emand for secure communication and secure operation due to rising online fraud and software attacks <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Some of these vulnerabilities are due to the complexity and  a binary positive number to each BB, and the length of a signature Len can be obtained by equation <ref type=\"bibr\" target=\"#b0\">(1)</ref>, where N is the number of total BBs. </p></div> <div xmlns=\". Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: </p><p>Finally, a recent trend in computer graphics has been the use of rendered images as textures <ref type=\"bibr\" target=\"#b2\">[3]</ref>. As a result, it has become desirable to unify the framebuff  accessed in parallel is possible if the texels are stored in a morton order within the cache lines <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Morton order implies that the texels are stored in 2x2 bloc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: CAREER Award CCR -0133777.</p><p>technology, the larger the cache, the slower the cache will become <ref type=\"bibr\" target=\"#b1\">[2]</ref>, and larger caches increase the cost of manufacturing. Anoth he. An inclusive cache system implies that the contents of the L1 cache be a subset of the L2 cache <ref type=\"bibr\" target=\"#b1\">[2]</ref>. This decreases the effective cache capacity available for u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  studies attribute the inefficiency to either the high system call overhead of the operating system <ref type=\"bibr\" target=\"#b24\">[28,</ref><ref type=\"bibr\" target=\"#b36\">40,</ref><ref type=\"bibr\" ta nts. While packet-level batching <ref type=\"bibr\" target=\"#b23\">[27]</ref> and system-call batching <ref type=\"bibr\" target=\"#b24\">[28,</ref><ref type=\"bibr\" target=\"#b36\">40,</ref><ref type=\"bibr\" ta  etc.) pollution that causes performance penalties. Previous solutions propose system call batching <ref type=\"bibr\" target=\"#b24\">[28,</ref><ref type=\"bibr\" target=\"#b39\">43]</ref> or efficient syste  into a write queue. While the idea of amortizing the system call overhead using batches is not new <ref type=\"bibr\" target=\"#b24\">[28,</ref><ref type=\"bibr\" target=\"#b39\">43]</ref>, we demonstrate th ticore CPUs with receive-side scaling (RSS). The resulting TCP stack outperforms Linux and MegaPipe <ref type=\"bibr\" target=\"#b24\">[28]</ref> by up to 25x (w/o SO_REUSEPORT) and 3x, respectively, in h cality, shared file descriptor space, inefficient packet processing, and heavy system call overhead <ref type=\"bibr\" target=\"#b24\">[28]</ref>. Lack of connection locality: Many applications are multi- get=\"#b33\">[37]</ref>.</p><p>Affinity-Accept <ref type=\"bibr\" target=\"#b33\">[37]</ref> and MegaPipe <ref type=\"bibr\" target=\"#b24\">[28]</ref> address this issue by providing a local accept queue in ea inates this layer for sockets by explicitly partitioning the fd space for sockets and regular files <ref type=\"bibr\" target=\"#b24\">[28]</ref>. Inefficient per-packet processing: Previous studies indic ching in a similar way, but it uses a standard system call interface to communicate with the kernel <ref type=\"bibr\" target=\"#b24\">[28]</ref>.</p><p>Batching also has been applied to packet I/O to red. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r text classification. The architecture is a direct application of CNNs, as used in computer vision <ref type=\"bibr\" target=\"#b12\">(LeCun et al., 1998)</ref>, albeit with NLP interpretations. <ref typ. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: orithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bib s the substitution of an indirect method call with direct method calls in object-oriented languages <ref type=\"bibr\" target=\"#b10\">[11]</ref>, <ref type=\"bibr\" target=\"#b24\">[24]</ref>, <ref type=\"bib nce impact due to virtual function calls. These approaches include the method cache in Smalltalk-80 <ref type=\"bibr\" target=\"#b10\">[11]</ref>, polymorphic inline caches <ref type=\"bibr\" target=\"#b23\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: he temporal coherence exploration in video understanding <ref type=\"bibr\" target=\"#b47\">[48]</ref>, <ref type=\"bibr\" target=\"#b48\">[49]</ref>, <ref type=\"bibr\" target=\"#b49\">[50]</ref>, <ref type=\"bib. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: th different weights. However, these methods <ref type=\"bibr\" target=\"#b27\">(Wu et al., 2019b;</ref><ref type=\"bibr\" target=\"#b31\">Zhu et al., 2019;</ref><ref type=\"bibr\" target=\"#b0\">An et al., 2019)  set of documents given a query. Some works <ref type=\"bibr\" target=\"#b24\">(Wang et al., 2018;</ref><ref type=\"bibr\" target=\"#b31\">Zhu et al., 2019)</ref> propose to improve news representations via e eddings. These embeddings can be pre-trained from a large corpus or randomly initialized. Following <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we define the profile embedding</p><formula ed news representations would be taken as initial input embeddings of our model GNUD. Following DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we use two parallel convolutional neural ne sa-10week, which respectively collect news click logs as long as 1 week and 10 weeks. Following DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, we just select user id, news id, time-stamp ws title and profile as semantic-level and knowledge-level representations, respectively.</p><p>DAN <ref type=\"bibr\" target=\"#b31\">(Zhu et al., 2019)</ref>, a deep attention neural network for news re. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: information which may be head pose, expression, or landmarks <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b20\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: revious work on CycleGANs <ref type=\"bibr\" target=\"#b29\">(Zhu et al., 2017)</ref> and dual learning <ref type=\"bibr\" target=\"#b11\">(He et al., 2016)</ref>, our method takes two initial models in oppos ition to that, we would like to incorporate a language modeling loss during NMT training similar to <ref type=\"bibr\" target=\"#b11\">He et al. (2016)</ref>. Finally, we would like to adapt our approach . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: r training neural networks in the reinforcement learning setting, such as neural fitted Q-iteration <ref type=\"bibr\" target=\"#b18\">24</ref> , these methods involve the repeated training of networks de ters on each game, privy only to the inputs a human player would have. In contrast to previous work <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> , our approach incor e history and the action have been used as inputs to the neural network by some previous approaches <ref type=\"bibr\" target=\"#b18\">24,</ref><ref type=\"bibr\" target=\"#b20\">26</ref> . The main drawback . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: >Related Work</head><p>Early works conduct ST in a pipeline manner (Ney 1999; Matusov, Kanthak, and <ref type=\"bibr\" target=\"#b22\">Ney 2005)</ref>, where the ASR output are fed into an MT system to ge. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Optimizer</head><p>We used the Adam optimizer <ref type=\"bibr\" target=\"#b16\">[17]</ref> with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and = 10 \u22129 . We varied the lea. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  superresolution of arbitrary scale factor. As for the state-of-theart SISR methods, such as ESPCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref>, EDSR <ref type=\"bibr\" target=\"#b17\">[18]</ref>, RDN <ref  the same size as the final highresolution image, these methods are time-consuming.</p><p>Shi et al. <ref type=\"bibr\" target=\"#b21\">[22]</ref> firstly proposed a real-time superresolution algorithm ESP real-time superresolution algorithm ESPCNN by proposing the sub-pixel convolution layer. The ESPCNN <ref type=\"bibr\" target=\"#b21\">[22]</ref> upscaled the image at the end of the network to reduce the applied to these networks by simply replacing the traditional upscale module (sub-pixel convolution <ref type=\"bibr\" target=\"#b21\">[22]</ref>). We choose the state-of-the-art SISR network, called resi etwork, the Feature Learning Module, and the Meta-Upscale Module. Most the state-of-the-art methods <ref type=\"bibr\" target=\"#b21\">[22,</ref><ref type=\"bibr\" target=\"#b35\">36,</ref><ref type=\"bibr\" ta. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: core uses state-of-art TAGE and ITTAGE branch predictors <ref type=\"bibr\" target=\"#b20\">[21]</ref>, <ref type=\"bibr\" target=\"#b21\">[22]</ref>, and a memory dependence predictor similar to Alpha 21264 . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  results, when compared against the Transformer models as well as models from previous work (SeqRNN <ref type=\"bibr\" target=\"#b22\">(Hellendoorn and Devanbu, 2017b)</ref>, Deep3 <ref type=\"bibr\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: nthesize an audio signal from the predicted spectrogram, we primarily use the Griffin-Lim algorithm <ref type=\"bibr\" target=\"#b27\">[28]</ref> to estimate a phase consistent with the predicted magnitud. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: through which they occur, are important and challenging problems that have attracted much attention <ref type=\"bibr\" target=\"#b9\">[10]</ref>. This paper focuses on predicting protein interfaces. Despi ppears to be saturated. This calls for new methodologies or sources of information to be exploited\" <ref type=\"bibr\" target=\"#b9\">[10]</ref>. Most machine learning methods for interface prediction use. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ective for blackbox attacks. Given the difficulties of practical black-box attacks, Papernot et al. <ref type=\"bibr\" target=\"#b15\">[16]</ref> use adaptive queries to train a surrogate model to fully c. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: bital area and by deducing the respiration rate from the thermal nostril areas. Granhag and Hartwig <ref type=\"bibr\" target=\"#b15\">[16]</ref> proposed a methodology using psychologically informed mind. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  \"the deeper the better\" might not be the case in SR. Inspired by the success of very deep networks <ref type=\"bibr\" target=\"#b7\">[8,</ref><ref type=\"bibr\" target=\"#b25\">27,</ref><ref type=\"bibr\" targ p networks could suffer from the performance degradation problem, as observed in visual recognition <ref type=\"bibr\" target=\"#b7\">[8]</ref> and image restoration <ref type=\"bibr\" target=\"#b16\">[17]</r nce Sec. 1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref> and DRCN <re iv> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1.\">ResNet</head><p>The main idea of ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref> is to use a residual learning framework to ease the training iple weight layers in the residual unit) Table <ref type=\"table\">1</ref>. Strategies used in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, VDSR <ref type=\"bibr\" target=\"#b12\">[13]</ref>, DRCN <ref t bel>(1)</label></formula><p>where x is the output of the residual unit, h(x) is an identity mapping <ref type=\"bibr\" target=\"#b7\">[8]</ref> : h(x) = x, W is a set of weights (the biases are omitted to ng the recursive block structure, in which several residual units are stacked. Noted that in ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, different residual units use different inputs for the ident </p></div> <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1.\">Residual Unit</head><p>In ResNet <ref type=\"bibr\" target=\"#b7\">[8]</ref>, the basic residual unit is formulated as Eq. 1 and the acti fig_1\"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Simplified structures of (a) ResNet<ref type=\"bibr\" target=\"#b7\">[8]</ref>. The green dashed box means a residual unit. (b) VDSR<ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: on LR images directly and progressively reconstruct the sub-band residuals of HR images. Tai et al. <ref type=\"bibr\" target=\"#b33\">[34]</ref> proposed deep recursive residual network (DRRN) to address the estimated high-quality patch with the same resolution as the input low-quality patch. We follow <ref type=\"bibr\" target=\"#b33\">[34]</ref> to do data augmentation. For each task, we train a single . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: resholding algorithm <ref type=\"bibr\" target=\"#b4\">[5]</ref>, Cascaded Sparse Coding Network (CSCN) <ref type=\"bibr\" target=\"#b30\">[32]</ref> is trained end-to-end to fully exploit the natural sparsit SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, DJSR <ref type=\"bibr\" target=\"#b31\">[33]</ref>, CSCN <ref type=\"bibr\" target=\"#b30\">[32]</ref>, ESPCN <ref type=\"bibr\" target=\"#b23\">[25]</ref> and FSRCN bibr\" target=\"#b23\">[25]</ref> observe that the prior models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b30\">32]</ref> increase LR image's resolution via bicubic interpolation be get=\"#b13\">14,</ref><ref type=\"bibr\" target=\"#b16\">17,</ref><ref type=\"bibr\" target=\"#b23\">25,</ref><ref type=\"bibr\" target=\"#b30\">32]</ref> versus the number of parameters, denoted as k. Compared to . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: researchers have found convolutional networks (ConvNets) <ref type=\"bibr\" target=\"#b16\">[17]</ref>  <ref type=\"bibr\" target=\"#b17\">[18]</ref> are useful in extracting information from raw signals, ran. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  not enough for the stream fetch engine to completely overcome the need for an overriding mechanism <ref type=\"bibr\" target=\"#b16\">[17]</ref>.</p><p>Looking for novel ways of enlarging streams, we pre on allows to hide the branch predictor access delay. Our next stream predictor has the same ability <ref type=\"bibr\" target=\"#b16\">[17]</ref>, since a stream prediction is also a multiple branch predi hnology.</p><p>We have found that the best performance is achieved using three-cycle latency tables <ref type=\"bibr\" target=\"#b16\">[17]</ref>. Although bigger predictors are slightly more accurate, th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  challenged by their vulnerability to adversarial examples <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>, which are crafted by adding small, human-imperceptible noise icted labels and probabilities of these images given by the Inception v3. more varied training data <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10]</ref>.</p><p>With the knowl >, adversarial training is the most extensively investigated way to increase the robustness of DNNs <ref type=\"bibr\" target=\"#b4\">[5,</ref><ref type=\"bibr\" target=\"#b9\">10,</ref><ref type=\"bibr\" targe ef type=\"bibr\" target=\"#b22\">[23]</ref>, one-step gradient-based methods such as fast gradient sign <ref type=\"bibr\" target=\"#b4\">[5]</ref> and iterative variants of gradient-based methods <ref type=\" t-based methods that iteratively perturb the input with the gradients to maximize the loss function <ref type=\"bibr\" target=\"#b4\">[5]</ref>, momentum-based methods accumulate a velocity vector in the  e-box attacks and the transferability, and act as a stronger attack algorithm than one-step methods <ref type=\"bibr\" target=\"#b4\">[5]</ref> and vanilla iterative methods <ref type=\"bibr\" target=\"#b8\"> ply derived.</p><p>One-step gradient-based approaches, such as the fast gradient sign method (FGSM) <ref type=\"bibr\" target=\"#b4\">[5]</ref>, find an adversarial example x * by maximizing the loss func. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: s. Plenty of researches have been conducted to solve the name ambiguity problem. Supervised methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>,</p><p>The research is supported by the National Key Researc representations play a critical role to quantify distinctions and similarities between publications <ref type=\"bibr\" target=\"#b0\">[1]</ref>. The majority of existing solutions utilize biographical fea lustering (HAC) method works well for skewed data and is widely in many name disambiguation methods <ref type=\"bibr\" target=\"#b0\">[1]</ref>, <ref type=\"bibr\" target=\"#b4\">[5]</ref>, <ref type=\"bibr\" t ected components to generate the clustering results for each name to be disambiguated. Zhang et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>: This method uses a global metric learning and local linkage For example, <ref type=\"bibr\" target=\"#b4\">[5]</ref> need to specify the number of distinct author, <ref type=\"bibr\" target=\"#b0\">[1]</ref> need labeled data to estimate the number. For a fair compari d co-occurrence information of text and loss a certain amount of semantic information. Zhang et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref> also use a graph convolutional network based encoder-decoder based framework to extract multiple types of characteristics and relations in publication database. <ref type=\"bibr\" target=\"#b0\">[1]</ref> use a global metric learning and local linkage graph auto-en. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: a text word by word and stores the semantics of all the previous text in a fixed-sized hidden layer <ref type=\"bibr\" target=\"#b7\">(Elman 1990</ref>). The advantage of RecurrentNN is the ability to bet. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aints. As a result, researchers have explored software-based techniques to tolerate hardware faults <ref type=\"bibr\" target=\"#b2\">[3]</ref>. Softwarebased techniques do not require any modification in. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n the same train/validation/test splits of the same three datasets (CORA, CiteSeer and PubMed) from <ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref>. Such experimental setup favors the model th  consider the problem of semi-supervised transductive node classification in a graph, as defined in <ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref>. In this paper we compare the four following raged over all datasets. See text for the definition. (b) Model accuracy on the Planetoid split from<ref type=\"bibr\" target=\"#b15\">Yang et al. [2016]</ref> and another split on the same datasets. Diff ute the following simple experiment. We run the 4 models on the datasets and respective splits from <ref type=\"bibr\" target=\"#b15\">[Yang et al., 2016]</ref>. As shown in Table <ref type=\"table\" target. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ectures, e.g., Siamese networks <ref type=\"bibr\" target=\"#b9\">(He et al., 2016)</ref> and attention <ref type=\"bibr\" target=\"#b26\">(Seo et al., 2017;</ref><ref type=\"bibr\" target=\"#b31\">Tay et al., 20 her than specific term matches. Context-aware representation learning, such as co-attention methods <ref type=\"bibr\" target=\"#b26\">(Seo et al., 2017)</ref>, has been proved effective in many benchmark ng elements in the missing dimen-sions. Softmax col is the column-wise softmax operator. Similar to <ref type=\"bibr\" target=\"#b26\">Seo et al. (2017)</ref>, we perform co-attention from two directions: ; (2) interaction and attention mecha-nisms <ref type=\"bibr\" target=\"#b31\">(Tay et al., 2019b;</ref><ref type=\"bibr\" target=\"#b26\">Seo et al., 2017;</ref><ref type=\"bibr\" target=\"#b21\">Parikh et al., . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: procedures such as generating forced alignments and decision trees. Meanwhile, another line of work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" target ction to infer speech-label alignments automatically. This CTC technique is further investigated in <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b7\">7,</ref><ref type=\"bibr\" target incorporate lexicons and language models into decoding. When decoding CTC-trained models, past work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target enchmark show that Eesen results in superior performance than the existing end-to-end ASR pipelines <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>. The WERs of Eesen are ained using frame-level labels with respect to the cross-entropy (CE) criterion. Instead, following <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target /1.0\"><head n=\"3.1.\">Decoding with WFSTs</head><p>Previous work has introduced a variety of methods <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8,</ref><ref type=\"bibr\" target  ARPA format (which we will consistently refer to as standard). To be consistent with previous work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref>, we report our results e\">3</ref> lists the results of end-to-end ASR systems that have been reported in the previous work <ref type=\"bibr\" target=\"#b6\">[6,</ref><ref type=\"bibr\" target=\"#b8\">8]</ref> and on the same datase ature differ not only in their model architectures but also in their decoding methods. For example, <ref type=\"bibr\" target=\"#b6\">[6]</ref> and <ref type=\"bibr\" target=\"#b8\">[8]</ref> adopt two distin \">[10]</ref> or achieve the integration under constrained conditions (e.g., nbest list rescoring in <ref type=\"bibr\" target=\"#b6\">[6]</ref>). In this work, we propose a generalized decoding approach b ed in decoding. When only the lexicon is used, our decoding behaves similarly as the beam search in <ref type=\"bibr\" target=\"#b6\">[6]</ref>. In this case, the WER rises quickly to 26.92%. This obvious dard language model, the character-based system gets the WER of 9.07%. CTC experiments in past work <ref type=\"bibr\" target=\"#b6\">[6]</ref> have adopted an expanded vocabulary, and re-trained the lang ref type=\"bibr\" target=\"#b8\">8]</ref> and on the same dataset. Our Eesen framework outperforms both <ref type=\"bibr\" target=\"#b6\">[6]</ref> and <ref type=\"bibr\" target=\"#b8\">[8]</ref> in terms of WERs ]</ref> in terms of WERs on the testing set. It is worth pointing out that the 8.7% WER reported in <ref type=\"bibr\" target=\"#b6\">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the  bibr\" target=\"#b6\">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the authors of <ref type=\"bibr\" target=\"#b6\">[6]</ref> generate a nbest list of hypotheses from a hybrid DNN model, e WERs of Eesen systems via more advanced learning techniques (e.g., expected transcription loss in <ref type=\"bibr\" target=\"#b6\">[6]</ref>) and alternative decoding approach (e.g., dynamic decoders <. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ent works have focused on learning deep embeddings that can be used as universal object descriptors <ref type=\"bibr\" target=\"#b2\">[3,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" targ  effect of incorporating the CF into the fully-convolutional Siamese framework of Bertinetto et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We find that the CF does not improve results for networks t e performance. For our method, we prefer to build upon the fully-convolutional Siamese architecture <ref type=\"bibr\" target=\"#b2\">[3]</ref>, as it enforces the prior that the appearance similarity fun .\">Fully-convolutional Siamese networks</head><p>Our starting point is a network similar to that of <ref type=\"bibr\" target=\"#b2\">[3]</ref>, which we later modify in order to allow the model to be int t is necessary to combine this with a procedure that describes the logic of the tracker. Similar to <ref type=\"bibr\" target=\"#b2\">[3]</ref>, we employ a simplistic tracking algorithm to assess the uti r during training. We first compare against the symmetric Siamese architecture of Bertinetto et al. <ref type=\"bibr\" target=\"#b2\">[3]</ref>. We then compare the endto-end trained CFNet to a variant wh  random seeds, this would require significantly more resources. Our baseline diverges slightly from <ref type=\"bibr\" target=\"#b2\">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw ). We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s <ref type=\"bibr\" target=\"#b2\">[3]</ref>, Staple <ref type=\"bibr\" target=\"#b1\">[2]</ref> and LCT <ref s=\"http://www.tei-c.org/ns/1.0\"><head>A. Implementation details</head><p>We follow the procedure of <ref type=\"bibr\" target=\"#b2\">[3]</ref> to minimize the loss (equation 2) through SGD, with the Xavi  the following ReLU but not the following pooling layer (if any).Our baseline diverges slightly from<ref type=\"bibr\" target=\"#b2\">[3]</ref> in two ways. Firstly, we reduce the total stride of the netw e xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\">Note that this differs from<ref type=\"bibr\" target=\"#b2\">[3]</ref>, in which the target object and search area were instead den ve been introduced <ref type=\"bibr\" target=\"#b27\">[28,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>, raising interest in the tracking community for their simplic rget=\"#b12\">[13,</ref><ref type=\"bibr\" target=\"#b5\">6,</ref><ref type=\"bibr\" target=\"#b21\">22,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref> with CNN features, as proposed in previous work <ref type=\"bi. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ation facts that describe semantic relations between entities; (ii) Uncertain KGs including ProBase <ref type=\"bibr\" target=\"#b21\">(Wu et al. 2012)</ref>, ConceptNet <ref type=\"bibr\" target=\"#b19\">(Sp preting real-world concepts that are ambiguous or intrinsically vague. The probabilistic KG Probase <ref type=\"bibr\" target=\"#b21\">(Wu et al. 2012)</ref> provides a prior probability distribution of c Net mainly come from the cooccurrence frequency of the labels in crowdsourced task results. Probase <ref type=\"bibr\" target=\"#b21\">(Wu et al. 2012</ref>) is a universal probabilistic taxonomy built by. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: odel training. To validate the proposed model, we chose two state-ofthe-art recommender models-CDAE <ref type=\"bibr\" target=\"#b7\">[8]</ref> and Caser <ref type=\"bibr\" target=\"#b6\">[7]</ref>. (This pap tion N (0, 1). Specifically, each baseline CF model had the following hyperparameters.</p><p>\u2022 CDAE <ref type=\"bibr\" target=\"#b7\">[8]</ref>: The latent dimensions for the teacher and the student model. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: 2017;</ref><ref type=\"bibr\" target=\"#b34\">Veli\u010dkovi\u0107 et al., 2018)</ref> or variants of Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref> that apply self-attention on all nodes t ype=\"bibr\" target=\"#b7\">Cai and Lam, 2020)</ref> base their encoder on the Transformer architecture <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref> and thus, in each layer, compute self-at raformer follows the general multi-layer encoderdecoder pattern known from the original Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>. In the following, we first describe our  computations for one head. The output of multiple heads is combined as in the original Transformer <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>.</p><p>Text self-attention. <ref type=\"b ead n=\"3.4\">Graformer decoder</head><p>Our decoder follows closely the standard Transformer decoder <ref type=\"bibr\" target=\"#b33\">(Vaswani et al., 2017)</ref>, except for the modifications suggested  of the ith node's label.</p><p>To compute the node representation H (L) in the Lth layer, we follow <ref type=\"bibr\" target=\"#b33\">Vaswani et al. (2017)</ref>, i.e., we first normalize the input from . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ressing these concerns, approaches utilizing a dynamic vocabulary of slot values have been proposed <ref type=\"bibr\" target=\"#b13\">(Rastogi, Gupta, and Hakkani-Tur 2018;</ref><ref type=\"bibr\" target=\" representations for potentially unseen inputs from new services. Recent pretrained models like ELMo <ref type=\"bibr\" target=\"#b13\">(Peters et al. 2018)</ref> and BERT <ref type=\"bibr\" target=\"#b3\">(De. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: d Image Analysis (OBIA or GEOBIA) has emerged and become an effective approach in HR image analysis <ref type=\"bibr\" target=\"#b16\">(Hossain and Chen, 2019;</ref><ref type=\"bibr\" target=\"#b27\">Ma et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: valuation and learning <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b12\">12,</ref><ref type=\"bibr\" target=\"#b21\">21,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ., the rise-and-fall patterns <ref type=\"bibr\" target=\"#b31\">[32]</ref>, external influence sources <ref type=\"bibr\" target=\"#b32\">[33]</ref>, and conformity phenomenon <ref type=\"bibr\" target=\"#b42\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  complete KGs, extensive research efforts <ref type=\"bibr\" target=\"#b21\">(Nickel et al., 2011;</ref><ref type=\"bibr\" target=\"#b2\">Bordes et al., 2013</ref>  et <ref type=\"bibr\">al., 2014;</ref><ref ty t al., 2011)</ref> is one of the earlier work that models the relationship using tensor operations. <ref type=\"bibr\" target=\"#b2\">Bordes et al. (2013)</ref> proposed to model relationships in the 1-D  local connections in knowledge graph.</p><p>Although the entity embeddings from KG embedding models <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013;</ref><ref type=\"bibr\" target=\"#b38\">Yang et al.,  embedding-based methods: RESCAL <ref type=\"bibr\" target=\"#b21\">(Nickel et al., 2011)</ref>, TransE <ref type=\"bibr\" target=\"#b2\">(Bordes et al., 2013)</ref>, Dist-Mult <ref type=\"bibr\" target=\"#b38\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: rks (CNNs) <ref type=\"bibr\" target=\"#b0\">[1]</ref> have achieved great success in acoustic modeling <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target \" target=\"#b6\">6]</ref>, like regular Deep Neural Networks (DNNs), which results in a hybrid system <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target the required non-linear modeling capabilities.</p><p>Unlike the time windows applied in DNN systems <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target ://www.tei-c.org/ns/1.0\"><head n=\"2.\">Convolutional Neural Networks</head><p>Most of the CNN models <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target n be very slow due to the iterative multiplications over time when the input sequence is very long; <ref type=\"bibr\" target=\"#b1\">(2)</ref> The training process is sometimes tricky due to the well-kno. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: type=\"bibr\" target=\"#b42\">43]</ref> and have connections to the large literature on metric learning <ref type=\"bibr\" target=\"#b47\">[48,</ref><ref type=\"bibr\" target=\"#b4\">5]</ref>.</p><p>As the name s function encourages learning from hard positives and hard negatives. We also show that triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref> is a special case of our loss when only a single positive  ss can thus be seen to be efficient in its training. Other contrastive losses, such as triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref>, often use the computationally expensive technique of hard .\">Connections to Triplet Loss</head><p>Contrastive learning is closely related to the triplet loss <ref type=\"bibr\" target=\"#b47\">[48]</ref>, which is one of the widely-used alternatives to cross-ent  contrastive learning are metric learning and triplet losses <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b47\">48,</ref><ref type=\"bibr\" target=\"#b39\">40]</ref>. These losses have . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: tructure characteristics and alleviate the sparsity, thus improving the rec-ommendation performance <ref type=\"bibr\" target=\"#b25\">(Wang et al., 2019)</ref>. For example, as shown in Figure <ref type= bedding because of its powerful representation learning based on node features and graph structure. <ref type=\"bibr\" target=\"#b25\">Wang et al. (2019)</ref> explored the GNN to capture high-order conne. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e embedding space. Word2vec is widely applied in NLP tasks <ref type=\"bibr\" target=\"#b18\">[19,</ref><ref type=\"bibr\" target=\"#b34\">35]</ref> but unable to represent entire documents. Paragraph Vectors. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: \" target=\"#b16\">[17]</ref>, or generating intermediate representation in speech-to-text translation <ref type=\"bibr\" target=\"#b17\">[18]</ref>. Our deliberation model has a similar structure as <ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  models perform competitively compared to more sophisticated conventional systems on Google traffic <ref type=\"bibr\" target=\"#b5\">[6,</ref><ref type=\"bibr\" target=\"#b6\">7]</ref>. Given its all-neural  ef>. Given its all-neural nature, an E2E model can be reasonably downsized to fit on mobile devices <ref type=\"bibr\" target=\"#b5\">[6]</ref>.</p><p>Despite the rapid progress made by E2E models, they s 9]</ref>. To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) <ref type=\"bibr\" target=\"#b5\">[6]</ref> and a large conventional model <ref type=\"bibr\" target=\"#b7\" ass hypotheses. The two-pass model achieves 17%-22% relative WER reduction (WERR) compared to RNN-T <ref type=\"bibr\" target=\"#b5\">[6]</ref> and has a similar WER to a large conventional model <ref typ ype=\"bibr\" target=\"#b9\">[10]</ref>, and thus use a two-step training process: Train the RNN-T as in <ref type=\"bibr\" target=\"#b5\">[6]</ref>, and then fix the RNN-T parameters and only train the delibe  recognition, we report performance on a side-by-side (SxS) test set, and 4 voice command test sets <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The SxS set contains utterances where the LAS rescoring mod >Architecture Details and Training</head><p>Our first-pass RNN-T model has the same architecture as <ref type=\"bibr\" target=\"#b5\">[6]</ref>. The encoder of the RNN-T consists of an 8-layer Long Short-. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: e Visual Instruction Set (VIS) in UltraSPARC TM <ref type=\"bibr\" target=\"#b15\">[16]</ref> and FBRAM <ref type=\"bibr\" target=\"#b17\">[18]</ref> from Sun Microsystems, MMX TM Technology <ref type=\"bibr\" . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: veloped Transformers, a new neural architecture for even more effective natural language processing <ref type=\"bibr\" target=\"#b42\">(Vaswani et al., 2017)</ref>. Transformers overcome a major drawback  al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some Transformers uses positional encoding <ref type=\"bibr\" target=\"#b42\">(Vaswani et al., 2017)</ref> or positional embedding <ref type=\"bibr\" presented in Sec 2.3.1. For other details (especially on the multi-head attention), please refer to <ref type=\"bibr\" target=\"#b42\">Vaswani et al. (2017)</ref> and in particular, <ref type=\"bibr\">GPT-2. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: elets. Later, <ref type=\"bibr\" target=\"#b16\">[17]</ref>, <ref type=\"bibr\" target=\"#b40\">[41]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref> integrate per-frame proposals into tubelets for re-scoring cly available, we follow the widely adopted protocols in <ref type=\"bibr\" target=\"#b14\">[15]</ref>, <ref type=\"bibr\" target=\"#b41\">[42]</ref>, <ref type=\"bibr\" target=\"#b23\">[24]</ref> to report the r. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: en 0 and 255, graph-structured data is often depicted in the discrete domains. Thus existing attack <ref type=\"bibr\" target=\"#b13\">(Dong et al., 2018;</ref><ref type=\"bibr\" target=\"#b26\">Szegedy et al. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: re summarized in TABLE 2.</p><p>\u2022 Pol.Blogs: The Pol.Blogs dataset is compiled by Adamic and Glance <ref type=\"bibr\" target=\"#b41\">[42]</ref>. This dataset is about political leaning collected from bl. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: m recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet <ref type=\"bibr\" target=\"#b37\">(Oord et al., 2016a;</ref><ref type=\"bibr\">b)</ref>. Discretization m. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: lso, we are interested to apply Eesen to various languages <ref type=\"bibr\" target=\"#b33\">[32,</ref><ref type=\"bibr\" target=\"#b34\">33,</ref><ref type=\"bibr\" target=\"#b35\">34]</ref> and different types. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: entation learning algorithms that use a contrastive loss have outperformed even supervised learning <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" targ rvation that a larger number of negative/positive examples in the objective leads to better results <ref type=\"bibr\" target=\"#b1\">[2,</ref><ref type=\"bibr\" target=\"#b15\">16]</ref>. The last two terms  </ref>, or different views of the same scene <ref type=\"bibr\" target=\"#b32\">[33]</ref>. Chen et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> extensively study verious data augmentation methods. For lan br\" target=\"#b22\">[23]</ref> and STL10 <ref type=\"bibr\" target=\"#b5\">[6]</ref>, we implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with ResNet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as  ef type=\"bibr\" target=\"#b18\">[19]</ref> with learning rate 0.001 and weight decay 1e \u2212 6. Following <ref type=\"bibr\" target=\"#b1\">[2]</ref>, we set the temperature t = 0.5 and the dimension of the lat /ns/1.0\"><head>B Experiment Details</head><p>Cifar10 and STL10 We adopt PyTorch to implement SimCLR <ref type=\"bibr\" target=\"#b1\">[2]</ref> with Resnet-50 <ref type=\"bibr\" target=\"#b14\">[15]</ref> as . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> has demonstrated superior performance to the previous hand-cr m, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> has drawn considerable attention due to its simple network st Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>. Motivated by SRCNN, some problems such as face hallucination ><p>We first briefly describe the network structure of SRCNN <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref>, and then we detail how we reformulate the network layer by l Different Upscaling Factors</head><p>Unlike existing methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2]</ref> that need to train a network from scratch for a different sca lgorithms are mostly learning-based (or patch-based) methods <ref type=\"bibr\" target=\"#b0\">[1,</ref><ref type=\"bibr\" target=\"#b1\">2,</ref><ref type=\"bibr\" target=\"#b2\">3,</ref><ref type=\"bibr\" target= space, then followed by a complex mapping to another high-dimensional HR feature space. Dong et al. <ref type=\"bibr\" target=\"#b1\">[2]</ref> show that the mapping accuracy can be substantially improved a wider mapping layer, but at the cost of the running time. For example, the large SRCNN (SRCNN-Ex) <ref type=\"bibr\" target=\"#b1\">[2]</ref> has 57,184 parameters, which are six times larger than that  with no pre-processing. 2) The proposed model achieves a speed up of at least 40\u00d7 than the SRCNN-Ex <ref type=\"bibr\" target=\"#b1\">[2]</ref> while still keeping its exceptional performance. One of its  ters in a layer) and depth (i.e., the number of layers) of the mapping layer. As indicated in SRCNN <ref type=\"bibr\" target=\"#b1\">[2]</ref>, a 5 \u00d7 5 layer achieves much better results than a 1 \u00d7 1 lay s an average PSNR of 32.87 dB, which is already higher than that of SRCNN-Ex (32.75 dB) reported in <ref type=\"bibr\" target=\"#b1\">[2]</ref>. The FSRCNN (48,12,2) contains only 8,832 parameters, then t F) <ref type=\"bibr\" target=\"#b6\">[7]</ref>, SRCNN <ref type=\"bibr\" target=\"#b0\">[1]</ref>, SRCNN-Ex <ref type=\"bibr\" target=\"#b1\">[2]</ref> and the sparse coding based network (SCN) <ref type=\"bibr\" t. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: f the procedure, additional gains can be achieved with an ensemble of multiple students generations <ref type=\"bibr\" target=\"#b6\">[7]</ref>. However, multiple self-distillation processes multiply the . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ecific sequence generation. A coverage driven test generation technique is presented by Fine et al. <ref type=\"bibr\" target=\"#b0\">[1]</ref>. Shen et al. <ref type=\"bibr\" target=\"#b7\">[8]</ref> have us. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\">[21]</ref>, node classification <ref type=\"bibr\" target=\"#b29\">[3]</ref>, link prediction <ref type=\"bibr\" target=\"#b36\">[10]</ref>, and recommendation <ref type=\"bibr\" target=\"#b49\">[23]</r world information network, the links observed are only a small proportion, with many others missing <ref type=\"bibr\" target=\"#b36\">[10]</ref>. A pair of nodes on a missing link has a zero first-order  ><p>wji log p1(vj, vi), or \u2212</p><formula xml:id=\"formula_12\">j\u2208N (i)</formula><p>wji log p2(vj|vi), <ref type=\"bibr\" target=\"#b36\">(10)</ref> by updating the embedding of the new vertex and keeping th. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: to-translations it has already been shown that a suitable structure for \u03c6 is a tensor field network <ref type=\"bibr\" target=\"#b24\">[25]</ref>, explained below. Note that Romero et al. <ref type=\"bibr\" q. <ref type=\"bibr\" target=\"#b4\">(5)</ref>.</p><p>Tensor Field Networks Tensor field networks (TFN) <ref type=\"bibr\" target=\"#b24\">[25]</ref> are neural networks, which map point clouds to point cloud annels, but we omit it here. Weiler et al. <ref type=\"bibr\" target=\"#b32\">[33]</ref>, Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref> and Kondor <ref type=\"bibr\" target=\"#b12\">[13]</ref> showe duces the kernel to a scalar w multiplied by the identity, W = w I, referred to as self-interaction <ref type=\"bibr\" target=\"#b24\">[25]</ref>. As such we can rewrite the TFN layer as</p><formula xml:i c c = w c c per representation degree, shared across all points.</p><p>As proposed in Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>, this is followed by a norm-based non-linearity.</p><p>Att tron-proton simulation.</p><p>Linear DeepSet <ref type=\"bibr\" target=\"#b39\">[40]</ref> Tensor Field <ref type=\"bibr\" target=\"#b24\">[25]</ref> Set Transformer <ref type=\"bibr\" target=\"#b13\">[14]</ref>   type=\"bibr\" target=\"#b13\">[14]</ref>, a non-equivariant attention model, and Tensor Field Networks <ref type=\"bibr\" target=\"#b24\">[25]</ref>, which is similar to SE(3)-Transformer but does not levera s to train significantly larger versions of both the SE(3)-Transformer and the Tensor Field network <ref type=\"bibr\" target=\"#b24\">[25]</ref> and to apply these models to real-world datasets.</p><p>Ou be found in many mathematical physics libraries.</p><p>Tensor Field Layers In Tensor Field Networks <ref type=\"bibr\" target=\"#b24\">[25]</ref> and 3D Steerable CNNs <ref type=\"bibr\" target=\"#b32\">[33]<  that all the Tensor Field networks we trained were significantly bigger than in the original paper <ref type=\"bibr\" target=\"#b24\">[25]</ref>, mostly enabled by the faster computation of the spherical k to obtain stable training. We used a norm based non-linearity for the Tensor Field network (as in <ref type=\"bibr\" target=\"#b24\">[25]</ref>) and no extra non-linearity (beyond the softmax in the sel he Tensor Field network and the linear baseline are SE(3) equivariant. For the Tensor Field Network <ref type=\"bibr\" target=\"#b24\">[25]</ref> baseline, we used the same hyper parameters as for the SE( linear self-interaction and an additional norm-based nonlinearity in each layer as in Thomas et al. <ref type=\"bibr\" target=\"#b24\">[25]</ref>. For the DeepSet <ref type=\"bibr\" target=\"#b39\">[40]</ref>. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: aints that help generate more content rich responses that are based on a model of syntax and topics <ref type=\"bibr\" target=\"#b12\">(Griffiths et al., 2005)</ref> and semantic similarity <ref type=\"bib o estimate these distributions, we leverage the unsupervised model of topics and syntax proposed by <ref type=\"bibr\" target=\"#b12\">Griffiths and Steyvers (2005)</ref>. The second constraint encourages. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: as in <ref type=\"bibr\">(Zhang et al., 2015)</ref>. We initialize our convolutional layers following <ref type=\"bibr\" target=\"#b5\">(He et al., 2015)</ref>. One epoch took from 24 minutes to 2h45 for de. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: pretrain the word embeddings -they are learned from scratch during training. We train using Adagrad <ref type=\"bibr\" target=\"#b5\">(Duchi et al., 2011)</ref> with learning rate 0.15 and an initial accu. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: cs. The proposal stage (e.g., Selective Search <ref type=\"bibr\" target=\"#b33\">[34]</ref>, EdgeBoxes <ref type=\"bibr\" target=\"#b36\">[37]</ref>, DeepMask <ref type=\"bibr\" target=\"#b22\">[23,</ref><ref ty. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: n technique for achieving truthfulness in online auctions is based on the concept of a supply curve <ref type=\"bibr\" target=\"#b21\">[22]</ref>, as applied by Zhang et al. <ref type=\"bibr\" target=\"#b3\">. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: pe=\"bibr\" target=\"#b34\">31]</ref>, ASPP <ref type=\"bibr\" target=\"#b6\">[3]</ref>, and Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref>.</p><p>The Inception block adopts multiple branches with dif all the positions equally, probably leading to confusion between object and context. Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref> learns distinctive resolutions of individual objects, unfort pe=\"bibr\" target=\"#b36\">[33]</ref>, ASPP <ref type=\"bibr\" target=\"#b6\">[3]</ref> and Deformable CNN <ref type=\"bibr\" target=\"#b7\">[4]</ref>. For Inception, besides the original version, we change its . Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is:  order to achieve better power/performance characteristics <ref type=\"bibr\" target=\"#b0\">[1]</ref>- <ref type=\"bibr\" target=\"#b11\">[12]</ref>. Similarly, on the software side, dynamic code optimizatio phase changes <ref type=\"bibr\" target=\"#b1\">[2]</ref>[10] <ref type=\"bibr\" target=\"#b10\">[11]</ref> <ref type=\"bibr\" target=\"#b11\">[12]</ref> [19] <ref type=\"bibr\" target=\"#b17\">[20]</ref> <ref type=\" icting phases <ref type=\"bibr\" target=\"#b1\">[2]</ref>[10] <ref type=\"bibr\" target=\"#b10\">[11]</ref> <ref type=\"bibr\" target=\"#b11\">[12]</ref> <ref type=\"bibr\" target=\"#b16\">[19]</ref>. In this work, w lts indicate that as few as 32 24-bit counters are sufficient to represent BBVs.</p><p>Huang et al. <ref type=\"bibr\" target=\"#b11\">[12]</ref> use subroutines to identify program phases. They propose t rvals at the beginning of a stable phase to complete the tuning process (the algorithm presented in <ref type=\"bibr\" target=\"#b11\">[12]</ref> is an exception). If phases are short (i.e. small number o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: -word dictionary by greedily keep the most frequent co-occurring character sequences. Concurrently, <ref type=\"bibr\" target=\"#b14\">[15]</ref> borrow the practice in voice search <ref type=\"bibr\" targe. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: #b3\">[4]</ref><ref type=\"bibr\" target=\"#b4\">[5]</ref><ref type=\"bibr\" target=\"#b5\">[6]</ref> and MT <ref type=\"bibr\" target=\"#b6\">[7]</ref><ref type=\"bibr\" target=\"#b7\">[8]</ref><ref type=\"bibr\" targe -</note> \t\t\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"7\" xml:id=\"foot_3\">insensitive<ref type=\"bibr\" target=\"#b6\">7</ref> . The translation models are evaluated using the official scri. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ent and separation <ref type=\"bibr\" target=\"#b17\">[18,</ref><ref type=\"bibr\" target=\"#b18\">19,</ref><ref type=\"bibr\" target=\"#b19\">20,</ref><ref type=\"bibr\" target=\"#b2\">3]</ref>. However, this usuall. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ational learning <ref type=\"bibr\">(Nickel, Rosasco, and Poggio 2016)</ref>, and ontology population <ref type=\"bibr\" target=\"#b4\">(Chen et al. 2018)</ref>.</p><p>While current embedding models focus o. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: used by data sparseness, only to a limited extent. Instead, researchers have explored meta-learning <ref type=\"bibr\" target=\"#b4\">(Finn et al., 2017)</ref> to leverage the distribution over similar ta th complex iterative inference strategies. More recently, many approaches have used a meta-learning <ref type=\"bibr\" target=\"#b4\">(Finn et al., 2017;</ref><ref type=\"bibr\" target=\"#b11\">Mishra et al.,. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: hing that often occurs in direct-mapped buffers. A 3-bit performance counter based on Heil's design <ref type=\"bibr\" target=\"#b16\">[17]</ref> tracks the effectiveness of each entry and is used to sele pendence chain. If the generating values are present then ARVI's predictions are near perfect. Heil <ref type=\"bibr\" target=\"#b16\">[17]</ref> proposed another approach that correlates on the differenc. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: predicted probabilities. Note that this is a generalization of the decision-only setting defined in <ref type=\"bibr\" target=\"#b3\">Brendel et al. (2018)</ref>, where k = 1, and the attacker only has ac 0\"><head n=\"4.2.\">Adversarial attacks with limited information</head><p>Our work is concurrent with <ref type=\"bibr\" target=\"#b3\">Brendel et al. (2018)</ref>, which also explores the label-only case u. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is: ms that are applied to object segmentation based on CNNs <ref type=\"bibr\" target=\"#b21\">[22]</ref>- <ref type=\"bibr\" target=\"#b23\">[24]</ref>. Because the performance of deep learning algorithms depen. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: n n \u00d7 n conv layer followed by two sibling 1 \u00d7 1 conv layers (for reg and cls, respectively). ReLUs <ref type=\"bibr\" target=\"#b14\">[15]</ref> are applied to the output of the n \u00d7 n conv layer.</p></di. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
{"content": "The context is:  loss from the image-space to a higher-level feature space of an object recognition system like VGG <ref type=\"bibr\" target=\"#b48\">[49]</ref>, resulting in sharper results despite lower PSNR values.</ etwork once to get the result. The exclusive use of 3\u00d73 filters is inspired by the VGG architecture <ref type=\"bibr\" target=\"#b48\">[49]</ref> and allows for deeper models at a low number of parameters  map \u03c6, we use a pre-trained implementation of the popular VGG-19 network <ref type=\"bibr\">[1,</ref><ref type=\"bibr\" target=\"#b48\">49]</ref>. It consists of stacked convolutions coupled with pooling l. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "Yes"}
{"content": "The context is: ph structure is utilized by GNNs to operate convolution directly on graphs by passing node features <ref type=\"bibr\" target=\"#b11\">[12,</ref><ref type=\"bibr\" target=\"#b13\">14]</ref> to neighbors, or p  type=\"bibr\" target=\"#b37\">38]</ref> and nonspectral methods <ref type=\"bibr\" target=\"#b6\">[7,</ref><ref type=\"bibr\" target=\"#b11\">12,</ref><ref type=\"bibr\" target=\"#b13\">14,</ref><ref type=\"bibr\" tar. Is the current reference important? Please answer Yes or No. The answer is [MASK].", "summary": "No"}
